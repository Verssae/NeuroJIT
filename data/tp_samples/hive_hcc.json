{
    "d7e2745eac5e4d56e8af928e98626194406ca9f2": [
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::getReduceSinkOp(List,List,List,List,ArrayList,ArrayList,int,Operator,AcidUtils)",
            " 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  ",
            "    public ReduceSinkOperator getReduceSinkOp(List<Integer> partitionPositions,\n        List<Integer> sortPositions, List<Integer> sortOrder, List<Integer> sortNullOrder,\n        ArrayList<ExprNodeDesc> allCols, ArrayList<ExprNodeDesc> bucketColumns, int numBuckets,\n        Operator<? extends OperatorDesc> parent, AcidUtils.Operation writeType) throws SemanticException {\n\n      // Order of KEY columns\n      // 1) Partition columns\n      // 2) Bucket number column\n      // 3) Sort columns\n      Set<Integer> keyColsPosInVal = Sets.newLinkedHashSet();\n      ArrayList<ExprNodeDesc> keyCols = Lists.newArrayList();\n      List<Integer> newSortOrder = Lists.newArrayList();\n      List<Integer> newSortNullOrder = Lists.newArrayList();\n      int numPartAndBuck = partitionPositions.size();\n\n      keyColsPosInVal.addAll(partitionPositions);\n      if (!bucketColumns.isEmpty()) {\n        keyColsPosInVal.add(-1);\n        numPartAndBuck += 1;\n      }\n      keyColsPosInVal.addAll(sortPositions);\n\n      // by default partition and bucket columns are sorted in ascending order\n      Integer order = 1;\n      if (sortOrder != null && !sortOrder.isEmpty()) {\n        if (sortOrder.get(0).intValue() == 0) {\n          order = 0;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortOrder.add(order);\n      }\n      newSortOrder.addAll(sortOrder);\n\n      String orderStr = \"\";\n      for (Integer i : newSortOrder) {\n        if(i.intValue() == 1) {\n          orderStr += \"+\";\n        } else {\n          orderStr += \"-\";\n        }\n      }\n\n      // if partition and bucket columns are sorted in ascending order, by default\n      // nulls come first; otherwise nulls come last\n      Integer nullOrder = order == 1 ? 0 : 1;\n      if (sortNullOrder != null && !sortNullOrder.isEmpty()) {\n        if (sortNullOrder.get(0).intValue() == 0) {\n          nullOrder = 0;\n        } else {\n          nullOrder = 1;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortNullOrder.add(nullOrder);\n      }\n      newSortNullOrder.addAll(sortNullOrder);\n\n      String nullOrderStr = \"\";\n      for (Integer i : newSortNullOrder) {\n        if(i.intValue() == 0) {\n          nullOrderStr += \"a\";\n        } else {\n          nullOrderStr += \"z\";\n        }\n      }\n\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      ArrayList<ExprNodeDesc> partCols = Lists.newArrayList();\n\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      for (Integer idx : keyColsPosInVal) {\n        if (idx < 0) {\n          ExprNodeConstantDesc bucketNumCol = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, BUCKET_NUMBER_COL_NAME);\n          keyCols.add(bucketNumCol);\n          colExprMap.put(Utilities.ReduceField.KEY + \".'\" +BUCKET_NUMBER_COL_NAME+\"'\", bucketNumCol);\n        } else {\n          keyCols.add(allCols.get(idx).clone());\n        }\n      }\n\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (!keyColsPosInVal.contains(i)) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      for (Integer idx : partitionPositions) {\n        partCols.add(allCols.get(idx).clone());\n      }\n\n      // in the absence of SORTED BY clause, the sorted dynamic partition insert\n      // should honor the ordering of records provided by ORDER BY in SELECT statement\n      ReduceSinkOperator parentRSOp = OperatorUtils.findSingleOperatorUpstream(parent,\n          ReduceSinkOperator.class);\n      if (parentRSOp != null && parseCtx.getQueryProperties().hasOuterOrderBy()) {\n        String parentRSOpOrder = parentRSOp.getConf().getOrder();\n        String parentRSOpNullOrder = parentRSOp.getConf().getNullOrder();\n        if (parentRSOpOrder != null && !parentRSOpOrder.isEmpty() && sortPositions.isEmpty()) {\n          keyCols.addAll(parentRSOp.getConf().getKeyCols());\n          orderStr += parentRSOpOrder;\n          nullOrderStr += parentRSOpNullOrder;\n        }\n      }\n\n      // map _col0 to KEY._col0, etc\n      Map<String, String> nameMapping = new HashMap<>();\n      ArrayList<String> keyColNames = Lists.newArrayList();\n      for (ExprNodeDesc keyCol : keyCols) {\n        String keyColName = keyCol.getExprString();\n        keyColNames.add(keyColName);\n        colExprMap.put(Utilities.ReduceField.KEY + \".\" +keyColName, keyCol);\n        nameMapping.put(keyColName, Utilities.ReduceField.KEY + \".\" + keyColName);\n      }\n      ArrayList<String> valColNames = Lists.newArrayList();\n      for (ExprNodeDesc valCol : valCols) {\n        String colName = valCol.getExprString();\n        valColNames.add(colName);\n        colExprMap.put(Utilities.ReduceField.VALUE + \".\" + colName, valCol);\n        nameMapping.put(colName, Utilities.ReduceField.VALUE + \".\" + colName);\n      }\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable, writeType);\n      rsConf.setBucketCols(bucketColumns);\n      rsConf.setNumBuckets(numBuckets);\n\n      ArrayList<ColumnInfo> signature = new ArrayList<>();\n      for (int index = 0; index < parent.getSchema().getSignature().size(); index++) {\n        ColumnInfo colInfo = new ColumnInfo(parent.getSchema().getSignature().get(index));\n        colInfo.setInternalName(nameMapping.get(colInfo.getInternalName()));\n        signature.add(colInfo);\n      }\n      ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }",
            " 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  ",
            "    public ReduceSinkOperator getReduceSinkOp(List<Integer> partitionPositions,\n        List<Integer> sortPositions, List<Integer> sortOrder, List<Integer> sortNullOrder,\n        ArrayList<ExprNodeDesc> allCols, ArrayList<ExprNodeDesc> bucketColumns, int numBuckets,\n        Operator<? extends OperatorDesc> parent, AcidUtils.Operation writeType) throws SemanticException {\n\n      // Order of KEY columns\n      // 1) Partition columns\n      // 2) Bucket number column\n      // 3) Sort columns\n      Set<Integer> keyColsPosInVal = Sets.newLinkedHashSet();\n      ArrayList<ExprNodeDesc> keyCols = Lists.newArrayList();\n      List<Integer> newSortOrder = Lists.newArrayList();\n      List<Integer> newSortNullOrder = Lists.newArrayList();\n      int numPartAndBuck = partitionPositions.size();\n\n      keyColsPosInVal.addAll(partitionPositions);\n      if (!bucketColumns.isEmpty() || writeType == Operation.DELETE || writeType == Operation.UPDATE) {\n        keyColsPosInVal.add(-1);\n        numPartAndBuck += 1;\n      }\n      keyColsPosInVal.addAll(sortPositions);\n\n      // by default partition and bucket columns are sorted in ascending order\n      Integer order = 1;\n      if (sortOrder != null && !sortOrder.isEmpty()) {\n        if (sortOrder.get(0).intValue() == 0) {\n          order = 0;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortOrder.add(order);\n      }\n      newSortOrder.addAll(sortOrder);\n\n      String orderStr = \"\";\n      for (Integer i : newSortOrder) {\n        if(i.intValue() == 1) {\n          orderStr += \"+\";\n        } else {\n          orderStr += \"-\";\n        }\n      }\n\n      // if partition and bucket columns are sorted in ascending order, by default\n      // nulls come first; otherwise nulls come last\n      Integer nullOrder = order == 1 ? 0 : 1;\n      if (sortNullOrder != null && !sortNullOrder.isEmpty()) {\n        if (sortNullOrder.get(0).intValue() == 0) {\n          nullOrder = 0;\n        } else {\n          nullOrder = 1;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortNullOrder.add(nullOrder);\n      }\n      newSortNullOrder.addAll(sortNullOrder);\n\n      String nullOrderStr = \"\";\n      for (Integer i : newSortNullOrder) {\n        if(i.intValue() == 0) {\n          nullOrderStr += \"a\";\n        } else {\n          nullOrderStr += \"z\";\n        }\n      }\n\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      ArrayList<ExprNodeDesc> partCols = Lists.newArrayList();\n\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      for (Integer idx : keyColsPosInVal) {\n        if (idx < 0) {\n          ExprNodeConstantDesc bucketNumCol = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, BUCKET_NUMBER_COL_NAME);\n          keyCols.add(bucketNumCol);\n          colExprMap.put(Utilities.ReduceField.KEY + \".'\" +BUCKET_NUMBER_COL_NAME+\"'\", bucketNumCol);\n        } else {\n          keyCols.add(allCols.get(idx).clone());\n        }\n      }\n\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (!keyColsPosInVal.contains(i)) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      for (Integer idx : partitionPositions) {\n        partCols.add(allCols.get(idx).clone());\n      }\n\n      // in the absence of SORTED BY clause, the sorted dynamic partition insert\n      // should honor the ordering of records provided by ORDER BY in SELECT statement\n      ReduceSinkOperator parentRSOp = OperatorUtils.findSingleOperatorUpstream(parent,\n          ReduceSinkOperator.class);\n      if (parentRSOp != null && parseCtx.getQueryProperties().hasOuterOrderBy()) {\n        String parentRSOpOrder = parentRSOp.getConf().getOrder();\n        String parentRSOpNullOrder = parentRSOp.getConf().getNullOrder();\n        if (parentRSOpOrder != null && !parentRSOpOrder.isEmpty() && sortPositions.isEmpty()) {\n          keyCols.addAll(parentRSOp.getConf().getKeyCols());\n          orderStr += parentRSOpOrder;\n          nullOrderStr += parentRSOpNullOrder;\n        }\n      }\n\n      // map _col0 to KEY._col0, etc\n      Map<String, String> nameMapping = new HashMap<>();\n      ArrayList<String> keyColNames = Lists.newArrayList();\n      for (ExprNodeDesc keyCol : keyCols) {\n        String keyColName = keyCol.getExprString();\n        keyColNames.add(keyColName);\n        colExprMap.put(Utilities.ReduceField.KEY + \".\" +keyColName, keyCol);\n        nameMapping.put(keyColName, Utilities.ReduceField.KEY + \".\" + keyColName);\n      }\n      ArrayList<String> valColNames = Lists.newArrayList();\n      for (ExprNodeDesc valCol : valCols) {\n        String colName = valCol.getExprString();\n        valColNames.add(colName);\n        colExprMap.put(Utilities.ReduceField.VALUE + \".\" + colName, valCol);\n        nameMapping.put(colName, Utilities.ReduceField.VALUE + \".\" + colName);\n      }\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable, writeType);\n      rsConf.setBucketCols(bucketColumns);\n      rsConf.setNumBuckets(numBuckets);\n\n      ArrayList<ColumnInfo> signature = new ArrayList<>();\n      for (int index = 0; index < parent.getSchema().getSignature().size(); index++) {\n        ColumnInfo colInfo = new ColumnInfo(parent.getSchema().getSignature().get(index));\n        colInfo.setInternalName(nameMapping.get(colInfo.getInternalName()));\n        signature.add(colInfo);\n      }\n      ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }"
        ]
    ],
    "ed82cfa914769cfabfc7460b7b5abbdae71e562a": [
        [
            "FileSinkOperator::process(Object,int)",
            " 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 -\n 770 -\n 771  \n 772  \n 773  \n 774 -\n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781 -\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n    runTimeNumRows++;\n    /* Create list bucketing sub-directory only if stored-as-directories is on. */\n    String lbDirName = null;\n    lbDirName = (lbCtx == null) ? null : generateListBucketingDirName(row);\n\n    if (!bDynParts && !filesCreated) {\n      if (lbDirName != null) {\n        FSPaths fsp2 = lookupListBucketingPaths(lbDirName);\n      } else {\n        createBucketFiles(fsp);\n      }\n    }\n\n    try {\n      updateProgress();\n\n      // if DP is enabled, get the final output writers and prepare the real output row\n      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT\n          : \"input object inspector is not struct\";\n\n      if (bDynParts) {\n\n        // we need to read bucket number which is the last column in value (after partition columns)\n        if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {\n          numDynParts += 1;\n        }\n\n        // copy the DP column values from the input row to dpVals\n        dpVals.clear();\n        dpWritables.clear();\n        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol,numDynParts,\n            (StructObjectInspector) inputObjInspectors[0],ObjectInspectorCopyOption.WRITABLE);\n\n        // get a set of RecordWriter based on the DP column values\n        // pass the null value along to the escaping process to determine what the dir should be\n        for (Object o : dpWritables) {\n          if (o == null || o.toString().length() == 0) {\n            dpVals.add(dpCtx.getDefaultPartitionName());\n          } else {\n            dpVals.add(o.toString());\n          }\n        }\n\n        String invalidPartitionVal;\n        if((invalidPartitionVal = HiveStringUtils.getPartitionValWithInvalidCharacter(dpVals, dpCtx.getWhiteListPattern()))!=null) {\n          throw new HiveFatalException(\"Partition value '\" + invalidPartitionVal +\n              \"' contains a character not matched by whitelist pattern '\" +\n              dpCtx.getWhiteListPattern().toString() + \"'.  \" + \"(configure with \" +\n              HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname + \")\");\n        }\n        fpaths = getDynOutPaths(dpVals, lbDirName);\n\n        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row\n        recordValue = serializer.serialize(row, subSetOI);\n      } else {\n        if (lbDirName != null) {\n          fpaths = lookupListBucketingPaths(lbDirName);\n        } else {\n          fpaths = fsp;\n        }\n        recordValue = serializer.serialize(row, inputObjInspectors[0]);\n        // if serializer is ThriftJDBCBinarySerDe, then recordValue is null if the buffer is not full (the size of buffer\n        // is kept track of in the SerDe)\n        if (recordValue == null) {\n          return;\n        }\n      }\n\n      rowOutWriters = fpaths.outWriters;\n      // check if all record writers implement statistics. if atleast one RW\n      // doesn't implement stats interface we will fallback to conventional way\n      // of gathering stats\n      isCollectRWStats = areAllTrue(statsFromRecordWriter);\n      if (conf.isGatherStats() && !isCollectRWStats) {\n        SerDeStats stats = serializer.getSerDeStats();\n        if (stats != null) {\n          fpaths.stat.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());\n        }\n        fpaths.stat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n      }\n\n      if ((++numRows == cntr) && isLogInfoEnabled) {\n        cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;\n        if (cntr < 0 || numRows < 0) {\n          cntr = 0;\n          numRows = 1;\n        }\n        LOG.info(toString() + \": records written - \" + numRows);\n      }\n\n      // This should always be 0 for the final result file\n      int writerOffset = findWriterOffset(row);\n      // This if/else chain looks ugly in the inner loop, but given that it will be 100% the same\n      // for a given operator branch prediction should work quite nicely on it.\n      // RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we\n      // pass the row rather than recordValue.\n      if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID) {\n        rowOutWriters[writerOffset].write(recordValue);\n      } else if (conf.getWriteType() == AcidUtils.Operation.INSERT) {\n        fpaths.updaters[writerOffset].insert(conf.getTransactionId(), row);\n      } else {\n        // TODO I suspect we could skip much of the stuff above this in the function in the case\n        // of update and delete.  But I don't understand all of the side effects of the above\n        // code and don't want to skip over it yet.\n\n        // Find the bucket id, and switch buckets if need to\n        ObjectInspector rowInspector = bDynParts ? subSetOI : outputObjInspector;\n        Object recId = ((StructObjectInspector)rowInspector).getStructFieldData(row, recIdField);\n        int bucketNum =\n            bucketInspector.get(recIdInspector.getStructFieldData(recId, bucketField));\n        if (fpaths.acidLastBucket != bucketNum) {\n          fpaths.acidLastBucket = bucketNum;\n          // Switch files\n          fpaths.updaters[++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(\n              jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[fpaths.acidFileOffset],\n              rowInspector, reporter, 0);\n          if (isDebugEnabled) {\n            LOG.debug(\"Created updater for bucket number \" + bucketNum + \" using file \" +\n                fpaths.outPaths[fpaths.acidFileOffset]);\n          }\n        }\n\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE) {\n          fpaths.updaters[fpaths.acidFileOffset].update(conf.getTransactionId(), row);\n        } else if (conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          fpaths.updaters[fpaths.acidFileOffset].delete(conf.getTransactionId(), row);\n        } else {\n          throw new HiveException(\"Unknown write type \" + conf.getWriteType().toString());\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(e);\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n  }",
            " 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 +\n 770 +\n 771  \n 772  \n 773  \n 774 +\n 775  \n 776  \n 777  \n 778  \n 779 +\n 780  \n 781 +\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n    runTimeNumRows++;\n    /* Create list bucketing sub-directory only if stored-as-directories is on. */\n    String lbDirName = null;\n    lbDirName = (lbCtx == null) ? null : generateListBucketingDirName(row);\n\n    if (!bDynParts && !filesCreated) {\n      if (lbDirName != null) {\n        FSPaths fsp2 = lookupListBucketingPaths(lbDirName);\n      } else {\n        createBucketFiles(fsp);\n      }\n    }\n\n    try {\n      updateProgress();\n\n      // if DP is enabled, get the final output writers and prepare the real output row\n      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT\n          : \"input object inspector is not struct\";\n\n      if (bDynParts) {\n\n        // we need to read bucket number which is the last column in value (after partition columns)\n        if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {\n          numDynParts += 1;\n        }\n\n        // copy the DP column values from the input row to dpVals\n        dpVals.clear();\n        dpWritables.clear();\n        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol,numDynParts,\n            (StructObjectInspector) inputObjInspectors[0],ObjectInspectorCopyOption.WRITABLE);\n\n        // get a set of RecordWriter based on the DP column values\n        // pass the null value along to the escaping process to determine what the dir should be\n        for (Object o : dpWritables) {\n          if (o == null || o.toString().length() == 0) {\n            dpVals.add(dpCtx.getDefaultPartitionName());\n          } else {\n            dpVals.add(o.toString());\n          }\n        }\n\n        String invalidPartitionVal;\n        if((invalidPartitionVal = HiveStringUtils.getPartitionValWithInvalidCharacter(dpVals, dpCtx.getWhiteListPattern()))!=null) {\n          throw new HiveFatalException(\"Partition value '\" + invalidPartitionVal +\n              \"' contains a character not matched by whitelist pattern '\" +\n              dpCtx.getWhiteListPattern().toString() + \"'.  \" + \"(configure with \" +\n              HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname + \")\");\n        }\n        fpaths = getDynOutPaths(dpVals, lbDirName);\n\n        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row\n        recordValue = serializer.serialize(row, subSetOI);\n      } else {\n        if (lbDirName != null) {\n          fpaths = lookupListBucketingPaths(lbDirName);\n        } else {\n          fpaths = fsp;\n        }\n        recordValue = serializer.serialize(row, inputObjInspectors[0]);\n        // if serializer is ThriftJDBCBinarySerDe, then recordValue is null if the buffer is not full (the size of buffer\n        // is kept track of in the SerDe)\n        if (recordValue == null) {\n          return;\n        }\n      }\n\n      rowOutWriters = fpaths.outWriters;\n      // check if all record writers implement statistics. if atleast one RW\n      // doesn't implement stats interface we will fallback to conventional way\n      // of gathering stats\n      isCollectRWStats = areAllTrue(statsFromRecordWriter);\n      if (conf.isGatherStats() && !isCollectRWStats) {\n        SerDeStats stats = serializer.getSerDeStats();\n        if (stats != null) {\n          fpaths.stat.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());\n        }\n        fpaths.stat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n      }\n\n      if ((++numRows == cntr) && isLogInfoEnabled) {\n        cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;\n        if (cntr < 0 || numRows < 0) {\n          cntr = 0;\n          numRows = 1;\n        }\n        LOG.info(toString() + \": records written - \" + numRows);\n      }\n\n      // This should always be 0 for the final result file\n      int writerOffset = findWriterOffset(row);\n      // This if/else chain looks ugly in the inner loop, but given that it will be 100% the same\n      // for a given operator branch prediction should work quite nicely on it.\n      // RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we\n      // pass the row rather than recordValue.\n      if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID) {\n        rowOutWriters[writerOffset].write(recordValue);\n      } else if (conf.getWriteType() == AcidUtils.Operation.INSERT) {\n        fpaths.updaters[writerOffset].insert(conf.getTransactionId(), row);\n      } else {\n        // TODO I suspect we could skip much of the stuff above this in the function in the case\n        // of update and delete.  But I don't understand all of the side effects of the above\n        // code and don't want to skip over it yet.\n\n        // Find the bucket id, and switch buckets if need to\n        ObjectInspector rowInspector = bDynParts ? subSetOI : outputObjInspector;\n        Object recId = ((StructObjectInspector)rowInspector).getStructFieldData(row, recIdField);\n        int bucketNum =\n            bucketInspector.get(recIdInspector.getStructFieldData(recId, bucketField));\n        if (fpaths.acidLastBucket != bucketNum) {\n          fpaths.acidLastBucket = bucketNum;\n          // Switch files\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 : ++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(\n              jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset],\n              rowInspector, reporter, 0);\n          if (isDebugEnabled) {\n            LOG.debug(\"Created updater for bucket number \" + bucketNum + \" using file \" +\n                fpaths.outPaths[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset]);\n          }\n        }\n\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE) {\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset].update(conf.getTransactionId(), row);\n        } else if (conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset].delete(conf.getTransactionId(), row);\n        } else {\n          throw new HiveException(\"Unknown write type \" + conf.getWriteType().toString());\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(e);\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271 -\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271 +\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty() || fsOp.getConf().getWriteType() == Operation.DELETE || fsOp.getConf().getWriteType() == Operation.UPDATE) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0 || fsOp.getConf().getWriteType() == Operation.DELETE || fsOp.getConf().getWriteType() == Operation.UPDATE) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }"
        ],
        [
            "ReduceSinkOperator::initializeOp(Configuration)",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    try {\n\n      numRows = 0;\n      cntr = 1;\n      logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);\n\n      statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);\n\n      List<ExprNodeDesc> keys = conf.getKeyCols();\n\n      if (isLogDebugEnabled) {\n        LOG.debug(\"keys size is \" + keys.size());\n        for (ExprNodeDesc k : keys) {\n          LOG.debug(\"Key exprNodeDesc \" + k.getExprString());\n        }\n      }\n\n      keyEval = new ExprNodeEvaluator[keys.size()];\n      int i = 0;\n      for (ExprNodeDesc e : keys) {\n        keyEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      numDistributionKeys = conf.getNumDistributionKeys();\n      distinctColIndices = conf.getDistinctColumnIndices();\n      numDistinctExprs = distinctColIndices.size();\n\n      valueEval = new ExprNodeEvaluator[conf.getValueCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getValueCols()) {\n        valueEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      partitionEval = new ExprNodeEvaluator[conf.getPartitionCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getPartitionCols()) {\n        int index = ExprNodeDescUtils.indexOf(e, keys);\n        partitionEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e): keyEval[index];\n      }\n\n      if (conf.getBucketCols() != null && !conf.getBucketCols().isEmpty()) {\n        bucketEval = new ExprNodeEvaluator[conf.getBucketCols().size()];\n\n        i = 0;\n        for (ExprNodeDesc e : conf.getBucketCols()) {\n          int index = ExprNodeDescUtils.indexOf(e, keys);\n          bucketEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e) : keyEval[index];\n        }\n\n        buckColIdxInKey = conf.getPartitionCols().size();\n      }\n\n      tag = conf.getTag();\n      tagByte[0] = (byte) tag;\n      skipTag = conf.getSkipTag();\n      if (isLogInfoEnabled) {\n        LOG.info(\"Using tag = \" + tag);\n      }\n\n      TableDesc keyTableDesc = conf.getKeySerializeInfo();\n      keySerializer = (Serializer) keyTableDesc.getDeserializerClass()\n          .newInstance();\n      keySerializer.initialize(null, keyTableDesc.getProperties());\n      keyIsText = keySerializer.getSerializedClass().equals(Text.class);\n\n      TableDesc valueTableDesc = conf.getValueSerializeInfo();\n      valueSerializer = (Serializer) valueTableDesc.getDeserializerClass()\n          .newInstance();\n      valueSerializer.initialize(null, valueTableDesc.getProperties());\n\n      int limit = conf.getTopN();\n      float memUsage = conf.getTopNMemoryUsage();\n\n      if (limit >= 0 && memUsage > 0) {\n        reducerHash = conf.isPTFReduceSink() ? new PTFTopNHash() : new TopNHash();\n        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);\n      }\n\n      useUniformHash = conf.getReducerTraits().contains(UNIFORM);\n\n      firstRow = true;\n    } catch (Exception e) {\n      String msg = \"Error initializing ReduceSinkOperator: \" + e.getMessage();\n      LOG.error(msg, e);\n      throw new RuntimeException(e);\n    }\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190 +\n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    try {\n\n      numRows = 0;\n      cntr = 1;\n      logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);\n\n      statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);\n\n      List<ExprNodeDesc> keys = conf.getKeyCols();\n\n      if (isLogDebugEnabled) {\n        LOG.debug(\"keys size is \" + keys.size());\n        for (ExprNodeDesc k : keys) {\n          LOG.debug(\"Key exprNodeDesc \" + k.getExprString());\n        }\n      }\n\n      keyEval = new ExprNodeEvaluator[keys.size()];\n      int i = 0;\n      for (ExprNodeDesc e : keys) {\n        if (e instanceof ExprNodeConstantDesc && (\"_bucket_number\").equals(((ExprNodeConstantDesc)e).getValue())) {\n          buckColIdxInKeyForAcid = i;\n        }\n        keyEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      numDistributionKeys = conf.getNumDistributionKeys();\n      distinctColIndices = conf.getDistinctColumnIndices();\n      numDistinctExprs = distinctColIndices.size();\n\n      valueEval = new ExprNodeEvaluator[conf.getValueCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getValueCols()) {\n        valueEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      partitionEval = new ExprNodeEvaluator[conf.getPartitionCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getPartitionCols()) {\n        int index = ExprNodeDescUtils.indexOf(e, keys);\n        partitionEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e): keyEval[index];\n      }\n\n      if (conf.getBucketCols() != null && !conf.getBucketCols().isEmpty()) {\n        bucketEval = new ExprNodeEvaluator[conf.getBucketCols().size()];\n\n        i = 0;\n        for (ExprNodeDesc e : conf.getBucketCols()) {\n          int index = ExprNodeDescUtils.indexOf(e, keys);\n          bucketEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e) : keyEval[index];\n        }\n\n        buckColIdxInKey = conf.getPartitionCols().size();\n      }\n\n      tag = conf.getTag();\n      tagByte[0] = (byte) tag;\n      skipTag = conf.getSkipTag();\n      if (isLogInfoEnabled) {\n        LOG.info(\"Using tag = \" + tag);\n      }\n\n      TableDesc keyTableDesc = conf.getKeySerializeInfo();\n      keySerializer = (Serializer) keyTableDesc.getDeserializerClass()\n          .newInstance();\n      keySerializer.initialize(null, keyTableDesc.getProperties());\n      keyIsText = keySerializer.getSerializedClass().equals(Text.class);\n\n      TableDesc valueTableDesc = conf.getValueSerializeInfo();\n      valueSerializer = (Serializer) valueTableDesc.getDeserializerClass()\n          .newInstance();\n      valueSerializer.initialize(null, valueTableDesc.getProperties());\n\n      int limit = conf.getTopN();\n      float memUsage = conf.getTopNMemoryUsage();\n\n      if (limit >= 0 && memUsage > 0) {\n        reducerHash = conf.isPTFReduceSink() ? new PTFTopNHash() : new TopNHash();\n        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);\n      }\n\n      useUniformHash = conf.getReducerTraits().contains(UNIFORM);\n\n      firstRow = true;\n    } catch (Exception e) {\n      String msg = \"Error initializing ReduceSinkOperator: \" + e.getMessage();\n      LOG.error(msg, e);\n      throw new RuntimeException(e);\n    }\n  }"
        ],
        [
            "ReduceSinkOperator::process(Object,int)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Exptected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369 +\n 370 +\n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Exptected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        if (buckColIdxInKeyForAcid != -1) {\n          cachedKeys[0][buckColIdxInKeyForAcid] = new Text(String.valueOf(bucketNumber));\n        }\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ]
    ],
    "04b303b6957a6b6f580ee10a4215b21071d87999": [
        [
            "Hive::loadPartition(Path,Table,Map,boolean,boolean,boolean,boolean,boolean,boolean)",
            "1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }",
            "1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571 +\n1572 +\n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584 +\n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      PerfLogger perfLogger = SessionState.getPerfLogger();\n      perfLogger.PerfLogBegin(\"MoveTask\", \"FileMoves\");\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      perfLogger.PerfLogEnd(\"MoveTask\", \"FileMoves\");\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "MoveTask::execute(DriverContext)",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            console.printInfo(\"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449 +\n 450 +\n 451 +\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            String loadTime = \"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\";\n            console.printInfo(loadTime);\n            LOG.info(loadTime);\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }"
        ],
        [
            "Utilities::mvFileToFinalPath(Path,Configuration,boolean,Logger,DynamicPartitionCtx,FileSinkDesc,Reporter)",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405 +\n1406 +\n1407  \n1408  \n1409 +\n1410  \n1411  \n1412 +\n1413  \n1414 +\n1415  \n1416  \n1417  \n1418  \n1419 +\n1420  \n1421 +\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        PerfLogger perfLogger = SessionState.getPerfLogger();\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          perfLogger.PerfLogBegin(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n          perfLogger.PerfLogEnd(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }"
        ]
    ],
    "dcfb1ed2788a4c497bc251ab777c2d04652fa20c": [
        [
            "HiveDruidQueryBasedInputFormat::splitSelectQuery(Configuration,String,String,Path)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }"
        ],
        [
            "HiveDruidQueryBasedInputFormat::createSplitsIntervals(List,int)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 +\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }"
        ]
    ],
    "9de529acf4268900f52b5df70e12c74fb4966bba": [
        [
            "ContainerRunnerImpl::ContainerRunnerImpl(Configuration,int,int,boolean,String,AtomicReference,AtomicReference,long,LlapDaemonExecutorMetrics,AMReporter,ClassLoader,DaemonId,UgiFactory)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133 -\n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  public ContainerRunnerImpl(Configuration conf, int numExecutors, int waitQueueSize,\n      boolean enablePreemption, String[] localDirsBase, AtomicReference<Integer> localShufflePort,\n      AtomicReference<InetSocketAddress> localAddress,\n      long totalMemoryAvailableBytes, LlapDaemonExecutorMetrics metrics,\n      AMReporter amReporter, ClassLoader classLoader, DaemonId daemonId, UgiFactory fsUgiFactory) {\n    super(\"ContainerRunnerImpl\");\n    Preconditions.checkState(numExecutors > 0,\n        \"Invalid number of executors: \" + numExecutors + \". Must be > 0\");\n    this.localAddress = localAddress;\n    this.localShufflePort = localShufflePort;\n    this.amReporter = amReporter;\n    this.signer = UserGroupInformation.isSecurityEnabled()\n        ? new LlapSignerImpl(conf, daemonId.getClusterString()) : null;\n    this.fsUgiFactory = fsUgiFactory;\n\n    this.clusterId = daemonId.getClusterString();\n    this.queryTracker = new QueryTracker(conf, localDirsBase, clusterId);\n    addIfService(queryTracker);\n    String waitQueueSchedulerClassName = HiveConf.getVar(\n        conf, ConfVars.LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME);\n    this.executorService = new TaskExecutorService(numExecutors, waitQueueSize,\n        waitQueueSchedulerClassName, enablePreemption, classLoader, metrics);\n\n    addIfService(executorService);\n\n    // 80% of memory considered for accounted buffers. Rest for objects.\n    // TODO Tune this based on the available size.\n    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes * 0.8 / (float) numExecutors);\n    this.metrics = metrics;\n\n    confParams = new TaskRunnerCallable.ConfParams(\n        conf.getInt(TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS_DEFAULT),\n        conf.getLong(\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS_DEFAULT),\n        conf.getInt(TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT,\n            TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT_DEFAULT)\n    );\n    tezHadoopShim = new HadoopShimsLoader(conf).getHadoopShim();\n\n    LOG.info(\"ContainerRunnerImpl config: \" +\n            \"memoryPerExecutorDerviced=\" + memoryPerExecutor",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "  public ContainerRunnerImpl(Configuration conf, int numExecutors, int waitQueueSize,\n      boolean enablePreemption, String[] localDirsBase, AtomicReference<Integer> localShufflePort,\n      AtomicReference<InetSocketAddress> localAddress,\n      long totalMemoryAvailableBytes, LlapDaemonExecutorMetrics metrics,\n      AMReporter amReporter, ClassLoader classLoader, DaemonId daemonId, UgiFactory fsUgiFactory) {\n    super(\"ContainerRunnerImpl\");\n    Preconditions.checkState(numExecutors > 0,\n        \"Invalid number of executors: \" + numExecutors + \". Must be > 0\");\n    this.localAddress = localAddress;\n    this.localShufflePort = localShufflePort;\n    this.amReporter = amReporter;\n    this.signer = UserGroupInformation.isSecurityEnabled()\n        ? new LlapSignerImpl(conf, daemonId.getClusterString()) : null;\n    this.fsUgiFactory = fsUgiFactory;\n\n    this.clusterId = daemonId.getClusterString();\n    this.queryTracker = new QueryTracker(conf, localDirsBase, clusterId);\n    addIfService(queryTracker);\n    String waitQueueSchedulerClassName = HiveConf.getVar(\n        conf, ConfVars.LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME);\n    this.executorService = new TaskExecutorService(numExecutors, waitQueueSize,\n        waitQueueSchedulerClassName, enablePreemption, classLoader, metrics);\n\n    addIfService(executorService);\n\n    // Distribute the available memory between the tasks.\n    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes / (float) numExecutors);\n    this.metrics = metrics;\n\n    confParams = new TaskRunnerCallable.ConfParams(\n        conf.getInt(TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS_DEFAULT),\n        conf.getLong(\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS_DEFAULT),\n        conf.getInt(TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT,\n            TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT_DEFAULT)\n    );\n    tezHadoopShim = new HadoopShimsLoader(conf).getHadoopShim();\n\n    LOG.info(\"ContainerRunnerImpl config: \" +\n            \"memoryPerExecutorDerviced=\" + memoryPerExecutor"
        ]
    ],
    "36ff484f79b8d2f6771ef42ef6789678b7dcd295": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = JobClient.runJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }"
        ]
    ],
    "53f03358377f3dde21f58e6c841142c6db8a9c32": [
        [
            "TezSessionPoolManager::getSession(HiveConf,boolean)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.set(\"tez.queue.name\", null);\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 +\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.unset(\"tez.queue.name\");\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }"
        ]
    ],
    "14f6f164d1b1a40e1474a17399869dea2bd2348d": [
        [
            "TestHiveMetaStoreChecker::setUp()",
            "  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testErrorForMissingPartitionsSingleThreaded()",
            " 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }",
            " 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testSingleThreadedDeeplyNestedTables()",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360 -\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 +\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testSingleThreadedCheckMetastore()",
            " 335  \n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 336  \n 337  \n 338  \n 339 +\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ]
    ],
    "ce037d14645599a5357fab4ca63167566b7037cb": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 +\n 317 +\n 318 +\n 319  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n    if (!rj.isSuccessful()) {\n      throw new IOException(\"Job failed!\");\n    }\n  }"
        ]
    ],
    "bbf0629a5a2e43531c4fd5e17d727497e89d267d": [
        [
            "CachedStore::CacheUpdateMasterWork::run()",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      try {\n        RawStore rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364 +\n 365 +\n 366 +\n 367 +\n 368 +\n 369 +\n 370  \n 371  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      RawStore rawStore = null;\n      try {\n        rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      } finally {\n        try {\n          if (rawStore != null) {\n            rawStore.shutdown();\n          }\n        } catch (Exception e) {\n          LOG.error(\"Error shutting down RawStore\", e);\n        }\n      }\n    }"
        ],
        [
            "SessionState::unCacheDataNucleusClassLoaders()",
            "1682  \n1683  \n1684  \n1685  \n1686  \n1687 -\n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }",
            "1683  \n1684  \n1685  \n1686  \n1687  \n1688 +\n1689 +\n1690 +\n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())\n            || sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(CachedStore.class.getName()) &&\n            sessionConf.getVar(ConfVars.METASTORE_CACHED_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }"
        ],
        [
            "CachedStore::setConf(Configuration)",
            " 196  \n 197  \n 198  \n 199  \n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    try {\n      rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n          rawStoreClassName)).newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }",
            " 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    if (rawStore == null) {\n      try {\n        rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n            rawStoreClassName)).newInstance();\n      } catch (Exception e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }"
        ]
    ],
    "4a14cfc01b6c06817899290b6d2c5f0849cb44dd": [
        [
            "HCatLoader::getStatistics(String,Job)",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 -\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setmBytes(getSizeInBytes(inputJobInfo) / 1024 / 1024);\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setSizeInBytes(getSizeInBytes(inputJobInfo));\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ],
    "37be57b647932eab98e9ce77c44f10a0c58f1a6a": [
        [
            "DruidQueryBasedInputFormat::createSelectStarQuery(String)",
            " 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList();\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }",
            " 164  \n 165  \n 166  \n 167  \n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList(DruidStorageHandlerUtils.DEFAULT_INTERVAL);\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }"
        ],
        [
            "DruidQueryBasedInputFormat::getInputSplits(Configuration)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null) {\n        throw new IOException(\"Druid data source cannot be empty\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null || dataSource.isEmpty()) {\n        throw new IOException(\"Druid data source cannot be empty or null\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }"
        ]
    ],
    "4bba139d3719c11a015919c1560ac473651f93c5": [
        [
            "TestSQLStdHiveAccessControllerHS2::getSettableParams()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\",\n        \"distcp.options.px\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }"
        ]
    ],
    "2139ef601b91d2982acd25ed1450ab3bda0dbc49": [
        [
            "Utilities::moveSpecifiedFiles(FileSystem,Path,Path,Set)",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      }\n    }\n  }",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184 +\n1185 +\n1186 +\n1187 +\n1188 +\n1189  \n1190  \n1191  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      } else if (file.isDir()) {\n        // Traverse directory contents.\n        // Directory nesting for dst needs to match src.\n        Path nestedDstPath = new Path(dst, file.getPath().getName());\n        Utilities.moveSpecifiedFiles(fs, file.getPath(), nestedDstPath, filesToMove);\n      }\n    }\n  }"
        ]
    ],
    "bc8307eacd291368b9822c1820a047febdb76952": [
        [
            "DruidSerDe::deserialize(Writable)",
            " 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }",
            " 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 +\n 470 +\n 471 +\n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484 +\n 485 +\n 486 +\n 487 +\n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case BYTE:\n          output.add(new ByteWritable(((Number) value).byteValue()));\n          break;\n        case SHORT:\n          output.add(new ShortWritable(((Number) value).shortValue()));\n          break;\n        case INT:\n          output.add(new IntWritable(((Number) value).intValue()));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case DOUBLE:\n          output.add(new DoubleWritable(((Number) value).doubleValue()));\n          break;\n        case DECIMAL:\n          output.add(new HiveDecimalWritable(HiveDecimal.create(((Number) value).doubleValue())));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }"
        ]
    ],
    "2433fed55bf3417c02551ccee6bb76b282905a13": [
        [
            "SemanticAnalyzer::setupStats(TableScanDesc,QBParseInfo,Table,String,RowResolver)",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 -\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getExtTmpPathRelTo(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 +\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getTempDirForPath(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }"
        ]
    ],
    "48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c": [
        [
            "ExplainTask::outputPlan(Object,PrintStream,boolean,boolean,int,String)",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 -\n 801 -\n 802 -\n 803 -\n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n                  OUTPUT_OPERATORS,\n                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n                      .toArray()));\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n              if (outputOperators != null) {\n                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n                    Arrays.toString(outputOperators.toArray()));\n              }\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }"
        ]
    ],
    "28a2efd0c9fde800b9220bddad93c8afafb911bf": [
        [
            "Driver::compileInternal(String,boolean)",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318 -\n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345 -\n1346  \n1347  \n1348  \n1349  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316 +\n1317 +\n1318  \n1319  \n1320 +\n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }"
        ]
    ],
    "302360f51c2f6d93db244b6e67fc05517a654b2b": [
        [
            "ObjectStore::shutdown()",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n    }\n  }",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 +\n 566  \n 567  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n      pm = null;\n    }\n  }"
        ],
        [
            "Hive::close()",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }",
            " 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418 +\n 419 +\n 420 +\n 421  \n 422  \n 423  \n 424  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (syncMetaStoreClient != null) {\n      syncMetaStoreClient.close();\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }"
        ],
        [
            "Hive::loadDynamicPartitions(Path,String,Map,boolean,int,boolean,boolean,long,boolean,AcidUtils)",
            "1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }",
            "1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980 +\n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007 +\n2008 +\n2009 +\n2010 +\n2011 +\n2012 +\n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034 +\n2035 +\n2036 +\n2037 +\n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      final Map<Long, RawStore> rawStoreMap = new HashMap<Long, RawStore>();\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              // Add embedded rawstore, so we can cleanup later to avoid memory leak\n              if (getMSC().isLocalMetaStore()) {\n                if (!rawStoreMap.containsKey(Thread.currentThread().getId())) {\n                  rawStoreMap.put(Thread.currentThread().getId(), HiveMetaStore.HMSHandler.getRawStore());\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n\n      for (RawStore rs : rawStoreMap.values()) {\n        rs.shutdown();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }"
        ]
    ],
    "5db30cd5aeb926d0ebae0a3c2447feb76056abe1": [
        [
            "preAlterTable(Table,EnvironmentContext)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ],
        [
            "DruidStorageHandler::preAlterTable(Table,EnvironmentContext)",
            " 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 689  \n 690  \n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ]
    ],
    "f56abb4054cbc4ba8c8511596117f7823d60dbe6": [
        [
            "DruidQueryBasedInputFormat::distributeSelectQuery(Configuration,String,SelectQuery,Path)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), intervals);\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), URLEncoder.encode(intervals, \"UTF-8\"));\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }"
        ]
    ],
    "780b0127fd22ec95a6b225a493872bcec364ef76": [
        [
            "HiveMaterializedViewsRegistry::init()",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(HiveConf.ConfVars.METASTORE_FILTER_HOOK.varname,\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(MetastoreConf.ConfVars.FILTER_HOOK.getVarname(),\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }"
        ],
        [
            "HiveMaterializedViewsRegistry::parseQuery(HiveConf,String)",
            " 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 -\n 417  \n 418  \n 419  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(e.getMessage());\n      return null;\n    }\n  }",
            " 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420  \n 421  \n 422  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(\"Error parsing original query for materialized view\", e);\n      return null;\n    }\n  }"
        ],
        [
            "HiveMaterializedViewsRegistry::Loader::run()",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    @Override\n    public void run() {\n      try {\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }",
            " 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "    @Override\n    public void run() {\n      try {\n        SessionState.start(db.getConf());\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }"
        ]
    ],
    "539165119e0f09b46b4422ac79677f5e95075c80": [
        [
            "CliConfigs::MiniDruidCliConfig::MiniDruidCliConfig()",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179 +\n 180 +\n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n        excludeQuery(\"druid_timestamptz.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_joins.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_masking.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_test1.q\"); // Disabled in HIVE-20322\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "CliConfigs::MiniLlapLocalCliConfig::MiniLlapLocalCliConfig()",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234 -\n 235 -\n 236 -\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"special_character_in_tabnames_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"sysdb.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"tez_smb_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"union_fast_stats.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"druid_timestamptz.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_joins.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_masking.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_test1.q\"); // Disabled in HIVE-20322\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"special_character_in_tabnames_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"sysdb.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"tez_smb_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"union_fast_stats.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\"); // Disabled in HIVE-19509\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ]
    ],
    "ae82715f1014c4ed514441311b61ed1891e2a12b": [
        [
            "ThriftCLIService::GetQueryId(TGetQueryIdReq)",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    }\n  }",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857 +\n 858 +\n 859 +\n 860  \n 861  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return new TGetQueryIdResp((String)null);\n    }\n  }"
        ],
        [
            "HiveStatement::getQueryId()",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026 -\n1027 -\n1028 -\n1029  \n1030  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return null;\n    }\n  }",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    }\n  }"
        ]
    ],
    "84b5ba7ac9f93c6a496386db91ae4cd5ab7a451d": [
        [
            "GenericUDTFGetSplits::getSplits(JobConf,int,TezWork,Schema,ApplicationId,boolean)",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 -\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = UserGroupInformation.getLoginUser().getShortUserName();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = RegistryUtils.currentUser();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ]
}