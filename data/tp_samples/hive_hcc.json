{
    "dcfb1ed2788a4c497bc251ab777c2d04652fa20c": [
        [
            "HiveDruidQueryBasedInputFormat::splitSelectQuery(Configuration,String,String,Path)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }"
        ],
        [
            "HiveDruidQueryBasedInputFormat::createSplitsIntervals(List,int)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 +\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }"
        ]
    ],
    "bc8307eacd291368b9822c1820a047febdb76952": [
        [
            "DruidSerDe::deserialize(Writable)",
            " 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }",
            " 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 +\n 470 +\n 471 +\n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484 +\n 485 +\n 486 +\n 487 +\n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case BYTE:\n          output.add(new ByteWritable(((Number) value).byteValue()));\n          break;\n        case SHORT:\n          output.add(new ShortWritable(((Number) value).shortValue()));\n          break;\n        case INT:\n          output.add(new IntWritable(((Number) value).intValue()));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case DOUBLE:\n          output.add(new DoubleWritable(((Number) value).doubleValue()));\n          break;\n        case DECIMAL:\n          output.add(new HiveDecimalWritable(HiveDecimal.create(((Number) value).doubleValue())));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }"
        ]
    ],
    "2433fed55bf3417c02551ccee6bb76b282905a13": [
        [
            "SemanticAnalyzer::setupStats(TableScanDesc,QBParseInfo,Table,String,RowResolver)",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 -\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getExtTmpPathRelTo(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 +\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getTempDirForPath(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }"
        ]
    ],
    "53f03358377f3dde21f58e6c841142c6db8a9c32": [
        [
            "TezSessionPoolManager::getSession(HiveConf,boolean)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.set(\"tez.queue.name\", null);\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 +\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.unset(\"tez.queue.name\");\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }"
        ]
    ],
    "14f6f164d1b1a40e1474a17399869dea2bd2348d": [
        [
            "TestHiveMetaStoreChecker::testSingleThreadedCheckMetastore()",
            " 335  \n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 336  \n 337  \n 338  \n 339 +\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testErrorForMissingPartitionsSingleThreaded()",
            " 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }",
            " 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::setUp()",
            "  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testSingleThreadedDeeplyNestedTables()",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360 -\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 +\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ]
    ],
    "a4fd2ea2313efb9850715a03bd826975c60e0ede": [
        [
            "ConvertJoinMapJoin::removeCycleCreatingSemiJoinOps(MapJoinOperator,Operator,ParseContext)",
            " 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    Map<ReduceSinkOperator, TableScanOperator> semiJoinMap =\n            new HashMap<ReduceSinkOperator, TableScanOperator>();\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0) {\n        op = op.getChildOperators().get(0);\n      }\n\n      // If not ReduceSink Op, skip\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      ReduceSinkOperator rs = (ReduceSinkOperator) op;\n      TableScanOperator ts = parseContext.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // skip, no semijoin branch\n        continue;\n      }\n\n      // Found a semijoin branch.\n      for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n        if (!(parent instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                TableScanOperator.class);\n        for (TableScanOperator parentTS : tsOps) {\n          // If the parent is same as the ts, then we have a cycle.\n          if (ts == parentTS) {\n            semiJoinMap.put(rs, ts);\n            break;\n          }\n        }\n      }\n    }\n    if (semiJoinMap.size() > 0) {\n      for (ReduceSinkOperator rs : semiJoinMap.keySet()) {\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(parseContext, rs,\n                semiJoinMap.get(rs));\n      }\n    }\n  }",
            " 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859  \n 860  \n 861  \n 862  \n 863  \n 864  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    Map<ReduceSinkOperator, TableScanOperator> semiJoinMap =\n            new HashMap<ReduceSinkOperator, TableScanOperator>();\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0) {\n        op = op.getChildOperators().get(0);\n      }\n\n      // If not ReduceSink Op, skip\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      ReduceSinkOperator rs = (ReduceSinkOperator) op;\n      TableScanOperator ts = parseContext.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // skip, no semijoin branch\n        continue;\n      }\n\n      // Found a semijoin branch.\n      for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n        if (!(parent instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                TableScanOperator.class);\n        for (TableScanOperator parentTS : tsOps) {\n          // If the parent is same as the ts, then we have a cycle.\n          if (ts == parentTS) {\n            semiJoinMap.put(rs, ts);\n            break;\n          }\n        }\n      }\n    }\n    if (semiJoinMap.size() > 0) {\n      for (ReduceSinkOperator rs : semiJoinMap.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Found semijoin optimization from the big table side of a map join, which will cause a task cycle. \"\n              + \"Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semiJoinMap.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(parseContext, rs,\n                semiJoinMap.get(rs));\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::SemiJoinRemovalIfNoStatsProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      boolean removeSemiJoin = false;\n      for (AggregationDesc agg : aggregationDescs) {\n        if (agg.getGenericUDAFName() != \"bloom_filter\") {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          removeSemiJoin = true;\n          break;\n        }\n      }\n\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          removeSemiJoin = true;\n        }\n      }\n\n      if (removeSemiJoin) {\n        // The stats are not annotated, remove the semijoin operator\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n      }\n      return null;\n    }",
            " 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809 +\n 810 +\n 811 +\n 812 +\n 813 +\n 814 +\n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825 +\n 826 +\n 827 +\n 828 +\n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      boolean removeSemiJoin = false;\n      for (AggregationDesc agg : aggregationDescs) {\n        if (agg.getGenericUDAFName() != \"bloom_filter\") {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          removeSemiJoin = true;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                + \"Removing semijoin \"\n                + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n          break;\n        }\n      }\n\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          removeSemiJoin = true;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n        }\n      }\n\n      if (removeSemiJoin) {\n        // The stats are not annotated, remove the semijoin operator\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n      }\n      return null;\n    }"
        ],
        [
            "TezCompiler::removeSemijoinsParallelToMapJoin(OptimizeTezProcContext)",
            " 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN)) {\n      // Not needed without semi-join reduction\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }",
            " 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961 +\n 962 +\n 963 +\n 964 +\n 965  \n 966  \n 967  \n 968  \n 969  \n 970  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN)) {\n      // Not needed without semi-join reduction\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Semijoin optimization with parallel edge to map join. Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semijoins.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeSemiJoinCyclesDueToMapsideJoins(OptimizeTezProcContext)",
            " 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n        if (ts == null) {\n          continue;\n        }\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }",
            " 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766 +\n 767 +\n 768 +\n 769 +\n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n        if (ts == null) {\n          continue;\n        }\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin cycle due to mapjoin. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeSemijoinOptimizationFromSMBJoins(OptimizeTezProcContext)",
            " 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  ",
            "  private static void removeSemijoinOptimizationFromSMBJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", TableScanOperator.getOperatorName() + \"%\" +\n                    \".*\" + TezDummyStoreOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SMBJoinOpProc());\n\n    SMBJoinOpProcContext ctx = new SMBJoinOpProcContext();\n    // The dispatcher finds SMB and if there is semijoin optimization before it, removes it.\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // Iterate over the map and remove semijoin optimizations if needed.\n    for (CommonMergeJoinOperator joinOp : ctx.JoinOpToTsOpMap.keySet()) {\n      List<TableScanOperator> tsOps = new ArrayList<TableScanOperator>();\n      // Get one top level TS Op directly from the stack\n      tsOps.add(ctx.JoinOpToTsOpMap.get(joinOp));\n\n      // Get the other one by examining Join Op\n      List<Operator<?>> parents = joinOp.getParentOperators();\n      for (Operator<?> parent : parents) {\n        if (parent instanceof TezDummyStoreOperator) {\n          // already accounted for\n          continue;\n        }\n\n        assert parent instanceof SelectOperator;\n        while(parent != null) {\n          if (parent instanceof TableScanOperator) {\n            tsOps.add((TableScanOperator) parent);\n            break;\n          }\n          parent = parent.getParentOperators().get(0);\n        }\n      }\n\n      // Now the relevant TableScanOperators are known, find if there exists\n      // a semijoin filter on any of them, if so, remove it.\n      ParseContext pctx = procCtx.parseContext;\n      for (TableScanOperator ts : tsOps) {\n        for (ReduceSinkOperator rs : pctx.getRsOpToTsOpMap().keySet()) {\n          if (ts == pctx.getRsOpToTsOpMap().get(rs)) {\n            // match!\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pctx, rs, ts);\n          }\n        }\n      }\n    }\n  }",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663 +\n 664 +\n 665 +\n 666 +\n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "  private static void removeSemijoinOptimizationFromSMBJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", TableScanOperator.getOperatorName() + \"%\" +\n                    \".*\" + TezDummyStoreOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SMBJoinOpProc());\n\n    SMBJoinOpProcContext ctx = new SMBJoinOpProcContext();\n    // The dispatcher finds SMB and if there is semijoin optimization before it, removes it.\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // Iterate over the map and remove semijoin optimizations if needed.\n    for (CommonMergeJoinOperator joinOp : ctx.JoinOpToTsOpMap.keySet()) {\n      List<TableScanOperator> tsOps = new ArrayList<TableScanOperator>();\n      // Get one top level TS Op directly from the stack\n      tsOps.add(ctx.JoinOpToTsOpMap.get(joinOp));\n\n      // Get the other one by examining Join Op\n      List<Operator<?>> parents = joinOp.getParentOperators();\n      for (Operator<?> parent : parents) {\n        if (parent instanceof TezDummyStoreOperator) {\n          // already accounted for\n          continue;\n        }\n\n        assert parent instanceof SelectOperator;\n        while(parent != null) {\n          if (parent instanceof TableScanOperator) {\n            tsOps.add((TableScanOperator) parent);\n            break;\n          }\n          parent = parent.getParentOperators().get(0);\n        }\n      }\n\n      // Now the relevant TableScanOperators are known, find if there exists\n      // a semijoin filter on any of them, if so, remove it.\n      ParseContext pctx = procCtx.parseContext;\n      for (TableScanOperator ts : tsOps) {\n        for (ReduceSinkOperator rs : pctx.getRsOpToTsOpMap().keySet()) {\n          if (ts == pctx.getRsOpToTsOpMap().get(rs)) {\n            // match!\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin optimization found going to SMB join. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pctx, rs, ts);\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeCycleOperator(Set,OptimizeTezProcContext)",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n        TableScanOperator ts = context.parseContext.getRsOpToTsOpMap().get(o);\n        if (ts == null) {\n          continue;\n        }\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                victimTS.getStatistics().getDataSize()) {\n            victimRS = (ReduceSinkOperator) o;\n            victimTS = ts;\n          }\n        }\n      }\n\n    // Always set the min/max optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    }\n    return;\n  }",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239 +\n 240 +\n 241 +\n 242  \n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248  \n 249  \n 250  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n        TableScanOperator ts = context.parseContext.getRsOpToTsOpMap().get(o);\n        if (ts == null) {\n          continue;\n        }\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                victimTS.getStatistics().getDataSize()) {\n            victimRS = (ReduceSinkOperator) o;\n            victimTS = ts;\n          }\n        }\n      }\n\n    // Always set the min/max optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cycle found. Removing semijoin \"\n            + OperatorUtils.getOpNamePretty(victimRS) + \" - \" + OperatorUtils.getOpNamePretty(victimTS));\n      }\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    } else {\n      // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n          + \". Needed to break cyclic dependency\");\n    }\n    return;\n  }"
        ]
    ],
    "3a5edc97e562661b7e54b970c2fc123a26e79708": [
        [
            "DruidGroupByQueryRecordReader::next(NullWritable,DruidWritable)",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  @Override\n  public boolean next(NullWritable key, DruidWritable value) {\n    if (nextKeyValue()) {\n      // Update value\n      value.getValue().clear();\n      // 1) The timestamp column\n      value.getValue().put(DruidTable.DEFAULT_TIMESTAMP_COLUMN, current.getTimestamp().getMillis());\n      // 2) The dimension columns\n      for (int i = 0; i < query.getDimensions().size(); i++) {\n        DimensionSpec ds = query.getDimensions().get(i);\n        List<String> dims = current.getDimension(ds.getDimension());\n        if (dims.size() == 0) {\n          // NULL value for dimension\n          value.getValue().put(ds.getOutputName(), null);\n        } else {\n          int pos = dims.size() - indexes[i] - 1;\n          value.getValue().put(ds.getOutputName(), dims.get(pos));\n        }\n      }\n      int counter = 0;\n      // 3) The aggregation columns\n      for (AggregatorFactory af : query.getAggregatorSpecs()) {\n        switch (extractors[counter++]) {\n          case FLOAT:\n            value.getValue().put(af.getName(), current.getFloatMetric(af.getName()));\n            break;\n          case LONG:\n            value.getValue().put(af.getName(), current.getLongMetric(af.getName()));\n            break;\n        }\n      }\n      // 4) The post-aggregation columns\n      for (PostAggregator pa : query.getPostAggregatorSpecs()) {\n        assert extractors[counter++] == Extract.FLOAT;\n        value.getValue().put(pa.getName(), current.getFloatMetric(pa.getName()));\n      }\n      return true;\n    }\n    return false;\n  }",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  @Override\n  public boolean next(NullWritable key, DruidWritable value) {\n    if (nextKeyValue()) {\n      // Update value\n      value.getValue().clear();\n      // 1) The timestamp column\n      value.getValue().put(DruidTable.DEFAULT_TIMESTAMP_COLUMN, current.getTimestamp().getMillis());\n      // 2) The dimension columns\n      for (int i = 0; i < query.getDimensions().size(); i++) {\n        DimensionSpec ds = query.getDimensions().get(i);\n        List<String> dims = current.getDimension(ds.getOutputName());\n        if (dims.size() == 0) {\n          // NULL value for dimension\n          value.getValue().put(ds.getOutputName(), null);\n        } else {\n          int pos = dims.size() - indexes[i] - 1;\n          value.getValue().put(ds.getOutputName(), dims.get(pos));\n        }\n      }\n      int counter = 0;\n      // 3) The aggregation columns\n      for (AggregatorFactory af : query.getAggregatorSpecs()) {\n        switch (extractors[counter++]) {\n          case FLOAT:\n            value.getValue().put(af.getName(), current.getFloatMetric(af.getName()));\n            break;\n          case LONG:\n            value.getValue().put(af.getName(), current.getLongMetric(af.getName()));\n            break;\n        }\n      }\n      // 4) The post-aggregation columns\n      for (PostAggregator pa : query.getPostAggregatorSpecs()) {\n        assert extractors[counter++] == Extract.FLOAT;\n        value.getValue().put(pa.getName(), current.getFloatMetric(pa.getName()));\n      }\n      return true;\n    }\n    return false;\n  }"
        ],
        [
            "DruidGroupByQueryRecordReader::getCurrentValue()",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "  @Override\n  public DruidWritable getCurrentValue() throws IOException, InterruptedException {\n    // Create new value\n    DruidWritable value = new DruidWritable();\n    // 1) The timestamp column\n    value.getValue().put(DruidTable.DEFAULT_TIMESTAMP_COLUMN, current.getTimestamp().getMillis());\n    // 2) The dimension columns\n    for (int i = 0; i < query.getDimensions().size(); i++) {\n      DimensionSpec ds = query.getDimensions().get(i);\n      List<String> dims = current.getDimension(ds.getDimension());\n      if (dims.size() == 0) {\n        // NULL value for dimension\n        value.getValue().put(ds.getOutputName(), null);\n      } else {\n        int pos = dims.size() - indexes[i] - 1;\n        value.getValue().put(ds.getOutputName(), dims.get(pos));\n      }\n    }\n    int counter = 0;\n    // 3) The aggregation columns\n    for (AggregatorFactory af : query.getAggregatorSpecs()) {\n      switch (extractors[counter++]) {\n        case FLOAT:\n          value.getValue().put(af.getName(), current.getFloatMetric(af.getName()));\n          break;\n        case LONG:\n          value.getValue().put(af.getName(), current.getLongMetric(af.getName()));\n          break;\n      }\n    }\n    // 4) The post-aggregation columns\n    for (PostAggregator pa : query.getPostAggregatorSpecs()) {\n      assert extractors[counter++] == Extract.FLOAT;\n      value.getValue().put(pa.getName(), current.getFloatMetric(pa.getName()));\n    }\n    return value;\n  }",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "  @Override\n  public DruidWritable getCurrentValue() throws IOException, InterruptedException {\n    // Create new value\n    DruidWritable value = new DruidWritable();\n    // 1) The timestamp column\n    value.getValue().put(DruidTable.DEFAULT_TIMESTAMP_COLUMN, current.getTimestamp().getMillis());\n    // 2) The dimension columns\n    for (int i = 0; i < query.getDimensions().size(); i++) {\n      DimensionSpec ds = query.getDimensions().get(i);\n      List<String> dims = current.getDimension(ds.getOutputName());\n      if (dims.size() == 0) {\n        // NULL value for dimension\n        value.getValue().put(ds.getOutputName(), null);\n      } else {\n        int pos = dims.size() - indexes[i] - 1;\n        value.getValue().put(ds.getOutputName(), dims.get(pos));\n      }\n    }\n    int counter = 0;\n    // 3) The aggregation columns\n    for (AggregatorFactory af : query.getAggregatorSpecs()) {\n      switch (extractors[counter++]) {\n        case FLOAT:\n          value.getValue().put(af.getName(), current.getFloatMetric(af.getName()));\n          break;\n        case LONG:\n          value.getValue().put(af.getName(), current.getLongMetric(af.getName()));\n          break;\n      }\n    }\n    // 4) The post-aggregation columns\n    for (PostAggregator pa : query.getPostAggregatorSpecs()) {\n      assert extractors[counter++] == Extract.FLOAT;\n      value.getValue().put(pa.getName(), current.getFloatMetric(pa.getName()));\n    }\n    return value;\n  }"
        ],
        [
            "DruidGroupByQueryRecordReader::nextKeyValue()",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "  @Override\n  public boolean nextKeyValue() {\n    // Refresh indexes\n    for (int i = indexes.length - 1; i >= 0; i--) {\n      if (indexes[i] > 0) {\n        indexes[i]--;\n        for (int j = i + 1; j < indexes.length; j++) {\n          indexes[j] = current.getDimension(\n                  query.getDimensions().get(j).getDimension()).size() - 1;\n        }\n        return true;\n      }\n    }\n    // Results\n    if (results.hasNext()) {\n      current = results.next();\n      indexes = new int[query.getDimensions().size()];\n      for (int i = 0; i < query.getDimensions().size(); i++) {\n        DimensionSpec ds = query.getDimensions().get(i);\n        indexes[i] = current.getDimension(ds.getDimension()).size() - 1;\n      }\n      return true;\n    }\n    return false;\n  }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "  @Override\n  public boolean nextKeyValue() {\n    // Refresh indexes\n    for (int i = indexes.length - 1; i >= 0; i--) {\n      if (indexes[i] > 0) {\n        indexes[i]--;\n        for (int j = i + 1; j < indexes.length; j++) {\n          indexes[j] = current.getDimension(\n                  query.getDimensions().get(j).getOutputName()).size() - 1;\n        }\n        return true;\n      }\n    }\n    // Results\n    if (results.hasNext()) {\n      current = results.next();\n      indexes = new int[query.getDimensions().size()];\n      for (int i = 0; i < query.getDimensions().size(); i++) {\n        DimensionSpec ds = query.getDimensions().get(i);\n        indexes[i] = current.getDimension(ds.getOutputName()).size() - 1;\n      }\n      return true;\n    }\n    return false;\n  }"
        ]
    ],
    "f4d017b801ea256bf076160a31dba88e61c80422": [
        [
            "EncodedReaderImpl::readEncodedColumns(int,StripeInformation,OrcProto,List,List,boolean,boolean,Consumer)",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 -\n 420 -\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 -\n 434 -\n 435 -\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[][] colRgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    boolean[] includedRgs = null; // Will always be the same for all cols at the moment.\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      includedRgs = colRgs[ctx.includedIx];\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (includedRgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (includedRgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead.get());\n    if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n      LOG.info(\"Resulting disk ranges to read (file \" + fileKey + \"): \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n    BooleanRef isAllInCache = new BooleanRef();\n    if (hasFileId) {\n      cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);\n      if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after cache (found everything \" + isAllInCache.value + \"; file \"\n            + fileKey + \", base offset \" + stripeOffset  + \"): \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n    }\n\n    // TODO: the memory release could be optimized - we could release original buffers after we\n    //       are fully done with each original buffer from disk. For now release all at the end;\n    //       it doesn't increase the total amount of memory we hold, just the duration a bit.\n    //       This is much simpler - we can just remember original ranges after reading them, and\n    //       release them at the end. In a few cases where it's easy to determine that a buffer\n    //       can be freed in advance, we remove it from the map.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = null;\n    if (!isAllInCache.value) {\n      if (!isDataReaderOpen) {\n        this.dataReader.open();\n        isDataReaderOpen = true;\n      }\n      dataReader.readFileData(toRead.next, stripeOffset, cacheWrapper.getAllocator().isDirectAlloc());\n      toRelease = new IdentityHashMap<>();\n      DiskRangeList drl = toRead.next;\n      while (drl != null) {\n        if (drl instanceof BufferChunk) {\n          toRelease.put(drl.getData(), true);\n        }\n        drl = drl.next;\n      }\n    }\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    if (codec == null) {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease = null;\n      }\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n    }\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n    for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n      boolean isLastRg = rgIx == rgCount - 1;\n      // Create the batch we will use to return data for this RG.\n      OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n      ecb.init(fileKey, stripeIx, rgIx, included.length);\n      boolean isRGSelected = true;\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        if (isTracingEnabled) {\n          LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n        }\n        // TODO: simplify this now that high-level cache has been removed. Same RGs for all cols.\n        if (colRgs[ctx.includedIx] != null && !colRgs[ctx.includedIx][rgIx]) {\n          // RG x col filtered.\n          isRGSelected = false;\n          if (isTracingEnabled) {\n            LOG.trace(\"colIxMod: {} rgIx: {} colRgs[{}]: {} colRgs[{}][{}]: {}\", ctx.includedIx, rgIx, ctx.includedIx,\n              Arrays.toString(colRgs[ctx.includedIx]), ctx.includedIx, rgIx, colRgs[ctx.includedIx][rgIx]);\n          }\n           continue;\n        }\n        OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n            nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n        ecb.initOrcColumn(ctx.colIx);\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          ColumnStreamData cb = null;\n          try {\n            if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n              // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n              if (isTracingEnabled) {\n                LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n              }\n              if (sctx.stripeLevelStream == null) {\n                sctx.stripeLevelStream = POOLS.csdPool.take();\n                // We will be using this for each RG while also sending RGs to processing.\n                // To avoid buffers being unlocked, run refcount one ahead; we will not increase\n                // it when building the last RG, so each RG processing will decref once, and the\n                // last one will unlock the buffers.\n                sctx.stripeLevelStream.incRef();\n                // For stripe-level streams we don't need the extra refcount on the block.\n                // See class comment about refcounts.\n                long unlockUntilCOffset = sctx.offset + sctx.length;\n                DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                    sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                    unlockUntilCOffset, sctx.offset, toRelease);\n                if (lastCached != null) {\n                  iter = lastCached;\n                }\n              }\n              if (!isLastRg) {\n                sctx.stripeLevelStream.incRef();\n              }\n              cb = sctx.stripeLevelStream;\n            } else {\n              // This stream can be separated by RG using index. Let's do that.\n              // Offset to where this RG begins.\n              long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n              // Offset relative to the beginning of the stream of where this RG ends.\n              long nextCOffsetRel = isLastRg ? sctx.length\n                  : nextIndex.getPositions(sctx.streamIndexOffset);\n              // Offset before which this RG is guaranteed to end. Can only be estimated.\n              // We estimate the same way for compressed and uncompressed for now.\n              long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                  isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n              // As we read, we can unlock initial refcounts for the buffers that end before\n              // the data that we need for this RG.\n              long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n              cb = createRgColumnStreamData(\n                  rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);\n              boolean isStartOfStream = sctx.bufferIter == null;\n              DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                  (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                  unlockUntilCOffset, sctx.offset, toRelease);\n              if (lastCached != null) {\n                sctx.bufferIter = iter = lastCached;\n              }\n            }\n            ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n          } catch (Exception ex) {\n            DiskRangeList drl = toRead == null ? null : toRead.next;\n            LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n            throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n          }\n        }\n      }\n      if (isRGSelected) {\n        consumer.consumeData(ecb);\n      }\n    }\n\n    if (isTracingEnabled) {\n      LOG.trace(\"Disk ranges after preparing all the data \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n\n    // Release the unreleased buffers. See class comment about refcounts.\n    releaseInitialRefcounts(toRead.next);\n    // Release buffers as we are done with all the streams... also see toRelease comment.\n    if (toRelease != null) {\n      releaseBuffers(toRelease.keySet(), true);\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420 +\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 +\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482 +\n 483 +\n 484 +\n 485 +\n 486 +\n 487 +\n 488 +\n 489 +\n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[][] colRgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    boolean[] includedRgs = null; // Will always be the same for all cols at the moment.\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      includedRgs = colRgs[ctx.includedIx];\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (includedRgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (includedRgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead.get());\n    if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n      LOG.info(\"Resulting disk ranges to read (file \" + fileKey + \"): \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n    BooleanRef isAllInCache = new BooleanRef();\n    if (hasFileId) {\n      cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);\n      if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after cache (found everything \" + isAllInCache.value + \"; file \"\n            + fileKey + \", base offset \" + stripeOffset  + \"): \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n    }\n\n    // TODO: the memory release could be optimized - we could release original buffers after we\n    //       are fully done with each original buffer from disk. For now release all at the end;\n    //       it doesn't increase the total amount of memory we hold, just the duration a bit.\n    //       This is much simpler - we can just remember original ranges after reading them, and\n    //       release them at the end. In a few cases where it's easy to determine that a buffer\n    //       can be freed in advance, we remove it from the map.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = null;\n    if (!isAllInCache.value) {\n      if (!isDataReaderOpen) {\n        this.dataReader.open();\n        isDataReaderOpen = true;\n      }\n      dataReader.readFileData(toRead.next, stripeOffset, cacheWrapper.getAllocator().isDirectAlloc());\n      toRelease = new IdentityHashMap<>();\n      DiskRangeList drl = toRead.next;\n      while (drl != null) {\n        if (drl instanceof BufferChunk) {\n          toRelease.put(drl.getData(), true);\n        }\n        drl = drl.next;\n      }\n    }\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    if (codec == null) {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease = null;\n      }\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n    }\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n    for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n      boolean isLastRg = rgIx == rgCount - 1;\n      // Create the batch we will use to return data for this RG.\n      OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n      ecb.init(fileKey, stripeIx, rgIx, included.length);\n      boolean isRGSelected = true;\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        if (isTracingEnabled) {\n          LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n        }\n        // TODO: simplify this now that high-level cache has been removed. Same RGs for all cols.\n        if (colRgs[ctx.includedIx] != null && !colRgs[ctx.includedIx][rgIx]) {\n          // RG x col filtered.\n          isRGSelected = false;\n          if (isTracingEnabled) {\n            LOG.trace(\"colIxMod: {} rgIx: {} colRgs[{}]: {} colRgs[{}][{}]: {}\", ctx.includedIx, rgIx, ctx.includedIx,\n              Arrays.toString(colRgs[ctx.includedIx]), ctx.includedIx, rgIx, colRgs[ctx.includedIx][rgIx]);\n          }\n           continue;\n        }\n        OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n            nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n        ecb.initOrcColumn(ctx.colIx);\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          ColumnStreamData cb = null;\n          try {\n            if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n              // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n              if (isTracingEnabled) {\n                LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n              }\n              if (sctx.stripeLevelStream == null) {\n                sctx.stripeLevelStream = POOLS.csdPool.take();\n                // We will be using this for each RG while also sending RGs to processing.\n                // To avoid buffers being unlocked, run refcount one ahead; so each RG \n                 // processing will decref once, and the\n                // last one will unlock the buffers.\n                sctx.stripeLevelStream.incRef();\n                // For stripe-level streams we don't need the extra refcount on the block.\n                // See class comment about refcounts.\n                long unlockUntilCOffset = sctx.offset + sctx.length;\n                DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                    sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                    unlockUntilCOffset, sctx.offset, toRelease);\n                if (lastCached != null) {\n                  iter = lastCached;\n                }\n              }\n              sctx.stripeLevelStream.incRef();\n              cb = sctx.stripeLevelStream;\n            } else {\n              // This stream can be separated by RG using index. Let's do that.\n              // Offset to where this RG begins.\n              long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n              // Offset relative to the beginning of the stream of where this RG ends.\n              long nextCOffsetRel = isLastRg ? sctx.length\n                  : nextIndex.getPositions(sctx.streamIndexOffset);\n              // Offset before which this RG is guaranteed to end. Can only be estimated.\n              // We estimate the same way for compressed and uncompressed for now.\n              long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                  isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n              // As we read, we can unlock initial refcounts for the buffers that end before\n              // the data that we need for this RG.\n              long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n              cb = createRgColumnStreamData(\n                  rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);\n              boolean isStartOfStream = sctx.bufferIter == null;\n              DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                  (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                  unlockUntilCOffset, sctx.offset, toRelease);\n              if (lastCached != null) {\n                sctx.bufferIter = iter = lastCached;\n              }\n            }\n            ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n          } catch (Exception ex) {\n            DiskRangeList drl = toRead == null ? null : toRead.next;\n            LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n            throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n          }\n        }\n      }\n      if (isRGSelected) {\n        consumer.consumeData(ecb);\n      }\n    }\n\n    if (isTracingEnabled) {\n      LOG.trace(\"Disk ranges after preparing all the data \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n\n    // Release the unreleased buffers. See class comment about refcounts.\n    for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n      ColumnReadContext ctx = colCtxs[colIx];\n      if (ctx == null) continue; // This column is not included.\n      for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n        StreamContext sctx = ctx.streams[streamIx];\n        if (sctx == null || sctx.stripeLevelStream == null) continue;\n        if (0 != sctx.stripeLevelStream.decRef()) continue;\n        for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Unlocking {} at the end of processing\", buf);\n          }\n          cacheWrapper.releaseBuffer(buf);\n        }\n      }\n    }\n\n    releaseInitialRefcounts(toRead.next);\n    // Release buffers as we are done with all the streams... also see toRelease comment.\n    if (toRelease != null) {\n      releaseBuffers(toRelease.keySet(), true);\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }"
        ]
    ],
    "bbf0629a5a2e43531c4fd5e17d727497e89d267d": [
        [
            "CachedStore::CacheUpdateMasterWork::run()",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      try {\n        RawStore rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364 +\n 365 +\n 366 +\n 367 +\n 368 +\n 369 +\n 370  \n 371  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      RawStore rawStore = null;\n      try {\n        rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      } finally {\n        try {\n          if (rawStore != null) {\n            rawStore.shutdown();\n          }\n        } catch (Exception e) {\n          LOG.error(\"Error shutting down RawStore\", e);\n        }\n      }\n    }"
        ],
        [
            "SessionState::unCacheDataNucleusClassLoaders()",
            "1682  \n1683  \n1684  \n1685  \n1686  \n1687 -\n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }",
            "1683  \n1684  \n1685  \n1686  \n1687  \n1688 +\n1689 +\n1690 +\n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())\n            || sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(CachedStore.class.getName()) &&\n            sessionConf.getVar(ConfVars.METASTORE_CACHED_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }"
        ],
        [
            "CachedStore::setConf(Configuration)",
            " 196  \n 197  \n 198  \n 199  \n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    try {\n      rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n          rawStoreClassName)).newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }",
            " 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    if (rawStore == null) {\n      try {\n        rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n            rawStoreClassName)).newInstance();\n      } catch (Exception e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }"
        ]
    ],
    "4a14cfc01b6c06817899290b6d2c5f0849cb44dd": [
        [
            "HCatLoader::getStatistics(String,Job)",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 -\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setmBytes(getSizeInBytes(inputJobInfo) / 1024 / 1024);\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setSizeInBytes(getSizeInBytes(inputJobInfo));\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ],
    "37be57b647932eab98e9ce77c44f10a0c58f1a6a": [
        [
            "DruidQueryBasedInputFormat::getInputSplits(Configuration)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null) {\n        throw new IOException(\"Druid data source cannot be empty\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null || dataSource.isEmpty()) {\n        throw new IOException(\"Druid data source cannot be empty or null\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }"
        ],
        [
            "DruidQueryBasedInputFormat::createSelectStarQuery(String)",
            " 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList();\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }",
            " 164  \n 165  \n 166  \n 167  \n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList(DruidStorageHandlerUtils.DEFAULT_INTERVAL);\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }"
        ]
    ],
    "4bba139d3719c11a015919c1560ac473651f93c5": [
        [
            "TestSQLStdHiveAccessControllerHS2::getSettableParams()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\",\n        \"distcp.options.px\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }"
        ]
    ],
    "eca6b89458be22abae2529ae549894cb1ba75a6d": [
        [
            "LlapServiceDriver::run(String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.NDC.class, };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 +\n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.NDC.class,\n              io.netty.util.NetUtil.class };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }"
        ]
    ],
    "04b303b6957a6b6f580ee10a4215b21071d87999": [
        [
            "MoveTask::execute(DriverContext)",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            console.printInfo(\"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449 +\n 450 +\n 451 +\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            String loadTime = \"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\";\n            console.printInfo(loadTime);\n            LOG.info(loadTime);\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }"
        ],
        [
            "Hive::loadPartition(Path,Table,Map,boolean,boolean,boolean,boolean,boolean,boolean)",
            "1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }",
            "1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571 +\n1572 +\n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584 +\n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      PerfLogger perfLogger = SessionState.getPerfLogger();\n      perfLogger.PerfLogBegin(\"MoveTask\", \"FileMoves\");\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      perfLogger.PerfLogEnd(\"MoveTask\", \"FileMoves\");\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "Utilities::mvFileToFinalPath(Path,Configuration,boolean,Logger,DynamicPartitionCtx,FileSinkDesc,Reporter)",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405 +\n1406 +\n1407  \n1408  \n1409 +\n1410  \n1411  \n1412 +\n1413  \n1414 +\n1415  \n1416  \n1417  \n1418  \n1419 +\n1420  \n1421 +\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        PerfLogger perfLogger = SessionState.getPerfLogger();\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          perfLogger.PerfLogBegin(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n          perfLogger.PerfLogEnd(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }"
        ]
    ],
    "36ff484f79b8d2f6771ef42ef6789678b7dcd295": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = JobClient.runJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }"
        ]
    ],
    "48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c": [
        [
            "ExplainTask::outputPlan(Object,PrintStream,boolean,boolean,int,String)",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 -\n 801 -\n 802 -\n 803 -\n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n                  OUTPUT_OPERATORS,\n                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n                      .toArray()));\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n              if (outputOperators != null) {\n                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n                    Arrays.toString(outputOperators.toArray()));\n              }\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }"
        ]
    ],
    "ce037d14645599a5357fab4ca63167566b7037cb": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 +\n 317 +\n 318 +\n 319  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n    if (!rj.isSuccessful()) {\n      throw new IOException(\"Job failed!\");\n    }\n  }"
        ]
    ],
    "2139ef601b91d2982acd25ed1450ab3bda0dbc49": [
        [
            "Utilities::moveSpecifiedFiles(FileSystem,Path,Path,Set)",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      }\n    }\n  }",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184 +\n1185 +\n1186 +\n1187 +\n1188 +\n1189  \n1190  \n1191  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      } else if (file.isDir()) {\n        // Traverse directory contents.\n        // Directory nesting for dst needs to match src.\n        Path nestedDstPath = new Path(dst, file.getPath().getName());\n        Utilities.moveSpecifiedFiles(fs, file.getPath(), nestedDstPath, filesToMove);\n      }\n    }\n  }"
        ]
    ],
    "302360f51c2f6d93db244b6e67fc05517a654b2b": [
        [
            "Hive::close()",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }",
            " 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418 +\n 419 +\n 420 +\n 421  \n 422  \n 423  \n 424  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (syncMetaStoreClient != null) {\n      syncMetaStoreClient.close();\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }"
        ],
        [
            "Hive::loadDynamicPartitions(Path,String,Map,boolean,int,boolean,boolean,long,boolean,AcidUtils)",
            "1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }",
            "1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980 +\n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007 +\n2008 +\n2009 +\n2010 +\n2011 +\n2012 +\n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034 +\n2035 +\n2036 +\n2037 +\n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      final Map<Long, RawStore> rawStoreMap = new HashMap<Long, RawStore>();\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              // Add embedded rawstore, so we can cleanup later to avoid memory leak\n              if (getMSC().isLocalMetaStore()) {\n                if (!rawStoreMap.containsKey(Thread.currentThread().getId())) {\n                  rawStoreMap.put(Thread.currentThread().getId(), HiveMetaStore.HMSHandler.getRawStore());\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n\n      for (RawStore rs : rawStoreMap.values()) {\n        rs.shutdown();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }"
        ],
        [
            "ObjectStore::shutdown()",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n    }\n  }",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 +\n 566  \n 567  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n      pm = null;\n    }\n  }"
        ]
    ],
    "28a2efd0c9fde800b9220bddad93c8afafb911bf": [
        [
            "Driver::compileInternal(String,boolean)",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318 -\n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345 -\n1346  \n1347  \n1348  \n1349  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316 +\n1317 +\n1318  \n1319  \n1320 +\n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }"
        ]
    ],
    "5db30cd5aeb926d0ebae0a3c2447feb76056abe1": [
        [
            "preAlterTable(Table,EnvironmentContext)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ],
        [
            "DruidStorageHandler::preAlterTable(Table,EnvironmentContext)",
            " 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 689  \n 690  \n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ]
    ],
    "5616cbe40587c7cd218b15645c8563e6b4c6662e": [
        [
            "LlapServiceDriver::run(String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 -\n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.config.Log4j1ConfigurationFactory.class,\n              io.netty.util.NetUtil.class, // netty4\n              org.jboss.netty.util.NetUtil.class //netty3\n              };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 +\n 404 +\n 405 +\n 406 +\n 407 +\n 408 +\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.config.Log4j1ConfigurationFactory.class,\n              io.netty.util.NetUtil.class, // netty4\n              org.jboss.netty.util.NetUtil.class, //netty3\n              org.apache.arrow.vector.types.pojo.ArrowType.class, //arrow-vector\n              org.apache.arrow.memory.BaseAllocator.class, //arrow-memory\n              org.apache.arrow.flatbuf.Schema.class, //arrow-format\n              com.google.flatbuffers.Table.class, //flatbuffers\n              com.carrotsearch.hppc.ByteArrayDeque.class //hppc\n              };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }"
        ]
    ],
    "85475ae894a508393afcdf183c78447bc4c60369": [
        [
            "TransactionalValidationListener::validateTableStructure(IHMSHandler,Table)",
            " 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427 -\n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  ",
            "  /**\n   * It's assumed everywhere that original data files are named according to\n   * {@link #ORIGINAL_PATTERN} or{@link #ORIGINAL_PATTERN_COPY}\n   * This checks that when transaction=true is set and throws if it finds any files that don't\n   * follow convention.\n   */\n  private void validateTableStructure(IHMSHandler hmsHandler, Table table)\n    throws MetaException {\n    Path tablePath;\n    try {\n      Warehouse wh = hmsHandler.getWh();\n      if (table.getSd().getLocation() == null || table.getSd().getLocation().isEmpty()) {\n        tablePath = wh.getDefaultTablePath(hmsHandler.getMS().getDatabase(\n            MetaStoreUtils.getDefaultCatalog(getConf()), table.getDbName()), table.getTableName());\n      } else {\n        tablePath = wh.getDnsPath(new Path(table.getSd().getLocation()));\n      }\n      FileSystem fs = wh.getFs(tablePath);\n      //FileSystem fs = FileSystem.get(getConf());\n      RemoteIterator<LocatedFileStatus> iterator = fs.listFiles(tablePath, true);\n      while (iterator.hasNext()) {\n        LocatedFileStatus fileStatus = iterator.next();\n        if (!fileStatus.isFile()) {\n          continue;\n        }\n        boolean validFile =\n          (ORIGINAL_PATTERN.matcher(fileStatus.getPath().getName()).matches() ||\n            ORIGINAL_PATTERN_COPY.matcher(fileStatus.getPath().getName()).matches()\n          );\n        if (!validFile) {\n          throw new IllegalStateException(\"Unexpected data file name format.  Cannot convert \" +\n            Warehouse.getQualifiedName(table) + \" to transactional table.  File: \"\n            + fileStatus.getPath());\n        }\n      }\n    } catch (IOException|NoSuchObjectException e) {\n      String msg = \"Unable to list files for \" + Warehouse.getQualifiedName(table);\n      LOG.error(msg, e);\n      MetaException e1 = new MetaException(msg);\n      e1.initCause(e);\n      throw e1;\n    }\n  }",
            " 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 +\n 427 +\n 428  \n 429 +\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  ",
            "  /**\n   * It's assumed everywhere that original data files are named according to\n   * {@link #ORIGINAL_PATTERN} or{@link #ORIGINAL_PATTERN_COPY}\n   * This checks that when transaction=true is set and throws if it finds any files that don't\n   * follow convention.\n   */\n  private void validateTableStructure(IHMSHandler hmsHandler, Table table)\n    throws MetaException {\n    Path tablePath;\n    try {\n      Warehouse wh = hmsHandler.getWh();\n      if (table.getSd().getLocation() == null || table.getSd().getLocation().isEmpty()) {\n        String catName = table.isSetCatName() ? table.getCatName() :\n            MetaStoreUtils.getDefaultCatalog(getConf());\n        tablePath = wh.getDefaultTablePath(hmsHandler.getMS().getDatabase(\n            catName, table.getDbName()), table.getTableName());\n      } else {\n        tablePath = wh.getDnsPath(new Path(table.getSd().getLocation()));\n      }\n      FileSystem fs = wh.getFs(tablePath);\n      //FileSystem fs = FileSystem.get(getConf());\n      RemoteIterator<LocatedFileStatus> iterator = fs.listFiles(tablePath, true);\n      while (iterator.hasNext()) {\n        LocatedFileStatus fileStatus = iterator.next();\n        if (!fileStatus.isFile()) {\n          continue;\n        }\n        boolean validFile =\n          (ORIGINAL_PATTERN.matcher(fileStatus.getPath().getName()).matches() ||\n            ORIGINAL_PATTERN_COPY.matcher(fileStatus.getPath().getName()).matches()\n          );\n        if (!validFile) {\n          throw new IllegalStateException(\"Unexpected data file name format.  Cannot convert \" +\n            Warehouse.getQualifiedName(table) + \" to transactional table.  File: \"\n            + fileStatus.getPath());\n        }\n      }\n    } catch (IOException|NoSuchObjectException e) {\n      String msg = \"Unable to list files for \" + Warehouse.getQualifiedName(table);\n      LOG.error(msg, e);\n      MetaException e1 = new MetaException(msg);\n      e1.initCause(e);\n      throw e1;\n    }\n  }"
        ]
    ],
    "780b0127fd22ec95a6b225a493872bcec364ef76": [
        [
            "HiveMaterializedViewsRegistry::Loader::run()",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    @Override\n    public void run() {\n      try {\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }",
            " 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "    @Override\n    public void run() {\n      try {\n        SessionState.start(db.getConf());\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }"
        ],
        [
            "HiveMaterializedViewsRegistry::parseQuery(HiveConf,String)",
            " 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 -\n 417  \n 418  \n 419  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(e.getMessage());\n      return null;\n    }\n  }",
            " 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420  \n 421  \n 422  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(\"Error parsing original query for materialized view\", e);\n      return null;\n    }\n  }"
        ],
        [
            "HiveMaterializedViewsRegistry::init()",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(HiveConf.ConfVars.METASTORE_FILTER_HOOK.varname,\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(MetastoreConf.ConfVars.FILTER_HOOK.getVarname(),\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }"
        ]
    ],
    "610748287846cbd26d0b7c8ccc414f8636fb6ba1": [
        [
            "TestJdbcWithMiniHS2::testHttpHeaderSize()",
            " 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 -\n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 -\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  ",
            "  /**\n   * Test for http header size\n   * @throws Exception\n   */\n  @Test\n  public void testHttpHeaderSize() throws Exception {\n    // Stop HiveServer2\n    stopMiniHS2();\n    HiveConf conf = new HiveConf();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 1024);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 1024);\n    startMiniHS2(conf, true);\n\n    // Username and password are added to the http request header.\n    // We will test the reconfiguration of the header size by changing the password length.\n    String userName = \"userName\";\n    String password = StringUtils.leftPad(\"*\", 100);\n    Connection conn = null;\n    // This should go fine, since header should be less than the configured header size\n    try {\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // This should fail with given HTTP response code 413 in error message, since header is more\n    // than the configured the header size\n    password = StringUtils.leftPad(\"*\", 2000);\n    Exception headerException = null;\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      headerException = e;\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n\n      assertTrue(\"Header exception should be thrown\", headerException != null);\n      assertTrue(\"Incorrect HTTP Response:\" + headerException.getMessage(),\n          headerException.getMessage().contains(\"HTTP Response code: 413\"));\n    }\n\n    // Stop HiveServer2 to increase header size\n    stopMiniHS2();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 3000);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 3000);\n    startMiniHS2(conf);\n\n    // This should now go fine, since we increased the configured header size\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // Restore original state\n    restoreMiniHS2AndConnections();\n  }",
            " 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 +\n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 +\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  ",
            "  /**\n   * Test for http header size\n   * @throws Exception\n   */\n  @Test\n  public void testHttpHeaderSize() throws Exception {\n    // Stop HiveServer2\n    stopMiniHS2();\n    HiveConf conf = new HiveConf();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 1024);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 1024);\n    startMiniHS2(conf, true);\n\n    // Username and password are added to the http request header.\n    // We will test the reconfiguration of the header size by changing the password length.\n    String userName = \"userName\";\n    String password = StringUtils.leftPad(\"*\", 100);\n    Connection conn = null;\n    // This should go fine, since header should be less than the configured header size\n    try {\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // This should fail with given HTTP response code 431 in error message, since header is more\n    // than the configured the header size\n    password = StringUtils.leftPad(\"*\", 2000);\n    Exception headerException = null;\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      headerException = e;\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n\n      assertTrue(\"Header exception should be thrown\", headerException != null);\n      assertTrue(\"Incorrect HTTP Response:\" + headerException.getMessage(),\n          headerException.getMessage().contains(\"HTTP Response code: 431\"));\n    }\n\n    // Stop HiveServer2 to increase header size\n    stopMiniHS2();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 3000);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 3000);\n    startMiniHS2(conf);\n\n    // This should now go fine, since we increased the configured header size\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // Restore original state\n    restoreMiniHS2AndConnections();\n  }"
        ]
    ],
    "5ba634aa665416743d2f63cbb12f601fe408fe9a": [
        [
            "TezSessionState::ensureLocalResources(Configuration,String)",
            " 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598 -\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610 -\n 611 -\n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  ",
            "  /** This is called in openInternal and in TezTask.updateSession to localize conf resources. */\n  public void ensureLocalResources(Configuration conf, String[] newFilesNotFromConf)\n          throws IOException, LoginException, URISyntaxException, TezException {\n    if (resources == null) {\n      throw new AssertionError(\"Ensure called on an unitialized (or closed) session \" + sessionId);\n    }\n    String dir = resources.dagResourcesDir.toString();\n    resources.localizedResources.clear();\n\n    // Always localize files from conf; duplicates are handled on FS level.\n    // TODO: we could do the same thing as below and only localize if missing.\n    //       That could be especially valuable given that this almost always the same set.\n    List<LocalResource> lrs = utils.localizeTempFilesFromConf(dir, conf);\n    if (lrs != null) {\n      resources.localizedResources.addAll(lrs);\n    }\n\n    // Localize the non-conf resources that are missing from the current list.\n    List<LocalResource> newResources = null;\n    if (newFilesNotFromConf != null && newFilesNotFromConf.length > 0) {\n      boolean hasResources = !resources.additionalFilesNotFromConf.isEmpty();\n      if (hasResources) {\n        for (String s : newFilesNotFromConf) {\n          hasResources = resources.additionalFilesNotFromConf.contains(s);\n          if (!hasResources) {\n            break;\n          }\n        }\n      }\n      if (!hasResources) {\n        String[] skipFilesFromConf = DagUtils.getTempFilesFromConf(conf);\n        newResources = utils.localizeTempFiles(dir, conf, newFilesNotFromConf, skipFilesFromConf);\n        if (newResources != null) {\n          resources.localizedResources.addAll(newResources);\n        }\n        for (String fullName : newFilesNotFromConf) {\n          resources.additionalFilesNotFromConf.add(fullName);\n        }\n      }\n    }\n\n    // Finally, add the files to the existing AM (if any). The old code seems to do this twice,\n    // first for all the new resources regardless of type; and then for all the session resources\n    // that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession\n    // and with resourceMap from submit).\n    // TODO: Do we really need all this nonsense?\n    if (session != null) {\n      if (newResources != null && !newResources.isEmpty()) {\n        session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n      }\n      if (!resources.localizedResources.isEmpty()) {\n        session.addAppMasterLocalFiles(\n            DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n      }\n    }\n  }",
            " 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598 +\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610 +\n 611 +\n 612  \n 613 +\n 614 +\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  /** This is called in openInternal and in TezTask.updateSession to localize conf resources. */\n  public void ensureLocalResources(Configuration conf, String[] newFilesNotFromConf)\n          throws IOException, LoginException, URISyntaxException, TezException {\n    if (resources == null) {\n      throw new AssertionError(\"Ensure called on an unitialized (or closed) session \" + sessionId);\n    }\n    String dir = resources.dagResourcesDir.toString();\n    resources.localizedResources.clear();\n\n    // Always localize files from conf; duplicates are handled on FS level.\n    // TODO: we could do the same thing as below and only localize if missing.\n    //       That could be especially valuable given that this almost always the same set.\n    List<LocalResource> lrs = utils.localizeTempFilesFromConf(dir, conf);\n    if (lrs != null) {\n      resources.localizedResources.addAll(lrs);\n    }\n\n    // Localize the non-conf resources that are missing from the current list.\n    List<LocalResource> newResources = null;\n    if (newFilesNotFromConf != null && newFilesNotFromConf.length > 0) {\n      boolean hasResources = !resources.additionalFilesNotFromConf.isEmpty();\n      if (hasResources) {\n        for (String s : newFilesNotFromConf) {\n          hasResources = resources.additionalFilesNotFromConf.keySet().contains(s);\n          if (!hasResources) {\n            break;\n          }\n        }\n      }\n      if (!hasResources) {\n        String[] skipFilesFromConf = DagUtils.getTempFilesFromConf(conf);\n        newResources = utils.localizeTempFiles(dir, conf, newFilesNotFromConf, skipFilesFromConf);\n        if (newResources != null) {\n          resources.localizedResources.addAll(newResources);\n        }\n        for (int i=0;i<newFilesNotFromConf.length;i++) {\n          resources.additionalFilesNotFromConf.put(newFilesNotFromConf[i], newResources.get(i));\n        }\n      } else {\n        resources.localizedResources.addAll(resources.additionalFilesNotFromConf.values());\n      }\n    }\n\n    // Finally, add the files to the existing AM (if any). The old code seems to do this twice,\n    // first for all the new resources regardless of type; and then for all the session resources\n    // that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession\n    // and with resourceMap from submit).\n    // TODO: Do we really need all this nonsense?\n    if (session != null) {\n      if (newResources != null && !newResources.isEmpty()) {\n        session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n      }\n      if (!resources.localizedResources.isEmpty()) {\n        session.addAppMasterLocalFiles(\n            DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n      }\n    }\n  }"
        ]
    ],
    "f5618d9227e5f6e643aaf9d8d625dc1fc42180dc": [
        [
            "AbstractRecordWriter::close()",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  ",
            "  @Override\n  public void close() throws StreamingIOFailure {\n    heapMemoryMonitor.close();\n    boolean haveError = false;\n    String partition = null;\n    if (LOG.isDebugEnabled()) {\n      logStats(\"Stats before close:\");\n    }\n    for (Map.Entry<String, List<RecordUpdater>> entry : updaters.entrySet()) {\n      partition = entry.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Closing updater for partitions: {}\", partition);\n      }\n      for (RecordUpdater updater : entry.getValue()) {\n        if (updater != null) {\n          try {\n            //try not to leave any files open\n            updater.close(false);\n          } catch (Exception ex) {\n            haveError = true;\n            LOG.error(\"Unable to close \" + updater + \" due to: \" + ex.getMessage(), ex);\n          }\n        }\n      }\n      entry.getValue().clear();\n    }\n    updaters.clear();\n    if (LOG.isDebugEnabled()) {\n      logStats(\"Stats after close:\");\n    }\n    if (haveError) {\n      throw new StreamingIOFailure(\"Encountered errors while closing (see logs) \" + getWatermark(partition));\n    }\n  }",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 +\n 398 +\n 399 +\n 400 +\n 401 +\n 402  \n 403  \n 404  \n 405  ",
            "  @Override\n  public void close() throws StreamingIOFailure {\n    heapMemoryMonitor.close();\n    boolean haveError = false;\n    String partition = null;\n    if (LOG.isDebugEnabled()) {\n      logStats(\"Stats before close:\");\n    }\n    for (Map.Entry<String, List<RecordUpdater>> entry : updaters.entrySet()) {\n      partition = entry.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Closing updater for partitions: {}\", partition);\n      }\n      for (RecordUpdater updater : entry.getValue()) {\n        if (updater != null) {\n          try {\n            //try not to leave any files open\n            updater.close(false);\n          } catch (Exception ex) {\n            haveError = true;\n            LOG.error(\"Unable to close \" + updater + \" due to: \" + ex.getMessage(), ex);\n          }\n        }\n      }\n      entry.getValue().clear();\n    }\n    updaters.clear();\n    if (LOG.isDebugEnabled()) {\n      logStats(\"Stats after close:\");\n    }\n    try {\n      this.fs.close();\n    } catch (IOException e) {\n      throw new StreamingIOFailure(\"Error while closing FileSystem\", e);\n    }\n    if (haveError) {\n      throw new StreamingIOFailure(\"Encountered errors while closing (see logs) \" + getWatermark(partition));\n    }\n  }"
        ],
        [
            "HiveStreamingConnection::Builder::connect()",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 -\n 395  \n 396  \n 397  ",
            "    /**\n     * Returning a streaming connection to hive.\n     *\n     * @return - hive streaming connection\n     */\n    public HiveStreamingConnection connect() throws StreamingException {\n      if (database == null) {\n        throw new StreamingException(\"Database cannot be null for streaming connection\");\n      }\n      if (table == null) {\n        if (tableObject == null) {\n          throw new StreamingException(\"Table and table object cannot be \"\n              + \"null for streaming connection\");\n        } else {\n          table = tableObject.getTableName();\n        }\n      }\n\n      if (tableObject != null && !tableObject.getTableName().equals(table)) {\n        throw new StreamingException(\"Table must match tableObject table name\");\n      }\n\n      if (recordWriter == null) {\n        throw new StreamingException(\"Record writer cannot be null for streaming connection\");\n      }\n      if ((writeId != -1 && tableObject == null) ||\n          (writeId == -1 && tableObject != null)){\n        throw new StreamingException(\"If writeId is set, tableObject \"\n            + \"must be set as well and vice versa\");\n      }\n\n      HiveStreamingConnection streamingConnection = new HiveStreamingConnection(this);\n      // assigning higher priority than FileSystem shutdown hook so that streaming connection gets closed first before\n      // filesystem close (to avoid ClosedChannelException)\n      ShutdownHookManager.addShutdownHook(streamingConnection::close,  FileSystem.SHUTDOWN_HOOK_PRIORITY + 1);\n      Thread.setDefaultUncaughtExceptionHandler((t, e) -> streamingConnection.close());\n      return streamingConnection;\n    }",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393 +\n 394  \n 395  \n 396 +\n 397  \n 398  \n 399  ",
            "    /**\n     * Returning a streaming connection to hive.\n     *\n     * @return - hive streaming connection\n     */\n    public HiveStreamingConnection connect() throws StreamingException {\n      if (database == null) {\n        throw new StreamingException(\"Database cannot be null for streaming connection\");\n      }\n      if (table == null) {\n        if (tableObject == null) {\n          throw new StreamingException(\"Table and table object cannot be \"\n              + \"null for streaming connection\");\n        } else {\n          table = tableObject.getTableName();\n        }\n      }\n\n      if (tableObject != null && !tableObject.getTableName().equals(table)) {\n        throw new StreamingException(\"Table must match tableObject table name\");\n      }\n\n      if (recordWriter == null) {\n        throw new StreamingException(\"Record writer cannot be null for streaming connection\");\n      }\n      if ((writeId != -1 && tableObject == null) ||\n          (writeId == -1 && tableObject != null)){\n        throw new StreamingException(\"If writeId is set, tableObject \"\n            + \"must be set as well and vice versa\");\n      }\n\n      HiveStreamingConnection streamingConnection = new HiveStreamingConnection(this);\n      streamingConnection.onShutdownRunner = streamingConnection::close;\n      // assigning higher priority than FileSystem shutdown hook so that streaming connection gets closed first before\n      // filesystem close (to avoid ClosedChannelException)\n      ShutdownHookManager.addShutdownHook(streamingConnection.onShutdownRunner,  FileSystem.SHUTDOWN_HOOK_PRIORITY + 1);\n      Thread.setDefaultUncaughtExceptionHandler((t, e) -> streamingConnection.close());\n      return streamingConnection;\n    }"
        ],
        [
            "HiveStreamingConnection::close()",
            " 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  ",
            "  /**\n   * Close connection\n   */\n  @Override\n  public void close() {\n    if (isConnectionClosed.get()) {\n      return;\n    }\n    isConnectionClosed.set(true);\n    try {\n      if (currentTransactionBatch != null) {\n        currentTransactionBatch.close();\n      }\n    } catch (StreamingException e) {\n      LOG.warn(\"Unable to close current transaction batch: \" + currentTransactionBatch, e);\n    } finally {\n      if (manageTransactions) {\n        getMSC().close();\n        getHeatbeatMSC().close();\n      }\n    }\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Closed streaming connection. Agent: {} Stats: {}\", getAgentInfo(), getConnectionStats());\n    }\n  }",
            " 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656 +\n 657 +\n 658 +\n 659 +\n 660  \n 661  \n 662  \n 663  \n 664  ",
            "  /**\n   * Close connection\n   */\n  @Override\n  public void close() {\n    if (isConnectionClosed.get()) {\n      return;\n    }\n    isConnectionClosed.set(true);\n    try {\n      if (currentTransactionBatch != null) {\n        currentTransactionBatch.close();\n      }\n    } catch (StreamingException e) {\n      LOG.warn(\"Unable to close current transaction batch: \" + currentTransactionBatch, e);\n    } finally {\n      if (manageTransactions) {\n        getMSC().close();\n        getHeatbeatMSC().close();\n      }\n      //remove shutdown hook entry added while creating this connection via HiveStreamingConnection.Builder#connect()\n      if (!ShutdownHookManager.isShutdownInProgress()) {\n        ShutdownHookManager.removeShutdownHook(this.onShutdownRunner);\n      }\n    }\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Closed streaming connection. Agent: {} Stats: {}\", getAgentInfo(), getConnectionStats());\n    }\n  }"
        ]
    ],
    "ae82715f1014c4ed514441311b61ed1891e2a12b": [
        [
            "HiveStatement::getQueryId()",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026 -\n1027 -\n1028 -\n1029  \n1030  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return null;\n    }\n  }",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    }\n  }"
        ],
        [
            "ThriftCLIService::GetQueryId(TGetQueryIdReq)",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    }\n  }",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857 +\n 858 +\n 859 +\n 860  \n 861  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return new TGetQueryIdResp((String)null);\n    }\n  }"
        ]
    ],
    "84b5ba7ac9f93c6a496386db91ae4cd5ab7a451d": [
        [
            "GenericUDTFGetSplits::getSplits(JobConf,int,TezWork,Schema,ApplicationId,boolean)",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 -\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = UserGroupInformation.getLoginUser().getShortUserName();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = RegistryUtils.currentUser();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ]
}