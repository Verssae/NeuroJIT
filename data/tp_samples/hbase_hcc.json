{
    "47063031866e321655e63fd6c59128409c31b3b1": [
        [
            "HFilePrettyPrinter::scanKeysValues(Path,KeyValueStatsCollector,HFileScanner,byte)",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  private void scanKeysValues(Path file, KeyValueStatsCollector fileStats,\n      HFileScanner scanner,  byte[] row) throws IOException {\n    Cell pCell = null;\n    FileSystem fs = FileSystem.get(getConf());\n    Set<String> foundMobFiles = new LinkedHashSet<String>(FOUND_MOB_FILES_CACHE_CAPACITY);\n    Set<String> missingMobFiles = new LinkedHashSet<String>(MISSING_MOB_FILES_CACHE_CAPACITY);\n    do {\n      Cell cell = scanner.getCell();\n      if (row != null && row.length != 0) {\n        int result = CellComparator.COMPARATOR.compareRows(cell, row, 0, row.length);\n        if (result > 0) {\n          break;\n        } else if (result < 0) {\n          continue;\n        }\n      }\n      // collect stats\n      if (printStats) {\n        fileStats.collect(cell);\n      }\n      // dump key value\n      if (printKey) {\n        System.out.print(\"K: \" + cell);\n        if (printValue) {\n          System.out.print(\" V: \"\n              + Bytes.toStringBinary(cell.getValueArray(), cell.getValueOffset(),\n                  cell.getValueLength()));\n          int i = 0;\n          List<Tag> tags = TagUtil.asList(cell.getTagsArray(), cell.getTagsOffset(),\n              cell.getTagsLength());\n          for (Tag tag : tags) {\n            System.out.print(String.format(\" T[%d]: %s\", i++, TagUtil.getValueAsString(tag)));\n          }\n        }\n        System.out.println();\n      }\n      // check if rows are in order\n      if (checkRow && pCell != null) {\n        if (CellComparator.COMPARATOR.compareRows(pCell, cell) > 0) {\n          System.err.println(\"WARNING, previous row is greater then\"\n              + \" current row\\n\\tfilename -> \" + file + \"\\n\\tprevious -> \"\n              + CellUtil.getCellKeyAsString(pCell) + \"\\n\\tcurrent  -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if families are consistent\n      if (checkFamily) {\n        String fam = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(),\n            cell.getFamilyLength());\n        if (!file.toString().contains(fam)) {\n          System.err.println(\"WARNING, filename does not match kv family,\"\n              + \"\\n\\tfilename -> \" + file + \"\\n\\tkeyvalue -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n        if (pCell != null && CellComparator.compareFamilies(pCell, cell) != 0) {\n          System.err.println(\"WARNING, previous kv has different family\"\n              + \" compared to current key\\n\\tfilename -> \" + file\n              + \"\\n\\tprevious -> \" + CellUtil.getCellKeyAsString(pCell)\n              + \"\\n\\tcurrent  -> \" + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if mob files are missing.\n      if (checkMobIntegrity && MobUtils.isMobReferenceCell(cell)) {\n        Tag tnTag = MobUtils.getTableNameTag(cell);\n        if (tnTag == null) {\n          System.err.println(\"ERROR, wrong tag format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {\n          System.err.println(\"ERROR, wrong value format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else {\n          TableName tn = TableName.valueOf(TagUtil.cloneValue(tnTag));\n          String mobFileName = MobUtils.getMobFileName(cell);\n          boolean exist = mobFileExists(fs, tn, mobFileName,\n            Bytes.toString(CellUtil.cloneFamily(cell)), foundMobFiles, missingMobFiles);\n          if (!exist) {\n            // report error\n            System.err.println(\"ERROR, the mob file [\" + mobFileName\n              + \"] is missing referenced by cell \" + CellUtil.getCellKeyAsString(cell));\n          }\n        }\n      }\n      pCell = cell;\n      ++count;\n    } while (scanner.next());\n  }",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 +\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  private void scanKeysValues(Path file, KeyValueStatsCollector fileStats,\n      HFileScanner scanner,  byte[] row) throws IOException {\n    Cell pCell = null;\n    FileSystem fs = FileSystem.get(getConf());\n    Set<String> foundMobFiles = new LinkedHashSet<String>(FOUND_MOB_FILES_CACHE_CAPACITY);\n    Set<String> missingMobFiles = new LinkedHashSet<String>(MISSING_MOB_FILES_CACHE_CAPACITY);\n    do {\n      Cell cell = scanner.getCell();\n      if (row != null && row.length != 0) {\n        int result = CellComparator.COMPARATOR.compareRows(cell, row, 0, row.length);\n        if (result > 0) {\n          break;\n        } else if (result < 0) {\n          continue;\n        }\n      }\n      // collect stats\n      if (printStats) {\n        fileStats.collect(cell);\n      }\n      // dump key value\n      if (printKey) {\n        System.out.print(\"K: \" + cell);\n        if (printValue) {\n          System.out.print(\" V: \"\n              + Bytes.toStringBinary(cell.getValueArray(), cell.getValueOffset(),\n                  cell.getValueLength()));\n          int i = 0;\n          List<Tag> tags = TagUtil.asList(cell.getTagsArray(), cell.getTagsOffset(),\n              cell.getTagsLength());\n          for (Tag tag : tags) {\n            System.out.print(String.format(\" T[%d]: %s\", i++, tag.toString()));\n          }\n        }\n        System.out.println();\n      }\n      // check if rows are in order\n      if (checkRow && pCell != null) {\n        if (CellComparator.COMPARATOR.compareRows(pCell, cell) > 0) {\n          System.err.println(\"WARNING, previous row is greater then\"\n              + \" current row\\n\\tfilename -> \" + file + \"\\n\\tprevious -> \"\n              + CellUtil.getCellKeyAsString(pCell) + \"\\n\\tcurrent  -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if families are consistent\n      if (checkFamily) {\n        String fam = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(),\n            cell.getFamilyLength());\n        if (!file.toString().contains(fam)) {\n          System.err.println(\"WARNING, filename does not match kv family,\"\n              + \"\\n\\tfilename -> \" + file + \"\\n\\tkeyvalue -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n        if (pCell != null && CellComparator.compareFamilies(pCell, cell) != 0) {\n          System.err.println(\"WARNING, previous kv has different family\"\n              + \" compared to current key\\n\\tfilename -> \" + file\n              + \"\\n\\tprevious -> \" + CellUtil.getCellKeyAsString(pCell)\n              + \"\\n\\tcurrent  -> \" + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if mob files are missing.\n      if (checkMobIntegrity && MobUtils.isMobReferenceCell(cell)) {\n        Tag tnTag = MobUtils.getTableNameTag(cell);\n        if (tnTag == null) {\n          System.err.println(\"ERROR, wrong tag format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {\n          System.err.println(\"ERROR, wrong value format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else {\n          TableName tn = TableName.valueOf(TagUtil.cloneValue(tnTag));\n          String mobFileName = MobUtils.getMobFileName(cell);\n          boolean exist = mobFileExists(fs, tn, mobFileName,\n            Bytes.toString(CellUtil.cloneFamily(cell)), foundMobFiles, missingMobFiles);\n          if (!exist) {\n            // report error\n            System.err.println(\"ERROR, the mob file [\" + mobFileName\n              + \"] is missing referenced by cell \" + CellUtil.getCellKeyAsString(cell));\n          }\n        }\n      }\n      pCell = cell;\n      ++count;\n    } while (scanner.next());\n  }"
        ],
        [
            "KeyValue::toStringMap()",
            "1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187 -\n1188  \n1189  \n1190  \n1191  \n1192  ",
            "  /**\n   * Produces a string map for this key/value pair. Useful for programmatic use\n   * and manipulation of the data stored in an WALKey, for example, printing\n   * as JSON. Values are left out due to their tendency to be large. If needed,\n   * they can be added manually.\n   *\n   * @return the Map&lt;String,?&gt; containing data from this key\n   */\n  public Map<String, Object> toStringMap() {\n    Map<String, Object> stringMap = new HashMap<String, Object>();\n    stringMap.put(\"row\", Bytes.toStringBinary(getRowArray(), getRowOffset(), getRowLength()));\n    stringMap.put(\"family\",\n      Bytes.toStringBinary(getFamilyArray(), getFamilyOffset(), getFamilyLength()));\n    stringMap.put(\"qualifier\",\n      Bytes.toStringBinary(getQualifierArray(), getQualifierOffset(), getQualifierLength()));\n    stringMap.put(\"timestamp\", getTimestamp());\n    stringMap.put(\"vlen\", getValueLength());\n    List<Tag> tags = getTags();\n    if (tags != null) {\n      List<String> tagsString = new ArrayList<String>();\n      for (Tag t : tags) {\n        tagsString.add((t.getType()) + \":\" + TagUtil.getValueAsString(t));\n      }\n      stringMap.put(\"tag\", tagsString);\n    }\n    return stringMap;\n  }",
            "1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187 +\n1188  \n1189  \n1190  \n1191  \n1192  ",
            "  /**\n   * Produces a string map for this key/value pair. Useful for programmatic use\n   * and manipulation of the data stored in an WALKey, for example, printing\n   * as JSON. Values are left out due to their tendency to be large. If needed,\n   * they can be added manually.\n   *\n   * @return the Map&lt;String,?&gt; containing data from this key\n   */\n  public Map<String, Object> toStringMap() {\n    Map<String, Object> stringMap = new HashMap<String, Object>();\n    stringMap.put(\"row\", Bytes.toStringBinary(getRowArray(), getRowOffset(), getRowLength()));\n    stringMap.put(\"family\",\n      Bytes.toStringBinary(getFamilyArray(), getFamilyOffset(), getFamilyLength()));\n    stringMap.put(\"qualifier\",\n      Bytes.toStringBinary(getQualifierArray(), getQualifierOffset(), getQualifierLength()));\n    stringMap.put(\"timestamp\", getTimestamp());\n    stringMap.put(\"vlen\", getValueLength());\n    List<Tag> tags = getTags();\n    if (tags != null) {\n      List<String> tagsString = new ArrayList<String>();\n      for (Tag t : tags) {\n        tagsString.add(t.toString());\n      }\n      stringMap.put(\"tag\", tagsString);\n    }\n    return stringMap;\n  }"
        ]
    ],
    "f4470af95db383a47d1bd431a32cd8a6949dd7c0": [
        [
            "JarFinder::createJar(File,File)",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129  ",
            "  private static void createJar(File dir, File jarFile) throws IOException {\n    Preconditions.checkNotNull(dir, \"dir\");\n    Preconditions.checkNotNull(jarFile, \"jarFile\");\n    File jarDir = jarFile.getParentFile();\n    if (!jarDir.exists()) {\n      if (!jarDir.mkdirs()) {\n        throw new IOException(MessageFormat.format(\"could not create dir [{0}]\",\n                                                   jarDir));\n      }\n    }\n    JarOutputStream zos = new JarOutputStream(new FileOutputStream(jarFile));\n    jarDir(dir, \"\", zos);\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129 +\n 130  ",
            "  private static void createJar(File dir, File jarFile) throws IOException {\n    Preconditions.checkNotNull(dir, \"dir\");\n    Preconditions.checkNotNull(jarFile, \"jarFile\");\n    File jarDir = jarFile.getParentFile();\n    if (!jarDir.exists()) {\n      if (!jarDir.mkdirs()) {\n        throw new IOException(MessageFormat.format(\"could not create dir [{0}]\",\n                                                   jarDir));\n      }\n    }\n    try (JarOutputStream zos = new JarOutputStream(new FileOutputStream(jarFile))) {\n      jarDir(dir, \"\", zos);\n    }\n  }"
        ],
        [
            "CoprocessorClassLoader::init(Path,String,Configuration)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "  private void init(Path path, String pathPrefix,\n      Configuration conf) throws IOException {\n    // Copy the jar to the local filesystem\n    String parentDirStr =\n      conf.get(LOCAL_DIR_KEY, DEFAULT_LOCAL_DIR) + TMP_JARS_DIR;\n    synchronized (parentDirLockSet) {\n      if (!parentDirLockSet.contains(parentDirStr)) {\n        Path parentDir = new Path(parentDirStr);\n        FileSystem fs = FileSystem.getLocal(conf);\n        fs.delete(parentDir, true); // it's ok if the dir doesn't exist now\n        parentDirLockSet.add(parentDirStr);\n        if (!fs.mkdirs(parentDir) && !fs.getFileStatus(parentDir).isDirectory()) {\n          throw new RuntimeException(\"Failed to create local dir \" + parentDirStr\n            + \", CoprocessorClassLoader failed to init\");\n        }\n      }\n    }\n\n    FileSystem fs = path.getFileSystem(conf);\n    File dst = new File(parentDirStr, \".\" + pathPrefix + \".\"\n      + path.getName() + \".\" + System.currentTimeMillis() + \".jar\");\n    fs.copyToLocalFile(path, new Path(dst.toString()));\n    dst.deleteOnExit();\n\n    addURL(dst.getCanonicalFile().toURI().toURL());\n\n    JarFile jarFile = new JarFile(dst.toString());\n    try {\n      Enumeration<JarEntry> entries = jarFile.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = entries.nextElement();\n        Matcher m = libJarPattern.matcher(entry.getName());\n        if (m.matches()) {\n          File file = new File(parentDirStr, \".\" + pathPrefix + \".\"\n            + path.getName() + \".\" + System.currentTimeMillis() + \".\" + m.group(1));\n          IOUtils.copyBytes(jarFile.getInputStream(entry),\n            new FileOutputStream(file), conf, true);\n          file.deleteOnExit();\n          addURL(file.toURI().toURL());\n        }\n      }\n    } finally {\n      jarFile.close();\n    }\n  }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181 +\n 182 +\n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "  private void init(Path path, String pathPrefix,\n      Configuration conf) throws IOException {\n    // Copy the jar to the local filesystem\n    String parentDirStr =\n      conf.get(LOCAL_DIR_KEY, DEFAULT_LOCAL_DIR) + TMP_JARS_DIR;\n    synchronized (parentDirLockSet) {\n      if (!parentDirLockSet.contains(parentDirStr)) {\n        Path parentDir = new Path(parentDirStr);\n        FileSystem fs = FileSystem.getLocal(conf);\n        fs.delete(parentDir, true); // it's ok if the dir doesn't exist now\n        parentDirLockSet.add(parentDirStr);\n        if (!fs.mkdirs(parentDir) && !fs.getFileStatus(parentDir).isDirectory()) {\n          throw new RuntimeException(\"Failed to create local dir \" + parentDirStr\n            + \", CoprocessorClassLoader failed to init\");\n        }\n      }\n    }\n\n    FileSystem fs = path.getFileSystem(conf);\n    File dst = new File(parentDirStr, \".\" + pathPrefix + \".\"\n      + path.getName() + \".\" + System.currentTimeMillis() + \".jar\");\n    fs.copyToLocalFile(path, new Path(dst.toString()));\n    dst.deleteOnExit();\n\n    addURL(dst.getCanonicalFile().toURI().toURL());\n\n    JarFile jarFile = new JarFile(dst.toString());\n    try {\n      Enumeration<JarEntry> entries = jarFile.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = entries.nextElement();\n        Matcher m = libJarPattern.matcher(entry.getName());\n        if (m.matches()) {\n          File file = new File(parentDirStr, \".\" + pathPrefix + \".\"\n            + path.getName() + \".\" + System.currentTimeMillis() + \".\" + m.group(1));\n          try (FileOutputStream outStream = new FileOutputStream(file)) {\n            IOUtils.copyBytes(jarFile.getInputStream(entry),\n              outStream, conf, true);\n          }\n          file.deleteOnExit();\n          addURL(file.toURI().toURL());\n        }\n      }\n    } finally {\n      jarFile.close();\n    }\n  }"
        ],
        [
            "ZKUtil::getServerStats(String,int)",
            "1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929 -\n1930  \n1931 -\n1932 -\n1933 -\n1934 -\n1935 -\n1936 -\n1937 -\n1938 -\n1939 -\n1940 -\n1941 -\n1942 -\n1943 -\n1944 -\n1945 -\n1946  \n1947  \n1948 -\n1949 -\n1950  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n  throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;\n\n    Socket socket = new Socket();\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    socket.connect(sockAddr, timeout);\n\n    socket.setSoTimeout(timeout);\n    PrintWriter out = new PrintWriter(socket.getOutputStream(), true);\n    BufferedReader in = new BufferedReader(new InputStreamReader(\n      socket.getInputStream()));\n    out.println(\"stat\");\n    out.flush();\n    ArrayList<String> res = new ArrayList<String>();\n    while (true) {\n      String line = in.readLine();\n      if (line != null) {\n        res.add(line);\n      } else {\n        break;\n      }\n    }\n    socket.close();\n    return res.toArray(new String[res.size()]);\n  }",
            "1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930 +\n1931 +\n1932 +\n1933 +\n1934 +\n1935 +\n1936 +\n1937 +\n1938 +\n1939 +\n1940 +\n1941 +\n1942 +\n1943 +\n1944 +\n1945 +\n1946 +\n1947  \n1948 +\n1949  \n1950  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n  throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;\n\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    try (Socket socket = new Socket()) {\n      socket.connect(sockAddr, timeout);\n\n      socket.setSoTimeout(timeout);\n      PrintWriter out = new PrintWriter(socket.getOutputStream(), true);\n      BufferedReader in = new BufferedReader(new InputStreamReader(\n        socket.getInputStream()));\n      out.println(\"stat\");\n      out.flush();\n      ArrayList<String> res = new ArrayList<String>();\n      while (true) {\n        String line = in.readLine();\n        if (line != null) {\n          res.add(line);\n        } else {\n          break;\n        }\n      }\n      return res.toArray(new String[res.size()]);\n    }\n  }"
        ],
        [
            "LogLevel::process(String)",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76 -\n  77 -\n  78 -\n  79 -\n  80 -\n  81  \n  82 -\n  83  \n  84  \n  85  \n  86  ",
            "  private static void process(String urlstring) {\n    try {\n      URL url = new URL(urlstring);\n      System.out.println(\"Connecting to \" + url);\n      URLConnection connection = url.openConnection();\n      connection.connect();\n\n      BufferedReader in = new BufferedReader(new InputStreamReader(\n          connection.getInputStream()));\n      for(String line; (line = in.readLine()) != null; )\n        if (line.startsWith(MARKER)) {\n          System.out.println(TAG.matcher(line).replaceAll(\"\"));\n        }\n      in.close();\n    } catch (IOException ioe) {\n      System.err.println(\"\" + ioe);\n    }\n  }",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  ",
            "  private static void process(String urlstring) {\n    try {\n      URL url = new URL(urlstring);\n      System.out.println(\"Connecting to \" + url);\n      URLConnection connection = url.openConnection();\n      connection.connect();\n      try (BufferedReader in = new BufferedReader(new InputStreamReader(\n               connection.getInputStream()))) {\n        for(String line; (line = in.readLine()) != null; ) {\n          if (line.startsWith(MARKER)) {\n            System.out.println(TAG.matcher(line).replaceAll(\"\"));\n          }\n        }\n      }\n    } catch (IOException ioe) {\n      System.err.println(\"\" + ioe);\n    }\n  }"
        ]
    ],
    "cd2588001cf31ad2fb2020f9e021c9b1be1b76fc": [
        [
            "SimpleRegionNormalizer::computePlanForTable(TableName)",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<NormalizationPlan>();\n    List<HRegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < MIN_REGION_COUNT) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + MIN_REGION_COUNT + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      HRegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        totalSizeMb += regionSize;\n      }\n    }\n\n    double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      HRegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          HRegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<NormalizationPlan>();\n    List<HRegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < MIN_REGION_COUNT) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + MIN_REGION_COUNT + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      HRegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        totalSizeMb += regionSize;\n      }\n    }\n\n    double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      HRegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          HRegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize > 0 && regionSize2 > 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }"
        ]
    ],
    "fa50d456a8a05517004fec853258e4a01ca35a23": [
        [
            "SimpleRpcScheduler::SimpleRpcScheduler(Configuration,int,int,int,PriorityFunction,Abortable,int)",
            " 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203 -\n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  ",
            "  /**\n   * @param conf\n   * @param handlerCount the number of handler threads that will be used to process calls\n   * @param priorityHandlerCount How many threads for priority handling.\n   * @param replicationHandlerCount How many threads for replication handling.\n   * @param highPriorityLevel\n   * @param priority Function to extract request priority.\n   */\n  public SimpleRpcScheduler(\n      Configuration conf,\n      int handlerCount,\n      int priorityHandlerCount,\n      int replicationHandlerCount,\n      PriorityFunction priority,\n      Abortable server,\n      int highPriorityLevel) {\n\n    int maxQueueLength = conf.getInt(RpcScheduler.IPC_SERVER_MAX_CALLQUEUE_LENGTH,\n        handlerCount * RpcServer.DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER);\n    int maxPriorityQueueLength =\n        conf.getInt(RpcScheduler.IPC_SERVER_PRIORITY_MAX_CALLQUEUE_LENGTH, maxQueueLength);\n\n    this.priority = priority;\n    this.highPriorityLevel = highPriorityLevel;\n    this.abortable = server;\n\n    String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY,\n      CALL_QUEUE_TYPE_DEADLINE_CONF_VALUE);\n    float callqReadShare = conf.getFloat(CALL_QUEUE_READ_SHARE_CONF_KEY, 0);\n    float callqScanShare = conf.getFloat(CALL_QUEUE_SCAN_SHARE_CONF_KEY, 0);\n\n    int codelTargetDelay = conf.getInt(CALL_QUEUE_CODEL_TARGET_DELAY,\n      CALL_QUEUE_CODEL_DEFAULT_TARGET_DELAY);\n    int codelInterval = conf.getInt(CALL_QUEUE_CODEL_INTERVAL,\n      CALL_QUEUE_CODEL_DEFAULT_INTERVAL);\n    double codelLifoThreshold = conf.getDouble(CALL_QUEUE_CODEL_LIFO_THRESHOLD,\n      CALL_QUEUE_CODEL_DEFAULT_LIFO_THRESHOLD);\n\n    float callQueuesHandlersFactor = conf.getFloat(CALL_QUEUE_HANDLER_FACTOR_CONF_KEY, 0);\n    int numCallQueues = Math.max(1, (int)Math.round(handlerCount * callQueuesHandlersFactor));\n    LOG.info(\"Using \" + callQueueType + \" as user call queue; numCallQueues=\" + numCallQueues +\n        \"; callQReadShare=\" + callqReadShare + \", callQScanShare=\" + callqScanShare);\n    if (numCallQueues > 1 && callqReadShare > 0) {\n      // multiple read/write queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount, numCallQueues,\n            callqReadShare, callqScanShare, maxQueueLength, conf, abortable,\n            BoundedPriorityBlockingQueue.class, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        Object[] callQueueInitArgs = {maxQueueLength, codelTargetDelay, codelInterval,\n          codelLifoThreshold, numGeneralCallsDropped, numLifoModeSwitches};\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount,\n          numCallQueues, callqReadShare, callqScanShare,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs);\n      } else {\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount, numCallQueues,\n          callqReadShare, callqScanShare, maxQueueLength, conf, abortable);\n      }\n    } else {\n      // multiple queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount, numCallQueues,\n            conf, abortable, BoundedPriorityBlockingQueue.class, maxQueueLength, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount, numCallQueues,\n            conf, abortable, AdaptiveLifoCoDelCallQueue.class, maxQueueLength,\n            codelTargetDelay, codelInterval, codelLifoThreshold,\n            numGeneralCallsDropped, numLifoModeSwitches);\n      } else {\n        callExecutor = new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount,\n            numCallQueues, maxQueueLength, conf, abortable);\n      }\n    }\n    // Create 2 queues to help priorityExecutor be more scalable.\n    this.priorityExecutor = priorityHandlerCount > 0 ?\n      new BalancedQueueRpcExecutor(\"BalancedQ.priority\", priorityHandlerCount, 2,\n          maxPriorityQueueLength):\n      null;\n   this.replicationExecutor =\n     replicationHandlerCount > 0 ? new BalancedQueueRpcExecutor(\"BalancedQ.replication\",\n       replicationHandlerCount, 1, maxQueueLength, conf, abortable) : null;\n  }",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233  \n 234  \n 235 +\n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242  \n 243  \n 244  \n 245 +\n 246  \n 247  ",
            "  /**\n   * @param conf\n   * @param handlerCount the number of handler threads that will be used to process calls\n   * @param priorityHandlerCount How many threads for priority handling.\n   * @param replicationHandlerCount How many threads for replication handling.\n   * @param highPriorityLevel\n   * @param priority Function to extract request priority.\n   */\n  public SimpleRpcScheduler(\n      Configuration conf,\n      int handlerCount,\n      int priorityHandlerCount,\n      int replicationHandlerCount,\n      PriorityFunction priority,\n      Abortable server,\n      int highPriorityLevel) {\n\n    int maxQueueLength = conf.getInt(RpcScheduler.IPC_SERVER_MAX_CALLQUEUE_LENGTH,\n        handlerCount * RpcServer.DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER);\n    int maxPriorityQueueLength =\n        conf.getInt(RpcScheduler.IPC_SERVER_PRIORITY_MAX_CALLQUEUE_LENGTH, maxQueueLength);\n\n    this.priority = priority;\n    this.highPriorityLevel = highPriorityLevel;\n    this.abortable = server;\n\n    String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY,\n        CALL_QUEUE_TYPE_FIFO_CONF_VALUE);\n    float callqReadShare = conf.getFloat(CALL_QUEUE_READ_SHARE_CONF_KEY, 0);\n    float callqScanShare = conf.getFloat(CALL_QUEUE_SCAN_SHARE_CONF_KEY, 0);\n\n    int codelTargetDelay = conf.getInt(CALL_QUEUE_CODEL_TARGET_DELAY,\n      CALL_QUEUE_CODEL_DEFAULT_TARGET_DELAY);\n    int codelInterval = conf.getInt(CALL_QUEUE_CODEL_INTERVAL,\n      CALL_QUEUE_CODEL_DEFAULT_INTERVAL);\n    double codelLifoThreshold = conf.getDouble(CALL_QUEUE_CODEL_LIFO_THRESHOLD,\n      CALL_QUEUE_CODEL_DEFAULT_LIFO_THRESHOLD);\n\n    float callQueuesHandlersFactor = conf.getFloat(CALL_QUEUE_HANDLER_FACTOR_CONF_KEY, 0);\n    int numCallQueues = Math.max(1, (int)Math.round(handlerCount * callQueuesHandlersFactor));\n    LOG.info(\"Using \" + callQueueType + \" as user call queue; numCallQueues=\" + numCallQueues +\n        \"; callQReadShare=\" + callqReadShare + \", callQScanShare=\" + callqScanShare);\n    if (numCallQueues > 1 && callqReadShare > 0) {\n      // multiple read/write queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor = new RWQueueRpcExecutor(\"RW.deadline.Q\", handlerCount, numCallQueues,\n            callqReadShare, callqScanShare, maxQueueLength, conf, abortable,\n            BoundedPriorityBlockingQueue.class, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        Object[] callQueueInitArgs = {maxQueueLength, codelTargetDelay, codelInterval,\n          codelLifoThreshold, numGeneralCallsDropped, numLifoModeSwitches};\n        callExecutor = new RWQueueRpcExecutor(\"RW.codel.Q\", handlerCount,\n          numCallQueues, callqReadShare, callqScanShare,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs);\n      } else {\n        callExecutor = new RWQueueRpcExecutor(\"RW.fifo.Q\", handlerCount, numCallQueues,\n          callqReadShare, callqScanShare, maxQueueLength, conf, abortable);\n      }\n    } else {\n      // multiple queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"B.deadline.Q\", handlerCount, numCallQueues,\n            conf, abortable, BoundedPriorityBlockingQueue.class, maxQueueLength, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"B.codel.Q\", handlerCount, numCallQueues,\n            conf, abortable, AdaptiveLifoCoDelCallQueue.class, maxQueueLength,\n            codelTargetDelay, codelInterval, codelLifoThreshold,\n            numGeneralCallsDropped, numLifoModeSwitches);\n      } else {\n        callExecutor = new BalancedQueueRpcExecutor(\"B.fifo.Q\", handlerCount,\n            numCallQueues, maxQueueLength, conf, abortable);\n      }\n    }\n    // Create 2 queues to help priorityExecutor be more scalable.\n    this.priorityExecutor = priorityHandlerCount > 0 ?\n      new BalancedQueueRpcExecutor(\"B.priority.fifo.Q\", priorityHandlerCount, 2,\n          maxPriorityQueueLength):\n      null;\n   this.replicationExecutor =\n     replicationHandlerCount > 0 ? new BalancedQueueRpcExecutor(\"B.replication.fifo.Q\",\n       replicationHandlerCount, 1, maxQueueLength, conf, abortable) : null;\n  }"
        ]
    ],
    "d8902ba0e68ec7bc38a8aa8d212353c380e5d378": [
        [
            "VerifyReplication::doCommandLine(String)",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 +\n 404 +\n 405 +\n 406 +\n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--\")) {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "VerifyReplication::printUsage(String)",
            " 433  \n 434  \n 435  \n 436  \n 437  \n 438 -\n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--stoptime=Y] [--families=A] [--row-prefixes=B] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--endtime=Y] [--families=A] [--row-prefixes=B] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }"
        ]
    ],
    "7f44dfd85fc1aacd451cb8514fbce6dafd3443ca": [
        [
            "ScannerCallableWithReplicas::call(int)",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 -\n 220  ",
            "  @Override\n  public Result [] call(int timeout) throws IOException {\n    // If the active replica callable was closed somewhere, invoke the RPC to\n    // really close it. In the case of regular scanners, this applies. We make couple\n    // of RPCs to a RegionServer, and when that region is exhausted, we set\n    // the closed flag. Then an RPC is required to actually close the scanner.\n    if (currentScannerCallable != null && currentScannerCallable.closed) {\n      // For closing we target that exact scanner (and not do replica fallback like in\n      // the case of normal reads)\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Closing scanner id=\" + currentScannerCallable.scannerId);\n      }\n      Result[] r = currentScannerCallable.call(timeout);\n      currentScannerCallable = null;\n      return r;\n    }\n    // We need to do the following:\n    //1. When a scan goes out to a certain replica (default or not), we need to\n    //   continue to hit that until there is a failure. So store the last successfully invoked\n    //   replica\n    //2. We should close the \"losing\" scanners (scanners other than the ones we hear back\n    //   from first)\n    //\n    RegionLocations rl = RpcRetryingCallerWithReadReplicas.getRegionLocations(true,\n        RegionReplicaUtil.DEFAULT_REPLICA_ID, cConnection, tableName,\n        currentScannerCallable.getRow());\n\n    // allocate a boundedcompletion pool of some multiple of number of replicas.\n    // We want to accomodate some RPCs for redundant replica scans (but are still in progress)\n    ResultBoundedCompletionService<Pair<Result[], ScannerCallable>> cs =\n        new ResultBoundedCompletionService<Pair<Result[], ScannerCallable>>(\n            RpcRetryingCallerFactory.instantiate(ScannerCallableWithReplicas.this.conf), pool,\n            rl.size() * 5);\n\n    AtomicBoolean done = new AtomicBoolean(false);\n    replicaSwitched.set(false);\n    // submit call for the primary replica.\n    addCallsForCurrentReplica(cs, rl);\n\n    try {\n      // wait for the timeout to see whether the primary responds back\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeBeforeReplicas,\n          TimeUnit.MICROSECONDS); // Yes, microseconds\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); //great we got a response\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    }\n\n    // submit call for the all of the secondaries at once\n    // TODO: this may be an overkill for large region replication\n    addCallsForOtherReplicas(cs, rl, 0, rl.size() - 1);\n\n    try {\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeout, TimeUnit.MILLISECONDS);\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); // great we got an answer\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } finally {\n      // We get there because we were interrupted or because one or more of the\n      // calls succeeded or failed. In all case, we stop all our tasks.\n      cs.cancelAll();\n    }\n    return null; // unreachable\n  }",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206 +\n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223 +\n 224  ",
            "  @Override\n  public Result [] call(int timeout) throws IOException {\n    // If the active replica callable was closed somewhere, invoke the RPC to\n    // really close it. In the case of regular scanners, this applies. We make couple\n    // of RPCs to a RegionServer, and when that region is exhausted, we set\n    // the closed flag. Then an RPC is required to actually close the scanner.\n    if (currentScannerCallable != null && currentScannerCallable.closed) {\n      // For closing we target that exact scanner (and not do replica fallback like in\n      // the case of normal reads)\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Closing scanner id=\" + currentScannerCallable.scannerId);\n      }\n      Result[] r = currentScannerCallable.call(timeout);\n      currentScannerCallable = null;\n      return r;\n    }\n    // We need to do the following:\n    //1. When a scan goes out to a certain replica (default or not), we need to\n    //   continue to hit that until there is a failure. So store the last successfully invoked\n    //   replica\n    //2. We should close the \"losing\" scanners (scanners other than the ones we hear back\n    //   from first)\n    //\n    RegionLocations rl = RpcRetryingCallerWithReadReplicas.getRegionLocations(true,\n        RegionReplicaUtil.DEFAULT_REPLICA_ID, cConnection, tableName,\n        currentScannerCallable.getRow());\n\n    // allocate a boundedcompletion pool of some multiple of number of replicas.\n    // We want to accomodate some RPCs for redundant replica scans (but are still in progress)\n    ResultBoundedCompletionService<Pair<Result[], ScannerCallable>> cs =\n        new ResultBoundedCompletionService<Pair<Result[], ScannerCallable>>(\n            RpcRetryingCallerFactory.instantiate(ScannerCallableWithReplicas.this.conf), pool,\n            rl.size() * 5);\n\n    AtomicBoolean done = new AtomicBoolean(false);\n    replicaSwitched.set(false);\n    // submit call for the primary replica.\n    addCallsForCurrentReplica(cs, rl);\n\n    try {\n      // wait for the timeout to see whether the primary responds back\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeBeforeReplicas,\n          TimeUnit.MICROSECONDS); // Yes, microseconds\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); //great we got a response\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    }\n\n    // submit call for the all of the secondaries at once\n    // TODO: this may be an overkill for large region replication\n    addCallsForOtherReplicas(cs, rl, 0, rl.size() - 1);\n\n    try {\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeout, TimeUnit.MILLISECONDS);\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); // great we got an answer\n      } else {\n        throw new IOException(\"Failed to get result within timeout, timeout=\"\n            + timeout + \"ms\");\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } finally {\n      // We get there because we were interrupted or because one or more of the\n      // calls succeeded or failed. In all case, we stop all our tasks.\n      cs.cancelAll();\n    }\n    LOG.error(\"Imposible? Arrive at an unreachable line...\"); // unreachable\n    throw new IOException(\"Imposible? Arrive at an unreachable line...\");\n  }"
        ],
        [
            "HMasterCommandLine::run(String)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  public int run(String args[]) throws Exception {\n    Options opt = new Options();\n    opt.addOption(\"localRegionServers\", true,\n      \"RegionServers to start in master process when running standalone\");\n    opt.addOption(\"masters\", true, \"Masters to start in this process\");\n    opt.addOption(\"minRegionServers\", true, \"Minimum RegionServers needed to host user tables\");\n    opt.addOption(\"backup\", false, \"Do not try to become HMaster until the primary fails\");\n\n    CommandLine cmd;\n    try {\n      cmd = new GnuParser().parse(opt, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      usage(null);\n      return 1;\n    }\n\n\n    if (cmd.hasOption(\"minRegionServers\")) {\n      String val = cmd.getOptionValue(\"minRegionServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minRegionServers set to \" + val);\n    }\n\n    // minRegionServers used to be minServers.  Support it too.\n    if (cmd.hasOption(\"minServers\")) {\n      String val = cmd.getOptionValue(\"minServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minServers set to \" + val);\n    }\n\n    // check if we are the backup master - override the conf if so\n    if (cmd.hasOption(\"backup\")) {\n      getConf().setBoolean(HConstants.MASTER_TYPE_BACKUP, true);\n    }\n\n    // How many regionservers to startup in this process (we run regionservers in same process as\n    // master when we are in local/standalone mode. Useful testing)\n    if (cmd.hasOption(\"localRegionServers\")) {\n      String val = cmd.getOptionValue(\"localRegionServers\");\n      getConf().setInt(\"hbase.regionservers\", Integer.parseInt(val));\n      LOG.debug(\"localRegionServers set to \" + val);\n    }\n    // How many masters to startup inside this process; useful testing\n    if (cmd.hasOption(\"masters\")) {\n      String val = cmd.getOptionValue(\"masters\");\n      getConf().setInt(\"hbase.masters\", Integer.parseInt(val));\n      LOG.debug(\"masters set to \" + val);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = cmd.getArgList();\n    if (remainingArgs.size() != 1) {\n      usage(null);\n      return 1;\n    }\n\n    String command = remainingArgs.get(0);\n\n    if (\"start\".equals(command)) {\n      return startMaster();\n    } else if (\"stop\".equals(command)) {\n      return stopMaster();\n    } else if (\"clear\".equals(command)) {\n      return (ZNodeClearer.clear(getConf()) ? 0 : 1);\n    } else {\n      usage(\"Invalid command: \" + command);\n      return 1;\n    }\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "  public int run(String args[]) throws Exception {\n    Options opt = new Options();\n    opt.addOption(\"localRegionServers\", true,\n      \"RegionServers to start in master process when running standalone\");\n    opt.addOption(\"masters\", true, \"Masters to start in this process\");\n    opt.addOption(\"minRegionServers\", true, \"Minimum RegionServers needed to host user tables\");\n    opt.addOption(\"backup\", false, \"Do not try to become HMaster until the primary fails\");\n\n    CommandLine cmd;\n    try {\n      cmd = new GnuParser().parse(opt, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      usage(null);\n      return 1;\n    }\n\n\n    if (cmd.hasOption(\"minRegionServers\")) {\n      String val = cmd.getOptionValue(\"minRegionServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minRegionServers set to \" + val);\n    }\n\n    // minRegionServers used to be minServers.  Support it too.\n    if (cmd.hasOption(\"minServers\")) {\n      String val = cmd.getOptionValue(\"minServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\", Integer.parseInt(val));\n      LOG.debug(\"minServers set to \" + val);\n    }\n\n    // check if we are the backup master - override the conf if so\n    if (cmd.hasOption(\"backup\")) {\n      getConf().setBoolean(HConstants.MASTER_TYPE_BACKUP, true);\n    }\n\n    // How many regionservers to startup in this process (we run regionservers in same process as\n    // master when we are in local/standalone mode. Useful testing)\n    if (cmd.hasOption(\"localRegionServers\")) {\n      String val = cmd.getOptionValue(\"localRegionServers\");\n      getConf().setInt(\"hbase.regionservers\", Integer.parseInt(val));\n      LOG.debug(\"localRegionServers set to \" + val);\n    }\n    // How many masters to startup inside this process; useful testing\n    if (cmd.hasOption(\"masters\")) {\n      String val = cmd.getOptionValue(\"masters\");\n      getConf().setInt(\"hbase.masters\", Integer.parseInt(val));\n      LOG.debug(\"masters set to \" + val);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = cmd.getArgList();\n    if (remainingArgs.size() != 1) {\n      usage(null);\n      return 1;\n    }\n\n    String command = remainingArgs.get(0);\n\n    if (\"start\".equals(command)) {\n      return startMaster();\n    } else if (\"stop\".equals(command)) {\n      return stopMaster();\n    } else if (\"clear\".equals(command)) {\n      return (ZNodeClearer.clear(getConf()) ? 0 : 1);\n    } else {\n      usage(\"Invalid command: \" + command);\n      return 1;\n    }\n  }"
        ]
    ],
    "ac6b998afe033cbb6a307d249c8e18bb97d54c9f": [
        [
            "IntegrationTestRSGroup::beforeMethod()",
            "  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  ",
            "  @Before\n  public void beforeMethod() throws Exception {\n    if(!initialized) {\n      LOG.info(\"Setting up IntegrationTestRSGroup\");\n      LOG.info(\"Initializing cluster with \" + NUM_SLAVES_BASE + \" servers\");\n      TEST_UTIL = new IntegrationTestingUtility();\n      ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);\n      //set shared configs\n      admin = TEST_UTIL.getAdmin();\n      cluster = TEST_UTIL.getHBaseClusterInterface();\n      rsGroupAdmin = new VerifyingRSGroupAdminClient(new RSGroupAdminClient(TEST_UTIL.getConnection()),\n          TEST_UTIL.getConfiguration());\n      LOG.info(\"Done initializing cluster\");\n      initialized = true;\n      //cluster may not be clean\n      //cleanup when initializing\n      afterMethod();\n    }\n  }",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 +\n  49 +\n  50 +\n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "  @Before\n  public void beforeMethod() throws Exception {\n    if(!initialized) {\n      LOG.info(\"Setting up IntegrationTestRSGroup\");\n      LOG.info(\"Initializing cluster with \" + NUM_SLAVES_BASE + \" servers\");\n      TEST_UTIL = new IntegrationTestingUtility();\n      TEST_UTIL.getConfiguration().set(HConstants.HBASE_MASTER_LOADBALANCER_CLASS,\n        RSGroupBasedLoadBalancer.class.getName());\n      TEST_UTIL.getConfiguration().set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,\n        RSGroupAdminEndpoint.class.getName());\n      ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);\n      //set shared configs\n      admin = TEST_UTIL.getAdmin();\n      cluster = TEST_UTIL.getHBaseClusterInterface();\n      rsGroupAdmin = new VerifyingRSGroupAdminClient(new RSGroupAdminClient(TEST_UTIL.getConnection()),\n          TEST_UTIL.getConfiguration());\n      LOG.info(\"Done initializing cluster\");\n      initialized = true;\n      //cluster may not be clean\n      //cleanup when initializing\n      afterMethod();\n    }\n  }"
        ]
    ],
    "f5dbdf0dab731a986d9aea2ad3dfdb400f1ba46c": [
        [
            "IntegrationTestingUtility::createDistributedHBaseCluster()",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  public void createDistributedHBaseCluster() throws IOException {\n    Configuration conf = getConfiguration();\n    Class<? extends ClusterManager> clusterManagerClass = conf.getClass(HBASE_CLUSTER_MANAGER_CLASS,\n      DEFAULT_HBASE_CLUSTER_MANAGER_CLASS, ClusterManager.class);\n    ClusterManager clusterManager = ReflectionUtils.newInstance(\n      clusterManagerClass, conf);\n    setHBaseCluster(new DistributedHBaseCluster(conf, clusterManager));\n    getAdmin();\n  }",
            " 133  \n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  ",
            "  public void createDistributedHBaseCluster() throws IOException {\n    Configuration conf = getConfiguration();\n    conf.set(\"fs.defaultFS\", conf.get(\"original.defaultFS\"));\n    Class<? extends ClusterManager> clusterManagerClass = conf.getClass(HBASE_CLUSTER_MANAGER_CLASS,\n      DEFAULT_HBASE_CLUSTER_MANAGER_CLASS, ClusterManager.class);\n    ClusterManager clusterManager = ReflectionUtils.newInstance(\n      clusterManagerClass, conf);\n    setHBaseCluster(new DistributedHBaseCluster(conf, clusterManager));\n    getAdmin();\n  }"
        ],
        [
            "HBaseTestingUtility::HBaseTestingUtility(Configuration)",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "  /**\n   * <p>Create an HBaseTestingUtility using a given configuration.\n   *\n   * <p>Initially, all tmp files are written to a local test data directory.\n   * Once {@link #startMiniDFSCluster} is called, either directly or via\n   * {@link #startMiniCluster()}, tmp data will be written to the DFS directory instead.\n   *\n   * <p>Previously, there was a distinction between the type of utility returned by\n   * {@link #createLocalHTU()} and this constructor; this is no longer the case. All\n   * HBaseTestingUtility objects will behave as local until a DFS cluster is started,\n   * at which point they will switch to using mini DFS for storage.\n   *\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtility(@Nullable Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // if conf is provided, prevent contention for ports if other hbase thread(s) are running\n    if (conf != null) {\n      if (conf.getInt(HConstants.MASTER_INFO_PORT, HConstants.DEFAULT_MASTER_INFOPORT)\n              == HConstants.DEFAULT_MASTER_INFOPORT) {\n        conf.setInt(HConstants.MASTER_INFO_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.MASTER_INFO_PORT);\n      }\n      if (conf.getInt(HConstants.REGIONSERVER_PORT, HConstants.DEFAULT_REGIONSERVER_PORT)\n              == HConstants.DEFAULT_REGIONSERVER_PORT) {\n        conf.setInt(HConstants.REGIONSERVER_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.REGIONSERVER_PORT);\n      }\n    }\n\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\",\"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE,false);\n  }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349 +\n 350 +\n 351 +\n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "  /**\n   * <p>Create an HBaseTestingUtility using a given configuration.\n   *\n   * <p>Initially, all tmp files are written to a local test data directory.\n   * Once {@link #startMiniDFSCluster} is called, either directly or via\n   * {@link #startMiniCluster()}, tmp data will be written to the DFS directory instead.\n   *\n   * <p>Previously, there was a distinction between the type of utility returned by\n   * {@link #createLocalHTU()} and this constructor; this is no longer the case. All\n   * HBaseTestingUtility objects will behave as local until a DFS cluster is started,\n   * at which point they will switch to using mini DFS for storage.\n   *\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtility(@Nullable Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // if conf is provided, prevent contention for ports if other hbase thread(s) are running\n    if (conf != null) {\n      if (conf.getInt(HConstants.MASTER_INFO_PORT, HConstants.DEFAULT_MASTER_INFOPORT)\n              == HConstants.DEFAULT_MASTER_INFOPORT) {\n        conf.setInt(HConstants.MASTER_INFO_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.MASTER_INFO_PORT);\n      }\n      if (conf.getInt(HConstants.REGIONSERVER_PORT, HConstants.DEFAULT_REGIONSERVER_PORT)\n              == HConstants.DEFAULT_REGIONSERVER_PORT) {\n        conf.setInt(HConstants.REGIONSERVER_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.REGIONSERVER_PORT);\n      }\n    }\n\n    // Save this for when setting default file:// breaks things\n    this.conf.set(\"original.defaultFS\", this.conf.get(\"fs.defaultFS\"));\n\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\",\"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE,false);\n  }"
        ]
    ],
    "c1ada0a373561132a3359b48a27975b2e85978da": [
        [
            "MiniHBaseCluster::MiniHBaseCluster(Configuration,int,int,Class,Class)",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "  public MiniHBaseCluster(Configuration conf, int numMasters, int numRegionServers,\n         Class<? extends HMaster> masterClass,\n         Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> regionserverClass)\n      throws IOException, InterruptedException {\n    super(conf);\n    conf.set(HConstants.MASTER_PORT, \"0\");\n\n    // Hadoop 2\n    CompatibilityFactory.getInstance(MetricsAssertHelper.class).init();\n\n    init(numMasters, numRegionServers, masterClass, regionserverClass);\n    this.initialClusterStatus = getClusterStatus();\n  }",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "  public MiniHBaseCluster(Configuration conf, int numMasters, int numRegionServers,\n         Class<? extends HMaster> masterClass,\n         Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> regionserverClass)\n      throws IOException, InterruptedException {\n    super(conf);\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    if (conf.getInt(HConstants.MASTER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.MASTER_INFO_PORT, \"0\");\n    }\n\n    // Hadoop 2\n    CompatibilityFactory.getInstance(MetricsAssertHelper.class).init();\n\n    init(numMasters, numRegionServers, masterClass, regionserverClass);\n    this.initialClusterStatus = getClusterStatus();\n  }"
        ],
        [
            "LocalHBaseCluster::LocalHBaseCluster(Configuration,int,int,Class,Class)",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148 -\n 149 -\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  /**\n   * Constructor.\n   * @param conf Configuration to use.  Post construction has the master's\n   * address.\n   * @param noMasters Count of masters to start.\n   * @param noRegionServers Count of regionservers to start.\n   * @param masterClass\n   * @param regionServerClass\n   * @throws IOException\n   */\n  @SuppressWarnings(\"unchecked\")\n  public LocalHBaseCluster(final Configuration conf, final int noMasters,\n    final int noRegionServers, final Class<? extends HMaster> masterClass,\n    final Class<? extends HRegionServer> regionServerClass)\n  throws IOException {\n    this.conf = conf;\n\n    // Always have masters and regionservers come up on port '0' so we don't\n    // clash over default ports.\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    if (conf.getInt(HConstants.MASTER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.MASTER_INFO_PORT, \"0\");\n    }\n    conf.set(HConstants.REGIONSERVER_PORT, \"0\");\n    if (conf.getInt(HConstants.REGIONSERVER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.REGIONSERVER_INFO_PORT, \"0\");\n    }\n\n    this.masterClass = (Class<? extends HMaster>)\n      conf.getClass(HConstants.MASTER_IMPL, masterClass);\n    // Start the HMasters.\n    for (int i = 0; i < noMasters; i++) {\n      addMaster(new Configuration(conf), i);\n    }\n    // Start the HRegionServers.\n    this.regionServerClass =\n      (Class<? extends HRegionServer>)conf.getClass(HConstants.REGION_SERVER_IMPL,\n       regionServerClass);\n\n    for (int i = 0; i < noRegionServers; i++) {\n      addRegionServer(new Configuration(conf), i);\n    }\n  }",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  /**\n   * Constructor.\n   * @param conf Configuration to use.  Post construction has the master's\n   * address.\n   * @param noMasters Count of masters to start.\n   * @param noRegionServers Count of regionservers to start.\n   * @param masterClass\n   * @param regionServerClass\n   * @throws IOException\n   */\n  @SuppressWarnings(\"unchecked\")\n  public LocalHBaseCluster(final Configuration conf, final int noMasters,\n    final int noRegionServers, final Class<? extends HMaster> masterClass,\n    final Class<? extends HRegionServer> regionServerClass)\n  throws IOException {\n    this.conf = conf;\n\n    // Always have masters and regionservers come up on port '0' so we don't\n    // clash over default ports.\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    conf.set(HConstants.REGIONSERVER_PORT, \"0\");\n    if (conf.getInt(HConstants.REGIONSERVER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.REGIONSERVER_INFO_PORT, \"0\");\n    }\n\n    this.masterClass = (Class<? extends HMaster>)\n      conf.getClass(HConstants.MASTER_IMPL, masterClass);\n    // Start the HMasters.\n    for (int i = 0; i < noMasters; i++) {\n      addMaster(new Configuration(conf), i);\n    }\n    // Start the HRegionServers.\n    this.regionServerClass =\n      (Class<? extends HRegionServer>)conf.getClass(HConstants.REGION_SERVER_IMPL,\n       regionServerClass);\n\n    for (int i = 0; i < noRegionServers; i++) {\n      addRegionServer(new Configuration(conf), i);\n    }\n  }"
        ]
    ],
    "66781864aaf78e8c8afb0978a7f68b6773d69649": [
        [
            "SnapshotFileCache::refreshCache()",
            " 207  \n 208 -\n 209 -\n 210 -\n 211  \n 212  \n 213 -\n 214 -\n 215 -\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242 -\n 243  \n 244 -\n 245 -\n 246 -\n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "  private synchronized void refreshCache() throws IOException {\n    long lastTimestamp = Long.MAX_VALUE;\n    boolean hasChanges = false;\n\n    // get the status of the snapshots directory and check if it is has changes\n    try {\n      FileStatus dirStatus = fs.getFileStatus(snapshotDir);\n      lastTimestamp = dirStatus.getModificationTime();\n      hasChanges |= (lastTimestamp >= lastModifiedTime);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // get the status of the snapshots temporary directory and check if it has changes\n    // The top-level directory timestamp is not updated, so we have to check the inner-level.\n    try {\n      Path snapshotTmpDir = new Path(snapshotDir, SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME);\n      FileStatus tempDirStatus = fs.getFileStatus(snapshotTmpDir);\n      lastTimestamp = Math.min(lastTimestamp, tempDirStatus.getModificationTime());\n      hasChanges |= (lastTimestamp >= lastModifiedTime);\n      if (!hasChanges) {\n        FileStatus[] tmpSnapshots = FSUtils.listStatus(fs, snapshotDir);\n        if (tmpSnapshots != null) {\n          for (FileStatus dirStatus: tmpSnapshots) {\n            lastTimestamp = Math.min(lastTimestamp, dirStatus.getModificationTime());\n          }\n          hasChanges |= (lastTimestamp >= lastModifiedTime);\n        }\n      }\n    } catch (FileNotFoundException e) {\n      // Nothing todo, if the tmp dir is empty\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (!hasChanges) {\n      return;\n    }\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = lastTimestamp;\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<String, SnapshotDirectoryInfo>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }",
            " 207  \n 208  \n 209 +\n 210  \n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  private synchronized void refreshCache() throws IOException {\n    // get the status of the snapshots directory and check if it is has changes\n    FileStatus dirStatus;\n    try {\n      dirStatus = fs.getFileStatus(snapshotDir);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (dirStatus.getModificationTime() <= this.lastModifiedTime) return;\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = dirStatus.getModificationTime();\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<String, SnapshotDirectoryInfo>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }"
        ]
    ],
    "287f95a579ee95a40e0f3a0986a246d29718ee3b": [
        [
            "HRegionFileSystem::setStoragePolicy(String,String)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 -\n 200  \n 201  ",
            "  /**\n   * Set the directory of CF to the specified storage policy. <br>\n   * <i>\"LAZY_PERSIST\"</i>, <i>\"ALL_SSD\"</i>, <i>\"ONE_SSD\"</i>, <i>\"HOT\"</i>, <i>\"WARM\"</i>,\n   * <i>\"COLD\"</i> <br>\n   * <br>\n   * See {@link org.apache.hadoop.hdfs.protocol.HdfsConstants} for more details.\n   * @param familyName The name of column family.\n   * @param policyName The name of the storage policy.\n   */\n  public void setStoragePolicy(String familyName, String policyName) {\n    Path storeDir = getStoreDir(familyName);\n    try {\n      ReflectionUtils.invokeMethod(this.fs, \"setStoragePolicy\", storeDir, policyName);\n    } catch (Exception e) {\n      LOG.warn(\"Failed to set storage policy of [\" + storeDir + \"] to [\" + policyName + \"]\", e);\n    }\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203  \n 204  ",
            "  /**\n   * Set the directory of CF to the specified storage policy. <br>\n   * <i>\"LAZY_PERSIST\"</i>, <i>\"ALL_SSD\"</i>, <i>\"ONE_SSD\"</i>, <i>\"HOT\"</i>, <i>\"WARM\"</i>,\n   * <i>\"COLD\"</i> <br>\n   * <br>\n   * See {@link org.apache.hadoop.hdfs.protocol.HdfsConstants} for more details.\n   * @param familyName The name of column family.\n   * @param policyName The name of the storage policy.\n   */\n  public void setStoragePolicy(String familyName, String policyName) {\n    Path storeDir = getStoreDir(familyName);\n    try {\n      ReflectionUtils.invokeMethod(this.fs, \"setStoragePolicy\", storeDir, policyName);\n    } catch (Exception e) {\n      if (!(this.fs instanceof LocalFileSystem)) {\n        LOG.warn(\"Failed to set storage policy of [\" + storeDir + \"] to [\" + policyName + \"]\", e);\n      }\n    }\n  }"
        ]
    ],
    "000a84cb4afce33e6d6601f65955f2de9924cf8a": [
        [
            "BackupLogCleaner::init(Map)",
            "  56  \n  57  \n  58 -\n  59 -\n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "  @Override\n  public void init(Map<String, Object> params) {\n    if (params != null && params.containsKey(HMaster.MASTER)) {\n      MasterServices master = (MasterServices) params.get(HMaster.MASTER);\n      conn = master.getConnection();\n      if (getConf() == null) {\n        super.setConf(conn.getConfiguration());\n      }\n    }\n    if (conn == null) {\n      try {\n        conn = ConnectionFactory.createConnection(getConf());\n      } catch (IOException ioe) {\n        throw new RuntimeException(\"Failed to create connection\", ioe);\n      }\n    }\n  }",
            "  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  ",
            "  @Override\n  public void init(Map<String, Object> params) {\n    MasterServices master = (MasterServices) MapUtils.getObject(params,\n      HMaster.MASTER);\n    if (master != null) {\n      conn = master.getConnection();\n      if (getConf() == null) {\n        super.setConf(conn.getConfiguration());\n      }\n    }\n    if (conn == null) {\n      try {\n        conn = ConnectionFactory.createConnection(getConf());\n      } catch (IOException ioe) {\n        throw new RuntimeException(\"Failed to create connection\", ioe);\n      }\n    }\n  }"
        ],
        [
            "BackupLogCleaner::stop(String)",
            " 133  \n 134  \n 135 -\n 136 -\n 137  \n 138 -\n 139 -\n 140  ",
            "  @Override\n  public void stop(String why) {\n    if (this.stopped) {\n      return;\n    }\n    this.stopped = true;\n    LOG.info(\"Stopping BackupLogCleaner\");\n  }",
            " 131  \n 132  \n 133 +\n 134 +\n 135 +\n 136  \n 137  ",
            "  @Override\n  public void stop(String why) {\n    if (!this.stopped) {\n      this.stopped = true;\n      LOG.info(\"Stopping BackupLogCleaner\");\n    }\n  }"
        ],
        [
            "BackupLogCleaner::getDeletableFiles(Iterable)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80 -\n  81 -\n  82 -\n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107 -\n 108  \n 109  \n 110 -\n 111 -\n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  ",
            "  @Override\n  public Iterable<FileStatus> getDeletableFiles(Iterable<FileStatus> files) {\n    // all members of this class are null if backup is disabled,\n    // so we cannot filter the files\n    if (this.getConf() == null || !BackupManager.isBackupEnabled(getConf())) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Backup is not enabled. Check your \"\n            + BackupRestoreConstants.BACKUP_ENABLE_KEY + \" setting\");\n      }\n      return files;\n    }\n\n    List<FileStatus> list = new ArrayList<>();\n    try (final BackupSystemTable table = new BackupSystemTable(conn)) {\n      // If we do not have recorded backup sessions\n      try {\n        if (!table.hasBackupSessions()) {\n          LOG.trace(\"BackupLogCleaner has no backup sessions\");\n          return files;\n        }\n      } catch (TableNotFoundException tnfe) {\n        LOG.warn(\"backup system table is not available\" + tnfe.getMessage());\n        return files;\n      }\n\n      Map<FileStatus, Boolean> walFilesDeletableMap = table.areWALFilesDeletable(files);\n      for (Map.Entry<FileStatus, Boolean> entry: walFilesDeletableMap.entrySet()) {\n        FileStatus file = entry.getKey();\n        String wal = file.getPath().toString();\n        boolean deletable = entry.getValue();\n        if (deletable) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found log file in backup system table, deleting: \" + wal);\n          }\n          list.add(file);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Didn't find this log in backup system table, keeping: \" + wal);\n          }\n        }\n      }\n      return list;\n    } catch (IOException e) {\n      LOG.error(\"Failed to get backup system table table, therefore will keep all files\", e);\n      // nothing to delete\n      return new ArrayList<>();\n    }\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  ",
            "  @Override\n  public Iterable<FileStatus> getDeletableFiles(Iterable<FileStatus> files) {\n    // all members of this class are null if backup is disabled,\n    // so we cannot filter the files\n    if (this.getConf() == null || !BackupManager.isBackupEnabled(getConf())) {\n      LOG.debug(\"Backup is not enabled. Check your {} setting\",\n          BackupRestoreConstants.BACKUP_ENABLE_KEY);\n      return files;\n    }\n\n    try (final BackupSystemTable table = new BackupSystemTable(conn)) {\n      // If we do not have recorded backup sessions\n      try {\n        if (!table.hasBackupSessions()) {\n          LOG.trace(\"BackupLogCleaner has no backup sessions\");\n          return files;\n        }\n      } catch (TableNotFoundException tnfe) {\n        LOG.warn(\"Backup system table is not available: {}\", tnfe.getMessage());\n        return files;\n      }\n\n      List<FileStatus> list = new ArrayList<>();\n      Map<FileStatus, Boolean> walFilesDeletableMap = table.areWALFilesDeletable(files);\n      for (Map.Entry<FileStatus, Boolean> entry: walFilesDeletableMap.entrySet()) {\n        FileStatus file = entry.getKey();\n        String wal = file.getPath().toString();\n        boolean deletable = entry.getValue();\n        if (deletable) {\n          LOG.debug(\"Found log file in backup system table, deleting: {}\", wal);\n          list.add(file);\n        } else {\n          LOG.debug(\"Did not find this log in backup system table, keeping: {}\", wal);\n        }\n      }\n      return list;\n    } catch (IOException e) {\n      LOG.error(\"Failed to get backup system table table, therefore will keep all files\", e);\n      // nothing to delete\n      return Collections.emptyList();\n    }\n  }"
        ]
    ],
    "3c0750de54276cf19a35feb6351c71b8dea360b7": [
        [
            "HBaseInterClusterReplicationEndpoint::replicate(ReplicateContext)",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  ",
            "  /**\n   * Do the shipping logic\n   */\n  @Override\n  public boolean replicate(ReplicateContext replicateContext) {\n    CompletionService<Integer> pool = new ExecutorCompletionService<Integer>(this.exec);\n    List<Entry> entries = replicateContext.getEntries();\n    String walGroupId = replicateContext.getWalGroupId();\n    int sleepMultiplier = 1;\n    int numReplicated = 0;\n\n    if (!peersSelected && this.isRunning()) {\n      connectToPeers();\n      peersSelected = true;\n    }\n\n    int numSinks = replicationSinkMgr.getNumSinks();\n    if (numSinks == 0) {\n      LOG.warn(\"No replication sinks found, returning without replicating. The source should retry\"\n          + \" with the same set of edits.\");\n      return false;\n    }\n\n    // minimum of: configured threads, number of 100-waledit batches,\n    //  and number of current sinks\n    int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);\n\n    List<List<Entry>> entryLists = new ArrayList<List<Entry>>(n);\n    if (n == 1) {\n      entryLists.add(entries);\n    } else {\n      for (int i=0; i<n; i++) {\n        entryLists.add(new ArrayList<Entry>(entries.size()/n+1));\n      }\n      // now group by region\n      for (Entry e : entries) {\n        entryLists.get(Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n)).add(e);\n      }\n    }\n    while (this.isRunning() && !exec.isShutdown()) {\n      if (!isPeerEnabled()) {\n        if (sleepForRetries(\"Replication is disabled\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n        continue;\n      }\n      try {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Replicating \" + entries.size() +\n              \" entries of total size \" + replicateContext.getSize());\n        }\n\n        int futures = 0;\n        for (int i=0; i<entryLists.size(); i++) {\n          if (!entryLists.get(i).isEmpty()) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Submitting \" + entryLists.get(i).size() +\n                  \" entries of total size \" + replicateContext.getSize());\n            }\n            // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource\n            pool.submit(createReplicator(entryLists.get(i), i));\n            futures++;\n          }\n        }\n        IOException iox = null;\n\n        for (int i=0; i<futures; i++) {\n          try {\n            // wait for all futures, remove successful parts\n            // (only the remaining parts will be retried)\n            Future<Integer> f = pool.take();\n            int index = f.get().intValue();\n            int batchSize =  entryLists.get(index).size();\n            entryLists.set(index, Collections.<Entry>emptyList());\n            // Now, we have marked the batch as done replicating, record its size\n            numReplicated += batchSize;\n          } catch (InterruptedException ie) {\n            iox =  new IOException(ie);\n          } catch (ExecutionException ee) {\n            // cause must be an IOException\n            iox = (IOException)ee.getCause();\n          }\n        }\n        if (iox != null) {\n          // if we had any exceptions, try again\n          throw iox;\n        }\n        if (numReplicated != entries.size()) {\n          // Something went wrong here and we don't know what, let's just fail and retry.\n          LOG.warn(\"The number of edits replicated is different from the number received,\"\n              + \" failing for now.\");\n          return false;\n        }\n        // update metrics\n        this.metrics.setAgeOfLastShippedOp(entries.get(entries.size() - 1).getKey().getWriteTime(),\n          walGroupId);\n        return true;\n\n      } catch (IOException ioe) {\n        // Didn't ship anything, but must still age the last time we did\n        this.metrics.refreshAgeOfLastShippedOp(walGroupId);\n        if (ioe instanceof RemoteException) {\n          ioe = ((RemoteException) ioe).unwrapRemoteException();\n          LOG.warn(\"Can't replicate because of an error on the remote cluster: \", ioe);\n          if (ioe instanceof TableNotFoundException) {\n            if (sleepForRetries(\"A table is missing in the peer cluster. \"\n                + \"Replication cannot proceed without losing data.\", sleepMultiplier)) {\n              sleepMultiplier++;\n            }\n          }\n        } else {\n          if (ioe instanceof SocketTimeoutException) {\n            // This exception means we waited for more than 60s and nothing\n            // happened, the cluster is alive and calling it right away\n            // even for a test just makes things worse.\n            sleepForRetries(\"Encountered a SocketTimeoutException. Since the \" +\n              \"call to the remote cluster timed out, which is usually \" +\n              \"caused by a machine failure or a massive slowdown\",\n              this.socketTimeoutMultiplier);\n          } else if (ioe instanceof ConnectException) {\n            LOG.warn(\"Peer is unavailable, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          } else {\n            LOG.warn(\"Can't replicate because of a local or network error: \", ioe);\n          }\n        }\n        if (sleepForRetries(\"Since we are unable to replicate\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n      }\n    }\n    return false; // in case we exited before replicating\n  }",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 +\n 298 +\n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "  /**\n   * Do the shipping logic\n   */\n  @Override\n  public boolean replicate(ReplicateContext replicateContext) {\n    CompletionService<Integer> pool = new ExecutorCompletionService<Integer>(this.exec);\n    List<Entry> entries = replicateContext.getEntries();\n    String walGroupId = replicateContext.getWalGroupId();\n    int sleepMultiplier = 1;\n    int numReplicated = 0;\n\n    if (!peersSelected && this.isRunning()) {\n      connectToPeers();\n      peersSelected = true;\n    }\n\n    int numSinks = replicationSinkMgr.getNumSinks();\n    if (numSinks == 0) {\n      LOG.warn(\"No replication sinks found, returning without replicating. The source should retry\"\n          + \" with the same set of edits.\");\n      return false;\n    }\n\n    // minimum of: configured threads, number of 100-waledit batches,\n    //  and number of current sinks\n    int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);\n\n    List<List<Entry>> entryLists = new ArrayList<List<Entry>>(n);\n    if (n == 1) {\n      entryLists.add(entries);\n    } else {\n      for (int i=0; i<n; i++) {\n        entryLists.add(new ArrayList<Entry>(entries.size()/n+1));\n      }\n      // now group by region\n      for (Entry e : entries) {\n        entryLists.get(Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n)).add(e);\n      }\n    }\n    while (this.isRunning() && !exec.isShutdown()) {\n      if (!isPeerEnabled()) {\n        if (sleepForRetries(\"Replication is disabled\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n        continue;\n      }\n      try {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Replicating \" + entries.size() +\n              \" entries of total size \" + replicateContext.getSize());\n        }\n\n        int futures = 0;\n        for (int i=0; i<entryLists.size(); i++) {\n          if (!entryLists.get(i).isEmpty()) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Submitting \" + entryLists.get(i).size() +\n                  \" entries of total size \" + replicateContext.getSize());\n            }\n            // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource\n            pool.submit(createReplicator(entryLists.get(i), i));\n            futures++;\n          }\n        }\n        IOException iox = null;\n\n        for (int i=0; i<futures; i++) {\n          try {\n            // wait for all futures, remove successful parts\n            // (only the remaining parts will be retried)\n            Future<Integer> f = pool.take();\n            int index = f.get().intValue();\n            int batchSize =  entryLists.get(index).size();\n            entryLists.set(index, Collections.<Entry>emptyList());\n            // Now, we have marked the batch as done replicating, record its size\n            numReplicated += batchSize;\n          } catch (InterruptedException ie) {\n            iox =  new IOException(ie);\n          } catch (ExecutionException ee) {\n            // cause must be an IOException\n            iox = (IOException)ee.getCause();\n          }\n        }\n        if (iox != null) {\n          // if we had any exceptions, try again\n          throw iox;\n        }\n        if (numReplicated != entries.size()) {\n          // Something went wrong here and we don't know what, let's just fail and retry.\n          LOG.warn(\"The number of edits replicated is different from the number received,\"\n              + \" failing for now.\");\n          return false;\n        }\n        // update metrics\n        this.metrics.setAgeOfLastShippedOp(entries.get(entries.size() - 1).getKey().getWriteTime(),\n          walGroupId);\n        return true;\n\n      } catch (IOException ioe) {\n        // Didn't ship anything, but must still age the last time we did\n        this.metrics.refreshAgeOfLastShippedOp(walGroupId);\n        if (ioe instanceof RemoteException) {\n          ioe = ((RemoteException) ioe).unwrapRemoteException();\n          LOG.warn(\"Can't replicate because of an error on the remote cluster: \", ioe);\n          if (ioe instanceof TableNotFoundException) {\n            if (sleepForRetries(\"A table is missing in the peer cluster. \"\n                + \"Replication cannot proceed without losing data.\", sleepMultiplier)) {\n              sleepMultiplier++;\n            }\n          } else if (ioe instanceof SaslException) {\n            LOG.warn(\"Peer encountered SaslException, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          }\n        } else {\n          if (ioe instanceof SocketTimeoutException) {\n            // This exception means we waited for more than 60s and nothing\n            // happened, the cluster is alive and calling it right away\n            // even for a test just makes things worse.\n            sleepForRetries(\"Encountered a SocketTimeoutException. Since the \" +\n              \"call to the remote cluster timed out, which is usually \" +\n              \"caused by a machine failure or a massive slowdown\",\n              this.socketTimeoutMultiplier);\n          } else if (ioe instanceof ConnectException) {\n            LOG.warn(\"Peer is unavailable, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          } else {\n            LOG.warn(\"Can't replicate because of a local or network error: \", ioe);\n          }\n        }\n        if (sleepForRetries(\"Since we are unable to replicate\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n      }\n    }\n    return false; // in case we exited before replicating\n  }"
        ]
    ],
    "43a8ac00158e92c3015af7753edd8e835dc6054b": [
        [
            "MemStoreSize::equals(Object)",
            "  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || !(obj instanceof MemStoreSize)) {\n      return false;\n    }\n    MemStoreSize other = (MemStoreSize) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }",
            "  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || getClass() != obj.getClass()) {\n      return false;\n    }\n    MemStoreSize other = (MemStoreSize) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }"
        ],
        [
            "MemStoreSizing::equals(Object)",
            "  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || !(obj instanceof MemStoreSizing)) {\n      return false;\n    }\n    MemStoreSizing other = (MemStoreSizing) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }",
            "  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || (getClass() != obj.getClass())) {\n      return false;\n    }\n    MemStoreSizing other = (MemStoreSizing) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }"
        ]
    ],
    "104afd74a664d90fe8e3aa57b0722ab04908525c": [
        [
            "NamespaceExistException::NamespaceExistException(String)",
            "  34  \n  35 -\n  36  ",
            "  public NamespaceExistException(String namespace) {\n    super(namespace);\n  }",
            "  34  \n  35 +\n  36  ",
            "  public NamespaceExistException(String namespace) {\n    super(\"Namespace \" + namespace + \" already exists\");\n  }"
        ]
    ],
    "69431c75c16d8d863932815f0460322153a25dbb": [
        [
            "BaseLoadBalancer::retainAssignment(Map,List)",
            "1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460 -\n1461 -\n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  ",
            "  /**\n   * Generates a bulk assignment startup plan, attempting to reuse the existing\n   * assignment information from META, but adjusting for the specified list of\n   * available/online servers available for assignment.\n   * <p>\n   * Takes a map of all regions to their existing assignment from META. Also\n   * takes a list of online servers for regions to be assigned to. Attempts to\n   * retain all assignment, so in some instances initial assignment will not be\n   * completely balanced.\n   * <p>\n   * Any leftover regions without an existing server to be assigned to will be\n   * assigned randomly to available servers.\n   *\n   * @param regions regions and existing assignment from meta\n   * @param servers available servers\n   * @return map of servers and regions to be assigned to them\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    // Update metrics\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions.keySet(), servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      regions = regions.entrySet().stream().filter(e -> !masterRegions.contains(e.getKey()))\n          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n    }\n    if (regions.isEmpty()) {\n      return assignments;\n    }\n\n    int numServers = servers == null ? 0 : servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to do retain assignment but no servers to assign to\");\n      return null;\n    }\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions.keySet()));\n      return assignments;\n    }\n\n    // Group all of the old assignments by their hostname.\n    // We can't group directly by ServerName since the servers all have\n    // new start-codes.\n\n    // Group the servers by their hostname. It's possible we have multiple\n    // servers on the same host on different ports.\n    ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();\n    for (ServerName server : servers) {\n      assignments.put(server, new ArrayList<>());\n      serversByHostname.put(server.getHostnameLowerCase(), server);\n    }\n\n    // Collection of the hostnames that used to have regions\n    // assigned, but for which we no longer have any RS running\n    // after the cluster restart.\n    Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();\n\n    // If the old servers aren't present, lets assign those regions later.\n    List<RegionInfo> randomAssignRegions = Lists.newArrayList();\n\n    int numRandomAssignments = 0;\n    int numRetainedAssigments = 0;\n    boolean hasRegionReplica = false;\n    for (Map.Entry<RegionInfo, ServerName> entry : regions.entrySet()) {\n      RegionInfo region = entry.getKey();\n      ServerName oldServerName = entry.getValue();\n      // In the current set of regions even if one has region replica let us go with\n      // getting the entire snapshot\n      if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n        if (!hasRegionReplica && this.services.getAssignmentManager().getRegionStates()\n            .isReplicaAvailableForRegion(region)) {\n          hasRegionReplica = true;\n        }\n      }\n      List<ServerName> localServers = new ArrayList<>();\n      if (oldServerName != null) {\n        localServers = serversByHostname.get(oldServerName.getHostnameLowerCase());\n      }\n      if (localServers.isEmpty()) {\n        // No servers on the new cluster match up with this hostname, assign randomly, later.\n        randomAssignRegions.add(region);\n        if (oldServerName != null) {\n          oldHostsNoLongerPresent.add(oldServerName.getHostnameLowerCase());\n        }\n      } else if (localServers.size() == 1) {\n        // the usual case - one new server on same host\n        ServerName target = localServers.get(0);\n        assignments.get(target).add(region);\n        numRetainedAssigments++;\n      } else {\n        // multiple new servers in the cluster on this same host\n        if (localServers.contains(oldServerName)) {\n          assignments.get(oldServerName).add(region);\n          numRetainedAssigments++;\n        } else {\n          ServerName target = null;\n          for (ServerName tmp : localServers) {\n            if (tmp.getPort() == oldServerName.getPort()) {\n              target = tmp;\n              assignments.get(tmp).add(region);\n              numRetainedAssigments++;\n              break;\n            }\n          }\n          if (target == null) {\n            randomAssignRegions.add(region);\n          }\n        }\n      }\n    }\n\n    // If servers from prior assignment aren't present, then lets do randomAssignment on regions.\n    if (randomAssignRegions.size() > 0) {\n      Cluster cluster = createCluster(servers, regions.keySet(), hasRegionReplica);\n      for (Map.Entry<ServerName, List<RegionInfo>> entry : assignments.entrySet()) {\n        ServerName sn = entry.getKey();\n        for (RegionInfo region : entry.getValue()) {\n          cluster.doAssignRegion(region, sn);\n        }\n      }\n      for (RegionInfo region : randomAssignRegions) {\n        ServerName target = randomAssignment(cluster, region, servers);\n        assignments.get(target).add(region);\n        numRandomAssignments++;\n      }\n    }\n\n    String randomAssignMsg = \"\";\n    if (numRandomAssignments > 0) {\n      randomAssignMsg =\n          numRandomAssignments + \" regions were assigned \"\n              + \"to random hosts, since the old hosts for these regions are no \"\n              + \"longer present in the cluster. These hosts were:\\n  \"\n              + Joiner.on(\"\\n  \").join(oldHostsNoLongerPresent);\n    }\n\n    LOG.info(\"Reassigned \" + regions.size() + \" regions. \" + numRetainedAssigments\n        + \" retained the pre-restart assignment. \" + randomAssignMsg);\n    return assignments;\n  }",
            "1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461 +\n1462 +\n1463 +\n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  ",
            "  /**\n   * Generates a bulk assignment startup plan, attempting to reuse the existing\n   * assignment information from META, but adjusting for the specified list of\n   * available/online servers available for assignment.\n   * <p>\n   * Takes a map of all regions to their existing assignment from META. Also\n   * takes a list of online servers for regions to be assigned to. Attempts to\n   * retain all assignment, so in some instances initial assignment will not be\n   * completely balanced.\n   * <p>\n   * Any leftover regions without an existing server to be assigned to will be\n   * assigned randomly to available servers.\n   *\n   * @param regions regions and existing assignment from meta\n   * @param servers available servers\n   * @return map of servers and regions to be assigned to them\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    // Update metrics\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions.keySet(), servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      regions = regions.entrySet().stream().filter(e -> !masterRegions.contains(e.getKey()))\n          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n    }\n    if (regions.isEmpty()) {\n      return assignments;\n    }\n\n    int numServers = servers == null ? 0 : servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to do retain assignment but no servers to assign to\");\n      return null;\n    }\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions.keySet()));\n      return assignments;\n    }\n\n    // Group all of the old assignments by their hostname.\n    // We can't group directly by ServerName since the servers all have\n    // new start-codes.\n\n    // Group the servers by their hostname. It's possible we have multiple\n    // servers on the same host on different ports.\n    ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();\n    for (ServerName server : servers) {\n      assignments.put(server, new ArrayList<>());\n      serversByHostname.put(server.getHostnameLowerCase(), server);\n    }\n\n    // Collection of the hostnames that used to have regions\n    // assigned, but for which we no longer have any RS running\n    // after the cluster restart.\n    Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();\n\n    // If the old servers aren't present, lets assign those regions later.\n    List<RegionInfo> randomAssignRegions = Lists.newArrayList();\n\n    int numRandomAssignments = 0;\n    int numRetainedAssigments = 0;\n    boolean hasRegionReplica = false;\n    for (Map.Entry<RegionInfo, ServerName> entry : regions.entrySet()) {\n      RegionInfo region = entry.getKey();\n      ServerName oldServerName = entry.getValue();\n      // In the current set of regions even if one has region replica let us go with\n      // getting the entire snapshot\n      if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n        RegionStates states = this.services.getAssignmentManager().getRegionStates();\n        if (!hasRegionReplica && states != null &&\n            states.isReplicaAvailableForRegion(region)) {\n          hasRegionReplica = true;\n        }\n      }\n      List<ServerName> localServers = new ArrayList<>();\n      if (oldServerName != null) {\n        localServers = serversByHostname.get(oldServerName.getHostnameLowerCase());\n      }\n      if (localServers.isEmpty()) {\n        // No servers on the new cluster match up with this hostname, assign randomly, later.\n        randomAssignRegions.add(region);\n        if (oldServerName != null) {\n          oldHostsNoLongerPresent.add(oldServerName.getHostnameLowerCase());\n        }\n      } else if (localServers.size() == 1) {\n        // the usual case - one new server on same host\n        ServerName target = localServers.get(0);\n        assignments.get(target).add(region);\n        numRetainedAssigments++;\n      } else {\n        // multiple new servers in the cluster on this same host\n        if (localServers.contains(oldServerName)) {\n          assignments.get(oldServerName).add(region);\n          numRetainedAssigments++;\n        } else {\n          ServerName target = null;\n          for (ServerName tmp : localServers) {\n            if (tmp.getPort() == oldServerName.getPort()) {\n              target = tmp;\n              assignments.get(tmp).add(region);\n              numRetainedAssigments++;\n              break;\n            }\n          }\n          if (target == null) {\n            randomAssignRegions.add(region);\n          }\n        }\n      }\n    }\n\n    // If servers from prior assignment aren't present, then lets do randomAssignment on regions.\n    if (randomAssignRegions.size() > 0) {\n      Cluster cluster = createCluster(servers, regions.keySet(), hasRegionReplica);\n      for (Map.Entry<ServerName, List<RegionInfo>> entry : assignments.entrySet()) {\n        ServerName sn = entry.getKey();\n        for (RegionInfo region : entry.getValue()) {\n          cluster.doAssignRegion(region, sn);\n        }\n      }\n      for (RegionInfo region : randomAssignRegions) {\n        ServerName target = randomAssignment(cluster, region, servers);\n        assignments.get(target).add(region);\n        numRandomAssignments++;\n      }\n    }\n\n    String randomAssignMsg = \"\";\n    if (numRandomAssignments > 0) {\n      randomAssignMsg =\n          numRandomAssignments + \" regions were assigned \"\n              + \"to random hosts, since the old hosts for these regions are no \"\n              + \"longer present in the cluster. These hosts were:\\n  \"\n              + Joiner.on(\"\\n  \").join(oldHostsNoLongerPresent);\n    }\n\n    LOG.info(\"Reassigned \" + regions.size() + \" regions. \" + numRetainedAssigments\n        + \" retained the pre-restart assignment. \" + randomAssignMsg);\n    return assignments;\n  }"
        ]
    ],
    "4489598a8e4cd920d2fa36d7f84f195d8aa40736": [
        [
            "SnapshotFileCache::RefreshCacheTask::run()",
            " 284  \n 285  \n 286 -\n 287 -\n 288 -\n 289 -\n 290  \n 291  ",
            "    @Override\n    public void run() {\n      try {\n        SnapshotFileCache.this.refreshCache();\n      } catch (IOException e) {\n        LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n      }\n    }",
            " 258  \n 259  \n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269  \n 270 +\n 271  ",
            "    @Override\n    public void run() {\n      synchronized (SnapshotFileCache.this) {\n        try {\n          SnapshotFileCache.this.refreshCache();\n        } catch (IOException e) {\n          LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n          // clear all the cached entries if we meet an error\n          cache.clear();\n          snapshots.clear();\n        }\n      }\n\n    }"
        ],
        [
            "SnapshotFileCache::SnapshotFileCache(FileSystem,Path,long,long,String,SnapshotFileInspector)",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem\n   * @param fs {@link FileSystem} where the snapshots are stored\n   * @param rootDir hbase root directory\n   * @param cacheRefreshPeriod period (ms) with which the cache should be refreshed\n   * @param cacheRefreshDelay amount of time to wait for the cache to be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   */\n  public SnapshotFileCache(FileSystem fs, Path rootDir, long cacheRefreshPeriod,\n      long cacheRefreshDelay, String refreshThreadName, SnapshotFileInspector inspectSnapshotFiles) {\n    this.fs = fs;\n    this.fileInspector = inspectSnapshotFiles;\n    this.snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);\n    // periodically refresh the file cache to make sure we aren't superfluously saving files.\n    this.refreshTimer = new Timer(refreshThreadName, true);\n    this.refreshTimer.scheduleAtFixedRate(new RefreshCacheTask(), cacheRefreshDelay,\n      cacheRefreshPeriod);\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem\n   * @param fs {@link FileSystem} where the snapshots are stored\n   * @param rootDir hbase root directory\n   * @param cacheRefreshPeriod period (ms) with which the cache should be refreshed\n   * @param cacheRefreshDelay amount of time to wait for the cache to be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   */\n  public SnapshotFileCache(FileSystem fs, Path rootDir, long cacheRefreshPeriod,\n      long cacheRefreshDelay, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) {\n    this.fs = fs;\n    this.fileInspector = inspectSnapshotFiles;\n    this.snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);\n    // periodically refresh the file cache to make sure we aren't superfluously saving files.\n    this.refreshTimer = new Timer(refreshThreadName, true);\n    this.refreshTimer.scheduleAtFixedRate(new RefreshCacheTask(), cacheRefreshDelay,\n      cacheRefreshPeriod);\n  }"
        ],
        [
            "SnapshotFileCache::triggerCacheRefreshForTesting()",
            " 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  ",
            "  /**\n   * Trigger a cache refresh, even if its before the next cache refresh. Does not affect pending\n   * cache refreshes.\n   * <p>\n   * Blocks until the cache is refreshed.\n   * <p>\n   * Exposed for TESTING.\n   */\n  public void triggerCacheRefreshForTesting() {\n    try {\n      SnapshotFileCache.this.refreshCache();\n    } catch (IOException e) {\n      LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n    }\n    LOG.debug(\"Current cache:\" + cache);\n  }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Trigger a cache refresh, even if its before the next cache refresh. Does not affect pending\n   * cache refreshes.\n   * <p/>\n   * Blocks until the cache is refreshed.\n   * <p/>\n   * Exposed for TESTING.\n   */\n  public synchronized void triggerCacheRefreshForTesting() {\n    try {\n      refreshCache();\n    } catch (IOException e) {\n      LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n    }\n    LOG.debug(\"Current cache:\" + cache);\n  }"
        ],
        [
            "SnapshotFileCache::getUnreferencedFiles(Iterable,SnapshotManager)",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "   * is refreshed and the cache checked again for that file.\n   * This ensures that we never return files that exist.\n   * <p>\n   * Note this may lead to periodic false positives for the file being referenced. Periodically, the\n   * cache is refreshed even if there are no requests to ensure that the false negatives get removed\n   * eventually. For instance, suppose you have a file in the snapshot and it gets loaded into the\n   * cache. Then at some point later that snapshot is deleted. If the cache has not been refreshed\n   * at that point, cache will still think the file system contains that file and return\n   * <tt>true</tt>, even if it is no longer present (false positive). However, if the file never was\n   * on the filesystem, we will never find it and always return <tt>false</tt>.\n   * @param files file to check, NOTE: Relies that files are loaded from hdfs before method\n   *              is called (NOT LAZY)\n   * @return <tt>unReferencedFiles</tt> the collection of files that do not have snapshot references\n   * @throws IOException if there is an unexpected error reaching the filesystem.\n   */\n  // XXX this is inefficient to synchronize on the method, when what we really need to guard against\n  // is an illegal access to the cache. Really we could do a mutex-guarded pointer swap on the\n  // cache, but that seems overkill at the moment and isn't necessarily a bottleneck.\n  public synchronized Iterable<FileStatus> getUnreferencedFiles(Iterable<FileStatus> files,\n      final SnapshotManager snapshotManager)\n      throws IOException {\n    List<FileStatus> unReferencedFiles = Lists.newArrayList();\n    boolean refreshed = false;\n    Lock lock = null;\n    if (snapshotManager != null) {\n      lock = snapshotManager.getTakingSnapshotLock().writeLock();\n    }\n    if (lock == null || lock.tryLock()) {\n      try {\n        if (snapshotManager != null && snapshotManager.isTakingAnySnapshot()) {\n          LOG.warn(\"Not checking unreferenced files since snapshot is running, it will \"\n              + \"skip to clean the HFiles this time\");\n          return unReferencedFiles;\n        }\n        for (FileStatus file : files) {\n          String fileName = file.getPath().getName();\n          if (!refreshed && !cache.contains(fileName)) {\n            refreshCache();\n            refreshed = true;\n          }\n          if (cache.contains(fileName)) {\n            continue;\n          }\n          unReferencedFiles.add(file);\n        }\n      } finally {\n        if (lock != null) {\n          lock.unlock();\n        }\n      }\n    }\n    return unReferencedFiles;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 +\n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 +\n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  ",
            "   * and the cache checked again for that file. This ensures that we never return files that exist.\n   * <p>\n   * Note this may lead to periodic false positives for the file being referenced. Periodically, the\n   * cache is refreshed even if there are no requests to ensure that the false negatives get removed\n   * eventually. For instance, suppose you have a file in the snapshot and it gets loaded into the\n   * cache. Then at some point later that snapshot is deleted. If the cache has not been refreshed\n   * at that point, cache will still think the file system contains that file and return\n   * <tt>true</tt>, even if it is no longer present (false positive). However, if the file never was\n   * on the filesystem, we will never find it and always return <tt>false</tt>.\n   * @param files file to check, NOTE: Relies that files are loaded from hdfs before method is\n   *          called (NOT LAZY)\n   * @return <tt>unReferencedFiles</tt> the collection of files that do not have snapshot references\n   * @throws IOException if there is an unexpected error reaching the filesystem.\n   */\n  // XXX this is inefficient to synchronize on the method, when what we really need to guard against\n  // is an illegal access to the cache. Really we could do a mutex-guarded pointer swap on the\n  // cache, but that seems overkill at the moment and isn't necessarily a bottleneck.\n  public synchronized Iterable<FileStatus> getUnreferencedFiles(Iterable<FileStatus> files,\n      final SnapshotManager snapshotManager) throws IOException {\n    List<FileStatus> unReferencedFiles = Lists.newArrayList();\n    boolean refreshed = false;\n    Lock lock = null;\n    if (snapshotManager != null) {\n      lock = snapshotManager.getTakingSnapshotLock().writeLock();\n    }\n    if (lock == null || lock.tryLock()) {\n      try {\n        if (snapshotManager != null && snapshotManager.isTakingAnySnapshot()) {\n          LOG.warn(\"Not checking unreferenced files since snapshot is running, it will \" +\n            \"skip to clean the HFiles this time\");\n          return unReferencedFiles;\n        }\n        for (FileStatus file : files) {\n          String fileName = file.getPath().getName();\n          if (!refreshed && !cache.contains(fileName)) {\n            refreshCache();\n            refreshed = true;\n          }\n          if (cache.contains(fileName)) {\n            continue;\n          }\n          unReferencedFiles.add(file);\n        }\n      } finally {\n        if (lock != null) {\n          lock.unlock();\n        }\n      }\n    }\n    return unReferencedFiles;\n  }"
        ],
        [
            "SnapshotFileCache::SnapshotFileCache(Configuration,long,String,SnapshotFileInspector)",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem.\n   * <p>\n   * Immediately loads the file cache.\n   * @param conf to extract the configured {@link FileSystem} where the snapshots are stored and\n   *          hbase root directory\n   * @param cacheRefreshPeriod frequency (ms) with which the cache should be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   * @throws IOException if the {@link FileSystem} or root directory cannot be loaded\n   */\n  public SnapshotFileCache(Configuration conf, long cacheRefreshPeriod, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) throws IOException {\n    this(FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf), 0, cacheRefreshPeriod,\n        refreshThreadName, inspectSnapshotFiles);\n  }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 +\n 115  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem.\n   * <p>\n   * Immediately loads the file cache.\n   * @param conf to extract the configured {@link FileSystem} where the snapshots are stored and\n   *          hbase root directory\n   * @param cacheRefreshPeriod frequency (ms) with which the cache should be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   * @throws IOException if the {@link FileSystem} or root directory cannot be loaded\n   */\n  public SnapshotFileCache(Configuration conf, long cacheRefreshPeriod, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) throws IOException {\n    this(FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf), 0, cacheRefreshPeriod,\n      refreshThreadName, inspectSnapshotFiles);\n  }"
        ],
        [
            "SnapshotFileCache::refreshCache()",
            " 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229  \n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239  \n 240 -\n 241 -\n 242 -\n 243 -\n 244 -\n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253 -\n 254 -\n 255 -\n 256 -\n 257 -\n 258 -\n 259 -\n 260 -\n 261 -\n 262 -\n 263 -\n 264 -\n 265 -\n 266 -\n 267 -\n 268 -\n 269 -\n 270 -\n 271 -\n 272  \n 273  \n 274 -\n 275 -\n 276  \n 277 -\n 278  ",
            "  private synchronized void refreshCache() throws IOException {\n    // get the status of the snapshots directory and check if it is has changes\n    FileStatus dirStatus;\n    try {\n      dirStatus = fs.getFileStatus(snapshotDir);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (dirStatus.getModificationTime() <= this.lastModifiedTime) return;\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = dirStatus.getModificationTime();\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }",
            " 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217  \n 218 +\n 219 +\n 220  \n 221 +\n 222  \n 223  \n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244  \n 245 +\n 246 +\n 247 +\n 248  \n 249 +\n 250  \n 251 +\n 252  ",
            "  private void refreshCache() throws IOException {\n    // just list the snapshot directory directly, do not check the modification time for the root\n    // snapshot directory, as some file system implementations do not modify the parent directory's\n    // modTime when there are new sub items, for example, S3.\n    FileStatus[] snapshotDirs = FSUtils.listStatus(fs, snapshotDir,\n      p -> !p.getName().equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME));\n\n    // clear the cache, as in the below code, either we will also clear the snapshots, or we will\n    // refill the file name cache again.\n    this.cache.clear();\n    if (ArrayUtils.isEmpty(snapshotDirs)) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, clear cache\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // iterate over all the cached snapshots and see if we need to update some, it is not an\n    // expensive operation if we do not reload the manifest of snapshots.\n    Map<String, SnapshotDirectoryInfo> newSnapshots = new HashMap<>();\n    for (FileStatus snapshotDir : snapshotDirs) {\n      String name = snapshotDir.getPath().getName();\n      SnapshotDirectoryInfo files = this.snapshots.remove(name);\n      // if we don't know about the snapshot or its been modified, we need to update the\n      // files the latter could occur where I create a snapshot, then delete it, and then make a\n      // new snapshot with the same name. We will need to update the cache the information from\n      // that new snapshot, even though it has the same name as the files referenced have\n      // probably changed.\n      if (files == null || files.hasBeenModified(snapshotDir.getModificationTime())) {\n        Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshotDir.getPath());\n        files = new SnapshotDirectoryInfo(snapshotDir.getModificationTime(), storedFiles);\n      }\n      // add all the files to cache\n      this.cache.addAll(files.getFiles());\n      newSnapshots.put(name, files);\n    }\n    // set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(newSnapshots);\n  }"
        ]
    ],
    "3434e99e6c728d59ba99141df3730f3e70e0059c": [
        [
            "TestClientClusterStatus::testNone()",
            " 101  \n 102  \n 103 -\n 104 -\n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110  ",
            "  @Test\n  public void testNone() throws Exception {\n    ClusterStatus status0\n      = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.allOf(Option.class)));\n    ClusterStatus status1\n      = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.noneOf(Option.class)));\n    Assert.assertEquals(status0, status1);\n    checkPbObjectNotNull(status0);\n    checkPbObjectNotNull(status1);\n  }",
            " 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112  ",
            "  @Test\n  public void testNone() throws Exception {\n    ClusterMetrics status0 = ADMIN.getClusterMetrics(EnumSet.allOf(Option.class));\n    ClusterMetrics status1 = ADMIN.getClusterMetrics(EnumSet.noneOf(Option.class));\n    // Do a rough compare. More specific compares can fail because all regions not deployed yet\n    // or more requests than expected.\n    Assert.assertEquals(status0.getLiveServerMetrics().size(),\n        status1.getLiveServerMetrics().size());\n    checkPbObjectNotNull(new ClusterStatus(status0));\n    checkPbObjectNotNull(new ClusterStatus(status1));\n  }"
        ]
    ],
    "125767b44e93f1094b77a6cf8c2a5ca19b5cabd2": [
        [
            "TestClientOperationTimeout::setUpClass()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "  @BeforeClass\n  public static void setUpClass() throws Exception {\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n\n    TESTING_UTIL.startMiniCluster(1, 1, null, null, DelayedRegionServer.class);\n  }",
            "  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  ",
            "  @BeforeClass\n  public static void setUpClass() throws Exception {\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_META_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n\n    TESTING_UTIL.startMiniCluster(1, 1, null, null, DelayedRegionServer.class);\n  }"
        ]
    ],
    "64f88906f7cc7265fe0c42a4c42530dbd660c70b": [
        [
            "SimpleRegionNormalizer::computePlanForTable(TableName)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193 -\n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<>();\n    List<RegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < minRegionCount) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + minRegionCount + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n    int acutalRegionCnt = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      RegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        acutalRegionCnt++;\n        totalSizeMb += regionSize;\n      }\n    }\n    int targetRegionCount = -1;\n    long targetRegionSize = -1;\n    try {\n      TableDescriptor tableDescriptor = masterServices.getTableDescriptors().get(table);\n      if(tableDescriptor != null) {\n        targetRegionCount =\n            tableDescriptor.getNormalizerTargetRegionCount();\n        targetRegionSize =\n            tableDescriptor.getNormalizerTargetRegionSize();\n        LOG.debug(\"Table {}:  target region count is {}, target region size is {}\", table,\n            targetRegionCount, targetRegionSize);\n      }\n    } catch (IOException e) {\n      LOG.warn(\n        \"cannot get the target number and target size of table {}, they will be default value -1.\",\n        table);\n    }\n\n    double avgRegionSize;\n    if (targetRegionSize > 0) {\n      avgRegionSize = targetRegionSize;\n    } else if (targetRegionCount > 0) {\n      avgRegionSize = totalSizeMb / (double) targetRegionCount;\n    } else {\n      avgRegionSize = acutalRegionCnt == 0 ? 0 : totalSizeMb / (double) acutalRegionCnt;\n    }\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      RegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          RegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize >= 0 && regionSize2 >= 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    if (!mergeEnabled && !splitEnabled) {\n      LOG.debug(\"Both split and merge are disabled for table: \" + table);\n      return null;\n    }\n    List<NormalizationPlan> plans = new ArrayList<>();\n    List<RegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < minRegionCount) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + minRegionCount + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n    int acutalRegionCnt = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      RegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        acutalRegionCnt++;\n        totalSizeMb += regionSize;\n      }\n    }\n    int targetRegionCount = -1;\n    long targetRegionSize = -1;\n    try {\n      TableDescriptor tableDescriptor = masterServices.getTableDescriptors().get(table);\n      if(tableDescriptor != null) {\n        targetRegionCount =\n            tableDescriptor.getNormalizerTargetRegionCount();\n        targetRegionSize =\n            tableDescriptor.getNormalizerTargetRegionSize();\n        LOG.debug(\"Table {}:  target region count is {}, target region size is {}\", table,\n            targetRegionCount, targetRegionSize);\n      }\n    } catch (IOException e) {\n      LOG.warn(\n        \"cannot get the target number and target size of table {}, they will be default value -1.\",\n        table);\n    }\n\n    double avgRegionSize;\n    if (targetRegionSize > 0) {\n      avgRegionSize = targetRegionSize;\n    } else if (targetRegionCount > 0) {\n      avgRegionSize = totalSizeMb / (double) targetRegionCount;\n    } else {\n      avgRegionSize = acutalRegionCnt == 0 ? 0 : totalSizeMb / (double) acutalRegionCnt;\n    }\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    while (candidateIdx < tableRegions.size()) {\n      RegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          RegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize >= 0 && regionSize2 >= 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }"
        ]
    ]
}