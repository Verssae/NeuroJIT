{
    "893a54c3a40e08b6f50d753fca01688f491893d0": [
        [
            "Canary::run(String)",
            " 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550 -\n 551 -\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    int index = parseArgs(args);\n    ChoreService choreService = null;\n\n    // Launches chore for refreshing kerberos credentials if security is enabled.\n    // Please see http://hbase.apache.org/book.html#_running_canary_in_a_kerberos_enabled_cluster\n    // for more details.\n    final ScheduledChore authChore = AuthUtil.getAuthChore(conf);\n    if (authChore != null) {\n      choreService = new ChoreService(\"CANARY_TOOL\");\n      choreService.scheduleChore(authChore);\n    }\n\n    // Start to prepare the stuffs\n    Monitor monitor = null;\n    Thread monitorThread = null;\n    long startTime = 0;\n    long currentTimeLength = 0;\n    // Get a connection to use in below.\n    // try-with-resources jdk7 construct. See\n    // http://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html\n    try (Connection connection = ConnectionFactory.createConnection(this.conf)) {\n      do {\n        // Do monitor !!\n        try {\n          monitor = this.newMonitor(connection, index, args);\n          monitorThread = new Thread(monitor);\n          startTime = System.currentTimeMillis();\n          monitorThread.start();\n          while (!monitor.isDone()) {\n            // wait for 1 sec\n            Thread.sleep(1000);\n            // exit if any error occurs\n            if (this.failOnError && monitor.hasError()) {\n              monitorThread.interrupt();\n              if (monitor.initialized) {\n                System.exit(monitor.errorCode);\n              } else {\n                System.exit(INIT_ERROR_EXIT_CODE);\n              }\n            }\n            currentTimeLength = System.currentTimeMillis() - startTime;\n            if (currentTimeLength > this.timeout) {\n              LOG.error(\"The monitor is running too long (\" + currentTimeLength\n                  + \") after timeout limit:\" + this.timeout\n                  + \" will be killed itself !!\");\n              if (monitor.initialized) {\n                System.exit(TIMEOUT_ERROR_EXIT_CODE);\n              } else {\n                System.exit(INIT_ERROR_EXIT_CODE);\n              }\n              break;\n            }\n          }\n\n          if (this.failOnError && monitor.hasError()) {\n            monitorThread.interrupt();\n            System.exit(monitor.errorCode);\n          }\n        } finally {\n          if (monitor != null) monitor.close();\n        }\n\n        Thread.sleep(interval);\n      } while (interval > 0);\n    } // try-with-resources close\n\n    if (choreService != null) {\n      choreService.shutdown();\n    }\n    return(monitor.errorCode);\n  }",
            " 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    int index = parseArgs(args);\n    ChoreService choreService = null;\n\n    // Launches chore for refreshing kerberos credentials if security is enabled.\n    // Please see http://hbase.apache.org/book.html#_running_canary_in_a_kerberos_enabled_cluster\n    // for more details.\n    final ScheduledChore authChore = AuthUtil.getAuthChore(conf);\n    if (authChore != null) {\n      choreService = new ChoreService(\"CANARY_TOOL\");\n      choreService.scheduleChore(authChore);\n    }\n\n    // Start to prepare the stuffs\n    Monitor monitor = null;\n    Thread monitorThread = null;\n    long startTime = 0;\n    long currentTimeLength = 0;\n    // Get a connection to use in below.\n    try (Connection connection = ConnectionFactory.createConnection(this.conf)) {\n      do {\n        // Do monitor !!\n        try {\n          monitor = this.newMonitor(connection, index, args);\n          monitorThread = new Thread(monitor);\n          startTime = System.currentTimeMillis();\n          monitorThread.start();\n          while (!monitor.isDone()) {\n            // wait for 1 sec\n            Thread.sleep(1000);\n            // exit if any error occurs\n            if (this.failOnError && monitor.hasError()) {\n              monitorThread.interrupt();\n              if (monitor.initialized) {\n                System.exit(monitor.errorCode);\n              } else {\n                System.exit(INIT_ERROR_EXIT_CODE);\n              }\n            }\n            currentTimeLength = System.currentTimeMillis() - startTime;\n            if (currentTimeLength > this.timeout) {\n              LOG.error(\"The monitor is running too long (\" + currentTimeLength\n                  + \") after timeout limit:\" + this.timeout\n                  + \" will be killed itself !!\");\n              if (monitor.initialized) {\n                System.exit(TIMEOUT_ERROR_EXIT_CODE);\n              } else {\n                System.exit(INIT_ERROR_EXIT_CODE);\n              }\n              break;\n            }\n          }\n\n          if (this.failOnError && monitor.hasError()) {\n            monitorThread.interrupt();\n            System.exit(monitor.errorCode);\n          }\n        } finally {\n          if (monitor != null) monitor.close();\n        }\n\n        Thread.sleep(interval);\n      } while (interval > 0);\n    } // try-with-resources close\n\n    if (choreService != null) {\n      choreService.shutdown();\n    }\n    return(monitor.errorCode);\n  }"
        ],
        [
            "Canary::RegionMonitor::sniff(TaskType)",
            " 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  ",
            "    private List<Future<Void>> sniff(TaskType taskType) throws Exception {\n      List<Future<Void>> taskFutures = new LinkedList<Future<Void>>();\n      for (HTableDescriptor table : admin.listTables()) {\n        if (admin.isTableEnabled(table.getTableName())\n            && (!table.getTableName().equals(writeTableName))) {\n          taskFutures.addAll(Canary.sniff(admin, sink, table, executor, taskType));\n        }\n      }\n      return taskFutures;\n    }",
            " 854  \n 855 +\n 856 +\n 857 +\n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  ",
            "    private List<Future<Void>> sniff(TaskType taskType) throws Exception {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(\"reading list of tables\"));\n      }\n      List<Future<Void>> taskFutures = new LinkedList<Future<Void>>();\n      for (HTableDescriptor table : admin.listTables()) {\n        if (admin.isTableEnabled(table.getTableName())\n            && (!table.getTableName().equals(writeTableName))) {\n          taskFutures.addAll(Canary.sniff(admin, sink, table, executor, taskType));\n        }\n      }\n      return taskFutures;\n    }"
        ],
        [
            "Canary::RegionServerMonitor::getAllRegionServerByName()",
            "1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  ",
            "    private Map<String, List<HRegionInfo>> getAllRegionServerByName() {\n      Map<String, List<HRegionInfo>> rsAndRMap = new HashMap<String, List<HRegionInfo>>();\n      Table table = null;\n      RegionLocator regionLocator = null;\n      try {\n        HTableDescriptor[] tableDescs = this.admin.listTables();\n        List<HRegionInfo> regions = null;\n        for (HTableDescriptor tableDesc : tableDescs) {\n          table = this.admin.getConnection().getTable(tableDesc.getTableName());\n          regionLocator = this.admin.getConnection().getRegionLocator(tableDesc.getTableName());\n\n          for (HRegionLocation location : regionLocator.getAllRegionLocations()) {\n            ServerName rs = location.getServerName();\n            String rsName = rs.getHostname();\n            HRegionInfo r = location.getRegionInfo();\n\n            if (rsAndRMap.containsKey(rsName)) {\n              regions = rsAndRMap.get(rsName);\n            } else {\n              regions = new ArrayList<HRegionInfo>();\n              rsAndRMap.put(rsName, regions);\n            }\n            regions.add(r);\n          }\n          table.close();\n        }\n\n      } catch (IOException e) {\n        String msg = \"Get HTables info failed\";\n        LOG.error(msg, e);\n        this.errorCode = INIT_ERROR_EXIT_CODE;\n      } finally {\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException e) {\n            LOG.warn(\"Close table failed\", e);\n          }\n        }\n      }\n\n      return rsAndRMap;\n    }",
            "1099  \n1100  \n1101  \n1102  \n1103  \n1104 +\n1105 +\n1106 +\n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  ",
            "    private Map<String, List<HRegionInfo>> getAllRegionServerByName() {\n      Map<String, List<HRegionInfo>> rsAndRMap = new HashMap<String, List<HRegionInfo>>();\n      Table table = null;\n      RegionLocator regionLocator = null;\n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"reading list of tables and locations\"));\n        }\n        HTableDescriptor[] tableDescs = this.admin.listTables();\n        List<HRegionInfo> regions = null;\n        for (HTableDescriptor tableDesc : tableDescs) {\n          table = this.admin.getConnection().getTable(tableDesc.getTableName());\n          regionLocator = this.admin.getConnection().getRegionLocator(tableDesc.getTableName());\n\n          for (HRegionLocation location : regionLocator.getAllRegionLocations()) {\n            ServerName rs = location.getServerName();\n            String rsName = rs.getHostname();\n            HRegionInfo r = location.getRegionInfo();\n\n            if (rsAndRMap.containsKey(rsName)) {\n              regions = rsAndRMap.get(rsName);\n            } else {\n              regions = new ArrayList<HRegionInfo>();\n              rsAndRMap.put(rsName, regions);\n            }\n            regions.add(r);\n          }\n          table.close();\n        }\n\n      } catch (IOException e) {\n        String msg = \"Get HTables info failed\";\n        LOG.error(msg, e);\n        this.errorCode = INIT_ERROR_EXIT_CODE;\n      } finally {\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException e) {\n            LOG.warn(\"Close table failed\", e);\n          }\n        }\n      }\n\n      return rsAndRMap;\n    }"
        ],
        [
            "Canary::RegionServerTask::call()",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "    @Override\n    public Void call() {\n      TableName tableName = null;\n      Table table = null;\n      Get get = null;\n      byte[] startKey = null;\n      Scan scan = null;\n      StopWatch stopWatch = new StopWatch();\n      // monitor one region on every region server\n      stopWatch.reset();\n      try {\n        tableName = region.getTable();\n        table = connection.getTable(tableName);\n        startKey = region.getStartKey();\n        // Can't do a get on empty start row so do a Scan of first element if any instead.\n        if (startKey.length > 0) {\n          get = new Get(startKey);\n          get.setCacheBlocks(false);\n          get.setFilter(new FirstKeyOnlyFilter());\n          stopWatch.start();\n          table.get(get);\n          stopWatch.stop();\n        } else {\n          scan = new Scan();\n          scan.setCacheBlocks(false);\n          scan.setFilter(new FirstKeyOnlyFilter());\n          scan.setCaching(1);\n          scan.setMaxResultSize(1L);\n          stopWatch.start();\n          ResultScanner s = table.getScanner(scan);\n          s.close();\n          stopWatch.stop();\n        }\n        successes.incrementAndGet();\n        sink.publishReadTiming(tableName.getNameAsString(), serverName, stopWatch.getTime());\n      } catch (TableNotFoundException tnfe) {\n        LOG.error(\"Table may be deleted\", tnfe);\n        // This is ignored because it doesn't imply that the regionserver is dead\n      } catch (TableNotEnabledException tnee) {\n        // This is considered a success since we got a response.\n        successes.incrementAndGet();\n        LOG.debug(\"The targeted table was disabled.  Assuming success.\");\n      } catch (DoNotRetryIOException dnrioe) {\n        sink.publishReadFailure(tableName.getNameAsString(), serverName);\n        LOG.error(dnrioe);\n      } catch (IOException e) {\n        sink.publishReadFailure(tableName.getNameAsString(), serverName);\n        LOG.error(e);\n      } finally {\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException e) {/* DO NOTHING */\n            LOG.error(\"Close table failed\", e);\n          }\n        }\n        scan = null;\n        get = null;\n        startKey = null;\n      }\n      return null;\n    }",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354 +\n 355 +\n 356 +\n 357 +\n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 +\n 373  \n 374  \n 375 +\n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  ",
            "    @Override\n    public Void call() {\n      TableName tableName = null;\n      Table table = null;\n      Get get = null;\n      byte[] startKey = null;\n      Scan scan = null;\n      StopWatch stopWatch = new StopWatch();\n      // monitor one region on every region server\n      stopWatch.reset();\n      try {\n        tableName = region.getTable();\n        table = connection.getTable(tableName);\n        startKey = region.getStartKey();\n        // Can't do a get on empty start row so do a Scan of first element if any instead.\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"reading from region server %s table %s region %s and key %s\",\n            serverName, region.getTable(), region.getRegionNameAsString(),\n            Bytes.toStringBinary(startKey)));\n        }\n        if (startKey.length > 0) {\n          get = new Get(startKey);\n          get.setCacheBlocks(false);\n          get.setFilter(new FirstKeyOnlyFilter());\n          stopWatch.start();\n          table.get(get);\n          stopWatch.stop();\n        } else {\n          scan = new Scan();\n          scan.setCacheBlocks(false);\n          scan.setFilter(new FirstKeyOnlyFilter());\n          scan.setCaching(1);\n          scan.setMaxResultSize(1L);\n          scan.setSmall(true);\n          stopWatch.start();\n          ResultScanner s = table.getScanner(scan);\n          s.next();\n          s.close();\n          stopWatch.stop();\n        }\n        successes.incrementAndGet();\n        sink.publishReadTiming(tableName.getNameAsString(), serverName, stopWatch.getTime());\n      } catch (TableNotFoundException tnfe) {\n        LOG.error(\"Table may be deleted\", tnfe);\n        // This is ignored because it doesn't imply that the regionserver is dead\n      } catch (TableNotEnabledException tnee) {\n        // This is considered a success since we got a response.\n        successes.incrementAndGet();\n        LOG.debug(\"The targeted table was disabled.  Assuming success.\");\n      } catch (DoNotRetryIOException dnrioe) {\n        sink.publishReadFailure(tableName.getNameAsString(), serverName);\n        LOG.error(dnrioe);\n      } catch (IOException e) {\n        sink.publishReadFailure(tableName.getNameAsString(), serverName);\n        LOG.error(e);\n      } finally {\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException e) {/* DO NOTHING */\n            LOG.error(\"Close table failed\", e);\n          }\n        }\n        scan = null;\n        get = null;\n        startKey = null;\n      }\n      return null;\n    }"
        ],
        [
            "Canary::sniff(Admin,Sink,String,ExecutorService,TaskType)",
            " 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  ",
            "  /**\n   * Canary entry point for specified table.\n   * @throws Exception\n   */\n  private static List<Future<Void>> sniff(final Admin admin, final Sink sink, String tableName,\n      ExecutorService executor, TaskType taskType) throws Exception {\n    if (admin.isTableEnabled(TableName.valueOf(tableName))) {\n      return Canary.sniff(admin, sink, admin.getTableDescriptor(TableName.valueOf(tableName)),\n        executor, taskType);\n    } else {\n      LOG.warn(String.format(\"Table %s is not enabled\", tableName));\n    }\n    return new LinkedList<Future<Void>>();\n  }",
            " 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948 +\n 949 +\n 950 +\n 951 +\n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  ",
            "  /**\n   * Canary entry point for specified table.\n   * @throws Exception\n   */\n  private static List<Future<Void>> sniff(final Admin admin, final Sink sink, String tableName,\n      ExecutorService executor, TaskType taskType) throws Exception {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(String.format(\"checking table is enabled and getting table descriptor for table %s\",\n        tableName));\n    }\n    if (admin.isTableEnabled(TableName.valueOf(tableName))) {\n      return Canary.sniff(admin, sink, admin.getTableDescriptor(TableName.valueOf(tableName)),\n        executor, taskType);\n    } else {\n      LOG.warn(String.format(\"Table %s is not enabled\", tableName));\n    }\n    return new LinkedList<Future<Void>>();\n  }"
        ],
        [
            "Canary::RegionTask::write()",
            " 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  ",
            "    /**\n     * Check writes for the canary table\n     * @return\n     */\n    private Void write() {\n      Table table = null;\n      HTableDescriptor tableDesc = null;\n      try {\n        table = connection.getTable(region.getTable());\n        tableDesc = table.getTableDescriptor();\n        byte[] rowToCheck = region.getStartKey();\n        if (rowToCheck.length == 0) {\n          rowToCheck = new byte[]{0x0};\n        }\n        int writeValueSize =\n            connection.getConfiguration().getInt(HConstants.HBASE_CANARY_WRITE_VALUE_SIZE_KEY, 10);\n        for (HColumnDescriptor column : tableDesc.getColumnFamilies()) {\n          Put put = new Put(rowToCheck);\n          byte[] value = new byte[writeValueSize];\n          Bytes.random(value);\n          put.addColumn(column.getName(), HConstants.EMPTY_BYTE_ARRAY, value);\n          try {\n            long startTime = System.currentTimeMillis();\n            table.put(put);\n            long time = System.currentTimeMillis() - startTime;\n            sink.publishWriteTiming(region, column, time);\n          } catch (Exception e) {\n            sink.publishWriteFailure(region, column, e);\n          }\n        }\n        table.close();\n      } catch (IOException e) {\n        sink.publishWriteFailure(region, e);\n      }\n      return null;\n    }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 +\n 298 +\n 299 +\n 300 +\n 301 +\n 302 +\n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  ",
            "    /**\n     * Check writes for the canary table\n     * @return\n     */\n    private Void write() {\n      Table table = null;\n      HTableDescriptor tableDesc = null;\n      try {\n        table = connection.getTable(region.getTable());\n        tableDesc = table.getTableDescriptor();\n        byte[] rowToCheck = region.getStartKey();\n        if (rowToCheck.length == 0) {\n          rowToCheck = new byte[]{0x0};\n        }\n        int writeValueSize =\n            connection.getConfiguration().getInt(HConstants.HBASE_CANARY_WRITE_VALUE_SIZE_KEY, 10);\n        for (HColumnDescriptor column : tableDesc.getColumnFamilies()) {\n          Put put = new Put(rowToCheck);\n          byte[] value = new byte[writeValueSize];\n          Bytes.random(value);\n          put.addColumn(column.getName(), HConstants.EMPTY_BYTE_ARRAY, value);\n\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(String.format(\"writing to table %s region %s column family %s and key %s\",\n              tableDesc.getTableName(), region.getRegionNameAsString(), column.getNameAsString(),\n              Bytes.toStringBinary(rowToCheck)));\n          }\n          try {\n            long startTime = System.currentTimeMillis();\n            table.put(put);\n            long time = System.currentTimeMillis() - startTime;\n            sink.publishWriteTiming(region, column, time);\n          } catch (Exception e) {\n            sink.publishWriteFailure(region, column, e);\n          }\n        }\n        table.close();\n      } catch (IOException e) {\n        sink.publishWriteFailure(region, e);\n      }\n      return null;\n    }"
        ],
        [
            "Canary::sniff(Admin,Sink,HTableDescriptor,ExecutorService,TaskType)",
            " 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  ",
            "  private static List<Future<Void>> sniff(final Admin admin, final Sink sink,\n      HTableDescriptor tableDesc, ExecutorService executor, TaskType taskType) throws Exception {\n    Table table = null;\n    try {\n      table = admin.getConnection().getTable(tableDesc.getTableName());\n    } catch (TableNotFoundException e) {\n      return new ArrayList<Future<Void>>();\n    }\n    List<RegionTask> tasks = new ArrayList<RegionTask>();\n    try {\n      for (HRegionInfo region : admin.getTableRegions(tableDesc.getTableName())) {\n        tasks.add(new RegionTask(admin.getConnection(), region, sink, taskType));\n      }\n    } finally {\n      table.close();\n    }\n    return executor.invokeAll(tasks);\n  }",
            " 964  \n 965  \n 966 +\n 967 +\n 968 +\n 969 +\n 970 +\n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  ",
            "  private static List<Future<Void>> sniff(final Admin admin, final Sink sink,\n      HTableDescriptor tableDesc, ExecutorService executor, TaskType taskType) throws Exception {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(String.format(\"reading list of regions for table %s\", tableDesc.getTableName()));\n    }\n\n    Table table = null;\n    try {\n      table = admin.getConnection().getTable(tableDesc.getTableName());\n    } catch (TableNotFoundException e) {\n      return new ArrayList<Future<Void>>();\n    }\n    List<RegionTask> tasks = new ArrayList<RegionTask>();\n    try {\n      for (HRegionInfo region : admin.getTableRegions(tableDesc.getTableName())) {\n        tasks.add(new RegionTask(admin.getConnection(), region, sink, taskType));\n      }\n    } finally {\n      table.close();\n    }\n    return executor.invokeAll(tasks);\n  }"
        ],
        [
            "Canary::printUsageAndExit()",
            " 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616 -\n 617 -\n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  ",
            "  private void printUsageAndExit() {\n    System.err.printf(\n      \"Usage: bin/hbase %s [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]%n\",\n        getClass().getName());\n    System.err.println(\" where [opts] are:\");\n    System.err.println(\"   -help          Show this help and exit.\");\n    System.err.println(\"   -regionserver  replace the table argument to regionserver,\");\n    System.err.println(\"      which means to enable regionserver mode\");\n    System.err.println(\"   -allRegions    Tries all regions on a regionserver,\");\n    System.err.println(\"      only works in regionserver mode.\");\n    System.err.println(\"   -daemon        Continuous check at defined intervals.\");\n    System.err.println(\"   -interval <N>  Interval between checks (sec)\");\n    System.err.println(\"   -e             Use region/regionserver as regular expression\");\n    System.err.println(\"      which means the region/regionserver is regular expression pattern\");\n    System.err.println(\"   -f <B>         stop whole program if first error occurs,\" +\n        \" default is true\");\n    System.err.println(\"   -t <N>         timeout for a check, default is 600000 (milisecs)\");\n    System.err.println(\"   -writeSniffing enable the write sniffing in canary\");\n    System.err.println(\"   -writeTable    The table used for write sniffing.\"\n        + \" Default is hbase:canary\");\n    System.exit(USAGE_EXIT_CODE);\n  }",
            " 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635 +\n 636 +\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  ",
            "  private void printUsageAndExit() {\n    System.err.printf(\n      \"Usage: bin/hbase %s [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]%n\",\n        getClass().getName());\n    System.err.println(\" where [opts] are:\");\n    System.err.println(\"   -help          Show this help and exit.\");\n    System.err.println(\"   -regionserver  replace the table argument to regionserver,\");\n    System.err.println(\"      which means to enable regionserver mode\");\n    System.err.println(\"   -allRegions    Tries all regions on a regionserver,\");\n    System.err.println(\"      only works in regionserver mode.\");\n    System.err.println(\"   -daemon        Continuous check at defined intervals.\");\n    System.err.println(\"   -interval <N>  Interval between checks (sec)\");\n    System.err.println(\"   -e             Use table/regionserver as regular expression\");\n    System.err.println(\"      which means the table/regionserver is regular expression pattern\");\n    System.err.println(\"   -f <B>         stop whole program if first error occurs,\" +\n        \" default is true\");\n    System.err.println(\"   -t <N>         timeout for a check, default is 600000 (milisecs)\");\n    System.err.println(\"   -writeSniffing enable the write sniffing in canary\");\n    System.err.println(\"   -writeTable    The table used for write sniffing.\"\n        + \" Default is hbase:canary\");\n    System.exit(USAGE_EXIT_CODE);\n  }"
        ],
        [
            "Canary::RegionTask::read()",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 -\n 240  \n 241 -\n 242 -\n 243  \n 244 -\n 245  \n 246 -\n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  ",
            "    public Void read() {\n      Table table = null;\n      HTableDescriptor tableDesc = null;\n      try {\n        table = connection.getTable(region.getTable());\n        tableDesc = table.getTableDescriptor();\n      } catch (IOException e) {\n        LOG.debug(\"sniffRegion failed\", e);\n        sink.publishReadFailure(region, e);\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException ioe) {\n            LOG.error(\"Close table failed\", e);\n          }\n        }\n        return null;\n      }\n\n      byte[] startKey = null;\n      Get get = null;\n      Scan scan = null;\n      ResultScanner rs = null;\n      StopWatch stopWatch = new StopWatch();\n      for (HColumnDescriptor column : tableDesc.getColumnFamilies()) {\n        stopWatch.reset();\n        startKey = region.getStartKey();\n        // Can't do a get on empty start row so do a Scan of first element if any instead.\n        if (startKey.length > 0) {\n          get = new Get(startKey);\n          get.setCacheBlocks(false);\n          get.setFilter(new FirstKeyOnlyFilter());\n          get.addFamily(column.getName());\n        } else {\n          scan = new Scan();\n          scan.setRaw(true);\n          scan.setCaching(1);\n          scan.setCacheBlocks(false);\n          scan.setFilter(new FirstKeyOnlyFilter());\n          scan.addFamily(column.getName());\n          scan.setMaxResultSize(1L);\n        }\n\n        try {\n          if (startKey.length > 0) {\n            stopWatch.start();\n            table.get(get);\n            stopWatch.stop();\n            sink.publishReadTiming(region, column, stopWatch.getTime());\n          } else {\n            stopWatch.start();\n            rs = table.getScanner(scan);\n            stopWatch.stop();\n            sink.publishReadTiming(region, column, stopWatch.getTime());\n          }\n        } catch (Exception e) {\n          sink.publishReadFailure(region, column, e);\n        } finally {\n          if (rs != null) {\n            rs.close();\n          }\n          scan = null;\n          get = null;\n          startKey = null;\n        }\n      }\n      try {\n        table.close();\n      } catch (IOException e) {\n        LOG.error(\"Close table failed\", e);\n      }\n      return null;\n    }",
            " 194  \n 195  \n 196  \n 197  \n 198 +\n 199 +\n 200 +\n 201 +\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 +\n 240  \n 241  \n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247  \n 248 +\n 249  \n 250  \n 251  \n 252  \n 253 +\n 254  \n 255 +\n 256 +\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "    public Void read() {\n      Table table = null;\n      HTableDescriptor tableDesc = null;\n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"reading table descriptor for table %s\",\n            region.getTable()));\n        }\n        table = connection.getTable(region.getTable());\n        tableDesc = table.getTableDescriptor();\n      } catch (IOException e) {\n        LOG.debug(\"sniffRegion failed\", e);\n        sink.publishReadFailure(region, e);\n        if (table != null) {\n          try {\n            table.close();\n          } catch (IOException ioe) {\n            LOG.error(\"Close table failed\", e);\n          }\n        }\n        return null;\n      }\n\n      byte[] startKey = null;\n      Get get = null;\n      Scan scan = null;\n      ResultScanner rs = null;\n      StopWatch stopWatch = new StopWatch();\n      for (HColumnDescriptor column : tableDesc.getColumnFamilies()) {\n        stopWatch.reset();\n        startKey = region.getStartKey();\n        // Can't do a get on empty start row so do a Scan of first element if any instead.\n        if (startKey.length > 0) {\n          get = new Get(startKey);\n          get.setCacheBlocks(false);\n          get.setFilter(new FirstKeyOnlyFilter());\n          get.addFamily(column.getName());\n        } else {\n          scan = new Scan();\n          scan.setRaw(true);\n          scan.setCaching(1);\n          scan.setCacheBlocks(false);\n          scan.setFilter(new FirstKeyOnlyFilter());\n          scan.addFamily(column.getName());\n          scan.setMaxResultSize(1L);\n          scan.setSmall(true);\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(String.format(\"reading from table %s region %s column family %s and key %s\",\n            tableDesc.getTableName(), region.getRegionNameAsString(), column.getNameAsString(),\n            Bytes.toStringBinary(startKey)));\n        }\n        try {\n          stopWatch.start();\n          if (startKey.length > 0) {\n            table.get(get);\n          } else {\n            rs = table.getScanner(scan);\n            rs.next();\n          }\n          stopWatch.stop();\n          sink.publishReadTiming(region, column, stopWatch.getTime());\n        } catch (Exception e) {\n          sink.publishReadFailure(region, column, e);\n        } finally {\n          if (rs != null) {\n            rs.close();\n          }\n          scan = null;\n          get = null;\n          startKey = null;\n        }\n      }\n      try {\n        table.close();\n      } catch (IOException e) {\n        LOG.error(\"Close table failed\", e);\n      }\n      return null;\n    }"
        ],
        [
            "Canary::RegionMonitor::createWriteTable(int)",
            " 872  \n 873  \n 874  \n 875  \n 876 -\n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  ",
            "    private void createWriteTable(int numberOfServers) throws IOException {\n      int numberOfRegions = (int)(numberOfServers * regionsLowerLimit);\n      LOG.info(\"Number of live regionservers: \" + numberOfServers + \", \"\n          + \"pre-splitting the canary table into \" + numberOfRegions + \" regions \"\n          + \"(current  lower limi of regions per server is \" + regionsLowerLimit\n          + \" and you can change it by config: \"\n          + HConstants.HBASE_CANARY_WRITE_PERSERVER_REGIONS_LOWERLIMIT_KEY + \" )\");\n      HTableDescriptor desc = new HTableDescriptor(writeTableName);\n      HColumnDescriptor family = new HColumnDescriptor(CANARY_TABLE_FAMILY_NAME);\n      family.setMaxVersions(1);\n      family.setTimeToLive(writeDataTTL);\n\n      desc.addFamily(family);\n      byte[][] splits = new RegionSplitter.HexStringSplit().split(numberOfRegions);\n      admin.createTable(desc, splits);\n    }",
            " 901  \n 902  \n 903  \n 904  \n 905 +\n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  ",
            "    private void createWriteTable(int numberOfServers) throws IOException {\n      int numberOfRegions = (int)(numberOfServers * regionsLowerLimit);\n      LOG.info(\"Number of live regionservers: \" + numberOfServers + \", \"\n          + \"pre-splitting the canary table into \" + numberOfRegions + \" regions \"\n          + \"(current lower limit of regions per server is \" + regionsLowerLimit\n          + \" and you can change it by config: \"\n          + HConstants.HBASE_CANARY_WRITE_PERSERVER_REGIONS_LOWERLIMIT_KEY + \" )\");\n      HTableDescriptor desc = new HTableDescriptor(writeTableName);\n      HColumnDescriptor family = new HColumnDescriptor(CANARY_TABLE_FAMILY_NAME);\n      family.setMaxVersions(1);\n      family.setTimeToLive(writeDataTTL);\n\n      desc.addFamily(family);\n      byte[][] splits = new RegionSplitter.HexStringSplit().split(numberOfRegions);\n      admin.createTable(desc, splits);\n    }"
        ],
        [
            "Canary::RegionServerMonitor::checkNoTableNames()",
            " 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  ",
            "    private boolean checkNoTableNames() {\n      List<String> foundTableNames = new ArrayList<String>();\n      TableName[] tableNames = null;\n\n      try {\n        tableNames = this.admin.listTableNames();\n      } catch (IOException e) {\n        LOG.error(\"Get listTableNames failed\", e);\n        this.errorCode = INIT_ERROR_EXIT_CODE;\n        return false;\n      }\n\n      if (this.targets == null || this.targets.length == 0) return true;\n\n      for (String target : this.targets) {\n        for (TableName tableName : tableNames) {\n          if (target.equals(tableName.getNameAsString())) {\n            foundTableNames.add(target);\n          }\n        }\n      }\n\n      if (foundTableNames.size() > 0) {\n        System.err.println(\"Cannot pass a tablename when using the -regionserver \" +\n            \"option, tablenames:\" + foundTableNames.toString());\n        this.errorCode = USAGE_EXIT_CODE;\n      }\n      return foundTableNames.size() == 0;\n    }",
            "1012  \n1013  \n1014  \n1015  \n1016 +\n1017 +\n1018 +\n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  ",
            "    private boolean checkNoTableNames() {\n      List<String> foundTableNames = new ArrayList<String>();\n      TableName[] tableNames = null;\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(\"reading list of tables\"));\n      }\n      try {\n        tableNames = this.admin.listTableNames();\n      } catch (IOException e) {\n        LOG.error(\"Get listTableNames failed\", e);\n        this.errorCode = INIT_ERROR_EXIT_CODE;\n        return false;\n      }\n\n      if (this.targets == null || this.targets.length == 0) return true;\n\n      for (String target : this.targets) {\n        for (TableName tableName : tableNames) {\n          if (target.equals(tableName.getNameAsString())) {\n            foundTableNames.add(target);\n          }\n        }\n      }\n\n      if (foundTableNames.size() > 0) {\n        System.err.println(\"Cannot pass a tablename when using the -regionserver \" +\n            \"option, tablenames:\" + foundTableNames.toString());\n        this.errorCode = USAGE_EXIT_CODE;\n      }\n      return foundTableNames.size() == 0;\n    }"
        ],
        [
            "Canary::RegionMonitor::generateMonitorTables(String)",
            " 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798 -\n 799 -\n 800 -\n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  ",
            "    private String[] generateMonitorTables(String[] monitorTargets) throws IOException {\n      String[] returnTables = null;\n\n      if (this.useRegExp) {\n        Pattern pattern = null;\n        HTableDescriptor[] tds = null;\n        Set<String> tmpTables = new TreeSet<String>();\n        try {\n          for (String monitorTarget : monitorTargets) {\n            pattern = Pattern.compile(monitorTarget);\n            tds = this.admin.listTables(pattern);\n            if (tds != null) {\n              for (HTableDescriptor td : tds) {\n                tmpTables.add(td.getNameAsString());\n              }\n            }\n          }\n        } catch (IOException e) {\n          LOG.error(\"Communicate with admin failed\", e);\n          throw e;\n        }\n\n        if (tmpTables.size() > 0) {\n          returnTables = tmpTables.toArray(new String[tmpTables.size()]);\n        } else {\n          String msg = \"No HTable found, tablePattern:\" + Arrays.toString(monitorTargets);\n          LOG.error(msg);\n          this.errorCode = INIT_ERROR_EXIT_CODE;\n          throw new TableNotFoundException(msg);\n        }\n      } else {\n        returnTables = monitorTargets;\n      }\n\n      return returnTables;\n    }",
            " 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816 +\n 817 +\n 818 +\n 819 +\n 820 +\n 821 +\n 822 +\n 823  \n 824  \n 825 +\n 826 +\n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  ",
            "    private String[] generateMonitorTables(String[] monitorTargets) throws IOException {\n      String[] returnTables = null;\n\n      if (this.useRegExp) {\n        Pattern pattern = null;\n        HTableDescriptor[] tds = null;\n        Set<String> tmpTables = new TreeSet<String>();\n        try {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(String.format(\"reading list of tables\"));\n          }\n          tds = this.admin.listTables(pattern);\n          if (tds == null) {\n            tds = new HTableDescriptor[0];\n          }\n          for (String monitorTarget : monitorTargets) {\n            pattern = Pattern.compile(monitorTarget);\n            for (HTableDescriptor td : tds) {\n              if (pattern.matcher(td.getNameAsString()).matches()) {\n                tmpTables.add(td.getNameAsString());\n              }\n            }\n          }\n        } catch (IOException e) {\n          LOG.error(\"Communicate with admin failed\", e);\n          throw e;\n        }\n\n        if (tmpTables.size() > 0) {\n          returnTables = tmpTables.toArray(new String[tmpTables.size()]);\n        } else {\n          String msg = \"No HTable found, tablePattern:\" + Arrays.toString(monitorTargets);\n          LOG.error(msg);\n          this.errorCode = INIT_ERROR_EXIT_CODE;\n          throw new TableNotFoundException(msg);\n        }\n      } else {\n        returnTables = monitorTargets;\n      }\n\n      return returnTables;\n    }"
        ]
    ],
    "1942a99b831bb4c41c0e09d6b93df5e1d060f58e": [
        [
            "TableMapReduceUtil::convertScanToString(Scan)",
            " 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 -\n 565  \n 566  \n 567  ",
            "  /**\n   * Writes the given scan into a Base64 encoded string.\n   *\n   * @param scan  The scan to write out.\n   * @return The scan saved in a Base64 encoded string.\n   * @throws IOException When writing the scan fails.\n   */\n  static String convertScanToString(Scan scan) throws IOException {\n    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);\n    return Base64.encodeBytes(proto.toByteArray());\n  }",
            " 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 +\n 565  \n 566  \n 567  ",
            "  /**\n   * Writes the given scan into a Base64 encoded string.\n   *\n   * @param scan  The scan to write out.\n   * @return The scan saved in a Base64 encoded string.\n   * @throws IOException When writing the scan fails.\n   */\n  public static String convertScanToString(Scan scan) throws IOException {\n    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);\n    return Base64.encodeBytes(proto.toByteArray());\n  }"
        ],
        [
            "TableSplit::toString()",
            " 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 -\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  ",
            "  /**\n   * Returns the details about this instance as a string.\n   *\n   * @return The values of this instance as a string.\n   * @see java.lang.Object#toString()\n   */\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"HBase table split(\");\n    sb.append(\"table name: \").append(tableName);\n    sb.append(\", scan: \").append(scan);\n    sb.append(\", start row: \").append(Bytes.toStringBinary(startRow));\n    sb.append(\", end row: \").append(Bytes.toStringBinary(endRow));\n    sb.append(\", region location: \").append(regionLocation);\n    sb.append(\")\");\n    return sb.toString();\n  }",
            " 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305 +\n 306 +\n 307 +\n 308 +\n 309 +\n 310 +\n 311 +\n 312 +\n 313 +\n 314 +\n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  ",
            "  /**\n   * Returns the details about this instance as a string.\n   *\n   * @return The values of this instance as a string.\n   * @see java.lang.Object#toString()\n   */\n  @Override\n  public String toString() {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"HBase table split(\");\n    sb.append(\"table name: \").append(tableName);\n    // null scan input is represented by \"\"\n    String printScan = \"\";\n    if (!scan.equals(\"\")) {\n      try {\n        // get the real scan here in toString, not the Base64 string\n        printScan = TableMapReduceUtil.convertStringToScan(scan).toString();\n      }\n      catch (IOException e) {\n        printScan = \"\";\n      }\n    }\n    sb.append(\", scan: \").append(printScan);\n    sb.append(\", start row: \").append(Bytes.toStringBinary(startRow));\n    sb.append(\", end row: \").append(Bytes.toStringBinary(endRow));\n    sb.append(\", region location: \").append(regionLocation);\n    sb.append(\")\");\n    return sb.toString();\n  }"
        ],
        [
            "TableMapReduceUtil::convertStringToScan(String)",
            " 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576 -\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  ",
            "  /**\n   * Converts the given Base64 string back into a Scan instance.\n   *\n   * @param base64  The scan details.\n   * @return The newly created Scan instance.\n   * @throws IOException When reading the scan instance fails.\n   */\n  static Scan convertStringToScan(String base64) throws IOException {\n    byte [] decoded = Base64.decode(base64);\n    ClientProtos.Scan scan;\n    try {\n      scan = ClientProtos.Scan.parseFrom(decoded);\n    } catch (InvalidProtocolBufferException ipbe) {\n      throw new IOException(ipbe);\n    }\n\n    return ProtobufUtil.toScan(scan);\n  }",
            " 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576 +\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  ",
            "  /**\n   * Converts the given Base64 string back into a Scan instance.\n   *\n   * @param base64  The scan details.\n   * @return The newly created Scan instance.\n   * @throws IOException When reading the scan instance fails.\n   */\n  public static Scan convertStringToScan(String base64) throws IOException {\n    byte [] decoded = Base64.decode(base64);\n    ClientProtos.Scan scan;\n    try {\n      scan = ClientProtos.Scan.parseFrom(decoded);\n    } catch (InvalidProtocolBufferException ipbe) {\n      throw new IOException(ipbe);\n    }\n\n    return ProtobufUtil.toScan(scan);\n  }"
        ],
        [
            "TableInputFormatBase::calculateRebalancedSplits(List,JobContext,long)",
            " 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 -\n 401  \n 402 -\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429 -\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "  /**\n   * Calculates the number of MapReduce input splits for the map tasks. The number of\n   * MapReduce input splits depends on the average region size and the \"data skew ratio\" user set in\n   * configuration.\n   *\n   * @param list  The list of input splits before balance.\n   * @param context  The current job context.\n   * @param average  The average size of all regions .\n   * @return The list of input splits.\n   * @throws IOException When creating the list of splits fails.\n   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits(\n   *   org.apache.hadoop.mapreduce.JobContext)\n   */\n  private List<InputSplit> calculateRebalancedSplits(List<InputSplit> list, JobContext context,\n                                               long average) throws IOException {\n    List<InputSplit> resultList = new ArrayList<InputSplit>();\n    Configuration conf = context.getConfiguration();\n    //The default data skew ratio is 3\n    long dataSkewRatio = conf.getLong(INPUT_AUTOBALANCE_MAXSKEWRATIO, 3);\n    //It determines which mode to use: text key mode or binary key mode. The default is text mode.\n    boolean isTextKey = context.getConfiguration().getBoolean(TABLE_ROW_TEXTKEY, true);\n    long dataSkewThreshold = dataSkewRatio * average;\n    int count = 0;\n    while (count < list.size()) {\n      TableSplit ts = (TableSplit)list.get(count);\n      TableName tableName = ts.getTable();\n      String regionLocation = ts.getRegionLocation();\n      long regionSize = ts.getLength();\n      if (regionSize >= dataSkewThreshold) {\n        // if the current region size is large than the data skew threshold,\n        // split the region into two MapReduce input splits.\n        byte[] splitKey = getSplitKey(ts.getStartRow(), ts.getEndRow(), isTextKey);\n         //Set the size of child TableSplit as 1/2 of the region size. The exact size of the\n         // MapReduce input splits is not far off.\n        TableSplit t1 = new TableSplit(tableName, ts.getStartRow(), splitKey, regionLocation,\n                regionSize / 2);\n        TableSplit t2 = new TableSplit(tableName, splitKey, ts.getEndRow(), regionLocation,\n                regionSize - regionSize / 2);\n        resultList.add(t1);\n        resultList.add(t2);\n        count++;\n      } else if (regionSize >= average) {\n        // if the region size between average size and data skew threshold size,\n        // make this region as one MapReduce input split.\n        resultList.add(ts);\n        count++;\n      } else {\n        // if the total size of several small continuous regions less than the average region size,\n        // combine them into one MapReduce input split.\n        long totalSize = regionSize;\n        byte[] splitStartKey = ts.getStartRow();\n        byte[] splitEndKey = ts.getEndRow();\n        count++;\n        for (; count < list.size(); count++) {\n          TableSplit nextRegion = (TableSplit)list.get(count);\n          long nextRegionSize = nextRegion.getLength();\n          if (totalSize + nextRegionSize <= dataSkewThreshold) {\n            totalSize = totalSize + nextRegionSize;\n            splitEndKey = nextRegion.getEndRow();\n          } else {\n            break;\n          }\n        }\n        TableSplit t = new TableSplit(tableName, splitStartKey, splitEndKey,\n                regionLocation, totalSize);\n        resultList.add(t);\n      }\n    }\n    return resultList;\n  }",
            " 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 +\n 401  \n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429 +\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "  /**\n   * Calculates the number of MapReduce input splits for the map tasks. The number of\n   * MapReduce input splits depends on the average region size and the \"data skew ratio\" user set in\n   * configuration.\n   *\n   * @param list  The list of input splits before balance.\n   * @param context  The current job context.\n   * @param average  The average size of all regions .\n   * @return The list of input splits.\n   * @throws IOException When creating the list of splits fails.\n   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits(\n   *   org.apache.hadoop.mapreduce.JobContext)\n   */\n  private List<InputSplit> calculateRebalancedSplits(List<InputSplit> list, JobContext context,\n                                               long average) throws IOException {\n    List<InputSplit> resultList = new ArrayList<InputSplit>();\n    Configuration conf = context.getConfiguration();\n    //The default data skew ratio is 3\n    long dataSkewRatio = conf.getLong(INPUT_AUTOBALANCE_MAXSKEWRATIO, 3);\n    //It determines which mode to use: text key mode or binary key mode. The default is text mode.\n    boolean isTextKey = context.getConfiguration().getBoolean(TABLE_ROW_TEXTKEY, true);\n    long dataSkewThreshold = dataSkewRatio * average;\n    int count = 0;\n    while (count < list.size()) {\n      TableSplit ts = (TableSplit)list.get(count);\n      TableName tableName = ts.getTable();\n      String regionLocation = ts.getRegionLocation();\n      long regionSize = ts.getLength();\n      if (regionSize >= dataSkewThreshold) {\n        // if the current region size is large than the data skew threshold,\n        // split the region into two MapReduce input splits.\n        byte[] splitKey = getSplitKey(ts.getStartRow(), ts.getEndRow(), isTextKey);\n         //Set the size of child TableSplit as 1/2 of the region size. The exact size of the\n         // MapReduce input splits is not far off.\n        TableSplit t1 = new TableSplit(tableName, scan, ts.getStartRow(), splitKey, regionLocation,\n                regionSize / 2);\n        TableSplit t2 = new TableSplit(tableName, scan, splitKey, ts.getEndRow(), regionLocation,\n                regionSize - regionSize / 2);\n        resultList.add(t1);\n        resultList.add(t2);\n        count++;\n      } else if (regionSize >= average) {\n        // if the region size between average size and data skew threshold size,\n        // make this region as one MapReduce input split.\n        resultList.add(ts);\n        count++;\n      } else {\n        // if the total size of several small continuous regions less than the average region size,\n        // combine them into one MapReduce input split.\n        long totalSize = regionSize;\n        byte[] splitStartKey = ts.getStartRow();\n        byte[] splitEndKey = ts.getEndRow();\n        count++;\n        for (; count < list.size(); count++) {\n          TableSplit nextRegion = (TableSplit)list.get(count);\n          long nextRegionSize = nextRegion.getLength();\n          if (totalSize + nextRegionSize <= dataSkewThreshold) {\n            totalSize = totalSize + nextRegionSize;\n            splitEndKey = nextRegion.getEndRow();\n          } else {\n            break;\n          }\n        }\n        TableSplit t = new TableSplit(tableName, scan, splitStartKey, splitEndKey,\n                regionLocation, totalSize);\n        resultList.add(t);\n      }\n    }\n    return resultList;\n  }"
        ],
        [
            "TableInputFormatBase::getSplits(JobContext)",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 -\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "  /**\n   * Calculates the splits that will serve as input for the map tasks. The\n   * number of splits matches the number of regions in a table.\n   *\n   * @param context  The current job context.\n   * @return The list of input splits.\n   * @throws IOException When creating the list of splits fails.\n   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits(\n   *   org.apache.hadoop.mapreduce.JobContext)\n   */\n  @Override\n  public List<InputSplit> getSplits(JobContext context) throws IOException {\n    boolean closeOnFinish = false;\n\n    // Just in case a subclass is relying on JobConfigurable magic.\n    if (table == null) {\n      initialize(context);\n      closeOnFinish = true;\n    }\n\n    // null check in case our child overrides getTable to not throw.\n    try {\n      if (getTable() == null) {\n        // initialize() must not have been implemented in the subclass.\n        throw new IOException(INITIALIZATION_ERROR);\n      }\n    } catch (IllegalStateException exception) {\n      throw new IOException(INITIALIZATION_ERROR, exception);\n    }\n\n    try {\n      RegionSizeCalculator sizeCalculator =\n          new RegionSizeCalculator(getRegionLocator(), getAdmin());\n      \n      TableName tableName = getTable().getName();\n  \n      Pair<byte[][], byte[][]> keys = getStartEndKeys();\n      if (keys == null || keys.getFirst() == null ||\n          keys.getFirst().length == 0) {\n        HRegionLocation regLoc =\n            getRegionLocator().getRegionLocation(HConstants.EMPTY_BYTE_ARRAY, false);\n        if (null == regLoc) {\n          throw new IOException(\"Expecting at least one region.\");\n        }\n        List<InputSplit> splits = new ArrayList<InputSplit>(1);\n        long regionSize = sizeCalculator.getRegionSize(regLoc.getRegionInfo().getRegionName());\n        TableSplit split = new TableSplit(tableName,\n            HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY, regLoc\n                .getHostnamePort().split(Addressing.HOSTNAME_PORT_SEPARATOR)[0], regionSize);\n        splits.add(split);\n        return splits;\n      }\n      List<InputSplit> splits = new ArrayList<InputSplit>(keys.getFirst().length);\n      for (int i = 0; i < keys.getFirst().length; i++) {\n        if (!includeRegionInSplit(keys.getFirst()[i], keys.getSecond()[i])) {\n          continue;\n        }\n        HRegionLocation location = getRegionLocator().getRegionLocation(keys.getFirst()[i], false);\n        // The below InetSocketAddress creation does a name resolution.\n        InetSocketAddress isa = new InetSocketAddress(location.getHostname(), location.getPort());\n        if (isa.isUnresolved()) {\n          LOG.warn(\"Failed resolve \" + isa);\n        }\n        InetAddress regionAddress = isa.getAddress();\n        String regionLocation;\n        try {\n          regionLocation = reverseDNS(regionAddress);\n        } catch (NamingException e) {\n          LOG.warn(\"Cannot resolve the host name for \" + regionAddress + \" because of \" + e);\n          regionLocation = location.getHostname();\n        }\n  \n        byte[] startRow = scan.getStartRow();\n        byte[] stopRow = scan.getStopRow();\n        // determine if the given start an stop key fall into the region\n        if ((startRow.length == 0 || keys.getSecond()[i].length == 0 ||\n            Bytes.compareTo(startRow, keys.getSecond()[i]) < 0) &&\n            (stopRow.length == 0 ||\n             Bytes.compareTo(stopRow, keys.getFirst()[i]) > 0)) {\n          byte[] splitStart = startRow.length == 0 ||\n            Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ?\n              keys.getFirst()[i] : startRow;\n          byte[] splitStop = (stopRow.length == 0 ||\n            Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) &&\n            keys.getSecond()[i].length > 0 ?\n              keys.getSecond()[i] : stopRow;\n  \n          byte[] regionName = location.getRegionInfo().getRegionName();\n          long regionSize = sizeCalculator.getRegionSize(regionName);\n          TableSplit split = new TableSplit(tableName,\n            splitStart, splitStop, regionLocation, regionSize);\n          splits.add(split);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getSplits: split -> \" + i + \" -> \" + split);\n          }\n        }\n      }\n      //The default value of \"hbase.mapreduce.input.autobalance\" is false, which means not enabled.\n      boolean enableAutoBalance = context.getConfiguration()\n        .getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false);\n      if (enableAutoBalance) {\n        long totalRegionSize=0;\n        for (int i = 0; i < splits.size(); i++){\n          TableSplit ts = (TableSplit)splits.get(i);\n          totalRegionSize += ts.getLength();\n        }\n        long averageRegionSize = totalRegionSize / splits.size();\n        // the averageRegionSize must be positive.\n        if (averageRegionSize <= 0) {\n            LOG.warn(\"The averageRegionSize is not positive: \"+ averageRegionSize + \", \" +\n                    \"set it to 1.\");\n            averageRegionSize = 1;\n        }\n        return calculateRebalancedSplits(splits, context, averageRegionSize);\n      } else {\n        return splits;\n      }\n    } finally {\n      if (closeOnFinish) {\n        closeTable();\n      }\n    }\n  }",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 +\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "  /**\n   * Calculates the splits that will serve as input for the map tasks. The\n   * number of splits matches the number of regions in a table.\n   *\n   * @param context  The current job context.\n   * @return The list of input splits.\n   * @throws IOException When creating the list of splits fails.\n   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits(\n   *   org.apache.hadoop.mapreduce.JobContext)\n   */\n  @Override\n  public List<InputSplit> getSplits(JobContext context) throws IOException {\n    boolean closeOnFinish = false;\n\n    // Just in case a subclass is relying on JobConfigurable magic.\n    if (table == null) {\n      initialize(context);\n      closeOnFinish = true;\n    }\n\n    // null check in case our child overrides getTable to not throw.\n    try {\n      if (getTable() == null) {\n        // initialize() must not have been implemented in the subclass.\n        throw new IOException(INITIALIZATION_ERROR);\n      }\n    } catch (IllegalStateException exception) {\n      throw new IOException(INITIALIZATION_ERROR, exception);\n    }\n\n    try {\n      RegionSizeCalculator sizeCalculator =\n          new RegionSizeCalculator(getRegionLocator(), getAdmin());\n      \n      TableName tableName = getTable().getName();\n  \n      Pair<byte[][], byte[][]> keys = getStartEndKeys();\n      if (keys == null || keys.getFirst() == null ||\n          keys.getFirst().length == 0) {\n        HRegionLocation regLoc =\n            getRegionLocator().getRegionLocation(HConstants.EMPTY_BYTE_ARRAY, false);\n        if (null == regLoc) {\n          throw new IOException(\"Expecting at least one region.\");\n        }\n        List<InputSplit> splits = new ArrayList<InputSplit>(1);\n        long regionSize = sizeCalculator.getRegionSize(regLoc.getRegionInfo().getRegionName());\n        TableSplit split = new TableSplit(tableName, scan,\n            HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY, regLoc\n                .getHostnamePort().split(Addressing.HOSTNAME_PORT_SEPARATOR)[0], regionSize);\n        splits.add(split);\n        return splits;\n      }\n      List<InputSplit> splits = new ArrayList<InputSplit>(keys.getFirst().length);\n      for (int i = 0; i < keys.getFirst().length; i++) {\n        if (!includeRegionInSplit(keys.getFirst()[i], keys.getSecond()[i])) {\n          continue;\n        }\n        HRegionLocation location = getRegionLocator().getRegionLocation(keys.getFirst()[i], false);\n        // The below InetSocketAddress creation does a name resolution.\n        InetSocketAddress isa = new InetSocketAddress(location.getHostname(), location.getPort());\n        if (isa.isUnresolved()) {\n          LOG.warn(\"Failed resolve \" + isa);\n        }\n        InetAddress regionAddress = isa.getAddress();\n        String regionLocation;\n        try {\n          regionLocation = reverseDNS(regionAddress);\n        } catch (NamingException e) {\n          LOG.warn(\"Cannot resolve the host name for \" + regionAddress + \" because of \" + e);\n          regionLocation = location.getHostname();\n        }\n  \n        byte[] startRow = scan.getStartRow();\n        byte[] stopRow = scan.getStopRow();\n        // determine if the given start an stop key fall into the region\n        if ((startRow.length == 0 || keys.getSecond()[i].length == 0 ||\n            Bytes.compareTo(startRow, keys.getSecond()[i]) < 0) &&\n            (stopRow.length == 0 ||\n             Bytes.compareTo(stopRow, keys.getFirst()[i]) > 0)) {\n          byte[] splitStart = startRow.length == 0 ||\n            Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ?\n              keys.getFirst()[i] : startRow;\n          byte[] splitStop = (stopRow.length == 0 ||\n            Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) &&\n            keys.getSecond()[i].length > 0 ?\n              keys.getSecond()[i] : stopRow;\n  \n          byte[] regionName = location.getRegionInfo().getRegionName();\n          long regionSize = sizeCalculator.getRegionSize(regionName);\n          TableSplit split = new TableSplit(tableName, scan,\n            splitStart, splitStop, regionLocation, regionSize);\n          splits.add(split);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getSplits: split -> \" + i + \" -> \" + split);\n          }\n        }\n      }\n      //The default value of \"hbase.mapreduce.input.autobalance\" is false, which means not enabled.\n      boolean enableAutoBalance = context.getConfiguration()\n        .getBoolean(MAPREDUCE_INPUT_AUTOBALANCE, false);\n      if (enableAutoBalance) {\n        long totalRegionSize=0;\n        for (int i = 0; i < splits.size(); i++){\n          TableSplit ts = (TableSplit)splits.get(i);\n          totalRegionSize += ts.getLength();\n        }\n        long averageRegionSize = totalRegionSize / splits.size();\n        // the averageRegionSize must be positive.\n        if (averageRegionSize <= 0) {\n            LOG.warn(\"The averageRegionSize is not positive: \"+ averageRegionSize + \", \" +\n                    \"set it to 1.\");\n            averageRegionSize = 1;\n        }\n        return calculateRebalancedSplits(splits, context, averageRegionSize);\n      } else {\n        return splits;\n      }\n    } finally {\n      if (closeOnFinish) {\n        closeTable();\n      }\n    }\n  }"
        ]
    ],
    "664575598ea81fe2a53d8d88ead4ce318ff89d86": [
        [
            "ReplicationPeersZKImpl::addPeer(String,ReplicationPeerConfig,String)",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129 -\n 130 -\n 131 -\n 132 -\n 133 -\n 134 -\n 135 -\n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  @Override\n  public void addPeer(String id, ReplicationPeerConfig peerConfig, String tableCFs)\n      throws ReplicationException {\n    try {\n      if (peerExists(id)) {\n        throw new IllegalArgumentException(\"Cannot add a peer with id=\" + id\n            + \" because that id already exists.\");\n      }\n\n      if(id.contains(\"-\")){\n        throw new IllegalArgumentException(\"Found invalid peer name:\" + id);\n      }\n\n      checkQueuesDeleted(id);\n\n      ZKUtil.createWithParents(this.zookeeper, this.peersZNode);\n\n      // If only bulk load hfile replication is enabled then add peerId node to hfile-refs node\n      if (replicationForBulkLoadEnabled) {\n        try {\n          String peerId = ZKUtil.joinZNode(this.hfileRefsZNode, id);\n          LOG.info(\"Adding peer \" + peerId + \" to hfile reference queue.\");\n          ZKUtil.createWithParents(this.zookeeper, peerId);\n        } catch (KeeperException e) {\n          throw new ReplicationException(\"Failed to add peer with id=\" + id\n              + \", node under hfile references node.\", e);\n        }\n      }\n\n      List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>();\n      ZKUtilOp op1 = ZKUtilOp.createAndFailSilent(ZKUtil.joinZNode(this.peersZNode, id),\n        toByteArray(peerConfig));\n      // There is a race (if hbase.zookeeper.useMulti is false)\n      // b/w PeerWatcher and ReplicationZookeeper#add method to create the\n      // peer-state znode. This happens while adding a peer\n      // The peer state data is set as \"ENABLED\" by default.\n      ZKUtilOp op2 = ZKUtilOp.createAndFailSilent(getPeerStateNode(id), ENABLED_ZNODE_BYTES);\n      String tableCFsStr = (tableCFs == null) ? \"\" : tableCFs;\n      ZKUtilOp op3 = ZKUtilOp.createAndFailSilent(getTableCFsNode(id), Bytes.toBytes(tableCFsStr));\n      listOfOps.add(op1);\n      listOfOps.add(op2);\n      listOfOps.add(op3);\n      ZKUtil.multiOrSequential(this.zookeeper, listOfOps, false);\n      // A peer is enabled by default\n    } catch (KeeperException e) {\n      throw new ReplicationException(\"Could not add peer with id=\" + id\n          + \", peerConfif=>\" + peerConfig, e);\n    }\n  }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "  @Override\n  public void addPeer(String id, ReplicationPeerConfig peerConfig, String tableCFs)\n      throws ReplicationException {\n    try {\n      if (peerExists(id)) {\n        throw new IllegalArgumentException(\"Cannot add a peer with id=\" + id\n            + \" because that id already exists.\");\n      }\n\n      if(id.contains(\"-\")){\n        throw new IllegalArgumentException(\"Found invalid peer name:\" + id);\n      }\n\n      checkQueuesDeleted(id);\n\n      ZKUtil.createWithParents(this.zookeeper, this.peersZNode);\n\n      // Irrespective of bulk load hfile replication is enabled or not we add peerId node to\n      // hfile-refs node -- HBASE-15397\n      try {\n        String peerId = ZKUtil.joinZNode(this.hfileRefsZNode, id);\n        LOG.info(\"Adding peer \" + peerId + \" to hfile reference queue.\");\n        ZKUtil.createWithParents(this.zookeeper, peerId);\n      } catch (KeeperException e) {\n        throw new ReplicationException(\"Failed to add peer with id=\" + id\n            + \", node under hfile references node.\", e);\n      }\n\n      List<ZKUtilOp> listOfOps = new ArrayList<ZKUtil.ZKUtilOp>();\n      ZKUtilOp op1 = ZKUtilOp.createAndFailSilent(ZKUtil.joinZNode(this.peersZNode, id),\n        toByteArray(peerConfig));\n      // There is a race (if hbase.zookeeper.useMulti is false)\n      // b/w PeerWatcher and ReplicationZookeeper#add method to create the\n      // peer-state znode. This happens while adding a peer\n      // The peer state data is set as \"ENABLED\" by default.\n      ZKUtilOp op2 = ZKUtilOp.createAndFailSilent(getPeerStateNode(id), ENABLED_ZNODE_BYTES);\n      String tableCFsStr = (tableCFs == null) ? \"\" : tableCFs;\n      ZKUtilOp op3 = ZKUtilOp.createAndFailSilent(getTableCFsNode(id), Bytes.toBytes(tableCFsStr));\n      listOfOps.add(op1);\n      listOfOps.add(op2);\n      listOfOps.add(op3);\n      ZKUtil.multiOrSequential(this.zookeeper, listOfOps, false);\n      // A peer is enabled by default\n    } catch (KeeperException e) {\n      throw new ReplicationException(\"Could not add peer with id=\" + id\n          + \", peerConfif=>\" + peerConfig, e);\n    }\n  }"
        ],
        [
            "ReplicationStateZKBase::ReplicationStateZKBase(ZooKeeperWatcher,Configuration,Abortable)",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  ",
            "  public ReplicationStateZKBase(ZooKeeperWatcher zookeeper, Configuration conf,\n      Abortable abortable) {\n    this.zookeeper = zookeeper;\n    this.conf = conf;\n    this.abortable = abortable;\n    this.replicationForBulkLoadEnabled = conf.getBoolean(HConstants.REPLICATION_BULKLOAD_ENABLE_KEY,\n      HConstants.REPLICATION_BULKLOAD_ENABLE_DEFAULT);\n\n    String replicationZNodeName = conf.get(\"zookeeper.znode.replication\", \"replication\");\n    String peersZNodeName = conf.get(\"zookeeper.znode.replication.peers\", \"peers\");\n    String queuesZNodeName = conf.get(\"zookeeper.znode.replication.rs\", \"rs\");\n    String hfileRefsZNodeName = conf.get(ZOOKEEPER_ZNODE_REPLICATION_HFILE_REFS_KEY,\n      ZOOKEEPER_ZNODE_REPLICATION_HFILE_REFS_DEFAULT);\n    this.peerStateNodeName = conf.get(\"zookeeper.znode.replication.peers.state\", \"peer-state\");\n    this.ourClusterKey = ZKConfig.getZooKeeperClusterKey(this.conf);\n    this.replicationZNode = ZKUtil.joinZNode(this.zookeeper.baseZNode, replicationZNodeName);\n    this.peersZNode = ZKUtil.joinZNode(replicationZNode, peersZNodeName);\n    this.queuesZNode = ZKUtil.joinZNode(replicationZNode, queuesZNodeName);\n    this.hfileRefsZNode = ZKUtil.joinZNode(replicationZNode, hfileRefsZNodeName);\n  }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  ",
            "  public ReplicationStateZKBase(ZooKeeperWatcher zookeeper, Configuration conf,\n      Abortable abortable) {\n    this.zookeeper = zookeeper;\n    this.conf = conf;\n    this.abortable = abortable;\n\n    String replicationZNodeName = conf.get(\"zookeeper.znode.replication\", \"replication\");\n    String peersZNodeName = conf.get(\"zookeeper.znode.replication.peers\", \"peers\");\n    String queuesZNodeName = conf.get(\"zookeeper.znode.replication.rs\", \"rs\");\n    String hfileRefsZNodeName = conf.get(ZOOKEEPER_ZNODE_REPLICATION_HFILE_REFS_KEY,\n      ZOOKEEPER_ZNODE_REPLICATION_HFILE_REFS_DEFAULT);\n    this.peerStateNodeName = conf.get(\"zookeeper.znode.replication.peers.state\", \"peer-state\");\n    this.ourClusterKey = ZKConfig.getZooKeeperClusterKey(this.conf);\n    this.replicationZNode = ZKUtil.joinZNode(this.zookeeper.baseZNode, replicationZNodeName);\n    this.peersZNode = ZKUtil.joinZNode(replicationZNode, peersZNodeName);\n    this.queuesZNode = ZKUtil.joinZNode(replicationZNode, queuesZNodeName);\n    this.hfileRefsZNode = ZKUtil.joinZNode(replicationZNode, hfileRefsZNodeName);\n  }"
        ],
        [
            "ReplicationQueuesZKImpl::init(String)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88 -\n  89 -\n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95  \n  96  ",
            "  @Override\n  public void init(String serverName) throws ReplicationException {\n    this.myQueuesZnode = ZKUtil.joinZNode(this.queuesZNode, serverName);\n    try {\n      ZKUtil.createWithParents(this.zookeeper, this.myQueuesZnode);\n    } catch (KeeperException e) {\n      throw new ReplicationException(\"Could not initialize replication queues.\", e);\n    }\n    // If only bulk load hfile replication is enabled then create the hfile-refs znode\n    if (replicationForBulkLoadEnabled) {\n      try {\n        ZKUtil.createWithParents(this.zookeeper, this.hfileRefsZNode);\n      } catch (KeeperException e) {\n        throw new ReplicationException(\"Could not initialize hfile references replication queue.\",\n            e);\n      }\n    }\n  }",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93  \n  94  ",
            "  @Override\n  public void init(String serverName) throws ReplicationException {\n    this.myQueuesZnode = ZKUtil.joinZNode(this.queuesZNode, serverName);\n    try {\n      ZKUtil.createWithParents(this.zookeeper, this.myQueuesZnode);\n    } catch (KeeperException e) {\n      throw new ReplicationException(\"Could not initialize replication queues.\", e);\n    }\n    // Irrespective of bulk load hfile replication is enabled or not we add peerId node to\n    // hfile-refs node -- HBASE-15397\n    try {\n      ZKUtil.createWithParents(this.zookeeper, this.hfileRefsZNode);\n    } catch (KeeperException e) {\n      throw new ReplicationException(\"Could not initialize hfile references replication queue.\", e);\n    }\n  }"
        ]
    ],
    "cbf9c1e116717972c243abedb53e7d26ef03cc85": [
        [
            "CopyTable::createSubmittableJob(String)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "  /**\n   * Sets up the actual job.\n   *\n   * @param args  The command line parameters.\n   * @return The newly created job.\n   * @throws IOException When setting up the job fails.\n   */\n  public Job createSubmittableJob(String[] args)\n  throws IOException {\n    if (!doCommandLine(args)) {\n      return null;\n    }\n\n    Job job = Job.getInstance(getConf(), getConf().get(JOB_NAME_CONF_KEY, NAME + \"_\" + tableName));\n    job.setJarByClass(CopyTable.class);\n    Scan scan = new Scan();\n    scan.setCacheBlocks(false);\n    if (startTime != 0) {\n      scan.setTimeRange(startTime,\n          endTime == 0 ? HConstants.LATEST_TIMESTAMP : endTime);\n    }\n    if (allCells) {\n      scan.setRaw(true);\n    }\n    if (shuffle) {\n      job.getConfiguration().set(TableInputFormat.SHUFFLE_MAPS, \"true\");\n    }\n    if (versions >= 0) {\n      scan.setMaxVersions(versions);\n    }\n\n    if (startRow != null) {\n      scan.setStartRow(Bytes.toBytes(startRow));\n    }\n\n    if (stopRow != null) {\n      scan.setStopRow(Bytes.toBytes(stopRow));\n    }\n\n    if(families != null) {\n      String[] fams = families.split(\",\");\n      Map<String,String> cfRenameMap = new HashMap<String,String>();\n      for(String fam : fams) {\n        String sourceCf;\n        if(fam.contains(\":\")) {\n            // fam looks like \"sourceCfName:destCfName\"\n            String[] srcAndDest = fam.split(\":\", 2);\n            sourceCf = srcAndDest[0];\n            String destCf = srcAndDest[1];\n            cfRenameMap.put(sourceCf, destCf);\n        } else {\n            // fam is just \"sourceCf\"\n            sourceCf = fam;\n        }\n        scan.addFamily(Bytes.toBytes(sourceCf));\n      }\n      Import.configureCfRenaming(job.getConfiguration(), cfRenameMap);\n    }\n    job.setNumReduceTasks(0);\n\n    if (bulkload) {\n      TableMapReduceUtil.initTableMapperJob(tableName, scan, Import.KeyValueImporter.class, null,\n        null, job);\n\n      // We need to split the inputs by destination tables so that output of Map can be bulk-loaded.\n      TableInputFormat.configureSplitTable(job, TableName.valueOf(dstTableName));\n\n      FileSystem fs = FileSystem.get(getConf());\n      Random rand = new Random();\n      Path root = new Path(fs.getWorkingDirectory(), \"copytable\");\n      fs.mkdirs(root);\n      while (true) {\n        bulkloadDir = new Path(root, \"\" + rand.nextLong());\n        if (!fs.exists(bulkloadDir)) {\n          break;\n        }\n      }\n\n      System.out.println(\"HFiles will be stored at \" + this.bulkloadDir);\n      HFileOutputFormat2.setOutputPath(job, bulkloadDir);\n      try (Connection conn = ConnectionFactory.createConnection(getConf());\n          Admin admin = conn.getAdmin()) {\n        HFileOutputFormat2.configureIncrementalLoadMap(job,\n            admin.getTableDescriptor((TableName.valueOf(dstTableName))));\n      }\n    } else {\n      TableMapReduceUtil.initTableMapperJob(tableName, scan,\n        Import.Importer.class, null, null, job);\n\n      TableMapReduceUtil.initTableReducerJob(dstTableName, null, job, null, peerAddress, null,\n        null);\n    }\n\n    return job;\n  }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91 +\n  92 +\n  93  \n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99  \n 100 +\n 101 +\n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  /**\n   * Sets up the actual job.\n   *\n   * @param args  The command line parameters.\n   * @return The newly created job.\n   * @throws IOException When setting up the job fails.\n   */\n  public Job createSubmittableJob(String[] args)\n  throws IOException {\n    if (!doCommandLine(args)) {\n      return null;\n    }\n    \n    Job job = Job.getInstance(getConf(), getConf().get(JOB_NAME_CONF_KEY, NAME + \"_\" + tableName));\n    job.setJarByClass(CopyTable.class);\n    Scan scan = new Scan();\n    \n    scan.setBatch(batch);\n    scan.setCacheBlocks(false);\n    \n    if (cacheRow > 0) {\n      scan.setCaching(cacheRow);\n    } else {\n      scan.setCaching(getConf().getInt(HConstants.HBASE_CLIENT_SCANNER_CACHING, 100));\n    }\n    \n    scan.setTimeRange(startTime, endTime);\n    \n    if (allCells) {\n      scan.setRaw(true);\n    }\n    if (shuffle) {\n      job.getConfiguration().set(TableInputFormat.SHUFFLE_MAPS, \"true\");\n    }\n    if (versions >= 0) {\n      scan.setMaxVersions(versions);\n    }\n\n    if (startRow != null) {\n      scan.setStartRow(Bytes.toBytes(startRow));\n    }\n\n    if (stopRow != null) {\n      scan.setStopRow(Bytes.toBytes(stopRow));\n    }\n\n    if(families != null) {\n      String[] fams = families.split(\",\");\n      Map<String,String> cfRenameMap = new HashMap<String,String>();\n      for(String fam : fams) {\n        String sourceCf;\n        if(fam.contains(\":\")) {\n            // fam looks like \"sourceCfName:destCfName\"\n            String[] srcAndDest = fam.split(\":\", 2);\n            sourceCf = srcAndDest[0];\n            String destCf = srcAndDest[1];\n            cfRenameMap.put(sourceCf, destCf);\n        } else {\n            // fam is just \"sourceCf\"\n            sourceCf = fam;\n        }\n        scan.addFamily(Bytes.toBytes(sourceCf));\n      }\n      Import.configureCfRenaming(job.getConfiguration(), cfRenameMap);\n    }\n    job.setNumReduceTasks(0);\n\n    if (bulkload) {\n      TableMapReduceUtil.initTableMapperJob(tableName, scan, Import.KeyValueImporter.class, null,\n        null, job);\n\n      // We need to split the inputs by destination tables so that output of Map can be bulk-loaded.\n      TableInputFormat.configureSplitTable(job, TableName.valueOf(dstTableName));\n\n      FileSystem fs = FileSystem.get(getConf());\n      Random rand = new Random();\n      Path root = new Path(fs.getWorkingDirectory(), \"copytable\");\n      fs.mkdirs(root);\n      while (true) {\n        bulkloadDir = new Path(root, \"\" + rand.nextLong());\n        if (!fs.exists(bulkloadDir)) {\n          break;\n        }\n      }\n\n      System.out.println(\"HFiles will be stored at \" + this.bulkloadDir);\n      HFileOutputFormat2.setOutputPath(job, bulkloadDir);\n      try (Connection conn = ConnectionFactory.createConnection(getConf());\n          Admin admin = conn.getAdmin()) {\n        HFileOutputFormat2.configureIncrementalLoadMap(job,\n            admin.getTableDescriptor((TableName.valueOf(dstTableName))));\n      }\n    } else {\n      TableMapReduceUtil.initTableMapperJob(tableName, scan,\n        Import.Importer.class, null, null, job);\n\n      TableMapReduceUtil.initTableReducerJob(dstTableName, null, job, null, peerAddress, null,\n        null);\n    }\n\n    return job;\n  }"
        ],
        [
            "CopyTable::doCommandLine(String)",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  ",
            "  private boolean doCommandLine(final String[] args) {\n    // Process command-line args. TODO: Better cmd-line processing\n    // (but hopefully something not as painful as cli options).\n    if (args.length < 1) {\n      printUsage(null);\n      return false;\n    }\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startRowArgKey = \"--startrow=\";\n        if (cmd.startsWith(startRowArgKey)) {\n          startRow = cmd.substring(startRowArgKey.length());\n          continue;\n        }\n\n        final String stopRowArgKey = \"--stoprow=\";\n        if (cmd.startsWith(stopRowArgKey)) {\n          stopRow = cmd.substring(stopRowArgKey.length());\n          continue;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n\n        final String newNameArgKey = \"--new.name=\";\n        if (cmd.startsWith(newNameArgKey)) {\n          dstTableName = cmd.substring(newNameArgKey.length());\n          continue;\n        }\n\n        final String peerAdrArgKey = \"--peer.adr=\";\n        if (cmd.startsWith(peerAdrArgKey)) {\n          peerAddress = cmd.substring(peerAdrArgKey.length());\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--all.cells\")) {\n          allCells = true;\n          continue;\n        }\n\n        if (cmd.startsWith(\"--bulkload\")) {\n          bulkload = true;\n          continue;\n        }\n\n        if (cmd.startsWith(\"--shuffle\")) {\n          shuffle = true;\n          continue;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        } else {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n          return false;\n        }\n      }\n      if (dstTableName == null && peerAddress == null) {\n        printUsage(\"At least a new table name or a \" +\n            \"peer address must be specified\");\n        return false;\n      }\n      if ((endTime != 0) && (startTime > endTime)) {\n        printUsage(\"Invalid time range filter: starttime=\" + startTime + \" >  endtime=\" + endTime);\n        return false;\n      }\n\n      if (bulkload && peerAddress != null) {\n        printUsage(\"Remote bulkload is not supported!\");\n        return false;\n      }\n\n      // set dstTableName if necessary\n      if (dstTableName == null) {\n        dstTableName = tableName;\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273 +\n 274 +\n 275 +\n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "  private boolean doCommandLine(final String[] args) {\n    // Process command-line args. TODO: Better cmd-line processing\n    // (but hopefully something not as painful as cli options).\n    if (args.length < 1) {\n      printUsage(null);\n      return false;\n    }\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startRowArgKey = \"--startrow=\";\n        if (cmd.startsWith(startRowArgKey)) {\n          startRow = cmd.substring(startRowArgKey.length());\n          continue;\n        }\n\n        final String stopRowArgKey = \"--stoprow=\";\n        if (cmd.startsWith(stopRowArgKey)) {\n          stopRow = cmd.substring(stopRowArgKey.length());\n          continue;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n        \n        final String cacheRowArgKey = \"--cacheRow=\";\n        if (cmd.startsWith(cacheRowArgKey)) {\n          cacheRow = Integer.parseInt(cmd.substring(cacheRowArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n\n        final String newNameArgKey = \"--new.name=\";\n        if (cmd.startsWith(newNameArgKey)) {\n          dstTableName = cmd.substring(newNameArgKey.length());\n          continue;\n        }\n\n        final String peerAdrArgKey = \"--peer.adr=\";\n        if (cmd.startsWith(peerAdrArgKey)) {\n          peerAddress = cmd.substring(peerAdrArgKey.length());\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--all.cells\")) {\n          allCells = true;\n          continue;\n        }\n\n        if (cmd.startsWith(\"--bulkload\")) {\n          bulkload = true;\n          continue;\n        }\n\n        if (cmd.startsWith(\"--shuffle\")) {\n          shuffle = true;\n          continue;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        } else {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n          return false;\n        }\n      }\n      if (dstTableName == null && peerAddress == null) {\n        printUsage(\"At least a new table name or a \" +\n            \"peer address must be specified\");\n        return false;\n      }\n      if ((endTime != 0) && (startTime > endTime)) {\n        printUsage(\"Invalid time range filter: starttime=\" + startTime + \" >  endtime=\" + endTime);\n        return false;\n      }\n\n      if (bulkload && peerAddress != null) {\n        printUsage(\"Remote bulkload is not supported!\");\n        return false;\n      }\n\n      // set dstTableName if necessary\n      if (dstTableName == null) {\n        dstTableName = tableName;\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "VerifyReplication::doCommandLine(String)",
            " 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }",
            " 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349 +\n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "VerifyReplication::Verifier::map(ImmutableBytesWritable,Result,Context)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "    /**\n     * Map method that compares every scanned row with the equivalent from\n     * a distant cluster.\n     * @param row  The current table row key.\n     * @param value  The columns.\n     * @param context  The current context.\n     * @throws IOException When something is broken with the data.\n     */\n    @Override\n    public void map(ImmutableBytesWritable row, final Result value,\n                    Context context)\n        throws IOException {\n      if (replicatedScanner == null) {\n        Configuration conf = context.getConfiguration();\n        final Scan scan = new Scan();\n        scan.setCaching(conf.getInt(TableInputFormat.SCAN_CACHEDROWS, 1));\n        long startTime = conf.getLong(NAME + \".startTime\", 0);\n        long endTime = conf.getLong(NAME + \".endTime\", Long.MAX_VALUE);\n        String families = conf.get(NAME + \".families\", null);\n        if(families != null) {\n          String[] fams = families.split(\",\");\n          for(String fam : fams) {\n            scan.addFamily(Bytes.toBytes(fam));\n          }\n        }\n        scan.setTimeRange(startTime, endTime);\n        int versions = conf.getInt(NAME+\".versions\", -1);\n        LOG.info(\"Setting number of version inside map as: \" + versions);\n        if (versions >= 0) {\n          scan.setMaxVersions(versions);\n        }\n\n        final TableSplit tableSplit = (TableSplit)(context.getInputSplit());\n\n        String zkClusterKey = conf.get(NAME + \".peerQuorumAddress\");\n        Configuration peerConf = HBaseConfiguration.createClusterConf(conf,\n            zkClusterKey, PEER_CONFIG_PREFIX);\n\n        TableName tableName = TableName.valueOf(conf.get(NAME + \".tableName\"));\n        connection = ConnectionFactory.createConnection(peerConf);\n        replicatedTable = connection.getTable(tableName);\n        scan.setStartRow(value.getRow());\n        scan.setStopRow(tableSplit.getEndRow());\n        replicatedScanner = replicatedTable.getScanner(scan);\n        currentCompareRowInPeerTable = replicatedScanner.next();\n      }\n      while (true) {\n        if (currentCompareRowInPeerTable == null) {\n          // reach the region end of peer table, row only in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        }\n        int rowCmpRet = Bytes.compareTo(value.getRow(), currentCompareRowInPeerTable.getRow());\n        if (rowCmpRet == 0) {\n          // rowkey is same, need to compare the content of the row\n          try {\n            Result.compareResults(value, currentCompareRowInPeerTable);\n            context.getCounter(Counters.GOODROWS).increment(1);\n          } catch (Exception e) {\n            logFailRowAndIncreaseCounter(context, Counters.CONTENT_DIFFERENT_ROWS, value);\n            LOG.error(\"Exception while comparing row : \" + e);\n          }\n          currentCompareRowInPeerTable = replicatedScanner.next();\n          break;\n        } else if (rowCmpRet < 0) {\n          // row only exists in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        } else {\n          // row only exists in peer table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_PEER_TABLE_ROWS,\n            currentCompareRowInPeerTable);\n          currentCompareRowInPeerTable = replicatedScanner.next();\n        }\n      }\n    }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 +\n 115 +\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    /**\n     * Map method that compares every scanned row with the equivalent from\n     * a distant cluster.\n     * @param row  The current table row key.\n     * @param value  The columns.\n     * @param context  The current context.\n     * @throws IOException When something is broken with the data.\n     */\n    @Override\n    public void map(ImmutableBytesWritable row, final Result value,\n                    Context context)\n        throws IOException {\n      if (replicatedScanner == null) {\n        Configuration conf = context.getConfiguration();\n        final Scan scan = new Scan();\n        scan.setBatch(batch);\n        scan.setCacheBlocks(false);\n        scan.setCaching(conf.getInt(TableInputFormat.SCAN_CACHEDROWS, 1));\n        long startTime = conf.getLong(NAME + \".startTime\", 0);\n        long endTime = conf.getLong(NAME + \".endTime\", Long.MAX_VALUE);\n        String families = conf.get(NAME + \".families\", null);\n        if(families != null) {\n          String[] fams = families.split(\",\");\n          for(String fam : fams) {\n            scan.addFamily(Bytes.toBytes(fam));\n          }\n        }\n        scan.setTimeRange(startTime, endTime);\n        int versions = conf.getInt(NAME+\".versions\", -1);\n        LOG.info(\"Setting number of version inside map as: \" + versions);\n        if (versions >= 0) {\n          scan.setMaxVersions(versions);\n        }\n\n        final TableSplit tableSplit = (TableSplit)(context.getInputSplit());\n\n        String zkClusterKey = conf.get(NAME + \".peerQuorumAddress\");\n        Configuration peerConf = HBaseConfiguration.createClusterConf(conf,\n            zkClusterKey, PEER_CONFIG_PREFIX);\n\n        TableName tableName = TableName.valueOf(conf.get(NAME + \".tableName\"));\n        connection = ConnectionFactory.createConnection(peerConf);\n        replicatedTable = connection.getTable(tableName);\n        scan.setStartRow(value.getRow());\n        scan.setStopRow(tableSplit.getEndRow());\n        replicatedScanner = replicatedTable.getScanner(scan);\n        currentCompareRowInPeerTable = replicatedScanner.next();\n      }\n      while (true) {\n        if (currentCompareRowInPeerTable == null) {\n          // reach the region end of peer table, row only in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        }\n        int rowCmpRet = Bytes.compareTo(value.getRow(), currentCompareRowInPeerTable.getRow());\n        if (rowCmpRet == 0) {\n          // rowkey is same, need to compare the content of the row\n          try {\n            Result.compareResults(value, currentCompareRowInPeerTable);\n            context.getCounter(Counters.GOODROWS).increment(1);\n          } catch (Exception e) {\n            logFailRowAndIncreaseCounter(context, Counters.CONTENT_DIFFERENT_ROWS, value);\n            LOG.error(\"Exception while comparing row : \" + e);\n          }\n          currentCompareRowInPeerTable = replicatedScanner.next();\n          break;\n        } else if (rowCmpRet < 0) {\n          // row only exists in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        } else {\n          // row only exists in peer table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_PEER_TABLE_ROWS,\n            currentCompareRowInPeerTable);\n          currentCompareRowInPeerTable = replicatedScanner.next();\n        }\n      }\n    }"
        ]
    ],
    "2515b0974d84421f47a18f9e20b5fe215a95cf8f": [
        [
            "AbstractTestShell::setUpBeforeClass()",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setBoolean(\"hbase.online.schema.update.enable\", true);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n    TEST_UTIL.getConfiguration().setInt(HConstants.MASTER_INFO_PORT, -1);\n    TEST_UTIL.getConfiguration().setInt(HConstants.REGIONSERVER_INFO_PORT, -1);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    TEST_UTIL.startMiniCluster();\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList();\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.getProvider().setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48  \n  49 +\n  50 +\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setBoolean(\"hbase.online.schema.update.enable\", true);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n\n    //NOTE: Below Settings are disabled for taskmonitor_test\n    TEST_UTIL.getConfiguration().setInt(HConstants.MASTER_INFO_PORT, -1);\n    TEST_UTIL.getConfiguration().setInt(HConstants.REGIONSERVER_INFO_PORT, 0);\n    TEST_UTIL.getConfiguration().setBoolean(HConstants.REGIONSERVER_INFO_PORT_AUTO, true);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    TEST_UTIL.startMiniCluster();\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList();\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.getProvider().setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }"
        ]
    ],
    "3d7840a173aab97fb72409fa8c0f161fd7ad0e8f": [
        [
            "ProtobufStreamingUtil::writeToStream(CellSetModel,String,OutputStream)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86  ",
            "  private void writeToStream(CellSetModel model, String contentType, OutputStream outStream)\n      throws IOException {\n    byte[] objectBytes = model.createProtobufOutput();\n    outStream.write(Bytes.toBytes((short)objectBytes.length));\n    outStream.write(objectBytes);\n    outStream.flush();\n    LOG.trace(\"Wrote \" + model.getRows().size() + \" rows to stream successfully.\");\n  }",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88 +\n  89 +\n  90  ",
            "  private void writeToStream(CellSetModel model, String contentType, OutputStream outStream)\n      throws IOException {\n    byte[] objectBytes = model.createProtobufOutput();\n    outStream.write(Bytes.toBytes((short)objectBytes.length));\n    outStream.write(objectBytes);\n    outStream.flush();\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Wrote \" + model.getRows().size() + \" rows to stream successfully.\");\n    }\n  }"
        ],
        [
            "ScannerInstanceResource::getBinary(UriInfo)",
            " 147  \n 148  \n 149  \n 150 -\n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163 -\n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  ",
            "  @GET\n  @Produces(MIMETYPE_BINARY)\n  public Response getBinary(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath() + \" as \" +\n        MIMETYPE_BINARY);\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      Cell value = generator.next();\n      if (value == null) {\n        LOG.info(\"generator exhausted\");\n        return Response.noContent().build();\n      }\n      ResponseBuilder response = Response.ok(CellUtil.cloneValue(value));\n      response.cacheControl(cacheControl);\n      response.header(\"X-Row\", Base64.encodeBytes(CellUtil.cloneRow(value)));      \n      response.header(\"X-Column\", \n        Base64.encodeBytes(\n          KeyValue.makeColumn(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value))));\n      response.header(\"X-Timestamp\", value.getTimestamp());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IllegalStateException e) {\n      if (ScannerResource.delete(id)) {\n        servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      } else {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n      }\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.GONE)\n        .type(MIMETYPE_TEXT).entity(\"Gone\" + CRLF)\n        .build();\n    }\n  }",
            " 149  \n 150  \n 151  \n 152 +\n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161 +\n 162 +\n 163  \n 164  \n 165  \n 166  \n 167 +\n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "  @GET\n  @Produces(MIMETYPE_BINARY)\n  public Response getBinary(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath() + \" as \" +\n        MIMETYPE_BINARY);\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      Cell value = generator.next();\n      if (value == null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"generator exhausted\");\n        }\n        return Response.noContent().build();\n      }\n      ResponseBuilder response = Response.ok(CellUtil.cloneValue(value));\n      response.cacheControl(cacheControl);\n      response.header(\"X-Row\", Base64.encodeBytes(CellUtil.cloneRow(value)));\n      response.header(\"X-Column\",\n        Base64.encodeBytes(\n          KeyValue.makeColumn(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value))));\n      response.header(\"X-Timestamp\", value.getTimestamp());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IllegalStateException e) {\n      if (ScannerResource.delete(id)) {\n        servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      } else {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n      }\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.GONE)\n        .type(MIMETYPE_TEXT).entity(\"Gone\" + CRLF)\n        .build();\n    }\n  }"
        ],
        [
            "NamespacesInstanceResource::put(NamespacesInstanceModel,UriInfo)",
            " 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139 -\n 140  \n 141  \n 142  \n 143  ",
            "  /**\n   * Build a response for PUT alter namespace with properties specified.\n   * @param model properties used for alter.\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return response code.\n   */\n  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final NamespacesInstanceModel model, final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return processUpdate(model, true, uriInfo);\n  }",
            " 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  ",
            "  /**\n   * Build a response for PUT alter namespace with properties specified.\n   * @param model properties used for alter.\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return response code.\n   */\n  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final NamespacesInstanceModel model, final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return processUpdate(model, true, uriInfo);\n  }"
        ],
        [
            "ScannerResource::put(ScannerModel,UriInfo)",
            " 129  \n 130  \n 131  \n 132 -\n 133  \n 134 -\n 135 -\n 136  \n 137  \n 138  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final ScannerModel model, \n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    return update(model, true, uriInfo);\n  }",
            " 128  \n 129  \n 130  \n 131 +\n 132  \n 133 +\n 134 +\n 135  \n 136  \n 137  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final ScannerModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    return update(model, true, uriInfo);\n  }"
        ],
        [
            "RestCsrfPreventionFilter::init(FilterConfig)",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93  ",
            "  @Override\n  public void init(FilterConfig filterConfig) throws ServletException {\n    String customHeader = filterConfig.getInitParameter(CUSTOM_HEADER_PARAM);\n    if (customHeader != null) {\n      headerName = customHeader;\n    }\n    String customMethodsToIgnore =\n        filterConfig.getInitParameter(CUSTOM_METHODS_TO_IGNORE_PARAM);\n    if (customMethodsToIgnore != null) {\n      parseMethodsToIgnore(customMethodsToIgnore);\n    } else {\n      parseMethodsToIgnore(METHODS_TO_IGNORE_DEFAULT);\n    }\n\n    String agents = filterConfig.getInitParameter(BROWSER_USER_AGENT_PARAM);\n    if (agents == null) {\n      agents = BROWSER_USER_AGENTS_DEFAULT;\n    }\n    parseBrowserUserAgents(agents);\n    LOG.info(\"Adding cross-site request forgery (CSRF) protection, \"\n        + \"headerName = {}, methodsToIgnore = {}, browserUserAgents = {}\",\n        headerName, methodsToIgnore, browserUserAgents);\n  }",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91 +\n  92  ",
            "  @Override\n  public void init(FilterConfig filterConfig) throws ServletException {\n    String customHeader = filterConfig.getInitParameter(CUSTOM_HEADER_PARAM);\n    if (customHeader != null) {\n      headerName = customHeader;\n    }\n    String customMethodsToIgnore =\n        filterConfig.getInitParameter(CUSTOM_METHODS_TO_IGNORE_PARAM);\n    if (customMethodsToIgnore != null) {\n      parseMethodsToIgnore(customMethodsToIgnore);\n    } else {\n      parseMethodsToIgnore(METHODS_TO_IGNORE_DEFAULT);\n    }\n\n    String agents = filterConfig.getInitParameter(BROWSER_USER_AGENT_PARAM);\n    if (agents == null) {\n      agents = BROWSER_USER_AGENTS_DEFAULT;\n    }\n    parseBrowserUserAgents(agents);\n    LOG.info(String.format(\"Adding cross-site request forgery (CSRF) protection, \"\n        + \"headerName = %s, methodsToIgnore = %s, browserUserAgents = %s\",\n        headerName, methodsToIgnore, browserUserAgents));\n  }"
        ],
        [
            "StorageClusterStatusResource::get(UriInfo)",
            "  62  \n  63  \n  64  \n  65  \n  66 -\n  67 -\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ClusterStatus status = servlet.getAdmin().getClusterStatus();\n      StorageClusterStatusModel model = new StorageClusterStatusModel();\n      model.setRegions(status.getRegionsCount());\n      model.setRequests(status.getRequestsCount());\n      model.setAverageLoad(status.getAverageLoad());\n      for (ServerName info: status.getServers()) {\n        ServerLoad load = status.getLoad(info);\n        StorageClusterStatusModel.Node node =\n          model.addLiveNode(\n            info.getHostname() + \":\" +\n            Integer.toString(info.getPort()),\n            info.getStartcode(), load.getUsedHeapMB(),\n            load.getMaxHeapMB());\n        node.setRequests(load.getNumberOfRequests());\n        for (RegionLoad region: load.getRegionsLoad().values()) {\n          node.addRegion(region.getName(), region.getStores(),\n            region.getStorefiles(), region.getStorefileSizeMB(),\n            region.getMemStoreSizeMB(), region.getStorefileIndexSizeMB(),\n            region.getReadRequestsCount(), region.getWriteRequestsCount(),\n            region.getRootIndexSizeKB(), region.getTotalStaticIndexSizeKB(),\n            region.getTotalStaticBloomSizeKB(), region.getTotalCompactingKVs(),\n            region.getCurrentCompactedKVs());\n        }\n      }\n      for (ServerName name: status.getDeadServerNames()) {\n        model.addDeadNode(name.toString());\n      }\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }",
            "  62  \n  63  \n  64  \n  65  \n  66 +\n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ClusterStatus status = servlet.getAdmin().getClusterStatus();\n      StorageClusterStatusModel model = new StorageClusterStatusModel();\n      model.setRegions(status.getRegionsCount());\n      model.setRequests(status.getRequestsCount());\n      model.setAverageLoad(status.getAverageLoad());\n      for (ServerName info: status.getServers()) {\n        ServerLoad load = status.getLoad(info);\n        StorageClusterStatusModel.Node node =\n          model.addLiveNode(\n            info.getHostname() + \":\" +\n            Integer.toString(info.getPort()),\n            info.getStartcode(), load.getUsedHeapMB(),\n            load.getMaxHeapMB());\n        node.setRequests(load.getNumberOfRequests());\n        for (RegionLoad region: load.getRegionsLoad().values()) {\n          node.addRegion(region.getName(), region.getStores(),\n            region.getStorefiles(), region.getStorefileSizeMB(),\n            region.getMemStoreSizeMB(), region.getStorefileIndexSizeMB(),\n            region.getReadRequestsCount(), region.getWriteRequestsCount(),\n            region.getRootIndexSizeKB(), region.getTotalStaticIndexSizeKB(),\n            region.getTotalStaticBloomSizeKB(), region.getTotalCompactingKVs(),\n            region.getCurrentCompactedKVs());\n        }\n      }\n      for (ServerName name: status.getDeadServerNames()) {\n        model.addDeadNode(name.toString());\n      }\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }"
        ],
        [
            "RowResource::post(CellSetModel,UriInfo)",
            " 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337 -\n 338  \n 339  \n 340  \n 341  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final CellSetModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"POST \" + uriInfo.getAbsolutePath()\n        + \" \" + uriInfo.getQueryParameters());\n    }\n    return update(model, false);\n  }",
            " 331  \n 332  \n 333  \n 334  \n 335  \n 336 +\n 337 +\n 338  \n 339  \n 340  \n 341  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final CellSetModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"POST \" + uriInfo.getAbsolutePath()\n        + \" \" + uriInfo.getQueryParameters());\n    }\n    return update(model, false);\n  }"
        ],
        [
            "SchemaResource::delete(UriInfo)",
            " 222  \n 223  \n 224  \n 225  \n 226 -\n 227 -\n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"Expected\")\n  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN).type(MIMETYPE_TEXT)\n          .entity(\"Forbidden\" + CRLF).build();\n    }\n    try {\n      Admin admin = servlet.getAdmin();\n      try {\n        admin.disableTable(TableName.valueOf(tableResource.getName()));\n      } catch (TableNotEnabledException e) { /* this is what we want anyway */ }\n      admin.deleteTable(TableName.valueOf(tableResource.getName()));\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return Response.ok().build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    }\n  }",
            " 222  \n 223  \n 224  \n 225  \n 226 +\n 227 +\n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"Expected\")\n  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN).type(MIMETYPE_TEXT)\n          .entity(\"Forbidden\" + CRLF).build();\n    }\n    try {\n      Admin admin = servlet.getAdmin();\n      try {\n        admin.disableTable(TableName.valueOf(tableResource.getName()));\n      } catch (TableNotEnabledException e) { /* this is what we want anyway */ }\n      admin.deleteTable(TableName.valueOf(tableResource.getName()));\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return Response.ok().build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "StorageClusterVersionResource::get(UriInfo)",
            "  58  \n  59  \n  60  \n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      StorageClusterVersionModel model = new StorageClusterVersionModel();\n      model.setVersion(servlet.getAdmin().getClusterStatus().getHBaseVersion());\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }",
            "  58  \n  59  \n  60  \n  61 +\n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      StorageClusterVersionModel model = new StorageClusterVersionModel();\n      model.setVersion(servlet.getAdmin().getClusterStatus().getHBaseVersion());\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }"
        ],
        [
            "RowResource::getBinary(UriInfo)",
            " 130  \n 131  \n 132  \n 133 -\n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  @GET\n  @Produces(MIMETYPE_BINARY)\n  public Response getBinary(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath() + \" as \"+ MIMETYPE_BINARY);\n    }\n    servlet.getMetrics().incrementRequests(1);\n    // doesn't make sense to use a non specific coordinate as this can only\n    // return a single cell\n    if (!rowspec.hasColumns() || rowspec.getColumns().length > 1) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n          .entity(\"Bad request: Either 0 or more than 1 columns specified.\" + CRLF).build();\n    }\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n    try {\n      ResultGenerator generator =\n        ResultGenerator.fromRowSpec(tableResource.getName(), rowspec, null,\n          !params.containsKey(NOCACHE_PARAM_NAME));\n      if (!generator.hasNext()) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      }\n      Cell value = generator.next();\n      ResponseBuilder response = Response.ok(CellUtil.cloneValue(value));\n      response.header(\"X-Timestamp\", value.getTimestamp());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }",
            " 130  \n 131  \n 132  \n 133 +\n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  @GET\n  @Produces(MIMETYPE_BINARY)\n  public Response getBinary(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath() + \" as \"+ MIMETYPE_BINARY);\n    }\n    servlet.getMetrics().incrementRequests(1);\n    // doesn't make sense to use a non specific coordinate as this can only\n    // return a single cell\n    if (!rowspec.hasColumns() || rowspec.getColumns().length > 1) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n          .entity(\"Bad request: Either 0 or more than 1 columns specified.\" + CRLF).build();\n    }\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n    try {\n      ResultGenerator generator =\n        ResultGenerator.fromRowSpec(tableResource.getName(), rowspec, null,\n          !params.containsKey(NOCACHE_PARAM_NAME));\n      if (!generator.hasNext()) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      }\n      Cell value = generator.next();\n      ResponseBuilder response = Response.ok(CellUtil.cloneValue(value));\n      response.header(\"X-Timestamp\", value.getTimestamp());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "NamespacesInstanceResource::get(ServletContext,UriInfo)",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "  /**\n   * Build a response for GET namespace description or GET list of namespace tables.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return A response containing NamespacesInstanceModel for a namespace descriptions and\n   * TableListModel for a list of namespace tables.\n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n\n    // Respond to list of namespace tables requests.\n    if(queryTables){\n      TableListModel tableModel = new TableListModel();\n      try{\n        HTableDescriptor[] tables = servlet.getAdmin().listTableDescriptorsByNamespace(namespace);\n        for(int i = 0; i < tables.length; i++){\n          tableModel.add(new TableModel(tables[i].getTableName().getQualifierAsString()));\n        }\n\n        servlet.getMetrics().incrementSucessfulGetRequests(1);\n        return Response.ok(tableModel).build();\n      }catch(IOException e) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        throw new RuntimeException(\"Cannot retrieve table list for '\" + namespace + \"'.\");\n      }\n    }\n\n    // Respond to namespace description requests.\n    try {\n      NamespacesInstanceModel rowModel =\n          new NamespacesInstanceModel(servlet.getAdmin(), namespace);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(rowModel).build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95 +\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "  /**\n   * Build a response for GET namespace description or GET list of namespace tables.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return A response containing NamespacesInstanceModel for a namespace descriptions and\n   * TableListModel for a list of namespace tables.\n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n\n    // Respond to list of namespace tables requests.\n    if(queryTables){\n      TableListModel tableModel = new TableListModel();\n      try{\n        HTableDescriptor[] tables = servlet.getAdmin().listTableDescriptorsByNamespace(namespace);\n        for(int i = 0; i < tables.length; i++){\n          tableModel.add(new TableModel(tables[i].getTableName().getQualifierAsString()));\n        }\n\n        servlet.getMetrics().incrementSucessfulGetRequests(1);\n        return Response.ok(tableModel).build();\n      }catch(IOException e) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        throw new RuntimeException(\"Cannot retrieve table list for '\" + namespace + \"'.\");\n      }\n    }\n\n    // Respond to namespace description requests.\n    try {\n      NamespacesInstanceModel rowModel =\n          new NamespacesInstanceModel(servlet.getAdmin(), namespace);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(rowModel).build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }"
        ],
        [
            "NamespacesResource::get(ServletContext,UriInfo)",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 -\n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "  /**\n   * Build a response for a list of all namespaces request.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return a response for a version request\n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      NamespacesModel rowModel = null;\n      rowModel = new NamespacesModel(servlet.getAdmin());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(rowModel).build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      throw new RuntimeException(\"Cannot retrieve list of namespaces.\");\n    }\n  }",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "  /**\n   * Build a response for a list of all namespaces request.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return a response for a version request\n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      NamespacesModel rowModel = null;\n      rowModel = new NamespacesModel(servlet.getAdmin());\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(rowModel).build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      throw new RuntimeException(\"Cannot retrieve list of namespaces.\");\n    }\n  }"
        ],
        [
            "ProtobufStreamingUtil::ProtobufStreamingUtil(ResultScanner,String,int,int)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 -\n  53 -\n  54  ",
            "  protected ProtobufStreamingUtil(ResultScanner scanner, String type, int limit, int fetchSize) {\n    this.resultScanner = scanner;\n    this.contentType = type;\n    this.limit = limit;\n    this.fetchSize = fetchSize;\n    LOG.debug(\"Created ScanStreamingUtil with content type = \" + this.contentType + \" user limit : \"\n        + this.limit + \" scan fetch size : \" + this.fetchSize);\n  }",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53 +\n  54 +\n  55 +\n  56  ",
            "  protected ProtobufStreamingUtil(ResultScanner scanner, String type, int limit, int fetchSize) {\n    this.resultScanner = scanner;\n    this.contentType = type;\n    this.limit = limit;\n    this.fetchSize = fetchSize;\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Created ScanStreamingUtil with content type = \" + this.contentType\n        + \" user limit : \" + this.limit + \" scan fetch size : \" + this.fetchSize);\n    }\n  }"
        ],
        [
            "RowResource::checkAndPut(CellSetModel)",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502 -\n 503 -\n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520 -\n 521  \n 522  \n 523  \n 524  ",
            "  /**\n   * Validates the input request parameters, parses columns from CellSetModel,\n   * and invokes checkAndPut on HTable.\n   *\n   * @param model instance of CellSetModel\n   * @return Response 200 OK, 304 Not modified, 400 Bad request\n   */\n  Response checkAndPut(final CellSetModel model) {\n    Table table = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      if (model.getRows().size() != 1) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n            .entity(\"Bad request: Number of rows specified is not 1.\" + CRLF).build();\n      }\n\n      RowModel rowModel = model.getRows().get(0);\n      byte[] key = rowModel.getKey();\n      if (key == null) {\n        key = rowspec.getRow();\n      }\n\n      List<CellModel> cellModels = rowModel.getCells();\n      int cellModelCount = cellModels.size();\n      if (key == null || cellModelCount <= 1) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response\n            .status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT)\n            .entity(\n              \"Bad request: Either row key is null or no data found for columns specified.\" + CRLF)\n            .build();\n      }\n\n      Put put = new Put(key);\n      boolean retValue;\n      CellModel valueToCheckCell = cellModels.get(cellModelCount - 1);\n      byte[] valueToCheckColumn = valueToCheckCell.getColumn();\n      byte[][] valueToPutParts = KeyValue.parseColumn(valueToCheckColumn);\n      if (valueToPutParts.length == 2 && valueToPutParts[1].length > 0) {\n        CellModel valueToPutCell = null;\n\n        // Copy all the cells to the Put request\n        // and track if the check cell's latest value is also sent\n        for (int i = 0, n = cellModelCount - 1; i < n ; i++) {\n          CellModel cell = cellModels.get(i);\n          byte[] col = cell.getColumn();\n\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n                    .build();\n          }\n\n          byte [][] parts = KeyValue.parseColumn(col);\n\n          if (parts.length != 2) {\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n                    .build();\n          }\n          put.addImmutable(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n\n          if(Bytes.equals(col,\n                  valueToCheckCell.getColumn())) {\n            valueToPutCell = cell;\n          }\n        }\n\n        if (valueToPutCell == null) {\n          servlet.getMetrics().incrementFailedPutRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n              .entity(\"Bad request: The column to put and check do not match.\" + CRLF).build();\n        } else {\n          retValue = table.checkAndPut(key, valueToPutParts[0], valueToPutParts[1],\n            valueToCheckCell.getValue(), put);\n        }\n      } else {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Column incorrectly specified.\" + CRLF)\n          .build();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"CHECK-AND-PUT \" + put.toString() + \", returns \" + retValue);\n      }\n      if (!retValue) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.NOT_MODIFIED)\n          .type(MIMETYPE_TEXT).entity(\"Value not Modified\" + CRLF)\n          .build();\n      }\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) { \n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502 +\n 503 +\n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520 +\n 521  \n 522  \n 523  \n 524  ",
            "  /**\n   * Validates the input request parameters, parses columns from CellSetModel,\n   * and invokes checkAndPut on HTable.\n   *\n   * @param model instance of CellSetModel\n   * @return Response 200 OK, 304 Not modified, 400 Bad request\n   */\n  Response checkAndPut(final CellSetModel model) {\n    Table table = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      if (model.getRows().size() != 1) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n            .entity(\"Bad request: Number of rows specified is not 1.\" + CRLF).build();\n      }\n\n      RowModel rowModel = model.getRows().get(0);\n      byte[] key = rowModel.getKey();\n      if (key == null) {\n        key = rowspec.getRow();\n      }\n\n      List<CellModel> cellModels = rowModel.getCells();\n      int cellModelCount = cellModels.size();\n      if (key == null || cellModelCount <= 1) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response\n            .status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT)\n            .entity(\n              \"Bad request: Either row key is null or no data found for columns specified.\" + CRLF)\n            .build();\n      }\n\n      Put put = new Put(key);\n      boolean retValue;\n      CellModel valueToCheckCell = cellModels.get(cellModelCount - 1);\n      byte[] valueToCheckColumn = valueToCheckCell.getColumn();\n      byte[][] valueToPutParts = KeyValue.parseColumn(valueToCheckColumn);\n      if (valueToPutParts.length == 2 && valueToPutParts[1].length > 0) {\n        CellModel valueToPutCell = null;\n\n        // Copy all the cells to the Put request\n        // and track if the check cell's latest value is also sent\n        for (int i = 0, n = cellModelCount - 1; i < n ; i++) {\n          CellModel cell = cellModels.get(i);\n          byte[] col = cell.getColumn();\n\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n                    .build();\n          }\n\n          byte [][] parts = KeyValue.parseColumn(col);\n\n          if (parts.length != 2) {\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n                    .build();\n          }\n          put.addImmutable(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n\n          if(Bytes.equals(col,\n                  valueToCheckCell.getColumn())) {\n            valueToPutCell = cell;\n          }\n        }\n\n        if (valueToPutCell == null) {\n          servlet.getMetrics().incrementFailedPutRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)\n              .entity(\"Bad request: The column to put and check do not match.\" + CRLF).build();\n        } else {\n          retValue = table.checkAndPut(key, valueToPutParts[0], valueToPutParts[1],\n            valueToCheckCell.getValue(), put);\n        }\n      } else {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Column incorrectly specified.\" + CRLF)\n          .build();\n      }\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"CHECK-AND-PUT \" + put.toString() + \", returns \" + retValue);\n      }\n      if (!retValue) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.NOT_MODIFIED)\n          .type(MIMETYPE_TEXT).entity(\"Value not Modified\" + CRLF)\n          .build();\n      }\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }"
        ],
        [
            "ScannerResource::post(ScannerModel,UriInfo)",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146 -\n 147  \n 148  \n 149  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final ScannerModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    return update(model, false, uriInfo);\n  }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145 +\n 146  \n 147  \n 148  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final ScannerModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    return update(model, false, uriInfo);\n  }"
        ],
        [
            "ScannerResource::update(ScannerModel,boolean,UriInfo)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "  Response update(final ScannerModel model, final boolean replace,\n      final UriInfo uriInfo) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    byte[] endRow = model.hasEndRow() ? model.getEndRow() : null;\n    RowSpec spec = null;\n    if (model.getLabels() != null) {\n      spec = new RowSpec(model.getStartRow(), endRow, model.getColumns(), model.getStartTime(),\n          model.getEndTime(), model.getMaxVersions(), model.getLabels());\n    } else {\n      spec = new RowSpec(model.getStartRow(), endRow, model.getColumns(), model.getStartTime(),\n          model.getEndTime(), model.getMaxVersions());\n    }\n    \n    try {\n      Filter filter = ScannerResultGenerator.buildFilterFromModel(model);\n      String tableName = tableResource.getName();\n      ScannerResultGenerator gen =\n        new ScannerResultGenerator(tableName, spec, filter, model.getCaching(),\n          model.getCacheBlocks());\n      String id = gen.getID();\n      ScannerInstanceResource instance =\n        new ScannerInstanceResource(tableName, id, gen, model.getBatch());\n      scanners.put(id, instance);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"new scanner: \" + id);\n      }\n      UriBuilder builder = uriInfo.getAbsolutePathBuilder();\n      URI uri = builder.path(id).build();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return Response.created(uri).build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      if (e instanceof TableNotFoundException) {\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      } else if (e instanceof RuntimeException) {\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "  Response update(final ScannerModel model, final boolean replace,\n      final UriInfo uriInfo) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    byte[] endRow = model.hasEndRow() ? model.getEndRow() : null;\n    RowSpec spec = null;\n    if (model.getLabels() != null) {\n      spec = new RowSpec(model.getStartRow(), endRow, model.getColumns(), model.getStartTime(),\n          model.getEndTime(), model.getMaxVersions(), model.getLabels());\n    } else {\n      spec = new RowSpec(model.getStartRow(), endRow, model.getColumns(), model.getStartTime(),\n          model.getEndTime(), model.getMaxVersions());\n    }\n\n    try {\n      Filter filter = ScannerResultGenerator.buildFilterFromModel(model);\n      String tableName = tableResource.getName();\n      ScannerResultGenerator gen =\n        new ScannerResultGenerator(tableName, spec, filter, model.getCaching(),\n          model.getCacheBlocks());\n      String id = gen.getID();\n      ScannerInstanceResource instance =\n        new ScannerInstanceResource(tableName, id, gen, model.getBatch());\n      scanners.put(id, instance);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"new scanner: \" + id);\n      }\n      UriBuilder builder = uriInfo.getAbsolutePathBuilder();\n      URI uri = builder.path(id).build();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return Response.created(uri).build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      if (e instanceof TableNotFoundException) {\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      } else if (e instanceof RuntimeException) {\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }"
        ],
        [
            "VersionResource::get(ServletContext,UriInfo)",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78 -\n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "  /**\n   * Build a response for a version request.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return a response for a version request \n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context, \n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    ResponseBuilder response = Response.ok(new VersionModel(context));\n    response.cacheControl(cacheControl);\n    servlet.getMetrics().incrementSucessfulGetRequests(1);\n    return response.build();\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 +\n  77  \n  78 +\n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "  /**\n   * Build a response for a version request.\n   * @param context servlet context\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return a response for a version request\n   */\n  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context ServletContext context,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    ResponseBuilder response = Response.ok(new VersionModel(context));\n    response.cacheControl(cacheControl);\n    servlet.getMetrics().incrementSucessfulGetRequests(1);\n    return response.build();\n  }"
        ],
        [
            "NamespacesInstanceResource::deleteNoBody(byte,UriInfo,HttpHeaders)",
            " 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 -\n 291 -\n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "  /**\n   * Build a response for DELETE delete namespace.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @DELETE\n  public Response deleteNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return Response.status(Response.Status.FORBIDDEN).type(MIMETYPE_TEXT)\n          .entity(\"Forbidden\" + CRLF).build();\n    }\n\n    try{\n      Admin admin = servlet.getAdmin();\n      if (!doesNamespaceExist(admin, namespace)){\n        return Response.status(Response.Status.NOT_FOUND).type(MIMETYPE_TEXT).\n            entity(\"Namespace '\" + namespace + \"' does not exists.  Cannot \" +\n            \"drop namespace.\").build();\n      }\n\n      admin.deleteNamespace(namespace);\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return Response.ok().build();\n\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    }\n  }",
            " 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 +\n 291 +\n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "  /**\n   * Build a response for DELETE delete namespace.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @DELETE\n  public Response deleteNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return Response.status(Response.Status.FORBIDDEN).type(MIMETYPE_TEXT)\n          .entity(\"Forbidden\" + CRLF).build();\n    }\n\n    try{\n      Admin admin = servlet.getAdmin();\n      if (!doesNamespaceExist(admin, namespace)){\n        return Response.status(Response.Status.NOT_FOUND).type(MIMETYPE_TEXT).\n            entity(\"Namespace '\" + namespace + \"' does not exists.  Cannot \" +\n            \"drop namespace.\").build();\n      }\n\n      admin.deleteNamespace(namespace);\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return Response.ok().build();\n\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "RowResource::updateBinary(byte,HttpHeaders,boolean)",
            " 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293 -\n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 -\n 305  \n 306  \n 307  ",
            "  Response updateBinary(final byte[] message, final HttpHeaders headers,\n      final boolean replace) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    Table table = null;\n    try {\n      byte[] row = rowspec.getRow();\n      byte[][] columns = rowspec.getColumns();\n      byte[] column = null;\n      if (columns != null) {\n        column = columns[0];\n      }\n      long timestamp = HConstants.LATEST_TIMESTAMP;\n      List<String> vals = headers.getRequestHeader(\"X-Row\");\n      if (vals != null && !vals.isEmpty()) {\n        row = Bytes.toBytes(vals.get(0));\n      }\n      vals = headers.getRequestHeader(\"X-Column\");\n      if (vals != null && !vals.isEmpty()) {\n        column = Bytes.toBytes(vals.get(0));\n      }\n      vals = headers.getRequestHeader(\"X-Timestamp\");\n      if (vals != null && !vals.isEmpty()) {\n        timestamp = Long.parseLong(vals.get(0));\n      }\n      if (column == null) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n            .build();\n      }\n      Put put = new Put(row);\n      byte parts[][] = KeyValue.parseColumn(column);\n      if (parts.length != 2) {\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      put.addImmutable(parts[0], parts[1], timestamp, message);\n      table = servlet.getTable(tableResource.getName());\n      table.put(put);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"PUT \" + put.toString());\n      }\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return Response.ok().build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(ioe);\n      }\n    }\n  }",
            " 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 +\n 293 +\n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305  \n 306  \n 307  ",
            "  Response updateBinary(final byte[] message, final HttpHeaders headers,\n      final boolean replace) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    Table table = null;\n    try {\n      byte[] row = rowspec.getRow();\n      byte[][] columns = rowspec.getColumns();\n      byte[] column = null;\n      if (columns != null) {\n        column = columns[0];\n      }\n      long timestamp = HConstants.LATEST_TIMESTAMP;\n      List<String> vals = headers.getRequestHeader(\"X-Row\");\n      if (vals != null && !vals.isEmpty()) {\n        row = Bytes.toBytes(vals.get(0));\n      }\n      vals = headers.getRequestHeader(\"X-Column\");\n      if (vals != null && !vals.isEmpty()) {\n        column = Bytes.toBytes(vals.get(0));\n      }\n      vals = headers.getRequestHeader(\"X-Timestamp\");\n      if (vals != null && !vals.isEmpty()) {\n        timestamp = Long.parseLong(vals.get(0));\n      }\n      if (column == null) {\n        servlet.getMetrics().incrementFailedPutRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n            .build();\n      }\n      Put put = new Put(row);\n      byte parts[][] = KeyValue.parseColumn(column);\n      if (parts.length != 2) {\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      put.addImmutable(parts[0], parts[1], timestamp, message);\n      table = servlet.getTable(tableResource.getName());\n      table.put(put);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"PUT \" + put.toString());\n      }\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return Response.ok().build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }"
        ],
        [
            "RowResource::update(CellSetModel,boolean)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  Response update(final CellSetModel model, final boolean replace) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n\n    if (CHECK_PUT.equalsIgnoreCase(check)) {\n      return checkAndPut(model);\n    } else if (CHECK_DELETE.equalsIgnoreCase(check)) {\n      return checkAndDelete(model);\n    } else if (check != null && check.length() > 0) {\n      return Response.status(Response.Status.BAD_REQUEST)\n        .type(MIMETYPE_TEXT).entity(\"Invalid check value '\" + check + \"'\" + CRLF)\n        .build();\n    }\n\n    Table table = null;\n    try {\n      List<RowModel> rows = model.getRows();\n      List<Put> puts = new ArrayList<Put>();\n      for (RowModel row: rows) {\n        byte[] key = row.getKey();\n        if (key == null) {\n          key = rowspec.getRow();\n        }\n        if (key == null) {\n          servlet.getMetrics().incrementFailedPutRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Row key not specified.\" + CRLF)\n            .build();\n        }\n        Put put = new Put(key);\n        int i = 0;\n        for (CellModel cell: row.getCells()) {\n          byte[] col = cell.getColumn();\n          if (col == null) try {\n            col = rowspec.getColumns()[i++];\n          } catch (ArrayIndexOutOfBoundsException e) {\n            col = null;\n          }\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n              .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n              .build();\n          }\n          byte [][] parts = KeyValue.parseColumn(col);\n          if (parts.length != 2) {\n            return Response.status(Response.Status.BAD_REQUEST)\n              .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n              .build();\n          }\n          put.addImmutable(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n        }\n        puts.add(put);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"PUT \" + put.toString());\n        }\n      }\n      table = servlet.getTable(tableResource.getName());\n      table.put(puts);\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 +\n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  Response update(final CellSetModel model, final boolean replace) {\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n\n    if (CHECK_PUT.equalsIgnoreCase(check)) {\n      return checkAndPut(model);\n    } else if (CHECK_DELETE.equalsIgnoreCase(check)) {\n      return checkAndDelete(model);\n    } else if (check != null && check.length() > 0) {\n      return Response.status(Response.Status.BAD_REQUEST)\n        .type(MIMETYPE_TEXT).entity(\"Invalid check value '\" + check + \"'\" + CRLF)\n        .build();\n    }\n\n    Table table = null;\n    try {\n      List<RowModel> rows = model.getRows();\n      List<Put> puts = new ArrayList<Put>();\n      for (RowModel row: rows) {\n        byte[] key = row.getKey();\n        if (key == null) {\n          key = rowspec.getRow();\n        }\n        if (key == null) {\n          servlet.getMetrics().incrementFailedPutRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Row key not specified.\" + CRLF)\n            .build();\n        }\n        Put put = new Put(key);\n        int i = 0;\n        for (CellModel cell: row.getCells()) {\n          byte[] col = cell.getColumn();\n          if (col == null) try {\n            col = rowspec.getColumns()[i++];\n          } catch (ArrayIndexOutOfBoundsException e) {\n            col = null;\n          }\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n              .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n              .build();\n          }\n          byte [][] parts = KeyValue.parseColumn(col);\n          if (parts.length != 2) {\n            return Response.status(Response.Status.BAD_REQUEST)\n              .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n              .build();\n          }\n          put.addImmutable(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n        }\n        puts.add(put);\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"PUT \" + put.toString());\n        }\n      }\n      table = servlet.getTable(tableResource.getName());\n      table.put(puts);\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulPutRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }"
        ],
        [
            "Client::executeURI(HttpMethod,Header,String)",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 -\n 213 -\n 214  \n 215  \n 216  \n 217  ",
            "  /**\n   * Execute a transaction method given a complete URI.\n   * @param method the transaction method\n   * @param headers HTTP header values to send\n   * @param uri a properly urlencoded URI\n   * @return the HTTP response code\n   * @throws IOException\n   */\n  public int executeURI(HttpMethod method, Header[] headers, String uri)\n      throws IOException {\n    method.setURI(new URI(uri, true));\n    for (Map.Entry<String, String> e: extraHeaders.entrySet()) {\n      method.addRequestHeader(e.getKey(), e.getValue());\n    }\n    if (headers != null) {\n      for (Header header: headers) {\n        method.addRequestHeader(header);\n      }\n    }\n    long startTime = System.currentTimeMillis();\n    int code = httpClient.executeMethod(method);\n    long endTime = System.currentTimeMillis();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(method.getName() + \" \" + uri + \" \" + code + \" \" +\n        method.getStatusText() + \" in \" + (endTime - startTime) + \" ms\");\n    }\n    return code;\n  }",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213 +\n 214  \n 215  \n 216  \n 217  ",
            "  /**\n   * Execute a transaction method given a complete URI.\n   * @param method the transaction method\n   * @param headers HTTP header values to send\n   * @param uri a properly urlencoded URI\n   * @return the HTTP response code\n   * @throws IOException\n   */\n  public int executeURI(HttpMethod method, Header[] headers, String uri)\n      throws IOException {\n    method.setURI(new URI(uri, true));\n    for (Map.Entry<String, String> e: extraHeaders.entrySet()) {\n      method.addRequestHeader(e.getKey(), e.getValue());\n    }\n    if (headers != null) {\n      for (Header header: headers) {\n        method.addRequestHeader(header);\n      }\n    }\n    long startTime = System.currentTimeMillis();\n    int code = httpClient.executeMethod(method);\n    long endTime = System.currentTimeMillis();\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(method.getName() + \" \" + uri + \" \" + code + \" \" +\n        method.getStatusText() + \" in \" + (endTime - startTime) + \" ms\");\n    }\n    return code;\n  }"
        ],
        [
            "ScannerInstanceResource::get(UriInfo,int,int)",
            "  72  \n  73  \n  74  \n  75 -\n  76  \n  77 -\n  78 -\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  ",
            "  @GET\n  @Produces({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo, \n      @QueryParam(\"n\") int maxRows, final @QueryParam(\"c\") int maxValues) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (generator == null) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.NOT_FOUND)\n        .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n        .build();\n    }\n    CellSetModel model = new CellSetModel();\n    RowModel rowModel = null;\n    byte[] rowKey = null;\n    int limit = batch;\n    if (maxValues > 0) {\n      limit = maxValues;\n    }\n    int count = limit;\n    do {\n      Cell value = null;\n      try {\n        value = generator.next();\n      } catch (IllegalStateException e) {\n        if (ScannerResource.delete(id)) {\n          servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n        } else {\n          servlet.getMetrics().incrementFailedDeleteRequests(1);\n        }\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.GONE)\n          .type(MIMETYPE_TEXT).entity(\"Gone\" + CRLF)\n          .build();\n      }\n      if (value == null) {\n        LOG.info(\"generator exhausted\");\n        // respond with 204 (No Content) if an empty cell set would be\n        // returned\n        if (count == limit) {\n          return Response.noContent().build();\n        }\n        break;\n      }\n      if (rowKey == null) {\n        rowKey = CellUtil.cloneRow(value);\n        rowModel = new RowModel(rowKey);\n      }\n      if (!Bytes.equals(CellUtil.cloneRow(value), rowKey)) {\n        // if maxRows was given as a query param, stop if we would exceed the\n        // specified number of rows\n        if (maxRows > 0) { \n          if (--maxRows == 0) {\n            generator.putBack(value);\n            break;\n          }\n        }\n        model.addRow(rowModel);\n        rowKey = CellUtil.cloneRow(value);\n        rowModel = new RowModel(rowKey);\n      }\n      rowModel.addCell(\n        new CellModel(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value), \n          value.getTimestamp(), CellUtil.cloneValue(value)));\n    } while (--count > 0);\n    model.addRow(rowModel);\n    ResponseBuilder response = Response.ok(model);\n    response.cacheControl(cacheControl);\n    servlet.getMetrics().incrementSucessfulGetRequests(1);\n    return response.build();\n  }",
            "  72  \n  73  \n  74  \n  75 +\n  76  \n  77 +\n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112 +\n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "  @GET\n  @Produces({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo,\n      @QueryParam(\"n\") int maxRows, final @QueryParam(\"c\") int maxValues) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (generator == null) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.NOT_FOUND)\n        .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n        .build();\n    }\n    CellSetModel model = new CellSetModel();\n    RowModel rowModel = null;\n    byte[] rowKey = null;\n    int limit = batch;\n    if (maxValues > 0) {\n      limit = maxValues;\n    }\n    int count = limit;\n    do {\n      Cell value = null;\n      try {\n        value = generator.next();\n      } catch (IllegalStateException e) {\n        if (ScannerResource.delete(id)) {\n          servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n        } else {\n          servlet.getMetrics().incrementFailedDeleteRequests(1);\n        }\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.GONE)\n          .type(MIMETYPE_TEXT).entity(\"Gone\" + CRLF)\n          .build();\n      }\n      if (value == null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"generator exhausted\");\n        }\n        // respond with 204 (No Content) if an empty cell set would be\n        // returned\n        if (count == limit) {\n          return Response.noContent().build();\n        }\n        break;\n      }\n      if (rowKey == null) {\n        rowKey = CellUtil.cloneRow(value);\n        rowModel = new RowModel(rowKey);\n      }\n      if (!Bytes.equals(CellUtil.cloneRow(value), rowKey)) {\n        // if maxRows was given as a query param, stop if we would exceed the\n        // specified number of rows\n        if (maxRows > 0) {\n          if (--maxRows == 0) {\n            generator.putBack(value);\n            break;\n          }\n        }\n        model.addRow(rowModel);\n        rowKey = CellUtil.cloneRow(value);\n        rowModel = new RowModel(rowKey);\n      }\n      rowModel.addCell(\n        new CellModel(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value),\n          value.getTimestamp(), CellUtil.cloneValue(value)));\n    } while (--count > 0);\n    model.addRow(rowModel);\n    ResponseBuilder response = Response.ok(model);\n    response.cacheControl(cacheControl);\n    servlet.getMetrics().incrementSucessfulGetRequests(1);\n    return response.build();\n  }"
        ],
        [
            "NamespacesInstanceResource::postNoBody(byte,UriInfo,HttpHeaders)",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 -\n 196 -\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "  /**\n   * Build a response for POST create namespace with no properties specified.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @POST\n  public Response postNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try{\n      NamespacesInstanceModel model = new NamespacesInstanceModel(namespace);\n      return processUpdate(model, false, uriInfo);\n    }catch(IOException ioe){\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "  /**\n   * Build a response for POST create namespace with no properties specified.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @POST\n  public Response postNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try{\n      NamespacesInstanceModel model = new NamespacesInstanceModel(namespace);\n      return processUpdate(model, false, uriInfo);\n    }catch(IOException ioe){\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }"
        ],
        [
            "SchemaResource::post(TableSchemaModel,UriInfo)",
            " 210  \n 211  \n 212  \n 213 -\n 214  \n 215 -\n 216 -\n 217  \n 218  \n 219  \n 220  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final TableSchemaModel model, \n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return update(model, false, uriInfo);\n  }",
            " 210  \n 211  \n 212  \n 213 +\n 214  \n 215 +\n 216 +\n 217  \n 218  \n 219  \n 220  ",
            "  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final TableSchemaModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return update(model, false, uriInfo);\n  }"
        ],
        [
            "NamespacesInstanceResource::putNoBody(byte,UriInfo,HttpHeaders)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155 -\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  /**\n   * Build a response for PUT alter namespace with no properties specified.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @PUT\n  public Response putNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try{\n      NamespacesInstanceModel model = new NamespacesInstanceModel(namespace);\n      return processUpdate(model, true, uriInfo);\n    }catch(IOException ioe){\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 +\n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  /**\n   * Build a response for PUT alter namespace with no properties specified.\n   * @param message value not used.\n   * @param headers value not used.\n   * @return response code.\n   */\n  @PUT\n  public Response putNoBody(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try{\n      NamespacesInstanceModel model = new NamespacesInstanceModel(namespace);\n      return processUpdate(model, true, uriInfo);\n    }catch(IOException ioe){\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      throw new RuntimeException(\"Cannot retrieve info for '\" + namespace + \"'.\");\n    }\n  }"
        ],
        [
            "RegionsResource::get(UriInfo)",
            "  70  \n  71  \n  72  \n  73  \n  74 -\n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      TableName tableName = TableName.valueOf(tableResource.getName());\n      TableInfoModel model = new TableInfoModel(tableName.getNameAsString());\n\n      Connection connection = ConnectionFactory.createConnection(servlet.getConfiguration());\n      @SuppressWarnings(\"deprecation\")\n      Map<HRegionInfo, ServerName> regions = MetaTableAccessor\n          .allTableRegions(connection, tableName);\n      connection.close();\n      for (Map.Entry<HRegionInfo,ServerName> e: regions.entrySet()) {\n        HRegionInfo hri = e.getKey();\n        ServerName addr = e.getValue();\n        model.add(\n          new TableRegionModel(tableName.getNameAsString(), hri.getRegionId(),\n            hri.getStartKey(), hri.getEndKey(), addr.getHostAndPort()));\n      }\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (TableNotFoundException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.NOT_FOUND)\n        .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n        .build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }",
            "  70  \n  71  \n  72  \n  73  \n  74 +\n  75 +\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      TableName tableName = TableName.valueOf(tableResource.getName());\n      TableInfoModel model = new TableInfoModel(tableName.getNameAsString());\n\n      Connection connection = ConnectionFactory.createConnection(servlet.getConfiguration());\n      @SuppressWarnings(\"deprecation\")\n      Map<HRegionInfo, ServerName> regions = MetaTableAccessor\n          .allTableRegions(connection, tableName);\n      connection.close();\n      for (Map.Entry<HRegionInfo,ServerName> e: regions.entrySet()) {\n        HRegionInfo hri = e.getKey();\n        ServerName addr = e.getValue();\n        model.add(\n          new TableRegionModel(tableName.getNameAsString(), hri.getRegionId(),\n            hri.getStartKey(), hri.getEndKey(), addr.getHostAndPort()));\n      }\n      ResponseBuilder response = Response.ok(model);\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (TableNotFoundException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.NOT_FOUND)\n        .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n        .build();\n    } catch (IOException e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return Response.status(Response.Status.SERVICE_UNAVAILABLE)\n        .type(MIMETYPE_TEXT).entity(\"Unavailable\" + CRLF)\n        .build();\n    }\n  }"
        ],
        [
            "RowResource::postBinary(byte,UriInfo,HttpHeaders)",
            " 343  \n 344  \n 345  \n 346  \n 347 -\n 348 -\n 349  \n 350  \n 351  ",
            "  @POST\n  @Consumes(MIMETYPE_BINARY)\n  public Response postBinary(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"POST \" + uriInfo.getAbsolutePath() + \" as \"+MIMETYPE_BINARY);\n    }\n    return updateBinary(message, headers, false);\n  }",
            " 343  \n 344  \n 345  \n 346  \n 347 +\n 348 +\n 349  \n 350  \n 351  ",
            "  @POST\n  @Consumes(MIMETYPE_BINARY)\n  public Response postBinary(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"POST \" + uriInfo.getAbsolutePath() + \" as \"+MIMETYPE_BINARY);\n    }\n    return updateBinary(message, headers, false);\n  }"
        ],
        [
            "RowResource::putBinary(byte,UriInfo,HttpHeaders)",
            " 321  \n 322  \n 323  \n 324  \n 325 -\n 326 -\n 327  \n 328  \n 329  ",
            "  @PUT\n  @Consumes(MIMETYPE_BINARY)\n  public Response putBinary(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath() + \" as \"+ MIMETYPE_BINARY);\n    }\n    return updateBinary(message, headers, true);\n  }",
            " 321  \n 322  \n 323  \n 324  \n 325 +\n 326 +\n 327  \n 328  \n 329  ",
            "  @PUT\n  @Consumes(MIMETYPE_BINARY)\n  public Response putBinary(final byte[] message,\n      final @Context UriInfo uriInfo, final @Context HttpHeaders headers) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath() + \" as \"+ MIMETYPE_BINARY);\n    }\n    return updateBinary(message, headers, true);\n  }"
        ],
        [
            "RootResource::get(UriInfo)",
            "  71  \n  72  \n  73  \n  74  \n  75 -\n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ResponseBuilder response = Response.ok(getTableList());\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }",
            "  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ResponseBuilder response = Response.ok(getTableList());\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "NamespacesInstanceResource::post(NamespacesInstanceModel,UriInfo)",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179 -\n 180 -\n 181  \n 182  \n 183  \n 184  ",
            "  /**\n   * Build a response for POST create namespace with properties specified.\n   * @param model properties used for create.\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return response code.\n   */\n  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final NamespacesInstanceModel model,\n      final @Context UriInfo uriInfo) {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return processUpdate(model, false, uriInfo);\n  }",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179 +\n 180 +\n 181  \n 182  \n 183  \n 184  ",
            "  /**\n   * Build a response for POST create namespace with properties specified.\n   * @param model properties used for create.\n   * @param uriInfo (JAX-RS context variable) request URL\n   * @return response code.\n   */\n  @POST\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response post(final NamespacesInstanceModel model,\n      final @Context UriInfo uriInfo) {\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"POST \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return processUpdate(model, false, uriInfo);\n  }"
        ],
        [
            "SchemaResource::put(TableSchemaModel,UriInfo)",
            " 198  \n 199  \n 200  \n 201 -\n 202  \n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final TableSchemaModel model, \n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return update(model, true, uriInfo);\n  }",
            " 198  \n 199  \n 200  \n 201 +\n 202  \n 203 +\n 204 +\n 205  \n 206  \n 207  \n 208  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final TableSchemaModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    return update(model, true, uriInfo);\n  }"
        ],
        [
            "RowResource::checkAndDelete(CellSetModel)",
            " 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630 -\n 631 -\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  ",
            "  /**\n   * Validates the input request parameters, parses columns from CellSetModel,\n   * and invokes checkAndDelete on HTable.\n   *\n   * @param model instance of CellSetModel\n   * @return Response 200 OK, 304 Not modified, 400 Bad request\n   */\n  Response checkAndDelete(final CellSetModel model) {\n    Table table = null;\n    Delete delete = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      if (model.getRows().size() != 1) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      RowModel rowModel = model.getRows().get(0);\n      byte[] key = rowModel.getKey();\n      if (key == null) {\n        key = rowspec.getRow();\n      }\n      if (key == null) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Row key found to be null.\" + CRLF)\n          .build();\n      }\n\n      List<CellModel> cellModels = rowModel.getCells();\n      int cellModelCount = cellModels.size();\n\n      delete = new Delete(key);\n      boolean retValue;\n      CellModel valueToDeleteCell = rowModel.getCells().get(cellModelCount -1);\n      byte[] valueToDeleteColumn = valueToDeleteCell.getColumn();\n      if (valueToDeleteColumn == null) {\n        try {\n          valueToDeleteColumn = rowspec.getColumns()[0];\n        } catch (final ArrayIndexOutOfBoundsException e) {\n          servlet.getMetrics().incrementFailedDeleteRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Column not specified for check.\" + CRLF)\n            .build();\n        }\n      }\n\n      byte[][] parts ;\n      // Copy all the cells to the Delete request if extra cells are sent\n      if(cellModelCount > 1) {\n        for (int i = 0, n = cellModelCount - 1; i < n; i++) {\n          CellModel cell = cellModels.get(i);\n          byte[] col = cell.getColumn();\n\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n                    .build();\n          }\n\n          parts = KeyValue.parseColumn(col);\n\n          if (parts.length == 1) {\n            // Only Column Family is specified\n            delete.addFamily(parts[0], cell.getTimestamp());\n          } else if (parts.length == 2) {\n            delete.addColumn(parts[0], parts[1], cell.getTimestamp());\n          } else {\n            servlet.getMetrics().incrementFailedDeleteRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT)\n                    .entity(\"Bad request: Column to delete incorrectly specified.\" + CRLF)\n                    .build();\n          }\n        }\n      }\n\n      parts = KeyValue.parseColumn(valueToDeleteColumn);\n      if (parts.length == 2) {\n        if (parts[1].length != 0) {\n          // To support backcompat of deleting a cell\n          // if that is the only cell passed to the rest api\n          if(cellModelCount == 1) {\n            delete.addColumns(parts[0], parts[1]);\n          }\n          retValue = table.checkAndDelete(key, parts[0], parts[1],\n            valueToDeleteCell.getValue(), delete);\n        } else {\n          // The case of empty qualifier.\n          if(cellModelCount == 1) {\n            delete.addColumns(parts[0], Bytes.toBytes(StringUtils.EMPTY));\n          }\n          retValue = table.checkAndDelete(key, parts[0], Bytes.toBytes(StringUtils.EMPTY),\n            valueToDeleteCell.getValue(), delete);\n        }\n      } else {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Column to check incorrectly specified.\" + CRLF)\n          .build();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"CHECK-AND-DELETE \" + delete.toString() + \", returns \"\n          + retValue);\n      }\n\n      if (!retValue) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.NOT_MODIFIED)\n            .type(MIMETYPE_TEXT).entity(\" Delete check failed.\" + CRLF)\n            .build();\n      }\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }",
            " 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630 +\n 631 +\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  ",
            "  /**\n   * Validates the input request parameters, parses columns from CellSetModel,\n   * and invokes checkAndDelete on HTable.\n   *\n   * @param model instance of CellSetModel\n   * @return Response 200 OK, 304 Not modified, 400 Bad request\n   */\n  Response checkAndDelete(final CellSetModel model) {\n    Table table = null;\n    Delete delete = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      if (model.getRows().size() != 1) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n          .build();\n      }\n      RowModel rowModel = model.getRows().get(0);\n      byte[] key = rowModel.getKey();\n      if (key == null) {\n        key = rowspec.getRow();\n      }\n      if (key == null) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Row key found to be null.\" + CRLF)\n          .build();\n      }\n\n      List<CellModel> cellModels = rowModel.getCells();\n      int cellModelCount = cellModels.size();\n\n      delete = new Delete(key);\n      boolean retValue;\n      CellModel valueToDeleteCell = rowModel.getCells().get(cellModelCount -1);\n      byte[] valueToDeleteColumn = valueToDeleteCell.getColumn();\n      if (valueToDeleteColumn == null) {\n        try {\n          valueToDeleteColumn = rowspec.getColumns()[0];\n        } catch (final ArrayIndexOutOfBoundsException e) {\n          servlet.getMetrics().incrementFailedDeleteRequests(1);\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request: Column not specified for check.\" + CRLF)\n            .build();\n        }\n      }\n\n      byte[][] parts ;\n      // Copy all the cells to the Delete request if extra cells are sent\n      if(cellModelCount > 1) {\n        for (int i = 0, n = cellModelCount - 1; i < n; i++) {\n          CellModel cell = cellModels.get(i);\n          byte[] col = cell.getColumn();\n\n          if (col == null) {\n            servlet.getMetrics().incrementFailedPutRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT).entity(\"Bad request: Column found to be null.\" + CRLF)\n                    .build();\n          }\n\n          parts = KeyValue.parseColumn(col);\n\n          if (parts.length == 1) {\n            // Only Column Family is specified\n            delete.addFamily(parts[0], cell.getTimestamp());\n          } else if (parts.length == 2) {\n            delete.addColumn(parts[0], parts[1], cell.getTimestamp());\n          } else {\n            servlet.getMetrics().incrementFailedDeleteRequests(1);\n            return Response.status(Response.Status.BAD_REQUEST)\n                    .type(MIMETYPE_TEXT)\n                    .entity(\"Bad request: Column to delete incorrectly specified.\" + CRLF)\n                    .build();\n          }\n        }\n      }\n\n      parts = KeyValue.parseColumn(valueToDeleteColumn);\n      if (parts.length == 2) {\n        if (parts[1].length != 0) {\n          // To support backcompat of deleting a cell\n          // if that is the only cell passed to the rest api\n          if(cellModelCount == 1) {\n            delete.addColumns(parts[0], parts[1]);\n          }\n          retValue = table.checkAndDelete(key, parts[0], parts[1],\n            valueToDeleteCell.getValue(), delete);\n        } else {\n          // The case of empty qualifier.\n          if(cellModelCount == 1) {\n            delete.addColumns(parts[0], Bytes.toBytes(StringUtils.EMPTY));\n          }\n          retValue = table.checkAndDelete(key, parts[0], Bytes.toBytes(StringUtils.EMPTY),\n            valueToDeleteCell.getValue(), delete);\n        }\n      } else {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.BAD_REQUEST)\n          .type(MIMETYPE_TEXT).entity(\"Bad request: Column to check incorrectly specified.\" + CRLF)\n          .build();\n      }\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"CHECK-AND-DELETE \" + delete.toString() + \", returns \"\n          + retValue);\n      }\n\n      if (!retValue) {\n        servlet.getMetrics().incrementFailedDeleteRequests(1);\n        return Response.status(Response.Status.NOT_MODIFIED)\n            .type(MIMETYPE_TEXT).entity(\" Delete check failed.\" + CRLF)\n            .build();\n      }\n      ResponseBuilder response = Response.ok();\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n  }"
        ],
        [
            "RESTServer::parseCommandLine(String,RESTServlet)",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 -\n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  private static void parseCommandLine(String[] args, RESTServlet servlet) {\n    Options options = new Options();\n    options.addOption(\"p\", \"port\", true, \"Port to bind to [default: 8080]\");\n    options.addOption(\"ro\", \"readonly\", false, \"Respond only to GET HTTP \" +\n      \"method requests [default: false]\");\n    options.addOption(null, \"infoport\", true, \"Port for web UI\");\n\n    CommandLine commandLine = null;\n    try {\n      commandLine = new PosixParser().parse(options, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      printUsageAndExit(options, -1);\n    }\n\n    // check for user-defined port setting, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"port\")) {\n      String val = commandLine.getOptionValue(\"port\");\n      servlet.getConfiguration().setInt(\"hbase.rest.port\", Integer.parseInt(val));\n      LOG.debug(\"port set to \" + val);\n    }\n\n    // check if server should only process GET requests, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"readonly\")) {\n      servlet.getConfiguration().setBoolean(\"hbase.rest.readonly\", true);\n      LOG.debug(\"readonly set to true\");\n    }\n\n    // check for user-defined info server port setting, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"infoport\")) {\n      String val = commandLine.getOptionValue(\"infoport\");\n      servlet.getConfiguration().setInt(\"hbase.rest.info.port\", Integer.parseInt(val));\n      LOG.debug(\"Web UI port set to \" + val);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = commandLine != null ?\n        commandLine.getArgList() : new ArrayList<String>();\n    if (remainingArgs.size() != 1) {\n      printUsageAndExit(options, 1);\n    }\n\n    String command = remainingArgs.get(0);\n    if (\"start\".equals(command)) {\n      // continue and start container\n    } else if (\"stop\".equals(command)) {\n      System.exit(1);\n    } else {\n      printUsageAndExit(options, 1);\n    }\n  }",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179 +\n 180 +\n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189 +\n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  ",
            "  private static void parseCommandLine(String[] args, RESTServlet servlet) {\n    Options options = new Options();\n    options.addOption(\"p\", \"port\", true, \"Port to bind to [default: 8080]\");\n    options.addOption(\"ro\", \"readonly\", false, \"Respond only to GET HTTP \" +\n      \"method requests [default: false]\");\n    options.addOption(null, \"infoport\", true, \"Port for web UI\");\n\n    CommandLine commandLine = null;\n    try {\n      commandLine = new PosixParser().parse(options, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      printUsageAndExit(options, -1);\n    }\n\n    // check for user-defined port setting, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"port\")) {\n      String val = commandLine.getOptionValue(\"port\");\n      servlet.getConfiguration().setInt(\"hbase.rest.port\", Integer.parseInt(val));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"port set to \" + val);\n      }\n    }\n\n    // check if server should only process GET requests, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"readonly\")) {\n      servlet.getConfiguration().setBoolean(\"hbase.rest.readonly\", true);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"readonly set to true\");\n      }\n    }\n\n    // check for user-defined info server port setting, if so override the conf\n    if (commandLine != null && commandLine.hasOption(\"infoport\")) {\n      String val = commandLine.getOptionValue(\"infoport\");\n      servlet.getConfiguration().setInt(\"hbase.rest.info.port\", Integer.parseInt(val));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Web UI port set to \" + val);\n      }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = commandLine != null ?\n        commandLine.getArgList() : new ArrayList<String>();\n    if (remainingArgs.size() != 1) {\n      printUsageAndExit(options, 1);\n    }\n\n    String command = remainingArgs.get(0);\n    if (\"start\".equals(command)) {\n      // continue and start container\n    } else if (\"stop\".equals(command)) {\n      System.exit(1);\n    } else {\n      printUsageAndExit(options, 1);\n    }\n  }"
        ],
        [
            "RowResource::get(UriInfo)",
            "  84  \n  85  \n  86  \n  87  \n  88 -\n  89 -\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "  @GET\n  @Produces({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n    try {\n      ResultGenerator generator =\n        ResultGenerator.fromRowSpec(tableResource.getName(), rowspec, null,\n          !params.containsKey(NOCACHE_PARAM_NAME));\n      if (!generator.hasNext()) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      }\n      int count = 0;\n      CellSetModel model = new CellSetModel();\n      Cell value = generator.next();\n      byte[] rowKey = CellUtil.cloneRow(value);\n      RowModel rowModel = new RowModel(rowKey);\n      do {\n        if (!Bytes.equals(CellUtil.cloneRow(value), rowKey)) {\n          model.addRow(rowModel);\n          rowKey = CellUtil.cloneRow(value);\n          rowModel = new RowModel(rowKey);\n        }\n        rowModel.addCell(new CellModel(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value),\n          value.getTimestamp(), CellUtil.cloneValue(value)));\n        if (++count > rowspec.getMaxValues()) {\n          break;\n        }\n        value = generator.next();\n      } while (value != null);\n      model.addRow(rowModel);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(model).build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    }\n  }",
            "  84  \n  85  \n  86  \n  87  \n  88 +\n  89 +\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "  @GET\n  @Produces({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n    try {\n      ResultGenerator generator =\n        ResultGenerator.fromRowSpec(tableResource.getName(), rowspec, null,\n          !params.containsKey(NOCACHE_PARAM_NAME));\n      if (!generator.hasNext()) {\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n          .type(MIMETYPE_TEXT).entity(\"Not found\" + CRLF)\n          .build();\n      }\n      int count = 0;\n      CellSetModel model = new CellSetModel();\n      Cell value = generator.next();\n      byte[] rowKey = CellUtil.cloneRow(value);\n      RowModel rowModel = new RowModel(rowKey);\n      do {\n        if (!Bytes.equals(CellUtil.cloneRow(value), rowKey)) {\n          model.addRow(rowModel);\n          rowKey = CellUtil.cloneRow(value);\n          rowModel = new RowModel(rowKey);\n        }\n        rowModel.addCell(new CellModel(CellUtil.cloneFamily(value), CellUtil.cloneQualifier(value),\n          value.getTimestamp(), CellUtil.cloneValue(value)));\n        if (++count > rowspec.getMaxValues()) {\n          break;\n        }\n        value = generator.next();\n      } while (value != null);\n      model.addRow(rowModel);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return Response.ok(model).build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedPutRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "RowResource::delete(UriInfo)",
            " 353  \n 354  \n 355 -\n 356 -\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 -\n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 -\n 411  \n 412  \n 413  \n 414  ",
            "  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    Delete delete = null;\n    if (rowspec.hasTimestamp())\n      delete = new Delete(rowspec.getRow(), rowspec.getTimestamp());\n    else\n      delete = new Delete(rowspec.getRow());\n\n    for (byte[] column: rowspec.getColumns()) {\n      byte[][] split = KeyValue.parseColumn(column);\n      if (rowspec.hasTimestamp()) {\n        if (split.length == 1) {\n          delete.addFamily(split[0], rowspec.getTimestamp());\n        } else if (split.length == 2) {\n          delete.addColumns(split[0], split[1], rowspec.getTimestamp());\n        } else {\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n            .build();\n        }\n      } else {\n        if (split.length == 1) {\n          delete.addFamily(split[0]);\n        } else if (split.length == 2) {\n          delete.addColumns(split[0], split[1]);\n        } else {\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n            .build();\n        }\n      }\n    }\n    Table table = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      table.delete(delete);\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"DELETE \" + delete.toString());\n      }\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(ioe);\n      }\n    }\n    return Response.ok().build();\n  }",
            " 353  \n 354  \n 355 +\n 356 +\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 +\n 401 +\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 +\n 411  \n 412  \n 413  \n 414  ",
            "  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    Delete delete = null;\n    if (rowspec.hasTimestamp())\n      delete = new Delete(rowspec.getRow(), rowspec.getTimestamp());\n    else\n      delete = new Delete(rowspec.getRow());\n\n    for (byte[] column: rowspec.getColumns()) {\n      byte[][] split = KeyValue.parseColumn(column);\n      if (rowspec.hasTimestamp()) {\n        if (split.length == 1) {\n          delete.addFamily(split[0], rowspec.getTimestamp());\n        } else if (split.length == 2) {\n          delete.addColumns(split[0], split[1], rowspec.getTimestamp());\n        } else {\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n            .build();\n        }\n      } else {\n        if (split.length == 1) {\n          delete.addFamily(split[0]);\n        } else if (split.length == 2) {\n          delete.addColumns(split[0], split[1]);\n        } else {\n          return Response.status(Response.Status.BAD_REQUEST)\n            .type(MIMETYPE_TEXT).entity(\"Bad request\" + CRLF)\n            .build();\n        }\n      }\n    }\n    Table table = null;\n    try {\n      table = servlet.getTable(tableResource.getName());\n      table.delete(delete);\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"DELETE \" + delete.toString());\n      }\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n      return processException(e);\n    } finally {\n      if (table != null) try {\n        table.close();\n      } catch (IOException ioe) {\n        LOG.debug(\"Exception received while closing the table\", ioe);\n      }\n    }\n    return Response.ok().build();\n  }"
        ],
        [
            "MultiRowResource::get(UriInfo)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "  @GET\n  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })\n  public Response get(final @Context UriInfo uriInfo) {\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      CellSetModel model = new CellSetModel();\n      for (String rk : params.get(ROW_KEYS_PARAM_NAME)) {\n        RowSpec rowSpec = new RowSpec(rk);\n\n        if (this.versions != null) {\n          rowSpec.setMaxVersions(this.versions);\n        }\n        ResultGenerator generator =\n          ResultGenerator.fromRowSpec(this.tableResource.getName(), rowSpec, null,\n            !params.containsKey(NOCACHE_PARAM_NAME));\n        Cell value = null;\n        RowModel rowModel = new RowModel(rk);\n        if (generator.hasNext()) {\n          while ((value = generator.next()) != null) {\n            rowModel.addCell(new CellModel(CellUtil.cloneFamily(value), CellUtil\n                .cloneQualifier(value), value.getTimestamp(), CellUtil.cloneValue(value)));\n          }\n          model.addRow(rowModel);\n        } else {\n          LOG.trace(\"The row : \" + rk + \" not found in the table.\");\n        }\n      }\n\n      if (model.getRows().size() == 0) {\n      //If no rows found.\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n            .type(MIMETYPE_TEXT).entity(\"No rows found.\" + CRLF)\n            .build();\n      } else {\n        servlet.getMetrics().incrementSucessfulGetRequests(1);\n        return Response.ok(model).build();\n      }\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @GET\n  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })\n  public Response get(final @Context UriInfo uriInfo) {\n    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();\n\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      CellSetModel model = new CellSetModel();\n      for (String rk : params.get(ROW_KEYS_PARAM_NAME)) {\n        RowSpec rowSpec = new RowSpec(rk);\n\n        if (this.versions != null) {\n          rowSpec.setMaxVersions(this.versions);\n        }\n        ResultGenerator generator =\n          ResultGenerator.fromRowSpec(this.tableResource.getName(), rowSpec, null,\n            !params.containsKey(NOCACHE_PARAM_NAME));\n        Cell value = null;\n        RowModel rowModel = new RowModel(rk);\n        if (generator.hasNext()) {\n          while ((value = generator.next()) != null) {\n            rowModel.addCell(new CellModel(CellUtil.cloneFamily(value), CellUtil\n                .cloneQualifier(value), value.getTimestamp(), CellUtil.cloneValue(value)));\n          }\n          model.addRow(rowModel);\n        } else {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"The row : \" + rk + \" not found in the table.\");\n          }\n        }\n      }\n\n      if (model.getRows().size() == 0) {\n      //If no rows found.\n        servlet.getMetrics().incrementFailedGetRequests(1);\n        return Response.status(Response.Status.NOT_FOUND)\n            .type(MIMETYPE_TEXT).entity(\"No rows found.\" + CRLF)\n            .build();\n      } else {\n        servlet.getMetrics().incrementSucessfulGetRequests(1);\n        return Response.ok(model).build();\n      }\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }"
        ],
        [
            "AuthFilter::getConfiguration(String,FilterConfig)",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "  /**\n   * Returns the configuration to be used by the authentication filter\n   * to initialize the authentication handler.\n   *\n   * This filter retrieves all HBase configurations and passes those started\n   * with REST_PREFIX to the authentication handler.  It is useful to support\n   * plugging different authentication handlers.\n  */\n  @Override\n  protected Properties getConfiguration(\n      String configPrefix, FilterConfig filterConfig) throws ServletException {\n    Properties props = super.getConfiguration(configPrefix, filterConfig);\n    //setting the cookie path to root '/' so it is used for all resources.\n    props.setProperty(AuthenticationFilter.COOKIE_PATH, \"/\");\n\n    Configuration conf = HBaseConfiguration.create();\n    for (Map.Entry<String, String> entry : conf) {\n      String name = entry.getKey();\n      if (name.startsWith(REST_PREFIX)) {\n        String value = entry.getValue();\n        if(name.equals(REST_AUTHENTICATION_PRINCIPAL))  {\n          try {\n            String machineName = Strings.domainNamePointerToHostName(\n              DNS.getDefaultHost(conf.get(REST_DNS_INTERFACE, \"default\"),\n                conf.get(REST_DNS_NAMESERVER, \"default\")));\n            value = SecurityUtil.getServerPrincipal(value, machineName);\n          } catch (IOException ie) {\n            throw new ServletException(\"Failed to retrieve server principal\", ie);\n          }\n        }\n        LOG.debug(\"Setting property \" + name + \"=\" + value);\n        name = name.substring(REST_PREFIX_LEN);\n        props.setProperty(name, value);\n      }\n    }\n    return props;\n  }",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "  /**\n   * Returns the configuration to be used by the authentication filter\n   * to initialize the authentication handler.\n   *\n   * This filter retrieves all HBase configurations and passes those started\n   * with REST_PREFIX to the authentication handler.  It is useful to support\n   * plugging different authentication handlers.\n  */\n  @Override\n  protected Properties getConfiguration(\n      String configPrefix, FilterConfig filterConfig) throws ServletException {\n    Properties props = super.getConfiguration(configPrefix, filterConfig);\n    //setting the cookie path to root '/' so it is used for all resources.\n    props.setProperty(AuthenticationFilter.COOKIE_PATH, \"/\");\n\n    Configuration conf = HBaseConfiguration.create();\n    for (Map.Entry<String, String> entry : conf) {\n      String name = entry.getKey();\n      if (name.startsWith(REST_PREFIX)) {\n        String value = entry.getValue();\n        if(name.equals(REST_AUTHENTICATION_PRINCIPAL))  {\n          try {\n            String machineName = Strings.domainNamePointerToHostName(\n              DNS.getDefaultHost(conf.get(REST_DNS_INTERFACE, \"default\"),\n                conf.get(REST_DNS_NAMESERVER, \"default\")));\n            value = SecurityUtil.getServerPrincipal(value, machineName);\n          } catch (IOException ie) {\n            throw new ServletException(\"Failed to retrieve server principal\", ie);\n          }\n        }\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Setting property \" + name + \"=\" + value);\n        }\n        name = name.substring(REST_PREFIX_LEN);\n        props.setProperty(name, value);\n      }\n    }\n    return props;\n  }"
        ],
        [
            "RowResource::put(CellSetModel,UriInfo)",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314 -\n 315 -\n 316  \n 317  \n 318  \n 319  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final CellSetModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PUT \" + uriInfo.getAbsolutePath()\n        + \" \" + uriInfo.getQueryParameters());\n    }\n    return update(model, true);\n  }",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314 +\n 315 +\n 316  \n 317  \n 318  \n 319  ",
            "  @PUT\n  @Consumes({MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response put(final CellSetModel model,\n      final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PUT \" + uriInfo.getAbsolutePath()\n        + \" \" + uriInfo.getQueryParameters());\n    }\n    return update(model, true);\n  }"
        ],
        [
            "TableResource::getScanResource(UriInfo,String,String,int,String,String,List,int,int,long,long,boolean,String)",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150 -\n 151 -\n 152 -\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172 -\n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  @Path(\"{scanspec: .*[*]$}\")\n  public TableScanResource  getScanResource(\n      final @Context UriInfo uriInfo,\n      final @PathParam(\"scanspec\") String scanSpec,\n      final @HeaderParam(\"Accept\") String contentType,\n      @DefaultValue(Integer.MAX_VALUE + \"\")\n      @QueryParam(Constants.SCAN_LIMIT) int userRequestedLimit,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_START_ROW) String startRow,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_END_ROW) String endRow,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_COLUMN) List<String> column,\n      @DefaultValue(\"1\") @QueryParam(Constants.SCAN_MAX_VERSIONS) int maxVersions,\n      @DefaultValue(\"-1\") @QueryParam(Constants.SCAN_BATCH_SIZE) int batchSize,\n      @DefaultValue(\"0\") @QueryParam(Constants.SCAN_START_TIME) long startTime,\n      @DefaultValue(Long.MAX_VALUE + \"\") @QueryParam(Constants.SCAN_END_TIME) long endTime,\n      @DefaultValue(\"true\") @QueryParam(Constants.SCAN_BATCH_SIZE) boolean cacheBlocks, \n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_FILTER) String filters) {\n    try {\n      Filter filter = null;\n      Scan tableScan = new Scan();\n      if (scanSpec.indexOf('*') > 0) {\n        String prefix = scanSpec.substring(0, scanSpec.indexOf('*'));\n        byte[] prefixBytes = Bytes.toBytes(prefix);\n        filter = new PrefixFilter(Bytes.toBytes(prefix));\n        if (startRow.isEmpty()) {\n          tableScan.setStartRow(prefixBytes);\n        }\n      }\n      LOG.debug(\"Query parameters  : Table Name = > \" + this.table + \" Start Row => \" + startRow\n          + \" End Row => \" + endRow + \" Columns => \" + column + \" Start Time => \" + startTime\n          + \" End Time => \" + endTime + \" Cache Blocks => \" + cacheBlocks + \" Max Versions => \"\n          + maxVersions + \" Batch Size => \" + batchSize);\n      Table hTable = RESTServlet.getInstance().getTable(this.table);\n      tableScan.setBatch(batchSize);\n      tableScan.setMaxVersions(maxVersions);\n      tableScan.setTimeRange(startTime, endTime);\n      if (!startRow.isEmpty()) {\n        tableScan.setStartRow(Bytes.toBytes(startRow));\n      }\n      tableScan.setStopRow(Bytes.toBytes(endRow));\n      for (String csplit : column) {\n        String[] familysplit = csplit.trim().split(\":\");\n        if (familysplit.length == 2) {\n          if (familysplit[1].length() > 0) {\n            LOG.debug(\"Scan family and column : \" + familysplit[0] + \"  \" + familysplit[1]);\n            tableScan.addColumn(Bytes.toBytes(familysplit[0]), Bytes.toBytes(familysplit[1]));\n          } else {\n            tableScan.addFamily(Bytes.toBytes(familysplit[0]));\n            LOG.debug(\"Scan family : \" + familysplit[0] + \" and empty qualifier.\");\n            tableScan.addColumn(Bytes.toBytes(familysplit[0]), null);\n          }\n        } else if (StringUtils.isNotEmpty(familysplit[0])){\n          LOG.debug(\"Scan family : \" + familysplit[0]);\n          tableScan.addFamily(Bytes.toBytes(familysplit[0]));\n        }\n      }\n      FilterList filterList = null;\n      if (StringUtils.isNotEmpty(filters)) {\n          ParseFilter pf = new ParseFilter();\n          Filter filterParam = pf.parseFilterString(filters);\n          if (filter != null) {\n            filterList = new FilterList(filter, filterParam);\n          }\n          else {\n            filter = filterParam;\n          }\n      }\n      if (filterList != null) {\n        tableScan.setFilter(filterList);\n      } else if (filter != null) {\n        tableScan.setFilter(filter);\n      }\n      int fetchSize = this.servlet.getConfiguration().getInt(Constants.SCAN_FETCH_SIZE, 10);\n      tableScan.setCaching(fetchSize);\n      return new TableScanResource(hTable.getScanner(tableScan), userRequestedLimit);\n    } catch (IOException exp) {\n      servlet.getMetrics().incrementFailedScanRequests(1);\n      processException(exp);\n      LOG.warn(exp);\n      return null;\n    }\n  }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150 +\n 151 +\n 152 +\n 153 +\n 154 +\n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168 +\n 169 +\n 170  \n 171  \n 172  \n 173 +\n 174 +\n 175 +\n 176  \n 177  \n 178 +\n 179 +\n 180 +\n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  ",
            "  @Path(\"{scanspec: .*[*]$}\")\n  public TableScanResource  getScanResource(\n      final @Context UriInfo uriInfo,\n      final @PathParam(\"scanspec\") String scanSpec,\n      final @HeaderParam(\"Accept\") String contentType,\n      @DefaultValue(Integer.MAX_VALUE + \"\")\n      @QueryParam(Constants.SCAN_LIMIT) int userRequestedLimit,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_START_ROW) String startRow,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_END_ROW) String endRow,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_COLUMN) List<String> column,\n      @DefaultValue(\"1\") @QueryParam(Constants.SCAN_MAX_VERSIONS) int maxVersions,\n      @DefaultValue(\"-1\") @QueryParam(Constants.SCAN_BATCH_SIZE) int batchSize,\n      @DefaultValue(\"0\") @QueryParam(Constants.SCAN_START_TIME) long startTime,\n      @DefaultValue(Long.MAX_VALUE + \"\") @QueryParam(Constants.SCAN_END_TIME) long endTime,\n      @DefaultValue(\"true\") @QueryParam(Constants.SCAN_BATCH_SIZE) boolean cacheBlocks,\n      @DefaultValue(\"\") @QueryParam(Constants.SCAN_FILTER) String filters) {\n    try {\n      Filter filter = null;\n      Scan tableScan = new Scan();\n      if (scanSpec.indexOf('*') > 0) {\n        String prefix = scanSpec.substring(0, scanSpec.indexOf('*'));\n        byte[] prefixBytes = Bytes.toBytes(prefix);\n        filter = new PrefixFilter(Bytes.toBytes(prefix));\n        if (startRow.isEmpty()) {\n          tableScan.setStartRow(prefixBytes);\n        }\n      }\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Query parameters  : Table Name = > \" + this.table + \" Start Row => \" + startRow\n            + \" End Row => \" + endRow + \" Columns => \" + column + \" Start Time => \" + startTime\n            + \" End Time => \" + endTime + \" Cache Blocks => \" + cacheBlocks + \" Max Versions => \"\n            + maxVersions + \" Batch Size => \" + batchSize);\n      }\n      Table hTable = RESTServlet.getInstance().getTable(this.table);\n      tableScan.setBatch(batchSize);\n      tableScan.setMaxVersions(maxVersions);\n      tableScan.setTimeRange(startTime, endTime);\n      if (!startRow.isEmpty()) {\n        tableScan.setStartRow(Bytes.toBytes(startRow));\n      }\n      tableScan.setStopRow(Bytes.toBytes(endRow));\n      for (String csplit : column) {\n        String[] familysplit = csplit.trim().split(\":\");\n        if (familysplit.length == 2) {\n          if (familysplit[1].length() > 0) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Scan family and column : \" + familysplit[0] + \"  \" + familysplit[1]);\n            }\n            tableScan.addColumn(Bytes.toBytes(familysplit[0]), Bytes.toBytes(familysplit[1]));\n          } else {\n            tableScan.addFamily(Bytes.toBytes(familysplit[0]));\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Scan family : \" + familysplit[0] + \" and empty qualifier.\");\n            }\n            tableScan.addColumn(Bytes.toBytes(familysplit[0]), null);\n          }\n        } else if (StringUtils.isNotEmpty(familysplit[0])) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Scan family : \" + familysplit[0]);\n          }\n          tableScan.addFamily(Bytes.toBytes(familysplit[0]));\n        }\n      }\n      FilterList filterList = null;\n      if (StringUtils.isNotEmpty(filters)) {\n          ParseFilter pf = new ParseFilter();\n          Filter filterParam = pf.parseFilterString(filters);\n          if (filter != null) {\n            filterList = new FilterList(filter, filterParam);\n          }\n          else {\n            filter = filterParam;\n          }\n      }\n      if (filterList != null) {\n        tableScan.setFilter(filterList);\n      } else if (filter != null) {\n        tableScan.setFilter(filter);\n      }\n      int fetchSize = this.servlet.getConfiguration().getInt(Constants.SCAN_FETCH_SIZE, 10);\n      tableScan.setCaching(fetchSize);\n      return new TableScanResource(hTable.getScanner(tableScan), userRequestedLimit);\n    } catch (IOException exp) {\n      servlet.getMetrics().incrementFailedScanRequests(1);\n      processException(exp);\n      LOG.warn(exp);\n      return null;\n    }\n  }"
        ],
        [
            "ScannerInstanceResource::delete(UriInfo)",
            " 183  \n 184  \n 185 -\n 186 -\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    if (ScannerResource.delete(id)) {\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n    } else {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n    }\n    return Response.ok().build();\n  }",
            " 187  \n 188  \n 189 +\n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "  @DELETE\n  public Response delete(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"DELETE \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    if (servlet.isReadOnly()) {\n      return Response.status(Response.Status.FORBIDDEN)\n        .type(MIMETYPE_TEXT).entity(\"Forbidden\" + CRLF)\n        .build();\n    }\n    if (ScannerResource.delete(id)) {\n      servlet.getMetrics().incrementSucessfulDeleteRequests(1);\n    } else {\n      servlet.getMetrics().incrementFailedDeleteRequests(1);\n    }\n    return Response.ok().build();\n  }"
        ],
        [
            "ProtobufMessageBodyConsumer::readFrom(Class,Type,Annotation,MediaType,MultivaluedMap,InputStream)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "  @Override\n  public ProtobufMessageHandler readFrom(Class<ProtobufMessageHandler> type, Type genericType,\n      Annotation[] annotations, MediaType mediaType,\n      MultivaluedMap<String, String> httpHeaders, InputStream inputStream)\n      throws IOException, WebApplicationException {\n    ProtobufMessageHandler obj = null;\n    try {\n      obj = type.newInstance();\n      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n      byte[] buffer = new byte[4096];\n      int read;\n      do {\n        read = inputStream.read(buffer, 0, buffer.length);\n        if (read > 0) {\n          baos.write(buffer, 0, read);\n        }\n      } while (read > 0);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(getClass() + \": read \" + baos.size() + \" bytes from \" +\n          inputStream);\n      }\n      obj = obj.getObjectFromMessage(baos.toByteArray());\n    } catch (InstantiationException e) {\n      throw new WebApplicationException(e);\n    } catch (IllegalAccessException e) {\n      throw new WebApplicationException(e);\n    }\n    return obj;\n  }",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "  @Override\n  public ProtobufMessageHandler readFrom(Class<ProtobufMessageHandler> type, Type genericType,\n      Annotation[] annotations, MediaType mediaType,\n      MultivaluedMap<String, String> httpHeaders, InputStream inputStream)\n      throws IOException, WebApplicationException {\n    ProtobufMessageHandler obj = null;\n    try {\n      obj = type.newInstance();\n      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n      byte[] buffer = new byte[4096];\n      int read;\n      do {\n        read = inputStream.read(buffer, 0, buffer.length);\n        if (read > 0) {\n          baos.write(buffer, 0, read);\n        }\n      } while (read > 0);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(getClass() + \": read \" + baos.size() + \" bytes from \" +\n          inputStream);\n      }\n      obj = obj.getObjectFromMessage(baos.toByteArray());\n    } catch (InstantiationException e) {\n      throw new WebApplicationException(e);\n    } catch (IllegalAccessException e) {\n      throw new WebApplicationException(e);\n    }\n    return obj;\n  }"
        ],
        [
            "SchemaResource::get(UriInfo)",
            "  85  \n  86  \n  87  \n  88  \n  89 -\n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ResponseBuilder response =\n        Response.ok(new TableSchemaModel(getTableSchema()));\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    } \n  }",
            "  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  ",
            "  @GET\n  @Produces({MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,\n    MIMETYPE_PROTOBUF_IETF})\n  public Response get(final @Context UriInfo uriInfo) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"GET \" + uriInfo.getAbsolutePath());\n    }\n    servlet.getMetrics().incrementRequests(1);\n    try {\n      ResponseBuilder response =\n        Response.ok(new TableSchemaModel(getTableSchema()));\n      response.cacheControl(cacheControl);\n      servlet.getMetrics().incrementSucessfulGetRequests(1);\n      return response.build();\n    } catch (Exception e) {\n      servlet.getMetrics().incrementFailedGetRequests(1);\n      return processException(e);\n    }\n  }"
        ]
    ],
    "39d43ab779a90f28273426ee2887daacaa6b1f48": [
        [
            "LoadIncrementalHFiles::groupOrSplit(Multimap,LoadQueueItem,Table,Pair)",
            " 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820 -\n 821 -\n 822 -\n 823 -\n 824  \n 825  \n 826  \n 827  \n 828  \n 829 -\n 830 -\n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893 -\n 894  \n 895  \n 896  \n 897  \n 898  \n 899  ",
            "  /**\n   * Attempt to assign the given load queue item into its target region group.\n   * If the hfile boundary no longer fits into a region, physically splits\n   * the hfile such that the new bottom half will fit and returns the list of\n   * LQI's corresponding to the resultant hfiles.\n   *\n   * protected for testing\n   * @throws IOException if an IO failure is encountered\n   */\n  protected List<LoadQueueItem> groupOrSplit(Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n      final LoadQueueItem item, final Table table,\n      final Pair<byte[][], byte[][]> startEndKeys)\n      throws IOException {\n    final Path hfilePath = item.hfilePath;\n    // fs is the source filesystem\n    if (fs == null) {\n      fs = hfilePath.getFileSystem(getConf());\n    }\n    HFile.Reader hfr = HFile.createReader(fs, hfilePath,\n        new CacheConfig(getConf()), getConf());\n    final byte[] first, last;\n    try {\n      hfr.loadFileInfo();\n      first = hfr.getFirstRowKey();\n      last = hfr.getLastRowKey();\n    }  finally {\n      hfr.close();\n    }\n\n    LOG.info(\"Trying to load hfile=\" + hfilePath +\n        \" first=\" + Bytes.toStringBinary(first) +\n        \" last=\"  + Bytes.toStringBinary(last));\n    if (first == null || last == null) {\n      assert first == null && last == null;\n      // TODO what if this is due to a bad HFile?\n      LOG.info(\"hfile \" + hfilePath + \" has no entries, skipping\");\n      return null;\n    }\n    if (Bytes.compareTo(first, last) > 0) {\n      throw new IllegalArgumentException(\n      \"Invalid range: \" + Bytes.toStringBinary(first) +\n      \" > \" + Bytes.toStringBinary(last));\n    }\n    int idx = Arrays.binarySearch(startEndKeys.getFirst(), first,\n        Bytes.BYTES_COMPARATOR);\n    if (idx < 0) {\n      // not on boundary, returns -(insertion index).  Calculate region it\n      // would be in.\n      idx = -(idx + 1) - 1;\n    }\n    final int indexForCallable = idx;\n\n    /**\n     * we can consider there is a region hole in following conditions. 1) if idx < 0,then first\n     * region info is lost. 2) if the endkey of a region is not equal to the startkey of the next\n     * region. 3) if the endkey of the last region is not empty.\n     */\n    if (indexForCallable < 0) {\n      throw new IOException(\"The first region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if ((indexForCallable == startEndKeys.getFirst().length - 1)\n        && !Bytes.equals(startEndKeys.getSecond()[indexForCallable], HConstants.EMPTY_BYTE_ARRAY)) {\n      throw new IOException(\"The last region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if (indexForCallable + 1 < startEndKeys.getFirst().length\n        && !(Bytes.compareTo(startEndKeys.getSecond()[indexForCallable],\n          startEndKeys.getFirst()[indexForCallable + 1]) == 0)) {\n      throw new IOException(\"The endkey of one region for table \"\n          + table.getName()\n          + \" is not equal to the startkey of the next region in hbase:meta.\"\n          + \"Please use hbck tool to fix it first.\");\n    }\n\n    boolean lastKeyInRange =\n      Bytes.compareTo(last, startEndKeys.getSecond()[idx]) < 0 ||\n      Bytes.equals(startEndKeys.getSecond()[idx], HConstants.EMPTY_BYTE_ARRAY);\n    if (!lastKeyInRange) {\n      List<LoadQueueItem> lqis = splitStoreFile(item, table,\n          startEndKeys.getFirst()[indexForCallable],\n          startEndKeys.getSecond()[indexForCallable]);\n      return lqis;\n    }\n\n    // group regions.\n    regionGroups.put(ByteBuffer.wrap(startEndKeys.getFirst()[idx]), item);\n    return null;\n  }",
            " 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 +\n 838 +\n 839 +\n 840  \n 841  \n 842  \n 843  \n 844  \n 845 +\n 846 +\n 847 +\n 848 +\n 849 +\n 850 +\n 851 +\n 852 +\n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915 +\n 916  \n 917  \n 918  \n 919  \n 920  \n 921  ",
            "  /**\n   * Attempt to assign the given load queue item into its target region group.\n   * If the hfile boundary no longer fits into a region, physically splits\n   * the hfile such that the new bottom half will fit and returns the list of\n   * LQI's corresponding to the resultant hfiles.\n   *\n   * protected for testing\n   * @throws IOException if an IO failure is encountered\n   */\n  protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups, final LoadQueueItem item, final Table table,\n      final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n    final Path hfilePath = item.hfilePath;\n    // fs is the source filesystem\n    if (fs == null) {\n      fs = hfilePath.getFileSystem(getConf());\n    }\n    HFile.Reader hfr = null;\n    try {\n      hfr = HFile.createReader(fs, hfilePath,\n          new CacheConfig(getConf()), getConf());\n    } catch (FileNotFoundException fnfe) {\n      LOG.debug(\"encountered\", fnfe);\n      return new Pair<>(null, hfilePath.getName());\n    }\n    final byte[] first, last;\n    try {\n      hfr.loadFileInfo();\n      first = hfr.getFirstRowKey();\n      last = hfr.getLastRowKey();\n    }  finally {\n      hfr.close();\n    }\n\n    LOG.info(\"Trying to load hfile=\" + hfilePath +\n        \" first=\" + Bytes.toStringBinary(first) +\n        \" last=\"  + Bytes.toStringBinary(last));\n    if (first == null || last == null) {\n      assert first == null && last == null;\n      // TODO what if this is due to a bad HFile?\n      LOG.info(\"hfile \" + hfilePath + \" has no entries, skipping\");\n      return null;\n    }\n    if (Bytes.compareTo(first, last) > 0) {\n      throw new IllegalArgumentException(\n      \"Invalid range: \" + Bytes.toStringBinary(first) +\n      \" > \" + Bytes.toStringBinary(last));\n    }\n    int idx = Arrays.binarySearch(startEndKeys.getFirst(), first,\n        Bytes.BYTES_COMPARATOR);\n    if (idx < 0) {\n      // not on boundary, returns -(insertion index).  Calculate region it\n      // would be in.\n      idx = -(idx + 1) - 1;\n    }\n    final int indexForCallable = idx;\n\n    /**\n     * we can consider there is a region hole in following conditions. 1) if idx < 0,then first\n     * region info is lost. 2) if the endkey of a region is not equal to the startkey of the next\n     * region. 3) if the endkey of the last region is not empty.\n     */\n    if (indexForCallable < 0) {\n      throw new IOException(\"The first region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if ((indexForCallable == startEndKeys.getFirst().length - 1)\n        && !Bytes.equals(startEndKeys.getSecond()[indexForCallable], HConstants.EMPTY_BYTE_ARRAY)) {\n      throw new IOException(\"The last region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if (indexForCallable + 1 < startEndKeys.getFirst().length\n        && !(Bytes.compareTo(startEndKeys.getSecond()[indexForCallable],\n          startEndKeys.getFirst()[indexForCallable + 1]) == 0)) {\n      throw new IOException(\"The endkey of one region for table \"\n          + table.getName()\n          + \" is not equal to the startkey of the next region in hbase:meta.\"\n          + \"Please use hbck tool to fix it first.\");\n    }\n\n    boolean lastKeyInRange =\n      Bytes.compareTo(last, startEndKeys.getSecond()[idx]) < 0 ||\n      Bytes.equals(startEndKeys.getSecond()[idx], HConstants.EMPTY_BYTE_ARRAY);\n    if (!lastKeyInRange) {\n      List<LoadQueueItem> lqis = splitStoreFile(item, table,\n          startEndKeys.getFirst()[indexForCallable],\n          startEndKeys.getSecond()[indexForCallable]);\n      return new Pair<>(lqis, null);\n    }\n\n    // group regions.\n    regionGroups.put(ByteBuffer.wrap(startEndKeys.getFirst()[idx]), item);\n    return null;\n  }"
        ],
        [
            "TestLoadIncrementalHFilesSplitRecovery::testGroupOrSplitWhenRegionHoleExistsInMeta()",
            " 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524 -\n 525  \n 526  \n 527  \n 528 -\n 529 -\n 530 -\n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  ",
            "  @Test (timeout=120000)\n  public void testGroupOrSplitWhenRegionHoleExistsInMeta() throws Exception {\n    TableName tableName = TableName.valueOf(\"testGroupOrSplitWhenRegionHoleExistsInMeta\");\n    byte[][] SPLIT_KEYS = new byte[][] { Bytes.toBytes(\"row_00000100\") };\n    // Share connection. We were failing to find the table with our new reverse scan because it\n    // looks for first region, not any region -- that is how it works now.  The below removes first\n    // region in test.  Was reliant on the Connection caching having first region.\n    Connection connection = ConnectionFactory.createConnection(util.getConfiguration());\n    Table table = connection.getTable(tableName);\n\n    setupTableWithSplitkeys(tableName, 10, SPLIT_KEYS);\n    Path dir = buildBulkFiles(tableName, 2);\n\n    final AtomicInteger countedLqis = new AtomicInteger();\n    LoadIncrementalHFiles loader = new LoadIncrementalHFiles(util.getConfiguration()) {\n\n      @Override\n      protected List<LoadQueueItem> groupOrSplit(\n          Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n          final LoadQueueItem item, final Table htable,\n          final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n        List<LoadQueueItem> lqis = super.groupOrSplit(regionGroups, item, htable, startEndKeys);\n        if (lqis != null) {\n          countedLqis.addAndGet(lqis.size());\n        }\n        return lqis;\n      }\n    };\n\n    // do bulkload when there is no region hole in hbase:meta.\n    try (Table t = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName);\n        Admin admin = connection.getAdmin()) {\n      loader.doBulkLoad(dir, admin, t, locator);\n    } catch (Exception e) {\n      LOG.error(\"exeception=\", e);\n    }\n    // check if all the data are loaded into the table.\n    this.assertExpectedTable(tableName, ROWCOUNT, 2);\n\n    dir = buildBulkFiles(tableName, 3);\n\n    // Mess it up by leaving a hole in the hbase:meta\n    List<HRegionInfo> regionInfos = MetaTableAccessor.getTableRegions(connection, tableName);\n    for (HRegionInfo regionInfo : regionInfos) {\n      if (Bytes.equals(regionInfo.getStartKey(), HConstants.EMPTY_BYTE_ARRAY)) {\n        MetaTableAccessor.deleteRegion(connection, regionInfo);\n        break;\n      }\n    }\n\n    try (Table t = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName);\n        Admin admin = connection.getAdmin()) {\n      loader.doBulkLoad(dir, admin, t, locator);\n    } catch (Exception e) {\n      LOG.error(\"exception=\", e);\n      assertTrue(\"IOException expected\", e instanceof IOException);\n    }\n\n    table.close();\n\n    // Make sure at least the one region that still exists can be found.\n    regionInfos = MetaTableAccessor.getTableRegions(connection, tableName);\n    assertTrue(regionInfos.size() >= 1);\n\n    this.assertExpectedTable(connection, tableName, ROWCOUNT, 2);\n    connection.close();\n  }",
            " 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 +\n 526  \n 527  \n 528  \n 529 +\n 530 +\n 531 +\n 532 +\n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  ",
            "  @Test (timeout=120000)\n  public void testGroupOrSplitWhenRegionHoleExistsInMeta() throws Exception {\n    TableName tableName = TableName.valueOf(\"testGroupOrSplitWhenRegionHoleExistsInMeta\");\n    byte[][] SPLIT_KEYS = new byte[][] { Bytes.toBytes(\"row_00000100\") };\n    // Share connection. We were failing to find the table with our new reverse scan because it\n    // looks for first region, not any region -- that is how it works now.  The below removes first\n    // region in test.  Was reliant on the Connection caching having first region.\n    Connection connection = ConnectionFactory.createConnection(util.getConfiguration());\n    Table table = connection.getTable(tableName);\n\n    setupTableWithSplitkeys(tableName, 10, SPLIT_KEYS);\n    Path dir = buildBulkFiles(tableName, 2);\n\n    final AtomicInteger countedLqis = new AtomicInteger();\n    LoadIncrementalHFiles loader = new LoadIncrementalHFiles(util.getConfiguration()) {\n\n      @Override\n      protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n          Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n          final LoadQueueItem item, final Table htable,\n          final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n        Pair<List<LoadQueueItem>, String> lqis = super.groupOrSplit(regionGroups, item, htable,\n            startEndKeys);\n        if (lqis != null && lqis.getFirst() != null) {\n          countedLqis.addAndGet(lqis.getFirst().size());\n        }\n        return lqis;\n      }\n    };\n\n    // do bulkload when there is no region hole in hbase:meta.\n    try (Table t = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName);\n        Admin admin = connection.getAdmin()) {\n      loader.doBulkLoad(dir, admin, t, locator);\n    } catch (Exception e) {\n      LOG.error(\"exeception=\", e);\n    }\n    // check if all the data are loaded into the table.\n    this.assertExpectedTable(tableName, ROWCOUNT, 2);\n\n    dir = buildBulkFiles(tableName, 3);\n\n    // Mess it up by leaving a hole in the hbase:meta\n    List<HRegionInfo> regionInfos = MetaTableAccessor.getTableRegions(connection, tableName);\n    for (HRegionInfo regionInfo : regionInfos) {\n      if (Bytes.equals(regionInfo.getStartKey(), HConstants.EMPTY_BYTE_ARRAY)) {\n        MetaTableAccessor.deleteRegion(connection, regionInfo);\n        break;\n      }\n    }\n\n    try (Table t = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName);\n        Admin admin = connection.getAdmin()) {\n      loader.doBulkLoad(dir, admin, t, locator);\n    } catch (Exception e) {\n      LOG.error(\"exception=\", e);\n      assertTrue(\"IOException expected\", e instanceof IOException);\n    }\n\n    table.close();\n\n    // Make sure at least the one region that still exists can be found.\n    regionInfos = MetaTableAccessor.getTableRegions(connection, tableName);\n    assertTrue(regionInfos.size() >= 1);\n\n    this.assertExpectedTable(connection, tableName, ROWCOUNT, 2);\n    connection.close();\n  }"
        ],
        [
            "LoadIncrementalHFiles::run(String)",
            "1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218 -\n1219  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    if (args.length < 2) {\n      usage();\n      return -1;\n    }\n\n    String dirPath = args[0];\n    TableName tableName = TableName.valueOf(args[1]);\n    return run(dirPath, null, tableName);\n  }",
            "1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239 +\n1240 +\n1241 +\n1242  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    if (args.length < 2) {\n      usage();\n      return -1;\n    }\n\n    String dirPath = args[0];\n    TableName tableName = TableName.valueOf(args[1]);\n    List<String> missingHFiles = run(dirPath, null, tableName);\n    if (missingHFiles == null) return 0;\n    return -1;\n  }"
        ],
        [
            "LoadIncrementalHFiles::doBulkLoad(Map,Admin,Table,RegionLocator,boolean,boolean)",
            " 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367 -\n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382 -\n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392 -\n 393  \n 394  \n 395  \n 396  ",
            "  /**\n   * Perform a bulk load of the given directory into the given\n   * pre-existing table.  This method is not threadsafe.\n   *\n   * @param map map of family to List of hfiles\n   * @param admin the Admin\n   * @param table the table to load into\n   * @param regionLocator region locator\n   * @param silence true to ignore unmatched column families\n   * @param copyFile always copy hfiles if true\n   * @throws TableNotFoundException if table does not yet exist\n   */\n  public void doBulkLoad(Map<byte[], List<Path>> map, final Admin admin, Table table,\n          RegionLocator regionLocator, boolean silence, boolean copyFile)\n              throws TableNotFoundException, IOException {\n    if (!admin.isTableAvailable(regionLocator.getName())) {\n      throw new TableNotFoundException(\"Table \" + table.getName() + \" is not currently available.\");\n    }\n    // LQI queue does not need to be threadsafe -- all operations on this queue\n    // happen in this thread\n    Deque<LoadQueueItem> queue = new LinkedList<>();\n    ExecutorService pool = null;\n    SecureBulkLoadClient secureClient = null;\n    try {\n      prepareHFileQueue(map, table, queue, silence);\n      if (queue.isEmpty()) {\n        LOG.warn(\"Bulk load operation did not get any files to load\");\n        return;\n      }\n      pool = createExecutorService();\n      secureClient = new SecureBulkLoadClient(table.getConfiguration(), table);\n      for (Map.Entry<byte[], List<Path>> entry : map.entrySet()) {\n        for (Path p : entry.getValue()) {\n          fs = p.getFileSystem(table.getConfiguration());\n          break;\n        }\n      }\n      performBulkLoad(admin, table, regionLocator, queue, pool, secureClient, copyFile);\n    } finally {\n      cleanup(admin, queue, pool, secureClient);\n    }\n  }",
            " 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383 +\n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393 +\n 394  \n 395  \n 396  \n 397  ",
            "  /**\n   * Perform a bulk load of the given directory into the given\n   * pre-existing table.  This method is not threadsafe.\n   *\n   * @param map map of family to List of hfiles\n   * @param admin the Admin\n   * @param table the table to load into\n   * @param regionLocator region locator\n   * @param silence true to ignore unmatched column families\n   * @param copyFile always copy hfiles if true\n   * @return List of filenames which were not found\n   * @throws TableNotFoundException if table does not yet exist\n   */\n  public List<String> doBulkLoad(Map<byte[], List<Path>> map, final Admin admin, Table table,\n          RegionLocator regionLocator, boolean silence, boolean copyFile)\n              throws TableNotFoundException, IOException {\n    if (!admin.isTableAvailable(regionLocator.getName())) {\n      throw new TableNotFoundException(\"Table \" + table.getName() + \" is not currently available.\");\n    }\n    // LQI queue does not need to be threadsafe -- all operations on this queue\n    // happen in this thread\n    Deque<LoadQueueItem> queue = new LinkedList<>();\n    ExecutorService pool = null;\n    SecureBulkLoadClient secureClient = null;\n    try {\n      prepareHFileQueue(map, table, queue, silence);\n      if (queue.isEmpty()) {\n        LOG.warn(\"Bulk load operation did not get any files to load\");\n        return null;\n      }\n      pool = createExecutorService();\n      secureClient = new SecureBulkLoadClient(table.getConfiguration(), table);\n      for (Map.Entry<byte[], List<Path>> entry : map.entrySet()) {\n        for (Path p : entry.getValue()) {\n          fs = p.getFileSystem(table.getConfiguration());\n          break;\n        }\n      }\n      return performBulkLoad(admin, table, regionLocator, queue, pool, secureClient, copyFile);\n    } finally {\n      cleanup(admin, queue, pool, secureClient);\n    }\n  }"
        ],
        [
            "LoadIncrementalHFiles::performBulkLoad(Admin,Table,RegionLocator,Deque,ExecutorService,SecureBulkLoadClient,boolean)",
            " 451 -\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486 -\n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  ",
            "  void performBulkLoad(final Admin admin, Table table, RegionLocator regionLocator,\n      Deque<LoadQueueItem> queue, ExecutorService pool,\n      SecureBulkLoadClient secureClient, boolean copyFile) throws IOException {\n    int count = 0;\n\n    if(isSecureBulkLoadEndpointAvailable()) {\n      LOG.warn(\"SecureBulkLoadEndpoint is deprecated. It will be removed in future releases.\");\n      LOG.warn(\"Secure bulk load has been integrated into HBase core.\");\n    }\n\n    //If using secure bulk load, get source delegation token, and\n    //prepare staging directory and token\n    // fs is the source filesystem\n    fsDelegationToken.acquireDelegationToken(fs);\n    bulkToken = secureClient.prepareBulkLoad(admin.getConnection());\n\n    // Assumes that region splits can happen while this occurs.\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      final Pair<byte[][], byte[][]> startEndKeys = regionLocator.getStartEndKeys();\n      if (count != 0) {\n        LOG.info(\"Split occured while grouping HFiles, retry attempt \" +\n            + count + \" with \" + queue.size() + \" files remaining to group or split\");\n      }\n\n      int maxRetries = getConf().getInt(HConstants.BULKLOAD_MAX_RETRIES_NUMBER, 10);\n      maxRetries = Math.max(maxRetries, startEndKeys.getFirst().length + 1);\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count +\n            \" times without completing, bailing out\");\n      }\n      count++;\n\n      // Using ByteBuffer for byte[] equality semantics\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups = groupOrSplitPhase(table,\n          pool, queue, startEndKeys);\n\n      if (!checkHFilesCountPerRegionPerFamily(regionGroups)) {\n        // Error is logged inside checkHFilesCountPerRegionPerFamily.\n        throw new IOException(\"Trying to load more than \" + maxFilesPerRegionPerFamily\n            + \" hfiles to one family of one region\");\n      }\n\n      bulkLoadPhase(table, admin.getConnection(), pool, queue, regionGroups, copyFile);\n\n      // NOTE: The next iteration's split / group could happen in parallel to\n      // atomic bulkloads assuming that there are splits and no merges, and\n      // that we can atomically pull out the groups we want to retry.\n    }\n\n    if (!queue.isEmpty()) {\n      throw new RuntimeException(\"Bulk load aborted with some files not yet loaded.\"\n        + \"Please check log for more details.\");\n    }\n  }",
            " 452 +\n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467 +\n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487 +\n 488 +\n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507 +\n 508 +\n 509  ",
            "  List<String> performBulkLoad(final Admin admin, Table table, RegionLocator regionLocator,\n      Deque<LoadQueueItem> queue, ExecutorService pool,\n      SecureBulkLoadClient secureClient, boolean copyFile) throws IOException {\n    int count = 0;\n\n    if(isSecureBulkLoadEndpointAvailable()) {\n      LOG.warn(\"SecureBulkLoadEndpoint is deprecated. It will be removed in future releases.\");\n      LOG.warn(\"Secure bulk load has been integrated into HBase core.\");\n    }\n\n    //If using secure bulk load, get source delegation token, and\n    //prepare staging directory and token\n    // fs is the source filesystem\n    fsDelegationToken.acquireDelegationToken(fs);\n    bulkToken = secureClient.prepareBulkLoad(admin.getConnection());\n    Pair<Multimap<ByteBuffer, LoadQueueItem>, List<String>> pair = null;\n\n    // Assumes that region splits can happen while this occurs.\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      final Pair<byte[][], byte[][]> startEndKeys = regionLocator.getStartEndKeys();\n      if (count != 0) {\n        LOG.info(\"Split occured while grouping HFiles, retry attempt \" +\n            + count + \" with \" + queue.size() + \" files remaining to group or split\");\n      }\n\n      int maxRetries = getConf().getInt(HConstants.BULKLOAD_MAX_RETRIES_NUMBER, 10);\n      maxRetries = Math.max(maxRetries, startEndKeys.getFirst().length + 1);\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count +\n            \" times without completing, bailing out\");\n      }\n      count++;\n\n      // Using ByteBuffer for byte[] equality semantics\n      pair = groupOrSplitPhase(table, pool, queue, startEndKeys);\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups = pair.getFirst();\n\n      if (!checkHFilesCountPerRegionPerFamily(regionGroups)) {\n        // Error is logged inside checkHFilesCountPerRegionPerFamily.\n        throw new IOException(\"Trying to load more than \" + maxFilesPerRegionPerFamily\n            + \" hfiles to one family of one region\");\n      }\n\n      bulkLoadPhase(table, admin.getConnection(), pool, queue, regionGroups, copyFile);\n\n      // NOTE: The next iteration's split / group could happen in parallel to\n      // atomic bulkloads assuming that there are splits and no merges, and\n      // that we can atomically pull out the groups we want to retry.\n    }\n\n    if (!queue.isEmpty()) {\n      throw new RuntimeException(\"Bulk load aborted with some files not yet loaded.\"\n        + \"Please check log for more details.\");\n    }\n    if (pair == null) return null;\n    return pair.getSecond();\n  }"
        ],
        [
            "TestLoadIncrementalHFiles::runTest(String,HTableDescriptor,BloomType,boolean,byte,byte,boolean,boolean)",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349 -\n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "  private void runTest(String testName, HTableDescriptor htd, BloomType bloomType,\n      boolean preCreateTable, byte[][] tableSplitKeys, byte[][][] hfileRanges, boolean useMap,\n      boolean copyFiles) throws Exception {\n    Path dir = util.getDataTestDirOnTestFS(testName);\n    FileSystem fs = util.getTestFileSystem();\n    dir = dir.makeQualified(fs);\n    Path familyDir = new Path(dir, Bytes.toString(FAMILY));\n\n    int hfileIdx = 0;\n    Map<byte[], List<Path>> map = null;\n    List<Path> list = null;\n    if (useMap || copyFiles) {\n      list = new ArrayList<>();\n    }\n    if (useMap) {\n      map = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n      map.put(FAMILY, list);\n    }\n    for (byte[][] range : hfileRanges) {\n      byte[] from = range[0];\n      byte[] to = range[1];\n      Path path = new Path(familyDir, \"hfile_\" + hfileIdx++);\n      HFileTestUtil.createHFile(util.getConfiguration(), fs, path, FAMILY, QUALIFIER, from, to, 1000);\n      if (useMap) {\n        list.add(path);\n      }\n    }\n    int expectedRows = hfileIdx * 1000;\n\n    if (preCreateTable || map != null) {\n      util.getHBaseAdmin().createTable(htd, tableSplitKeys);\n    }\n\n    final TableName tableName = htd.getTableName();\n    Configuration conf = util.getConfiguration();\n    if (copyFiles) {\n      conf.setBoolean(LoadIncrementalHFiles.ALWAYS_COPY_FILES, true);\n    }\n    LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);\n    String [] args= {dir.toString(), tableName.toString()};\n    if (useMap) {\n      loader.run(null, map, tableName);\n    } else {\n      loader.run(args);\n    }\n\n    if (copyFiles) {\n      for (Path p : list) {\n        assertTrue(fs.exists(p));\n      }\n    }\n\n    Table table = util.getConnection().getTable(tableName);\n    try {\n      assertEquals(expectedRows, util.countRows(table));\n    } finally {\n      table.close();\n    }\n\n    // verify staging folder has been cleaned up\n    Path stagingBasePath = new Path(FSUtils.getRootDir(util.getConfiguration()), HConstants.BULKLOAD_STAGING_DIR_NAME);\n    if(fs.exists(stagingBasePath)) {\n      FileStatus[] files = fs.listStatus(stagingBasePath);\n      for(FileStatus file : files) {\n        assertTrue(\"Folder=\" + file.getPath() + \" is not cleaned up.\",\n          file.getPath().getName() != \"DONOTERASE\");\n      }\n    }\n\n    util.deleteTable(tableName);\n  }",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326 +\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 +\n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351 +\n 352 +\n 353 +\n 354 +\n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  ",
            "  private void runTest(String testName, HTableDescriptor htd, BloomType bloomType,\n      boolean preCreateTable, byte[][] tableSplitKeys, byte[][][] hfileRanges, boolean useMap,\n      boolean copyFiles) throws Exception {\n    Path dir = util.getDataTestDirOnTestFS(testName);\n    FileSystem fs = util.getTestFileSystem();\n    dir = dir.makeQualified(fs);\n    Path familyDir = new Path(dir, Bytes.toString(FAMILY));\n\n    int hfileIdx = 0;\n    Map<byte[], List<Path>> map = null;\n    List<Path> list = null;\n    if (useMap || copyFiles) {\n      list = new ArrayList<>();\n    }\n    if (useMap) {\n      map = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n      map.put(FAMILY, list);\n    }\n    Path last = null;\n    for (byte[][] range : hfileRanges) {\n      byte[] from = range[0];\n      byte[] to = range[1];\n      Path path = new Path(familyDir, \"hfile_\" + hfileIdx++);\n      HFileTestUtil.createHFile(util.getConfiguration(), fs, path, FAMILY, QUALIFIER, from, to, 1000);\n      if (useMap) {\n        last = path;\n        list.add(path);\n      }\n    }\n    int expectedRows = hfileIdx * 1000;\n\n    if (preCreateTable || map != null) {\n      util.getHBaseAdmin().createTable(htd, tableSplitKeys);\n    }\n\n    final TableName tableName = htd.getTableName();\n    Configuration conf = util.getConfiguration();\n    if (copyFiles) {\n      conf.setBoolean(LoadIncrementalHFiles.ALWAYS_COPY_FILES, true);\n    }\n    LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);\n    String [] args= {dir.toString(), tableName.toString()};\n    if (useMap) {\n      fs.delete(last);\n      List<String> missingHFiles = loader.run(null, map, tableName);\n      expectedRows -= 1000;\n      assertTrue(missingHFiles.contains(last.getName()));\n    } else {\n      loader.run(args);\n    }\n\n    if (copyFiles) {\n      for (Path p : list) {\n        assertTrue(fs.exists(p));\n      }\n    }\n\n    Table table = util.getConnection().getTable(tableName);\n    try {\n      assertEquals(expectedRows, util.countRows(table));\n    } finally {\n      table.close();\n    }\n\n    // verify staging folder has been cleaned up\n    Path stagingBasePath = new Path(FSUtils.getRootDir(util.getConfiguration()), HConstants.BULKLOAD_STAGING_DIR_NAME);\n    if(fs.exists(stagingBasePath)) {\n      FileStatus[] files = fs.listStatus(stagingBasePath);\n      for(FileStatus file : files) {\n        assertTrue(\"Folder=\" + file.getPath() + \" is not cleaned up.\",\n          file.getPath().getName() != \"DONOTERASE\");\n      }\n    }\n\n    util.deleteTable(tableName);\n  }"
        ],
        [
            "TestLoadIncrementalHFilesSplitRecovery::testGroupOrSplitPresplit()",
            " 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 -\n 408  \n 409  \n 410  \n 411 -\n 412 -\n 413 -\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  ",
            "  /**\n   * This test splits a table and attempts to bulk load.  The bulk import files\n   * should be split before atomically importing.\n   */\n  @Test (timeout=120000)\n  public void testGroupOrSplitPresplit() throws Exception {\n    final TableName table = TableName.valueOf(\"groupOrSplitPresplit\");\n    try (Connection connection = ConnectionFactory.createConnection(util.getConfiguration())) {\n      setupTable(connection, table, 10);\n      populateTable(connection, table, 1);\n      assertExpectedTable(connection, table, ROWCOUNT, 1);\n      forceSplit(table);\n\n      final AtomicInteger countedLqis= new AtomicInteger();\n      LoadIncrementalHFiles lih = new LoadIncrementalHFiles(\n          util.getConfiguration()) {\n        @Override\n        protected List<LoadQueueItem> groupOrSplit(\n            Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n            final LoadQueueItem item, final Table htable,\n            final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n          List<LoadQueueItem> lqis = super.groupOrSplit(regionGroups, item, htable, startEndKeys);\n          if (lqis != null) {\n            countedLqis.addAndGet(lqis.size());\n          }\n          return lqis;\n        }\n      };\n\n      // create HFiles for different column families\n      Path bulk = buildBulkFiles(table, 2);\n      try (Table t = connection.getTable(table);\n          RegionLocator locator = connection.getRegionLocator(table);\n          Admin admin = connection.getAdmin()) {\n        lih.doBulkLoad(bulk, admin, t, locator);\n      }\n      assertExpectedTable(connection, table, ROWCOUNT, 2);\n      assertEquals(20, countedLqis.get());\n    }\n  }",
            " 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 +\n 408  \n 409  \n 410  \n 411 +\n 412 +\n 413 +\n 414 +\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  ",
            "  /**\n   * This test splits a table and attempts to bulk load.  The bulk import files\n   * should be split before atomically importing.\n   */\n  @Test (timeout=120000)\n  public void testGroupOrSplitPresplit() throws Exception {\n    final TableName table = TableName.valueOf(\"groupOrSplitPresplit\");\n    try (Connection connection = ConnectionFactory.createConnection(util.getConfiguration())) {\n      setupTable(connection, table, 10);\n      populateTable(connection, table, 1);\n      assertExpectedTable(connection, table, ROWCOUNT, 1);\n      forceSplit(table);\n\n      final AtomicInteger countedLqis= new AtomicInteger();\n      LoadIncrementalHFiles lih = new LoadIncrementalHFiles(\n          util.getConfiguration()) {\n        @Override\n        protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n            Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n            final LoadQueueItem item, final Table htable,\n            final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n          Pair<List<LoadQueueItem>, String> lqis = super.groupOrSplit(regionGroups, item, htable,\n              startEndKeys);\n          if (lqis != null && lqis.getFirst() != null) {\n            countedLqis.addAndGet(lqis.getFirst().size());\n          }\n          return lqis;\n        }\n      };\n\n      // create HFiles for different column families\n      Path bulk = buildBulkFiles(table, 2);\n      try (Table t = connection.getTable(table);\n          RegionLocator locator = connection.getRegionLocator(table);\n          Admin admin = connection.getAdmin()) {\n        lih.doBulkLoad(bulk, admin, t, locator);\n      }\n      assertExpectedTable(connection, table, ROWCOUNT, 2);\n      assertEquals(20, countedLqis.get());\n    }\n  }"
        ],
        [
            "LoadIncrementalHFiles::loadHFileQueue(Table,Connection,Deque,Pair,boolean)",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628 -\n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  ",
            "  /**\n   * Used by the replication sink to load the hfiles from the source cluster. It does the following,\n   * <ol>\n   * <li>LoadIncrementalHFiles#groupOrSplitPhase(Table, ExecutorService, Deque, Pair)}</li>\n   * <li>LoadIncrementalHFiles#bulkLoadPhase(Table, Connection, ExecutorService, Deque, Multimap)\n   * </li>\n   * </ol>\n   * @param table Table to which these hfiles should be loaded to\n   * @param conn Connection to use\n   * @param queue {@link LoadQueueItem} has hfiles yet to be loaded\n   * @param startEndKeys starting and ending row keys of the region\n   */\n  public void loadHFileQueue(final Table table, final Connection conn, Deque<LoadQueueItem> queue,\n      Pair<byte[][], byte[][]> startEndKeys, boolean copyFile) throws IOException {\n    ExecutorService pool = null;\n    try {\n      pool = createExecutorService();\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups =\n          groupOrSplitPhase(table, pool, queue, startEndKeys);\n      bulkLoadPhase(table, conn, pool, queue, regionGroups, copyFile);\n    } finally {\n      if (pool != null) {\n        pool.shutdown();\n      }\n    }\n  }",
            " 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632 +\n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  ",
            "  /**\n   * Used by the replication sink to load the hfiles from the source cluster. It does the following,\n   * <ol>\n   * <li>LoadIncrementalHFiles#groupOrSplitPhase(Table, ExecutorService, Deque, Pair)}</li>\n   * <li>LoadIncrementalHFiles#bulkLoadPhase(Table, Connection, ExecutorService, Deque, Multimap)\n   * </li>\n   * </ol>\n   * @param table Table to which these hfiles should be loaded to\n   * @param conn Connection to use\n   * @param queue {@link LoadQueueItem} has hfiles yet to be loaded\n   * @param startEndKeys starting and ending row keys of the region\n   */\n  public void loadHFileQueue(final Table table, final Connection conn, Deque<LoadQueueItem> queue,\n      Pair<byte[][], byte[][]> startEndKeys, boolean copyFile) throws IOException {\n    ExecutorService pool = null;\n    try {\n      pool = createExecutorService();\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups =\n          groupOrSplitPhase(table, pool, queue, startEndKeys).getFirst();\n      bulkLoadPhase(table, conn, pool, queue, regionGroups, copyFile);\n    } finally {\n      if (pool != null) {\n        pool.shutdown();\n      }\n    }\n  }"
        ],
        [
            "LoadIncrementalHFiles::run(String,Map,TableName)",
            "1174 -\n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201 -\n1202  \n1203  \n1204  \n1205 -\n1206 -\n1207  ",
            "  public int run(String dirPath, Map<byte[], List<Path>> map, TableName tableName) throws Exception{\n    initialize();\n    try (Connection connection = ConnectionFactory.createConnection(getConf());\n        Admin admin = connection.getAdmin()) {\n\n      boolean tableExists = admin.tableExists(tableName);\n      if (!tableExists) {\n        if (dirPath != null && \"yes\".equalsIgnoreCase(getConf().get(CREATE_TABLE_CONF_KEY, \"yes\"))) {\n          this.createTable(tableName, dirPath, admin);\n        } else {\n          String errorMsg = format(\"Table '%s' does not exist.\", tableName);\n          LOG.error(errorMsg);\n          throw new TableNotFoundException(errorMsg);\n        }\n      }\n      Path hfofDir = null;\n      if (dirPath != null) {\n        hfofDir = new Path(dirPath);\n      }\n\n      try (Table table = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName)) {\n        boolean silence = \"yes\".equalsIgnoreCase(getConf().get(SILENCE_CONF_KEY, \"\"));\n        boolean copyFiles = \"yes\".equalsIgnoreCase(getConf().get(ALWAYS_COPY_FILES, \"\"));\n        if (dirPath != null) {\n          doBulkLoad(hfofDir, admin, table, locator, silence, copyFiles);\n        } else {\n          doBulkLoad(map, admin, table, locator, silence, copyFiles);\n        }\n      }\n    }\n\n    return 0;\n  }",
            "1196 +\n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222 +\n1223  \n1224 +\n1225  \n1226  \n1227  \n1228  ",
            "  public List<String> run(String dirPath, Map<byte[], List<Path>> map, TableName tableName) throws Exception{\n    initialize();\n    try (Connection connection = ConnectionFactory.createConnection(getConf());\n        Admin admin = connection.getAdmin()) {\n\n      boolean tableExists = admin.tableExists(tableName);\n      if (!tableExists) {\n        if (dirPath != null && \"yes\".equalsIgnoreCase(getConf().get(CREATE_TABLE_CONF_KEY, \"yes\"))) {\n          this.createTable(tableName, dirPath, admin);\n        } else {\n          String errorMsg = format(\"Table '%s' does not exist.\", tableName);\n          LOG.error(errorMsg);\n          throw new TableNotFoundException(errorMsg);\n        }\n      }\n      Path hfofDir = null;\n      if (dirPath != null) {\n        hfofDir = new Path(dirPath);\n      }\n\n      try (Table table = connection.getTable(tableName);\n        RegionLocator locator = connection.getRegionLocator(tableName)) {\n        boolean silence = \"yes\".equalsIgnoreCase(getConf().get(SILENCE_CONF_KEY, \"\"));\n        boolean copyFiles = \"yes\".equalsIgnoreCase(getConf().get(ALWAYS_COPY_FILES, \"\"));\n        if (dirPath != null) {\n          doBulkLoad(hfofDir, admin, table, locator, silence, copyFiles);\n          return null;\n        } else {\n          return doBulkLoad(map, admin, table, locator, silence, copyFiles);\n        }\n      }\n    }\n  }"
        ],
        [
            "LoadIncrementalHFiles::groupOrSplitPhase(Table,ExecutorService,Deque,Pair)",
            " 711  \n 712  \n 713  \n 714 -\n 715 -\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 -\n 724  \n 725  \n 726  \n 727 -\n 728  \n 729 -\n 730 -\n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738 -\n 739  \n 740 -\n 741  \n 742 -\n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757 -\n 758  ",
            "  /**\n   * @return A map that groups LQI by likely bulk load region targets.\n   */\n  private Multimap<ByteBuffer, LoadQueueItem> groupOrSplitPhase(final Table table,\n      ExecutorService pool, Deque<LoadQueueItem> queue,\n      final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n    // <region start key, LQI> need synchronized only within this scope of this\n    // phase because of the puts that happen in futures.\n    Multimap<ByteBuffer, LoadQueueItem> rgs = HashMultimap.create();\n    final Multimap<ByteBuffer, LoadQueueItem> regionGroups = Multimaps.synchronizedMultimap(rgs);\n\n    // drain LQIs and figure out bulk load groups\n    Set<Future<List<LoadQueueItem>>> splittingFutures = new HashSet<>();\n    while (!queue.isEmpty()) {\n      final LoadQueueItem item = queue.remove();\n\n      final Callable<List<LoadQueueItem>> call = new Callable<List<LoadQueueItem>>() {\n        @Override\n        public List<LoadQueueItem> call() throws Exception {\n          List<LoadQueueItem> splits = groupOrSplit(regionGroups, item, table, startEndKeys);\n          return splits;\n        }\n      };\n      splittingFutures.add(pool.submit(call));\n    }\n    // get all the results.  All grouping and splitting must finish before\n    // we can attempt the atomic loads.\n    for (Future<List<LoadQueueItem>> lqis : splittingFutures) {\n      try {\n        List<LoadQueueItem> splits = lqis.get();\n        if (splits != null) {\n          queue.addAll(splits);\n        }\n      } catch (ExecutionException e1) {\n        Throwable t = e1.getCause();\n        if (t instanceof IOException) {\n          LOG.error(\"IOException during splitting\", e1);\n          throw (IOException)t; // would have been thrown if not parallelized,\n        }\n        LOG.error(\"Unexpected execution exception during splitting\", e1);\n        throw new IllegalStateException(t);\n      } catch (InterruptedException e1) {\n        LOG.error(\"Unexpected interrupted exception during splitting\", e1);\n        throw (InterruptedIOException)new InterruptedIOException().initCause(e1);\n      }\n    }\n    return regionGroups;\n  }",
            " 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 +\n 723 +\n 724  \n 725  \n 726  \n 727  \n 728  \n 729 +\n 730 +\n 731 +\n 732  \n 733  \n 734 +\n 735  \n 736  \n 737  \n 738 +\n 739 +\n 740  \n 741 +\n 742 +\n 743 +\n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751 +\n 752  \n 753 +\n 754  \n 755 +\n 756 +\n 757 +\n 758 +\n 759 +\n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774 +\n 775  ",
            "  /**\n   * @param table the table to load into\n   * @param pool the ExecutorService\n   * @param queue the queue for LoadQueueItem\n   * @param startEndKeys start and end keys\n   * @return A map that groups LQI by likely bulk load region targets and List of missing hfiles.\n   */\n  private Pair<Multimap<ByteBuffer, LoadQueueItem>, List<String>> groupOrSplitPhase(\n      final Table table, ExecutorService pool, Deque<LoadQueueItem> queue,\n      final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n    // <region start key, LQI> need synchronized only within this scope of this\n    // phase because of the puts that happen in futures.\n    Multimap<ByteBuffer, LoadQueueItem> rgs = HashMultimap.create();\n    final Multimap<ByteBuffer, LoadQueueItem> regionGroups = Multimaps.synchronizedMultimap(rgs);\n    List<String> missingHFiles = new ArrayList<>();\n    Pair<Multimap<ByteBuffer, LoadQueueItem>, List<String>> pair = new Pair<>(regionGroups,\n        missingHFiles);\n\n    // drain LQIs and figure out bulk load groups\n    Set<Future<Pair<List<LoadQueueItem>, String>>> splittingFutures = new HashSet<>();\n    while (!queue.isEmpty()) {\n      final LoadQueueItem item = queue.remove();\n\n      final Callable<Pair<List<LoadQueueItem>, String>> call =\n          new Callable<Pair<List<LoadQueueItem>, String>>() {\n        @Override\n        public Pair<List<LoadQueueItem>, String> call() throws Exception {\n          Pair<List<LoadQueueItem>, String> splits = groupOrSplit(regionGroups, item, table,\n              startEndKeys);\n          return splits;\n        }\n      };\n      splittingFutures.add(pool.submit(call));\n    }\n    // get all the results.  All grouping and splitting must finish before\n    // we can attempt the atomic loads.\n    for (Future<Pair<List<LoadQueueItem>, String>> lqis : splittingFutures) {\n      try {\n        Pair<List<LoadQueueItem>, String> splits = lqis.get();\n        if (splits != null) {\n          if (splits.getFirst() != null) {\n            queue.addAll(splits.getFirst());\n          } else {\n            missingHFiles.add(splits.getSecond());\n          }\n        }\n      } catch (ExecutionException e1) {\n        Throwable t = e1.getCause();\n        if (t instanceof IOException) {\n          LOG.error(\"IOException during splitting\", e1);\n          throw (IOException)t; // would have been thrown if not parallelized,\n        }\n        LOG.error(\"Unexpected execution exception during splitting\", e1);\n        throw new IllegalStateException(t);\n      } catch (InterruptedException e1) {\n        LOG.error(\"Unexpected interrupted exception during splitting\", e1);\n        throw (InterruptedIOException)new InterruptedIOException().initCause(e1);\n      }\n    }\n    return pair;\n  }"
        ],
        [
            "TestLoadIncrementalHFilesSplitRecovery::testGroupOrSplitFailure()",
            " 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 -\n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  ",
            "  /**\n   * This simulates an remote exception which should cause LIHF to exit with an\n   * exception.\n   */\n  @Test(expected = IOException.class, timeout=120000)\n  public void testGroupOrSplitFailure() throws Exception {\n    TableName table = TableName.valueOf(\"groupOrSplitFailure\");\n    try (Connection connection = ConnectionFactory.createConnection(util.getConfiguration())) {\n      setupTable(connection, table, 10);\n\n      LoadIncrementalHFiles lih = new LoadIncrementalHFiles(\n          util.getConfiguration()) {\n        int i = 0;\n\n        @Override\n        protected List<LoadQueueItem> groupOrSplit(\n            Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n            final LoadQueueItem item, final Table table,\n            final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n          i++;\n\n          if (i == 5) {\n            throw new IOException(\"failure\");\n          }\n          return super.groupOrSplit(regionGroups, item, table, startEndKeys);\n        }\n      };\n\n      // create HFiles for different column families\n      Path dir = buildBulkFiles(table,1);\n      try (Table t = connection.getTable(table);\n          RegionLocator locator = connection.getRegionLocator(table);\n          Admin admin = connection.getAdmin()) {\n        lih.doBulkLoad(dir, admin, t, locator);\n      }\n    }\n\n    fail(\"doBulkLoad should have thrown an exception\");\n  }",
            " 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483 +\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  ",
            "  /**\n   * This simulates an remote exception which should cause LIHF to exit with an\n   * exception.\n   */\n  @Test(expected = IOException.class, timeout=120000)\n  public void testGroupOrSplitFailure() throws Exception {\n    TableName table = TableName.valueOf(\"groupOrSplitFailure\");\n    try (Connection connection = ConnectionFactory.createConnection(util.getConfiguration())) {\n      setupTable(connection, table, 10);\n\n      LoadIncrementalHFiles lih = new LoadIncrementalHFiles(\n          util.getConfiguration()) {\n        int i = 0;\n\n        @Override\n        protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n            Multimap<ByteBuffer, LoadQueueItem> regionGroups,\n            final LoadQueueItem item, final Table table,\n            final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n          i++;\n\n          if (i == 5) {\n            throw new IOException(\"failure\");\n          }\n          return super.groupOrSplit(regionGroups, item, table, startEndKeys);\n        }\n      };\n\n      // create HFiles for different column families\n      Path dir = buildBulkFiles(table,1);\n      try (Table t = connection.getTable(table);\n          RegionLocator locator = connection.getRegionLocator(table);\n          Admin admin = connection.getAdmin()) {\n        lih.doBulkLoad(dir, admin, t, locator);\n      }\n    }\n\n    fail(\"doBulkLoad should have thrown an exception\");\n  }"
        ]
    ],
    "674511875d513ca3c031e63987288c45dacf56d9": [
        [
            "ReplicationSourceManager::removePeer(String)",
            " 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  ",
            "  /**\n   * Thie method first deletes all the recovered sources for the specified\n   * id, then deletes the normal source (deleting all related data in ZK).\n   * @param id The id of the peer cluster\n   */\n  public void removePeer(String id) {\n    LOG.info(\"Closing the following queue \" + id + \", currently have \"\n        + sources.size() + \" and another \"\n        + oldsources.size() + \" that were recovered\");\n    String terminateMessage = \"Replication stream was removed by a user\";\n    List<ReplicationSourceInterface> oldSourcesToDelete =\n        new ArrayList<ReplicationSourceInterface>();\n    // synchronized on oldsources to avoid adding recovered source for the to-be-removed peer\n    // see NodeFailoverWorker.run\n    synchronized (oldsources) {\n      // First close all the recovered sources for this peer\n      for (ReplicationSourceInterface src : oldsources) {\n        if (id.equals(src.getPeerClusterId())) {\n          oldSourcesToDelete.add(src);\n        }\n      }\n      for (ReplicationSourceInterface src : oldSourcesToDelete) {\n        src.terminate(terminateMessage);\n        closeRecoveredQueue(src);\n      }\n    }\n    LOG.info(\"Number of deleted recovered sources for \" + id + \": \"\n        + oldSourcesToDelete.size());\n    // Now look for the one on this cluster\n    List<ReplicationSourceInterface> srcToRemove = new ArrayList<ReplicationSourceInterface>();\n    // synchronize on replicationPeers to avoid adding source for the to-be-removed peer\n    synchronized (this.replicationPeers) {\n      for (ReplicationSourceInterface src : this.sources) {\n        if (id.equals(src.getPeerClusterId())) {\n          srcToRemove.add(src);\n        }\n      }\n      if (srcToRemove.isEmpty()) {\n        LOG.error(\"The peer we wanted to remove is missing a ReplicationSourceInterface. \" +\n            \"This could mean that ReplicationSourceInterface initialization failed for this peer \" +\n            \"and that replication on this peer may not be caught up. peerId=\" + id);\n      }\n      for (ReplicationSourceInterface toRemove : srcToRemove) {\n        toRemove.terminate(terminateMessage);\n        this.sources.remove(toRemove);\n      }\n      deleteSource(id, true);\n    }\n  }",
            " 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585 +\n 586 +\n 587 +\n 588  \n 589  \n 590  \n 591  \n 592  ",
            "  /**\n   * Thie method first deletes all the recovered sources for the specified\n   * id, then deletes the normal source (deleting all related data in ZK).\n   * @param id The id of the peer cluster\n   */\n  public void removePeer(String id) {\n    LOG.info(\"Closing the following queue \" + id + \", currently have \"\n        + sources.size() + \" and another \"\n        + oldsources.size() + \" that were recovered\");\n    String terminateMessage = \"Replication stream was removed by a user\";\n    List<ReplicationSourceInterface> oldSourcesToDelete =\n        new ArrayList<ReplicationSourceInterface>();\n    // synchronized on oldsources to avoid adding recovered source for the to-be-removed peer\n    // see NodeFailoverWorker.run\n    synchronized (oldsources) {\n      // First close all the recovered sources for this peer\n      for (ReplicationSourceInterface src : oldsources) {\n        if (id.equals(src.getPeerClusterId())) {\n          oldSourcesToDelete.add(src);\n        }\n      }\n      for (ReplicationSourceInterface src : oldSourcesToDelete) {\n        src.terminate(terminateMessage);\n        closeRecoveredQueue(src);\n      }\n    }\n    LOG.info(\"Number of deleted recovered sources for \" + id + \": \"\n        + oldSourcesToDelete.size());\n    // Now look for the one on this cluster\n    List<ReplicationSourceInterface> srcToRemove = new ArrayList<ReplicationSourceInterface>();\n    // synchronize on replicationPeers to avoid adding source for the to-be-removed peer\n    synchronized (this.replicationPeers) {\n      for (ReplicationSourceInterface src : this.sources) {\n        if (id.equals(src.getPeerClusterId())) {\n          srcToRemove.add(src);\n        }\n      }\n      if (srcToRemove.isEmpty()) {\n        LOG.error(\"The peer we wanted to remove is missing a ReplicationSourceInterface. \" +\n            \"This could mean that ReplicationSourceInterface initialization failed for this peer \" +\n            \"and that replication on this peer may not be caught up. peerId=\" + id);\n      }\n      for (ReplicationSourceInterface toRemove : srcToRemove) {\n        toRemove.terminate(terminateMessage);\n        if (toRemove instanceof ReplicationSource) {\n          ((ReplicationSource) toRemove).getSourceMetrics().clear();\n        }\n        this.sources.remove(toRemove);\n      }\n      deleteSource(id, true);\n    }\n  }"
        ],
        [
            "MetricsSource::setAgeOfLastShippedOp(long,String)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86  \n  87  ",
            "  /**\n   * Set the age of the last edit that was shipped\n   * @param timestamp write time of the edit\n   * @param walGroup which group we are setting\n   */\n  public void setAgeOfLastShippedOp(long timestamp, String walGroup) {\n    long age = EnvironmentEdgeManager.currentTime() - timestamp;\n    singleSourceSource.setLastShippedAge(age);\n    globalSourceSource.setLastShippedAge(age);\n    this.lastTimeStamps.put(walGroup, timestamp);\n  }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  ",
            "  /**\n   * Set the age of the last edit that was shipped\n   * @param timestamp write time of the edit\n   * @param walGroup which group we are setting\n   */\n  public void setAgeOfLastShippedOp(long timestamp, String walGroup) {\n    long age = EnvironmentEdgeManager.currentTime() - timestamp;\n    singleSourceSource.setLastShippedAge(age);\n    globalSourceSource.setLastShippedAge(Math.max(age, globalSourceSource.getLastShippedAge()));\n    this.lastTimeStamps.put(walGroup, timestamp);\n  }"
        ],
        [
            "ReplicationSourceManager::closeRecoveredQueue(ReplicationSourceInterface)",
            " 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  ",
            "  /**\n   * Clear the references to the specified old source\n   * @param src source to clear\n   */\n  public void closeRecoveredQueue(ReplicationSourceInterface src) {\n    LOG.info(\"Done with the recovered queue \" + src.getPeerClusterZnode());\n    this.oldsources.remove(src);\n    deleteSource(src.getPeerClusterZnode(), false);\n    this.walsByIdRecoveredQueues.remove(src.getPeerClusterZnode());\n  }",
            " 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533 +\n 534 +\n 535 +\n 536  \n 537  \n 538  \n 539  ",
            "  /**\n   * Clear the references to the specified old source\n   * @param src source to clear\n   */\n  public void closeRecoveredQueue(ReplicationSourceInterface src) {\n    LOG.info(\"Done with the recovered queue \" + src.getPeerClusterZnode());\n    if (src instanceof ReplicationSource) {\n      ((ReplicationSource) src).getSourceMetrics().clear();\n    }\n    this.oldsources.remove(src);\n    deleteSource(src.getPeerClusterZnode(), false);\n    this.walsByIdRecoveredQueues.remove(src.getPeerClusterZnode());\n  }"
        ],
        [
            "Replication::buildReplicationLoad()",
            " 378  \n 379 -\n 380 -\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "  private void buildReplicationLoad() {\n    // get source\n    List<ReplicationSourceInterface> sources = this.replicationManager.getSources();\n    List<MetricsSource> sourceMetricsList = new ArrayList<MetricsSource>();\n\n    for (ReplicationSourceInterface source : sources) {\n      if (source instanceof ReplicationSource) {\n        sourceMetricsList.add(((ReplicationSource) source).getSourceMetrics());\n      }\n    }\n    // get sink\n    MetricsSink sinkMetrics = this.replicationSink.getSinkMetrics();\n    this.replicationLoad.buildReplicationLoad(sourceMetricsList, sinkMetrics);\n  }",
            " 378  \n 379  \n 380  \n 381 +\n 382 +\n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389 +\n 390 +\n 391 +\n 392 +\n 393 +\n 394 +\n 395 +\n 396 +\n 397  \n 398  \n 399  \n 400  ",
            "  private void buildReplicationLoad() {\n    List<MetricsSource> sourceMetricsList = new ArrayList<MetricsSource>();\n\n    // get source\n    List<ReplicationSourceInterface> sources = this.replicationManager.getSources();\n    for (ReplicationSourceInterface source : sources) {\n      if (source instanceof ReplicationSource) {\n        sourceMetricsList.add(((ReplicationSource) source).getSourceMetrics());\n      }\n    }\n\n    // get old source\n    List<ReplicationSourceInterface> oldSources = this.replicationManager.getOldSources();\n    for (ReplicationSourceInterface source : oldSources) {\n      if (source instanceof ReplicationSource) {\n        sourceMetricsList.add(((ReplicationSource) source).getSourceMetrics());\n      }\n    }\n\n    // get sink\n    MetricsSink sinkMetrics = this.replicationSink.getSinkMetrics();\n    this.replicationLoad.buildReplicationLoad(sourceMetricsList, sinkMetrics);\n  }"
        ],
        [
            "ReplicationLoad::buildReplicationLoad(List,MetricsSink)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98 -\n  99  ",
            "   * buildReplicationLoad\n   * @param srMetricsList\n   * @param skMetrics\n   */\n\n  public void buildReplicationLoad(final List<MetricsSource> srMetricsList,\n      final MetricsSink skMetrics) {\n    this.sourceMetricsList = srMetricsList;\n    this.sinkMetrics = skMetrics;\n\n    // build the SinkLoad\n    ClusterStatusProtos.ReplicationLoadSink.Builder rLoadSinkBuild =\n        ClusterStatusProtos.ReplicationLoadSink.newBuilder();\n    rLoadSinkBuild.setAgeOfLastAppliedOp(sinkMetrics.getAgeOfLastAppliedOp());\n    rLoadSinkBuild.setTimeStampsOfLastAppliedOp(sinkMetrics.getTimeStampOfLastAppliedOp());\n    this.replicationLoadSink = rLoadSinkBuild.build();\n\n    // build the SourceLoad List\n    this.replicationLoadSourceList = new ArrayList<ClusterStatusProtos.ReplicationLoadSource>();\n    for (MetricsSource sm : this.sourceMetricsList) {\n      long ageOfLastShippedOp = sm.getAgeOfLastShippedOp();\n      int sizeOfLogQueue = sm.getSizeOfLogQueue();\n      long timeStampOfLastShippedOp = sm.getTimeStampOfLastShippedOp();\n      long replicationLag;\n      long timePassedAfterLastShippedOp =\n          EnvironmentEdgeManager.currentTime() - timeStampOfLastShippedOp;\n      if (sizeOfLogQueue != 0) {\n        // err on the large side\n        replicationLag = Math.max(ageOfLastShippedOp, timePassedAfterLastShippedOp);\n      } else if (timePassedAfterLastShippedOp < 2 * ageOfLastShippedOp) {\n        replicationLag = ageOfLastShippedOp; // last shipped happen recently\n      } else {\n        // last shipped may happen last night,\n        // so NO real lag although ageOfLastShippedOp is non-zero\n        replicationLag = 0;\n      }\n\n      ClusterStatusProtos.ReplicationLoadSource.Builder rLoadSourceBuild =\n          ClusterStatusProtos.ReplicationLoadSource.newBuilder();\n      rLoadSourceBuild.setPeerID(sm.getPeerID());\n      rLoadSourceBuild.setAgeOfLastShippedOp(ageOfLastShippedOp);\n      rLoadSourceBuild.setSizeOfLogQueue(sizeOfLogQueue);\n      rLoadSourceBuild.setTimeStampOfLastShippedOp(timeStampOfLastShippedOp);\n      rLoadSourceBuild.setReplicationLag(replicationLag);\n\n      this.replicationLoadSourceList.add(rLoadSourceBuild.build());\n    }\n\n  }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72 +\n  73  \n  74 +\n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99 +\n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110 +\n 111 +\n 112  ",
            "   * buildReplicationLoad\n   * @param srMetricsList\n   * @param skMetrics\n   */\n\n  public void buildReplicationLoad(final List<MetricsSource> srMetricsList,\n      final MetricsSink skMetrics) {\n    this.sourceMetricsList = srMetricsList;\n    this.sinkMetrics = skMetrics;\n\n    // build the SinkLoad\n    ClusterStatusProtos.ReplicationLoadSink.Builder rLoadSinkBuild =\n        ClusterStatusProtos.ReplicationLoadSink.newBuilder();\n    rLoadSinkBuild.setAgeOfLastAppliedOp(sinkMetrics.getAgeOfLastAppliedOp());\n    rLoadSinkBuild.setTimeStampsOfLastAppliedOp(sinkMetrics.getTimeStampOfLastAppliedOp());\n    this.replicationLoadSink = rLoadSinkBuild.build();\n\n    // build the SourceLoad List\n    Map<String, ClusterStatusProtos.ReplicationLoadSource> replicationLoadSourceMap =\n        new HashMap<String, ClusterStatusProtos.ReplicationLoadSource>();\n    for (MetricsSource sm : this.sourceMetricsList) {\n      String peerId = sm.getPeerID();\n      long ageOfLastShippedOp = sm.getAgeOfLastShippedOp();\n      int sizeOfLogQueue = sm.getSizeOfLogQueue();\n      long timeStampOfLastShippedOp = sm.getTimeStampOfLastShippedOp();\n      long replicationLag;\n      long timePassedAfterLastShippedOp =\n          EnvironmentEdgeManager.currentTime() - timeStampOfLastShippedOp;\n      if (sizeOfLogQueue != 0) {\n        // err on the large side\n        replicationLag = Math.max(ageOfLastShippedOp, timePassedAfterLastShippedOp);\n      } else if (timePassedAfterLastShippedOp < 2 * ageOfLastShippedOp) {\n        replicationLag = ageOfLastShippedOp; // last shipped happen recently\n      } else {\n        // last shipped may happen last night,\n        // so NO real lag although ageOfLastShippedOp is non-zero\n        replicationLag = 0;\n      }\n\n      ClusterStatusProtos.ReplicationLoadSource rLoadSource = replicationLoadSourceMap.get(peerId);\n      if (rLoadSource != null) {\n        ageOfLastShippedOp = Math.max(rLoadSource.getAgeOfLastShippedOp(), ageOfLastShippedOp);\n        sizeOfLogQueue += rLoadSource.getSizeOfLogQueue();\n        timeStampOfLastShippedOp = Math.min(rLoadSource.getTimeStampOfLastShippedOp(),\n          timeStampOfLastShippedOp);\n        replicationLag = Math.max(rLoadSource.getReplicationLag(), replicationLag);\n      }\n      ClusterStatusProtos.ReplicationLoadSource.Builder rLoadSourceBuild =\n          ClusterStatusProtos.ReplicationLoadSource.newBuilder();\n      rLoadSourceBuild.setPeerID(peerId);\n      rLoadSourceBuild.setAgeOfLastShippedOp(ageOfLastShippedOp);\n      rLoadSourceBuild.setSizeOfLogQueue(sizeOfLogQueue);\n      rLoadSourceBuild.setTimeStampOfLastShippedOp(timeStampOfLastShippedOp);\n      rLoadSourceBuild.setReplicationLag(replicationLag);\n\n      replicationLoadSourceMap.put(peerId, rLoadSourceBuild.build());\n    }\n    this.replicationLoadSourceList = new ArrayList<ClusterStatusProtos.ReplicationLoadSource>(\n        replicationLoadSourceMap.values());\n  }"
        ],
        [
            "TestReplicationSmallTests::testReplicationStatus()",
            " 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 -\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733 -\n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742 -\n 743 -\n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  ",
            "  /**\n   * Test for HBASE-9531\n   * put a few rows into htable1, which should be replicated to htable2\n   * create a ClusterStatus instance 'status' from HBaseAdmin\n   * test : status.getLoad(server).getReplicationLoadSourceList()\n   * test : status.getLoad(server).getReplicationLoadSink()\n   * * @throws Exception\n   */\n  @Test(timeout = 300000)\n  public void testReplicationStatus() throws Exception {\n    LOG.info(\"testReplicationStatus\");\n\n    try (Admin admin = utility1.getConnection().getAdmin()) {\n\n      final byte[] qualName = Bytes.toBytes(\"q\");\n      Put p;\n\n      for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {\n        p = new Put(Bytes.toBytes(\"row\" + i));\n        p.addColumn(famName, qualName, Bytes.toBytes(\"val\" + i));\n        htable1.put(p);\n      }\n\n      ClusterStatus status = admin.getClusterStatus();\n\n      for (JVMClusterUtil.RegionServerThread thread :\n          utility1.getHBaseCluster().getRegionServerThreads()) {\n        ServerName server = thread.getRegionServer().getServerName();\n        ServerLoad sl = status.getLoad(server);\n        List<ReplicationLoadSource> rLoadSourceList = sl.getReplicationLoadSourceList();\n        ReplicationLoadSink rLoadSink = sl.getReplicationLoadSink();\n\n        // check SourceList has at least one entry\n        assertTrue(\"failed to get ReplicationLoadSourceList\", (rLoadSourceList.size() > 0));\n\n        // check Sink exist only as it is difficult to verify the value on the fly\n        assertTrue(\"failed to get ReplicationLoadSink.AgeOfLastShippedOp \",\n          (rLoadSink.getAgeOfLastAppliedOp() >= 0));\n        assertTrue(\"failed to get ReplicationLoadSink.TimeStampsOfLastAppliedOp \",\n          (rLoadSink.getTimeStampsOfLastAppliedOp() >= 0));\n      }\n    }\n  }",
            " 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 +\n 723 +\n 724 +\n 725 +\n 726 +\n 727 +\n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738 +\n 739 +\n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748 +\n 749 +\n 750 +\n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758 +\n 759 +\n 760 +\n 761 +\n 762 +\n 763 +\n 764 +\n 765 +\n 766 +\n 767 +\n 768 +\n 769 +\n 770 +\n 771 +\n 772 +\n 773  \n 774  ",
            "  /**\n   * Test for HBASE-9531\n   * put a few rows into htable1, which should be replicated to htable2\n   * create a ClusterStatus instance 'status' from HBaseAdmin\n   * test : status.getLoad(server).getReplicationLoadSourceList()\n   * test : status.getLoad(server).getReplicationLoadSink()\n   * * @throws Exception\n   */\n  @Test(timeout = 300000)\n  public void testReplicationStatus() throws Exception {\n    LOG.info(\"testReplicationStatus\");\n\n    try (Admin hbaseAdmin = utility1.getConnection().getAdmin()) {\n      // Wait roll log request in setUp() to finish\n      Thread.sleep(5000);\n\n      // disable peer\n      admin.disablePeer(PEER_ID);\n\n      final byte[] qualName = Bytes.toBytes(\"q\");\n      Put p;\n\n      for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {\n        p = new Put(Bytes.toBytes(\"row\" + i));\n        p.addColumn(famName, qualName, Bytes.toBytes(\"val\" + i));\n        htable1.put(p);\n      }\n\n      ClusterStatus status = hbaseAdmin.getClusterStatus();\n      long globalSizeOfLogQueue = 0;\n\n      for (JVMClusterUtil.RegionServerThread thread :\n          utility1.getHBaseCluster().getRegionServerThreads()) {\n        ServerName server = thread.getRegionServer().getServerName();\n        ServerLoad sl = status.getLoad(server);\n        List<ReplicationLoadSource> rLoadSourceList = sl.getReplicationLoadSourceList();\n        ReplicationLoadSink rLoadSink = sl.getReplicationLoadSink();\n\n        // check SourceList only has one entry\n        assertTrue(\"failed to get ReplicationLoadSourceList\", (rLoadSourceList.size() == 1));\n        globalSizeOfLogQueue += rLoadSourceList.get(0).getSizeOfLogQueue();\n\n        // check Sink exist only as it is difficult to verify the value on the fly\n        assertTrue(\"failed to get ReplicationLoadSink.AgeOfLastShippedOp \",\n          (rLoadSink.getAgeOfLastAppliedOp() >= 0));\n        assertTrue(\"failed to get ReplicationLoadSink.TimeStampsOfLastAppliedOp \",\n          (rLoadSink.getTimeStampsOfLastAppliedOp() >= 0));\n      }\n\n      // Stop one rs\n      utility1.getHBaseCluster().getRegionServer(1).stop(\"Stop RegionServer\");\n      Thread.sleep(5000);\n      status = hbaseAdmin.getClusterStatus();\n      ServerName server = utility1.getHBaseCluster().getRegionServer(0).getServerName();\n      ServerLoad sl = status.getLoad(server);\n      List<ReplicationLoadSource> rLoadSourceList = sl.getReplicationLoadSourceList();\n      // check SourceList only has one entry\n      assertTrue(\"failed to get ReplicationLoadSourceList\", (rLoadSourceList.size() == 1));\n      // Another rs has one queue and one recovery queue from died rs\n      assertEquals(globalSizeOfLogQueue, rLoadSourceList.get(0).getSizeOfLogQueue());\n    } finally {\n      utility1.getHBaseCluster().getRegionServer(1).start();\n      admin.enablePeer(PEER_ID);\n    }\n  }"
        ]
    ],
    "bb0fc6b602a62045e9b4eb9b617a322b9cf7380f": [
        [
            "Canary::run(String)",
            " 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 -\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    int index = parseArgs(args);\n    ChoreService choreService = null;\n\n    // Launches chore for refreshing kerberos credentials if security is enabled.\n    // Please see http://hbase.apache.org/book.html#_running_canary_in_a_kerberos_enabled_cluster\n    // for more details.\n    final ScheduledChore authChore = AuthUtil.getAuthChore(conf);\n    if (authChore != null) {\n      choreService = new ChoreService(\"CANARY_TOOL\");\n      choreService.scheduleChore(authChore);\n    }\n\n    // Start to prepare the stuffs\n    Monitor monitor = null;\n    Thread monitorThread = null;\n    long startTime = 0;\n    long currentTimeLength = 0;\n    // Get a connection to use in below.\n    try (Connection connection = ConnectionFactory.createConnection(this.conf)) {\n      do {\n        // Do monitor !!\n        try {\n          monitor = this.newMonitor(connection, index, args);\n          monitorThread = new Thread(monitor);\n          startTime = System.currentTimeMillis();\n          monitorThread.start();\n          while (!monitor.isDone()) {\n            // wait for 1 sec\n            Thread.sleep(1000);\n            // exit if any error occurs\n            if (this.failOnError && monitor.hasError()) {\n              monitorThread.interrupt();\n              if (monitor.initialized) {\n                return monitor.errorCode;\n              } else {\n                return INIT_ERROR_EXIT_CODE;\n              }\n            }\n            currentTimeLength = System.currentTimeMillis() - startTime;\n            if (currentTimeLength > this.timeout) {\n              LOG.error(\"The monitor is running too long (\" + currentTimeLength\n                  + \") after timeout limit:\" + this.timeout\n                  + \" will be killed itself !!\");\n              if (monitor.initialized) {\n                return TIMEOUT_ERROR_EXIT_CODE;\n              } else {\n                return INIT_ERROR_EXIT_CODE;\n              }\n            }\n          }\n\n          if (this.failOnError && monitor.finalCheckForErrors()) {\n            monitorThread.interrupt();\n            return monitor.errorCode;\n          }\n        } finally {\n          if (monitor != null) monitor.close();\n        }\n\n        Thread.sleep(interval);\n      } while (interval > 0);\n    } // try-with-resources close\n\n    if (choreService != null) {\n      choreService.shutdown();\n    }\n    return monitor.errorCode;\n  }",
            " 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 +\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  ",
            "  @Override\n  public int run(String[] args) throws Exception {\n    int index = parseArgs(args);\n    ChoreService choreService = null;\n\n    // Launches chore for refreshing kerberos credentials if security is enabled.\n    // Please see http://hbase.apache.org/book.html#_running_canary_in_a_kerberos_enabled_cluster\n    // for more details.\n    final ScheduledChore authChore = AuthUtil.getAuthChore(conf);\n    if (authChore != null) {\n      choreService = new ChoreService(\"CANARY_TOOL\");\n      choreService.scheduleChore(authChore);\n    }\n\n    // Start to prepare the stuffs\n    Monitor monitor = null;\n    Thread monitorThread = null;\n    long startTime = 0;\n    long currentTimeLength = 0;\n    // Get a connection to use in below.\n    try (Connection connection = ConnectionFactory.createConnection(this.conf)) {\n      do {\n        // Do monitor !!\n        try {\n          monitor = this.newMonitor(connection, index, args);\n          monitorThread = new Thread(monitor, \"CanaryMonitor-\" + System.currentTimeMillis());\n          startTime = System.currentTimeMillis();\n          monitorThread.start();\n          while (!monitor.isDone()) {\n            // wait for 1 sec\n            Thread.sleep(1000);\n            // exit if any error occurs\n            if (this.failOnError && monitor.hasError()) {\n              monitorThread.interrupt();\n              if (monitor.initialized) {\n                return monitor.errorCode;\n              } else {\n                return INIT_ERROR_EXIT_CODE;\n              }\n            }\n            currentTimeLength = System.currentTimeMillis() - startTime;\n            if (currentTimeLength > this.timeout) {\n              LOG.error(\"The monitor is running too long (\" + currentTimeLength\n                  + \") after timeout limit:\" + this.timeout\n                  + \" will be killed itself !!\");\n              if (monitor.initialized) {\n                return TIMEOUT_ERROR_EXIT_CODE;\n              } else {\n                return INIT_ERROR_EXIT_CODE;\n              }\n            }\n          }\n\n          if (this.failOnError && monitor.finalCheckForErrors()) {\n            monitorThread.interrupt();\n            return monitor.errorCode;\n          }\n        } finally {\n          if (monitor != null) monitor.close();\n        }\n\n        Thread.sleep(interval);\n      } while (interval > 0);\n    } // try-with-resources close\n\n    if (choreService != null) {\n      choreService.shutdown();\n    }\n    return monitor.errorCode;\n  }"
        ],
        [
            "MultiThreadedWriterBase::start(long,long,int)",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  ",
            "  @Override\n  public void start(long startKey, long endKey, int numThreads) throws IOException {\n    super.start(startKey, endKey, numThreads);\n    nextKeyToWrite.set(startKey);\n    wroteUpToKey.set(startKey - 1);\n\n    if (trackWroteKeys) {\n      new Thread(new WroteKeysTracker()).start();\n      numThreadsWorking.incrementAndGet();\n    }\n  }",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93 +\n  94  \n  95  \n  96  ",
            "  @Override\n  public void start(long startKey, long endKey, int numThreads) throws IOException {\n    super.start(startKey, endKey, numThreads);\n    nextKeyToWrite.set(startKey);\n    wroteUpToKey.set(startKey - 1);\n\n    if (trackWroteKeys) {\n      new Thread(new WroteKeysTracker(),\n          \"MultiThreadedWriterBase-WroteKeysTracker-\" + System.currentTimeMillis()).start();\n      numThreadsWorking.incrementAndGet();\n    }\n  }"
        ],
        [
            "RegionServicesForStores::newThread(Runnable)",
            "  45  \n  46  \n  47 -\n  48 -\n  49 -\n  50 -\n  51 -\n  52  ",
            "            @Override\n            public Thread newThread(Runnable r) {\n              Thread t = new Thread(r);\n              t.setName(Thread.currentThread().getName()\n                  + \"-inmemoryCompactions-\"\n                  + System.currentTimeMillis());\n              return t;\n            }",
            "  45  \n  46  \n  47 +\n  48 +\n  49 +\n  50  ",
            "            @Override\n            public Thread newThread(Runnable r) {\n              String name = Thread.currentThread().getName() + \"-inmemoryCompactions-\" +\n                  System.currentTimeMillis();\n              return new Thread(r, name);\n            }"
        ],
        [
            "ModifyRegionUtils::getRegionOpenAndInitThreadPool(Configuration,String,int)",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 -\n 237 -\n 238  \n 239  \n 240  \n 241  ",
            "  static ThreadPoolExecutor getRegionOpenAndInitThreadPool(final Configuration conf,\n      final String threadNamePrefix, int regionNumber) {\n    int maxThreads = Math.min(regionNumber, conf.getInt(\n        \"hbase.hregion.open.and.init.threads.max\", 10));\n    ThreadPoolExecutor regionOpenAndInitThreadPool = Threads\n    .getBoundedCachedThreadPool(maxThreads, 30L, TimeUnit.SECONDS,\n        new ThreadFactory() {\n          private int count = 1;\n\n          @Override\n          public Thread newThread(Runnable r) {\n            Thread t = new Thread(r, threadNamePrefix + \"-\" + count++);\n            return t;\n          }\n        });\n    return regionOpenAndInitThreadPool;\n  }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237  \n 238  \n 239  \n 240  ",
            "  static ThreadPoolExecutor getRegionOpenAndInitThreadPool(final Configuration conf,\n      final String threadNamePrefix, int regionNumber) {\n    int maxThreads = Math.min(regionNumber, conf.getInt(\n        \"hbase.hregion.open.and.init.threads.max\", 10));\n    ThreadPoolExecutor regionOpenAndInitThreadPool = Threads\n    .getBoundedCachedThreadPool(maxThreads, 30L, TimeUnit.SECONDS,\n        new ThreadFactory() {\n          private int count = 1;\n\n          @Override\n          public Thread newThread(Runnable r) {\n            return new Thread(r, threadNamePrefix + \"-\" + count++);\n          }\n        });\n    return regionOpenAndInitThreadPool;\n  }"
        ],
        [
            "HRegionServer::HRegionServer(Configuration,CoordinatedStateManager)",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521 -\n 522 -\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  ",
            "  /**\n   * Starts a HRegionServer at the default location\n   * @param conf\n   * @param csm implementation of CoordinatedStateManager to be used\n   * @throws IOException\n   */\n  public HRegionServer(Configuration conf, CoordinatedStateManager csm)\n      throws IOException {\n    this.fsOk = true;\n    this.conf = conf;\n    HFile.checkHFileVersion(this.conf);\n    checkCodecs(this.conf);\n    this.userProvider = UserProvider.instantiate(conf);\n    FSUtils.setupShortCircuitRead(this.conf);\n    // Disable usage of meta replicas in the regionserver\n    this.conf.setBoolean(HConstants.USE_META_REPLICAS, false);\n\n    // Config'ed params\n    this.numRetries = this.conf.getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,\n        HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER);\n    this.threadWakeFrequency = conf.getInt(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);\n    this.msgInterval = conf.getInt(\"hbase.regionserver.msginterval\", 3 * 1000);\n\n    this.sleeper = new Sleeper(this.msgInterval, this);\n\n    boolean isNoncesEnabled = conf.getBoolean(HConstants.HBASE_RS_NONCES_ENABLED, true);\n    this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null;\n\n    this.numRegionsToReport = conf.getInt(\n      \"hbase.regionserver.numregionstoreport\", 10);\n\n    this.operationTimeout = conf.getInt(\n      HConstants.HBASE_CLIENT_OPERATION_TIMEOUT,\n      HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT);\n\n    this.shortOperationTimeout = conf.getInt(\n      HConstants.HBASE_RPC_SHORTOPERATION_TIMEOUT_KEY,\n      HConstants.DEFAULT_HBASE_RPC_SHORTOPERATION_TIMEOUT);\n\n    this.abortRequested = false;\n    this.stopped = false;\n\n    rpcServices = createRpcServices();\n    this.startcode = System.currentTimeMillis();\n    if (this instanceof HMaster) {\n      useThisHostnameInstead = conf.get(MASTER_HOSTNAME_KEY);\n    } else {\n      useThisHostnameInstead = conf.get(RS_HOSTNAME_KEY);\n    }\n    String hostName = shouldUseThisHostnameInstead() ? useThisHostnameInstead :\n      rpcServices.isa.getHostName();\n    serverName = ServerName.valueOf(hostName, rpcServices.isa.getPort(), startcode);\n\n    rpcControllerFactory = RpcControllerFactory.instantiate(this.conf);\n    rpcRetryingCallerFactory = RpcRetryingCallerFactory.instantiate(this.conf);\n\n    // login the zookeeper client principal (if using security)\n    ZKUtil.loginClient(this.conf, HConstants.ZK_CLIENT_KEYTAB_FILE,\n      HConstants.ZK_CLIENT_KERBEROS_PRINCIPAL, hostName);\n    // login the server principal (if using secure Hadoop)\n    login(userProvider, hostName);\n    // init superusers and add the server principal (if using security)\n    // or process owner as default super user.\n    Superusers.initialize(conf);\n\n    regionServerAccounting = new RegionServerAccounting();\n    cacheConfig = new CacheConfig(conf);\n    mobCacheConfig = new MobCacheConfig(conf);\n    uncaughtExceptionHandler = new UncaughtExceptionHandler() {\n      @Override\n      public void uncaughtException(Thread t, Throwable e) {\n        abort(\"Uncaught exception in service thread \" + t.getName(), e);\n      }\n    };\n\n    // Set 'fs.defaultFS' to match the filesystem on hbase.rootdir else\n    // underlying hadoop hdfs accessors will be going against wrong filesystem\n    // (unless all is set to defaults).\n    FSUtils.setFsDefault(this.conf, FSUtils.getRootDir(this.conf));\n    // Get fs instance used by this RS.  Do we use checksum verification in the hbase? If hbase\n    // checksum verification enabled, then automatically switch off hdfs checksum verification.\n    boolean useHBaseChecksum = conf.getBoolean(HConstants.HBASE_CHECKSUM_VERIFICATION, true);\n    this.fs = new HFileSystem(this.conf, useHBaseChecksum);\n    this.rootDir = FSUtils.getRootDir(this.conf);\n    this.tableDescriptors = getFsTableDescriptors();\n\n    service = new ExecutorService(getServerName().toShortString());\n    spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());\n\n    // Some unit tests don't need a cluster, so no zookeeper at all\n    if (!conf.getBoolean(\"hbase.testing.nocluster\", false)) {\n      // Open connection to zookeeper and set primary watcher\n      zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + \":\" +\n        rpcServices.isa.getPort(), this, canCreateBaseZNode());\n\n      this.csm = (BaseCoordinatedStateManager) csm;\n      this.csm.initialize(this);\n      this.csm.start();\n\n      tableLockManager = TableLockManager.createTableLockManager(\n        conf, zooKeeper, serverName);\n\n      masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);\n      masterAddressTracker.start();\n\n      clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this);\n      clusterStatusTracker.start();\n    }\n    this.configurationManager = new ConfigurationManager();\n\n    this.secureBulkLoadManager = new SecureBulkLoadManager(this.conf);\n    this.secureBulkLoadManager.start();\n\n    rpcServices.start();\n    putUpWebUI();\n    this.walRoller = new LogRoller(this, this);\n    this.choreService = new ChoreService(getServerName().toString(), true);\n    this.flushThroughputController = FlushThroughputControllerFactory.create(this, conf);\n\n    if (!SystemUtils.IS_OS_WINDOWS) {\n      Signal.handle(new Signal(\"HUP\"), new SignalHandler() {\n        @Override\n        public void handle(Signal signal) {\n          getConfiguration().reloadConfiguration();\n          configurationManager.notifyAllObservers(getConfiguration());\n        }\n      });\n    }\n    // Create the CompactedFileDischarger chore service. This chore helps to\n    // remove the compacted files\n    // that will no longer be used in reads.\n    // Default is 2 mins. The default value for TTLCleaner is 5 mins so we set this to\n    // 2 mins so that compacted files can be archived before the TTLCleaner runs\n    int cleanerInterval =\n        conf.getInt(\"hbase.hfile.compaction.discharger.interval\", 2 * 60 * 1000);\n    this.compactedFileDischarger =\n        new CompactedHFilesDischarger(cleanerInterval, (Stoppable)this, (RegionServerServices)this);\n    choreService.scheduleChore(compactedFileDischarger);\n  }",
            " 512  \n 513  \n 514  \n 515  \n 516 +\n 517 +\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "  /**\n   * Starts a HRegionServer at the default location\n   * @param csm implementation of CoordinatedStateManager to be used\n   */\n  public HRegionServer(Configuration conf, CoordinatedStateManager csm) throws IOException {\n    super(\"RegionServer\");  // thread name\n    this.fsOk = true;\n    this.conf = conf;\n    HFile.checkHFileVersion(this.conf);\n    checkCodecs(this.conf);\n    this.userProvider = UserProvider.instantiate(conf);\n    FSUtils.setupShortCircuitRead(this.conf);\n    // Disable usage of meta replicas in the regionserver\n    this.conf.setBoolean(HConstants.USE_META_REPLICAS, false);\n\n    // Config'ed params\n    this.numRetries = this.conf.getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,\n        HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER);\n    this.threadWakeFrequency = conf.getInt(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);\n    this.msgInterval = conf.getInt(\"hbase.regionserver.msginterval\", 3 * 1000);\n\n    this.sleeper = new Sleeper(this.msgInterval, this);\n\n    boolean isNoncesEnabled = conf.getBoolean(HConstants.HBASE_RS_NONCES_ENABLED, true);\n    this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null;\n\n    this.numRegionsToReport = conf.getInt(\n      \"hbase.regionserver.numregionstoreport\", 10);\n\n    this.operationTimeout = conf.getInt(\n      HConstants.HBASE_CLIENT_OPERATION_TIMEOUT,\n      HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT);\n\n    this.shortOperationTimeout = conf.getInt(\n      HConstants.HBASE_RPC_SHORTOPERATION_TIMEOUT_KEY,\n      HConstants.DEFAULT_HBASE_RPC_SHORTOPERATION_TIMEOUT);\n\n    this.abortRequested = false;\n    this.stopped = false;\n\n    rpcServices = createRpcServices();\n    this.startcode = System.currentTimeMillis();\n    if (this instanceof HMaster) {\n      useThisHostnameInstead = conf.get(MASTER_HOSTNAME_KEY);\n    } else {\n      useThisHostnameInstead = conf.get(RS_HOSTNAME_KEY);\n    }\n    String hostName = shouldUseThisHostnameInstead() ? useThisHostnameInstead :\n      rpcServices.isa.getHostName();\n    serverName = ServerName.valueOf(hostName, rpcServices.isa.getPort(), startcode);\n\n    rpcControllerFactory = RpcControllerFactory.instantiate(this.conf);\n    rpcRetryingCallerFactory = RpcRetryingCallerFactory.instantiate(this.conf);\n\n    // login the zookeeper client principal (if using security)\n    ZKUtil.loginClient(this.conf, HConstants.ZK_CLIENT_KEYTAB_FILE,\n      HConstants.ZK_CLIENT_KERBEROS_PRINCIPAL, hostName);\n    // login the server principal (if using secure Hadoop)\n    login(userProvider, hostName);\n    // init superusers and add the server principal (if using security)\n    // or process owner as default super user.\n    Superusers.initialize(conf);\n\n    regionServerAccounting = new RegionServerAccounting();\n    cacheConfig = new CacheConfig(conf);\n    mobCacheConfig = new MobCacheConfig(conf);\n    uncaughtExceptionHandler = new UncaughtExceptionHandler() {\n      @Override\n      public void uncaughtException(Thread t, Throwable e) {\n        abort(\"Uncaught exception in service thread \" + t.getName(), e);\n      }\n    };\n\n    // Set 'fs.defaultFS' to match the filesystem on hbase.rootdir else\n    // underlying hadoop hdfs accessors will be going against wrong filesystem\n    // (unless all is set to defaults).\n    FSUtils.setFsDefault(this.conf, FSUtils.getRootDir(this.conf));\n    // Get fs instance used by this RS.  Do we use checksum verification in the hbase? If hbase\n    // checksum verification enabled, then automatically switch off hdfs checksum verification.\n    boolean useHBaseChecksum = conf.getBoolean(HConstants.HBASE_CHECKSUM_VERIFICATION, true);\n    this.fs = new HFileSystem(this.conf, useHBaseChecksum);\n    this.rootDir = FSUtils.getRootDir(this.conf);\n    this.tableDescriptors = getFsTableDescriptors();\n\n    service = new ExecutorService(getServerName().toShortString());\n    spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());\n\n    // Some unit tests don't need a cluster, so no zookeeper at all\n    if (!conf.getBoolean(\"hbase.testing.nocluster\", false)) {\n      // Open connection to zookeeper and set primary watcher\n      zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + \":\" +\n        rpcServices.isa.getPort(), this, canCreateBaseZNode());\n\n      this.csm = (BaseCoordinatedStateManager) csm;\n      this.csm.initialize(this);\n      this.csm.start();\n\n      tableLockManager = TableLockManager.createTableLockManager(\n        conf, zooKeeper, serverName);\n\n      masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);\n      masterAddressTracker.start();\n\n      clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this);\n      clusterStatusTracker.start();\n    }\n    this.configurationManager = new ConfigurationManager();\n\n    this.secureBulkLoadManager = new SecureBulkLoadManager(this.conf);\n    this.secureBulkLoadManager.start();\n\n    rpcServices.start();\n    putUpWebUI();\n    this.walRoller = new LogRoller(this, this);\n    this.choreService = new ChoreService(getServerName().toString(), true);\n    this.flushThroughputController = FlushThroughputControllerFactory.create(this, conf);\n\n    if (!SystemUtils.IS_OS_WINDOWS) {\n      Signal.handle(new Signal(\"HUP\"), new SignalHandler() {\n        @Override\n        public void handle(Signal signal) {\n          getConfiguration().reloadConfiguration();\n          configurationManager.notifyAllObservers(getConfiguration());\n        }\n      });\n    }\n    // Create the CompactedFileDischarger chore service. This chore helps to\n    // remove the compacted files\n    // that will no longer be used in reads.\n    // Default is 2 mins. The default value for TTLCleaner is 5 mins so we set this to\n    // 2 mins so that compacted files can be archived before the TTLCleaner runs\n    int cleanerInterval =\n        conf.getInt(\"hbase.hfile.compaction.discharger.interval\", 2 * 60 * 1000);\n    this.compactedFileDischarger =\n        new CompactedHFilesDischarger(cleanerInterval, (Stoppable)this, (RegionServerServices)this);\n    choreService.scheduleChore(compactedFileDischarger);\n  }"
        ],
        [
            "HMaster::finishActiveMasterInitialization(MonitoredTask)",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669 -\n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  ",
            "  /**\n   * Finish initialization of HMaster after becoming the primary master.\n   *\n   * <ol>\n   * <li>Initialize master components - file system manager, server manager,\n   *     assignment manager, region server tracker, etc</li>\n   * <li>Start necessary service threads - balancer, catalog janior,\n   *     executor services, etc</li>\n   * <li>Set cluster as UP in ZooKeeper</li>\n   * <li>Wait for RegionServers to check-in</li>\n   * <li>Split logs and perform data recovery, if necessary</li>\n   * <li>Ensure assignment of meta/namespace regions<li>\n   * <li>Handle either fresh cluster start or master failover</li>\n   * </ol>\n   */\n  private void finishActiveMasterInitialization(MonitoredTask status)\n      throws IOException, InterruptedException, KeeperException, CoordinatedStateException {\n\n    isActiveMaster = true;\n    Thread zombieDetector = new Thread(new InitializationMonitor(this));\n    zombieDetector.start();\n\n    /*\n     * We are active master now... go initialize components we need to run.\n     * Note, there may be dross in zk from previous runs; it'll get addressed\n     * below after we determine if cluster startup or failover.\n     */\n\n    status.setStatus(\"Initializing Master file system\");\n\n    this.masterActiveTime = System.currentTimeMillis();\n    // TODO: Do this using Dependency Injection, using PicoContainer, Guice or Spring.\n    this.fileSystemManager = new MasterFileSystem(this);\n    this.walManager = new MasterWalManager(this);\n\n    // enable table descriptors cache\n    this.tableDescriptors.setCacheOn();\n    // set the META's descriptor to the correct replication\n    this.tableDescriptors.get(TableName.META_TABLE_NAME).setRegionReplication(\n        conf.getInt(HConstants.META_REPLICAS_NUM, HConstants.DEFAULT_META_REPLICA_NUM));\n    // warm-up HTDs cache on master initialization\n    if (preLoadTableDescriptors) {\n      status.setStatus(\"Pre-loading table descriptors\");\n      this.tableDescriptors.getAll();\n    }\n\n    // publish cluster ID\n    status.setStatus(\"Publishing Cluster ID in ZooKeeper\");\n    ZKClusterId.setClusterId(this.zooKeeper, fileSystemManager.getClusterId());\n    this.initLatch.countDown();\n\n    this.serverManager = createServerManager(this);\n\n    // Invalidate all write locks held previously\n    this.tableLockManager.reapWriteLocks();\n    this.tableStateManager = new TableStateManager(this);\n\n    status.setStatus(\"Initializing ZK system trackers\");\n    initializeZKBasedSystemTrackers();\n\n    // This is for backwards compatibility\n    // See HBASE-11393\n    status.setStatus(\"Update TableCFs node in ZNode\");\n    TableCFsUpdater tableCFsUpdater = new TableCFsUpdater(zooKeeper,\n            conf, this.clusterConnection);\n    tableCFsUpdater.update();\n\n    // initialize master side coprocessors before we start handling requests\n    status.setStatus(\"Initializing master coprocessors\");\n    this.cpHost = new MasterCoprocessorHost(this, this.conf);\n\n    // start up all service threads.\n    status.setStatus(\"Initializing master service threads\");\n    startServiceThreads();\n\n    // Wake up this server to check in\n    sleeper.skipSleepCycle();\n\n    // Wait for region servers to report in\n    status.setStatus(\"Wait for region servers to report in\");\n    waitForRegionServers(status);\n\n    // get a list for previously failed RS which need log splitting work\n    // we recover hbase:meta region servers inside master initialization and\n    // handle other failed servers in SSH in order to start up master node ASAP\n    MasterMetaBootstrap metaBootstrap = createMetaBootstrap(this, status);\n    metaBootstrap.splitMetaLogsBeforeAssignment();\n\n    this.initializationBeforeMetaAssignment = true;\n\n    // Wait for regionserver to finish initialization.\n    if (BaseLoadBalancer.tablesOnMaster(conf)) {\n      waitForServerOnline();\n    }\n\n    //initialize load balancer\n    this.balancer.setMasterServices(this);\n    this.balancer.setClusterStatus(getClusterStatus());\n    this.balancer.initialize();\n\n    // Check if master is shutting down because of some issue\n    // in initializing the regionserver or the balancer.\n    if (isStopped()) return;\n\n    // Make sure meta assigned before proceeding.\n    status.setStatus(\"Assigning Meta Region\");\n    metaBootstrap.assignMeta();\n\n    // check if master is shutting down because above assignMeta could return even hbase:meta isn't\n    // assigned when master is shutting down\n    if (isStopped()) return;\n\n    // migrating existent table state from zk, so splitters\n    // and recovery process treat states properly.\n    for (Map.Entry<TableName, TableState.State> entry : ZKDataMigrator\n        .queryForTableStates(getZooKeeper()).entrySet()) {\n      LOG.info(\"Converting state from zk to new states:\" + entry);\n      tableStateManager.setTableState(entry.getKey(), entry.getValue());\n    }\n    ZKUtil.deleteChildrenRecursively(getZooKeeper(), getZooKeeper().znodePaths.tableZNode);\n\n    status.setStatus(\"Submitting log splitting work for previously failed region servers\");\n    metaBootstrap.processDeadServers();\n\n    // Fix up assignment manager status\n    status.setStatus(\"Starting assignment manager\");\n    this.assignmentManager.joinCluster();\n\n    // set cluster status again after user regions are assigned\n    this.balancer.setClusterStatus(getClusterStatus());\n\n    // Start balancer and meta catalog janitor after meta and regions have been assigned.\n    status.setStatus(\"Starting balancer and catalog janitor\");\n    this.clusterStatusChore = new ClusterStatusChore(this, balancer);\n    getChoreService().scheduleChore(clusterStatusChore);\n    this.balancerChore = new BalancerChore(this);\n    getChoreService().scheduleChore(balancerChore);\n    this.normalizerChore = new RegionNormalizerChore(this);\n    getChoreService().scheduleChore(normalizerChore);\n    this.catalogJanitorChore = new CatalogJanitor(this);\n    getChoreService().scheduleChore(catalogJanitorChore);\n\n    // Do Metrics periodically\n    periodicDoMetricsChore = new PeriodicDoMetrics(msgInterval, this);\n    getChoreService().scheduleChore(periodicDoMetricsChore);\n\n    status.setStatus(\"Starting cluster schema service\");\n    initClusterSchemaService();\n\n    if (this.cpHost != null) {\n      try {\n        this.cpHost.preMasterInitialization();\n      } catch (IOException e) {\n        LOG.error(\"Coprocessor preMasterInitialization() hook failed\", e);\n      }\n    }\n\n    status.markComplete(\"Initialization successful\");\n    LOG.info(\"Master has completed initialization\");\n    configurationManager.registerObserver(this.balancer);\n\n    // Set master as 'initialized'.\n    setInitialized(true);\n\n    status.setStatus(\"Assign meta replicas\");\n    metaBootstrap.assignMetaReplicas();\n\n    status.setStatus(\"Starting quota manager\");\n    initQuotaManager();\n\n    // clear the dead servers with same host name and port of online server because we are not\n    // removing dead server with same hostname and port of rs which is trying to check in before\n    // master initialization. See HBASE-5916.\n    this.serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer();\n\n    // Check and set the znode ACLs if needed in case we are overtaking a non-secure configuration\n    status.setStatus(\"Checking ZNode ACLs\");\n    zooKeeper.checkAndSetZNodeAcls();\n\n    status.setStatus(\"Initializing MOB Cleaner\");\n    initMobCleaner();\n\n    status.setStatus(\"Calling postStartMaster coprocessors\");\n    if (this.cpHost != null) {\n      // don't let cp initialization errors kill the master\n      try {\n        this.cpHost.postStartMaster();\n      } catch (IOException ioe) {\n        LOG.error(\"Coprocessor postStartMaster() hook failed\", ioe);\n      }\n    }\n\n    zombieDetector.interrupt();\n  }",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669 +\n 670 +\n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  ",
            "  /**\n   * Finish initialization of HMaster after becoming the primary master.\n   *\n   * <ol>\n   * <li>Initialize master components - file system manager, server manager,\n   *     assignment manager, region server tracker, etc</li>\n   * <li>Start necessary service threads - balancer, catalog janior,\n   *     executor services, etc</li>\n   * <li>Set cluster as UP in ZooKeeper</li>\n   * <li>Wait for RegionServers to check-in</li>\n   * <li>Split logs and perform data recovery, if necessary</li>\n   * <li>Ensure assignment of meta/namespace regions<li>\n   * <li>Handle either fresh cluster start or master failover</li>\n   * </ol>\n   */\n  private void finishActiveMasterInitialization(MonitoredTask status)\n      throws IOException, InterruptedException, KeeperException, CoordinatedStateException {\n\n    isActiveMaster = true;\n    Thread zombieDetector = new Thread(new InitializationMonitor(this),\n        \"ActiveMasterInitializationMonitor-\" + System.currentTimeMillis());\n    zombieDetector.start();\n\n    /*\n     * We are active master now... go initialize components we need to run.\n     * Note, there may be dross in zk from previous runs; it'll get addressed\n     * below after we determine if cluster startup or failover.\n     */\n\n    status.setStatus(\"Initializing Master file system\");\n\n    this.masterActiveTime = System.currentTimeMillis();\n    // TODO: Do this using Dependency Injection, using PicoContainer, Guice or Spring.\n    this.fileSystemManager = new MasterFileSystem(this);\n    this.walManager = new MasterWalManager(this);\n\n    // enable table descriptors cache\n    this.tableDescriptors.setCacheOn();\n    // set the META's descriptor to the correct replication\n    this.tableDescriptors.get(TableName.META_TABLE_NAME).setRegionReplication(\n        conf.getInt(HConstants.META_REPLICAS_NUM, HConstants.DEFAULT_META_REPLICA_NUM));\n    // warm-up HTDs cache on master initialization\n    if (preLoadTableDescriptors) {\n      status.setStatus(\"Pre-loading table descriptors\");\n      this.tableDescriptors.getAll();\n    }\n\n    // publish cluster ID\n    status.setStatus(\"Publishing Cluster ID in ZooKeeper\");\n    ZKClusterId.setClusterId(this.zooKeeper, fileSystemManager.getClusterId());\n    this.initLatch.countDown();\n\n    this.serverManager = createServerManager(this);\n\n    // Invalidate all write locks held previously\n    this.tableLockManager.reapWriteLocks();\n    this.tableStateManager = new TableStateManager(this);\n\n    status.setStatus(\"Initializing ZK system trackers\");\n    initializeZKBasedSystemTrackers();\n\n    // This is for backwards compatibility\n    // See HBASE-11393\n    status.setStatus(\"Update TableCFs node in ZNode\");\n    TableCFsUpdater tableCFsUpdater = new TableCFsUpdater(zooKeeper,\n            conf, this.clusterConnection);\n    tableCFsUpdater.update();\n\n    // initialize master side coprocessors before we start handling requests\n    status.setStatus(\"Initializing master coprocessors\");\n    this.cpHost = new MasterCoprocessorHost(this, this.conf);\n\n    // start up all service threads.\n    status.setStatus(\"Initializing master service threads\");\n    startServiceThreads();\n\n    // Wake up this server to check in\n    sleeper.skipSleepCycle();\n\n    // Wait for region servers to report in\n    status.setStatus(\"Wait for region servers to report in\");\n    waitForRegionServers(status);\n\n    // get a list for previously failed RS which need log splitting work\n    // we recover hbase:meta region servers inside master initialization and\n    // handle other failed servers in SSH in order to start up master node ASAP\n    MasterMetaBootstrap metaBootstrap = createMetaBootstrap(this, status);\n    metaBootstrap.splitMetaLogsBeforeAssignment();\n\n    this.initializationBeforeMetaAssignment = true;\n\n    // Wait for regionserver to finish initialization.\n    if (BaseLoadBalancer.tablesOnMaster(conf)) {\n      waitForServerOnline();\n    }\n\n    //initialize load balancer\n    this.balancer.setMasterServices(this);\n    this.balancer.setClusterStatus(getClusterStatus());\n    this.balancer.initialize();\n\n    // Check if master is shutting down because of some issue\n    // in initializing the regionserver or the balancer.\n    if (isStopped()) return;\n\n    // Make sure meta assigned before proceeding.\n    status.setStatus(\"Assigning Meta Region\");\n    metaBootstrap.assignMeta();\n\n    // check if master is shutting down because above assignMeta could return even hbase:meta isn't\n    // assigned when master is shutting down\n    if (isStopped()) return;\n\n    // migrating existent table state from zk, so splitters\n    // and recovery process treat states properly.\n    for (Map.Entry<TableName, TableState.State> entry : ZKDataMigrator\n        .queryForTableStates(getZooKeeper()).entrySet()) {\n      LOG.info(\"Converting state from zk to new states:\" + entry);\n      tableStateManager.setTableState(entry.getKey(), entry.getValue());\n    }\n    ZKUtil.deleteChildrenRecursively(getZooKeeper(), getZooKeeper().znodePaths.tableZNode);\n\n    status.setStatus(\"Submitting log splitting work for previously failed region servers\");\n    metaBootstrap.processDeadServers();\n\n    // Fix up assignment manager status\n    status.setStatus(\"Starting assignment manager\");\n    this.assignmentManager.joinCluster();\n\n    // set cluster status again after user regions are assigned\n    this.balancer.setClusterStatus(getClusterStatus());\n\n    // Start balancer and meta catalog janitor after meta and regions have been assigned.\n    status.setStatus(\"Starting balancer and catalog janitor\");\n    this.clusterStatusChore = new ClusterStatusChore(this, balancer);\n    getChoreService().scheduleChore(clusterStatusChore);\n    this.balancerChore = new BalancerChore(this);\n    getChoreService().scheduleChore(balancerChore);\n    this.normalizerChore = new RegionNormalizerChore(this);\n    getChoreService().scheduleChore(normalizerChore);\n    this.catalogJanitorChore = new CatalogJanitor(this);\n    getChoreService().scheduleChore(catalogJanitorChore);\n\n    // Do Metrics periodically\n    periodicDoMetricsChore = new PeriodicDoMetrics(msgInterval, this);\n    getChoreService().scheduleChore(periodicDoMetricsChore);\n\n    status.setStatus(\"Starting cluster schema service\");\n    initClusterSchemaService();\n\n    if (this.cpHost != null) {\n      try {\n        this.cpHost.preMasterInitialization();\n      } catch (IOException e) {\n        LOG.error(\"Coprocessor preMasterInitialization() hook failed\", e);\n      }\n    }\n\n    status.markComplete(\"Initialization successful\");\n    LOG.info(\"Master has completed initialization\");\n    configurationManager.registerObserver(this.balancer);\n\n    // Set master as 'initialized'.\n    setInitialized(true);\n\n    status.setStatus(\"Assign meta replicas\");\n    metaBootstrap.assignMetaReplicas();\n\n    status.setStatus(\"Starting quota manager\");\n    initQuotaManager();\n\n    // clear the dead servers with same host name and port of online server because we are not\n    // removing dead server with same hostname and port of rs which is trying to check in before\n    // master initialization. See HBASE-5916.\n    this.serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer();\n\n    // Check and set the znode ACLs if needed in case we are overtaking a non-secure configuration\n    status.setStatus(\"Checking ZNode ACLs\");\n    zooKeeper.checkAndSetZNodeAcls();\n\n    status.setStatus(\"Initializing MOB Cleaner\");\n    initMobCleaner();\n\n    status.setStatus(\"Calling postStartMaster coprocessors\");\n    if (this.cpHost != null) {\n      // don't let cp initialization errors kill the master\n      try {\n        this.cpHost.postStartMaster();\n      } catch (IOException ioe) {\n        LOG.error(\"Coprocessor postStartMaster() hook failed\", ioe);\n      }\n    }\n\n    zombieDetector.interrupt();\n  }"
        ],
        [
            "MultiThreadedAction::start(long,long,int)",
            " 157  \n 158  \n 159  \n 160  \n 161 -\n 162  ",
            "  public void start(long startKey, long endKey, int numThreads) throws IOException {\n    this.startKey = startKey;\n    this.endKey = endKey;\n    this.numThreads = numThreads;\n    (new Thread(new ProgressReporter(actionLetter))).start();\n  }",
            " 157  \n 158  \n 159  \n 160  \n 161 +\n 162 +\n 163  ",
            "  public void start(long startKey, long endKey, int numThreads) throws IOException {\n    this.startKey = startKey;\n    this.endKey = endKey;\n    this.numThreads = numThreads;\n    (new Thread(new ProgressReporter(actionLetter),\n        \"MultiThreadedAction-ProgressReporter-\" + System.currentTimeMillis())).start();\n  }"
        ],
        [
            "CompactSplitThread::CompactSplitThread(HRegionServer)",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140 -\n 141 -\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152 -\n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162 -\n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "  /** @param server */\n  CompactSplitThread(HRegionServer server) {\n    super();\n    this.server = server;\n    this.conf = server.getConfiguration();\n    this.regionSplitLimit = conf.getInt(REGION_SERVER_REGION_SPLIT_LIMIT,\n        DEFAULT_REGION_SERVER_REGION_SPLIT_LIMIT);\n\n    int largeThreads = Math.max(1, conf.getInt(\n        LARGE_COMPACTION_THREADS, LARGE_COMPACTION_THREADS_DEFAULT));\n    int smallThreads = conf.getInt(\n        SMALL_COMPACTION_THREADS, SMALL_COMPACTION_THREADS_DEFAULT);\n\n    int splitThreads = conf.getInt(SPLIT_THREADS, SPLIT_THREADS_DEFAULT);\n\n    // if we have throttle threads, make sure the user also specified size\n    Preconditions.checkArgument(largeThreads > 0 && smallThreads > 0);\n\n    final String n = Thread.currentThread().getName();\n\n    StealJobQueue<Runnable> stealJobQueue = new StealJobQueue<>();\n    this.longCompactions = new ThreadPoolExecutor(largeThreads, largeThreads,\n        60, TimeUnit.SECONDS, stealJobQueue,\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            Thread t = new Thread(r);\n            t.setName(n + \"-longCompactions-\" + System.currentTimeMillis());\n            return t;\n          }\n      });\n    this.longCompactions.setRejectedExecutionHandler(new Rejection());\n    this.longCompactions.prestartAllCoreThreads();\n    this.shortCompactions = new ThreadPoolExecutor(smallThreads, smallThreads,\n        60, TimeUnit.SECONDS, stealJobQueue.getStealFromQueue(),\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            Thread t = new Thread(r);\n            t.setName(n + \"-shortCompactions-\" + System.currentTimeMillis());\n            return t;\n          }\n      });\n    this.shortCompactions\n        .setRejectedExecutionHandler(new Rejection());\n    this.splits = (ThreadPoolExecutor)\n        Executors.newFixedThreadPool(splitThreads,\n            new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            Thread t = new Thread(r);\n            t.setName(n + \"-splits-\" + System.currentTimeMillis());\n            return t;\n          }\n      });\n    int mergeThreads = conf.getInt(MERGE_THREADS, MERGE_THREADS_DEFAULT);\n    this.mergePool = (ThreadPoolExecutor) Executors.newFixedThreadPool(\n        mergeThreads, new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            Thread t = new Thread(r);\n            t.setName(n + \"-merges-\" + System.currentTimeMillis());\n            return t;\n          }\n        });\n\n    // compaction throughput controller\n    this.compactionThroughputController =\n        CompactionThroughputControllerFactory.create(server, conf);\n  }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150 +\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159 +\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  /** @param server */\n  CompactSplitThread(HRegionServer server) {\n    super();\n    this.server = server;\n    this.conf = server.getConfiguration();\n    this.regionSplitLimit = conf.getInt(REGION_SERVER_REGION_SPLIT_LIMIT,\n        DEFAULT_REGION_SERVER_REGION_SPLIT_LIMIT);\n\n    int largeThreads = Math.max(1, conf.getInt(\n        LARGE_COMPACTION_THREADS, LARGE_COMPACTION_THREADS_DEFAULT));\n    int smallThreads = conf.getInt(\n        SMALL_COMPACTION_THREADS, SMALL_COMPACTION_THREADS_DEFAULT);\n\n    int splitThreads = conf.getInt(SPLIT_THREADS, SPLIT_THREADS_DEFAULT);\n\n    // if we have throttle threads, make sure the user also specified size\n    Preconditions.checkArgument(largeThreads > 0 && smallThreads > 0);\n\n    final String n = Thread.currentThread().getName();\n\n    StealJobQueue<Runnable> stealJobQueue = new StealJobQueue<>();\n    this.longCompactions = new ThreadPoolExecutor(largeThreads, largeThreads,\n        60, TimeUnit.SECONDS, stealJobQueue,\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            String name = n + \"-longCompactions-\" + System.currentTimeMillis();\n            return new Thread(r, name);\n          }\n      });\n    this.longCompactions.setRejectedExecutionHandler(new Rejection());\n    this.longCompactions.prestartAllCoreThreads();\n    this.shortCompactions = new ThreadPoolExecutor(smallThreads, smallThreads,\n        60, TimeUnit.SECONDS, stealJobQueue.getStealFromQueue(),\n        new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            String name = n + \"-shortCompactions-\" + System.currentTimeMillis();\n            return new Thread(r, name);\n          }\n      });\n    this.shortCompactions\n        .setRejectedExecutionHandler(new Rejection());\n    this.splits = (ThreadPoolExecutor)\n        Executors.newFixedThreadPool(splitThreads,\n            new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            String name = n + \"-splits-\" + System.currentTimeMillis();\n            return new Thread(r, name);\n          }\n      });\n    int mergeThreads = conf.getInt(MERGE_THREADS, MERGE_THREADS_DEFAULT);\n    this.mergePool = (ThreadPoolExecutor) Executors.newFixedThreadPool(\n        mergeThreads, new ThreadFactory() {\n          @Override\n          public Thread newThread(Runnable r) {\n            String name = n + \"-merges-\" + System.currentTimeMillis();\n            return new Thread(r, name);\n          }\n        });\n\n    // compaction throughput controller\n    this.compactionThroughputController =\n        CompactionThroughputControllerFactory.create(server, conf);\n  }"
        ],
        [
            "PolicyBasedChaosMonkey::start()",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  ",
            "  @Override\n  public void start() throws Exception {\n    monkeyThreads = new Thread[policies.length];\n\n    for (int i=0; i<policies.length; i++) {\n      policies[i].init(new Policy.PolicyContext(this.util));\n      Thread monkeyThread = new Thread(policies[i]);\n      monkeyThread.start();\n      monkeyThreads[i] = monkeyThread;\n    }\n  }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117  \n 118  \n 119  \n 120  ",
            "  @Override\n  public void start() throws Exception {\n    monkeyThreads = new Thread[policies.length];\n\n    for (int i=0; i<policies.length; i++) {\n      policies[i].init(new Policy.PolicyContext(this.util));\n      Thread monkeyThread = new Thread(policies[i], \"ChaosMonkeyThread\");\n      monkeyThread.start();\n      monkeyThreads[i] = monkeyThread;\n    }\n  }"
        ],
        [
            "PerformanceEvaluationCommons::concurrentReads(Runnable)",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73 -\n  74 -\n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "  public static void concurrentReads(final Runnable r) {\n    final int count = 1;\n    long now = System.currentTimeMillis();\n    List<Thread> threads = new ArrayList<Thread>(count);\n    for (int i = 0; i < count; i++) {\n      Thread t = new Thread(r);\n      t.setName(\"\" + i);\n      threads.add(t);\n    }\n    for (Thread t: threads) {\n      t.start();\n    }\n    for (Thread t: threads) {\n      try {\n        t.join();\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    LOG.info(\"Test took \" + (System.currentTimeMillis() - now));\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  ",
            "  public static void concurrentReads(final Runnable r) {\n    final int count = 1;\n    long now = System.currentTimeMillis();\n    List<Thread> threads = new ArrayList<Thread>(count);\n    for (int i = 0; i < count; i++) {\n      threads.add(new Thread(r, \"concurrentRead-\" + i));\n    }\n    for (Thread t: threads) {\n      t.start();\n    }\n    for (Thread t: threads) {\n      try {\n        t.join();\n      } catch (InterruptedException e) {\n        e.printStackTrace();\n      }\n    }\n    LOG.info(\"Test took \" + (System.currentTimeMillis() - now));\n  }"
        ],
        [
            "Leases::Leases(int)",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  ",
            "  /**\n   * Creates a lease monitor\n   * \n   * @param leaseCheckFrequency - how often the lease should be checked\n   *          (milliseconds)\n   */\n  public Leases(final int leaseCheckFrequency) {\n    this.leaseCheckFrequency = leaseCheckFrequency;\n    setDaemon(true);\n  }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  ",
            "  /**\n   * Creates a lease monitor\n   * \n   * @param leaseCheckFrequency - how often the lease should be checked\n   *          (milliseconds)\n   */\n  public Leases(final int leaseCheckFrequency) {\n    super(\"RegionServerLeases\");  // thread name\n    this.leaseCheckFrequency = leaseCheckFrequency;\n    setDaemon(true);\n  }"
        ],
        [
            "BucketCache::WriterThread::WriterThread(BlockingQueue)",
            " 786  \n 787  \n 788  ",
            "    WriterThread(BlockingQueue<RAMQueueEntry> queue) {\n      this.inputQueue = queue;\n    }",
            " 786  \n 787 +\n 788  \n 789  ",
            "    WriterThread(BlockingQueue<RAMQueueEntry> queue) {\n      super(\"BucketCacheWriterThread\");\n      this.inputQueue = queue;\n    }"
        ],
        [
            "LogRoller::LogRoller(Server,RegionServerServices)",
            "  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  /** @param server */\n  public LogRoller(final Server server, final RegionServerServices services) {\n    super();\n    this.server = server;\n    this.services = services;\n    this.rollperiod = this.server.getConfiguration().\n      getLong(\"hbase.regionserver.logroll.period\", 3600000);\n    this.threadWakeFrequency = this.server.getConfiguration().\n      getInt(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);\n  }",
            "  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  /** @param server */\n  public LogRoller(final Server server, final RegionServerServices services) {\n    super(\"LogRoller\");\n    this.server = server;\n    this.services = services;\n    this.rollperiod = this.server.getConfiguration().\n      getLong(\"hbase.regionserver.logroll.period\", 3600000);\n    this.threadWakeFrequency = this.server.getConfiguration().\n      getInt(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);\n  }"
        ],
        [
            "JvmPauseMonitor::start()",
            "  84  \n  85  \n  86 -\n  87  \n  88 -\n  89  \n  90  ",
            "  public void start() {\n    Preconditions.checkState(monitorThread == null, \"Already started\");\n    monitorThread = new Thread(new Monitor());\n    monitorThread.setDaemon(true);\n    monitorThread.setName(\"JvmPauseMonitor\");\n    monitorThread.start();\n  }",
            "  84  \n  85  \n  86 +\n  87  \n  88  \n  89  ",
            "  public void start() {\n    Preconditions.checkState(monitorThread == null, \"Already started\");\n    monitorThread = new Thread(new Monitor(), \"JvmPauseMonitor\");\n    monitorThread.setDaemon(true);\n    monitorThread.start();\n  }"
        ],
        [
            "PrefetchExecutor::newThread(Runnable)",
            "  64  \n  65  \n  66 -\n  67 -\n  68  \n  69  \n  70  ",
            "        @Override\n        public Thread newThread(Runnable r) {\n          Thread t = new Thread(r);\n          t.setName(\"hfile-prefetch-\" + System.currentTimeMillis());\n          t.setDaemon(true);\n          return t;\n        }",
            "  64  \n  65  \n  66 +\n  67 +\n  68  \n  69  \n  70  ",
            "        @Override\n        public Thread newThread(Runnable r) {\n          String name = \"hfile-prefetch-\" + System.currentTimeMillis();\n          Thread t = new Thread(r, name);\n          t.setDaemon(true);\n          return t;\n        }"
        ],
        [
            "ShutdownHook::ShutdownHookThread::run()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "    @Override\n    public void run() {\n      boolean b = this.conf.getBoolean(RUN_SHUTDOWN_HOOK, true);\n      LOG.info(\"Shutdown hook starting; \" + RUN_SHUTDOWN_HOOK + \"=\" + b +\n        \"; fsShutdownHook=\" + this.fsShutdownHook);\n      if (b) {\n        this.stop.stop(\"Shutdown hook\");\n        Threads.shutdown(this.threadToJoin);\n        if (this.fsShutdownHook != null) {\n          synchronized (fsShutdownHooks) {\n            int refs = fsShutdownHooks.get(fsShutdownHook);\n            if (refs == 1) {\n              LOG.info(\"Starting fs shutdown hook thread.\");\n              Thread fsShutdownHookThread = (fsShutdownHook instanceof Thread) ?\n                (Thread)fsShutdownHook : new Thread(fsShutdownHook);\n              fsShutdownHookThread.start();\n              Threads.shutdown(fsShutdownHookThread,\n              this.conf.getLong(FS_SHUTDOWN_HOOK_WAIT, 30000));\n            }\n            if (refs > 0) {\n              fsShutdownHooks.put(fsShutdownHook, refs - 1);\n            }\n          }\n        }\n      }\n      LOG.info(\"Shutdown hook finished.\");\n    }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "    @Override\n    public void run() {\n      boolean b = this.conf.getBoolean(RUN_SHUTDOWN_HOOK, true);\n      LOG.info(\"Shutdown hook starting; \" + RUN_SHUTDOWN_HOOK + \"=\" + b +\n        \"; fsShutdownHook=\" + this.fsShutdownHook);\n      if (b) {\n        this.stop.stop(\"Shutdown hook\");\n        Threads.shutdown(this.threadToJoin);\n        if (this.fsShutdownHook != null) {\n          synchronized (fsShutdownHooks) {\n            int refs = fsShutdownHooks.get(fsShutdownHook);\n            if (refs == 1) {\n              LOG.info(\"Starting fs shutdown hook thread.\");\n              Thread fsShutdownHookThread = (fsShutdownHook instanceof Thread) ?\n                (Thread)fsShutdownHook : new Thread(fsShutdownHook,\n                  fsShutdownHook.getClass().getSimpleName() + \"-shutdown-hook\");\n              fsShutdownHookThread.start();\n              Threads.shutdown(fsShutdownHookThread,\n              this.conf.getLong(FS_SHUTDOWN_HOOK_WAIT, 30000));\n            }\n            if (refs > 0) {\n              fsShutdownHooks.put(fsShutdownHook, refs - 1);\n            }\n          }\n        }\n      }\n      LOG.info(\"Shutdown hook finished.\");\n    }"
        ],
        [
            "MasterMobCompactionThread::MasterMobCompactionThread(HMaster)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 -\n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  ",
            "  public MasterMobCompactionThread(HMaster master) {\n    this.master = master;\n    this.conf = master.getConfiguration();\n    final String n = Thread.currentThread().getName();\n    // this pool is used to run the mob compaction\n    this.masterMobPool = new ThreadPoolExecutor(1, 2, 60, TimeUnit.SECONDS,\n      new SynchronousQueue<Runnable>(), new ThreadFactory() {\n        @Override\n        public Thread newThread(Runnable r) {\n          Thread t = new Thread(r);\n          t.setName(n + \"-MasterMobCompaction-\" + EnvironmentEdgeManager.currentTime());\n          return t;\n        }\n      });\n    ((ThreadPoolExecutor) this.masterMobPool).allowCoreThreadTimeOut(true);\n    // this pool is used in the mob compaction to compact the mob files by partitions\n    // in parallel\n    this.mobCompactorPool = MobUtils\n      .createMobCompactorThreadPool(master.getConfiguration());\n  }",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 +\n  61 +\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "  public MasterMobCompactionThread(HMaster master) {\n    this.master = master;\n    this.conf = master.getConfiguration();\n    final String n = Thread.currentThread().getName();\n    // this pool is used to run the mob compaction\n    this.masterMobPool = new ThreadPoolExecutor(1, 2, 60, TimeUnit.SECONDS,\n      new SynchronousQueue<Runnable>(), new ThreadFactory() {\n        @Override\n        public Thread newThread(Runnable r) {\n          String name = n + \"-MasterMobCompaction-\" + EnvironmentEdgeManager.currentTime();\n          return new Thread(r, name);\n        }\n      });\n    ((ThreadPoolExecutor) this.masterMobPool).allowCoreThreadTimeOut(true);\n    // this pool is used in the mob compaction to compact the mob files by partitions\n    // in parallel\n    this.mobCompactorPool = MobUtils\n      .createMobCompactorThreadPool(master.getConfiguration());\n  }"
        ],
        [
            "HFileSystem::addLocationsOrderInterceptor(Configuration,ReorderBlocks)",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276 -\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "  /**\n   * Add an interceptor on the calls to the namenode#getBlockLocations from the DFSClient\n   * linked to this FileSystem. See HBASE-6435 for the background.\n   * <p/>\n   * There should be no reason, except testing, to create a specific ReorderBlocks.\n   *\n   * @return true if the interceptor was added, false otherwise.\n   */\n  static boolean addLocationsOrderInterceptor(Configuration conf, final ReorderBlocks lrb) {\n    if (!conf.getBoolean(\"hbase.filesystem.reorder.blocks\", true)) {  // activated by default\n      LOG.debug(\"addLocationsOrderInterceptor configured to false\");\n      return false;\n    }\n\n    FileSystem fs;\n    try {\n      fs = FileSystem.get(conf);\n    } catch (IOException e) {\n      LOG.warn(\"Can't get the file system from the conf.\", e);\n      return false;\n    }\n\n    if (!(fs instanceof DistributedFileSystem)) {\n      LOG.debug(\"The file system is not a DistributedFileSystem. \" +\n          \"Skipping on block location reordering\");\n      return false;\n    }\n\n    DistributedFileSystem dfs = (DistributedFileSystem) fs;\n    DFSClient dfsc = dfs.getClient();\n    if (dfsc == null) {\n      LOG.warn(\"The DistributedFileSystem does not contain a DFSClient. Can't add the location \" +\n          \"block reordering interceptor. Continuing, but this is unexpected.\"\n      );\n      return false;\n    }\n\n    try {\n      Field nf = DFSClient.class.getDeclaredField(\"namenode\");\n      nf.setAccessible(true);\n      Field modifiersField = Field.class.getDeclaredField(\"modifiers\");\n      modifiersField.setAccessible(true);\n      modifiersField.setInt(nf, nf.getModifiers() & ~Modifier.FINAL);\n\n      ClientProtocol namenode = (ClientProtocol) nf.get(dfsc);\n      if (namenode == null) {\n        LOG.warn(\"The DFSClient is not linked to a namenode. Can't add the location block\" +\n            \" reordering interceptor. Continuing, but this is unexpected.\"\n        );\n        return false;\n      }\n\n      ClientProtocol cp1 = createReorderingProxy(namenode, lrb, conf);\n      nf.set(dfsc, cp1);\n      LOG.info(\"Added intercepting call to namenode#getBlockLocations so can do block reordering\" +\n        \" using class \" + lrb.getClass());\n    } catch (NoSuchFieldException e) {\n      LOG.warn(\"Can't modify the DFSClient#namenode field to add the location reorder.\", e);\n      return false;\n    } catch (IllegalAccessException e) {\n      LOG.warn(\"Can't modify the DFSClient#namenode field to add the location reorder.\", e);\n      return false;\n    }\n\n    return true;\n  }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "  /**\n   * Add an interceptor on the calls to the namenode#getBlockLocations from the DFSClient\n   * linked to this FileSystem. See HBASE-6435 for the background.\n   * <p/>\n   * There should be no reason, except testing, to create a specific ReorderBlocks.\n   *\n   * @return true if the interceptor was added, false otherwise.\n   */\n  static boolean addLocationsOrderInterceptor(Configuration conf, final ReorderBlocks lrb) {\n    if (!conf.getBoolean(\"hbase.filesystem.reorder.blocks\", true)) {  // activated by default\n      LOG.debug(\"addLocationsOrderInterceptor configured to false\");\n      return false;\n    }\n\n    FileSystem fs;\n    try {\n      fs = FileSystem.get(conf);\n    } catch (IOException e) {\n      LOG.warn(\"Can't get the file system from the conf.\", e);\n      return false;\n    }\n\n    if (!(fs instanceof DistributedFileSystem)) {\n      LOG.debug(\"The file system is not a DistributedFileSystem. \" +\n          \"Skipping on block location reordering\");\n      return false;\n    }\n\n    DistributedFileSystem dfs = (DistributedFileSystem) fs;\n    DFSClient dfsc = dfs.getClient();\n    if (dfsc == null) {\n      LOG.warn(\"The DistributedFileSystem does not contain a DFSClient. Can't add the location \" +\n          \"block reordering interceptor. Continuing, but this is unexpected.\"\n      );\n      return false;\n    }\n\n    try {\n      Field nf = DFSClient.class.getDeclaredField(\"namenode\");\n      nf.setAccessible(true);\n      Field modifiersField = Field.class.getDeclaredField(\"modifiers\");\n      modifiersField.setAccessible(true);\n      modifiersField.setInt(nf, nf.getModifiers() & ~Modifier.FINAL);\n\n      ClientProtocol namenode = (ClientProtocol) nf.get(dfsc);\n      if (namenode == null) {\n        LOG.warn(\"The DFSClient is not linked to a namenode. Can't add the location block\" +\n            \" reordering interceptor. Continuing, but this is unexpected.\"\n        );\n        return false;\n      }\n\n      ClientProtocol cp1 = createReorderingProxy(namenode, lrb, conf);\n      nf.set(dfsc, cp1);\n      LOG.info(\"Added intercepting call to namenode#getBlockLocations so can do block reordering\" +\n        \" using class \" + lrb.getClass().getName());\n    } catch (NoSuchFieldException e) {\n      LOG.warn(\"Can't modify the DFSClient#namenode field to add the location reorder.\", e);\n      return false;\n    } catch (IllegalAccessException e) {\n      LOG.warn(\"Can't modify the DFSClient#namenode field to add the location reorder.\", e);\n      return false;\n    }\n\n    return true;\n  }"
        ]
    ],
    "eeaea4aea3dcd3e9e2ec5a05d77fd1dd0f06382b": [
        [
            "HBaseFsck::checkRegionBoundaries()",
            " 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902 -\n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  ",
            "  public void checkRegionBoundaries() {\n    try {\n      ByteArrayComparator comparator = new ByteArrayComparator();\n      List<HRegionInfo> regions = MetaTableAccessor.getAllRegions(connection, true);\n      final RegionBoundariesInformation currentRegionBoundariesInformation =\n          new RegionBoundariesInformation();\n      Path hbaseRoot = FSUtils.getRootDir(getConf());\n      for (HRegionInfo regionInfo : regions) {\n        Path tableDir = FSUtils.getTableDir(hbaseRoot, regionInfo.getTable());\n        currentRegionBoundariesInformation.regionName = regionInfo.getRegionName();\n        // For each region, get the start and stop key from the META and compare them to the\n        // same information from the Stores.\n        Path path = new Path(tableDir, regionInfo.getEncodedName());\n        FileSystem fs = path.getFileSystem(getConf());\n        FileStatus[] files = fs.listStatus(path);\n        // For all the column families in this region...\n        byte[] storeFirstKey = null;\n        byte[] storeLastKey = null;\n        for (FileStatus file : files) {\n          String fileName = file.getPath().toString();\n          fileName = fileName.substring(fileName.lastIndexOf(\"/\") + 1);\n          if (!fileName.startsWith(\".\") && !fileName.endsWith(\"recovered.edits\")) {\n            FileStatus[] storeFiles = fs.listStatus(file.getPath());\n            // For all the stores in this column family.\n            for (FileStatus storeFile : storeFiles) {\n              HFile.Reader reader = HFile.createReader(fs, storeFile.getPath(), new CacheConfig(\n                  getConf()), getConf());\n              if ((reader.getFirstKey() != null)\n                  && ((storeFirstKey == null) || (comparator.compare(storeFirstKey,\n                      ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey()).getKey()) > 0))) {\n                storeFirstKey = ((KeyValue.KeyOnlyKeyValue)reader.getFirstKey()).getKey();\n              }\n              if ((reader.getLastKey() != null)\n                  && ((storeLastKey == null) || (comparator.compare(storeLastKey,\n                      ((KeyValue.KeyOnlyKeyValue)reader.getLastKey()).getKey())) < 0)) {\n                storeLastKey = ((KeyValue.KeyOnlyKeyValue)reader.getLastKey()).getKey();\n              }\n              reader.close();\n            }\n          }\n        }\n        currentRegionBoundariesInformation.metaFirstKey = regionInfo.getStartKey();\n        currentRegionBoundariesInformation.metaLastKey = regionInfo.getEndKey();\n        currentRegionBoundariesInformation.storesFirstKey = keyOnly(storeFirstKey);\n        currentRegionBoundariesInformation.storesLastKey = keyOnly(storeLastKey);\n        if (currentRegionBoundariesInformation.metaFirstKey.length == 0)\n          currentRegionBoundariesInformation.metaFirstKey = null;\n        if (currentRegionBoundariesInformation.metaLastKey.length == 0)\n          currentRegionBoundariesInformation.metaLastKey = null;\n\n        // For a region to be correct, we need the META start key to be smaller or equal to the\n        // smallest start key from all the stores, and the start key from the next META entry to\n        // be bigger than the last key from all the current stores. First region start key is null;\n        // Last region end key is null; some regions can be empty and not have any store.\n\n        boolean valid = true;\n        // Checking start key.\n        if ((currentRegionBoundariesInformation.storesFirstKey != null)\n            && (currentRegionBoundariesInformation.metaFirstKey != null)) {\n          valid = valid\n              && comparator.compare(currentRegionBoundariesInformation.storesFirstKey,\n                currentRegionBoundariesInformation.metaFirstKey) >= 0;\n        }\n        // Checking stop key.\n        if ((currentRegionBoundariesInformation.storesLastKey != null)\n            && (currentRegionBoundariesInformation.metaLastKey != null)) {\n          valid = valid\n              && comparator.compare(currentRegionBoundariesInformation.storesLastKey,\n                currentRegionBoundariesInformation.metaLastKey) < 0;\n        }\n        if (!valid) {\n          errors.reportError(ERROR_CODE.BOUNDARIES_ERROR, \"Found issues with regions boundaries\",\n            tablesInfo.get(regionInfo.getTable()));\n          LOG.warn(\"Region's boundaries not alligned between stores and META for:\");\n          LOG.warn(currentRegionBoundariesInformation);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(e);\n    }\n  }",
            " 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902 +\n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  ",
            "  public void checkRegionBoundaries() {\n    try {\n      ByteArrayComparator comparator = new ByteArrayComparator();\n      List<HRegionInfo> regions = MetaTableAccessor.getAllRegions(connection, true);\n      final RegionBoundariesInformation currentRegionBoundariesInformation =\n          new RegionBoundariesInformation();\n      Path hbaseRoot = FSUtils.getRootDir(getConf());\n      for (HRegionInfo regionInfo : regions) {\n        Path tableDir = FSUtils.getTableDir(hbaseRoot, regionInfo.getTable());\n        currentRegionBoundariesInformation.regionName = regionInfo.getRegionName();\n        // For each region, get the start and stop key from the META and compare them to the\n        // same information from the Stores.\n        Path path = new Path(tableDir, regionInfo.getEncodedName());\n        FileSystem fs = path.getFileSystem(getConf());\n        FileStatus[] files = fs.listStatus(path);\n        // For all the column families in this region...\n        byte[] storeFirstKey = null;\n        byte[] storeLastKey = null;\n        for (FileStatus file : files) {\n          String fileName = file.getPath().toString();\n          fileName = fileName.substring(fileName.lastIndexOf(\"/\") + 1);\n          if (!fileName.startsWith(\".\") && !fileName.endsWith(\"recovered.edits\")) {\n            FileStatus[] storeFiles = fs.listStatus(file.getPath());\n            // For all the stores in this column family.\n            for (FileStatus storeFile : storeFiles) {\n              HFile.Reader reader = HFile.createReader(fs, storeFile.getPath(), new CacheConfig(\n                  getConf()), getConf());\n              if ((reader.getFirstKey() != null)\n                  && ((storeFirstKey == null) || (comparator.compare(storeFirstKey,\n                      ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey()).getKey()) > 0))) {\n                storeFirstKey = ((KeyValue.KeyOnlyKeyValue)reader.getFirstKey()).getKey();\n              }\n              if ((reader.getLastKey() != null)\n                  && ((storeLastKey == null) || (comparator.compare(storeLastKey,\n                      ((KeyValue.KeyOnlyKeyValue)reader.getLastKey()).getKey())) < 0)) {\n                storeLastKey = ((KeyValue.KeyOnlyKeyValue)reader.getLastKey()).getKey();\n              }\n              reader.close();\n            }\n          }\n        }\n        currentRegionBoundariesInformation.metaFirstKey = regionInfo.getStartKey();\n        currentRegionBoundariesInformation.metaLastKey = regionInfo.getEndKey();\n        currentRegionBoundariesInformation.storesFirstKey = keyOnly(storeFirstKey);\n        currentRegionBoundariesInformation.storesLastKey = keyOnly(storeLastKey);\n        if (currentRegionBoundariesInformation.metaFirstKey.length == 0)\n          currentRegionBoundariesInformation.metaFirstKey = null;\n        if (currentRegionBoundariesInformation.metaLastKey.length == 0)\n          currentRegionBoundariesInformation.metaLastKey = null;\n\n        // For a region to be correct, we need the META start key to be smaller or equal to the\n        // smallest start key from all the stores, and the start key from the next META entry to\n        // be bigger than the last key from all the current stores. First region start key is null;\n        // Last region end key is null; some regions can be empty and not have any store.\n\n        boolean valid = true;\n        // Checking start key.\n        if ((currentRegionBoundariesInformation.storesFirstKey != null)\n            && (currentRegionBoundariesInformation.metaFirstKey != null)) {\n          valid = valid\n              && comparator.compare(currentRegionBoundariesInformation.storesFirstKey,\n                currentRegionBoundariesInformation.metaFirstKey) >= 0;\n        }\n        // Checking stop key.\n        if ((currentRegionBoundariesInformation.storesLastKey != null)\n            && (currentRegionBoundariesInformation.metaLastKey != null)) {\n          valid = valid\n              && comparator.compare(currentRegionBoundariesInformation.storesLastKey,\n                currentRegionBoundariesInformation.metaLastKey) < 0;\n        }\n        if (!valid) {\n          errors.reportError(ERROR_CODE.BOUNDARIES_ERROR, \"Found issues with regions boundaries\",\n            tablesInfo.get(regionInfo.getTable()));\n          LOG.warn(\"Region's boundaries not aligned between stores and META for:\");\n          LOG.warn(currentRegionBoundariesInformation);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(e);\n    }\n  }"
        ],
        [
            "FavoredNodeAssignmentHelper::multiRackCase(HRegionInfo,ServerName,String)",
            " 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 -\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  ",
            "  private ServerName[] multiRackCase(HRegionInfo regionInfo,\n      ServerName primaryRS,\n      String primaryRack) throws IOException {\n\n    // Random to choose the secondary and tertiary region server\n    // from another rack to place the secondary and tertiary\n\n    // Random to choose one rack except for the current rack\n    Set<String> rackSkipSet = new HashSet<String>();\n    rackSkipSet.add(primaryRack);\n    ServerName[] favoredNodes = new ServerName[2];\n    String secondaryRack = getOneRandomRack(rackSkipSet);\n    List<ServerName> serverList = getServersFromRack(secondaryRack);\n    if (serverList.size() >= 2) {\n      // Randomly pick up two servers from this secondary rack\n\n      // Place the secondary RS\n      ServerName secondaryRS = getOneRandomServer(secondaryRack);\n\n      // Skip the secondary for the tertiary placement\n      Set<ServerName> skipServerSet = new HashSet<ServerName>();\n      skipServerSet.add(secondaryRS);\n      // Place the tertiary RS\n      ServerName tertiaryRS = getOneRandomServer(secondaryRack, skipServerSet);\n\n      if (secondaryRS == null || tertiaryRS == null) {\n        LOG.error(\"Cannot place the secondary and terinary\" +\n            \"region server for region \" +\n            regionInfo.getRegionNameAsString());\n      }\n      // Create the secondary and tertiary pair\n      favoredNodes[0] = secondaryRS;\n      favoredNodes[1] = tertiaryRS;\n    } else {\n      // Pick the secondary rs from this secondary rack\n      // and pick the tertiary from another random rack\n      favoredNodes[0] = getOneRandomServer(secondaryRack);\n\n      // Pick the tertiary\n      if (getTotalNumberOfRacks() == 2) {\n        // Pick the tertiary from the same rack of the primary RS\n        Set<ServerName> serverSkipSet = new HashSet<ServerName>();\n        serverSkipSet.add(primaryRS);\n        favoredNodes[1] = getOneRandomServer(primaryRack, serverSkipSet);\n      } else {\n        // Pick the tertiary from another rack\n        rackSkipSet.add(secondaryRack);\n        String tertiaryRandomRack = getOneRandomRack(rackSkipSet);\n        favoredNodes[1] = getOneRandomServer(tertiaryRandomRack);\n      }\n    }\n    return favoredNodes;\n  }",
            " 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 +\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  ",
            "  private ServerName[] multiRackCase(HRegionInfo regionInfo,\n      ServerName primaryRS,\n      String primaryRack) throws IOException {\n\n    // Random to choose the secondary and tertiary region server\n    // from another rack to place the secondary and tertiary\n\n    // Random to choose one rack except for the current rack\n    Set<String> rackSkipSet = new HashSet<String>();\n    rackSkipSet.add(primaryRack);\n    ServerName[] favoredNodes = new ServerName[2];\n    String secondaryRack = getOneRandomRack(rackSkipSet);\n    List<ServerName> serverList = getServersFromRack(secondaryRack);\n    if (serverList.size() >= 2) {\n      // Randomly pick up two servers from this secondary rack\n\n      // Place the secondary RS\n      ServerName secondaryRS = getOneRandomServer(secondaryRack);\n\n      // Skip the secondary for the tertiary placement\n      Set<ServerName> skipServerSet = new HashSet<ServerName>();\n      skipServerSet.add(secondaryRS);\n      // Place the tertiary RS\n      ServerName tertiaryRS = getOneRandomServer(secondaryRack, skipServerSet);\n\n      if (secondaryRS == null || tertiaryRS == null) {\n        LOG.error(\"Cannot place the secondary and ternary\" +\n            \"region server for region \" +\n            regionInfo.getRegionNameAsString());\n      }\n      // Create the secondary and tertiary pair\n      favoredNodes[0] = secondaryRS;\n      favoredNodes[1] = tertiaryRS;\n    } else {\n      // Pick the secondary rs from this secondary rack\n      // and pick the tertiary from another random rack\n      favoredNodes[0] = getOneRandomServer(secondaryRack);\n\n      // Pick the tertiary\n      if (getTotalNumberOfRacks() == 2) {\n        // Pick the tertiary from the same rack of the primary RS\n        Set<ServerName> serverSkipSet = new HashSet<ServerName>();\n        serverSkipSet.add(primaryRS);\n        favoredNodes[1] = getOneRandomServer(primaryRack, serverSkipSet);\n      } else {\n        // Pick the tertiary from another rack\n        rackSkipSet.add(secondaryRack);\n        String tertiaryRandomRack = getOneRandomRack(rackSkipSet);\n        favoredNodes[1] = getOneRandomServer(tertiaryRandomRack);\n      }\n    }\n    return favoredNodes;\n  }"
        ],
        [
            "CompactedHFilesDischarger::chore()",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  ",
            "  @Override\n  public void chore() {\n    // Noop if rss is null. This will never happen in a normal condition except for cases\n    // when the test case is not spinning up a cluster\n    if (regionServerServices == null) return;\n    List<Region> onlineRegions = regionServerServices.getOnlineRegions();\n    if (onlineRegions != null) {\n      for (Region region : onlineRegions) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\n              \"Started the compacted hfiles cleaner for the region \" + region.getRegionInfo());\n        }\n        for (Store store : region.getStores()) {\n          try {\n            if (useExecutor && regionServerServices != null) {\n              CompactedHFilesDischargeHandler handler = new CompactedHFilesDischargeHandler(\n                  (Server) regionServerServices, EventType.RS_COMPACTED_FILES_DISCHARGER,\n                  (HStore) store);\n              regionServerServices.getExecutorService().submit(handler);\n            } else {\n              // call synchronously if the RegionServerServices are not\n              // available\n              store.closeAndArchiveCompactedFiles();\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Completed archiving the compacted files for the region \"\n                  + region.getRegionInfo() + \" under the store \" + store.getColumnFamilyName());\n            }\n          } catch (Exception e) {\n            LOG.error(\"Exception while trying to close and archive the comapcted store \"\n                + \"files of the store  \" + store.getColumnFamilyName() + \" in the\" + \" region \"\n                + region.getRegionInfo(), e);\n          }\n        }\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\n              \"Completed the compacted hfiles cleaner for the region \" + region.getRegionInfo());\n        }\n      }\n    }\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  ",
            "  @Override\n  public void chore() {\n    // Noop if rss is null. This will never happen in a normal condition except for cases\n    // when the test case is not spinning up a cluster\n    if (regionServerServices == null) return;\n    List<Region> onlineRegions = regionServerServices.getOnlineRegions();\n    if (onlineRegions != null) {\n      for (Region region : onlineRegions) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\n              \"Started the compacted hfiles cleaner for the region \" + region.getRegionInfo());\n        }\n        for (Store store : region.getStores()) {\n          try {\n            if (useExecutor && regionServerServices != null) {\n              CompactedHFilesDischargeHandler handler = new CompactedHFilesDischargeHandler(\n                  (Server) regionServerServices, EventType.RS_COMPACTED_FILES_DISCHARGER,\n                  (HStore) store);\n              regionServerServices.getExecutorService().submit(handler);\n            } else {\n              // call synchronously if the RegionServerServices are not\n              // available\n              store.closeAndArchiveCompactedFiles();\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Completed archiving the compacted files for the region \"\n                  + region.getRegionInfo() + \" under the store \" + store.getColumnFamilyName());\n            }\n          } catch (Exception e) {\n            LOG.error(\"Exception while trying to close and archive the compacted store \"\n                + \"files of the store  \" + store.getColumnFamilyName() + \" in the\" + \" region \"\n                + region.getRegionInfo(), e);\n          }\n        }\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\n              \"Completed the compacted hfiles cleaner for the region \" + region.getRegionInfo());\n        }\n      }\n    }\n  }"
        ],
        [
            "HBaseFsck::offlineReferenceFileRepair()",
            "1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106 -\n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  ",
            "  /**\n   * Scan all the store file names to find any lingering reference files,\n   * which refer to some none-exiting files. If \"fix\" option is enabled,\n   * any lingering reference file will be sidelined if found.\n   * <p>\n   * Lingering reference file prevents a region from opening. It has to\n   * be fixed before a cluster can start properly.\n   */\n  private void offlineReferenceFileRepair() throws IOException, InterruptedException {\n    clearState();\n    Configuration conf = getConf();\n    Path hbaseRoot = FSUtils.getRootDir(conf);\n    FileSystem fs = hbaseRoot.getFileSystem(conf);\n    LOG.info(\"Computing mapping of all store files\");\n    Map<String, Path> allFiles = FSUtils.getTableStoreFilePathMap(fs, hbaseRoot,\n      new FSUtils.ReferenceFileFilter(fs), executor, errors);\n    errors.print(\"\");\n    LOG.info(\"Validating mapping using HDFS state\");\n    for (Path path: allFiles.values()) {\n      Path referredToFile = StoreFileInfo.getReferredToFile(path);\n      if (fs.exists(referredToFile)) continue;  // good, expected\n\n      // Found a lingering reference file\n      errors.reportError(ERROR_CODE.LINGERING_REFERENCE_HFILE,\n        \"Found lingering reference file \" + path);\n      if (!shouldFixReferenceFiles()) continue;\n\n      // Now, trying to fix it since requested\n      boolean success = false;\n      String pathStr = path.toString();\n\n      // A reference file path should be like\n      // ${hbase.rootdir}/data/namespace/table_name/region_id/family_name/referred_file.region_name\n      // Up 5 directories to get the root folder.\n      // So the file will be sidelined to a similar folder structure.\n      int index = pathStr.lastIndexOf(Path.SEPARATOR_CHAR);\n      for (int i = 0; index > 0 && i < 5; i++) {\n        index = pathStr.lastIndexOf(Path.SEPARATOR_CHAR, index - 1);\n      }\n      if (index > 0) {\n        Path rootDir = getSidelineDir();\n        Path dst = new Path(rootDir, pathStr.substring(index + 1));\n        fs.mkdirs(dst.getParent());\n        LOG.info(\"Trying to sildeline reference file \"\n          + path + \" to \" + dst);\n        setShouldRerun();\n\n        success = fs.rename(path, dst);\n      }\n      if (!success) {\n        LOG.error(\"Failed to sideline reference file \" + path);\n      }\n    }\n  }",
            "1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106 +\n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  ",
            "  /**\n   * Scan all the store file names to find any lingering reference files,\n   * which refer to some none-exiting files. If \"fix\" option is enabled,\n   * any lingering reference file will be sidelined if found.\n   * <p>\n   * Lingering reference file prevents a region from opening. It has to\n   * be fixed before a cluster can start properly.\n   */\n  private void offlineReferenceFileRepair() throws IOException, InterruptedException {\n    clearState();\n    Configuration conf = getConf();\n    Path hbaseRoot = FSUtils.getRootDir(conf);\n    FileSystem fs = hbaseRoot.getFileSystem(conf);\n    LOG.info(\"Computing mapping of all store files\");\n    Map<String, Path> allFiles = FSUtils.getTableStoreFilePathMap(fs, hbaseRoot,\n      new FSUtils.ReferenceFileFilter(fs), executor, errors);\n    errors.print(\"\");\n    LOG.info(\"Validating mapping using HDFS state\");\n    for (Path path: allFiles.values()) {\n      Path referredToFile = StoreFileInfo.getReferredToFile(path);\n      if (fs.exists(referredToFile)) continue;  // good, expected\n\n      // Found a lingering reference file\n      errors.reportError(ERROR_CODE.LINGERING_REFERENCE_HFILE,\n        \"Found lingering reference file \" + path);\n      if (!shouldFixReferenceFiles()) continue;\n\n      // Now, trying to fix it since requested\n      boolean success = false;\n      String pathStr = path.toString();\n\n      // A reference file path should be like\n      // ${hbase.rootdir}/data/namespace/table_name/region_id/family_name/referred_file.region_name\n      // Up 5 directories to get the root folder.\n      // So the file will be sidelined to a similar folder structure.\n      int index = pathStr.lastIndexOf(Path.SEPARATOR_CHAR);\n      for (int i = 0; index > 0 && i < 5; i++) {\n        index = pathStr.lastIndexOf(Path.SEPARATOR_CHAR, index - 1);\n      }\n      if (index > 0) {\n        Path rootDir = getSidelineDir();\n        Path dst = new Path(rootDir, pathStr.substring(index + 1));\n        fs.mkdirs(dst.getParent());\n        LOG.info(\"Trying to sideline reference file \"\n          + path + \" to \" + dst);\n        setShouldRerun();\n\n        success = fs.rename(path, dst);\n      }\n      if (!success) {\n        LOG.error(\"Failed to sideline reference file \" + path);\n      }\n    }\n  }"
        ],
        [
            "MasterMobCompactionThread::requestMobCompaction(Configuration,FileSystem,TableName,List,TableLockManager,boolean)",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  /**\n   * Requests mob compaction\n   * @param conf The Configuration\n   * @param fs The file system\n   * @param tableName The table the compact\n   * @param columns The column descriptors\n   * @param tableLockManager The tableLock manager\n   * @param allFiles Whether add all mob files into the compaction.\n   */\n  public void requestMobCompaction(Configuration conf, FileSystem fs, TableName tableName,\n    List<HColumnDescriptor> columns, TableLockManager tableLockManager, boolean allFiles)\n    throws IOException {\n    master.reportMobCompactionStart(tableName);\n    try {\n      masterMobPool.execute(new CompactionRunner(fs, tableName, columns, tableLockManager,\n        allFiles, mobCompactorPool));\n    } catch (RejectedExecutionException e) {\n      // in case the request is rejected by the pool\n      try {\n        master.reportMobCompactionEnd(tableName);\n      } catch (IOException e1) {\n        LOG.error(\"Failed to mark end of mob compation\", e1);\n      }\n      throw e;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"The mob compaction is requested for the columns \" + columns\n        + \" of the table \" + tableName.getNameAsString());\n    }\n  }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  /**\n   * Requests mob compaction\n   * @param conf The Configuration\n   * @param fs The file system\n   * @param tableName The table the compact\n   * @param columns The column descriptors\n   * @param tableLockManager The tableLock manager\n   * @param allFiles Whether add all mob files into the compaction.\n   */\n  public void requestMobCompaction(Configuration conf, FileSystem fs, TableName tableName,\n    List<HColumnDescriptor> columns, TableLockManager tableLockManager, boolean allFiles)\n    throws IOException {\n    master.reportMobCompactionStart(tableName);\n    try {\n      masterMobPool.execute(new CompactionRunner(fs, tableName, columns, tableLockManager,\n        allFiles, mobCompactorPool));\n    } catch (RejectedExecutionException e) {\n      // in case the request is rejected by the pool\n      try {\n        master.reportMobCompactionEnd(tableName);\n      } catch (IOException e1) {\n        LOG.error(\"Failed to mark end of mob compaction\", e1);\n      }\n      throw e;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"The mob compaction is requested for the columns \" + columns\n        + \" of the table \" + tableName.getNameAsString());\n    }\n  }"
        ],
        [
            "HFileCorruptionChecker::checkTableDir(Path)",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 -\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "  /**\n   * Check all the regiondirs in the specified tableDir\n   *\n   * @param tableDir\n   *          path to a table\n   * @throws IOException\n   */\n  void checkTableDir(Path tableDir) throws IOException {\n    List<FileStatus> rds = FSUtils.listStatusWithStatusFilter(fs, tableDir, new RegionDirFilter(fs));\n    if (rds == null) {\n      if (!fs.exists(tableDir)) {\n        LOG.warn(\"Table Directory \" + tableDir +\n            \" does not exist.  Likely due to concurrent delete. Skipping.\");\n        missing.add(tableDir);\n      }\n      return;\n    }\n\n    // Parallelize check at the region dir level\n    List<RegionDirChecker> rdcs = new ArrayList<RegionDirChecker>();\n    List<Future<Void>> rdFutures;\n\n    for (FileStatus rdFs : rds) {\n      Path rdDir = rdFs.getPath();\n      RegionDirChecker work = new RegionDirChecker(rdDir);\n      rdcs.add(work);\n    }\n\n    // add mob region\n    rdcs.add(createMobRegionDirChecker(tableDir));\n    // Submit and wait for completion\n    try {\n      rdFutures = executor.invokeAll(rdcs);\n    } catch (InterruptedException ie) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"Region dirs checking interrupted!\", ie);\n      return;\n    }\n\n    for (int i = 0; i < rdFutures.size(); i++) {\n      Future<Void> f = rdFutures.get(i);\n      try {\n        f.get();\n      } catch (ExecutionException e) {\n        LOG.warn(\"Failed to quaratine an HFile in regiondir \"\n            + rdcs.get(i).regionDir, e.getCause());\n        // rethrow IOExceptions\n        if (e.getCause() instanceof IOException) {\n          throw (IOException) e.getCause();\n        }\n\n        // rethrow RuntimeExceptions\n        if (e.getCause() instanceof RuntimeException) {\n          throw (RuntimeException) e.getCause();\n        }\n\n        // this should never happen\n        LOG.error(\"Unexpected exception encountered\", e);\n        return; // bailing out.\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"Region dirs check interrupted!\", ie);\n        // bailing out\n        return;\n      }\n    }\n  }",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 +\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "  /**\n   * Check all the regiondirs in the specified tableDir\n   *\n   * @param tableDir\n   *          path to a table\n   * @throws IOException\n   */\n  void checkTableDir(Path tableDir) throws IOException {\n    List<FileStatus> rds = FSUtils.listStatusWithStatusFilter(fs, tableDir, new RegionDirFilter(fs));\n    if (rds == null) {\n      if (!fs.exists(tableDir)) {\n        LOG.warn(\"Table Directory \" + tableDir +\n            \" does not exist.  Likely due to concurrent delete. Skipping.\");\n        missing.add(tableDir);\n      }\n      return;\n    }\n\n    // Parallelize check at the region dir level\n    List<RegionDirChecker> rdcs = new ArrayList<RegionDirChecker>();\n    List<Future<Void>> rdFutures;\n\n    for (FileStatus rdFs : rds) {\n      Path rdDir = rdFs.getPath();\n      RegionDirChecker work = new RegionDirChecker(rdDir);\n      rdcs.add(work);\n    }\n\n    // add mob region\n    rdcs.add(createMobRegionDirChecker(tableDir));\n    // Submit and wait for completion\n    try {\n      rdFutures = executor.invokeAll(rdcs);\n    } catch (InterruptedException ie) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"Region dirs checking interrupted!\", ie);\n      return;\n    }\n\n    for (int i = 0; i < rdFutures.size(); i++) {\n      Future<Void> f = rdFutures.get(i);\n      try {\n        f.get();\n      } catch (ExecutionException e) {\n        LOG.warn(\"Failed to quarantine an HFile in regiondir \"\n            + rdcs.get(i).regionDir, e.getCause());\n        // rethrow IOExceptions\n        if (e.getCause() instanceof IOException) {\n          throw (IOException) e.getCause();\n        }\n\n        // rethrow RuntimeExceptions\n        if (e.getCause() instanceof RuntimeException) {\n          throw (RuntimeException) e.getCause();\n        }\n\n        // this should never happen\n        LOG.error(\"Unexpected exception encountered\", e);\n        return; // bailing out.\n      } catch (InterruptedException ie) {\n        Thread.currentThread().interrupt();\n        LOG.warn(\"Region dirs check interrupted!\", ie);\n        // bailing out\n        return;\n      }\n    }\n  }"
        ],
        [
            "ZNodeClearer::parseMasterServerName(String)",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  \n 144  ",
            "   * See HBASE-14861. We are extracting master ServerName from rsZnodePath\n   * example: \"/hbase/rs/server.example.com,16020,1448266496481\"\n   * @param rsZnodePath from HBASE_ZNODE_FILE\n   * @return String representation of ServerName or null if fails\n   */\n  \n  public static String parseMasterServerName(String rsZnodePath) {\n    String masterServerName = null;\n    try {\n      String[] rsZnodeParts = rsZnodePath.split(\"/\");\n      masterServerName = rsZnodeParts[rsZnodeParts.length -1];\n    } catch (IndexOutOfBoundsException e) {\n      LOG.warn(\"String \" + rsZnodePath + \" has wrong fromat\", e);\n    }\n    return masterServerName; \n  }",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  ",
            "   * See HBASE-14861. We are extracting master ServerName from rsZnodePath\n   * example: \"/hbase/rs/server.example.com,16020,1448266496481\"\n   * @param rsZnodePath from HBASE_ZNODE_FILE\n   * @return String representation of ServerName or null if fails\n   */\n  \n  public static String parseMasterServerName(String rsZnodePath) {\n    String masterServerName = null;\n    try {\n      String[] rsZnodeParts = rsZnodePath.split(\"/\");\n      masterServerName = rsZnodeParts[rsZnodeParts.length -1];\n    } catch (IndexOutOfBoundsException e) {\n      LOG.warn(\"String \" + rsZnodePath + \" has wrong format\", e);\n    }\n    return masterServerName; \n  }"
        ],
        [
            "ZKProcedureMemberRpcs::startNewSubprocedure(String)",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  /**\n   * Kick off a new sub-procedure on the listener with the data stored in the passed znode.\n   * <p>\n   * Will attempt to create the same procedure multiple times if an procedure znode with the same\n   * name is created. It is left up the coordinator to ensure this doesn't occur.\n   * @param path full path to the znode for the procedure to start\n   */\n  private synchronized void startNewSubprocedure(String path) {\n    LOG.debug(\"Found procedure znode: \" + path);\n    String opName = ZKUtil.getNodeName(path);\n    // start watching for an abort notification for the procedure\n    String abortZNode = zkController.getAbortZNode(opName);\n    try {\n      if (ZKUtil.watchAndCheckExists(zkController.getWatcher(), abortZNode)) {\n        LOG.debug(\"Not starting:\" + opName + \" because we already have an abort notification.\");\n        return;\n      }\n    } catch (KeeperException e) {\n      member.controllerConnectionFailure(\"Failed to get the abort znode (\" + abortZNode\n          + \") for procedure :\" + opName, e, opName);\n      return;\n    }\n\n    // get the data for the procedure\n    Subprocedure subproc = null;\n    try {\n      byte[] data = ZKUtil.getData(zkController.getWatcher(), path);\n      if (!ProtobufUtil.isPBMagicPrefix(data)) {\n        String msg = \"Data in for starting procuedure \" + opName +\n          \" is illegally formatted (no pb magic). \" +\n          \"Killing the procedure: \" + Bytes.toString(data);\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      LOG.debug(\"start proc data length is \" + data.length);\n      data = Arrays.copyOfRange(data, ProtobufUtil.lengthOfPBMagic(), data.length);\n      LOG.debug(\"Found data for znode:\" + path);\n      subproc = member.createSubprocedure(opName, data);\n      member.submitSubprocedure(subproc);\n    } catch (IllegalArgumentException iae ) {\n      LOG.error(\"Illegal argument exception\", iae);\n      sendMemberAborted(subproc, new ForeignException(getMemberName(), iae));\n    } catch (IllegalStateException ise) {\n      LOG.error(\"Illegal state exception \", ise);\n      sendMemberAborted(subproc, new ForeignException(getMemberName(), ise));\n    } catch (KeeperException e) {\n      member.controllerConnectionFailure(\"Failed to get data for new procedure:\" + opName,\n        e, opName);\n    } catch (InterruptedException e) {\n      member.controllerConnectionFailure(\"Failed to get data for new procedure:\" + opName,\n        e, opName);\n      Thread.currentThread().interrupt();\n    }\n  }",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  /**\n   * Kick off a new sub-procedure on the listener with the data stored in the passed znode.\n   * <p>\n   * Will attempt to create the same procedure multiple times if an procedure znode with the same\n   * name is created. It is left up the coordinator to ensure this doesn't occur.\n   * @param path full path to the znode for the procedure to start\n   */\n  private synchronized void startNewSubprocedure(String path) {\n    LOG.debug(\"Found procedure znode: \" + path);\n    String opName = ZKUtil.getNodeName(path);\n    // start watching for an abort notification for the procedure\n    String abortZNode = zkController.getAbortZNode(opName);\n    try {\n      if (ZKUtil.watchAndCheckExists(zkController.getWatcher(), abortZNode)) {\n        LOG.debug(\"Not starting:\" + opName + \" because we already have an abort notification.\");\n        return;\n      }\n    } catch (KeeperException e) {\n      member.controllerConnectionFailure(\"Failed to get the abort znode (\" + abortZNode\n          + \") for procedure :\" + opName, e, opName);\n      return;\n    }\n\n    // get the data for the procedure\n    Subprocedure subproc = null;\n    try {\n      byte[] data = ZKUtil.getData(zkController.getWatcher(), path);\n      if (!ProtobufUtil.isPBMagicPrefix(data)) {\n        String msg = \"Data in for starting procedure \" + opName +\n          \" is illegally formatted (no pb magic). \" +\n          \"Killing the procedure: \" + Bytes.toString(data);\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      LOG.debug(\"start proc data length is \" + data.length);\n      data = Arrays.copyOfRange(data, ProtobufUtil.lengthOfPBMagic(), data.length);\n      LOG.debug(\"Found data for znode:\" + path);\n      subproc = member.createSubprocedure(opName, data);\n      member.submitSubprocedure(subproc);\n    } catch (IllegalArgumentException iae ) {\n      LOG.error(\"Illegal argument exception\", iae);\n      sendMemberAborted(subproc, new ForeignException(getMemberName(), iae));\n    } catch (IllegalStateException ise) {\n      LOG.error(\"Illegal state exception \", ise);\n      sendMemberAborted(subproc, new ForeignException(getMemberName(), ise));\n    } catch (KeeperException e) {\n      member.controllerConnectionFailure(\"Failed to get data for new procedure:\" + opName,\n        e, opName);\n    } catch (InterruptedException e) {\n      member.controllerConnectionFailure(\"Failed to get data for new procedure:\" + opName,\n        e, opName);\n      Thread.currentThread().interrupt();\n    }\n  }"
        ],
        [
            "TableAuthManager::updateGlobalCache(ListMultimap)",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 -\n 229  \n 230  ",
            "  /**\n   * Updates the internal global permissions cache\n   *\n   * @param userPerms\n   */\n  private void updateGlobalCache(ListMultimap<String,TablePermission> userPerms) {\n    PermissionCache<Permission> newCache = null;\n    try {\n      newCache = initGlobal(conf);\n      for (Map.Entry<String,TablePermission> entry : userPerms.entries()) {\n        if (AuthUtil.isGroupPrincipal(entry.getKey())) {\n          newCache.putGroup(AuthUtil.getGroupName(entry.getKey()),\n              new Permission(entry.getValue().getActions()));\n        } else {\n          newCache.putUser(entry.getKey(), new Permission(entry.getValue().getActions()));\n        }\n      }\n      globalCache = newCache;\n      mtime.incrementAndGet();\n    } catch (IOException e) {\n      // Never happens\n      LOG.error(\"Error occured while updating the global cache\", e);\n    }\n  }",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  ",
            "  /**\n   * Updates the internal global permissions cache\n   *\n   * @param userPerms\n   */\n  private void updateGlobalCache(ListMultimap<String,TablePermission> userPerms) {\n    PermissionCache<Permission> newCache = null;\n    try {\n      newCache = initGlobal(conf);\n      for (Map.Entry<String,TablePermission> entry : userPerms.entries()) {\n        if (AuthUtil.isGroupPrincipal(entry.getKey())) {\n          newCache.putGroup(AuthUtil.getGroupName(entry.getKey()),\n              new Permission(entry.getValue().getActions()));\n        } else {\n          newCache.putUser(entry.getKey(), new Permission(entry.getValue().getActions()));\n        }\n      }\n      globalCache = newCache;\n      mtime.incrementAndGet();\n    } catch (IOException e) {\n      // Never happens\n      LOG.error(\"Error occurred while updating the global cache\", e);\n    }\n  }"
        ],
        [
            "ReplicationAdmin::checkAndSyncTableDescToPeers(TableName,byte)",
            " 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554 -\n 555  \n 556  \n 557  \n 558  \n 559  \n 560  ",
            "  /**\n   * Connect to peer and check the table descriptor on peer:\n   * <ol>\n   * <li>Create the same table on peer when not exist.</li>\n   * <li>Throw exception if the table exists on peer cluster but descriptors are not same.</li>\n   * </ol>\n   * @param tableName name of the table to sync to the peer\n   * @param splits table split keys\n   * @throws IOException\n   */\n  private void checkAndSyncTableDescToPeers(final TableName tableName, final byte[][] splits)\n      throws IOException {\n    List<ReplicationPeer> repPeers = listReplicationPeers();\n    if (repPeers == null || repPeers.size() <= 0) {\n      throw new IllegalArgumentException(\"Found no peer cluster for replication.\");\n    }\n\n    final TableName onlyTableNameQualifier = TableName.valueOf(tableName.getQualifierAsString());\n\n    for (ReplicationPeer repPeer : repPeers) {\n      Map<TableName, List<String>> tableCFMap = repPeer.getTableCFs();\n      // TODO Currently peer TableCFs will not include namespace so we need to check only for table\n      // name without namespace in it. Need to correct this logic once we fix HBASE-11386.\n      if (tableCFMap != null && !tableCFMap.containsKey(onlyTableNameQualifier)) {\n        continue;\n      }\n\n      Configuration peerConf = repPeer.getConfiguration();\n      HTableDescriptor htd = null;\n      try (Connection conn = ConnectionFactory.createConnection(peerConf);\n          Admin admin = this.connection.getAdmin();\n          Admin repHBaseAdmin = conn.getAdmin()) {\n        htd = admin.getTableDescriptor(tableName);\n        HTableDescriptor peerHtd = null;\n        if (!repHBaseAdmin.tableExists(tableName)) {\n          repHBaseAdmin.createTable(htd, splits);\n        } else {\n          peerHtd = repHBaseAdmin.getTableDescriptor(tableName);\n          if (peerHtd == null) {\n            throw new IllegalArgumentException(\"Failed to get table descriptor for table \"\n                + tableName.getNameAsString() + \" from peer cluster \" + repPeer.getId());\n          } else if (!peerHtd.equals(htd)) {\n            throw new IllegalArgumentException(\"Table \" + tableName.getNameAsString()\n                + \" exists in peer cluster \" + repPeer.getId()\n                + \", but the table descriptors are not same when comapred with source cluster.\"\n                + \" Thus can not enable the table's replication switch.\");\n          }\n        }\n      }\n    }\n  }",
            " 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554 +\n 555  \n 556  \n 557  \n 558  \n 559  \n 560  ",
            "  /**\n   * Connect to peer and check the table descriptor on peer:\n   * <ol>\n   * <li>Create the same table on peer when not exist.</li>\n   * <li>Throw exception if the table exists on peer cluster but descriptors are not same.</li>\n   * </ol>\n   * @param tableName name of the table to sync to the peer\n   * @param splits table split keys\n   * @throws IOException\n   */\n  private void checkAndSyncTableDescToPeers(final TableName tableName, final byte[][] splits)\n      throws IOException {\n    List<ReplicationPeer> repPeers = listReplicationPeers();\n    if (repPeers == null || repPeers.size() <= 0) {\n      throw new IllegalArgumentException(\"Found no peer cluster for replication.\");\n    }\n\n    final TableName onlyTableNameQualifier = TableName.valueOf(tableName.getQualifierAsString());\n\n    for (ReplicationPeer repPeer : repPeers) {\n      Map<TableName, List<String>> tableCFMap = repPeer.getTableCFs();\n      // TODO Currently peer TableCFs will not include namespace so we need to check only for table\n      // name without namespace in it. Need to correct this logic once we fix HBASE-11386.\n      if (tableCFMap != null && !tableCFMap.containsKey(onlyTableNameQualifier)) {\n        continue;\n      }\n\n      Configuration peerConf = repPeer.getConfiguration();\n      HTableDescriptor htd = null;\n      try (Connection conn = ConnectionFactory.createConnection(peerConf);\n          Admin admin = this.connection.getAdmin();\n          Admin repHBaseAdmin = conn.getAdmin()) {\n        htd = admin.getTableDescriptor(tableName);\n        HTableDescriptor peerHtd = null;\n        if (!repHBaseAdmin.tableExists(tableName)) {\n          repHBaseAdmin.createTable(htd, splits);\n        } else {\n          peerHtd = repHBaseAdmin.getTableDescriptor(tableName);\n          if (peerHtd == null) {\n            throw new IllegalArgumentException(\"Failed to get table descriptor for table \"\n                + tableName.getNameAsString() + \" from peer cluster \" + repPeer.getId());\n          } else if (!peerHtd.equals(htd)) {\n            throw new IllegalArgumentException(\"Table \" + tableName.getNameAsString()\n                + \" exists in peer cluster \" + repPeer.getId()\n                + \", but the table descriptors are not same when compared with source cluster.\"\n                + \" Thus can not enable the table's replication switch.\");\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "DeleteTableProcedure::cleanAnyRemainingRows(MasterProcedureEnv,TableName)",
            " 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 -\n 353  \n 354  \n 355  \n 356  \n 357  ",
            "  /**\n   * There may be items for this table still up in hbase:meta in the case where the\n   * info:regioninfo column was empty because of some write error. Remove ALL rows from hbase:meta\n   * that have to do with this table. See HBASE-12980.\n   * @throws IOException\n   */\n  private static void cleanAnyRemainingRows(final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    Connection connection = env.getMasterServices().getConnection();\n    Scan tableScan = MetaTableAccessor.getScanForTableName(connection, tableName);\n    try (Table metaTable =\n        connection.getTable(TableName.META_TABLE_NAME)) {\n      List<Delete> deletes = new ArrayList<Delete>();\n      try (ResultScanner resScanner = metaTable.getScanner(tableScan)) {\n        for (Result result : resScanner) {\n          deletes.add(new Delete(result.getRow()));\n        }\n      }\n      if (!deletes.isEmpty()) {\n        LOG.warn(\"Deleting some vestigal \" + deletes.size() + \" rows of \" + tableName +\n          \" from \" + TableName.META_TABLE_NAME);\n        metaTable.delete(deletes);\n      }\n    }\n  }",
            " 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 +\n 353  \n 354  \n 355  \n 356  \n 357  ",
            "  /**\n   * There may be items for this table still up in hbase:meta in the case where the\n   * info:regioninfo column was empty because of some write error. Remove ALL rows from hbase:meta\n   * that have to do with this table. See HBASE-12980.\n   * @throws IOException\n   */\n  private static void cleanAnyRemainingRows(final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    Connection connection = env.getMasterServices().getConnection();\n    Scan tableScan = MetaTableAccessor.getScanForTableName(connection, tableName);\n    try (Table metaTable =\n        connection.getTable(TableName.META_TABLE_NAME)) {\n      List<Delete> deletes = new ArrayList<Delete>();\n      try (ResultScanner resScanner = metaTable.getScanner(tableScan)) {\n        for (Result result : resScanner) {\n          deletes.add(new Delete(result.getRow()));\n        }\n      }\n      if (!deletes.isEmpty()) {\n        LOG.warn(\"Deleting some vestigial \" + deletes.size() + \" rows of \" + tableName +\n          \" from \" + TableName.META_TABLE_NAME);\n        metaTable.delete(deletes);\n      }\n    }\n  }"
        ],
        [
            "HeapMemoryManager::stop()",
            " 207  \n 208  \n 209 -\n 210  \n 211  ",
            "  public void stop() {\n    // The thread is Daemon. Just interrupting the ongoing process.\n    LOG.info(\"Stoping HeapMemoryTuner chore.\");\n    this.heapMemTunerChore.cancel(true);\n  }",
            " 207  \n 208  \n 209 +\n 210  \n 211  ",
            "  public void stop() {\n    // The thread is Daemon. Just interrupting the ongoing process.\n    LOG.info(\"Stopping HeapMemoryTuner chore.\");\n    this.heapMemTunerChore.cancel(true);\n  }"
        ],
        [
            "HColumnDescriptor::setDFSReplication(short)",
            "1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252 -\n1253  \n1254  \n1255  \n1256  ",
            "  /**\n   * Set the replication factor to hfile(s) belonging to this family\n   * @param replication number of replicas the blocks(s) belonging to this CF should have, or\n   *          {@link #DEFAULT_DFS_REPLICATION} for the default replication factor set in the\n   *          filesystem\n   * @return this (for chained invocation)\n   */\n  public HColumnDescriptor setDFSReplication(short replication) {\n    if (replication < 1 && replication != DEFAULT_DFS_REPLICATION) {\n      throw new IllegalArgumentException(\n          \"DFS replication factor cannot be less than 1 if explictly set.\");\n    }\n    setValue(DFS_REPLICATION, Short.toString(replication));\n    return this;\n  }",
            "1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252 +\n1253  \n1254  \n1255  \n1256  ",
            "  /**\n   * Set the replication factor to hfile(s) belonging to this family\n   * @param replication number of replicas the blocks(s) belonging to this CF should have, or\n   *          {@link #DEFAULT_DFS_REPLICATION} for the default replication factor set in the\n   *          filesystem\n   * @return this (for chained invocation)\n   */\n  public HColumnDescriptor setDFSReplication(short replication) {\n    if (replication < 1 && replication != DEFAULT_DFS_REPLICATION) {\n      throw new IllegalArgumentException(\n          \"DFS replication factor cannot be less than 1 if explicitly set.\");\n    }\n    setValue(DFS_REPLICATION, Short.toString(replication));\n    return this;\n  }"
        ],
        [
            "BucketCache::checkIOErrorIsTolerated()",
            "1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027 -\n1028  \n1029  \n1030  \n1031  \n1032  \n1033  ",
            "  /**\n   * Check whether we tolerate IO error this time. If the duration of IOEngine\n   * throwing errors exceeds ioErrorsDurationTimeTolerated, we will disable the\n   * cache\n   */\n  private void checkIOErrorIsTolerated() {\n    long now = EnvironmentEdgeManager.currentTime();\n    if (this.ioErrorStartTime > 0) {\n      if (cacheEnabled && (now - ioErrorStartTime) > this.ioErrorsTolerationDuration) {\n        LOG.error(\"IO errors duration time has exceeded \" + ioErrorsTolerationDuration +\n          \"ms, disabing cache, please check your IOEngine\");\n        disableCache();\n      }\n    } else {\n      this.ioErrorStartTime = now;\n    }\n  }",
            "1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027 +\n1028  \n1029  \n1030  \n1031  \n1032  \n1033  ",
            "  /**\n   * Check whether we tolerate IO error this time. If the duration of IOEngine\n   * throwing errors exceeds ioErrorsDurationTimeTolerated, we will disable the\n   * cache\n   */\n  private void checkIOErrorIsTolerated() {\n    long now = EnvironmentEdgeManager.currentTime();\n    if (this.ioErrorStartTime > 0) {\n      if (cacheEnabled && (now - ioErrorStartTime) > this.ioErrorsTolerationDuration) {\n        LOG.error(\"IO errors duration time has exceeded \" + ioErrorsTolerationDuration +\n          \"ms, disabling cache, please check your IOEngine\");\n        disableCache();\n      }\n    } else {\n      this.ioErrorStartTime = now;\n    }\n  }"
        ],
        [
            "ReplicationPeerZKImpl::abort(String,Throwable)",
            " 183  \n 184  \n 185 -\n 186  \n 187  ",
            "  @Override\n  public void abort(String why, Throwable e) {\n    LOG.fatal(\"The ReplicationPeer coresponding to peer \" + peerConfig\n        + \" was aborted for the following reason(s):\" + why, e);\n  }",
            " 183  \n 184  \n 185 +\n 186  \n 187  ",
            "  @Override\n  public void abort(String why, Throwable e) {\n    LOG.fatal(\"The ReplicationPeer corresponding to peer \" + peerConfig\n        + \" was aborted for the following reason(s):\" + why, e);\n  }"
        ],
        [
            "RegionPlacementMaintainer::main(String)",
            " 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 -\n 980 -\n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  ",
            "  public static void main(String args[]) throws IOException {\n    Options opt = new Options();\n    opt.addOption(\"w\", \"write\", false, \"write the assignments to hbase:meta only\");\n    opt.addOption(\"u\", \"update\", false,\n        \"update the assignments to hbase:meta and RegionServers together\");\n    opt.addOption(\"n\", \"dry-run\", false, \"do not write assignments to META\");\n    opt.addOption(\"v\", \"verify\", false, \"verify current assignments against META\");\n    opt.addOption(\"p\", \"print\", false, \"print the current assignment plan in META\");\n    opt.addOption(\"h\", \"help\", false, \"print usage\");\n    opt.addOption(\"d\", \"verification-details\", false,\n        \"print the details of verification report\");\n\n    opt.addOption(\"zk\", true, \"to set the zookeeper quorum\");\n    opt.addOption(\"fs\", true, \"to set HDFS\");\n    opt.addOption(\"hbase_root\", true, \"to set hbase_root directory\");\n\n    opt.addOption(\"overwrite\", false,\n        \"overwrite the favored nodes for a single region,\" +\n        \"for example: -update -r regionName -f server1:port,server2:port,server3:port\");\n    opt.addOption(\"r\", true, \"The region name that needs to be updated\");\n    opt.addOption(\"f\", true, \"The new favored nodes\");\n\n    opt.addOption(\"tables\", true,\n        \"The list of table names splitted by ',' ;\" +\n        \"For example: -tables: t1,t2,...,tn\");\n    opt.addOption(\"l\", \"locality\", true, \"enforce the maxium locality\");\n    opt.addOption(\"m\", \"min-move\", true, \"enforce minium assignment move\");\n    opt.addOption(\"diff\", false, \"calculate difference between assignment plans\");\n    opt.addOption(\"munkres\", false,\n        \"use munkres to place secondaries and tertiaries\");\n    opt.addOption(\"ld\", \"locality-dispersion\", false, \"print locality and dispersion \" +\n        \"information for current plan\");\n    try {\n      // Set the log4j\n      Logger.getLogger(\"org.apache.zookeeper\").setLevel(Level.ERROR);\n      Logger.getLogger(\"org.apache.hadoop.hbase\").setLevel(Level.ERROR);\n      Logger.getLogger(\"org.apache.hadoop.hbase.master.RegionPlacementMaintainer\")\n      .setLevel(Level.INFO);\n\n      CommandLine cmd = new GnuParser().parse(opt, args);\n      Configuration conf = HBaseConfiguration.create();\n\n      boolean enforceMinAssignmentMove = true;\n      boolean enforceLocality = true;\n      boolean verificationDetails = false;\n\n      // Read all the options\n      if ((cmd.hasOption(\"l\") &&\n          cmd.getOptionValue(\"l\").equalsIgnoreCase(\"false\")) ||\n          (cmd.hasOption(\"locality\") &&\n              cmd.getOptionValue(\"locality\").equalsIgnoreCase(\"false\"))) {\n        enforceLocality = false;\n      }\n\n      if ((cmd.hasOption(\"m\") &&\n          cmd.getOptionValue(\"m\").equalsIgnoreCase(\"false\")) ||\n          (cmd.hasOption(\"min-move\") &&\n              cmd.getOptionValue(\"min-move\").equalsIgnoreCase(\"false\"))) {\n        enforceMinAssignmentMove = false;\n      }\n\n      if (cmd.hasOption(\"zk\")) {\n        conf.set(HConstants.ZOOKEEPER_QUORUM, cmd.getOptionValue(\"zk\"));\n        LOG.info(\"Setting the zk quorum: \" + conf.get(HConstants.ZOOKEEPER_QUORUM));\n      }\n\n      if (cmd.hasOption(\"fs\")) {\n        conf.set(FileSystem.FS_DEFAULT_NAME_KEY, cmd.getOptionValue(\"fs\"));\n        LOG.info(\"Setting the HDFS: \" + conf.get(FileSystem.FS_DEFAULT_NAME_KEY));\n      }\n\n      if (cmd.hasOption(\"hbase_root\")) {\n        conf.set(HConstants.HBASE_DIR, cmd.getOptionValue(\"hbase_root\"));\n        LOG.info(\"Setting the hbase root directory: \" + conf.get(HConstants.HBASE_DIR));\n      }\n\n      // Create the region placement obj\n      RegionPlacementMaintainer rp = new RegionPlacementMaintainer(conf, enforceLocality,\n          enforceMinAssignmentMove);\n\n      if (cmd.hasOption(\"d\") || cmd.hasOption(\"verification-details\")) {\n        verificationDetails = true;\n      }\n\n      if (cmd.hasOption(\"tables\")) {\n        String tableNameListStr = cmd.getOptionValue(\"tables\");\n        String[] tableNames = StringUtils.split(tableNameListStr, \",\");\n        rp.setTargetTableName(tableNames);\n      }\n\n      if (cmd.hasOption(\"munkres\")) {\n        USE_MUNKRES_FOR_PLACING_SECONDARY_AND_TERTIARY = true;\n      }\n\n      // Read all the modes\n      if (cmd.hasOption(\"v\") || cmd.hasOption(\"verify\")) {\n        // Verify the region placement.\n        rp.verifyRegionPlacement(verificationDetails);\n      } else if (cmd.hasOption(\"n\") || cmd.hasOption(\"dry-run\")) {\n        // Generate the assignment plan only without updating the hbase:meta and RS\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        printAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"w\") || cmd.hasOption(\"write\")) {\n        // Generate the new assignment plan\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        // Print the new assignment plan\n        printAssignmentPlan(plan);\n        // Write the new assignment plan to META\n        rp.updateAssignmentPlanToMeta(plan);\n      } else if (cmd.hasOption(\"u\") || cmd.hasOption(\"update\")) {\n        // Generate the new assignment plan\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        // Print the new assignment plan\n        printAssignmentPlan(plan);\n        // Update the assignment to hbase:meta and Region Servers\n        rp.updateAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"diff\")) {\n        FavoredNodesPlan newPlan = rp.getNewAssignmentPlan();\n        Map<String, Map<String, Float>> locality = FSUtils\n            .getRegionDegreeLocalityMappingFromFS(conf);\n        Map<TableName, Integer> movesPerTable = rp.getRegionsMovement(newPlan);\n        rp.checkDifferencesWithOldPlan(movesPerTable, locality, newPlan);\n        System.out.println(\"Do you want to update the assignment plan? [y/n]\");\n        Scanner s = new Scanner(System.in);\n        String input = s.nextLine().trim();\n        if (input.equals(\"y\")) {\n          System.out.println(\"Updating assignment plan...\");\n          rp.updateAssignmentPlan(newPlan);\n        }\n        s.close();\n      } else if (cmd.hasOption(\"ld\")) {\n        Map<String, Map<String, Float>> locality = FSUtils\n            .getRegionDegreeLocalityMappingFromFS(conf);\n        rp.printLocalityAndDispersionForCurrentPlan(locality);\n      } else if (cmd.hasOption(\"p\") || cmd.hasOption(\"print\")) {\n        FavoredNodesPlan plan = rp.getRegionAssignmentSnapshot().getExistingAssignmentPlan();\n        printAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"overwrite\")) {\n        if (!cmd.hasOption(\"f\") || !cmd.hasOption(\"r\")) {\n          throw new IllegalArgumentException(\"Please specify: \" +\n              \" -update -r regionName -f server1:port,server2:port,server3:port\");\n        }\n\n        String regionName = cmd.getOptionValue(\"r\");\n        String favoredNodesStr = cmd.getOptionValue(\"f\");\n        LOG.info(\"Going to update the region \" + regionName + \" with the new favored nodes \" +\n            favoredNodesStr);\n        List<ServerName> favoredNodes = null;\n        HRegionInfo regionInfo =\n            rp.getRegionAssignmentSnapshot().getRegionNameToRegionInfoMap().get(regionName);\n        if (regionInfo == null) {\n          LOG.error(\"Cannot find the region \" + regionName + \" from the META\");\n        } else {\n          try {\n            favoredNodes = getFavoredNodeList(favoredNodesStr);\n          } catch (IllegalArgumentException e) {\n            LOG.error(\"Cannot parse the invalid favored nodes because \" + e);\n          }\n          FavoredNodesPlan newPlan = new FavoredNodesPlan();\n          newPlan.updateFavoredNodesMap(regionInfo, favoredNodes);\n          rp.updateAssignmentPlan(newPlan);\n        }\n      } else {\n        printHelp(opt);\n      }\n    } catch (ParseException e) {\n      printHelp(opt);\n    }\n  }",
            " 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 +\n 980 +\n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  ",
            "  public static void main(String args[]) throws IOException {\n    Options opt = new Options();\n    opt.addOption(\"w\", \"write\", false, \"write the assignments to hbase:meta only\");\n    opt.addOption(\"u\", \"update\", false,\n        \"update the assignments to hbase:meta and RegionServers together\");\n    opt.addOption(\"n\", \"dry-run\", false, \"do not write assignments to META\");\n    opt.addOption(\"v\", \"verify\", false, \"verify current assignments against META\");\n    opt.addOption(\"p\", \"print\", false, \"print the current assignment plan in META\");\n    opt.addOption(\"h\", \"help\", false, \"print usage\");\n    opt.addOption(\"d\", \"verification-details\", false,\n        \"print the details of verification report\");\n\n    opt.addOption(\"zk\", true, \"to set the zookeeper quorum\");\n    opt.addOption(\"fs\", true, \"to set HDFS\");\n    opt.addOption(\"hbase_root\", true, \"to set hbase_root directory\");\n\n    opt.addOption(\"overwrite\", false,\n        \"overwrite the favored nodes for a single region,\" +\n        \"for example: -update -r regionName -f server1:port,server2:port,server3:port\");\n    opt.addOption(\"r\", true, \"The region name that needs to be updated\");\n    opt.addOption(\"f\", true, \"The new favored nodes\");\n\n    opt.addOption(\"tables\", true,\n        \"The list of table names splitted by ',' ;\" +\n        \"For example: -tables: t1,t2,...,tn\");\n    opt.addOption(\"l\", \"locality\", true, \"enforce the maximum locality\");\n    opt.addOption(\"m\", \"min-move\", true, \"enforce minimum assignment move\");\n    opt.addOption(\"diff\", false, \"calculate difference between assignment plans\");\n    opt.addOption(\"munkres\", false,\n        \"use munkres to place secondaries and tertiaries\");\n    opt.addOption(\"ld\", \"locality-dispersion\", false, \"print locality and dispersion \" +\n        \"information for current plan\");\n    try {\n      // Set the log4j\n      Logger.getLogger(\"org.apache.zookeeper\").setLevel(Level.ERROR);\n      Logger.getLogger(\"org.apache.hadoop.hbase\").setLevel(Level.ERROR);\n      Logger.getLogger(\"org.apache.hadoop.hbase.master.RegionPlacementMaintainer\")\n      .setLevel(Level.INFO);\n\n      CommandLine cmd = new GnuParser().parse(opt, args);\n      Configuration conf = HBaseConfiguration.create();\n\n      boolean enforceMinAssignmentMove = true;\n      boolean enforceLocality = true;\n      boolean verificationDetails = false;\n\n      // Read all the options\n      if ((cmd.hasOption(\"l\") &&\n          cmd.getOptionValue(\"l\").equalsIgnoreCase(\"false\")) ||\n          (cmd.hasOption(\"locality\") &&\n              cmd.getOptionValue(\"locality\").equalsIgnoreCase(\"false\"))) {\n        enforceLocality = false;\n      }\n\n      if ((cmd.hasOption(\"m\") &&\n          cmd.getOptionValue(\"m\").equalsIgnoreCase(\"false\")) ||\n          (cmd.hasOption(\"min-move\") &&\n              cmd.getOptionValue(\"min-move\").equalsIgnoreCase(\"false\"))) {\n        enforceMinAssignmentMove = false;\n      }\n\n      if (cmd.hasOption(\"zk\")) {\n        conf.set(HConstants.ZOOKEEPER_QUORUM, cmd.getOptionValue(\"zk\"));\n        LOG.info(\"Setting the zk quorum: \" + conf.get(HConstants.ZOOKEEPER_QUORUM));\n      }\n\n      if (cmd.hasOption(\"fs\")) {\n        conf.set(FileSystem.FS_DEFAULT_NAME_KEY, cmd.getOptionValue(\"fs\"));\n        LOG.info(\"Setting the HDFS: \" + conf.get(FileSystem.FS_DEFAULT_NAME_KEY));\n      }\n\n      if (cmd.hasOption(\"hbase_root\")) {\n        conf.set(HConstants.HBASE_DIR, cmd.getOptionValue(\"hbase_root\"));\n        LOG.info(\"Setting the hbase root directory: \" + conf.get(HConstants.HBASE_DIR));\n      }\n\n      // Create the region placement obj\n      RegionPlacementMaintainer rp = new RegionPlacementMaintainer(conf, enforceLocality,\n          enforceMinAssignmentMove);\n\n      if (cmd.hasOption(\"d\") || cmd.hasOption(\"verification-details\")) {\n        verificationDetails = true;\n      }\n\n      if (cmd.hasOption(\"tables\")) {\n        String tableNameListStr = cmd.getOptionValue(\"tables\");\n        String[] tableNames = StringUtils.split(tableNameListStr, \",\");\n        rp.setTargetTableName(tableNames);\n      }\n\n      if (cmd.hasOption(\"munkres\")) {\n        USE_MUNKRES_FOR_PLACING_SECONDARY_AND_TERTIARY = true;\n      }\n\n      // Read all the modes\n      if (cmd.hasOption(\"v\") || cmd.hasOption(\"verify\")) {\n        // Verify the region placement.\n        rp.verifyRegionPlacement(verificationDetails);\n      } else if (cmd.hasOption(\"n\") || cmd.hasOption(\"dry-run\")) {\n        // Generate the assignment plan only without updating the hbase:meta and RS\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        printAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"w\") || cmd.hasOption(\"write\")) {\n        // Generate the new assignment plan\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        // Print the new assignment plan\n        printAssignmentPlan(plan);\n        // Write the new assignment plan to META\n        rp.updateAssignmentPlanToMeta(plan);\n      } else if (cmd.hasOption(\"u\") || cmd.hasOption(\"update\")) {\n        // Generate the new assignment plan\n        FavoredNodesPlan plan = rp.getNewAssignmentPlan();\n        // Print the new assignment plan\n        printAssignmentPlan(plan);\n        // Update the assignment to hbase:meta and Region Servers\n        rp.updateAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"diff\")) {\n        FavoredNodesPlan newPlan = rp.getNewAssignmentPlan();\n        Map<String, Map<String, Float>> locality = FSUtils\n            .getRegionDegreeLocalityMappingFromFS(conf);\n        Map<TableName, Integer> movesPerTable = rp.getRegionsMovement(newPlan);\n        rp.checkDifferencesWithOldPlan(movesPerTable, locality, newPlan);\n        System.out.println(\"Do you want to update the assignment plan? [y/n]\");\n        Scanner s = new Scanner(System.in);\n        String input = s.nextLine().trim();\n        if (input.equals(\"y\")) {\n          System.out.println(\"Updating assignment plan...\");\n          rp.updateAssignmentPlan(newPlan);\n        }\n        s.close();\n      } else if (cmd.hasOption(\"ld\")) {\n        Map<String, Map<String, Float>> locality = FSUtils\n            .getRegionDegreeLocalityMappingFromFS(conf);\n        rp.printLocalityAndDispersionForCurrentPlan(locality);\n      } else if (cmd.hasOption(\"p\") || cmd.hasOption(\"print\")) {\n        FavoredNodesPlan plan = rp.getRegionAssignmentSnapshot().getExistingAssignmentPlan();\n        printAssignmentPlan(plan);\n      } else if (cmd.hasOption(\"overwrite\")) {\n        if (!cmd.hasOption(\"f\") || !cmd.hasOption(\"r\")) {\n          throw new IllegalArgumentException(\"Please specify: \" +\n              \" -update -r regionName -f server1:port,server2:port,server3:port\");\n        }\n\n        String regionName = cmd.getOptionValue(\"r\");\n        String favoredNodesStr = cmd.getOptionValue(\"f\");\n        LOG.info(\"Going to update the region \" + regionName + \" with the new favored nodes \" +\n            favoredNodesStr);\n        List<ServerName> favoredNodes = null;\n        HRegionInfo regionInfo =\n            rp.getRegionAssignmentSnapshot().getRegionNameToRegionInfoMap().get(regionName);\n        if (regionInfo == null) {\n          LOG.error(\"Cannot find the region \" + regionName + \" from the META\");\n        } else {\n          try {\n            favoredNodes = getFavoredNodeList(favoredNodesStr);\n          } catch (IllegalArgumentException e) {\n            LOG.error(\"Cannot parse the invalid favored nodes because \" + e);\n          }\n          FavoredNodesPlan newPlan = new FavoredNodesPlan();\n          newPlan.updateFavoredNodesMap(regionInfo, favoredNodes);\n          rp.updateAssignmentPlan(newPlan);\n        }\n      } else {\n        printHelp(opt);\n      }\n    } catch (ParseException e) {\n      printHelp(opt);\n    }\n  }"
        ],
        [
            "WALSplitter::EntryBuffers::waitUntilDrained()",
            " 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004 -\n1005  \n1006  \n1007  \n1008  \n1009  \n1010  ",
            "    public void waitUntilDrained() {\n      synchronized (controller.dataAvailable) {\n        while (totalBuffered > 0) {\n          try {\n            controller.dataAvailable.wait(2000);\n          } catch (InterruptedException e) {\n            LOG.warn(\"Got intrerrupted while waiting for EntryBuffers is drained\");\n            Thread.interrupted();\n            break;\n          }\n        }\n      }\n    }",
            " 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004 +\n1005  \n1006  \n1007  \n1008  \n1009  \n1010  ",
            "    public void waitUntilDrained() {\n      synchronized (controller.dataAvailable) {\n        while (totalBuffered > 0) {\n          try {\n            controller.dataAvailable.wait(2000);\n          } catch (InterruptedException e) {\n            LOG.warn(\"Got interrupted while waiting for EntryBuffers is drained\");\n            Thread.interrupted();\n            break;\n          }\n        }\n      }\n    }"
        ],
        [
            "BoundedByteBufferPool::getBuffer()",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  \n 144  ",
            "  public ByteBuffer getBuffer() {\n    ByteBuffer bb = buffers.poll();\n    if (bb != null) {\n      long state;\n      while (true) {\n        long prevState = stateRef.get();\n        state = subtractOneBufferFromState(prevState, bb.capacity());\n        if (stateRef.compareAndSet(prevState, state)) {\n          break;\n        }\n      }\n      // Clear sets limit == capacity. Postion == 0.\n      bb.clear();\n\n      if (LOG.isTraceEnabled()) {\n        int countOfBuffers = toCountOfBuffers(state);\n        int totalCapacity = toTotalCapacity(state);\n        LOG.trace(\"totalCapacity=\" + totalCapacity + \", count=\" + countOfBuffers);\n      }\n      return bb;\n    }\n\n    int runningAverage = runningAverageRef.get();\n    bb = ByteBuffer.allocateDirect(runningAverage);\n\n    if (LOG.isTraceEnabled()) {\n      long allocations = allocationsRef.incrementAndGet();\n      LOG.trace(\"runningAverage=\" + runningAverage + \", alloctions=\" + allocations);\n    }\n    return bb;\n  }",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  ",
            "  public ByteBuffer getBuffer() {\n    ByteBuffer bb = buffers.poll();\n    if (bb != null) {\n      long state;\n      while (true) {\n        long prevState = stateRef.get();\n        state = subtractOneBufferFromState(prevState, bb.capacity());\n        if (stateRef.compareAndSet(prevState, state)) {\n          break;\n        }\n      }\n      // Clear sets limit == capacity. Postion == 0.\n      bb.clear();\n\n      if (LOG.isTraceEnabled()) {\n        int countOfBuffers = toCountOfBuffers(state);\n        int totalCapacity = toTotalCapacity(state);\n        LOG.trace(\"totalCapacity=\" + totalCapacity + \", count=\" + countOfBuffers);\n      }\n      return bb;\n    }\n\n    int runningAverage = runningAverageRef.get();\n    bb = ByteBuffer.allocateDirect(runningAverage);\n\n    if (LOG.isTraceEnabled()) {\n      long allocations = allocationsRef.incrementAndGet();\n      LOG.trace(\"runningAverage=\" + runningAverage + \", allocations=\" + allocations);\n    }\n    return bb;\n  }"
        ],
        [
            "LoadIncrementalHFiles::performBulkLoad(Admin,Table,RegionLocator,Deque,ExecutorService,SecureBulkLoadClient,boolean)",
            " 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476 -\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  ",
            "  Map<LoadQueueItem, ByteBuffer> performBulkLoad(final Admin admin, Table table,\n      RegionLocator regionLocator, Deque<LoadQueueItem> queue, ExecutorService pool,\n      SecureBulkLoadClient secureClient, boolean copyFile) throws IOException {\n    int count = 0;\n\n    if(isSecureBulkLoadEndpointAvailable()) {\n      LOG.warn(\"SecureBulkLoadEndpoint is deprecated. It will be removed in future releases.\");\n      LOG.warn(\"Secure bulk load has been integrated into HBase core.\");\n    }\n\n    //If using secure bulk load, get source delegation token, and\n    //prepare staging directory and token\n    // fs is the source filesystem\n    fsDelegationToken.acquireDelegationToken(fs);\n    bulkToken = secureClient.prepareBulkLoad(admin.getConnection());\n    Pair<Multimap<ByteBuffer, LoadQueueItem>, Set<String>> pair = null;\n\n    Map<LoadQueueItem, ByteBuffer> item2RegionMap = new HashMap<>();\n    // Assumes that region splits can happen while this occurs.\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      final Pair<byte[][], byte[][]> startEndKeys = regionLocator.getStartEndKeys();\n      if (count != 0) {\n        LOG.info(\"Split occured while grouping HFiles, retry attempt \" +\n            + count + \" with \" + queue.size() + \" files remaining to group or split\");\n      }\n\n      int maxRetries = getConf().getInt(HConstants.BULKLOAD_MAX_RETRIES_NUMBER, 10);\n      maxRetries = Math.max(maxRetries, startEndKeys.getFirst().length + 1);\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count +\n            \" times without completing, bailing out\");\n      }\n      count++;\n\n      // Using ByteBuffer for byte[] equality semantics\n      pair = groupOrSplitPhase(table, pool, queue, startEndKeys);\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups = pair.getFirst();\n\n      if (!checkHFilesCountPerRegionPerFamily(regionGroups)) {\n        // Error is logged inside checkHFilesCountPerRegionPerFamily.\n        throw new IOException(\"Trying to load more than \" + maxFilesPerRegionPerFamily\n            + \" hfiles to one family of one region\");\n      }\n\n      bulkLoadPhase(table, admin.getConnection(), pool, queue, regionGroups, copyFile,\n          item2RegionMap);\n\n      // NOTE: The next iteration's split / group could happen in parallel to\n      // atomic bulkloads assuming that there are splits and no merges, and\n      // that we can atomically pull out the groups we want to retry.\n    }\n\n    if (!queue.isEmpty()) {\n      throw new RuntimeException(\"Bulk load aborted with some files not yet loaded.\"\n        + \"Please check log for more details.\");\n    }\n    return item2RegionMap;\n  }",
            " 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476 +\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  ",
            "  Map<LoadQueueItem, ByteBuffer> performBulkLoad(final Admin admin, Table table,\n      RegionLocator regionLocator, Deque<LoadQueueItem> queue, ExecutorService pool,\n      SecureBulkLoadClient secureClient, boolean copyFile) throws IOException {\n    int count = 0;\n\n    if(isSecureBulkLoadEndpointAvailable()) {\n      LOG.warn(\"SecureBulkLoadEndpoint is deprecated. It will be removed in future releases.\");\n      LOG.warn(\"Secure bulk load has been integrated into HBase core.\");\n    }\n\n    //If using secure bulk load, get source delegation token, and\n    //prepare staging directory and token\n    // fs is the source filesystem\n    fsDelegationToken.acquireDelegationToken(fs);\n    bulkToken = secureClient.prepareBulkLoad(admin.getConnection());\n    Pair<Multimap<ByteBuffer, LoadQueueItem>, Set<String>> pair = null;\n\n    Map<LoadQueueItem, ByteBuffer> item2RegionMap = new HashMap<>();\n    // Assumes that region splits can happen while this occurs.\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      final Pair<byte[][], byte[][]> startEndKeys = regionLocator.getStartEndKeys();\n      if (count != 0) {\n        LOG.info(\"Split occurred while grouping HFiles, retry attempt \" +\n            + count + \" with \" + queue.size() + \" files remaining to group or split\");\n      }\n\n      int maxRetries = getConf().getInt(HConstants.BULKLOAD_MAX_RETRIES_NUMBER, 10);\n      maxRetries = Math.max(maxRetries, startEndKeys.getFirst().length + 1);\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count +\n            \" times without completing, bailing out\");\n      }\n      count++;\n\n      // Using ByteBuffer for byte[] equality semantics\n      pair = groupOrSplitPhase(table, pool, queue, startEndKeys);\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups = pair.getFirst();\n\n      if (!checkHFilesCountPerRegionPerFamily(regionGroups)) {\n        // Error is logged inside checkHFilesCountPerRegionPerFamily.\n        throw new IOException(\"Trying to load more than \" + maxFilesPerRegionPerFamily\n            + \" hfiles to one family of one region\");\n      }\n\n      bulkLoadPhase(table, admin.getConnection(), pool, queue, regionGroups, copyFile,\n          item2RegionMap);\n\n      // NOTE: The next iteration's split / group could happen in parallel to\n      // atomic bulkloads assuming that there are splits and no merges, and\n      // that we can atomically pull out the groups we want to retry.\n    }\n\n    if (!queue.isEmpty()) {\n      throw new RuntimeException(\"Bulk load aborted with some files not yet loaded.\"\n        + \"Please check log for more details.\");\n    }\n    return item2RegionMap;\n  }"
        ],
        [
            "FavoredNodeAssignmentHelper::singleRackCase(HRegionInfo,ServerName,String)",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "  private ServerName[] singleRackCase(HRegionInfo regionInfo,\n      ServerName primaryRS,\n      String primaryRack) throws IOException {\n    // Single rack case: have to pick the secondary and tertiary\n    // from the same rack\n    List<ServerName> serverList = getServersFromRack(primaryRack);\n    if (serverList.size() <= 2) {\n      // Single region server case: cannot not place the favored nodes\n      // on any server;\n      return null;\n    } else {\n      // Randomly select two region servers from the server list and make sure\n      // they are not overlap with the primary region server;\n     Set<ServerName> serverSkipSet = new HashSet<ServerName>();\n     serverSkipSet.add(primaryRS);\n\n     // Place the secondary RS\n     ServerName secondaryRS = getOneRandomServer(primaryRack, serverSkipSet);\n     // Skip the secondary for the tertiary placement\n     serverSkipSet.add(secondaryRS);\n\n     // Place the tertiary RS\n     ServerName tertiaryRS =\n       getOneRandomServer(primaryRack, serverSkipSet);\n\n     if (secondaryRS == null || tertiaryRS == null) {\n       LOG.error(\"Cannot place the secondary and terinary\" +\n           \"region server for region \" +\n           regionInfo.getRegionNameAsString());\n     }\n     // Create the secondary and tertiary pair\n     ServerName[] favoredNodes = new ServerName[2];\n     favoredNodes[0] = secondaryRS;\n     favoredNodes[1] = tertiaryRS;\n     return favoredNodes;\n    }\n  }",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463 +\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "  private ServerName[] singleRackCase(HRegionInfo regionInfo,\n      ServerName primaryRS,\n      String primaryRack) throws IOException {\n    // Single rack case: have to pick the secondary and tertiary\n    // from the same rack\n    List<ServerName> serverList = getServersFromRack(primaryRack);\n    if (serverList.size() <= 2) {\n      // Single region server case: cannot not place the favored nodes\n      // on any server;\n      return null;\n    } else {\n      // Randomly select two region servers from the server list and make sure\n      // they are not overlap with the primary region server;\n     Set<ServerName> serverSkipSet = new HashSet<ServerName>();\n     serverSkipSet.add(primaryRS);\n\n     // Place the secondary RS\n     ServerName secondaryRS = getOneRandomServer(primaryRack, serverSkipSet);\n     // Skip the secondary for the tertiary placement\n     serverSkipSet.add(secondaryRS);\n\n     // Place the tertiary RS\n     ServerName tertiaryRS =\n       getOneRandomServer(primaryRack, serverSkipSet);\n\n     if (secondaryRS == null || tertiaryRS == null) {\n       LOG.error(\"Cannot place the secondary and ternary\" +\n           \"region server for region \" +\n           regionInfo.getRegionNameAsString());\n     }\n     // Create the secondary and tertiary pair\n     ServerName[] favoredNodes = new ServerName[2];\n     favoredNodes[0] = secondaryRS;\n     favoredNodes[1] = tertiaryRS;\n     return favoredNodes;\n    }\n  }"
        ],
        [
            "VerifyReplication::getPeerQuorumConfig(Configuration)",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301 -\n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  ",
            "  private static Pair<ReplicationPeerConfig, Configuration> getPeerQuorumConfig(\n      final Configuration conf) throws IOException {\n    ZooKeeperWatcher localZKW = null;\n    ReplicationPeerZKImpl peer = null;\n    try {\n      localZKW = new ZooKeeperWatcher(conf, \"VerifyReplication\",\n          new Abortable() {\n            @Override public void abort(String why, Throwable e) {}\n            @Override public boolean isAborted() {return false;}\n          });\n\n      ReplicationPeers rp = ReplicationFactory.getReplicationPeers(localZKW, conf, localZKW);\n      rp.init();\n\n      Pair<ReplicationPeerConfig, Configuration> pair = rp.getPeerConf(peerId);\n      if (pair == null) {\n        throw new IOException(\"Couldn't get peer conf!\");\n      }\n\n      return pair;\n    } catch (ReplicationException e) {\n      throw new IOException(\n          \"An error occured while trying to connect to the remove peer cluster\", e);\n    } finally {\n      if (peer != null) {\n        peer.close();\n      }\n      if (localZKW != null) {\n        localZKW.close();\n      }\n    }\n  }",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301 +\n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  ",
            "  private static Pair<ReplicationPeerConfig, Configuration> getPeerQuorumConfig(\n      final Configuration conf) throws IOException {\n    ZooKeeperWatcher localZKW = null;\n    ReplicationPeerZKImpl peer = null;\n    try {\n      localZKW = new ZooKeeperWatcher(conf, \"VerifyReplication\",\n          new Abortable() {\n            @Override public void abort(String why, Throwable e) {}\n            @Override public boolean isAborted() {return false;}\n          });\n\n      ReplicationPeers rp = ReplicationFactory.getReplicationPeers(localZKW, conf, localZKW);\n      rp.init();\n\n      Pair<ReplicationPeerConfig, Configuration> pair = rp.getPeerConf(peerId);\n      if (pair == null) {\n        throw new IOException(\"Couldn't get peer conf!\");\n      }\n\n      return pair;\n    } catch (ReplicationException e) {\n      throw new IOException(\n          \"An error occurred while trying to connect to the remove peer cluster\", e);\n    } finally {\n      if (peer != null) {\n        peer.close();\n      }\n      if (localZKW != null) {\n        localZKW.close();\n      }\n    }\n  }"
        ],
        [
            "MasterMobCompactionThread::CompactionRunner::run()",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  ",
            "    @Override\n    public void run() {\n      try {\n        for (HColumnDescriptor hcd : hcds) {\n          MobUtils.doMobCompaction(conf, fs, tableName, hcd, pool, tableLockManager,\n            allFiles);\n        }\n      } catch (IOException e) {\n        LOG.error(\"Failed to perform the mob compaction\", e);\n      } finally {\n        try {\n          master.reportMobCompactionEnd(tableName);\n        } catch (IOException e) {\n          LOG.error(\"Failed to mark end of mob compation\", e);\n        }\n      }\n    }",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  ",
            "    @Override\n    public void run() {\n      try {\n        for (HColumnDescriptor hcd : hcds) {\n          MobUtils.doMobCompaction(conf, fs, tableName, hcd, pool, tableLockManager,\n            allFiles);\n        }\n      } catch (IOException e) {\n        LOG.error(\"Failed to perform the mob compaction\", e);\n      } finally {\n        try {\n          master.reportMobCompactionEnd(tableName);\n        } catch (IOException e) {\n          LOG.error(\"Failed to mark end of mob compaction\", e);\n        }\n      }\n    }"
        ],
        [
            "SplitTableRegionProcedure::preSplitRegionBeforePONR(MasterProcedureEnv)",
            " 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700 -\n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  /**\n   * Post split region actions before the Point-of-No-Return step\n   * @param env MasterProcedureEnv\n   **/\n  private void preSplitRegionBeforePONR(final MasterProcedureEnv env)\n    throws IOException, InterruptedException {\n    final List<Mutation> metaEntries = new ArrayList<Mutation>();\n    final MasterCoprocessorHost cpHost = env.getMasterCoprocessorHost();\n    if (cpHost != null) {\n      if (cpHost.preSplitBeforePONRAction(getSplitRow(), metaEntries, getUser())) {\n        throw new IOException(\"Coprocessor bypassing region \" +\n            parentHRI.getRegionNameAsString() + \" split.\");\n      }\n      try {\n        for (Mutation p : metaEntries) {\n          HRegionInfo.parseRegionName(p.getRow());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Row key of mutation from coprossor is not parsable as region name.\"\n            + \"Mutations from coprocessor should only for hbase:meta table.\");\n        throw e;\n      }\n    }\n  }",
            " 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700 +\n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  /**\n   * Post split region actions before the Point-of-No-Return step\n   * @param env MasterProcedureEnv\n   **/\n  private void preSplitRegionBeforePONR(final MasterProcedureEnv env)\n    throws IOException, InterruptedException {\n    final List<Mutation> metaEntries = new ArrayList<Mutation>();\n    final MasterCoprocessorHost cpHost = env.getMasterCoprocessorHost();\n    if (cpHost != null) {\n      if (cpHost.preSplitBeforePONRAction(getSplitRow(), metaEntries, getUser())) {\n        throw new IOException(\"Coprocessor bypassing region \" +\n            parentHRI.getRegionNameAsString() + \" split.\");\n      }\n      try {\n        for (Mutation p : metaEntries) {\n          HRegionInfo.parseRegionName(p.getRow());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Row key of mutation from coprocessor is not parsable as region name.\"\n            + \"Mutations from coprocessor should only for hbase:meta table.\");\n        throw e;\n      }\n    }\n  }"
        ],
        [
            "HFileBlock::FSReaderImpl::readBlockData(long,long,boolean)",
            "1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601 -\n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  ",
            "    /**\n     * Reads a version 2 block (version 1 blocks not supported and not expected). Tries to do as\n     * little memory allocation as possible, using the provided on-disk size.\n     *\n     * @param offset the offset in the stream to read at\n     * @param onDiskSizeWithHeaderL the on-disk size of the block, including\n     *          the header, or -1 if unknown; i.e. when iterating over blocks reading\n     *          in the file metadata info.\n     * @param pread whether to use a positional read\n     */\n    @Override\n    public HFileBlock readBlockData(long offset, long onDiskSizeWithHeaderL, boolean pread)\n    throws IOException {\n      // Get a copy of the current state of whether to validate\n      // hbase checksums or not for this read call. This is not\n      // thread-safe but the one constaint is that if we decide\n      // to skip hbase checksum verification then we are\n      // guaranteed to use hdfs checksum verification.\n      boolean doVerificationThruHBaseChecksum = streamWrapper.shouldUseHBaseChecksum();\n      FSDataInputStream is = streamWrapper.getStream(doVerificationThruHBaseChecksum);\n\n      HFileBlock blk = readBlockDataInternal(is, offset,\n                         onDiskSizeWithHeaderL, pread,\n                         doVerificationThruHBaseChecksum);\n      if (blk == null) {\n        HFile.LOG.warn(\"HBase checksum verification failed for file \" +\n                       pathName + \" at offset \" +\n                       offset + \" filesize \" + fileSize +\n                       \". Retrying read with HDFS checksums turned on...\");\n\n        if (!doVerificationThruHBaseChecksum) {\n          String msg = \"HBase checksum verification failed for file \" +\n                       pathName + \" at offset \" +\n                       offset + \" filesize \" + fileSize +\n                       \" but this cannot happen because doVerify is \" +\n                       doVerificationThruHBaseChecksum;\n          HFile.LOG.warn(msg);\n          throw new IOException(msg); // cannot happen case here\n        }\n        HFile.CHECKSUM_FAILURES.increment(); // update metrics\n\n        // If we have a checksum failure, we fall back into a mode where\n        // the next few reads use HDFS level checksums. We aim to make the\n        // next CHECKSUM_VERIFICATION_NUM_IO_THRESHOLD reads avoid\n        // hbase checksum verification, but since this value is set without\n        // holding any locks, it can so happen that we might actually do\n        // a few more than precisely this number.\n        is = this.streamWrapper.fallbackToFsChecksum(CHECKSUM_VERIFICATION_NUM_IO_THRESHOLD);\n        doVerificationThruHBaseChecksum = false;\n        blk = readBlockDataInternal(is, offset, onDiskSizeWithHeaderL, pread,\n                                    doVerificationThruHBaseChecksum);\n        if (blk != null) {\n          HFile.LOG.warn(\"HDFS checksum verification suceeded for file \" +\n                         pathName + \" at offset \" +\n                         offset + \" filesize \" + fileSize);\n        }\n      }\n      if (blk == null && !doVerificationThruHBaseChecksum) {\n        String msg = \"readBlockData failed, possibly due to \" +\n                     \"checksum verification failed for file \" + pathName +\n                     \" at offset \" + offset + \" filesize \" + fileSize;\n        HFile.LOG.warn(msg);\n        throw new IOException(msg);\n      }\n\n      // If there is a checksum mismatch earlier, then retry with\n      // HBase checksums switched off and use HDFS checksum verification.\n      // This triggers HDFS to detect and fix corrupt replicas. The\n      // next checksumOffCount read requests will use HDFS checksums.\n      // The decrementing of this.checksumOffCount is not thread-safe,\n      // but it is harmless because eventually checksumOffCount will be\n      // a negative number.\n      streamWrapper.checksumOk();\n      return blk;\n    }",
            "1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601 +\n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  ",
            "    /**\n     * Reads a version 2 block (version 1 blocks not supported and not expected). Tries to do as\n     * little memory allocation as possible, using the provided on-disk size.\n     *\n     * @param offset the offset in the stream to read at\n     * @param onDiskSizeWithHeaderL the on-disk size of the block, including\n     *          the header, or -1 if unknown; i.e. when iterating over blocks reading\n     *          in the file metadata info.\n     * @param pread whether to use a positional read\n     */\n    @Override\n    public HFileBlock readBlockData(long offset, long onDiskSizeWithHeaderL, boolean pread)\n    throws IOException {\n      // Get a copy of the current state of whether to validate\n      // hbase checksums or not for this read call. This is not\n      // thread-safe but the one constaint is that if we decide\n      // to skip hbase checksum verification then we are\n      // guaranteed to use hdfs checksum verification.\n      boolean doVerificationThruHBaseChecksum = streamWrapper.shouldUseHBaseChecksum();\n      FSDataInputStream is = streamWrapper.getStream(doVerificationThruHBaseChecksum);\n\n      HFileBlock blk = readBlockDataInternal(is, offset,\n                         onDiskSizeWithHeaderL, pread,\n                         doVerificationThruHBaseChecksum);\n      if (blk == null) {\n        HFile.LOG.warn(\"HBase checksum verification failed for file \" +\n                       pathName + \" at offset \" +\n                       offset + \" filesize \" + fileSize +\n                       \". Retrying read with HDFS checksums turned on...\");\n\n        if (!doVerificationThruHBaseChecksum) {\n          String msg = \"HBase checksum verification failed for file \" +\n                       pathName + \" at offset \" +\n                       offset + \" filesize \" + fileSize +\n                       \" but this cannot happen because doVerify is \" +\n                       doVerificationThruHBaseChecksum;\n          HFile.LOG.warn(msg);\n          throw new IOException(msg); // cannot happen case here\n        }\n        HFile.CHECKSUM_FAILURES.increment(); // update metrics\n\n        // If we have a checksum failure, we fall back into a mode where\n        // the next few reads use HDFS level checksums. We aim to make the\n        // next CHECKSUM_VERIFICATION_NUM_IO_THRESHOLD reads avoid\n        // hbase checksum verification, but since this value is set without\n        // holding any locks, it can so happen that we might actually do\n        // a few more than precisely this number.\n        is = this.streamWrapper.fallbackToFsChecksum(CHECKSUM_VERIFICATION_NUM_IO_THRESHOLD);\n        doVerificationThruHBaseChecksum = false;\n        blk = readBlockDataInternal(is, offset, onDiskSizeWithHeaderL, pread,\n                                    doVerificationThruHBaseChecksum);\n        if (blk != null) {\n          HFile.LOG.warn(\"HDFS checksum verification succeeded for file \" +\n                         pathName + \" at offset \" +\n                         offset + \" filesize \" + fileSize);\n        }\n      }\n      if (blk == null && !doVerificationThruHBaseChecksum) {\n        String msg = \"readBlockData failed, possibly due to \" +\n                     \"checksum verification failed for file \" + pathName +\n                     \" at offset \" + offset + \" filesize \" + fileSize;\n        HFile.LOG.warn(msg);\n        throw new IOException(msg);\n      }\n\n      // If there is a checksum mismatch earlier, then retry with\n      // HBase checksums switched off and use HDFS checksum verification.\n      // This triggers HDFS to detect and fix corrupt replicas. The\n      // next checksumOffCount read requests will use HDFS checksums.\n      // The decrementing of this.checksumOffCount is not thread-safe,\n      // but it is harmless because eventually checksumOffCount will be\n      // a negative number.\n      streamWrapper.checksumOk();\n      return blk;\n    }"
        ],
        [
            "LoadIncrementalHFiles::groupOrSplit(Multimap,LoadQueueItem,Table,Pair)",
            " 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906 -\n 907  \n 908  \n 909  \n 910  \n 911 -\n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  ",
            "  /**\n   * Attempt to assign the given load queue item into its target region group.\n   * If the hfile boundary no longer fits into a region, physically splits\n   * the hfile such that the new bottom half will fit and returns the list of\n   * LQI's corresponding to the resultant hfiles.\n   *\n   * protected for testing\n   * @throws IOException if an IO failure is encountered\n   */\n  protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups, final LoadQueueItem item, final Table table,\n      final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n    final Path hfilePath = item.hfilePath;\n    // fs is the source filesystem\n    if (fs == null) {\n      fs = hfilePath.getFileSystem(getConf());\n    }\n    HFile.Reader hfr = null;\n    try {\n      hfr = HFile.createReader(fs, hfilePath,\n          new CacheConfig(getConf()), getConf());\n    } catch (FileNotFoundException fnfe) {\n      LOG.debug(\"encountered\", fnfe);\n      return new Pair<>(null, hfilePath.getName());\n    }\n    final byte[] first, last;\n    try {\n      hfr.loadFileInfo();\n      first = hfr.getFirstRowKey();\n      last = hfr.getLastRowKey();\n    }  finally {\n      hfr.close();\n    }\n\n    LOG.info(\"Trying to load hfile=\" + hfilePath +\n        \" first=\" + Bytes.toStringBinary(first) +\n        \" last=\"  + Bytes.toStringBinary(last));\n    if (first == null || last == null) {\n      assert first == null && last == null;\n      // TODO what if this is due to a bad HFile?\n      LOG.info(\"hfile \" + hfilePath + \" has no entries, skipping\");\n      return null;\n    }\n    if (Bytes.compareTo(first, last) > 0) {\n      throw new IllegalArgumentException(\n      \"Invalid range: \" + Bytes.toStringBinary(first) +\n      \" > \" + Bytes.toStringBinary(last));\n    }\n    int idx = Arrays.binarySearch(startEndKeys.getFirst(), first,\n        Bytes.BYTES_COMPARATOR);\n    if (idx < 0) {\n      // not on boundary, returns -(insertion index).  Calculate region it\n      // would be in.\n      idx = -(idx + 1) - 1;\n    }\n    final int indexForCallable = idx;\n\n    /**\n     * we can consider there is a region hole in following conditions. 1) if idx < 0,then first\n     * region info is lost. 2) if the endkey of a region is not equal to the startkey of the next\n     * region. 3) if the endkey of the last region is not empty.\n     */\n    if (indexForCallable < 0) {\n      throw new IOException(\"The first region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if ((indexForCallable == startEndKeys.getFirst().length - 1)\n        && !Bytes.equals(startEndKeys.getSecond()[indexForCallable], HConstants.EMPTY_BYTE_ARRAY)) {\n      throw new IOException(\"The last region info for table \"\n          + table.getName()\n          + \" cann't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if (indexForCallable + 1 < startEndKeys.getFirst().length\n        && !(Bytes.compareTo(startEndKeys.getSecond()[indexForCallable],\n          startEndKeys.getFirst()[indexForCallable + 1]) == 0)) {\n      throw new IOException(\"The endkey of one region for table \"\n          + table.getName()\n          + \" is not equal to the startkey of the next region in hbase:meta.\"\n          + \"Please use hbck tool to fix it first.\");\n    }\n\n    boolean lastKeyInRange =\n      Bytes.compareTo(last, startEndKeys.getSecond()[idx]) < 0 ||\n      Bytes.equals(startEndKeys.getSecond()[idx], HConstants.EMPTY_BYTE_ARRAY);\n    if (!lastKeyInRange) {\n      List<LoadQueueItem> lqis = splitStoreFile(item, table,\n          startEndKeys.getFirst()[indexForCallable],\n          startEndKeys.getSecond()[indexForCallable]);\n      return new Pair<>(lqis, null);\n    }\n\n    // group regions.\n    regionGroups.put(ByteBuffer.wrap(startEndKeys.getFirst()[idx]), item);\n    return null;\n  }",
            " 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906 +\n 907  \n 908  \n 909  \n 910  \n 911 +\n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  ",
            "  /**\n   * Attempt to assign the given load queue item into its target region group.\n   * If the hfile boundary no longer fits into a region, physically splits\n   * the hfile such that the new bottom half will fit and returns the list of\n   * LQI's corresponding to the resultant hfiles.\n   *\n   * protected for testing\n   * @throws IOException if an IO failure is encountered\n   */\n  protected Pair<List<LoadQueueItem>, String> groupOrSplit(\n      Multimap<ByteBuffer, LoadQueueItem> regionGroups, final LoadQueueItem item, final Table table,\n      final Pair<byte[][], byte[][]> startEndKeys) throws IOException {\n    final Path hfilePath = item.hfilePath;\n    // fs is the source filesystem\n    if (fs == null) {\n      fs = hfilePath.getFileSystem(getConf());\n    }\n    HFile.Reader hfr = null;\n    try {\n      hfr = HFile.createReader(fs, hfilePath,\n          new CacheConfig(getConf()), getConf());\n    } catch (FileNotFoundException fnfe) {\n      LOG.debug(\"encountered\", fnfe);\n      return new Pair<>(null, hfilePath.getName());\n    }\n    final byte[] first, last;\n    try {\n      hfr.loadFileInfo();\n      first = hfr.getFirstRowKey();\n      last = hfr.getLastRowKey();\n    }  finally {\n      hfr.close();\n    }\n\n    LOG.info(\"Trying to load hfile=\" + hfilePath +\n        \" first=\" + Bytes.toStringBinary(first) +\n        \" last=\"  + Bytes.toStringBinary(last));\n    if (first == null || last == null) {\n      assert first == null && last == null;\n      // TODO what if this is due to a bad HFile?\n      LOG.info(\"hfile \" + hfilePath + \" has no entries, skipping\");\n      return null;\n    }\n    if (Bytes.compareTo(first, last) > 0) {\n      throw new IllegalArgumentException(\n      \"Invalid range: \" + Bytes.toStringBinary(first) +\n      \" > \" + Bytes.toStringBinary(last));\n    }\n    int idx = Arrays.binarySearch(startEndKeys.getFirst(), first,\n        Bytes.BYTES_COMPARATOR);\n    if (idx < 0) {\n      // not on boundary, returns -(insertion index).  Calculate region it\n      // would be in.\n      idx = -(idx + 1) - 1;\n    }\n    final int indexForCallable = idx;\n\n    /**\n     * we can consider there is a region hole in following conditions. 1) if idx < 0,then first\n     * region info is lost. 2) if the endkey of a region is not equal to the startkey of the next\n     * region. 3) if the endkey of the last region is not empty.\n     */\n    if (indexForCallable < 0) {\n      throw new IOException(\"The first region info for table \"\n          + table.getName()\n          + \" can't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if ((indexForCallable == startEndKeys.getFirst().length - 1)\n        && !Bytes.equals(startEndKeys.getSecond()[indexForCallable], HConstants.EMPTY_BYTE_ARRAY)) {\n      throw new IOException(\"The last region info for table \"\n          + table.getName()\n          + \" can't be found in hbase:meta.Please use hbck tool to fix it first.\");\n    } else if (indexForCallable + 1 < startEndKeys.getFirst().length\n        && !(Bytes.compareTo(startEndKeys.getSecond()[indexForCallable],\n          startEndKeys.getFirst()[indexForCallable + 1]) == 0)) {\n      throw new IOException(\"The endkey of one region for table \"\n          + table.getName()\n          + \" is not equal to the startkey of the next region in hbase:meta.\"\n          + \"Please use hbck tool to fix it first.\");\n    }\n\n    boolean lastKeyInRange =\n      Bytes.compareTo(last, startEndKeys.getSecond()[idx]) < 0 ||\n      Bytes.equals(startEndKeys.getSecond()[idx], HConstants.EMPTY_BYTE_ARRAY);\n    if (!lastKeyInRange) {\n      List<LoadQueueItem> lqis = splitStoreFile(item, table,\n          startEndKeys.getFirst()[indexForCallable],\n          startEndKeys.getSecond()[indexForCallable]);\n      return new Pair<>(lqis, null);\n    }\n\n    // group regions.\n    regionGroups.put(ByteBuffer.wrap(startEndKeys.getFirst()[idx]), item);\n    return null;\n  }"
        ],
        [
            "HBaseFsck::adoptHdfsOrphan(HbckInfo)",
            " 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936 -\n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  ",
            "  /**\n   * Orphaned regions are regions without a .regioninfo file in them.  We \"adopt\"\n   * these orphans by creating a new region, and moving the column families,\n   * recovered edits, WALs, into the new region dir.  We determine the region\n   * startkey and endkeys by looking at all of the hfiles inside the column\n   * families to identify the min and max keys. The resulting region will\n   * likely violate table integrity but will be dealt with by merging\n   * overlapping regions.\n   */\n  @SuppressWarnings(\"deprecation\")\n  private void adoptHdfsOrphan(HbckInfo hi) throws IOException {\n    Path p = hi.getHdfsRegionDir();\n    FileSystem fs = p.getFileSystem(getConf());\n    FileStatus[] dirs = fs.listStatus(p);\n    if (dirs == null) {\n      LOG.warn(\"Attempt to adopt ophan hdfs region skipped becuase no files present in \" +\n          p + \". This dir could probably be deleted.\");\n      return ;\n    }\n\n    TableName tableName = hi.getTableName();\n    TableInfo tableInfo = tablesInfo.get(tableName);\n    Preconditions.checkNotNull(tableInfo, \"Table '\" + tableName + \"' not present!\");\n    HTableDescriptor template = tableInfo.getHTD();\n\n    // find min and max key values\n    Pair<byte[],byte[]> orphanRegionRange = null;\n    for (FileStatus cf : dirs) {\n      String cfName= cf.getPath().getName();\n      // TODO Figure out what the special dirs are\n      if (cfName.startsWith(\".\") || cfName.equals(HConstants.SPLIT_LOGDIR_NAME)) continue;\n\n      FileStatus[] hfiles = fs.listStatus(cf.getPath());\n      for (FileStatus hfile : hfiles) {\n        byte[] start, end;\n        HFile.Reader hf = null;\n        try {\n          CacheConfig cacheConf = new CacheConfig(getConf());\n          hf = HFile.createReader(fs, hfile.getPath(), cacheConf, getConf());\n          hf.loadFileInfo();\n          Cell startKv = hf.getFirstKey();\n          start = CellUtil.cloneRow(startKv);\n          Cell endKv = hf.getLastKey();\n          end = CellUtil.cloneRow(endKv);\n        } catch (IOException ioe) {\n          LOG.warn(\"Problem reading orphan file \" + hfile + \", skipping\");\n          continue;\n        } catch (NullPointerException ioe) {\n          LOG.warn(\"Orphan file \" + hfile + \" is possibly corrupted HFile, skipping\");\n          continue;\n        } finally {\n          if (hf != null) {\n            hf.close();\n          }\n        }\n\n        // expand the range to include the range of all hfiles\n        if (orphanRegionRange == null) {\n          // first range\n          orphanRegionRange = new Pair<byte[], byte[]>(start, end);\n        } else {\n          // TODO add test\n\n          // expand range only if the hfile is wider.\n          if (Bytes.compareTo(orphanRegionRange.getFirst(), start) > 0) {\n            orphanRegionRange.setFirst(start);\n          }\n          if (Bytes.compareTo(orphanRegionRange.getSecond(), end) < 0 ) {\n            orphanRegionRange.setSecond(end);\n          }\n        }\n      }\n    }\n    if (orphanRegionRange == null) {\n      LOG.warn(\"No data in dir \" + p + \", sidelining data\");\n      fixes++;\n      sidelineRegionDir(fs, hi);\n      return;\n    }\n    LOG.info(\"Min max keys are : [\" + Bytes.toString(orphanRegionRange.getFirst()) + \", \" +\n        Bytes.toString(orphanRegionRange.getSecond()) + \")\");\n\n    // create new region on hdfs. move data into place.\n    HRegionInfo hri = new HRegionInfo(template.getTableName(), orphanRegionRange.getFirst(),\n        Bytes.add(orphanRegionRange.getSecond(), new byte[1]));\n    LOG.info(\"Creating new region : \" + hri);\n    HRegion region = HBaseFsckRepair.createHDFSRegionDir(getConf(), hri, template);\n    Path target = region.getRegionFileSystem().getRegionDir();\n\n    // rename all the data to new region\n    mergeRegionDirs(target, hi);\n    fixes++;\n  }",
            " 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936 +\n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  ",
            "  /**\n   * Orphaned regions are regions without a .regioninfo file in them.  We \"adopt\"\n   * these orphans by creating a new region, and moving the column families,\n   * recovered edits, WALs, into the new region dir.  We determine the region\n   * startkey and endkeys by looking at all of the hfiles inside the column\n   * families to identify the min and max keys. The resulting region will\n   * likely violate table integrity but will be dealt with by merging\n   * overlapping regions.\n   */\n  @SuppressWarnings(\"deprecation\")\n  private void adoptHdfsOrphan(HbckInfo hi) throws IOException {\n    Path p = hi.getHdfsRegionDir();\n    FileSystem fs = p.getFileSystem(getConf());\n    FileStatus[] dirs = fs.listStatus(p);\n    if (dirs == null) {\n      LOG.warn(\"Attempt to adopt orphan hdfs region skipped because no files present in \" +\n          p + \". This dir could probably be deleted.\");\n      return ;\n    }\n\n    TableName tableName = hi.getTableName();\n    TableInfo tableInfo = tablesInfo.get(tableName);\n    Preconditions.checkNotNull(tableInfo, \"Table '\" + tableName + \"' not present!\");\n    HTableDescriptor template = tableInfo.getHTD();\n\n    // find min and max key values\n    Pair<byte[],byte[]> orphanRegionRange = null;\n    for (FileStatus cf : dirs) {\n      String cfName= cf.getPath().getName();\n      // TODO Figure out what the special dirs are\n      if (cfName.startsWith(\".\") || cfName.equals(HConstants.SPLIT_LOGDIR_NAME)) continue;\n\n      FileStatus[] hfiles = fs.listStatus(cf.getPath());\n      for (FileStatus hfile : hfiles) {\n        byte[] start, end;\n        HFile.Reader hf = null;\n        try {\n          CacheConfig cacheConf = new CacheConfig(getConf());\n          hf = HFile.createReader(fs, hfile.getPath(), cacheConf, getConf());\n          hf.loadFileInfo();\n          Cell startKv = hf.getFirstKey();\n          start = CellUtil.cloneRow(startKv);\n          Cell endKv = hf.getLastKey();\n          end = CellUtil.cloneRow(endKv);\n        } catch (IOException ioe) {\n          LOG.warn(\"Problem reading orphan file \" + hfile + \", skipping\");\n          continue;\n        } catch (NullPointerException ioe) {\n          LOG.warn(\"Orphan file \" + hfile + \" is possibly corrupted HFile, skipping\");\n          continue;\n        } finally {\n          if (hf != null) {\n            hf.close();\n          }\n        }\n\n        // expand the range to include the range of all hfiles\n        if (orphanRegionRange == null) {\n          // first range\n          orphanRegionRange = new Pair<byte[], byte[]>(start, end);\n        } else {\n          // TODO add test\n\n          // expand range only if the hfile is wider.\n          if (Bytes.compareTo(orphanRegionRange.getFirst(), start) > 0) {\n            orphanRegionRange.setFirst(start);\n          }\n          if (Bytes.compareTo(orphanRegionRange.getSecond(), end) < 0 ) {\n            orphanRegionRange.setSecond(end);\n          }\n        }\n      }\n    }\n    if (orphanRegionRange == null) {\n      LOG.warn(\"No data in dir \" + p + \", sidelining data\");\n      fixes++;\n      sidelineRegionDir(fs, hi);\n      return;\n    }\n    LOG.info(\"Min max keys are : [\" + Bytes.toString(orphanRegionRange.getFirst()) + \", \" +\n        Bytes.toString(orphanRegionRange.getSecond()) + \")\");\n\n    // create new region on hdfs. move data into place.\n    HRegionInfo hri = new HRegionInfo(template.getTableName(), orphanRegionRange.getFirst(),\n        Bytes.add(orphanRegionRange.getSecond(), new byte[1]));\n    LOG.info(\"Creating new region : \" + hri);\n    HRegion region = HBaseFsckRepair.createHDFSRegionDir(getConf(), hri, template);\n    Path target = region.getRegionFileSystem().getRegionDir();\n\n    // rename all the data to new region\n    mergeRegionDirs(target, hi);\n    fixes++;\n  }"
        ],
        [
            "WALSplitterHandler::process()",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  @Override\n  public void process() throws IOException {\n    long startTime = System.currentTimeMillis();\n    try {\n      Status status = this.splitTaskExecutor.exec(splitTaskDetails.getWALFile(), mode, reporter);\n      switch (status) {\n      case DONE:\n        coordination.endTask(new SplitLogTask.Done(this.serverName,this.mode),\n          SplitLogCounters.tot_wkr_task_done, splitTaskDetails);\n        break;\n      case PREEMPTED:\n        SplitLogCounters.tot_wkr_preempt_task.incrementAndGet();\n        LOG.warn(\"task execution prempted \" + splitTaskDetails.getWALFile());\n        break;\n      case ERR:\n        if (server != null && !server.isStopped()) {\n          coordination.endTask(new SplitLogTask.Err(this.serverName, this.mode),\n            SplitLogCounters.tot_wkr_task_err, splitTaskDetails);\n          break;\n        }\n        // if the RS is exiting then there is probably a tons of stuff\n        // that can go wrong. Resign instead of signaling error.\n        //$FALL-THROUGH$\n      case RESIGNED:\n        if (server != null && server.isStopped()) {\n          LOG.info(\"task execution interrupted because worker is exiting \"\n              + splitTaskDetails.toString());\n        }\n        coordination.endTask(new SplitLogTask.Resigned(this.serverName, this.mode),\n          SplitLogCounters.tot_wkr_task_resigned, splitTaskDetails);\n        break;\n      }\n    } finally {\n      LOG.info(\"worker \" + serverName + \" done with task \" + splitTaskDetails.toString() + \" in \"\n          + (System.currentTimeMillis() - startTime) + \"ms\");\n      this.inProgressTasks.decrementAndGet();\n    }\n  }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  @Override\n  public void process() throws IOException {\n    long startTime = System.currentTimeMillis();\n    try {\n      Status status = this.splitTaskExecutor.exec(splitTaskDetails.getWALFile(), mode, reporter);\n      switch (status) {\n      case DONE:\n        coordination.endTask(new SplitLogTask.Done(this.serverName,this.mode),\n          SplitLogCounters.tot_wkr_task_done, splitTaskDetails);\n        break;\n      case PREEMPTED:\n        SplitLogCounters.tot_wkr_preempt_task.incrementAndGet();\n        LOG.warn(\"task execution preempted \" + splitTaskDetails.getWALFile());\n        break;\n      case ERR:\n        if (server != null && !server.isStopped()) {\n          coordination.endTask(new SplitLogTask.Err(this.serverName, this.mode),\n            SplitLogCounters.tot_wkr_task_err, splitTaskDetails);\n          break;\n        }\n        // if the RS is exiting then there is probably a tons of stuff\n        // that can go wrong. Resign instead of signaling error.\n        //$FALL-THROUGH$\n      case RESIGNED:\n        if (server != null && server.isStopped()) {\n          LOG.info(\"task execution interrupted because worker is exiting \"\n              + splitTaskDetails.toString());\n        }\n        coordination.endTask(new SplitLogTask.Resigned(this.serverName, this.mode),\n          SplitLogCounters.tot_wkr_task_resigned, splitTaskDetails);\n        break;\n      }\n    } finally {\n      LOG.info(\"worker \" + serverName + \" done with task \" + splitTaskDetails.toString() + \" in \"\n          + (System.currentTimeMillis() - startTime) + \"ms\");\n      this.inProgressTasks.decrementAndGet();\n    }\n  }"
        ],
        [
            "SplitTransactionImpl::createDaughters(Server,RegionServerServices,User)",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260 -\n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "  /**\n   * Prepare the regions and region files.\n   * @param server Hosting server instance.  Can be null when testing (won't try\n   * and update in zk if a null server)\n   * @param services Used to online/offline regions.\n   * @param user\n   * @throws IOException If thrown, transaction failed.\n   *    Call {@link #rollback(Server, RegionServerServices)}\n   * @return Regions created\n   */\n  @VisibleForTesting\n  PairOfSameType<Region> createDaughters(final Server server,\n      final RegionServerServices services, User user) throws IOException {\n    LOG.info(\"Starting split of region \" + this.parent);\n    if ((server != null && server.isStopped()) ||\n        (services != null && services.isStopping())) {\n      throw new IOException(\"Server is stopped or stopping\");\n    }\n    assert !this.parent.lock.writeLock().isHeldByCurrentThread():\n      \"Unsafe to hold write lock while performing RPCs\";\n\n    transition(SplitTransactionPhase.BEFORE_PRE_SPLIT_HOOK);\n\n    // Coprocessor callback\n    if (this.parent.getCoprocessorHost() != null) {\n      // TODO: Remove one of these\n      parent.getCoprocessorHost().preSplit(user);\n      parent.getCoprocessorHost().preSplit(splitrow, user);\n    }\n\n    transition(SplitTransactionPhase.AFTER_PRE_SPLIT_HOOK);\n\n    // If true, no cluster to write meta edits to or to update znodes in.\n    boolean testing = server == null? true:\n        server.getConfiguration().getBoolean(\"hbase.testing.nocluster\", false);\n    this.fileSplitTimeout = testing ? this.fileSplitTimeout :\n        server.getConfiguration().getLong(\"hbase.regionserver.fileSplitTimeout\",\n          this.fileSplitTimeout);\n\n    PairOfSameType<Region> daughterRegions = stepsBeforePONR(server, services, testing);\n\n    final List<Mutation> metaEntries = new ArrayList<Mutation>();\n    boolean ret = false;\n    if (this.parent.getCoprocessorHost() != null) {\n      ret = parent.getCoprocessorHost().preSplitBeforePONR(splitrow, metaEntries, user);\n      if (ret) {\n          throw new IOException(\"Coprocessor bypassing region \"\n            + parent.getRegionInfo().getRegionNameAsString() + \" split.\");\n      }\n      try {\n        for (Mutation p : metaEntries) {\n          HRegionInfo.parseRegionName(p.getRow());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Row key of mutation from coprossor is not parsable as region name.\"\n            + \"Mutations from coprocessor should only for hbase:meta table.\");\n        throw e;\n      }\n    }\n\n    // This is the point of no return.  Adding subsequent edits to .META. as we\n    // do below when we do the daughter opens adding each to .META. can fail in\n    // various interesting ways the most interesting of which is a timeout\n    // BUT the edits all go through (See HBASE-3872).  IF we reach the PONR\n    // then subsequent failures need to crash out this regionserver; the\n    // server shutdown processing should be able to fix-up the incomplete split.\n    // The offlined parent will have the daughters as extra columns.  If\n    // we leave the daughter regions in place and do not remove them when we\n    // crash out, then they will have their references to the parent in place\n    // still and the server shutdown fixup of .META. will point to these\n    // regions.\n    // We should add PONR JournalEntry before offlineParentInMeta,so even if\n    // OfflineParentInMeta timeout,this will cause regionserver exit,and then\n    // master ServerShutdownHandler will fix daughter & avoid data loss. (See\n    // HBase-4562).\n\n    transition(SplitTransactionPhase.PONR);\n\n    // Edit parent in meta.  Offlines parent region and adds splita and splitb\n    // as an atomic update. See HBASE-7721. This update to META makes the region\n    // will determine whether the region is split or not in case of failures.\n    // If it is successful, master will roll-forward, if not, master will rollback\n    // and assign the parent region.\n    if (services != null && !services.reportRegionStateTransition(TransitionCode.SPLIT_PONR,\n        parent.getRegionInfo(), hri_a, hri_b)) {\n      // Passed PONR, let SSH clean it up\n      throw new IOException(\"Failed to notify master that split passed PONR: \"\n        + parent.getRegionInfo().getRegionNameAsString());\n    }\n    return daughterRegions;\n  }",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260 +\n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "  /**\n   * Prepare the regions and region files.\n   * @param server Hosting server instance.  Can be null when testing (won't try\n   * and update in zk if a null server)\n   * @param services Used to online/offline regions.\n   * @param user\n   * @throws IOException If thrown, transaction failed.\n   *    Call {@link #rollback(Server, RegionServerServices)}\n   * @return Regions created\n   */\n  @VisibleForTesting\n  PairOfSameType<Region> createDaughters(final Server server,\n      final RegionServerServices services, User user) throws IOException {\n    LOG.info(\"Starting split of region \" + this.parent);\n    if ((server != null && server.isStopped()) ||\n        (services != null && services.isStopping())) {\n      throw new IOException(\"Server is stopped or stopping\");\n    }\n    assert !this.parent.lock.writeLock().isHeldByCurrentThread():\n      \"Unsafe to hold write lock while performing RPCs\";\n\n    transition(SplitTransactionPhase.BEFORE_PRE_SPLIT_HOOK);\n\n    // Coprocessor callback\n    if (this.parent.getCoprocessorHost() != null) {\n      // TODO: Remove one of these\n      parent.getCoprocessorHost().preSplit(user);\n      parent.getCoprocessorHost().preSplit(splitrow, user);\n    }\n\n    transition(SplitTransactionPhase.AFTER_PRE_SPLIT_HOOK);\n\n    // If true, no cluster to write meta edits to or to update znodes in.\n    boolean testing = server == null? true:\n        server.getConfiguration().getBoolean(\"hbase.testing.nocluster\", false);\n    this.fileSplitTimeout = testing ? this.fileSplitTimeout :\n        server.getConfiguration().getLong(\"hbase.regionserver.fileSplitTimeout\",\n          this.fileSplitTimeout);\n\n    PairOfSameType<Region> daughterRegions = stepsBeforePONR(server, services, testing);\n\n    final List<Mutation> metaEntries = new ArrayList<Mutation>();\n    boolean ret = false;\n    if (this.parent.getCoprocessorHost() != null) {\n      ret = parent.getCoprocessorHost().preSplitBeforePONR(splitrow, metaEntries, user);\n      if (ret) {\n          throw new IOException(\"Coprocessor bypassing region \"\n            + parent.getRegionInfo().getRegionNameAsString() + \" split.\");\n      }\n      try {\n        for (Mutation p : metaEntries) {\n          HRegionInfo.parseRegionName(p.getRow());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Row key of mutation from coprocessor is not parsable as region name.\"\n            + \"Mutations from coprocessor should only for hbase:meta table.\");\n        throw e;\n      }\n    }\n\n    // This is the point of no return.  Adding subsequent edits to .META. as we\n    // do below when we do the daughter opens adding each to .META. can fail in\n    // various interesting ways the most interesting of which is a timeout\n    // BUT the edits all go through (See HBASE-3872).  IF we reach the PONR\n    // then subsequent failures need to crash out this regionserver; the\n    // server shutdown processing should be able to fix-up the incomplete split.\n    // The offlined parent will have the daughters as extra columns.  If\n    // we leave the daughter regions in place and do not remove them when we\n    // crash out, then they will have their references to the parent in place\n    // still and the server shutdown fixup of .META. will point to these\n    // regions.\n    // We should add PONR JournalEntry before offlineParentInMeta,so even if\n    // OfflineParentInMeta timeout,this will cause regionserver exit,and then\n    // master ServerShutdownHandler will fix daughter & avoid data loss. (See\n    // HBase-4562).\n\n    transition(SplitTransactionPhase.PONR);\n\n    // Edit parent in meta.  Offlines parent region and adds splita and splitb\n    // as an atomic update. See HBASE-7721. This update to META makes the region\n    // will determine whether the region is split or not in case of failures.\n    // If it is successful, master will roll-forward, if not, master will rollback\n    // and assign the parent region.\n    if (services != null && !services.reportRegionStateTransition(TransitionCode.SPLIT_PONR,\n        parent.getRegionInfo(), hri_a, hri_b)) {\n      // Passed PONR, let SSH clean it up\n      throw new IOException(\"Failed to notify master that split passed PONR: \"\n        + parent.getRegionInfo().getRegionNameAsString());\n    }\n    return daughterRegions;\n  }"
        ],
        [
            "AuthenticationTokenSecretManager::retrievePassword(AuthenticationTokenIdentifier)",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "  @Override\n  public byte[] retrievePassword(AuthenticationTokenIdentifier identifier)\n      throws InvalidToken {\n    long now = EnvironmentEdgeManager.currentTime();\n    if (identifier.getExpirationDate() < now) {\n      throw new InvalidToken(\"Token has expired\");\n    }\n    AuthenticationKey masterKey = allKeys.get(identifier.getKeyId());\n    if(masterKey == null) {\n      if(zkWatcher.getWatcher().isAborted()) {\n        LOG.error(\"ZooKeeperWatcher is abort\");\n        throw new InvalidToken(\"Token keys could not be sync from zookeeper\"\n            + \" because of ZooKeeperWatcher abort\");\n      }\n      synchronized (this) {\n        if (!leaderElector.isAlive() || leaderElector.isStopped()) {\n          LOG.warn(\"Thread leaderElector[\" + leaderElector.getName() + \":\"\n              + leaderElector.getId() + \"] is stoped or not alive\");\n          leaderElector.start();\n          LOG.info(\"Thread leaderElector [\" + leaderElector.getName() + \":\"\n              + leaderElector.getId() + \"] is started\");\n        }\n      }\n      zkWatcher.refreshKeys();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sync token keys from zookeeper\");\n      }\n      masterKey = allKeys.get(identifier.getKeyId());\n    }\n    if (masterKey == null) {\n      throw new InvalidToken(\"Unknown master key for token (id=\"+\n          identifier.getKeyId()+\")\");\n    }\n    // regenerate the password\n    return createPassword(identifier.getBytes(),\n        masterKey.getKey());\n  }",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "  @Override\n  public byte[] retrievePassword(AuthenticationTokenIdentifier identifier)\n      throws InvalidToken {\n    long now = EnvironmentEdgeManager.currentTime();\n    if (identifier.getExpirationDate() < now) {\n      throw new InvalidToken(\"Token has expired\");\n    }\n    AuthenticationKey masterKey = allKeys.get(identifier.getKeyId());\n    if(masterKey == null) {\n      if(zkWatcher.getWatcher().isAborted()) {\n        LOG.error(\"ZooKeeperWatcher is abort\");\n        throw new InvalidToken(\"Token keys could not be sync from zookeeper\"\n            + \" because of ZooKeeperWatcher abort\");\n      }\n      synchronized (this) {\n        if (!leaderElector.isAlive() || leaderElector.isStopped()) {\n          LOG.warn(\"Thread leaderElector[\" + leaderElector.getName() + \":\"\n              + leaderElector.getId() + \"] is stopped or not alive\");\n          leaderElector.start();\n          LOG.info(\"Thread leaderElector [\" + leaderElector.getName() + \":\"\n              + leaderElector.getId() + \"] is started\");\n        }\n      }\n      zkWatcher.refreshKeys();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Sync token keys from zookeeper\");\n      }\n      masterKey = allKeys.get(identifier.getKeyId());\n    }\n    if (masterKey == null) {\n      throw new InvalidToken(\"Unknown master key for token (id=\"+\n          identifier.getKeyId()+\")\");\n    }\n    // regenerate the password\n    return createPassword(identifier.getBytes(),\n        masterKey.getKey());\n  }"
        ],
        [
            "DispatchMergingRegionsProcedure::doMergeInRS(MasterProcedureEnv)",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 -\n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 -\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "  /**\n   * Do the real merge operation in the region server that hosts regions\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private void doMergeInRS(final MasterProcedureEnv env) throws IOException {\n    long duration = 0;\n    long startTime = EnvironmentEdgeManager.currentTime();\n    do {\n      try {\n        if (getServerName(env) == null) {\n          // The merge probably already happen. Check\n          RegionState regionState = getAssignmentManager(env).getRegionStates().getRegionState(\n            regionsToMerge[0].getEncodedName());\n          if (regionState.isMerging() || regionState.isMerged()) {\n            LOG.info(\"Merge regions \" +  getRegionsToMergeListEncodedNameString() +\n              \" is in progress or completed.  No need to send a new request.\");\n          } else {\n            LOG.warn(\"Cannot sending merge to hosting server of the regions \" +\n              getRegionsToMergeListEncodedNameString() + \" as the server is unknown\");\n          }\n          return;\n        }\n        // TODO: the following RPC call is not idempotent.  Multiple calls (eg. after master\n        // failover, re-execute this step) could result in some exception thrown that does not\n        // paint the correct picture.  This behavior is on-par with old releases.  Improvement\n        // could happen in the future.\n        env.getMasterServices().getServerManager().sendRegionsMerge(\n          getServerName(env),\n          regionsToMerge[0],\n          regionsToMerge[1],\n          forcible,\n          getUser());\n        LOG.info(\"Sent merge to server \" + getServerName(env) + \" for region \" +\n            getRegionsToMergeListEncodedNameString() + \", focible=\" + forcible);\n        return;\n      } catch (RegionOpeningException roe) {\n        // Do a retry since region should be online on RS immediately\n        LOG.warn(\"Failed mergering regions in \" + getServerName(env) + \", retrying...\", roe);\n      } catch (Exception ie) {\n        LOG.warn(\"Failed sending merge to \" + getServerName(env) + \" for regions \" +\n            getRegionsToMergeListEncodedNameString() + \", focible=\" + forcible, ie);\n        return;\n      }\n    } while ((duration = EnvironmentEdgeManager.currentTime() - startTime) <= getTimeout(env));\n\n    // If we reaches here, it means that we get timed out.\n    String msg = \"Failed sending merge to \" + getServerName(env) + \" after \" + duration + \"ms\";\n    LOG.warn(msg);\n    throw new IOException(msg);\n  }",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 +\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "  /**\n   * Do the real merge operation in the region server that hosts regions\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private void doMergeInRS(final MasterProcedureEnv env) throws IOException {\n    long duration = 0;\n    long startTime = EnvironmentEdgeManager.currentTime();\n    do {\n      try {\n        if (getServerName(env) == null) {\n          // The merge probably already happen. Check\n          RegionState regionState = getAssignmentManager(env).getRegionStates().getRegionState(\n            regionsToMerge[0].getEncodedName());\n          if (regionState.isMerging() || regionState.isMerged()) {\n            LOG.info(\"Merge regions \" +  getRegionsToMergeListEncodedNameString() +\n              \" is in progress or completed.  No need to send a new request.\");\n          } else {\n            LOG.warn(\"Cannot sending merge to hosting server of the regions \" +\n              getRegionsToMergeListEncodedNameString() + \" as the server is unknown\");\n          }\n          return;\n        }\n        // TODO: the following RPC call is not idempotent.  Multiple calls (eg. after master\n        // failover, re-execute this step) could result in some exception thrown that does not\n        // paint the correct picture.  This behavior is on-par with old releases.  Improvement\n        // could happen in the future.\n        env.getMasterServices().getServerManager().sendRegionsMerge(\n          getServerName(env),\n          regionsToMerge[0],\n          regionsToMerge[1],\n          forcible,\n          getUser());\n        LOG.info(\"Sent merge to server \" + getServerName(env) + \" for region \" +\n            getRegionsToMergeListEncodedNameString() + \", forcible=\" + forcible);\n        return;\n      } catch (RegionOpeningException roe) {\n        // Do a retry since region should be online on RS immediately\n        LOG.warn(\"Failed mergering regions in \" + getServerName(env) + \", retrying...\", roe);\n      } catch (Exception ie) {\n        LOG.warn(\"Failed sending merge to \" + getServerName(env) + \" for regions \" +\n            getRegionsToMergeListEncodedNameString() + \", forcible=\" + forcible, ie);\n        return;\n      }\n    } while ((duration = EnvironmentEdgeManager.currentTime() - startTime) <= getTimeout(env));\n\n    // If we reaches here, it means that we get timed out.\n    String msg = \"Failed sending merge to \" + getServerName(env) + \" after \" + duration + \"ms\";\n    LOG.warn(msg);\n    throw new IOException(msg);\n  }"
        ],
        [
            "RegionStates::getRegionInfo(byte)",
            "1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094 -\n1095  \n1096  \n1097  \n1098  ",
            "  /**\n   * Get the HRegionInfo from cache, if not there, from the hbase:meta table.\n   * Be careful. Does RPC. Do not hold a lock or synchronize when you call this method.\n   * @param  regionName\n   * @return HRegionInfo for the region\n   */\n  @SuppressWarnings(\"deprecation\")\n  protected HRegionInfo getRegionInfo(final byte [] regionName) {\n    String encodedName = HRegionInfo.encodeRegionName(regionName);\n    RegionState regionState = getRegionState(encodedName);\n    if (regionState != null) {\n      return regionState.getRegion();\n    }\n\n    try {\n      Pair<HRegionInfo, ServerName> p =\n        MetaTableAccessor.getRegion(server.getConnection(), regionName);\n      HRegionInfo hri = p == null ? null : p.getFirst();\n      if (hri != null) {\n        createRegionState(hri);\n      }\n      return hri;\n    } catch (IOException e) {\n      server.abort(\"Aborting because error occoured while reading \"\n        + Bytes.toStringBinary(regionName) + \" from hbase:meta\", e);\n      return null;\n    }\n  }",
            "1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094 +\n1095  \n1096  \n1097  \n1098  ",
            "  /**\n   * Get the HRegionInfo from cache, if not there, from the hbase:meta table.\n   * Be careful. Does RPC. Do not hold a lock or synchronize when you call this method.\n   * @param  regionName\n   * @return HRegionInfo for the region\n   */\n  @SuppressWarnings(\"deprecation\")\n  protected HRegionInfo getRegionInfo(final byte [] regionName) {\n    String encodedName = HRegionInfo.encodeRegionName(regionName);\n    RegionState regionState = getRegionState(encodedName);\n    if (regionState != null) {\n      return regionState.getRegion();\n    }\n\n    try {\n      Pair<HRegionInfo, ServerName> p =\n        MetaTableAccessor.getRegion(server.getConnection(), regionName);\n      HRegionInfo hri = p == null ? null : p.getFirst();\n      if (hri != null) {\n        createRegionState(hri);\n      }\n      return hri;\n    } catch (IOException e) {\n      server.abort(\"Aborting because error occurred while reading \"\n        + Bytes.toStringBinary(regionName) + \" from hbase:meta\", e);\n      return null;\n    }\n  }"
        ],
        [
            "HFileReplicator::doBulkLoad(LoadIncrementalHFiles,Table,Deque,RegionLocator,int)",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "  private void doBulkLoad(LoadIncrementalHFiles loadHFiles, Table table,\n      Deque<LoadQueueItem> queue, RegionLocator locator, int maxRetries) throws IOException {\n    int count = 0;\n    Pair<byte[][], byte[][]> startEndKeys;\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      startEndKeys = locator.getStartEndKeys();\n      if (count != 0) {\n        LOG.warn(\"Error occured while replicating HFiles, retry attempt \" + count + \" with \"\n            + queue.size() + \" files still remaining to replicate.\");\n      }\n\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count\n            + \" times without completing, bailing out.\");\n      }\n      count++;\n\n      // Try bulk load\n      loadHFiles.loadHFileQueue(table, connection, queue, startEndKeys);\n    }\n  }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "  private void doBulkLoad(LoadIncrementalHFiles loadHFiles, Table table,\n      Deque<LoadQueueItem> queue, RegionLocator locator, int maxRetries) throws IOException {\n    int count = 0;\n    Pair<byte[][], byte[][]> startEndKeys;\n    while (!queue.isEmpty()) {\n      // need to reload split keys each iteration.\n      startEndKeys = locator.getStartEndKeys();\n      if (count != 0) {\n        LOG.warn(\"Error occurred while replicating HFiles, retry attempt \" + count + \" with \"\n            + queue.size() + \" files still remaining to replicate.\");\n      }\n\n      if (maxRetries != 0 && count >= maxRetries) {\n        throw new IOException(\"Retry attempted \" + count\n            + \" times without completing, bailing out.\");\n      }\n      count++;\n\n      // Try bulk load\n      loadHFiles.loadHFileQueue(table, connection, queue, startEndKeys);\n    }\n  }"
        ],
        [
            "HDFSBlocksDistribution::toString()",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  ",
            "  /**\n   * @see java.lang.Object#toString()\n   */\n  @Override\n  public synchronized String toString() {\n    return \"number of unique hosts in the disribution=\" +\n      this.hostAndWeights.size();\n  }",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117  \n 118  ",
            "  /**\n   * @see java.lang.Object#toString()\n   */\n  @Override\n  public synchronized String toString() {\n    return \"number of unique hosts in the distribution=\" +\n      this.hostAndWeights.size();\n  }"
        ],
        [
            "AssignmentVerificationReport::print(boolean)",
            " 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  ",
            "  public void print(boolean isDetailMode) {\n    if (!isFilledUp) {\n      System.err.println(\"[Error] Region assignment verfication report\" +\n          \"hasn't been filled up\");\n    }\n    DecimalFormat df = new java.text.DecimalFormat( \"#.##\");\n\n    // Print some basic information\n    System.out.println(\"Region Assignment Verification for Table: \" + tableName +\n        \"\\n\\tTotal regions : \" + totalRegions);\n\n    // Print the number of regions on each kinds of the favored nodes\n    System.out.println(\"\\tTotal regions on favored nodes \" +\n        totalFavoredAssignments);\n    for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {\n      System.out.println(\"\\t\\tTotal regions on \"+ p.toString() +\n          \" region servers: \" + favoredNodes[p.ordinal()]);\n    }\n    // Print the number of regions in each kinds of invalid assignment\n    System.out.println(\"\\tTotal unassigned regions: \" +\n        unAssignedRegionsList.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : unAssignedRegionsList) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    System.out.println(\"\\tTotal regions NOT on favored nodes: \" +\n        nonFavoredAssignedRegionList.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : nonFavoredAssignedRegionList) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    System.out.println(\"\\tTotal regions without favored nodes: \" +\n        regionsWithoutValidFavoredNodes.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : regionsWithoutValidFavoredNodes) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    // Print the locality information if enabled\n    if (this.enforceLocality && totalRegions != 0) {\n      // Print the actual locality for this table\n      float actualLocality = 100 *\n        this.actualLocalitySummary / (float) totalRegions;\n      System.out.println(\"\\n\\tThe actual avg locality is \" +\n          df.format(actualLocality) + \" %\");\n\n      // Print the expected locality if regions are placed on the each kinds of\n      // favored nodes\n      for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {\n        float avgLocality = 100 *\n          (favoredNodesLocalitySummary[p.ordinal()] / (float) totalRegions);\n        System.out.println(\"\\t\\tThe expected avg locality if all regions\" +\n            \" on the \" + p.toString() + \" region servers: \"\n            + df.format(avgLocality) + \" %\");\n      }\n    }\n\n    // Print the region balancing information\n    System.out.println(\"\\n\\tTotal hosting region servers: \" +\n        totalRegionServers);\n    // Print the region balance information\n    if (totalRegionServers != 0) {\n      System.out.println(\n          \"\\tAvg dispersion num: \" +df.format(avgDispersionNum) +\n          \" hosts;\\tMax dispersion num: \" + df.format(maxDispersionNum) +\n          \" hosts;\\tMin dispersion num: \" + df.format(minDispersionNum) +\n          \" hosts;\");\n\n      System.out.println(\"\\t\\tThe number of the region servers with the max\" +\n          \" dispersion num: \" + this.maxDispersionNumServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionNumServerSet);\n      }\n\n      System.out.println(\"\\t\\tThe number of the region servers with the min\" +\n          \" dispersion num: \" + this.minDispersionNumServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionNumServerSet);\n      }\n\n      System.out.println(\n          \"\\tAvg dispersion score: \" + df.format(avgDispersionScore) +\n          \";\\tMax dispersion score: \" + df.format(maxDispersionScore) +\n          \";\\tMin dispersion score: \" + df.format(minDispersionScore) + \";\");\n\n      System.out.println(\"\\t\\tThe number of the region servers with the max\" +\n          \" dispersion score: \" + this.maxDispersionScoreServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionScoreServerSet);\n      }\n\n      System.out.println(\"\\t\\tThe number of the region servers with the min\" +\n          \" dispersion score: \" + this.minDispersionScoreServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(minDispersionScoreServerSet);\n      }\n\n      System.out.println(\n          \"\\tAvg regions/region server: \" + df.format(avgRegionsOnRS) +\n          \";\\tMax regions/region server: \" + maxRegionsOnRS +\n          \";\\tMin regions/region server: \" + minRegionsOnRS + \";\");\n\n      // Print the details about the most loaded region servers\n      System.out.println(\"\\t\\tThe number of the most loaded region servers: \"\n          + mostLoadedRSSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(mostLoadedRSSet);\n      }\n\n      // Print the details about the least loaded region servers\n      System.out.println(\"\\t\\tThe number of the least loaded region servers: \"\n          + leastLoadedRSSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(leastLoadedRSSet);\n      }\n    }\n    System.out.println(\"==============================\");\n  }",
            " 461  \n 462  \n 463 +\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  ",
            "  public void print(boolean isDetailMode) {\n    if (!isFilledUp) {\n      System.err.println(\"[Error] Region assignment verification report\" +\n          \"hasn't been filled up\");\n    }\n    DecimalFormat df = new java.text.DecimalFormat( \"#.##\");\n\n    // Print some basic information\n    System.out.println(\"Region Assignment Verification for Table: \" + tableName +\n        \"\\n\\tTotal regions : \" + totalRegions);\n\n    // Print the number of regions on each kinds of the favored nodes\n    System.out.println(\"\\tTotal regions on favored nodes \" +\n        totalFavoredAssignments);\n    for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {\n      System.out.println(\"\\t\\tTotal regions on \"+ p.toString() +\n          \" region servers: \" + favoredNodes[p.ordinal()]);\n    }\n    // Print the number of regions in each kinds of invalid assignment\n    System.out.println(\"\\tTotal unassigned regions: \" +\n        unAssignedRegionsList.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : unAssignedRegionsList) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    System.out.println(\"\\tTotal regions NOT on favored nodes: \" +\n        nonFavoredAssignedRegionList.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : nonFavoredAssignedRegionList) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    System.out.println(\"\\tTotal regions without favored nodes: \" +\n        regionsWithoutValidFavoredNodes.size());\n    if (isDetailMode) {\n      for (HRegionInfo region : regionsWithoutValidFavoredNodes) {\n        System.out.println(\"\\t\\t\" + region.getRegionNameAsString());\n      }\n    }\n\n    // Print the locality information if enabled\n    if (this.enforceLocality && totalRegions != 0) {\n      // Print the actual locality for this table\n      float actualLocality = 100 *\n        this.actualLocalitySummary / (float) totalRegions;\n      System.out.println(\"\\n\\tThe actual avg locality is \" +\n          df.format(actualLocality) + \" %\");\n\n      // Print the expected locality if regions are placed on the each kinds of\n      // favored nodes\n      for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {\n        float avgLocality = 100 *\n          (favoredNodesLocalitySummary[p.ordinal()] / (float) totalRegions);\n        System.out.println(\"\\t\\tThe expected avg locality if all regions\" +\n            \" on the \" + p.toString() + \" region servers: \"\n            + df.format(avgLocality) + \" %\");\n      }\n    }\n\n    // Print the region balancing information\n    System.out.println(\"\\n\\tTotal hosting region servers: \" +\n        totalRegionServers);\n    // Print the region balance information\n    if (totalRegionServers != 0) {\n      System.out.println(\n          \"\\tAvg dispersion num: \" +df.format(avgDispersionNum) +\n          \" hosts;\\tMax dispersion num: \" + df.format(maxDispersionNum) +\n          \" hosts;\\tMin dispersion num: \" + df.format(minDispersionNum) +\n          \" hosts;\");\n\n      System.out.println(\"\\t\\tThe number of the region servers with the max\" +\n          \" dispersion num: \" + this.maxDispersionNumServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionNumServerSet);\n      }\n\n      System.out.println(\"\\t\\tThe number of the region servers with the min\" +\n          \" dispersion num: \" + this.minDispersionNumServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionNumServerSet);\n      }\n\n      System.out.println(\n          \"\\tAvg dispersion score: \" + df.format(avgDispersionScore) +\n          \";\\tMax dispersion score: \" + df.format(maxDispersionScore) +\n          \";\\tMin dispersion score: \" + df.format(minDispersionScore) + \";\");\n\n      System.out.println(\"\\t\\tThe number of the region servers with the max\" +\n          \" dispersion score: \" + this.maxDispersionScoreServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(maxDispersionScoreServerSet);\n      }\n\n      System.out.println(\"\\t\\tThe number of the region servers with the min\" +\n          \" dispersion score: \" + this.minDispersionScoreServerSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(minDispersionScoreServerSet);\n      }\n\n      System.out.println(\n          \"\\tAvg regions/region server: \" + df.format(avgRegionsOnRS) +\n          \";\\tMax regions/region server: \" + maxRegionsOnRS +\n          \";\\tMin regions/region server: \" + minRegionsOnRS + \";\");\n\n      // Print the details about the most loaded region servers\n      System.out.println(\"\\t\\tThe number of the most loaded region servers: \"\n          + mostLoadedRSSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(mostLoadedRSSet);\n      }\n\n      // Print the details about the least loaded region servers\n      System.out.println(\"\\t\\tThe number of the least loaded region servers: \"\n          + leastLoadedRSSet.size());\n      if (isDetailMode) {\n        printHServerAddressSet(leastLoadedRSSet);\n      }\n    }\n    System.out.println(\"==============================\");\n  }"
        ]
    ],
    "f64512bee11112454fc3728fe5d344a838781e26": [
        [
            "IncreasingToUpperBoundRegionSplitPolicy::shouldSplit()",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88 -\n  89 -\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "  @Override\n  protected boolean shouldSplit() {\n    boolean force = region.shouldForceSplit();\n    boolean foundABigStore = false;\n    // Get count of regions that have the same common table as this.region\n    int tableRegionsCount = getCountOfCommonTableRegions();\n    // Get size to check\n    long sizeToCheck = getSizeToCheck(tableRegionsCount);\n\n    for (Store store : region.getStores()) {\n      // If any of the stores is unable to split (eg they contain reference files)\n      // then don't split\n      if (!store.canSplit()) {\n        return false;\n      }\n\n      // Mark if any store is big enough\n      long size = store.getSize();\n      if (size > sizeToCheck) {\n        LOG.debug(\"ShouldSplit because \" + store.getColumnFamilyName() + \" size=\" + size\n                  + \", sizeToCheck=\" + sizeToCheck + \", regionsWithCommonTable=\"\n                  + tableRegionsCount);\n        foundABigStore = true;\n      }\n    }\n\n    return foundABigStore | force;\n  }",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89 +\n  90 +\n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "  @Override\n  protected boolean shouldSplit() {\n    boolean force = region.shouldForceSplit();\n    boolean foundABigStore = false;\n    // Get count of regions that have the same common table as this.region\n    int tableRegionsCount = getCountOfCommonTableRegions();\n    // Get size to check\n    long sizeToCheck = getSizeToCheck(tableRegionsCount);\n\n    for (Store store : region.getStores()) {\n      // If any of the stores is unable to split (eg they contain reference files)\n      // then don't split\n      if (!store.canSplit()) {\n        return false;\n      }\n\n      // Mark if any store is big enough\n      long size = store.getSize();\n      if (size > sizeToCheck) {\n        LOG.debug(\"ShouldSplit because \" + store.getColumnFamilyName() +\n          \" size=\" + StringUtils.humanSize(size) +\n          \", sizeToCheck=\" + StringUtils.humanSize(sizeToCheck) +\n          \", regionsWithCommonTable=\" + tableRegionsCount);\n        foundABigStore = true;\n      }\n    }\n\n    return foundABigStore | force;\n  }"
        ],
        [
            "HStore::logCompactionEndMessage(CompactionRequest,List,long,long)",
            "1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447 -\n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  ",
            "  /**\n   * Log a very elaborate compaction completion message.\n   * @param cr Request.\n   * @param sfs Resulting files.\n   * @param compactionStartTime Start time.\n   */\n  private void logCompactionEndMessage(\n      CompactionRequest cr, List<StoreFile> sfs, long now, long compactionStartTime) {\n    StringBuilder message = new StringBuilder(\n      \"Completed\" + (cr.isMajor() ? \" major\" : \"\") + \" compaction of \"\n      + cr.getFiles().size() + (cr.isAllFiles() ? \" (all)\" : \"\") + \" file(s) in \"\n      + this + \" of \" + this.getRegionInfo().getRegionNameAsString() + \" into \");\n    if (sfs.isEmpty()) {\n      message.append(\"none, \");\n    } else {\n      for (StoreFile sf: sfs) {\n        message.append(sf.getPath().getName());\n        message.append(\"(size=\");\n        message.append(TraditionalBinaryPrefix.long2String(sf.getReader().length(), \"\", 1));\n        message.append(\"), \");\n      }\n    }\n    message.append(\"total size for store is \")\n      .append(StringUtils.TraditionalBinaryPrefix.long2String(storeSize, \"\", 1))\n      .append(\". This selection was in queue for \")\n      .append(StringUtils.formatTimeDiff(compactionStartTime, cr.getSelectionTime()))\n      .append(\", and took \").append(StringUtils.formatTimeDiff(now, compactionStartTime))\n      .append(\" to execute.\");\n    LOG.info(message.toString());\n    if (LOG.isTraceEnabled()) {\n      int fileCount = storeEngine.getStoreFileManager().getStorefileCount();\n      long resultSize = 0;\n      for (StoreFile sf : sfs) {\n        resultSize += sf.getReader().length();\n      }\n      String traceMessage = \"COMPACTION start,end,size out,files in,files out,store size,\"\n        + \"store files [\" + compactionStartTime + \",\" + now + \",\" + resultSize + \",\"\n          + cr.getFiles().size() + \",\" + sfs.size() + \",\" +  storeSize + \",\" + fileCount + \"]\";\n      LOG.trace(traceMessage);\n    }\n  }",
            "1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447 +\n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  ",
            "  /**\n   * Log a very elaborate compaction completion message.\n   * @param cr Request.\n   * @param sfs Resulting files.\n   * @param compactionStartTime Start time.\n   */\n  private void logCompactionEndMessage(\n      CompactionRequest cr, List<StoreFile> sfs, long now, long compactionStartTime) {\n    StringBuilder message = new StringBuilder(\n      \"Completed\" + (cr.isMajor() ? \" major\" : \"\") + \" compaction of \"\n      + cr.getFiles().size() + (cr.isAllFiles() ? \" (all)\" : \"\") + \" file(s) in \"\n      + this + \" of \" + this.getRegionInfo().getShortNameToLog() + \" into \");\n    if (sfs.isEmpty()) {\n      message.append(\"none, \");\n    } else {\n      for (StoreFile sf: sfs) {\n        message.append(sf.getPath().getName());\n        message.append(\"(size=\");\n        message.append(TraditionalBinaryPrefix.long2String(sf.getReader().length(), \"\", 1));\n        message.append(\"), \");\n      }\n    }\n    message.append(\"total size for store is \")\n      .append(StringUtils.TraditionalBinaryPrefix.long2String(storeSize, \"\", 1))\n      .append(\". This selection was in queue for \")\n      .append(StringUtils.formatTimeDiff(compactionStartTime, cr.getSelectionTime()))\n      .append(\", and took \").append(StringUtils.formatTimeDiff(now, compactionStartTime))\n      .append(\" to execute.\");\n    LOG.info(message.toString());\n    if (LOG.isTraceEnabled()) {\n      int fileCount = storeEngine.getStoreFileManager().getStorefileCount();\n      long resultSize = 0;\n      for (StoreFile sf : sfs) {\n        resultSize += sf.getReader().length();\n      }\n      String traceMessage = \"COMPACTION start,end,size out,files in,files out,store size,\"\n        + \"store files [\" + compactionStartTime + \",\" + now + \",\" + resultSize + \",\"\n          + cr.getFiles().size() + \",\" + sfs.size() + \",\" +  storeSize + \",\" + fileCount + \"]\";\n      LOG.trace(traceMessage);\n    }\n  }"
        ],
        [
            "StoreFileInfo::getReferredToFile(Path)",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 -\n 455 -\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  ",
            "  public static Path getReferredToFile(final Path p) {\n    Matcher m = REF_NAME_PATTERN.matcher(p.getName());\n    if (m == null || !m.matches()) {\n      LOG.warn(\"Failed match of store file name \" + p.toString());\n      throw new IllegalArgumentException(\"Failed match of store file name \" +\n          p.toString());\n    }\n\n    // Other region name is suffix on the passed Reference file name\n    String otherRegion = m.group(2);\n    // Tabledir is up two directories from where Reference was written.\n    Path tableDir = p.getParent().getParent().getParent();\n    String nameStrippedOfSuffix = m.group(1);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"reference '\" + p + \"' to region=\" + otherRegion\n        + \" hfile=\" + nameStrippedOfSuffix);\n    }\n\n    // Build up new path with the referenced region in place of our current\n    // region in the reference path.  Also strip regionname suffix from name.\n    return new Path(new Path(new Path(tableDir, otherRegion),\n      p.getParent().getName()), nameStrippedOfSuffix);\n  }",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 +\n 455 +\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  ",
            "  public static Path getReferredToFile(final Path p) {\n    Matcher m = REF_NAME_PATTERN.matcher(p.getName());\n    if (m == null || !m.matches()) {\n      LOG.warn(\"Failed match of store file name \" + p.toString());\n      throw new IllegalArgumentException(\"Failed match of store file name \" +\n          p.toString());\n    }\n\n    // Other region name is suffix on the passed Reference file name\n    String otherRegion = m.group(2);\n    // Tabledir is up two directories from where Reference was written.\n    Path tableDir = p.getParent().getParent().getParent();\n    String nameStrippedOfSuffix = m.group(1);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"reference '\" + p + \"' to region=\" + otherRegion\n        + \" hfile=\" + nameStrippedOfSuffix);\n    }\n\n    // Build up new path with the referenced region in place of our current\n    // region in the reference path.  Also strip regionname suffix from name.\n    return new Path(new Path(new Path(tableDir, otherRegion),\n      p.getParent().getName()), nameStrippedOfSuffix);\n  }"
        ],
        [
            "SplitTableRegionProcedure::splitStoreFiles(MasterProcedureEnv,HRegionFileSystem)",
            " 482  \n 483  \n 484  \n 485  \n 486  \n 487 -\n 488 -\n 489  \n 490  \n 491  \n 492 -\n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502 -\n 503 -\n 504 -\n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 -\n 517 -\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524 -\n 525 -\n 526 -\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573 -\n 574 -\n 575  \n 576  \n 577  ",
            "  /**\n   * Create Split directory\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private Pair<Integer, Integer> splitStoreFiles(\n      final MasterProcedureEnv env,\n      final HRegionFileSystem regionFs) throws IOException {\n    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n    final Configuration conf = env.getMasterConfiguration();\n\n    // The following code sets up a thread pool executor with as many slots as\n    // there's files to split. It then fires up everything, waits for\n    // completion and finally checks for any exception\n    //\n    // Note: splitStoreFiles creates daughter region dirs under the parent splits dir\n    // Nothing to unroll here if failure -- re-run createSplitsDir will\n    // clean this up.\n    int nbFiles = 0;\n    for (String family: regionFs.getFamilies()) {\n      final Collection<StoreFileInfo> storeFiles = regionFs.getStoreFiles(family);\n      if (storeFiles != null) {\n        nbFiles += storeFiles.size();\n      }\n    }\n    if (nbFiles == 0) {\n      // no file needs to be splitted.\n      return new Pair<Integer, Integer>(0,0);\n    }\n    // Max #threads is the smaller of the number of storefiles or the default max determined above.\n    int maxThreads = Math.min(\n      conf.getInt(HConstants.REGION_SPLIT_THREADS_MAX,\n        conf.getInt(HStore.BLOCKING_STOREFILES_KEY, HStore.DEFAULT_BLOCKING_STOREFILE_COUNT)),\n      nbFiles);\n    LOG.info(\"pid=\" + getProcId() + \" preparing to split \" + nbFiles + \" storefiles for region \" +\n      getParentRegion().getShortNameToLog() + \" using \" + maxThreads + \" threads\");\n    final ExecutorService threadPool = Executors.newFixedThreadPool(\n      maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n    final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n\n    // Split each store file.\n    final TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n    for (String family: regionFs.getFamilies()) {\n      final ColumnFamilyDescriptor hcd = htd.getColumnFamily(family.getBytes());\n      final Collection<StoreFileInfo> storeFiles = regionFs.getStoreFiles(family);\n      if (storeFiles != null && storeFiles.size() > 0) {\n        final CacheConfig cacheConf = new CacheConfig(conf, hcd);\n        for (StoreFileInfo storeFileInfo: storeFiles) {\n          StoreFileSplitter sfs =\n              new StoreFileSplitter(regionFs, family.getBytes(), new HStoreFile(mfs.getFileSystem(),\n                  storeFileInfo, conf, cacheConf, hcd.getBloomFilterType(), true));\n          futures.add(threadPool.submit(sfs));\n        }\n      }\n    }\n    // Shutdown the pool\n    threadPool.shutdown();\n\n    // Wait for all the tasks to finish\n    long fileSplitTimeout = conf.getLong(\"hbase.master.fileSplitTimeout\", 30000);\n    try {\n      boolean stillRunning = !threadPool.awaitTermination(fileSplitTimeout, TimeUnit.MILLISECONDS);\n      if (stillRunning) {\n        threadPool.shutdownNow();\n        // wait for the thread to shutdown completely.\n        while (!threadPool.isTerminated()) {\n          Thread.sleep(50);\n        }\n        throw new IOException(\"Took too long to split the\" +\n            \" files and create the references, aborting split\");\n      }\n    } catch (InterruptedException e) {\n      throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n    }\n\n    int daughterA = 0;\n    int daughterB = 0;\n    // Look for any exception\n    for (Future<Pair<Path, Path>> future : futures) {\n      try {\n        Pair<Path, Path> p = future.get();\n        daughterA += p.getFirst() != null ? 1 : 0;\n        daughterB += p.getSecond() != null ? 1 : 0;\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pid=\" + getProcId() + \" split storefiles for region \" + getParentRegion().getShortNameToLog() +\n          \" Daughter A: \" + daughterA + \" storefiles, Daughter B: \" + daughterB + \" storefiles.\");\n    }\n    return new Pair<Integer, Integer>(daughterA, daughterB);\n  }",
            " 478  \n 479  \n 480  \n 481  \n 482  \n 483 +\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495 +\n 496 +\n 497  \n 498 +\n 499 +\n 500 +\n 501 +\n 502 +\n 503 +\n 504 +\n 505 +\n 506 +\n 507 +\n 508 +\n 509 +\n 510 +\n 511 +\n 512 +\n 513 +\n 514 +\n 515 +\n 516 +\n 517 +\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529 +\n 530 +\n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 +\n 538 +\n 539 +\n 540 +\n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587 +\n 588 +\n 589 +\n 590 +\n 591  \n 592  \n 593  ",
            "  /**\n   * Create Split directory\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private Pair<Integer, Integer> splitStoreFiles(final MasterProcedureEnv env,\n      final HRegionFileSystem regionFs) throws IOException {\n    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n    final Configuration conf = env.getMasterConfiguration();\n    // The following code sets up a thread pool executor with as many slots as\n    // there's files to split. It then fires up everything, waits for\n    // completion and finally checks for any exception\n    //\n    // Note: splitStoreFiles creates daughter region dirs under the parent splits dir\n    // Nothing to unroll here if failure -- re-run createSplitsDir will\n    // clean this up.\n    int nbFiles = 0;\n    final Map<String, Collection<StoreFileInfo>> files =\n      new HashMap<String, Collection<StoreFileInfo>>(regionFs.getFamilies().size());\n    for (String family: regionFs.getFamilies()) {\n      Collection<StoreFileInfo> sfis = regionFs.getStoreFiles(family);\n      if (sfis == null) continue;\n      Collection<StoreFileInfo> filteredSfis = null;\n      for (StoreFileInfo sfi: sfis) {\n        // Filter. There is a lag cleaning up compacted reference files. They get cleared\n        // after a delay in case outstanding Scanners still have references. Because of this,\n        // the listing of the Store content may have straggler reference files. Skip these.\n        // It should be safe to skip references at this point because we checked above with\n        // the region if it thinks it is splittable and if we are here, it thinks it is\n        // splitable.\n        if (sfi.isReference()) {\n          LOG.info(\"Skipping split of \" + sfi + \"; presuming ready for archiving.\");\n          continue;\n        }\n        if (filteredSfis == null) {\n          filteredSfis = new ArrayList<StoreFileInfo>(sfis.size());\n          files.put(family, filteredSfis);\n        }\n        filteredSfis.add(sfi);\n        nbFiles++;\n      }\n    }\n    if (nbFiles == 0) {\n      // no file needs to be splitted.\n      return new Pair<Integer, Integer>(0,0);\n    }\n    // Max #threads is the smaller of the number of storefiles or the default max determined above.\n    int maxThreads = Math.min(\n      conf.getInt(HConstants.REGION_SPLIT_THREADS_MAX,\n        conf.getInt(HStore.BLOCKING_STOREFILES_KEY, HStore.DEFAULT_BLOCKING_STOREFILE_COUNT)),\n      nbFiles);\n    LOG.info(\"pid=\" + getProcId() + \" splitting \" + nbFiles + \" storefiles, region=\" +\n      getParentRegion().getShortNameToLog() + \", threads=\" + maxThreads);\n    final ExecutorService threadPool = Executors.newFixedThreadPool(\n      maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n    final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n\n    // Split each store file.\n    final TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n    for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n      byte [] familyName = Bytes.toBytes(e.getKey());\n      final HColumnDescriptor hcd = htd.getFamily(familyName);\n      final Collection<StoreFileInfo> storeFiles = e.getValue();\n      if (storeFiles != null && storeFiles.size() > 0) {\n        final CacheConfig cacheConf = new CacheConfig(conf, hcd);\n        for (StoreFileInfo storeFileInfo: storeFiles) {\n          StoreFileSplitter sfs =\n              new StoreFileSplitter(regionFs, family.getBytes(), new HStoreFile(mfs.getFileSystem(),\n                  storeFileInfo, conf, cacheConf, hcd.getBloomFilterType(), true));\n          futures.add(threadPool.submit(sfs));\n        }\n      }\n    }\n    // Shutdown the pool\n    threadPool.shutdown();\n\n    // Wait for all the tasks to finish\n    long fileSplitTimeout = conf.getLong(\"hbase.master.fileSplitTimeout\", 30000);\n    try {\n      boolean stillRunning = !threadPool.awaitTermination(fileSplitTimeout, TimeUnit.MILLISECONDS);\n      if (stillRunning) {\n        threadPool.shutdownNow();\n        // wait for the thread to shutdown completely.\n        while (!threadPool.isTerminated()) {\n          Thread.sleep(50);\n        }\n        throw new IOException(\"Took too long to split the\" +\n            \" files and create the references, aborting split\");\n      }\n    } catch (InterruptedException e) {\n      throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n    }\n\n    int daughterA = 0;\n    int daughterB = 0;\n    // Look for any exception\n    for (Future<Pair<Path, Path>> future : futures) {\n      try {\n        Pair<Path, Path> p = future.get();\n        daughterA += p.getFirst() != null ? 1 : 0;\n        daughterB += p.getSecond() != null ? 1 : 0;\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pid=\" + getProcId() + \" split storefiles for region \" +\n        getParentRegion().getShortNameToLog() +\n          \" Daughter A: \" + daughterA + \" storefiles, Daughter B: \" +\n          daughterB + \" storefiles.\");\n    }\n    return new Pair<Integer, Integer>(daughterA, daughterB);\n  }"
        ],
        [
            "RSRpcServices::getRegionScanner(ScanRequest)",
            "2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803  \n2804  \n2805  \n2806  \n2807  \n2808  \n2809  \n2810 -\n2811 -\n2812  \n2813  \n2814  \n2815  \n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  ",
            "  private RegionScannerHolder getRegionScanner(ScanRequest request) throws IOException {\n    String scannerName = Long.toString(request.getScannerId());\n    RegionScannerHolder rsh = scanners.get(scannerName);\n    if (rsh == null) {\n      // just ignore the next or close request if scanner does not exists.\n      if (closedScanners.getIfPresent(scannerName) != null) {\n        throw SCANNER_ALREADY_CLOSED;\n      } else {\n        LOG.warn(\"Client tried to access missing scanner \" + scannerName);\n        throw new UnknownScannerException(\n            \"Unknown scanner '\" + scannerName + \"'. This can happen due to any of the following \" +\n                \"reasons: a) Scanner id given is wrong, b) Scanner lease expired because of \" +\n                \"long wait between consecutive client checkins, c) Server may be closing down, \" +\n                \"d) RegionServer restart during upgrade.\\nIf the issue is due to reason (b), a \" +\n                \"possible fix would be increasing the value of\" +\n                \"'hbase.client.scanner.timeout.period' configuration.\");\n      }\n    }\n    HRegionInfo hri = rsh.s.getRegionInfo();\n    // Yes, should be the same instance\n    if (regionServer.getOnlineRegion(hri.getRegionName()) != rsh.r) {\n      String msg = \"Region was re-opened after the scanner\" + scannerName + \" was created: \"\n          + hri.getRegionNameAsString();\n      LOG.warn(msg + \", closing...\");\n      scanners.remove(scannerName);\n      try {\n        rsh.s.close();\n      } catch (IOException e) {\n        LOG.warn(\"Getting exception closing \" + scannerName, e);\n      } finally {\n        try {\n          regionServer.leases.cancelLease(scannerName);\n        } catch (LeaseException e) {\n          LOG.warn(\"Getting exception closing \" + scannerName, e);\n        }\n      }\n      throw new NotServingRegionException(msg);\n    }\n    return rsh;\n  }",
            "2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803  \n2804  \n2805  \n2806  \n2807  \n2808  \n2809  \n2810 +\n2811 +\n2812  \n2813  \n2814  \n2815  \n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  ",
            "  private RegionScannerHolder getRegionScanner(ScanRequest request) throws IOException {\n    String scannerName = Long.toString(request.getScannerId());\n    RegionScannerHolder rsh = scanners.get(scannerName);\n    if (rsh == null) {\n      // just ignore the next or close request if scanner does not exists.\n      if (closedScanners.getIfPresent(scannerName) != null) {\n        throw SCANNER_ALREADY_CLOSED;\n      } else {\n        LOG.warn(\"Client tried to access missing scanner \" + scannerName);\n        throw new UnknownScannerException(\n            \"Unknown scanner '\" + scannerName + \"'. This can happen due to any of the following \" +\n                \"reasons: a) Scanner id given is wrong, b) Scanner lease expired because of \" +\n                \"long wait between consecutive client checkins, c) Server may be closing down, \" +\n                \"d) RegionServer restart during upgrade.\\nIf the issue is due to reason (b), a \" +\n                \"possible fix would be increasing the value of\" +\n                \"'hbase.client.scanner.timeout.period' configuration.\");\n      }\n    }\n    HRegionInfo hri = rsh.s.getRegionInfo();\n    // Yes, should be the same instance\n    if (regionServer.getOnlineRegion(hri.getRegionName()) != rsh.r) {\n      String msg = \"Region has changed on the scanner \" + scannerName + \": regionName=\"\n          + hri.getRegionName() + \", scannerRegionName=\" + rsh.r;\n      LOG.warn(msg + \", closing...\");\n      scanners.remove(scannerName);\n      try {\n        rsh.s.close();\n      } catch (IOException e) {\n        LOG.warn(\"Getting exception closing \" + scannerName, e);\n      } finally {\n        try {\n          regionServer.leases.cancelLease(scannerName);\n        } catch (LeaseException e) {\n          LOG.warn(\"Getting exception closing \" + scannerName, e);\n        }\n      }\n      throw new NotServingRegionException(msg);\n    }\n    return rsh;\n  }"
        ],
        [
            "HRegion::isSplittable()",
            "1393  \n1394  \n1395  \n1396 -\n1397  \n1398  ",
            "  @Override\n  public boolean isSplittable() {\n    boolean result = isAvailable() && !hasReferences();\n    LOG.info(\"ASKED IF SPLITTABLE \" + result, new Throwable(\"LOGGING\"));\n    return result;\n  }",
            "1393  \n1394  \n1395  \n1396 +\n1397 +\n1398 +\n1399 +\n1400 +\n1401 +\n1402 +\n1403 +\n1404 +\n1405 +\n1406  \n1407  ",
            "  @Override\n  public boolean isSplittable() {\n    boolean result = isAvailable() && !hasReferences();\n    LOG.info(\"ASKED IF SPLITTABLE \" + result + \" \" + getRegionInfo().getShortNameToLog(),\n      new Throwable(\"LOGGING: REMOVE\"));\n    // REMOVE BELOW!!!!\n    LOG.info(\"DEBUG LIST ALL FILES\");\n    for (Store store: this.stores.values()) {\n      LOG.info(\"store \" + store.getColumnFamilyName());\n      for (StoreFile sf: store.getStorefiles()) {\n        LOG.info(sf.toStringDetailed());\n      }\n    }\n    return result;\n  }"
        ]
    ],
    "fd86de98e192d55f221f1891b712e927761736d9": [
        [
            "AssignmentManager::loadMeta()",
            "1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188 -\n1189  \n1190  \n1191  \n1192  \n1193 -\n1194  \n1195  \n1196 -\n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  ",
            "  private void loadMeta() throws IOException {\n    // TODO: use a thread pool\n    regionStateStore.visitMeta(new RegionStateStore.RegionStateVisitor() {\n      @Override\n      public void visitRegionState(final RegionInfo regionInfo, final State state,\n          final ServerName regionLocation, final ServerName lastHost, final long openSeqNum) {\n        final RegionStateNode regionNode = regionStates.getOrCreateRegionNode(regionInfo);\n        synchronized (regionNode) {\n          if (!regionNode.isInTransition()) {\n            regionNode.setState(state);\n            regionNode.setLastHost(lastHost);\n            regionNode.setRegionLocation(regionLocation);\n            regionNode.setOpenSeqNum(openSeqNum);\n\n            if (state == State.OPEN) {\n              assert regionLocation != null : \"found null region location for \" + regionNode;\n              regionStates.addRegionToServer(regionLocation, regionNode);\n            } else if (state == State.OFFLINE || regionInfo.isOffline()) {\n              regionStates.addToOfflineRegions(regionNode);\n            } else {\n              // These regions should have a procedure in replay\n              regionStates.addRegionInTransition(regionNode, null);\n            }\n          }\n        }\n      }\n    });\n\n    // every assignment is blocked until meta is loaded.\n    wakeMetaLoadedEvent();\n  }",
            "1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186 +\n1187 +\n1188 +\n1189 +\n1190 +\n1191 +\n1192 +\n1193 +\n1194 +\n1195  \n1196  \n1197 +\n1198  \n1199  \n1200  \n1201  \n1202 +\n1203  \n1204  \n1205 +\n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  ",
            "  private void loadMeta() throws IOException {\n    // TODO: use a thread pool\n    regionStateStore.visitMeta(new RegionStateStore.RegionStateVisitor() {\n      @Override\n      public void visitRegionState(final RegionInfo regionInfo, final State state,\n          final ServerName regionLocation, final ServerName lastHost, final long openSeqNum) {\n        final RegionStateNode regionNode = regionStates.getOrCreateRegionNode(regionInfo);\n        State localState = state;\n        if (localState == null) {\n          // No region state column data in hbase:meta table! Are I doing a rolling upgrade from\n          // hbase1 to hbase2? Am I restoring a SNAPSHOT or otherwise adding a region to hbase:meta?\n          // In any of these cases, state is empty. For now, presume OFFLINE but there are probably\n          // cases where we need to probe more to be sure this correct; TODO informed by experience.\n          LOG.info(regionInfo.getEncodedName() + \" state=null; presuming \" + State.OFFLINE);\n          localState = State.OFFLINE;\n        }\n        synchronized (regionNode) {\n          if (!regionNode.isInTransition()) {\n            regionNode.setState(localState);\n            regionNode.setLastHost(lastHost);\n            regionNode.setRegionLocation(regionLocation);\n            regionNode.setOpenSeqNum(openSeqNum);\n\n            if (localState == State.OPEN) {\n              assert regionLocation != null : \"found null region location for \" + regionNode;\n              regionStates.addRegionToServer(regionLocation, regionNode);\n            } else if (localState == State.OFFLINE || regionInfo.isOffline()) {\n              regionStates.addToOfflineRegions(regionNode);\n            } else {\n              // These regions should have a procedure in replay\n              regionStates.addRegionInTransition(regionNode, null);\n            }\n          }\n        }\n      }\n    });\n\n    // every assignment is blocked until meta is loaded.\n    wakeMetaLoadedEvent();\n  }"
        ],
        [
            "ZooKeeperMainServer::HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain::HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 -\n  60  \n  61  \n  62  ",
            "    public HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String[] args)\n    throws IOException, InterruptedException {\n      super(args);\n      // Make sure we are connected before we proceed. Can take a while on some systems. If we\n      // run the command without being connected, we get ConnectionLoss KeeperErrorConnection...\n      Stopwatch stopWatch = Stopwatch.createStarted();\n      while (!this.zk.getState().isConnected()) {\n        Thread.sleep(1);\n        if (stopWatch.elapsed(TimeUnit.SECONDS) > 10) {\n          throw new InterruptedException(\"Failed connect \" + this.zk);\n        }\n      }\n    }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62  \n  63  \n  64  ",
            "    public HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String[] args)\n    throws IOException, InterruptedException {\n      super(args);\n      // Make sure we are connected before we proceed. Can take a while on some systems. If we\n      // run the command without being connected, we get ConnectionLoss KeeperErrorConnection...\n      Stopwatch stopWatch = Stopwatch.createStarted();\n      while (!this.zk.getState().isConnected()) {\n        Thread.sleep(1);\n        if (stopWatch.elapsed(TimeUnit.SECONDS) > 10) {\n          throw new InterruptedException(\"Failed connect after waiting \" +\n              stopWatch.elapsed(TimeUnit.SECONDS) + \"seconds; state=\" + this.zk.getState() +\n              \"; \" + this.zk);\n        }\n      }\n    }"
        ],
        [
            "RegionStateStore::visitMeta(RegionStateVisitor)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95 -\n  96 -\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  ",
            "  public void visitMeta(final RegionStateVisitor visitor) throws IOException {\n    MetaTableAccessor.fullScanRegions(master.getConnection(), new MetaTableAccessor.Visitor() {\n      final boolean isDebugEnabled = LOG.isDebugEnabled();\n\n      @Override\n      public boolean visit(final Result r) throws IOException {\n        if (r !=  null && !r.isEmpty()) {\n          long st = System.currentTimeMillis();\n          visitMetaEntry(visitor, r);\n          long et = System.currentTimeMillis();\n          LOG.info(\"[T] LOAD META PERF \" + StringUtils.humanTimeDiff(et - st));\n        } else if (isDebugEnabled) {\n          LOG.debug(\"NULL result from meta - ignoring but this is strange.\");\n        }\n        return true;\n      }\n    });\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94 +\n  95 +\n  96 +\n  97  \n  98 +\n  99 +\n 100 +\n 101 +\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  public void visitMeta(final RegionStateVisitor visitor) throws IOException {\n    MetaTableAccessor.fullScanRegions(master.getConnection(), new MetaTableAccessor.Visitor() {\n      final boolean isDebugEnabled = LOG.isDebugEnabled();\n\n      @Override\n      public boolean visit(final Result r) throws IOException {\n        if (r !=  null && !r.isEmpty()) {\n          long st = 0;\n          if (LOG.isTraceEnabled()) {\n            st = System.currentTimeMillis();\n          }\n          visitMetaEntry(visitor, r);\n          if (LOG.isTraceEnabled()) {\n            long et = System.currentTimeMillis();\n            LOG.trace(\"[T] LOAD META PERF \" + StringUtils.humanTimeDiff(et - st));\n          }\n        } else if (isDebugEnabled) {\n          LOG.debug(\"NULL result from meta - ignoring but this is strange.\");\n        }\n        return true;\n      }\n    });\n  }"
        ],
        [
            "RegionStateStore::getRegionState(Result,int)",
            " 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  ",
            "  /**\n   * Pull the region state from a catalog table {@link Result}.\n   * @param r Result to pull the region state from\n   * @return the region state, or OPEN if there's no value written.\n   */\n  protected State getRegionState(final Result r, int replicaId) {\n    Cell cell = r.getColumnLatestCell(HConstants.CATALOG_FAMILY, getStateColumn(replicaId));\n    if (cell == null || cell.getValueLength() == 0) return State.OPENING;\n    return State.valueOf(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));\n  }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 +\n 323 +\n 324 +\n 325  \n 326  ",
            "  /**\n   * Pull the region state from a catalog table {@link Result}.\n   * @param r Result to pull the region state from\n   * @return the region state, or null if unknown.\n   */\n  protected State getRegionState(final Result r, int replicaId) {\n    Cell cell = r.getColumnLatestCell(HConstants.CATALOG_FAMILY, getStateColumn(replicaId));\n    if (cell == null || cell.getValueLength() == 0) {\n      return null;\n    }\n    return State.valueOf(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));\n  }"
        ],
        [
            "RecoverableZooKeeper::retryOrThrow(RetryCounter,KeeperException,String)",
            " 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "  private void retryOrThrow(RetryCounter retryCounter, KeeperException e,\n      String opName) throws KeeperException {\n    LOG.debug(\"Possibly transient ZooKeeper, quorum=\" + quorumServers + \", exception=\" + e);\n    if (!retryCounter.shouldRetry()) {\n      LOG.error(\"ZooKeeper \" + opName + \" failed after \"\n        + retryCounter.getMaxAttempts() + \" attempts\");\n      throw e;\n    }\n  }",
            " 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304 +\n 305  ",
            "  private void retryOrThrow(RetryCounter retryCounter, KeeperException e,\n      String opName) throws KeeperException {\n    if (!retryCounter.shouldRetry()) {\n      LOG.error(\"ZooKeeper \" + opName + \" failed after \"\n        + retryCounter.getMaxAttempts() + \" attempts\");\n      throw e;\n    }\n    LOG.debug(\"Retry, connectivity issue (JVM Pause?); quorum=\" + quorumServers + \",\" +\n        \"exception=\" + e);\n  }"
        ],
        [
            "AssignmentManager::checkOnlineRegionsReportForMeta(ServerStateNode,Set)",
            " 938 -\n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954 -\n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  ",
            "  public void checkOnlineRegionsReportForMeta(final ServerStateNode serverNode,\n      final Set<byte[]> regionNames) {\n    try {\n      for (byte[] regionName: regionNames) {\n        final RegionInfo hri = getMetaRegionFromName(regionName);\n        if (hri == null) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Skip online report for region=\" + Bytes.toStringBinary(regionName) +\n              \" while meta is loading\");\n          }\n          continue;\n        }\n\n        final RegionStateNode regionNode = regionStates.getOrCreateRegionNode(hri);\n        LOG.info(\"META REPORTED: \" + regionNode);\n        if (!reportTransition(regionNode, serverNode, TransitionCode.OPENED, 0)) {\n          LOG.warn(\"META REPORTED but no procedure found\");\n          regionNode.setRegionLocation(serverNode.getServerName());\n        } else if (LOG.isTraceEnabled()) {\n          LOG.trace(\"META REPORTED: \" + regionNode);\n        }\n      }\n    } catch (UnexpectedStateException e) {\n      final ServerName serverName = serverNode.getServerName();\n      LOG.warn(\"KILLING \" + serverName + \": \" + e.getMessage());\n      killRegionServer(serverNode);\n    }\n  }",
            " 938 +\n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954 +\n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  ",
            "  void checkOnlineRegionsReportForMeta(final ServerStateNode serverNode,\n      final Set<byte[]> regionNames) {\n    try {\n      for (byte[] regionName: regionNames) {\n        final RegionInfo hri = getMetaRegionFromName(regionName);\n        if (hri == null) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Skip online report for region=\" + Bytes.toStringBinary(regionName) +\n              \" while meta is loading\");\n          }\n          continue;\n        }\n\n        final RegionStateNode regionNode = regionStates.getOrCreateRegionNode(hri);\n        LOG.info(\"META REPORTED: \" + regionNode);\n        if (!reportTransition(regionNode, serverNode, TransitionCode.OPENED, 0)) {\n          LOG.warn(\"META REPORTED but no procedure found (complete?)\");\n          regionNode.setRegionLocation(serverNode.getServerName());\n        } else if (LOG.isTraceEnabled()) {\n          LOG.trace(\"META REPORTED: \" + regionNode);\n        }\n      }\n    } catch (UnexpectedStateException e) {\n      final ServerName serverName = serverNode.getServerName();\n      LOG.warn(\"KILLING \" + serverName + \": \" + e.getMessage());\n      killRegionServer(serverNode);\n    }\n  }"
        ],
        [
            "RegionStates::getRegionsOfTable(TableName,boolean)",
            " 549 -\n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  ",
            "  List<RegionInfo> getRegionsOfTable(final TableName table, final boolean offline) {\n    final ArrayList<RegionStateNode> nodes = getTableRegionStateNodes(table);\n    final ArrayList<RegionInfo> hris = new ArrayList<RegionInfo>(nodes.size());\n    for (RegionStateNode node: nodes) {\n      if (include(node, offline)) hris.add(node.getRegionInfo());\n    }\n    return hris;\n  }",
            " 552  \n 553  \n 554  \n 555  \n 556  \n 557 +\n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  ",
            "  /**\n   * @return Return the regions of the table; does not include OFFLINE unless you set\n   * <code>offline</code> to true. Does not include regions that are in the\n   * {@link State#SPLIT} state.\n   */\n  public List<RegionInfo> getRegionsOfTable(final TableName table, final boolean offline) {\n    final ArrayList<RegionStateNode> nodes = getTableRegionStateNodes(table);\n    final ArrayList<RegionInfo> hris = new ArrayList<RegionInfo>(nodes.size());\n    for (RegionStateNode node: nodes) {\n      if (include(node, offline)) hris.add(node.getRegionInfo());\n    }\n    return hris;\n  }"
        ],
        [
            "ServerManager::processQueuedDeadServers()",
            " 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659 -\n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  ",
            "  /**\n   * Process the servers which died during master's initialization. It will be\n   * called after HMaster#assignMeta and AssignmentManager#joinCluster.\n   * */\n  synchronized void processQueuedDeadServers() {\n    if (!master.isServerCrashProcessingEnabled()) {\n      LOG.info(\"Master hasn't enabled ServerShutdownHandler\");\n    }\n    Iterator<ServerName> serverIterator = queuedDeadServers.iterator();\n    while (serverIterator.hasNext()) {\n      ServerName tmpServerName = serverIterator.next();\n      expireServer(tmpServerName);\n      serverIterator.remove();\n      requeuedDeadServers.remove(tmpServerName);\n    }\n\n    if (!master.getAssignmentManager().isFailoverCleanupDone()) {\n      LOG.info(\"AssignmentManager hasn't finished failover cleanup; waiting\");\n    }\n\n    for (Map.Entry<ServerName, Boolean> entry : requeuedDeadServers.entrySet()) {\n      processDeadServer(entry.getKey(), entry.getValue());\n    }\n    requeuedDeadServers.clear();\n  }",
            " 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659 +\n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  ",
            "  /**\n   * Process the servers which died during master's initialization. It will be\n   * called after HMaster#assignMeta and AssignmentManager#joinCluster.\n   * */\n  synchronized void processQueuedDeadServers() {\n    if (!master.isServerCrashProcessingEnabled()) {\n      LOG.info(\"Master hasn't enabled ServerShutdownHandler\");\n    }\n    Iterator<ServerName> serverIterator = queuedDeadServers.iterator();\n    while (serverIterator.hasNext()) {\n      ServerName tmpServerName = serverIterator.next();\n      expireServer(tmpServerName);\n      serverIterator.remove();\n      requeuedDeadServers.remove(tmpServerName);\n    }\n\n    if (!master.getAssignmentManager().isFailoverCleanupDone()) {\n      LOG.debug(\"AssignmentManager failover cleanup not done.\");\n    }\n\n    for (Map.Entry<ServerName, Boolean> entry : requeuedDeadServers.entrySet()) {\n      processDeadServer(entry.getKey(), entry.getValue());\n    }\n    requeuedDeadServers.clear();\n  }"
        ],
        [
            "EnableTableProcedure::executeFromState(MasterProcedureEnv,EnableTableState)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 -\n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  @SuppressWarnings(\"deprecation\")\n  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, final EnableTableState state)\n      throws InterruptedException {\n    if (isTraceEnabled()) {\n      LOG.trace(this + \" execute state=\" + state);\n    }\n\n    try {\n      switch (state) {\n      case ENABLE_TABLE_PREPARE:\n        if (prepareEnable(env)) {\n          setNextState(EnableTableState.ENABLE_TABLE_PRE_OPERATION);\n        } else {\n          assert isFailed() : \"enable should have an exception here\";\n          return Flow.NO_MORE_STATE;\n        }\n        break;\n      case ENABLE_TABLE_PRE_OPERATION:\n        preEnable(env, state);\n        setNextState(EnableTableState.ENABLE_TABLE_SET_ENABLING_TABLE_STATE);\n        break;\n      case ENABLE_TABLE_SET_ENABLING_TABLE_STATE:\n        setTableStateToEnabling(env, tableName);\n        setNextState(EnableTableState.ENABLE_TABLE_MARK_REGIONS_ONLINE);\n        break;\n      case ENABLE_TABLE_MARK_REGIONS_ONLINE:\n        Connection connection = env.getMasterServices().getConnection();\n        // we will need to get the tableDescriptor here to see if there is a change in the replica\n        // count\n        TableDescriptor hTableDescriptor =\n            env.getMasterServices().getTableDescriptors().get(tableName);\n\n        // Get the replica count\n        int regionReplicaCount = hTableDescriptor.getRegionReplication();\n\n        // Get the regions for the table from the memory\n        List<RegionInfo> regionsOfTable =\n            env.getAssignmentManager().getRegionStates().getRegionsOfTable(tableName);\n\n        if (regionReplicaCount > 1) {\n          int currentMaxReplica = 0;\n          // Check if the regions in memory have replica regions as marked in META table\n          for (RegionInfo regionInfo : regionsOfTable) {\n            if (regionInfo.getReplicaId() > currentMaxReplica) {\n              // Iterating through all the list to identify the highest replicaID region.\n              // We can stop after checking with the first set of regions??\n              currentMaxReplica = regionInfo.getReplicaId();\n            }\n          }\n\n          // read the META table to know the actual number of replicas for the table - if there\n          // was a table modification on region replica then this will reflect the new entries also\n          int replicasFound =\n              getNumberOfReplicasFromMeta(connection, regionReplicaCount, regionsOfTable);\n          assert regionReplicaCount - 1 == replicasFound;\n          LOG.info(replicasFound + \" META entries added for the given regionReplicaCount \"\n              + regionReplicaCount + \" for the table \" + tableName.getNameAsString());\n          if (currentMaxReplica == (regionReplicaCount - 1)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"There is no change to the number of region replicas.\"\n                  + \" Assigning the available regions.\" + \" Current and previous\"\n                  + \"replica count is \" + regionReplicaCount);\n            }\n          } else if (currentMaxReplica > (regionReplicaCount - 1)) {\n            // we have additional regions as the replica count has been decreased. Delete\n            // those regions because already the table is in the unassigned state\n            LOG.info(\"The number of replicas \" + (currentMaxReplica + 1)\n                + \"  is more than the region replica count \" + regionReplicaCount);\n            List<RegionInfo> copyOfRegions = new ArrayList<RegionInfo>(regionsOfTable);\n            for (RegionInfo regionInfo : copyOfRegions) {\n              if (regionInfo.getReplicaId() > (regionReplicaCount - 1)) {\n                // delete the region from the regionStates\n                env.getAssignmentManager().getRegionStates().deleteRegion(regionInfo);\n                // remove it from the list of regions of the table\n                LOG.info(\"The regioninfo being removed is \" + regionInfo + \" \"\n                    + regionInfo.getReplicaId());\n                regionsOfTable.remove(regionInfo);\n              }\n            }\n          } else {\n            // the replicasFound is less than the regionReplication\n            LOG.info(\n              \"The number of replicas has been changed(increased).\"\n              + \" Lets assign the new region replicas. The previous replica count was \"\n                  + (currentMaxReplica + 1) + \". The current replica count is \"\n                  + regionReplicaCount);\n            regionsOfTable = RegionReplicaUtil.addReplicas(hTableDescriptor, regionsOfTable,\n              currentMaxReplica + 1, regionReplicaCount);\n          }\n        }\n        // Assign all the table regions. (including region replicas if added)\n        addChildProcedure(env.getAssignmentManager().createAssignProcedures(regionsOfTable));\n        setNextState(EnableTableState.ENABLE_TABLE_SET_ENABLED_TABLE_STATE);\n        break;\n      case ENABLE_TABLE_SET_ENABLED_TABLE_STATE:\n        setTableStateToEnabled(env, tableName);\n        setNextState(EnableTableState.ENABLE_TABLE_POST_OPERATION);\n        break;\n      case ENABLE_TABLE_POST_OPERATION:\n        postEnable(env, state);\n        return Flow.NO_MORE_STATE;\n      default:\n        throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-enable-table\", e);\n      } else {\n        LOG.warn(\"Retriable error trying to enable table=\" + tableName +\n          \" (in state=\" + state + \")\", e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  @SuppressWarnings(\"deprecation\")\n  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, final EnableTableState state)\n      throws InterruptedException {\n    if (isTraceEnabled()) {\n      LOG.trace(this + \" execute state=\" + state);\n    }\n\n    try {\n      switch (state) {\n      case ENABLE_TABLE_PREPARE:\n        if (prepareEnable(env)) {\n          setNextState(EnableTableState.ENABLE_TABLE_PRE_OPERATION);\n        } else {\n          assert isFailed() : \"enable should have an exception here\";\n          return Flow.NO_MORE_STATE;\n        }\n        break;\n      case ENABLE_TABLE_PRE_OPERATION:\n        preEnable(env, state);\n        setNextState(EnableTableState.ENABLE_TABLE_SET_ENABLING_TABLE_STATE);\n        break;\n      case ENABLE_TABLE_SET_ENABLING_TABLE_STATE:\n        setTableStateToEnabling(env, tableName);\n        setNextState(EnableTableState.ENABLE_TABLE_MARK_REGIONS_ONLINE);\n        break;\n      case ENABLE_TABLE_MARK_REGIONS_ONLINE:\n        Connection connection = env.getMasterServices().getConnection();\n        // we will need to get the tableDescriptor here to see if there is a change in the replica\n        // count\n        TableDescriptor hTableDescriptor =\n            env.getMasterServices().getTableDescriptors().get(tableName);\n\n        // Get the replica count\n        int regionReplicaCount = hTableDescriptor.getRegionReplication();\n\n        // Get the regions for the table from memory; get both online and offline regions ('true').\n        List<RegionInfo> regionsOfTable =\n            env.getAssignmentManager().getRegionStates().getRegionsOfTable(tableName, true);\n\n        if (regionReplicaCount > 1) {\n          int currentMaxReplica = 0;\n          // Check if the regions in memory have replica regions as marked in META table\n          for (RegionInfo regionInfo : regionsOfTable) {\n            if (regionInfo.getReplicaId() > currentMaxReplica) {\n              // Iterating through all the list to identify the highest replicaID region.\n              // We can stop after checking with the first set of regions??\n              currentMaxReplica = regionInfo.getReplicaId();\n            }\n          }\n\n          // read the META table to know the actual number of replicas for the table - if there\n          // was a table modification on region replica then this will reflect the new entries also\n          int replicasFound =\n              getNumberOfReplicasFromMeta(connection, regionReplicaCount, regionsOfTable);\n          assert regionReplicaCount - 1 == replicasFound;\n          LOG.info(replicasFound + \" META entries added for the given regionReplicaCount \"\n              + regionReplicaCount + \" for the table \" + tableName.getNameAsString());\n          if (currentMaxReplica == (regionReplicaCount - 1)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"There is no change to the number of region replicas.\"\n                  + \" Assigning the available regions.\" + \" Current and previous\"\n                  + \"replica count is \" + regionReplicaCount);\n            }\n          } else if (currentMaxReplica > (regionReplicaCount - 1)) {\n            // we have additional regions as the replica count has been decreased. Delete\n            // those regions because already the table is in the unassigned state\n            LOG.info(\"The number of replicas \" + (currentMaxReplica + 1)\n                + \"  is more than the region replica count \" + regionReplicaCount);\n            List<RegionInfo> copyOfRegions = new ArrayList<RegionInfo>(regionsOfTable);\n            for (RegionInfo regionInfo : copyOfRegions) {\n              if (regionInfo.getReplicaId() > (regionReplicaCount - 1)) {\n                // delete the region from the regionStates\n                env.getAssignmentManager().getRegionStates().deleteRegion(regionInfo);\n                // remove it from the list of regions of the table\n                LOG.info(\"The regioninfo being removed is \" + regionInfo + \" \"\n                    + regionInfo.getReplicaId());\n                regionsOfTable.remove(regionInfo);\n              }\n            }\n          } else {\n            // the replicasFound is less than the regionReplication\n            LOG.info(\n              \"The number of replicas has been changed(increased).\"\n              + \" Lets assign the new region replicas. The previous replica count was \"\n                  + (currentMaxReplica + 1) + \". The current replica count is \"\n                  + regionReplicaCount);\n            regionsOfTable = RegionReplicaUtil.addReplicas(hTableDescriptor, regionsOfTable,\n              currentMaxReplica + 1, regionReplicaCount);\n          }\n        }\n        // Assign all the table regions. (including region replicas if added)\n        addChildProcedure(env.getAssignmentManager().createAssignProcedures(regionsOfTable));\n        setNextState(EnableTableState.ENABLE_TABLE_SET_ENABLED_TABLE_STATE);\n        break;\n      case ENABLE_TABLE_SET_ENABLED_TABLE_STATE:\n        setTableStateToEnabled(env, tableName);\n        setNextState(EnableTableState.ENABLE_TABLE_POST_OPERATION);\n        break;\n      case ENABLE_TABLE_POST_OPERATION:\n        postEnable(env, state);\n        return Flow.NO_MORE_STATE;\n      default:\n        throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-enable-table\", e);\n      } else {\n        LOG.warn(\"Retriable error trying to enable table=\" + tableName +\n          \" (in state=\" + state + \")\", e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }"
        ]
    ],
    "bc321a3bddb153f09c9b3c953b4fce0efeeb2131": [
        [
            "BlockingRpcConnection::handleConnectionFailure(int,int,IOException)",
            " 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300 -\n 301  ",
            "  /**\n   * Handle connection failures If the current number of retries is equal to the max number of\n   * retries, stop retrying and throw the exception; Otherwise backoff N seconds and try connecting\n   * again. This Method is only called from inside setupIOstreams(), which is synchronized. Hence\n   * the sleep is synchronized; the locks will be retained.\n   * @param curRetries current number of retries\n   * @param maxRetries max number of retries allowed\n   * @param ioe failure reason\n   * @throws IOException if max number of retries is reached\n   */\n  private void handleConnectionFailure(int curRetries, int maxRetries, IOException ioe)\n      throws IOException {\n    closeSocket();\n\n    // throw the exception if the maximum number of retries is reached\n    if (curRetries >= maxRetries || ExceptionUtil.isInterrupt(ioe)) {\n      throw ioe;\n    }\n\n    // otherwise back off and retry\n    try {\n      Thread.sleep(this.rpcClient.failureSleep);\n    } catch (InterruptedException ie) {\n      ExceptionUtil.rethrowIfInterrupt(ie);\n    }\n\n    LOG.info(\"Retrying connect to server: \" + remoteId.getAddress() + \" after sleeping \"\n        + this.rpcClient.failureSleep + \"ms. Already tried \" + curRetries + \" time(s).\");\n  }",
            " 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307 +\n 308 +\n 309 +\n 310 +\n 311 +\n 312  ",
            "  /**\n   * Handle connection failures If the current number of retries is equal to the max number of\n   * retries, stop retrying and throw the exception; Otherwise backoff N seconds and try connecting\n   * again. This Method is only called from inside setupIOstreams(), which is synchronized. Hence\n   * the sleep is synchronized; the locks will be retained.\n   * @param curRetries current number of retries\n   * @param maxRetries max number of retries allowed\n   * @param ioe failure reason\n   * @throws IOException if max number of retries is reached\n   */\n  private void handleConnectionFailure(int curRetries, int maxRetries, IOException ioe)\n      throws IOException {\n    closeSocket();\n\n    // throw the exception if the maximum number of retries is reached\n    if (curRetries >= maxRetries || ExceptionUtil.isInterrupt(ioe)) {\n      throw ioe;\n    }\n\n    // otherwise back off and retry\n    try {\n      Thread.sleep(this.rpcClient.failureSleep);\n    } catch (InterruptedException ie) {\n      ExceptionUtil.rethrowIfInterrupt(ie);\n    }\n\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Retrying connect to server: \" + remoteId.getAddress() +\n        \" after sleeping \" + this.rpcClient.failureSleep + \"ms. Already tried \" + curRetries +\n        \" time(s).\");\n    }\n  }"
        ],
        [
            "BlockingRpcConnection::handleSaslConnectionFailure(int,int,Exception,UserGroupInformation)",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  ",
            "  /**\n   * If multiple clients with the same principal try to connect to the same server at the same time,\n   * the server assumes a replay attack is in progress. This is a feature of kerberos. In order to\n   * work around this, what is done is that the client backs off randomly and tries to initiate the\n   * connection again. The other problem is to do with ticket expiry. To handle that, a relogin is\n   * attempted.\n   * <p>\n   * The retry logic is governed by the {@link #shouldAuthenticateOverKrb} method. In case when the\n   * user doesn't have valid credentials, we don't need to retry (from cache or ticket). In such\n   * cases, it is prudent to throw a runtime exception when we receive a SaslException from the\n   * underlying authentication implementation, so there is no retry from other high level (for eg,\n   * HCM or HBaseAdmin).\n   * </p>\n   */\n  private void handleSaslConnectionFailure(final int currRetries, final int maxRetries,\n      final Exception ex, final UserGroupInformation user)\n      throws IOException, InterruptedException {\n    closeSocket();\n    user.doAs(new PrivilegedExceptionAction<Object>() {\n      @Override\n      public Object run() throws IOException, InterruptedException {\n        if (shouldAuthenticateOverKrb()) {\n          if (currRetries < maxRetries) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception encountered while connecting to \" + \"the server : \" + ex);\n            }\n            // try re-login\n            relogin();\n            disposeSasl();\n            // have granularity of milliseconds\n            // we are sleeping with the Connection lock held but since this\n            // connection instance is being used for connecting to the server\n            // in question, it is okay\n            Thread.sleep(ThreadLocalRandom.current().nextInt(reloginMaxBackoff) + 1);\n            return null;\n          } else {\n            String msg = \"Couldn't setup connection for \"\n                + UserGroupInformation.getLoginUser().getUserName() + \" to \" + serverPrincipal;\n            LOG.warn(msg, ex);\n            throw (IOException) new IOException(msg).initCause(ex);\n          }\n        } else {\n          LOG.warn(\"Exception encountered while connecting to \" + \"the server : \" + ex);\n        }\n        if (ex instanceof RemoteException) {\n          throw (RemoteException) ex;\n        }\n        if (ex instanceof SaslException) {\n          String msg = \"SASL authentication failed.\"\n              + \" The most likely cause is missing or invalid credentials.\" + \" Consider 'kinit'.\";\n          LOG.fatal(msg, ex);\n          throw new RuntimeException(msg, ex);\n        }\n        throw new IOException(ex);\n      }\n    });\n  }",
            " 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396 +\n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  ",
            "  /**\n   * If multiple clients with the same principal try to connect to the same server at the same time,\n   * the server assumes a replay attack is in progress. This is a feature of kerberos. In order to\n   * work around this, what is done is that the client backs off randomly and tries to initiate the\n   * connection again. The other problem is to do with ticket expiry. To handle that, a relogin is\n   * attempted.\n   * <p>\n   * The retry logic is governed by the {@link #shouldAuthenticateOverKrb} method. In case when the\n   * user doesn't have valid credentials, we don't need to retry (from cache or ticket). In such\n   * cases, it is prudent to throw a runtime exception when we receive a SaslException from the\n   * underlying authentication implementation, so there is no retry from other high level (for eg,\n   * HCM or HBaseAdmin).\n   * </p>\n   */\n  private void handleSaslConnectionFailure(final int currRetries, final int maxRetries,\n      final Exception ex, final UserGroupInformation user)\n      throws IOException, InterruptedException {\n    closeSocket();\n    user.doAs(new PrivilegedExceptionAction<Object>() {\n      @Override\n      public Object run() throws IOException, InterruptedException {\n        if (shouldAuthenticateOverKrb()) {\n          if (currRetries < maxRetries) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Exception encountered while connecting to \" +\n                \"the server : \" + StringUtils.stringifyException(ex));\n            }\n            // try re-login\n            relogin();\n            disposeSasl();\n            // have granularity of milliseconds\n            // we are sleeping with the Connection lock held but since this\n            // connection instance is being used for connecting to the server\n            // in question, it is okay\n            Thread.sleep(ThreadLocalRandom.current().nextInt(reloginMaxBackoff) + 1);\n            return null;\n          } else {\n            String msg = \"Couldn't setup connection for \"\n                + UserGroupInformation.getLoginUser().getUserName() + \" to \" + serverPrincipal;\n            LOG.warn(msg, ex);\n            throw (IOException) new IOException(msg).initCause(ex);\n          }\n        } else {\n          LOG.warn(\"Exception encountered while connecting to \" + \"the server : \" + ex);\n        }\n        if (ex instanceof RemoteException) {\n          throw (RemoteException) ex;\n        }\n        if (ex instanceof SaslException) {\n          String msg = \"SASL authentication failed.\"\n              + \" The most likely cause is missing or invalid credentials.\" + \" Consider 'kinit'.\";\n          LOG.fatal(msg, ex);\n          throw new RuntimeException(msg, ex);\n        }\n        throw new IOException(ex);\n      }\n    });\n  }"
        ],
        [
            "BlockingRpcConnection::setupConnection()",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "  protected void setupConnection() throws IOException {\n    short ioFailures = 0;\n    short timeoutFailures = 0;\n    while (true) {\n      try {\n        this.socket = this.rpcClient.socketFactory.createSocket();\n        this.socket.setTcpNoDelay(this.rpcClient.isTcpNoDelay());\n        this.socket.setKeepAlive(this.rpcClient.tcpKeepAlive);\n        if (this.rpcClient.localAddr != null) {\n          this.socket.bind(this.rpcClient.localAddr);\n        }\n        NetUtils.connect(this.socket, remoteId.getAddress(), this.rpcClient.connectTO);\n        this.socket.setSoTimeout(this.rpcClient.readTO);\n        return;\n      } catch (SocketTimeoutException toe) {\n        /*\n         * The max number of retries is 45, which amounts to 20s*45 = 15 minutes retries.\n         */\n        handleConnectionFailure(timeoutFailures++, this.rpcClient.maxRetries, toe);\n      } catch (IOException ie) {\n        handleConnectionFailure(ioFailures++, this.rpcClient.maxRetries, ie);\n      }\n    }\n  }",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266 +\n 267 +\n 268 +\n 269 +\n 270  \n 271  \n 272 +\n 273 +\n 274 +\n 275 +\n 276  \n 277  \n 278  \n 279  ",
            "  protected void setupConnection() throws IOException {\n    short ioFailures = 0;\n    short timeoutFailures = 0;\n    while (true) {\n      try {\n        this.socket = this.rpcClient.socketFactory.createSocket();\n        this.socket.setTcpNoDelay(this.rpcClient.isTcpNoDelay());\n        this.socket.setKeepAlive(this.rpcClient.tcpKeepAlive);\n        if (this.rpcClient.localAddr != null) {\n          this.socket.bind(this.rpcClient.localAddr);\n        }\n        NetUtils.connect(this.socket, remoteId.getAddress(), this.rpcClient.connectTO);\n        this.socket.setSoTimeout(this.rpcClient.readTO);\n        return;\n      } catch (SocketTimeoutException toe) {\n        /*\n         * The max number of retries is 45, which amounts to 20s*45 = 15 minutes retries.\n         */\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Received exception in connection setup.\\n\" +\n              StringUtils.stringifyException(toe));\n        }\n        handleConnectionFailure(timeoutFailures++, this.rpcClient.maxRetries, toe);\n      } catch (IOException ie) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Received exception in connection setup.\\n\" +\n              StringUtils.stringifyException(ie));\n        }\n        handleConnectionFailure(ioFailures++, this.rpcClient.maxRetries, ie);\n      }\n    }\n  }"
        ],
        [
            "RpcRetryingCallerImpl::callWithRetries(RetryingCallable,int)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123 -\n 124 -\n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  @Override\n  public T callWithRetries(RetryingCallable<T> callable, int callTimeout)\n  throws IOException, RuntimeException {\n    List<RetriesExhaustedException.ThrowableWithExtraContext> exceptions = new ArrayList<>();\n    tracker.start();\n    context.clear();\n    for (int tries = 0;; tries++) {\n      long expectedSleep;\n      try {\n        // bad cache entries are cleared in the call to RetryingCallable#throwable() in catch block\n        callable.prepare(tries != 0);\n        interceptor.intercept(context.prepare(callable, tries));\n        return callable.call(getTimeout(callTimeout));\n      } catch (PreemptiveFastFailException e) {\n        throw e;\n      } catch (Throwable t) {\n        Throwable e = t.getCause();\n        ExceptionUtil.rethrowIfInterrupt(t);\n        Throwable cause = t.getCause();\n        if (cause instanceof DoNotRetryIOException) {\n          // Fail fast\n          throw (DoNotRetryIOException) cause;\n        }\n        // translateException throws exception when should not retry: i.e. when request is bad.\n        interceptor.handleFailure(context, t);\n        t = translateException(t);\n\n        if (tries > startLogErrorsCnt) {\n          LOG.info(\"Call exception, tries=\" + tries + \", maxAttempts=\" + maxAttempts + \", started=\"\n              + (EnvironmentEdgeManager.currentTime() - tracker.getStartTime()) + \" ms ago, \"\n              + \"cancelled=\" + cancelled.get() + \", msg=\"\n              + t.getMessage() + \" \" + callable.getExceptionMessageAdditionalDetail());\n        }\n\n        callable.throwable(t, maxAttempts != 1);\n        RetriesExhaustedException.ThrowableWithExtraContext qt =\n            new RetriesExhaustedException.ThrowableWithExtraContext(t,\n                EnvironmentEdgeManager.currentTime(), toString());\n        exceptions.add(qt);\n        if (tries >= maxAttempts - 1) {\n          throw new RetriesExhaustedException(tries, exceptions);\n        }\n        // If the server is dead, we need to wait a little before retrying, to give\n        // a chance to the regions to be moved\n        // get right pause time, start by RETRY_BACKOFF[0] * pauseBase, where pauseBase might be\n        // special when encountering CallQueueTooBigException, see #HBASE-17114\n        long pauseBase = (t instanceof CallQueueTooBigException) ? pauseForCQTBE : pause;\n        expectedSleep = callable.sleep(pauseBase, tries);\n\n        // If, after the planned sleep, there won't be enough time left, we stop now.\n        long duration = singleCallDuration(expectedSleep);\n        if (duration > callTimeout) {\n          String msg = \"callTimeout=\" + callTimeout + \", callDuration=\" + duration +\n              \": \" +  t.getMessage() + \" \" + callable.getExceptionMessageAdditionalDetail();\n          throw (SocketTimeoutException)(new SocketTimeoutException(msg).initCause(t));\n        }\n      } finally {\n        interceptor.updateFailureInfo(context);\n      }\n      try {\n        if (expectedSleep > 0) {\n          synchronized (cancelled) {\n            if (cancelled.get()) return null;\n            cancelled.wait(expectedSleep);\n          }\n        }\n        if (cancelled.get()) return null;\n      } catch (InterruptedException e) {\n        throw new InterruptedIOException(\"Interrupted after \" + tries\n            + \" tries while maxAttempts=\" + maxAttempts);\n      }\n    }\n  }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "  @Override\n  public T callWithRetries(RetryingCallable<T> callable, int callTimeout)\n  throws IOException, RuntimeException {\n    List<RetriesExhaustedException.ThrowableWithExtraContext> exceptions = new ArrayList<>();\n    tracker.start();\n    context.clear();\n    for (int tries = 0;; tries++) {\n      long expectedSleep;\n      try {\n        // bad cache entries are cleared in the call to RetryingCallable#throwable() in catch block\n        callable.prepare(tries != 0);\n        interceptor.intercept(context.prepare(callable, tries));\n        return callable.call(getTimeout(callTimeout));\n      } catch (PreemptiveFastFailException e) {\n        throw e;\n      } catch (Throwable t) {\n        Throwable e = t.getCause();\n        ExceptionUtil.rethrowIfInterrupt(t);\n        Throwable cause = t.getCause();\n        if (cause instanceof DoNotRetryIOException) {\n          // Fail fast\n          throw (DoNotRetryIOException) cause;\n        }\n        // translateException throws exception when should not retry: i.e. when request is bad.\n        interceptor.handleFailure(context, t);\n        t = translateException(t);\n\n        if (tries > startLogErrorsCnt) {\n          if (LOG.isInfoEnabled()) {\n            StringBuilder builder = new StringBuilder(\"Call exception, tries=\").append(tries)\n              .append(\", retries=\").append(tries).append(\", started=\")\n              .append((EnvironmentEdgeManager.currentTime() - tracker.getStartTime()))\n              .append(\" ms ago, \").append(\"cancelled=\").append(cancelled.get())\n              .append(\", msg=\").append(t.getMessage())\n              .append(\", details=\").append(callable.getExceptionMessageAdditionalDetail());\n            if (LOG.isDebugEnabled()) {\n              builder.append(\", exception=\").append(StringUtils.stringifyException(t));\n              LOG.debug(builder.toString());\n            } else {\n              LOG.info(builder.toString());\n            }\n          }\n        }\n\n        callable.throwable(t, maxAttempts != 1);\n        RetriesExhaustedException.ThrowableWithExtraContext qt =\n            new RetriesExhaustedException.ThrowableWithExtraContext(t,\n                EnvironmentEdgeManager.currentTime(), toString());\n        exceptions.add(qt);\n        if (tries >= maxAttempts - 1) {\n          throw new RetriesExhaustedException(tries, exceptions);\n        }\n        // If the server is dead, we need to wait a little before retrying, to give\n        // a chance to the regions to be moved\n        // get right pause time, start by RETRY_BACKOFF[0] * pauseBase, where pauseBase might be\n        // special when encountering CallQueueTooBigException, see #HBASE-17114\n        long pauseBase = (t instanceof CallQueueTooBigException) ? pauseForCQTBE : pause;\n        expectedSleep = callable.sleep(pauseBase, tries);\n\n        // If, after the planned sleep, there won't be enough time left, we stop now.\n        long duration = singleCallDuration(expectedSleep);\n        if (duration > callTimeout) {\n          String msg = \"callTimeout=\" + callTimeout + \", callDuration=\" + duration +\n              \": \" +  t.getMessage() + \" \" + callable.getExceptionMessageAdditionalDetail();\n          throw (SocketTimeoutException)(new SocketTimeoutException(msg).initCause(t));\n        }\n      } finally {\n        interceptor.updateFailureInfo(context);\n      }\n      try {\n        if (expectedSleep > 0) {\n          synchronized (cancelled) {\n            if (cancelled.get()) return null;\n            cancelled.wait(expectedSleep);\n          }\n        }\n        if (cancelled.get()) return null;\n      } catch (InterruptedException e) {\n        throw new InterruptedIOException(\"Interrupted after \" + tries\n            + \" tries while maxAttempts=\" + maxAttempts);\n      }\n    }\n  }"
        ]
    ],
    "2c9ef8a471148ece655b881cc490b6b685d634f4": [
        [
            "WALProcedureStore::WALProcedureStore(Configuration,Path,Path,LeaseRecovery)",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "  @VisibleForTesting\n  public WALProcedureStore(final Configuration conf, final Path walDir, final Path walArchiveDir,\n      final LeaseRecovery leaseRecovery) throws IOException {\n    this.conf = conf;\n    this.leaseRecovery = leaseRecovery;\n    this.walDir = walDir;\n    this.walArchiveDir = walArchiveDir;\n    this.fs = walDir.getFileSystem(conf);\n\n    // Create the log directory for the procedure store\n    if (!fs.exists(walDir)) {\n      if (!fs.mkdirs(walDir)) {\n        throw new IOException(\"Unable to mkdir \" + walDir);\n      }\n    }\n    // Now that it exists, set the log policy\n    CommonFSUtils.setStoragePolicy(fs, conf, walDir, HConstants.WAL_STORAGE_POLICY,\n      HConstants.DEFAULT_WAL_STORAGE_POLICY);\n\n    // Create archive dir up front. Rename won't work w/o it up on HDFS.\n    if (this.walArchiveDir != null && !this.fs.exists(this.walArchiveDir)) {\n      if (this.fs.mkdirs(this.walArchiveDir)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Created Procedure Store WAL archive dir \" + this.walArchiveDir);\n        }\n      } else {\n        LOG.warn(\"Failed create of \" + this.walArchiveDir);\n      }\n    }\n  }",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "  @VisibleForTesting\n  public WALProcedureStore(final Configuration conf, final Path walDir, final Path walArchiveDir,\n      final LeaseRecovery leaseRecovery) throws IOException {\n    this.conf = conf;\n    this.leaseRecovery = leaseRecovery;\n    this.walDir = walDir;\n    this.walArchiveDir = walArchiveDir;\n    this.fs = walDir.getFileSystem(conf);\n    this.enforceStreamCapability = conf.getBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, true);\n\n    // Create the log directory for the procedure store\n    if (!fs.exists(walDir)) {\n      if (!fs.mkdirs(walDir)) {\n        throw new IOException(\"Unable to mkdir \" + walDir);\n      }\n    }\n    // Now that it exists, set the log policy\n    CommonFSUtils.setStoragePolicy(fs, conf, walDir, HConstants.WAL_STORAGE_POLICY,\n      HConstants.DEFAULT_WAL_STORAGE_POLICY);\n\n    // Create archive dir up front. Rename won't work w/o it up on HDFS.\n    if (this.walArchiveDir != null && !this.fs.exists(this.walArchiveDir)) {\n      if (this.fs.mkdirs(this.walArchiveDir)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Created Procedure Store WAL archive dir \" + this.walArchiveDir);\n        }\n      } else {\n        LOG.warn(\"Failed create of \" + this.walArchiveDir);\n      }\n    }\n  }"
        ],
        [
            "WALProcedureStore::rollWriter(long)",
            "1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031 -\n1032 -\n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  ",
            "  private boolean rollWriter(final long logId) throws IOException {\n    assert logId > flushLogId : \"logId=\" + logId + \" flushLogId=\" + flushLogId;\n    assert lock.isHeldByCurrentThread() : \"expected to be the lock owner. \" + lock.isLocked();\n\n    ProcedureWALHeader header = ProcedureWALHeader.newBuilder()\n      .setVersion(ProcedureWALFormat.HEADER_VERSION)\n      .setType(ProcedureWALFormat.LOG_TYPE_STREAM)\n      .setMinProcId(storeTracker.getActiveMinProcId())\n      .setLogId(logId)\n      .build();\n\n    FSDataOutputStream newStream = null;\n    Path newLogFile = null;\n    long startPos = -1;\n    newLogFile = getLogFilePath(logId);\n    try {\n      newStream = fs.create(newLogFile, false);\n    } catch (FileAlreadyExistsException e) {\n      LOG.error(\"Log file with id=\" + logId + \" already exists\", e);\n      return false;\n    } catch (RemoteException re) {\n      LOG.warn(\"failed to create log file with id=\" + logId, re);\n      return false;\n    }\n    // After we create the stream but before we attempt to use it at all\n    // ensure that we can provide the level of data safety we're configured\n    // to provide.\n    final String durability = useHsync ? \"hsync\" : \"hflush\";\n    if (!(CommonFSUtils.hasCapability(newStream, durability))) {\n      throw new IllegalStateException(\"The procedure WAL relies on the ability to \" + durability +\n          \" for proper operation during component failures, but the underlying filesystem does \" +\n          \"not support doing so. Please check the config value of '\" + USE_HSYNC_CONF_KEY +\n          \"' to set the desired level of robustness and ensure the config value of '\" +\n          CommonFSUtils.HBASE_WAL_DIR + \"' points to a FileSystem mount that can provide it.\");\n    }\n    try {\n      ProcedureWALFormat.writeHeader(newStream, header);\n      startPos = newStream.getPos();\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception writing header\", ioe);\n      newStream.close();\n      return false;\n    }\n\n    closeCurrentLogStream();\n\n    storeTracker.resetUpdates();\n    stream = newStream;\n    flushLogId = logId;\n    totalSynced.set(0);\n    long rollTs = System.currentTimeMillis();\n    lastRollTs.set(rollTs);\n    logs.add(new ProcedureWALFile(fs, newLogFile, header, startPos, rollTs));\n\n    // if it's the first next WAL being added, build the holding cleanup tracker\n    if (logs.size() == 2) {\n      buildHoldingCleanupTracker();\n    } else if (logs.size() > walCountWarnThreshold) {\n      LOG.warn(\"procedure WALs count=\" + logs.size() +\n        \" above the warning threshold \" + walCountWarnThreshold +\n        \". check running procedures to see if something is stuck.\");\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Roll new state log: \" + logId);\n    }\n    return true;\n  }",
            "1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033 +\n1034 +\n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  ",
            "  private boolean rollWriter(final long logId) throws IOException {\n    assert logId > flushLogId : \"logId=\" + logId + \" flushLogId=\" + flushLogId;\n    assert lock.isHeldByCurrentThread() : \"expected to be the lock owner. \" + lock.isLocked();\n\n    ProcedureWALHeader header = ProcedureWALHeader.newBuilder()\n      .setVersion(ProcedureWALFormat.HEADER_VERSION)\n      .setType(ProcedureWALFormat.LOG_TYPE_STREAM)\n      .setMinProcId(storeTracker.getActiveMinProcId())\n      .setLogId(logId)\n      .build();\n\n    FSDataOutputStream newStream = null;\n    Path newLogFile = null;\n    long startPos = -1;\n    newLogFile = getLogFilePath(logId);\n    try {\n      newStream = fs.create(newLogFile, false);\n    } catch (FileAlreadyExistsException e) {\n      LOG.error(\"Log file with id=\" + logId + \" already exists\", e);\n      return false;\n    } catch (RemoteException re) {\n      LOG.warn(\"failed to create log file with id=\" + logId, re);\n      return false;\n    }\n    // After we create the stream but before we attempt to use it at all\n    // ensure that we can provide the level of data safety we're configured\n    // to provide.\n    final String durability = useHsync ? \"hsync\" : \"hflush\";\n    if (enforceStreamCapability && !(CommonFSUtils.hasCapability(newStream, durability))) {\n        throw new IllegalStateException(\"The procedure WAL relies on the ability to \" + durability +\n          \" for proper operation during component failures, but the underlying filesystem does \" +\n          \"not support doing so. Please check the config value of '\" + USE_HSYNC_CONF_KEY +\n          \"' to set the desired level of robustness and ensure the config value of '\" +\n          CommonFSUtils.HBASE_WAL_DIR + \"' points to a FileSystem mount that can provide it.\");\n    }\n    try {\n      ProcedureWALFormat.writeHeader(newStream, header);\n      startPos = newStream.getPos();\n    } catch (IOException ioe) {\n      LOG.warn(\"Encountered exception writing header\", ioe);\n      newStream.close();\n      return false;\n    }\n\n    closeCurrentLogStream();\n\n    storeTracker.resetUpdates();\n    stream = newStream;\n    flushLogId = logId;\n    totalSynced.set(0);\n    long rollTs = System.currentTimeMillis();\n    lastRollTs.set(rollTs);\n    logs.add(new ProcedureWALFile(fs, newLogFile, header, startPos, rollTs));\n\n    // if it's the first next WAL being added, build the holding cleanup tracker\n    if (logs.size() == 2) {\n      buildHoldingCleanupTracker();\n    } else if (logs.size() > walCountWarnThreshold) {\n      LOG.warn(\"procedure WALs count=\" + logs.size() +\n        \" above the warning threshold \" + walCountWarnThreshold +\n        \". check running procedures to see if something is stuck.\");\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Roll new state log: \" + logId);\n    }\n    return true;\n  }"
        ],
        [
            "ProtobufLogWriter::initOutput(FileSystem,Path,boolean,int,short,long)",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  ",
            "  @SuppressWarnings(\"deprecation\")\n  @Override\n  protected void initOutput(FileSystem fs, Path path, boolean overwritable, int bufferSize,\n      short replication, long blockSize) throws IOException, StreamLacksCapabilityException {\n    this.output = fs.createNonRecursive(path, overwritable, bufferSize, replication, blockSize,\n      null);\n    // TODO Be sure to add a check for hsync if this branch includes HBASE-19024\n    if (!(CommonFSUtils.hasCapability(output, \"hflush\"))) {\n      throw new StreamLacksCapabilityException(\"hflush\");\n    }\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97 +\n  98  \n  99  \n 100  ",
            "  @SuppressWarnings(\"deprecation\")\n  @Override\n  protected void initOutput(FileSystem fs, Path path, boolean overwritable, int bufferSize,\n      short replication, long blockSize) throws IOException, StreamLacksCapabilityException {\n    this.output = fs.createNonRecursive(path, overwritable, bufferSize, replication, blockSize,\n      null);\n    // TODO Be sure to add a check for hsync if this branch includes HBASE-19024\n    if (fs.getConf().getBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, true) &&\n        !(CommonFSUtils.hasCapability(output, \"hflush\"))) {\n      throw new StreamLacksCapabilityException(\"hflush\");\n    }\n  }"
        ],
        [
            "AsyncFSOutputHelper::createOutput(FileSystem,Path,boolean,boolean,short,long,EventLoopGroup,Class)",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "  /**\n   * Create {@link FanOutOneBlockAsyncDFSOutput} for {@link DistributedFileSystem}, and a simple\n   * implementation for other {@link FileSystem} which wraps around a {@link FSDataOutputStream}.\n   */\n  public static AsyncFSOutput createOutput(FileSystem fs, Path f, boolean overwrite,\n      boolean createParent, short replication, long blockSize, EventLoopGroup eventLoopGroup,\n      Class<? extends Channel> channelClass)\n      throws IOException, CommonFSUtils.StreamLacksCapabilityException {\n    if (fs instanceof DistributedFileSystem) {\n      return FanOutOneBlockAsyncDFSOutputHelper.createOutput((DistributedFileSystem) fs, f,\n        overwrite, createParent, replication, blockSize, eventLoopGroup, channelClass);\n    }\n    final FSDataOutputStream fsOut;\n    int bufferSize = fs.getConf().getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY,\n      CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);\n    if (createParent) {\n      fsOut = fs.create(f, overwrite, bufferSize, replication, blockSize, null);\n    } else {\n      fsOut = fs.createNonRecursive(f, overwrite, bufferSize, replication, blockSize, null);\n    }\n    // After we create the stream but before we attempt to use it at all\n    // ensure that we can provide the level of data safety we're configured\n    // to provide.\n    if (!(CommonFSUtils.hasCapability(fsOut, \"hflush\") &&\n      CommonFSUtils.hasCapability(fsOut, \"hsync\"))) {\n      throw new CommonFSUtils.StreamLacksCapabilityException(\"hflush and hsync\");\n    }\n    final ExecutorService flushExecutor =\n      Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true)\n          .setNameFormat(\"AsyncFSOutputFlusher-\" + f.toString().replace(\"%\", \"%%\")).build());\n    return new AsyncFSOutput() {\n\n      private final ByteArrayOutputStream out = new ByteArrayOutputStream();\n\n      @Override\n      public void write(byte[] b, int off, int len) {\n        out.write(b, off, len);\n      }\n\n      @Override\n      public void write(byte[] b) {\n        write(b, 0, b.length);\n      }\n\n      @Override\n      public void writeInt(int i) {\n        out.writeInt(i);\n      }\n\n      @Override\n      public void write(ByteBuffer bb) {\n        out.write(bb, bb.position(), bb.remaining());\n      }\n\n      @Override\n      public void recoverAndClose(CancelableProgressable reporter) throws IOException {\n        fsOut.close();\n      }\n\n      @Override\n      public DatanodeInfo[] getPipeline() {\n        return new DatanodeInfo[0];\n      }\n\n      private void flush0(CompletableFuture<Long> future, boolean sync) {\n        try {\n          synchronized (out) {\n            fsOut.write(out.getBuffer(), 0, out.size());\n            out.reset();\n          }\n        } catch (IOException e) {\n          eventLoopGroup.next().execute(() -> future.completeExceptionally(e));\n          return;\n        }\n        try {\n          if (sync) {\n            fsOut.hsync();\n          } else {\n            fsOut.hflush();\n          }\n          long pos = fsOut.getPos();\n          eventLoopGroup.next().execute(() -> future.complete(pos));\n        } catch (IOException e) {\n          eventLoopGroup.next().execute(() -> future.completeExceptionally(e));\n        }\n      }\n\n      @Override\n      public CompletableFuture<Long> flush(boolean sync) {\n        CompletableFuture<Long> future = new CompletableFuture<>();\n        flushExecutor.execute(() -> flush0(future, sync));\n        return future;\n      }\n\n      @Override\n      public void close() throws IOException {\n        try {\n          flushExecutor.submit(() -> {\n            synchronized (out) {\n              fsOut.write(out.getBuffer(), 0, out.size());\n              out.reset();\n            }\n            return null;\n          }).get();\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException();\n        } catch (ExecutionException e) {\n          Throwables.propagateIfPossible(e.getCause(), IOException.class);\n          throw new IOException(e.getCause());\n        } finally {\n          flushExecutor.shutdown();\n        }\n        fsOut.close();\n      }\n\n      @Override\n      public int buffered() {\n        return out.size();\n      }\n    };\n  }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 +\n  77 +\n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "  /**\n   * Create {@link FanOutOneBlockAsyncDFSOutput} for {@link DistributedFileSystem}, and a simple\n   * implementation for other {@link FileSystem} which wraps around a {@link FSDataOutputStream}.\n   */\n  public static AsyncFSOutput createOutput(FileSystem fs, Path f, boolean overwrite,\n      boolean createParent, short replication, long blockSize, EventLoopGroup eventLoopGroup,\n      Class<? extends Channel> channelClass)\n      throws IOException, CommonFSUtils.StreamLacksCapabilityException {\n    if (fs instanceof DistributedFileSystem) {\n      return FanOutOneBlockAsyncDFSOutputHelper.createOutput((DistributedFileSystem) fs, f,\n        overwrite, createParent, replication, blockSize, eventLoopGroup, channelClass);\n    }\n    final FSDataOutputStream fsOut;\n    int bufferSize = fs.getConf().getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY,\n      CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);\n    if (createParent) {\n      fsOut = fs.create(f, overwrite, bufferSize, replication, blockSize, null);\n    } else {\n      fsOut = fs.createNonRecursive(f, overwrite, bufferSize, replication, blockSize, null);\n    }\n    // After we create the stream but before we attempt to use it at all\n    // ensure that we can provide the level of data safety we're configured\n    // to provide.\n    if (fs.getConf().getBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, true) &&\n        !(CommonFSUtils.hasCapability(fsOut, \"hflush\") &&\n          CommonFSUtils.hasCapability(fsOut, \"hsync\"))) {\n      throw new CommonFSUtils.StreamLacksCapabilityException(\"hflush and hsync\");\n    }\n    final ExecutorService flushExecutor =\n      Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true)\n          .setNameFormat(\"AsyncFSOutputFlusher-\" + f.toString().replace(\"%\", \"%%\")).build());\n    return new AsyncFSOutput() {\n\n      private final ByteArrayOutputStream out = new ByteArrayOutputStream();\n\n      @Override\n      public void write(byte[] b, int off, int len) {\n        out.write(b, off, len);\n      }\n\n      @Override\n      public void write(byte[] b) {\n        write(b, 0, b.length);\n      }\n\n      @Override\n      public void writeInt(int i) {\n        out.writeInt(i);\n      }\n\n      @Override\n      public void write(ByteBuffer bb) {\n        out.write(bb, bb.position(), bb.remaining());\n      }\n\n      @Override\n      public void recoverAndClose(CancelableProgressable reporter) throws IOException {\n        fsOut.close();\n      }\n\n      @Override\n      public DatanodeInfo[] getPipeline() {\n        return new DatanodeInfo[0];\n      }\n\n      private void flush0(CompletableFuture<Long> future, boolean sync) {\n        try {\n          synchronized (out) {\n            fsOut.write(out.getBuffer(), 0, out.size());\n            out.reset();\n          }\n        } catch (IOException e) {\n          eventLoopGroup.next().execute(() -> future.completeExceptionally(e));\n          return;\n        }\n        try {\n          if (sync) {\n            fsOut.hsync();\n          } else {\n            fsOut.hflush();\n          }\n          long pos = fsOut.getPos();\n          eventLoopGroup.next().execute(() -> future.complete(pos));\n        } catch (IOException e) {\n          eventLoopGroup.next().execute(() -> future.completeExceptionally(e));\n        }\n      }\n\n      @Override\n      public CompletableFuture<Long> flush(boolean sync) {\n        CompletableFuture<Long> future = new CompletableFuture<>();\n        flushExecutor.execute(() -> flush0(future, sync));\n        return future;\n      }\n\n      @Override\n      public void close() throws IOException {\n        try {\n          flushExecutor.submit(() -> {\n            synchronized (out) {\n              fsOut.write(out.getBuffer(), 0, out.size());\n              out.reset();\n            }\n            return null;\n          }).get();\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException();\n        } catch (ExecutionException e) {\n          Throwables.propagateIfPossible(e.getCause(), IOException.class);\n          throw new IOException(e.getCause());\n        } finally {\n          flushExecutor.shutdown();\n        }\n        fsOut.close();\n      }\n\n      @Override\n      public int buffered() {\n        return out.size();\n      }\n    };\n  }"
        ]
    ],
    "c24cf2d55ecd479c89b0613b6ebbdaba4eb793ad": [
        [
            "VerifyingRSGroupAdminClient::moveServersAndTables(Set,Set,String)",
            " 107  \n 108 -\n 109  \n 110  \n 111  ",
            "  @Override\n  public void moveServersAndTables(Set<Address> servers, Set<TableName> tables, String targetGroup) throws IOException {\n    wrapped.moveServersAndTables(servers, tables, targetGroup);\n    verify();\n  }",
            " 109  \n 110 +\n 111 +\n 112  \n 113  \n 114  ",
            "  @Override\n  public void moveServersAndTables(Set<Address> servers, Set<TableName> tables, String targetGroup)\n          throws IOException {\n    wrapped.moveServersAndTables(servers, tables, targetGroup);\n    verify();\n  }"
        ],
        [
            "RSGroupAdminEndpoint::RSGroupAdminServiceImpl::balanceRSGroup(RpcController,BalanceRSGroupRequest,RpcCallback)",
            " 230  \n 231  \n 232  \n 233  \n 234 -\n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "    @Override\n    public void balanceRSGroup(RpcController controller,\n        BalanceRSGroupRequest request, RpcCallback<BalanceRSGroupResponse> done) {\n      BalanceRSGroupResponse.Builder builder = BalanceRSGroupResponse.newBuilder();\n      LOG.info(master.getClientIdAuditPrefix() + \" balance rsgroup, group=\" + request.getRSGroupName());\n      try {\n        builder.setBalanceRan(groupAdminServer.balanceRSGroup(request.getRSGroupName()));\n      } catch (IOException e) {\n        CoprocessorRpcUtils.setControllerException(controller, e);\n        builder.setBalanceRan(false);\n      }\n      done.run(builder.build());\n    }",
            " 231  \n 232  \n 233  \n 234  \n 235 +\n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  ",
            "    @Override\n    public void balanceRSGroup(RpcController controller,\n        BalanceRSGroupRequest request, RpcCallback<BalanceRSGroupResponse> done) {\n      BalanceRSGroupResponse.Builder builder = BalanceRSGroupResponse.newBuilder();\n      LOG.info(master.getClientIdAuditPrefix() + \" balance rsgroup, group=\" +\n              request.getRSGroupName());\n      try {\n        builder.setBalanceRan(groupAdminServer.balanceRSGroup(request.getRSGroupName()));\n      } catch (IOException e) {\n        CoprocessorRpcUtils.setControllerException(controller, e);\n        builder.setBalanceRan(false);\n      }\n      done.run(builder.build());\n    }"
        ],
        [
            "RSGroupAdminServer::balanceRSGroup(String)",
            " 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  ",
            "  @Override\n  public boolean balanceRSGroup(String groupName) throws IOException {\n    ServerManager serverManager = master.getServerManager();\n    AssignmentManager assignmentManager = master.getAssignmentManager();\n    LoadBalancer balancer = master.getLoadBalancer();\n\n    synchronized (balancer) {\n      // If balance not true, don't run balancer.\n      if (!((HMaster) master).isBalancerOn()) return false;\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preBalanceRSGroup(groupName);\n      }\n      if (getRSGroupInfo(groupName) == null) {\n        throw new ConstraintException(\"RSGroup does not exist: \"+groupName);\n      }\n      // Only allow one balance run at at time.\n      Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);\n      if (groupRIT.size() > 0) {\n        LOG.debug(\"Not running balancer because \" + groupRIT.size() + \" region(s) in transition: \" +\n          StringUtils.abbreviate(\n              master.getAssignmentManager().getRegionStates().getRegionsInTransition().toString(),\n              256));\n        return false;\n      }\n      if (serverManager.areDeadServersInProgress()) {\n        LOG.debug(\"Not running balancer because processing dead regionserver(s): \" +\n            serverManager.getDeadServers());\n        return false;\n      }\n\n      //We balance per group instead of per table\n      List<RegionPlan> plans = new ArrayList<>();\n      for(Map.Entry<TableName, Map<ServerName, List<RegionInfo>>> tableMap:\n          getRSGroupAssignmentsByTable(groupName).entrySet()) {\n        LOG.info(\"Creating partial plan for table \" + tableMap.getKey() + \": \"\n            + tableMap.getValue());\n        List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());\n        LOG.info(\"Partial plan for table \" + tableMap.getKey() + \": \" + partialPlans);\n        if (partialPlans != null) {\n          plans.addAll(partialPlans);\n        }\n      }\n      long startTime = System.currentTimeMillis();\n      boolean balancerRan = !plans.isEmpty();\n      if (balancerRan) {\n        LOG.info(\"RSGroup balance \" + groupName + \" starting with plan count: \" + plans.size());\n        for (RegionPlan plan: plans) {\n          LOG.info(\"balance \" + plan);\n          assignmentManager.moveAsync(plan);\n        }\n        LOG.info(\"RSGroup balance \" + groupName + \" completed after \" +\n            (System.currentTimeMillis()-startTime) + \" seconds\");\n      }\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postBalanceRSGroup(groupName, balancerRan);\n      }\n      return balancerRan;\n    }\n  }",
            " 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492 +\n 493 +\n 494 +\n 495 +\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  ",
            "  @Override\n  public boolean balanceRSGroup(String groupName) throws IOException {\n    ServerManager serverManager = master.getServerManager();\n    AssignmentManager assignmentManager = master.getAssignmentManager();\n    LoadBalancer balancer = master.getLoadBalancer();\n\n    synchronized (balancer) {\n      // If balance not true, don't run balancer.\n      if (!((HMaster) master).isBalancerOn()) {\n        return false;\n      }\n\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preBalanceRSGroup(groupName);\n      }\n      if (getRSGroupInfo(groupName) == null) {\n        throw new ConstraintException(\"RSGroup does not exist: \"+groupName);\n      }\n      // Only allow one balance run at at time.\n      Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);\n      if (groupRIT.size() > 0) {\n        LOG.debug(\"Not running balancer because \" + groupRIT.size() + \" region(s) in transition: \" +\n          StringUtils.abbreviate(\n              master.getAssignmentManager().getRegionStates().getRegionsInTransition().toString(),\n              256));\n        return false;\n      }\n      if (serverManager.areDeadServersInProgress()) {\n        LOG.debug(\"Not running balancer because processing dead regionserver(s): \" +\n            serverManager.getDeadServers());\n        return false;\n      }\n\n      //We balance per group instead of per table\n      List<RegionPlan> plans = new ArrayList<>();\n      for(Map.Entry<TableName, Map<ServerName, List<RegionInfo>>> tableMap:\n          getRSGroupAssignmentsByTable(groupName).entrySet()) {\n        LOG.info(\"Creating partial plan for table \" + tableMap.getKey() + \": \"\n            + tableMap.getValue());\n        List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());\n        LOG.info(\"Partial plan for table \" + tableMap.getKey() + \": \" + partialPlans);\n        if (partialPlans != null) {\n          plans.addAll(partialPlans);\n        }\n      }\n      long startTime = System.currentTimeMillis();\n      boolean balancerRan = !plans.isEmpty();\n      if (balancerRan) {\n        LOG.info(\"RSGroup balance \" + groupName + \" starting with plan count: \" + plans.size());\n        for (RegionPlan plan: plans) {\n          LOG.info(\"balance \" + plan);\n          assignmentManager.moveAsync(plan);\n        }\n        LOG.info(\"RSGroup balance \" + groupName + \" completed after \" +\n            (System.currentTimeMillis()-startTime) + \" seconds\");\n      }\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postBalanceRSGroup(groupName, balancerRan);\n      }\n      return balancerRan;\n    }\n  }"
        ],
        [
            "RSGroupAdminServer::moveRegionsToServers(Set,Set,String)",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258 -\n 259  \n 260  \n 261  \n 262  \n 263  \n 264  ",
            "  /**\n   * Moves every region of tables which should be kept on the servers,\n   * but currently they are located on other servers.\n   * @param servers the regions of these servers will be kept on the servers,\n   * others will be moved\n   * @param tables the tables that will move to new group\n   * @param targetGroupName the target group name\n   * @throws IOException\n   */\n  private void moveRegionsToServers(Set<Address> servers, Set<TableName> tables,\n      String targetGroupName) throws IOException {\n    for (TableName table: tables) {\n      LOG.info(\"Moving region(s) from \" + table + \" for table move to \" + targetGroupName);\n      for (RegionInfo region : master.getAssignmentManager().getRegionStates().getRegionsOfTable(table)) {\n        ServerName sn = master.getAssignmentManager().getRegionStates().getRegionServerOfRegion(region);\n        if (!servers.contains(sn.getAddress())) {\n          master.getAssignmentManager().move(region);\n        }\n      }\n    }\n  }",
            " 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 +\n 263 +\n 264 +\n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "  /**\n   * Moves every region of tables which should be kept on the servers,\n   * but currently they are located on other servers.\n   * @param servers the regions of these servers will be kept on the servers, others will be moved\n   * @param tables the tables that will move to new group\n   * @param targetGroupName the target group name\n   * @throws IOException if moving the region fails\n   */\n  private void moveRegionsToServers(Set<Address> servers, Set<TableName> tables,\n      String targetGroupName) throws IOException {\n    for (TableName table: tables) {\n      LOG.info(\"Moving region(s) from \" + table + \" for table move to \" + targetGroupName);\n      for (RegionInfo region : master.getAssignmentManager().getRegionStates()\n              .getRegionsOfTable(table)) {\n        ServerName sn = master.getAssignmentManager().getRegionStates()\n                .getRegionServerOfRegion(region);\n        if (!servers.contains(sn.getAddress())) {\n          master.getAssignmentManager().move(region);\n        }\n      }\n    }\n  }"
        ],
        [
            "RSGroupAdminServer::moveServers(Set,String)",
            " 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 -\n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(\n      value=\"RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE\",\n      justification=\"Ignoring complaint because don't know what it is complaining about\")\n  @Override\n  public void moveServers(Set<Address> servers, String targetGroupName)\n  throws IOException {\n    if (servers == null) {\n      throw new ConstraintException(\"The list of servers to move cannot be null.\");\n    }\n    if (servers.isEmpty()) {\n      // For some reason this difference between null servers and isEmpty is important distinction.\n      // TODO. Why? Stuff breaks if I equate them.\n      return;\n    }\n    RSGroupInfo targetGrp = getAndCheckRSGroupInfo(targetGroupName);\n\n    // Hold a lock on the manager instance while moving servers to prevent\n    // another writer changing our state while we are working.\n    synchronized (rsGroupInfoManager) {\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preMoveServers(servers, targetGroupName);\n      }\n      // Presume first server's source group. Later ensure all servers are from this group.\n      Address firstServer = servers.iterator().next();\n      RSGroupInfo srcGrp = rsGroupInfoManager.getRSGroupOfServer(firstServer);\n      if (srcGrp == null) {\n        // Be careful. This exception message is tested for in TestRSGroupsBase...\n        throw new ConstraintException(\"Source RSGroup for server \" + firstServer\n            + \" does not exist.\");\n      }\n      if (srcGrp.getName().equals(targetGroupName)) {\n        throw new ConstraintException( \"Target RSGroup \" + targetGroupName +\n            \" is same as source \" + srcGrp + \" RSGroup.\");\n      }\n      // Only move online servers (when moving from 'default') or servers from other\n      // groups. This prevents bogus servers from entering groups\n      if (RSGroupInfo.DEFAULT_GROUP.equals(srcGrp.getName())) {\n        checkOnlineServersOnly(servers);\n      }\n      // Ensure all servers are of same rsgroup.\n      for (Address server: servers) {\n        String tmpGroup = rsGroupInfoManager.getRSGroupOfServer(server).getName();\n        if (!tmpGroup.equals(srcGrp.getName())) {\n          throw new ConstraintException(\"Move server request should only come from one source \" +\n              \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n        }\n      }\n      if (srcGrp.getServers().size() <= servers.size() && srcGrp.getTables().size() > 0) {\n        throw new ConstraintException(\"Cannot leave a RSGroup \" + srcGrp.getName() +\n            \" that contains tables without servers to host them.\");\n      }\n\n      // MovedServers may be < passed in 'servers'.\n      Set<Address> movedServers = rsGroupInfoManager.moveServers(servers, srcGrp.getName(),\n          targetGroupName);\n      List<Address> editableMovedServers = Lists.newArrayList(movedServers);\n      boolean foundRegionsToMove;\n      do {\n        foundRegionsToMove = false;\n        for (Iterator<Address> iter = editableMovedServers.iterator(); iter.hasNext();) {\n          Address rs = iter.next();\n          // Get regions that are associated with this server.\n          List<RegionInfo> regions = getRegions(rs);\n\n          LOG.info(\"Moving \" + regions.size() + \" region(s) from \" + rs +\n              \" for server move to \" + targetGroupName);\n\n          for (RegionInfo region: regions) {\n            // Regions might get assigned from tables of target group so we need to filter\n            if (targetGrp.containsTable(region.getTable())) {\n              continue;\n            }\n            LOG.info(\"Moving region \" + region.getShortNameToLog());\n            this.master.getAssignmentManager().move(region);\n            if (master.getAssignmentManager().getRegionStates().\n                getRegionState(region).isFailedOpen()) {\n              // If region is in FAILED_OPEN state, it won't recover, not without\n              // operator intervention... in hbase-2.0.0 at least. Continue rather\n              // than mark region as 'foundRegionsToMove'.\n              continue;\n            }\n            foundRegionsToMove = true;\n          }\n          if (!foundRegionsToMove) {\n            iter.remove();\n          }\n        }\n        try {\n          rsGroupInfoManager.wait(1000);\n        } catch (InterruptedException e) {\n          LOG.warn(\"Sleep interrupted\", e);\n          Thread.currentThread().interrupt();\n        }\n      } while (foundRegionsToMove);\n\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postMoveServers(servers, targetGroupName);\n      }\n      LOG.info(\"Move server done: \" + srcGrp.getName() + \"=>\" + targetGroupName);\n    }\n  }",
            " 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(\n      value=\"RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE\",\n      justification=\"Ignoring complaint because don't know what it is complaining about\")\n  @Override\n  public void moveServers(Set<Address> servers, String targetGroupName)\n  throws IOException {\n    if (servers == null) {\n      throw new ConstraintException(\"The list of servers to move cannot be null.\");\n    }\n    if (servers.isEmpty()) {\n      // For some reason this difference between null servers and isEmpty is important distinction.\n      // TODO. Why? Stuff breaks if I equate them.\n      return;\n    }\n    RSGroupInfo targetGrp = getAndCheckRSGroupInfo(targetGroupName);\n\n    // Hold a lock on the manager instance while moving servers to prevent\n    // another writer changing our state while we are working.\n    synchronized (rsGroupInfoManager) {\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preMoveServers(servers, targetGroupName);\n      }\n      // Presume first server's source group. Later ensure all servers are from this group.\n      Address firstServer = servers.iterator().next();\n      RSGroupInfo srcGrp = rsGroupInfoManager.getRSGroupOfServer(firstServer);\n      if (srcGrp == null) {\n        // Be careful. This exception message is tested for in TestRSGroupsBase...\n        throw new ConstraintException(\"Source RSGroup for server \" + firstServer\n            + \" does not exist.\");\n      }\n      if (srcGrp.getName().equals(targetGroupName)) {\n        throw new ConstraintException(\"Target RSGroup \" + targetGroupName +\n            \" is same as source \" + srcGrp + \" RSGroup.\");\n      }\n      // Only move online servers (when moving from 'default') or servers from other\n      // groups. This prevents bogus servers from entering groups\n      if (RSGroupInfo.DEFAULT_GROUP.equals(srcGrp.getName())) {\n        checkOnlineServersOnly(servers);\n      }\n      // Ensure all servers are of same rsgroup.\n      for (Address server: servers) {\n        String tmpGroup = rsGroupInfoManager.getRSGroupOfServer(server).getName();\n        if (!tmpGroup.equals(srcGrp.getName())) {\n          throw new ConstraintException(\"Move server request should only come from one source \" +\n              \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n        }\n      }\n      if (srcGrp.getServers().size() <= servers.size() && srcGrp.getTables().size() > 0) {\n        throw new ConstraintException(\"Cannot leave a RSGroup \" + srcGrp.getName() +\n            \" that contains tables without servers to host them.\");\n      }\n\n      // MovedServers may be < passed in 'servers'.\n      Set<Address> movedServers = rsGroupInfoManager.moveServers(servers, srcGrp.getName(),\n          targetGroupName);\n      List<Address> editableMovedServers = Lists.newArrayList(movedServers);\n      boolean foundRegionsToMove;\n      do {\n        foundRegionsToMove = false;\n        for (Iterator<Address> iter = editableMovedServers.iterator(); iter.hasNext();) {\n          Address rs = iter.next();\n          // Get regions that are associated with this server.\n          List<RegionInfo> regions = getRegions(rs);\n\n          LOG.info(\"Moving \" + regions.size() + \" region(s) from \" + rs +\n              \" for server move to \" + targetGroupName);\n\n          for (RegionInfo region: regions) {\n            // Regions might get assigned from tables of target group so we need to filter\n            if (targetGrp.containsTable(region.getTable())) {\n              continue;\n            }\n            LOG.info(\"Moving region \" + region.getShortNameToLog());\n            this.master.getAssignmentManager().move(region);\n            if (master.getAssignmentManager().getRegionStates().\n                getRegionState(region).isFailedOpen()) {\n              // If region is in FAILED_OPEN state, it won't recover, not without\n              // operator intervention... in hbase-2.0.0 at least. Continue rather\n              // than mark region as 'foundRegionsToMove'.\n              continue;\n            }\n            foundRegionsToMove = true;\n          }\n          if (!foundRegionsToMove) {\n            iter.remove();\n          }\n        }\n        try {\n          rsGroupInfoManager.wait(1000);\n        } catch (InterruptedException e) {\n          LOG.warn(\"Sleep interrupted\", e);\n          Thread.currentThread().interrupt();\n        }\n      } while (foundRegionsToMove);\n\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postMoveServers(servers, targetGroupName);\n      }\n      LOG.info(\"Move server done: \" + srcGrp.getName() + \"=>\" + targetGroupName);\n    }\n  }"
        ],
        [
            "RSGroupBasedLoadBalancer::findServerForRegion(Map,RegionInfo)",
            " 326  \n 327 -\n 328 -\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "  private ServerName findServerForRegion(\n      Map<ServerName, List<RegionInfo>> existingAssignments, RegionInfo region)\n  {\n    for (Map.Entry<ServerName, List<RegionInfo>> entry : existingAssignments.entrySet()) {\n      if (entry.getValue().contains(region)) {\n        return entry.getKey();\n      }\n    }\n\n    throw new IllegalStateException(\"Could not find server for region \"\n        + region.getShortNameToLog());\n  }",
            " 327  \n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "  private ServerName findServerForRegion(\n      Map<ServerName, List<RegionInfo>> existingAssignments, RegionInfo region) {\n    for (Map.Entry<ServerName, List<RegionInfo>> entry : existingAssignments.entrySet()) {\n      if (entry.getValue().contains(region)) {\n        return entry.getKey();\n      }\n    }\n\n    throw new IllegalStateException(\"Could not find server for region \"\n        + region.getShortNameToLog());\n  }"
        ],
        [
            "RSGroupAdminServer::getRegions(Address)",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "  /**\n   * @return List of Regions associated with this <code>server</code>.\n   */\n  private List<RegionInfo> getRegions(final Address server) {\n    LinkedList<RegionInfo> regions = new LinkedList<>();\n    for (Map.Entry<RegionInfo, ServerName> el :\n        master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {\n      if (el.getValue() == null) continue;\n      if (el.getValue().getAddress().equals(server)) {\n        addRegion(regions, el.getKey());\n      }\n    }\n    for (RegionStateNode state : master.getAssignmentManager().getRegionsInTransition()) {\n      if (state.getRegionLocation().getAddress().equals(server)) {\n        addRegion(regions, state.getRegionInfo());\n      }\n    }\n    return regions;\n  }",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121 +\n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  /**\n   * @return List of Regions associated with this <code>server</code>.\n   */\n  private List<RegionInfo> getRegions(final Address server) {\n    LinkedList<RegionInfo> regions = new LinkedList<>();\n    for (Map.Entry<RegionInfo, ServerName> el :\n        master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {\n      if (el.getValue() == null) {\n        continue;\n      }\n\n      if (el.getValue().getAddress().equals(server)) {\n        addRegion(regions, el.getKey());\n      }\n    }\n    for (RegionStateNode state : master.getAssignmentManager().getRegionsInTransition()) {\n      if (state.getRegionLocation().getAddress().equals(server)) {\n        addRegion(regions, state.getRegionInfo());\n      }\n    }\n    return regions;\n  }"
        ],
        [
            "TestRSGroupBasedLoadBalancer::assertRetainedAssignment(Map,List,Map)",
            " 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308 -\n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "  /**\n   * Asserts a valid retained assignment plan.\n   * <p>\n   * Must meet the following conditions:\n   * <ul>\n   * <li>Every input region has an assignment, and to an online server\n   * <li>If a region had an existing assignment to a server with the same\n   * address a a currently online server, it will be assigned to it\n   * </ul>\n   *\n   * @param existing\n   * @param assignment\n   * @throws java.io.IOException\n   * @throws java.io.FileNotFoundException\n   */\n  private void assertRetainedAssignment(\n      Map<RegionInfo, ServerName> existing, List<ServerName> servers,\n      Map<ServerName, List<RegionInfo>> assignment)\n      throws FileNotFoundException, IOException {\n    // Verify condition 1, every region assigned, and to online server\n    Set<ServerName> onlineServerSet = new TreeSet<>(servers);\n    Set<RegionInfo> assignedRegions = new TreeSet<>(RegionInfo.COMPARATOR);\n    for (Map.Entry<ServerName, List<RegionInfo>> a : assignment.entrySet()) {\n      assertTrue(\n          \"Region assigned to server that was not listed as online\",\n          onlineServerSet.contains(a.getKey()));\n      for (RegionInfo r : a.getValue())\n        assignedRegions.add(r);\n    }\n    assertEquals(existing.size(), assignedRegions.size());\n\n    // Verify condition 2, every region must be assigned to correct server.\n    Set<String> onlineHostNames = new TreeSet<>();\n    for (ServerName s : servers) {\n      onlineHostNames.add(s.getHostname());\n    }\n\n    for (Map.Entry<ServerName, List<RegionInfo>> a : assignment.entrySet()) {\n      ServerName currentServer = a.getKey();\n      for (RegionInfo r : a.getValue()) {\n        ServerName oldAssignedServer = existing.get(r);\n        TableName tableName = r.getTable();\n        String groupName =\n            getMockedGroupInfoManager().getRSGroupOfTable(tableName);\n        assertTrue(StringUtils.isNotEmpty(groupName));\n        RSGroupInfo gInfo = getMockedGroupInfoManager().getRSGroup(\n            groupName);\n        assertTrue(\n            \"Region is not correctly assigned to group servers.\",\n            gInfo.containsServer(currentServer.getAddress()));\n        if (oldAssignedServer != null\n            && onlineHostNames.contains(oldAssignedServer\n            .getHostname())) {\n          // this region was previously assigned somewhere, and that\n          // host is still around, then the host must have been is a\n          // different group.\n          if (!oldAssignedServer.getAddress().equals(currentServer.getAddress())) {\n            assertFalse(gInfo.containsServer(oldAssignedServer.getAddress()));\n          }\n        }\n      }\n    }\n  }",
            " 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 +\n 289  \n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  ",
            "  /**\n   * Asserts a valid retained assignment plan.\n   * <p>\n   * Must meet the following conditions:\n   * <ul>\n   * <li>Every input region has an assignment, and to an online server\n   * <li>If a region had an existing assignment to a server with the same\n   * address a a currently online server, it will be assigned to it\n   * </ul>\n   */\n  private void assertRetainedAssignment(\n      Map<RegionInfo, ServerName> existing, List<ServerName> servers,\n      Map<ServerName, List<RegionInfo>> assignment)\n      throws FileNotFoundException, IOException {\n    // Verify condition 1, every region assigned, and to online server\n    Set<ServerName> onlineServerSet = new TreeSet<>(servers);\n    Set<RegionInfo> assignedRegions = new TreeSet<>(RegionInfo.COMPARATOR);\n    for (Map.Entry<ServerName, List<RegionInfo>> a : assignment.entrySet()) {\n      assertTrue(\n          \"Region assigned to server that was not listed as online\",\n          onlineServerSet.contains(a.getKey()));\n      for (RegionInfo r : a.getValue()) {\n        assignedRegions.add(r);\n      }\n    }\n    assertEquals(existing.size(), assignedRegions.size());\n\n    // Verify condition 2, every region must be assigned to correct server.\n    Set<String> onlineHostNames = new TreeSet<>();\n    for (ServerName s : servers) {\n      onlineHostNames.add(s.getHostname());\n    }\n\n    for (Map.Entry<ServerName, List<RegionInfo>> a : assignment.entrySet()) {\n      ServerName currentServer = a.getKey();\n      for (RegionInfo r : a.getValue()) {\n        ServerName oldAssignedServer = existing.get(r);\n        TableName tableName = r.getTable();\n        String groupName =\n            getMockedGroupInfoManager().getRSGroupOfTable(tableName);\n        assertTrue(StringUtils.isNotEmpty(groupName));\n        RSGroupInfo gInfo = getMockedGroupInfoManager().getRSGroup(\n            groupName);\n        assertTrue(\n            \"Region is not correctly assigned to group servers.\",\n            gInfo.containsServer(currentServer.getAddress()));\n        if (oldAssignedServer != null\n            && onlineHostNames.contains(oldAssignedServer\n            .getHostname())) {\n          // this region was previously assigned somewhere, and that\n          // host is still around, then the host must have been is a\n          // different group.\n          if (!oldAssignedServer.getAddress().equals(currentServer.getAddress())) {\n            assertFalse(gInfo.containsServer(oldAssignedServer.getAddress()));\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "TestRSGroupsBase::testRegionMove()",
            " 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519 -\n 520 -\n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  ",
            "  @Test\n  public void testRegionMove() throws Exception {\n    final RSGroupInfo newGroup = addGroup(getGroupName(name.getMethodName()), 1);\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    // All the regions created below will be assigned to the default group.\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 6);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null)\n          return false;\n        return getTableRegionMap().get(tableName).size() >= 6;\n      }\n    });\n\n    //get target region to move\n    Map<ServerName,List<String>> assignMap =\n        getTableServerRegionMap().get(tableName);\n    String targetRegion = null;\n    for(ServerName server : assignMap.keySet()) {\n      targetRegion = assignMap.get(server).size() > 0 ? assignMap.get(server).get(0) : null;\n      if(targetRegion != null) {\n        break;\n      }\n    }\n    //get server which is not a member of new group\n    ServerName targetServer = null;\n    for (ServerName server : admin.getClusterStatus(EnumSet.of(Option.LIVE_SERVERS))\n                                  .getServers()) {\n      if (!newGroup.containsServer(server.getAddress())) {\n        targetServer = server;\n        break;\n      }\n    }\n\n    final AdminProtos.AdminService.BlockingInterface targetRS =\n      ((ClusterConnection) admin.getConnection()).getAdmin(targetServer);\n\n    //move target server to group\n    rsGroupAdmin.moveServers(Sets.newHashSet(targetServer.getAddress()),\n        newGroup.getName());\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return ProtobufUtil.getOnlineRegions(targetRS).size() <= 0;\n      }\n    });\n\n    // Lets move this region to the new group.\n    TEST_UTIL.getAdmin().move(Bytes.toBytes(RegionInfo.encodeRegionName(Bytes.toBytes(targetRegion))),\n        Bytes.toBytes(targetServer.getServerName()));\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return\n            getTableRegionMap().get(tableName) != null &&\n                getTableRegionMap().get(tableName).size() == 6 &&\n                admin.getClusterStatus(EnumSet.of(Option.REGIONS_IN_TRANSITION))\n                     .getRegionStatesInTransition().size() < 1;\n      }\n    });\n\n    //verify that targetServer didn't open it\n    for (RegionInfo region: ProtobufUtil.getOnlineRegions(targetRS)) {\n      if (targetRegion.equals(region.getRegionNameAsString())) {\n        fail(\"Target server opened region\");\n      }\n    }\n  }",
            " 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481 +\n 482  \n 483 +\n 484 +\n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523 +\n 524 +\n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  ",
            "  @Test\n  public void testRegionMove() throws Exception {\n    final RSGroupInfo newGroup = addGroup(getGroupName(name.getMethodName()), 1);\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    // All the regions created below will be assigned to the default group.\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 6);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null) {\n          return false;\n        }\n\n        return getTableRegionMap().get(tableName).size() >= 6;\n      }\n    });\n\n    //get target region to move\n    Map<ServerName,List<String>> assignMap =\n        getTableServerRegionMap().get(tableName);\n    String targetRegion = null;\n    for(ServerName server : assignMap.keySet()) {\n      targetRegion = assignMap.get(server).size() > 0 ? assignMap.get(server).get(0) : null;\n      if(targetRegion != null) {\n        break;\n      }\n    }\n    //get server which is not a member of new group\n    ServerName targetServer = null;\n    for (ServerName server : admin.getClusterStatus(EnumSet.of(Option.LIVE_SERVERS))\n                                  .getServers()) {\n      if (!newGroup.containsServer(server.getAddress())) {\n        targetServer = server;\n        break;\n      }\n    }\n\n    final AdminProtos.AdminService.BlockingInterface targetRS =\n      ((ClusterConnection) admin.getConnection()).getAdmin(targetServer);\n\n    //move target server to group\n    rsGroupAdmin.moveServers(Sets.newHashSet(targetServer.getAddress()),\n        newGroup.getName());\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return ProtobufUtil.getOnlineRegions(targetRS).size() <= 0;\n      }\n    });\n\n    // Lets move this region to the new group.\n    TEST_UTIL.getAdmin().move(Bytes.toBytes(RegionInfo.encodeRegionName(\n            Bytes.toBytes(targetRegion))), Bytes.toBytes(targetServer.getServerName()));\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return\n            getTableRegionMap().get(tableName) != null &&\n                getTableRegionMap().get(tableName).size() == 6 &&\n                admin.getClusterStatus(EnumSet.of(Option.REGIONS_IN_TRANSITION))\n                     .getRegionStatesInTransition().size() < 1;\n      }\n    });\n\n    //verify that targetServer didn't open it\n    for (RegionInfo region: ProtobufUtil.getOnlineRegions(targetRS)) {\n      if (targetRegion.equals(region.getRegionNameAsString())) {\n        fail(\"Target server opened region\");\n      }\n    }\n  }"
        ],
        [
            "TestRSGroupsBase::testMultiTableMove()",
            " 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679 -\n 680  \n 681  \n 682 -\n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707 -\n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  ",
            "  @Test\n  public void testMultiTableMove() throws Exception {\n    final TableName tableNameA = TableName.valueOf(tablePrefix + name.getMethodName() + \"A\");\n    final TableName tableNameB = TableName.valueOf(tablePrefix + name.getMethodName() + \"B\");\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    String newGroupName = getGroupName(name.getMethodName());\n    final RSGroupInfo newGroup = addGroup(newGroupName, 1);\n\n    TEST_UTIL.createTable(tableNameA, familyNameBytes);\n    TEST_UTIL.createTable(tableNameB, familyNameBytes);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regionsA = getTableRegionMap().get(tableNameA);\n        if (regionsA == null)\n          return false;\n        List<String> regionsB = getTableRegionMap().get(tableNameB);\n        if (regionsB == null)\n          return false;\n\n        return getTableRegionMap().get(tableNameA).size() >= 1\n                && getTableRegionMap().get(tableNameB).size() >= 1;\n      }\n    });\n\n    RSGroupInfo tableGrpA = rsGroupAdmin.getRSGroupInfoOfTable(tableNameA);\n    assertTrue(tableGrpA.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n\n    RSGroupInfo tableGrpB = rsGroupAdmin.getRSGroupInfoOfTable(tableNameB);\n    assertTrue(tableGrpB.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n    //change table's group\n    LOG.info(\"Moving table [\" + tableNameA + \",\" + tableNameB + \"] to \" + newGroup.getName());\n    rsGroupAdmin.moveTables(Sets.newHashSet(tableNameA, tableNameB), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableNameA).getName());\n\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableNameB).getName());\n\n    //verify tables' not exist in old group\n    Set<TableName> DefaultTables = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables();\n    assertFalse(DefaultTables.contains(tableNameA));\n    assertFalse(DefaultTables.contains(tableNameB));\n\n    //verify tables' exist in new group\n    Set<TableName> newGroupTables = rsGroupAdmin.getRSGroupInfo(newGroupName).getTables();\n    assertTrue(newGroupTables.contains(tableNameA));\n    assertTrue(newGroupTables.contains(tableNameB));\n  }",
            " 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683 +\n 684  \n 685 +\n 686 +\n 687  \n 688 +\n 689  \n 690 +\n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714 +\n 715 +\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  ",
            "  @Test\n  public void testMultiTableMove() throws Exception {\n    final TableName tableNameA = TableName.valueOf(tablePrefix + name.getMethodName() + \"A\");\n    final TableName tableNameB = TableName.valueOf(tablePrefix + name.getMethodName() + \"B\");\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    String newGroupName = getGroupName(name.getMethodName());\n    final RSGroupInfo newGroup = addGroup(newGroupName, 1);\n\n    TEST_UTIL.createTable(tableNameA, familyNameBytes);\n    TEST_UTIL.createTable(tableNameB, familyNameBytes);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regionsA = getTableRegionMap().get(tableNameA);\n        if (regionsA == null) {\n          return false;\n        }\n\n        List<String> regionsB = getTableRegionMap().get(tableNameB);\n        if (regionsB == null) {\n          return false;\n        }\n\n        return getTableRegionMap().get(tableNameA).size() >= 1\n                && getTableRegionMap().get(tableNameB).size() >= 1;\n      }\n    });\n\n    RSGroupInfo tableGrpA = rsGroupAdmin.getRSGroupInfoOfTable(tableNameA);\n    assertTrue(tableGrpA.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n\n    RSGroupInfo tableGrpB = rsGroupAdmin.getRSGroupInfoOfTable(tableNameB);\n    assertTrue(tableGrpB.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n    //change table's group\n    LOG.info(\"Moving table [\" + tableNameA + \",\" + tableNameB + \"] to \" + newGroup.getName());\n    rsGroupAdmin.moveTables(Sets.newHashSet(tableNameA, tableNameB), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableNameA).getName());\n\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableNameB).getName());\n\n    //verify tables' not exist in old group\n    Set<TableName> DefaultTables = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP)\n            .getTables();\n    assertFalse(DefaultTables.contains(tableNameA));\n    assertFalse(DefaultTables.contains(tableNameB));\n\n    //verify tables' exist in new group\n    Set<TableName> newGroupTables = rsGroupAdmin.getRSGroupInfo(newGroupName).getTables();\n    assertTrue(newGroupTables.contains(tableNameA));\n    assertTrue(newGroupTables.contains(tableNameB));\n  }"
        ],
        [
            "RSGroupBasedLoadBalancer::isOnline()",
            " 394  \n 395 -\n 396  \n 397  ",
            "  public boolean isOnline() {\n    if (this.rsGroupInfoManager == null) return false;\n    return this.rsGroupInfoManager.isOnline();\n  }",
            " 394  \n 395 +\n 396 +\n 397 +\n 398 +\n 399  \n 400  ",
            "  public boolean isOnline() {\n    if (this.rsGroupInfoManager == null) {\n      return false;\n    }\n\n    return this.rsGroupInfoManager.isOnline();\n  }"
        ],
        [
            "RSGroupAdminServer::checkServersAndTables(Set,Set,String)",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185 -\n 186  \n 187  \n 188  \n 189  ",
            "  /**\n   * Check servers and tables.\n   * Fail if nulls or if servers and tables not belong to the same group\n   * @param servers servers to move\n   * @param tables tables to move\n   * @param targetGroupName target group name\n   * @throws IOException\n   */\n  private void checkServersAndTables(Set<Address> servers, Set<TableName> tables,\n                                     String targetGroupName) throws IOException {\n    // Presume first server's source group. Later ensure all servers are from this group.\n    Address firstServer = servers.iterator().next();\n    RSGroupInfo tmpSrcGrp = rsGroupInfoManager.getRSGroupOfServer(firstServer);\n    if (tmpSrcGrp == null) {\n      // Be careful. This exception message is tested for in TestRSGroupsBase...\n      throw new ConstraintException(\"Source RSGroup for server \" + firstServer\n              + \" does not exist.\");\n    }\n    RSGroupInfo srcGrp = new RSGroupInfo(tmpSrcGrp);\n    if (srcGrp.getName().equals(targetGroupName)) {\n      throw new ConstraintException( \"Target RSGroup \" + targetGroupName +\n              \" is same as source \" + srcGrp.getName() + \" RSGroup.\");\n    }\n    // Only move online servers\n    checkOnlineServersOnly(servers);\n\n    // Ensure all servers are of same rsgroup.\n    for (Address server: servers) {\n      String tmpGroup = rsGroupInfoManager.getRSGroupOfServer(server).getName();\n      if (!tmpGroup.equals(srcGrp.getName())) {\n        throw new ConstraintException(\"Move server request should only come from one source \" +\n                \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n      }\n    }\n\n    // Ensure all tables and servers are of same rsgroup.\n    for (TableName table : tables) {\n      String tmpGroup = rsGroupInfoManager.getRSGroupOfTable(table);\n      if (!tmpGroup.equals(srcGrp.getName())) {\n        throw new ConstraintException(\"Move table request should only come from one source \" +\n                \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n      }\n    }\n\n    if (srcGrp.getServers().size() <= servers.size()\n            && srcGrp.getTables().size() > tables.size() ) {\n      throw new ConstraintException(\"Cannot leave a RSGroup \" + srcGrp.getName() +\n              \" that contains tables without servers to host them.\");\n    }\n  }",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192  \n 193  \n 194  \n 195  ",
            "  /**\n   * Check servers and tables.\n   *\n   * @param servers servers to move\n   * @param tables tables to move\n   * @param targetGroupName target group name\n   * @throws IOException if nulls or if servers and tables not belong to the same group\n   */\n  private void checkServersAndTables(Set<Address> servers, Set<TableName> tables,\n                                     String targetGroupName) throws IOException {\n    // Presume first server's source group. Later ensure all servers are from this group.\n    Address firstServer = servers.iterator().next();\n    RSGroupInfo tmpSrcGrp = rsGroupInfoManager.getRSGroupOfServer(firstServer);\n    if (tmpSrcGrp == null) {\n      // Be careful. This exception message is tested for in TestRSGroupsBase...\n      throw new ConstraintException(\"Source RSGroup for server \" + firstServer\n              + \" does not exist.\");\n    }\n    RSGroupInfo srcGrp = new RSGroupInfo(tmpSrcGrp);\n    if (srcGrp.getName().equals(targetGroupName)) {\n      throw new ConstraintException(\"Target RSGroup \" + targetGroupName +\n              \" is same as source \" + srcGrp.getName() + \" RSGroup.\");\n    }\n    // Only move online servers\n    checkOnlineServersOnly(servers);\n\n    // Ensure all servers are of same rsgroup.\n    for (Address server: servers) {\n      String tmpGroup = rsGroupInfoManager.getRSGroupOfServer(server).getName();\n      if (!tmpGroup.equals(srcGrp.getName())) {\n        throw new ConstraintException(\"Move server request should only come from one source \" +\n                \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n      }\n    }\n\n    // Ensure all tables and servers are of same rsgroup.\n    for (TableName table : tables) {\n      String tmpGroup = rsGroupInfoManager.getRSGroupOfTable(table);\n      if (!tmpGroup.equals(srcGrp.getName())) {\n        throw new ConstraintException(\"Move table request should only come from one source \" +\n                \"RSGroup. Expecting only \" + srcGrp.getName() + \" but contains \" + tmpGroup);\n      }\n    }\n\n    if (srcGrp.getServers().size() <= servers.size() && srcGrp.getTables().size() > tables.size()) {\n      throw new ConstraintException(\"Cannot leave a RSGroup \" + srcGrp.getName() +\n              \" that contains tables without servers to host them.\");\n    }\n  }"
        ],
        [
            "TestRSGroupsBase::testTableMoveTruncateAndDrop()",
            " 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 -\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "  @Test\n  public void testTableMoveTruncateAndDrop() throws Exception {\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    String newGroupName = getGroupName(name.getMethodName());\n    final RSGroupInfo newGroup = addGroup(newGroupName, 2);\n\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 5);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null)\n          return false;\n        return getTableRegionMap().get(tableName).size() >= 5;\n      }\n    });\n\n    RSGroupInfo tableGrp = rsGroupAdmin.getRSGroupInfoOfTable(tableName);\n    assertTrue(tableGrp.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n\n    //change table's group\n    LOG.info(\"Moving table \"+tableName+\" to \"+newGroup.getName());\n    rsGroupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n        rsGroupAdmin.getRSGroupInfoOfTable(tableName).getName());\n\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        Map<ServerName, List<String>> serverMap = getTableServerRegionMap().get(tableName);\n        int count = 0;\n        if (serverMap != null) {\n          for (ServerName rs : serverMap.keySet()) {\n            if (newGroup.containsServer(rs.getAddress())) {\n              count += serverMap.get(rs).size();\n            }\n          }\n        }\n        return count == 5;\n      }\n    });\n\n    //test truncate\n    admin.disableTable(tableName);\n    admin.truncateTable(tableName, true);\n    Assert.assertEquals(1, rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n    Assert.assertEquals(tableName, rsGroupAdmin.getRSGroupInfo(\n        newGroup.getName()).getTables().first());\n\n    //verify removed table is removed from group\n    TEST_UTIL.deleteTable(tableName);\n    Assert.assertEquals(0, rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n  }",
            " 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 +\n 347  \n 348 +\n 349 +\n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "  @Test\n  public void testTableMoveTruncateAndDrop() throws Exception {\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    String newGroupName = getGroupName(name.getMethodName());\n    final RSGroupInfo newGroup = addGroup(newGroupName, 2);\n\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 5);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null) {\n          return false;\n        }\n\n        return getTableRegionMap().get(tableName).size() >= 5;\n      }\n    });\n\n    RSGroupInfo tableGrp = rsGroupAdmin.getRSGroupInfoOfTable(tableName);\n    assertTrue(tableGrp.getName().equals(RSGroupInfo.DEFAULT_GROUP));\n\n    //change table's group\n    LOG.info(\"Moving table \"+tableName+\" to \"+newGroup.getName());\n    rsGroupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n        rsGroupAdmin.getRSGroupInfoOfTable(tableName).getName());\n\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        Map<ServerName, List<String>> serverMap = getTableServerRegionMap().get(tableName);\n        int count = 0;\n        if (serverMap != null) {\n          for (ServerName rs : serverMap.keySet()) {\n            if (newGroup.containsServer(rs.getAddress())) {\n              count += serverMap.get(rs).size();\n            }\n          }\n        }\n        return count == 5;\n      }\n    });\n\n    //test truncate\n    admin.disableTable(tableName);\n    admin.truncateTable(tableName, true);\n    Assert.assertEquals(1, rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n    Assert.assertEquals(tableName, rsGroupAdmin.getRSGroupInfo(\n        newGroup.getName()).getTables().first());\n\n    //verify removed table is removed from group\n    TEST_UTIL.deleteTable(tableName);\n    Assert.assertEquals(0, rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n  }"
        ],
        [
            "RSGroupAdminEndpoint::RSGroupAdminServiceImpl::getRSGroupInfoOfServer(RpcController,GetRSGroupInfoOfServerRequest,RpcCallback)",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266 -\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "    @Override\n    public void getRSGroupInfoOfServer(RpcController controller,\n        GetRSGroupInfoOfServerRequest request, RpcCallback<GetRSGroupInfoOfServerResponse> done) {\n      GetRSGroupInfoOfServerResponse.Builder builder = GetRSGroupInfoOfServerResponse.newBuilder();\n      try {\n        Address hp = Address.fromParts(request.getServer().getHostName(),\n            request.getServer().getPort());\n        LOG.info(master.getClientIdAuditPrefix() + \" initiates rsgroup info retrieval, server=\" + hp);\n        RSGroupInfo RSGroupInfo = groupAdminServer.getRSGroupOfServer(hp);\n        if (RSGroupInfo != null) {\n          builder.setRSGroupInfo(RSGroupProtobufUtil.toProtoGroupInfo(RSGroupInfo));\n        }\n      } catch (IOException e) {\n        CoprocessorRpcUtils.setControllerException(controller, e);\n      }\n      done.run(builder.build());\n    }",
            " 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269 +\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  ",
            "    @Override\n    public void getRSGroupInfoOfServer(RpcController controller,\n        GetRSGroupInfoOfServerRequest request, RpcCallback<GetRSGroupInfoOfServerResponse> done) {\n      GetRSGroupInfoOfServerResponse.Builder builder = GetRSGroupInfoOfServerResponse.newBuilder();\n      try {\n        Address hp = Address.fromParts(request.getServer().getHostName(),\n            request.getServer().getPort());\n        LOG.info(master.getClientIdAuditPrefix() + \" initiates rsgroup info retrieval, server=\" +\n                hp);\n        RSGroupInfo RSGroupInfo = groupAdminServer.getRSGroupOfServer(hp);\n        if (RSGroupInfo != null) {\n          builder.setRSGroupInfo(RSGroupProtobufUtil.toProtoGroupInfo(RSGroupInfo));\n        }\n      } catch (IOException e) {\n        CoprocessorRpcUtils.setControllerException(controller, e);\n      }\n      done.run(builder.build());\n    }"
        ],
        [
            "TestRSGroupsBase::testMoveServersAndTables()",
            " 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761 -\n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850 -\n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858 -\n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  ",
            "  @Test\n  public void testMoveServersAndTables() throws Exception {\n    LOG.info(\"testMoveServersAndTables\");\n    final RSGroupInfo newGroup = addGroup(getGroupName(name.getMethodName()), 1);\n    //create table\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 5);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null)\n          return false;\n        return getTableRegionMap().get(tableName).size() >= 5;\n      }\n    });\n\n    //get server which is not a member of new group\n    ServerName targetServer = null;\n    for(ServerName server : admin.getClusterStatus(EnumSet.of(Option.LIVE_SERVERS)).getServers()) {\n      if(!newGroup.containsServer(server.getAddress()) &&\n           !rsGroupAdmin.getRSGroupInfo(\"master\").containsServer(server.getAddress())) {\n        targetServer = server;\n        break;\n      }\n    }\n\n    LOG.debug(\"Print group info : \" + rsGroupAdmin.listRSGroups());\n    int oldDefaultGroupServerSize =\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getServers().size();\n    int oldDefaultGroupTableSize =\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables().size();\n\n    //test fail bogus server move\n    try {\n      rsGroupAdmin.moveServersAndTables(Sets.newHashSet(Address.fromString(\"foo:9999\")),\n              Sets.newHashSet(tableName), newGroup.getName());\n      fail(\"Bogus servers shouldn't have been successfully moved.\");\n    } catch(IOException ex) {\n      String exp = \"Source RSGroup for server foo:9999 does not exist.\";\n      String msg = \"Expected '\" + exp + \"' in exception message: \";\n      assertTrue(msg + \" \" + ex.getMessage(), ex.getMessage().contains(exp));\n    }\n\n    //test fail server move\n    try {\n      rsGroupAdmin.moveServersAndTables(Sets.newHashSet(targetServer.getAddress()),\n              Sets.newHashSet(tableName), RSGroupInfo.DEFAULT_GROUP);\n      fail(\"servers shouldn't have been successfully moved.\");\n    } catch(IOException ex) {\n      String exp = \"Target RSGroup \" + RSGroupInfo.DEFAULT_GROUP +\n              \" is same as source \" + RSGroupInfo.DEFAULT_GROUP + \" RSGroup.\";\n      String msg = \"Expected '\" + exp + \"' in exception message: \";\n      assertTrue(msg + \" \" + ex.getMessage(), ex.getMessage().contains(exp));\n    }\n\n    //verify default group info\n    Assert.assertEquals(oldDefaultGroupServerSize,\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getServers().size());\n    Assert.assertEquals(oldDefaultGroupTableSize,\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables().size());\n\n    //verify new group info\n    Assert.assertEquals(1,\n            rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getServers().size());\n    Assert.assertEquals(0,\n            rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n\n    //get all region to move targetServer\n    List<String> regionList = getTableRegionMap().get(tableName);\n    for(String region : regionList) {\n      // Lets move this region to the targetServer\n      TEST_UTIL.getAdmin().move(Bytes.toBytes(RegionInfo.encodeRegionName(Bytes.toBytes(region))),\n              Bytes.toBytes(targetServer.getServerName()));\n    }\n\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return getTableRegionMap().get(tableName) != null &&\n                getTableRegionMap().get(tableName).size() == 5 &&\n                getTableServerRegionMap().get(tableName).size() == 1 &&\n                admin.getClusterStatus(EnumSet.of(Option.REGIONS_IN_TRANSITION))\n                     .getRegionStatesInTransition().size() < 1;\n      }\n    });\n\n    //verify that all region move to targetServer\n    Assert.assertEquals(5, getTableServerRegionMap().get(tableName).get(targetServer).size());\n\n    //move targetServer and table to newGroup\n    LOG.info(\"moving server and table to newGroup\");\n    rsGroupAdmin.moveServersAndTables(Sets.newHashSet(targetServer.getAddress()),\n            Sets.newHashSet(tableName), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableName).getName());\n\n    //verify servers' not exist in old group\n    Set<Address> defaultServers = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getServers();\n    assertFalse(defaultServers.contains(targetServer.getAddress()));\n\n    //verify servers' exist in new group\n    Set<Address> newGroupServers = rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getServers();\n    assertTrue(newGroupServers.contains(targetServer.getAddress()));\n\n    //verify tables' not exist in old group\n    Set<TableName> defaultTables = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables();\n    assertFalse(defaultTables.contains(tableName));\n\n    //verify tables' exist in new group\n    Set<TableName> newGroupTables = rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables();\n    assertTrue(newGroupTables.contains(tableName));\n\n    //verify that all region still assgin on targetServer\n    Assert.assertEquals(5, getTableServerRegionMap().get(tableName).get(targetServer).size());\n  }",
            " 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 +\n 770  \n 771 +\n 772 +\n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860 +\n 861 +\n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869 +\n 870 +\n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  ",
            "  @Test\n  public void testMoveServersAndTables() throws Exception {\n    LOG.info(\"testMoveServersAndTables\");\n    final RSGroupInfo newGroup = addGroup(getGroupName(name.getMethodName()), 1);\n    //create table\n    final byte[] familyNameBytes = Bytes.toBytes(\"f\");\n    TEST_UTIL.createMultiRegionTable(tableName, familyNameBytes, 5);\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        List<String> regions = getTableRegionMap().get(tableName);\n        if (regions == null) {\n          return false;\n        }\n\n        return getTableRegionMap().get(tableName).size() >= 5;\n      }\n    });\n\n    //get server which is not a member of new group\n    ServerName targetServer = null;\n    for(ServerName server : admin.getClusterStatus(EnumSet.of(Option.LIVE_SERVERS)).getServers()) {\n      if(!newGroup.containsServer(server.getAddress()) &&\n           !rsGroupAdmin.getRSGroupInfo(\"master\").containsServer(server.getAddress())) {\n        targetServer = server;\n        break;\n      }\n    }\n\n    LOG.debug(\"Print group info : \" + rsGroupAdmin.listRSGroups());\n    int oldDefaultGroupServerSize =\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getServers().size();\n    int oldDefaultGroupTableSize =\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables().size();\n\n    //test fail bogus server move\n    try {\n      rsGroupAdmin.moveServersAndTables(Sets.newHashSet(Address.fromString(\"foo:9999\")),\n              Sets.newHashSet(tableName), newGroup.getName());\n      fail(\"Bogus servers shouldn't have been successfully moved.\");\n    } catch(IOException ex) {\n      String exp = \"Source RSGroup for server foo:9999 does not exist.\";\n      String msg = \"Expected '\" + exp + \"' in exception message: \";\n      assertTrue(msg + \" \" + ex.getMessage(), ex.getMessage().contains(exp));\n    }\n\n    //test fail server move\n    try {\n      rsGroupAdmin.moveServersAndTables(Sets.newHashSet(targetServer.getAddress()),\n              Sets.newHashSet(tableName), RSGroupInfo.DEFAULT_GROUP);\n      fail(\"servers shouldn't have been successfully moved.\");\n    } catch(IOException ex) {\n      String exp = \"Target RSGroup \" + RSGroupInfo.DEFAULT_GROUP +\n              \" is same as source \" + RSGroupInfo.DEFAULT_GROUP + \" RSGroup.\";\n      String msg = \"Expected '\" + exp + \"' in exception message: \";\n      assertTrue(msg + \" \" + ex.getMessage(), ex.getMessage().contains(exp));\n    }\n\n    //verify default group info\n    Assert.assertEquals(oldDefaultGroupServerSize,\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getServers().size());\n    Assert.assertEquals(oldDefaultGroupTableSize,\n            rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP).getTables().size());\n\n    //verify new group info\n    Assert.assertEquals(1,\n            rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getServers().size());\n    Assert.assertEquals(0,\n            rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables().size());\n\n    //get all region to move targetServer\n    List<String> regionList = getTableRegionMap().get(tableName);\n    for(String region : regionList) {\n      // Lets move this region to the targetServer\n      TEST_UTIL.getAdmin().move(Bytes.toBytes(RegionInfo.encodeRegionName(Bytes.toBytes(region))),\n              Bytes.toBytes(targetServer.getServerName()));\n    }\n\n    TEST_UTIL.waitFor(WAIT_TIMEOUT, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return getTableRegionMap().get(tableName) != null &&\n                getTableRegionMap().get(tableName).size() == 5 &&\n                getTableServerRegionMap().get(tableName).size() == 1 &&\n                admin.getClusterStatus(EnumSet.of(Option.REGIONS_IN_TRANSITION))\n                     .getRegionStatesInTransition().size() < 1;\n      }\n    });\n\n    //verify that all region move to targetServer\n    Assert.assertEquals(5, getTableServerRegionMap().get(tableName).get(targetServer).size());\n\n    //move targetServer and table to newGroup\n    LOG.info(\"moving server and table to newGroup\");\n    rsGroupAdmin.moveServersAndTables(Sets.newHashSet(targetServer.getAddress()),\n            Sets.newHashSet(tableName), newGroup.getName());\n\n    //verify group change\n    Assert.assertEquals(newGroup.getName(),\n            rsGroupAdmin.getRSGroupInfoOfTable(tableName).getName());\n\n    //verify servers' not exist in old group\n    Set<Address> defaultServers = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP)\n            .getServers();\n    assertFalse(defaultServers.contains(targetServer.getAddress()));\n\n    //verify servers' exist in new group\n    Set<Address> newGroupServers = rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getServers();\n    assertTrue(newGroupServers.contains(targetServer.getAddress()));\n\n    //verify tables' not exist in old group\n    Set<TableName> defaultTables = rsGroupAdmin.getRSGroupInfo(RSGroupInfo.DEFAULT_GROUP)\n            .getTables();\n    assertFalse(defaultTables.contains(tableName));\n\n    //verify tables' exist in new group\n    Set<TableName> newGroupTables = rsGroupAdmin.getRSGroupInfo(newGroup.getName()).getTables();\n    assertTrue(newGroupTables.contains(tableName));\n\n    //verify that all region still assgin on targetServer\n    Assert.assertEquals(5, getTableServerRegionMap().get(tableName).get(targetServer).size());\n  }"
        ],
        [
            "VerifyingRSGroupAdminClient::VerifyingRSGroupAdminClient(RSGroupAdmin,Configuration)",
            "  51  \n  52  \n  53  \n  54 -\n  55  \n  56  ",
            "  public VerifyingRSGroupAdminClient(RSGroupAdmin RSGroupAdmin, Configuration conf)\n      throws IOException {\n    wrapped = RSGroupAdmin;\n    table = ConnectionFactory.createConnection(conf).getTable(RSGroupInfoManager.RSGROUP_TABLE_NAME);\n    zkw = new ZKWatcher(conf, this.getClass().getSimpleName(), null);\n  }",
            "  52  \n  53  \n  54  \n  55 +\n  56 +\n  57  \n  58  ",
            "  public VerifyingRSGroupAdminClient(RSGroupAdmin RSGroupAdmin, Configuration conf)\n      throws IOException {\n    wrapped = RSGroupAdmin;\n    table = ConnectionFactory.createConnection(conf)\n            .getTable(RSGroupInfoManager.RSGROUP_TABLE_NAME);\n    zkw = new ZKWatcher(conf, this.getClass().getSimpleName(), null);\n  }"
        ],
        [
            "Utility::getOnlineServers(MasterServices)",
            "  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41 -\n  42  \n  43  \n  44  \n  45  \n  46  ",
            "  /**\n   * @param master\n   * @return Set of online Servers named for their hostname and port (not ServerName).\n   */\n  static Set<Address> getOnlineServers(final MasterServices master) {\n    Set<Address> onlineServers = new HashSet<Address>();\n    if (master == null) return onlineServers;\n    for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {\n      onlineServers.add(server.getAddress());\n    }\n    return onlineServers;\n  }",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44 +\n  45 +\n  46 +\n  47 +\n  48  \n  49  \n  50  \n  51  \n  52  ",
            "  /**\n   * @param master the master to get online servers for\n   * @return Set of online Servers named for their hostname and port (not ServerName).\n   */\n  static Set<Address> getOnlineServers(final MasterServices master) {\n    Set<Address> onlineServers = new HashSet<Address>();\n    if (master == null) {\n      return onlineServers;\n    }\n\n    for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {\n      onlineServers.add(server.getAddress());\n    }\n    return onlineServers;\n  }"
        ],
        [
            "RSGroupAdminServer::addRegion(LinkedList,RegionInfo)",
            " 132  \n 133  \n 134  \n 135  \n 136 -\n 137 -\n 138  ",
            "  private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {\n    // If meta, move it last otherwise other unassigns fail because meta is not\n    // online for them to update state in. This is dodgy. Needs to be made more\n    // robust. See TODO below.\n    if (hri.isMetaRegion()) regions.addLast(hri);\n    else regions.addFirst(hri);\n  }",
            " 136  \n 137  \n 138  \n 139  \n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145  ",
            "  private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {\n    // If meta, move it last otherwise other unassigns fail because meta is not\n    // online for them to update state in. This is dodgy. Needs to be made more\n    // robust. See TODO below.\n    if (hri.isMetaRegion()) {\n      regions.addLast(hri);\n    } else {\n      regions.addFirst(hri);\n    }\n  }"
        ],
        [
            "RSGroupAdminServer::moveServersAndTables(Set,Set,String)",
            " 547  \n 548  \n 549  \n 550 -\n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  ",
            "  @Override\n  public void moveServersAndTables(Set<Address> servers, Set<TableName> tables, String targetGroup)\n      throws IOException {\n    if (servers == null || servers.isEmpty() ) {\n      throw new ConstraintException(\"The list of servers to move cannot be null or empty.\");\n    }\n    if (tables == null || tables.isEmpty()) {\n      throw new ConstraintException(\"The list of tables to move cannot be null or empty.\");\n    }\n\n    //check target group\n    getAndCheckRSGroupInfo(targetGroup);\n\n    // Hold a lock on the manager instance while moving servers and tables to prevent\n    // another writer changing our state while we are working.\n    synchronized (rsGroupInfoManager) {\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preMoveServersAndTables(servers, tables, targetGroup);\n      }\n      //check servers and tables status\n      checkServersAndTables(servers, tables, targetGroup);\n\n      //Move servers and tables to a new group.\n      String srcGroup = getRSGroupOfServer(servers.iterator().next()).getName();\n      rsGroupInfoManager.moveServersAndTables(servers, tables, srcGroup, targetGroup);\n\n      //move regions which should not belong to these tables\n      moveRegionsFromServers(servers, tables, targetGroup);\n      //move regions which should belong to these servers\n      moveRegionsToServers(servers, tables, targetGroup);\n\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postMoveServersAndTables(servers, tables, targetGroup);\n      }\n    }\n    LOG.info(\"Move servers and tables done. Severs :\"\n            + servers + \" , Tables : \" + tables + \" => \" +  targetGroup);\n  }",
            " 557  \n 558  \n 559  \n 560 +\n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  ",
            "  @Override\n  public void moveServersAndTables(Set<Address> servers, Set<TableName> tables, String targetGroup)\n      throws IOException {\n    if (servers == null || servers.isEmpty()) {\n      throw new ConstraintException(\"The list of servers to move cannot be null or empty.\");\n    }\n    if (tables == null || tables.isEmpty()) {\n      throw new ConstraintException(\"The list of tables to move cannot be null or empty.\");\n    }\n\n    //check target group\n    getAndCheckRSGroupInfo(targetGroup);\n\n    // Hold a lock on the manager instance while moving servers and tables to prevent\n    // another writer changing our state while we are working.\n    synchronized (rsGroupInfoManager) {\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().preMoveServersAndTables(servers, tables, targetGroup);\n      }\n      //check servers and tables status\n      checkServersAndTables(servers, tables, targetGroup);\n\n      //Move servers and tables to a new group.\n      String srcGroup = getRSGroupOfServer(servers.iterator().next()).getName();\n      rsGroupInfoManager.moveServersAndTables(servers, tables, srcGroup, targetGroup);\n\n      //move regions which should not belong to these tables\n      moveRegionsFromServers(servers, tables, targetGroup);\n      //move regions which should belong to these servers\n      moveRegionsToServers(servers, tables, targetGroup);\n\n      if (master.getMasterCoprocessorHost() != null) {\n        master.getMasterCoprocessorHost().postMoveServersAndTables(servers, tables, targetGroup);\n      }\n    }\n    LOG.info(\"Move servers and tables done. Severs :\"\n            + servers + \" , Tables : \" + tables + \" => \" +  targetGroup);\n  }"
        ]
    ],
    "72c3d27bf6465dd84b99759476acb12eced33196": [
        [
            "ServerCrashProcedure::handleRIT(MasterProcedureEnv,List)",
            " 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366 -\n 367 -\n 368  \n 369 -\n 370  \n 371  \n 372  \n 373  \n 374 -\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392 -\n 393 -\n 394 -\n 395 -\n 396  \n 397  ",
            "  /**\n   * Handle any outstanding RIT that are up against this.serverName, the crashed server.\n   * Notify them of crash. Remove assign entries from the passed in <code>regions</code>\n   * otherwise we have two assigns going on and they will fight over who has lock.\n   * Notify Unassigns. If unable to unassign because server went away, unassigns block waiting\n   * on the below callback from a ServerCrashProcedure before proceeding.\n   * @param env\n   * @param regions Regions that were on crashed server\n   */\n  private void handleRIT(final MasterProcedureEnv env, final List<RegionInfo> regions) {\n    if (regions == null) return;\n    AssignmentManager am = env.getMasterServices().getAssignmentManager();\n    final Iterator<RegionInfo> it = regions.iterator();\n    ServerCrashException sce = null;\n    while (it.hasNext()) {\n      final RegionInfo hri = it.next();\n      RegionTransitionProcedure rtp = am.getRegionStates().getRegionTransitionProcedure(hri);\n      if (rtp == null) continue;\n      // Make sure the RIT is against this crashed server. In the case where there are many\n      // processings of a crashed server -- backed up for whatever reason (slow WAL split) --\n      // then a previous SCP may have already failed an assign, etc., and it may have a new\n      // location target; DO NOT fail these else we make for assign flux.\n      ServerName rtpServerName = rtp.getServer(env);\n      if (rtpServerName == null) {\n        LOG.warn(\"RIT with ServerName null! \" + rtp);\n        continue;\n      }\n      if (!rtpServerName.equals(this.serverName)) continue;\n      LOG.info(\"pid=\" + getProcId() + \" found RIT \" + rtp + \"; \" +\n        rtp.getRegionState(env).toShortString());\n      // Notify RIT on server crash.\n      if (sce == null) {\n        sce = new ServerCrashException(getProcId(), getServerName());\n      }\n      rtp.remoteCallFailed(env, this.serverName, sce);\n      if (rtp instanceof AssignProcedure) {\n        // If an assign, include it in our return and remove from passed-in list of regions.\n        it.remove();\n      }\n    }\n  }",
            " 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 +\n 378 +\n 379 +\n 380 +\n 381  \n 382 +\n 383 +\n 384 +\n 385  \n 386  \n 387  \n 388  \n 389 +\n 390 +\n 391 +\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409 +\n 410 +\n 411 +\n 412 +\n 413 +\n 414 +\n 415  \n 416 +\n 417  ",
            "  /**\n   * Handle any outstanding RIT that are up against this.serverName, the crashed server.\n   * Notify them of crash. Remove assign entries from the passed in <code>regions</code>\n   * otherwise we have two assigns going on and they will fight over who has lock.\n   * Notify Unassigns. If unable to unassign because server went away, unassigns block waiting\n   * on the below callback from a ServerCrashProcedure before proceeding.\n   * @param regions Regions on the Crashed Server.\n   * @return List of regions we should assign to new homes (not same as regions on crashed server).\n   */\n  private List<RegionInfo> handleRIT(final MasterProcedureEnv env, List<RegionInfo> regions) {\n    if (regions == null || regions.isEmpty()) {\n      return Collections.emptyList();\n    }\n    AssignmentManager am = env.getMasterServices().getAssignmentManager();\n    List<RegionInfo> toAssign = new ArrayList<RegionInfo>(regions);\n    // Get an iterator so can remove items.\n    final Iterator<RegionInfo> it = toAssign.iterator();\n    ServerCrashException sce = null;\n    while (it.hasNext()) {\n      final RegionInfo hri = it.next();\n      RegionTransitionProcedure rtp = am.getRegionStates().getRegionTransitionProcedure(hri);\n      if (rtp == null) {\n        continue;\n      }\n      // Make sure the RIT is against this crashed server. In the case where there are many\n      // processings of a crashed server -- backed up for whatever reason (slow WAL split) --\n      // then a previous SCP may have already failed an assign, etc., and it may have a new\n      // location target; DO NOT fail these else we make for assign flux.\n      ServerName rtpServerName = rtp.getServer(env);\n      if (rtpServerName == null) {\n        LOG.warn(\"RIT with ServerName null! \" + rtp);\n        continue;\n      }\n      if (!rtpServerName.equals(this.serverName)) continue;\n      LOG.info(\"pid=\" + getProcId() + \" found RIT \" + rtp + \"; \" +\n        rtp.getRegionState(env).toShortString());\n      // Notify RIT on server crash.\n      if (sce == null) {\n        sce = new ServerCrashException(getProcId(), getServerName());\n      }\n      rtp.remoteCallFailed(env, this.serverName, sce);\n      // If an assign, remove from passed-in list of regions so we subsequently do not create\n      // a new assign; the exisitng assign after the call to remoteCallFailed will recalibrate\n      // and assign to a server other than the crashed one; no need to create new assign.\n      // If an unassign, do not return this region; the above cancel will wake up the unassign and\n      // it will complete. Done.\n      it.remove();\n    }\n    return toAssign;\n  }"
        ],
        [
            "AssignmentManager::handleRegionOverStuckWarningThreshold(RegionInfo)",
            "1196  \n1197  \n1198  \n1199 -\n1200  ",
            "  private void handleRegionOverStuckWarningThreshold(final RegionInfo regionInfo) {\n    final RegionStateNode regionNode = regionStates.getRegionStateNode(regionInfo);\n    //if (regionNode.isStuck()) {\n    LOG.warn(\"TODO Handle stuck in transition: \" + regionNode);\n  }",
            "1196  \n1197  \n1198  \n1199 +\n1200  ",
            "  private void handleRegionOverStuckWarningThreshold(final RegionInfo regionInfo) {\n    final RegionStateNode regionNode = regionStates.getRegionStateNode(regionInfo);\n    //if (regionNode.isStuck()) {\n    LOG.warn(\"STUCK Region-In-Transition {}\", regionNode);\n  }"
        ],
        [
            "UnassignProcedure::remoteCallFailed(MasterProcedureEnv,RegionStateNode,IOException)",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252 -\n 253  \n 254 -\n 255 -\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266 -\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279 -\n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  ",
            "  @Override\n  protected boolean remoteCallFailed(final MasterProcedureEnv env, final RegionStateNode regionNode,\n      final IOException exception) {\n    // TODO: Is there on-going rpc to cleanup?\n    if (exception instanceof ServerCrashException) {\n      // This exception comes from ServerCrashProcedure after log splitting.\n      // SCP found this region as a RIT. Its call into here says it is ok to let this procedure go\n      // on to a complete close now. This will release lock on this region so subsequent action on\n      // region can succeed; e.g. the assign that follows this unassign when a move (w/o wait on SCP\n      // the assign could run w/o logs being split so data loss).\n      try {\n        reportTransition(env, regionNode, TransitionCode.CLOSED, HConstants.NO_SEQNUM);\n      } catch (UnexpectedStateException e) {\n        // Should never happen.\n        throw new RuntimeException(e);\n      }\n    } else if (exception instanceof RegionServerAbortedException ||\n        exception instanceof RegionServerStoppedException ||\n        exception instanceof ServerNotRunningYetException) {\n      // TODO\n      // RS is aborting, we cannot offline the region since the region may need to do WAL\n      // recovery. Until we see the RS expiration, we should retry.\n      // TODO: This should be suspend like the below where we call expire on server?\n      LOG.info(\"Ignoring; waiting on ServerCrashProcedure\", exception);\n    } else if (exception instanceof NotServingRegionException) {\n      LOG.info(\"IS THIS OK? ANY LOGS TO REPLAY; ACTING AS THOUGH ALL GOOD \" + regionNode,\n        exception);\n      setTransitionState(RegionTransitionState.REGION_TRANSITION_FINISH);\n    } else {\n      LOG.warn(\"Expiring server \" + this + \"; \" + regionNode.toShortString() +\n        \", exception=\" + exception);\n      env.getMasterServices().getServerManager().expireServer(regionNode.getRegionLocation());\n      // Return false so this procedure stays in suspended state. It will be woken up by a\n      // ServerCrashProcedure when it notices this RIT.\n      // TODO: Add a SCP as a new subprocedure that we now come to depend on.\n      return false;\n    }\n    return true;\n  }",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252 +\n 253  \n 254 +\n 255 +\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 +\n 279 +\n 280 +\n 281 +\n 282  \n 283  \n 284  \n 285  \n 286  ",
            "  @Override\n  protected boolean remoteCallFailed(final MasterProcedureEnv env, final RegionStateNode regionNode,\n      final IOException exception) {\n    // TODO: Is there on-going rpc to cleanup?\n    if (exception instanceof ServerCrashException) {\n      // This exception comes from ServerCrashProcedure AFTER log splitting.\n      // SCP found this region as a RIT. Its call into here says it is ok to let this procedure go\n      // complete. This complete will release lock on this region so subsequent action on region\n      // can succeed; e.g. the assign that follows this unassign when a move (w/o wait on SCP\n      // the assign could run w/o logs being split so data loss).\n      try {\n        reportTransition(env, regionNode, TransitionCode.CLOSED, HConstants.NO_SEQNUM);\n      } catch (UnexpectedStateException e) {\n        // Should never happen.\n        throw new RuntimeException(e);\n      }\n    } else if (exception instanceof RegionServerAbortedException ||\n        exception instanceof RegionServerStoppedException ||\n        exception instanceof ServerNotRunningYetException) {\n      // RS is aborting, we cannot offline the region since the region may need to do WAL\n      // recovery. Until we see the RS expiration, we should retry.\n      // TODO: This should be suspend like the below where we call expire on server?\n      LOG.info(\"Ignoring; waiting on ServerCrashProcedure\", exception);\n    } else if (exception instanceof NotServingRegionException) {\n      LOG.info(\"IS THIS OK? ANY LOGS TO REPLAY; ACTING AS THOUGH ALL GOOD \" + regionNode,\n        exception);\n      setTransitionState(RegionTransitionState.REGION_TRANSITION_FINISH);\n    } else {\n      LOG.warn(\"Expiring server \" + this + \"; \" + regionNode.toShortString() +\n        \", exception=\" + exception);\n      env.getMasterServices().getServerManager().expireServer(regionNode.getRegionLocation());\n      // Return false so this procedure stays in suspended state. It will be woken up by the\n      // ServerCrashProcedure that was scheduled when we called #expireServer above. SCP calls\n      // #handleRIT which will call this method only the exception will be a ServerCrashException\n      // this time around (See above).\n      // TODO: Add a SCP as a new subprocedure that we now come to depend on.\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "ServerCrashProcedure::executeFromState(MasterProcedureEnv,ServerCrashState)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118 -\n 119 -\n 120 -\n 121 -\n 122 -\n 123  \n 124 -\n 125 -\n 126 -\n 127 -\n 128 -\n 129 -\n 130 -\n 131 -\n 132 -\n 133 -\n 134 -\n 135 -\n 136 -\n 137 -\n 138  \n 139 -\n 140 -\n 141 -\n 142 -\n 143 -\n 144 -\n 145 -\n 146 -\n 147 -\n 148 -\n 149 -\n 150 -\n 151 -\n 152 -\n 153 -\n 154 -\n 155 -\n 156 -\n 157 -\n 158 -\n 159 -\n 160 -\n 161 -\n 162 -\n 163  \n 164  \n 165 -\n 166 -\n 167 -\n 168 -\n 169 -\n 170 -\n 171 -\n 172 -\n 173 -\n 174 -\n 175 -\n 176 -\n 177 -\n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "  @Override\n  protected Flow executeFromState(MasterProcedureEnv env, ServerCrashState state)\n      throws ProcedureSuspendedException, ProcedureYieldException {\n    final MasterServices services = env.getMasterServices();\n    // HBASE-14802\n    // If we have not yet notified that we are processing a dead server, we should do now.\n    if (!notifiedDeadServer) {\n      services.getServerManager().getDeadServers().notifyServer(serverName);\n      notifiedDeadServer = true;\n    }\n\n    try {\n      switch (state) {\n      case SERVER_CRASH_START:\n        LOG.info(\"Start \" + this);\n        // If carrying meta, process it first. Else, get list of regions on crashed server.\n        if (this.carryingMeta) {\n          setNextState(ServerCrashState.SERVER_CRASH_PROCESS_META);\n        } else {\n          setNextState(ServerCrashState.SERVER_CRASH_GET_REGIONS);\n        }\n        break;\n\n      case SERVER_CRASH_GET_REGIONS:\n        // If hbase:meta is not assigned, yield.\n        if (env.getAssignmentManager().waitMetaLoaded(this)) {\n          throw new ProcedureSuspendedException();\n        }\n\n        this.regionsOnCrashedServer = services.getAssignmentManager().getRegionStates()\n          .getServerRegionInfoSet(serverName);\n        // Where to go next? Depends on whether we should split logs at all or\n        // if we should do distributed log splitting.\n        if (!this.shouldSplitWal) {\n          setNextState(ServerCrashState.SERVER_CRASH_ASSIGN);\n        } else {\n          setNextState(ServerCrashState.SERVER_CRASH_SPLIT_LOGS);\n        }\n        break;\n\n      case SERVER_CRASH_PROCESS_META:\n        processMeta(env);\n        setNextState(ServerCrashState.SERVER_CRASH_GET_REGIONS);\n        break;\n\n      case SERVER_CRASH_SPLIT_LOGS:\n        splitLogs(env);\n        setNextState(ServerCrashState.SERVER_CRASH_ASSIGN);\n        break;\n\n      case SERVER_CRASH_ASSIGN:\n        // If no regions to assign, skip assign and skip to the finish.\n        // Filter out meta regions. Those are handled elsewhere in this procedure.\n        // Filter changes this.regionsOnCrashedServer.\n        if (filterDefaultMetaRegions(regionsOnCrashedServer)) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Assigning regions \" +\n              RegionInfo.getShortNameToLog(regionsOnCrashedServer) + \", \" + this +\n              \"; cycles=\" + getCycles());\n          }\n          handleRIT(env, regionsOnCrashedServer);\n          AssignmentManager am = env.getAssignmentManager();\n          // createAssignProcedure will try to use the old location for the region deploy.\n          addChildProcedure(am.createAssignProcedures(regionsOnCrashedServer));\n        }\n        setNextState(ServerCrashState.SERVER_CRASH_FINISH);\n        break;\n\n      case SERVER_CRASH_FINISH:\n        services.getAssignmentManager().getRegionStates().removeServer(serverName);\n        services.getServerManager().getDeadServers().finish(serverName);\n        return Flow.NO_MORE_STATE;\n\n      default:\n        throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Failed state=\" + state + \", retry \" + this + \"; cycles=\" + getCycles(), e);\n    }\n    return Flow.HAS_MORE_STATE;\n  }",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146  \n 147 +\n 148 +\n 149 +\n 150 +\n 151  \n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158 +\n 159 +\n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173  \n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179  \n 180 +\n 181 +\n 182 +\n 183 +\n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "  @Override\n  protected Flow executeFromState(MasterProcedureEnv env, ServerCrashState state)\n      throws ProcedureSuspendedException, ProcedureYieldException {\n    final MasterServices services = env.getMasterServices();\n    // HBASE-14802\n    // If we have not yet notified that we are processing a dead server, we should do now.\n    if (!notifiedDeadServer) {\n      services.getServerManager().getDeadServers().notifyServer(serverName);\n      notifiedDeadServer = true;\n    }\n\n    try {\n      switch (state) {\n        case SERVER_CRASH_START:\n          LOG.info(\"Start \" + this);\n          // If carrying meta, process it first. Else, get list of regions on crashed server.\n          if (this.carryingMeta) {\n            setNextState(ServerCrashState.SERVER_CRASH_PROCESS_META);\n          } else {\n            setNextState(ServerCrashState.SERVER_CRASH_GET_REGIONS);\n          }\n          break;\n\n        case SERVER_CRASH_GET_REGIONS:\n          // If hbase:meta is not assigned, yield.\n          if (env.getAssignmentManager().waitMetaLoaded(this)) {\n            throw new ProcedureSuspendedException();\n          }\n\n          this.regionsOnCrashedServer = services.getAssignmentManager().getRegionStates()\n            .getServerRegionInfoSet(serverName);\n          // Where to go next? Depends on whether we should split logs at all or\n          // if we should do distributed log splitting.\n          if (!this.shouldSplitWal) {\n            setNextState(ServerCrashState.SERVER_CRASH_ASSIGN);\n          } else {\n            setNextState(ServerCrashState.SERVER_CRASH_SPLIT_LOGS);\n          }\n          break;\n\n        case SERVER_CRASH_PROCESS_META:\n          processMeta(env);\n          setNextState(ServerCrashState.SERVER_CRASH_GET_REGIONS);\n          break;\n\n        case SERVER_CRASH_SPLIT_LOGS:\n          splitLogs(env);\n          setNextState(ServerCrashState.SERVER_CRASH_ASSIGN);\n          break;\n\n        case SERVER_CRASH_ASSIGN:\n          // If no regions to assign, skip assign and skip to the finish.\n          // Filter out meta regions. Those are handled elsewhere in this procedure.\n          // Filter changes this.regionsOnCrashedServer.\n          if (filterDefaultMetaRegions(regionsOnCrashedServer)) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Assigning regions \" +\n                RegionInfo.getShortNameToLog(regionsOnCrashedServer) + \", \" + this +\n                \"; cycles=\" + getCycles());\n            }\n            // Handle RIT against crashed server. Will cancel any ongoing assigns/unassigns.\n            // Returns list of regions we need to reassign.\n            List<RegionInfo> toAssign = handleRIT(env, regionsOnCrashedServer);\n            AssignmentManager am = env.getAssignmentManager();\n            // CreateAssignProcedure will try to use the old location for the region deploy.\n            addChildProcedure(am.createAssignProcedures(toAssign));\n            setNextState(ServerCrashState.SERVER_CRASH_HANDLE_RIT2);\n          } else {\n            setNextState(ServerCrashState.SERVER_CRASH_FINISH);\n          }\n          break;\n\n        case SERVER_CRASH_HANDLE_RIT2:\n          // Run the handleRIT again for case where another procedure managed to grab the lock on\n          // a region ahead of this crash handling procedure. Can happen in rare case. See\n          handleRIT(env, regionsOnCrashedServer);\n          setNextState(ServerCrashState.SERVER_CRASH_FINISH);\n          break;\n\n        case SERVER_CRASH_FINISH:\n          services.getAssignmentManager().getRegionStates().removeServer(serverName);\n          services.getServerManager().getDeadServers().finish(serverName);\n          return Flow.NO_MORE_STATE;\n\n        default:\n          throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Failed state=\" + state + \", retry \" + this + \"; cycles=\" + getCycles(), e);\n    }\n    return Flow.HAS_MORE_STATE;\n  }"
        ]
    ],
    "12c45cb2e8eb6b8dc1743eea6253ddd21b08a61b": [
        [
            "TestReplicationDroppedTables::testEditsBehindDroppedTable(boolean,String)",
            " 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 -\n 168 -\n 169  \n 170  \n 171  \n 172 -\n 173 -\n 174 -\n 175 -\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  ",
            "  private void testEditsBehindDroppedTable(boolean allowProceeding, String tName) throws Exception {\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, allowProceeding);\n    conf1.setInt(HConstants.REPLICATION_SOURCE_MAXTHREADS_KEY, 1);\n\n    // make sure we have a single region server only, so that all\n    // edits for all tables go there\n    utility1.shutdownMiniHBaseCluster();\n    utility1.startMiniHBaseCluster(1, 1);\n\n    TableName tablename = TableName.valueOf(tName);\n    byte[] familyname = Bytes.toBytes(\"fam\");\n    byte[] row = Bytes.toBytes(\"row\");\n\n    HTableDescriptor table = new HTableDescriptor(tablename);\n    HColumnDescriptor fam = new HColumnDescriptor(familyname);\n    fam.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);\n    table.addFamily(fam);\n\n    Connection connection1 = ConnectionFactory.createConnection(conf1);\n    Connection connection2 = ConnectionFactory.createConnection(conf2);\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.createTable(table);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.createTable(table);\n    }\n    utility1.waitUntilAllRegionsAssigned(tablename);\n    utility2.waitUntilAllRegionsAssigned(tablename);\n\n    Table lHtable1 = utility1.getConnection().getTable(tablename);\n\n    // now suspend replication\n    admin.disablePeer(\"2\");\n\n    // put some data (lead with 0 so the edit gets sorted before the other table's edits\n    //   in the replication batch)\n    // write a bunch of edits, making sure we fill a batch\n    byte[] rowkey = Bytes.toBytes(0+\" put on table to be dropped\");\n    Put put = new Put(rowkey);\n    put.addColumn(familyname, row, row);\n    lHtable1.put(put);\n\n    rowkey = Bytes.toBytes(\"normal put\");\n    put = new Put(rowkey);\n    put.addColumn(famName, row, row);\n    htable1.put(put);\n\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.disableTable(tablename);\n      admin1.deleteTable(tablename);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.disableTable(tablename);\n      admin2.deleteTable(tablename);\n    }\n\n    admin.enablePeer(\"2\");\n    if (allowProceeding) {\n      // in this we'd expect the key to make it over\n      verifyReplicationProceeded(rowkey);\n    } else {\n      verifyReplicationStuck(rowkey);\n    }\n    // just to be safe\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n  }",
            " 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 +\n 175 +\n 176  \n 177  \n 178  \n 179 +\n 180 +\n 181 +\n 182 +\n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198  \n 199 +\n 200  \n 201  \n 202  \n 203  ",
            "  private void testEditsBehindDroppedTable(boolean allowProceeding, String tName) throws Exception {\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, allowProceeding);\n    conf1.setInt(HConstants.REPLICATION_SOURCE_MAXTHREADS_KEY, 1);\n\n    // make sure we have a single region server only, so that all\n    // edits for all tables go there\n    utility1.shutdownMiniHBaseCluster();\n    utility1.startMiniHBaseCluster(1, 1);\n\n    TableName tablename = TableName.valueOf(tName);\n    byte[] familyname = Bytes.toBytes(\"fam\");\n    byte[] row = Bytes.toBytes(\"row\");\n\n    HTableDescriptor table = new HTableDescriptor(tablename);\n    HColumnDescriptor fam = new HColumnDescriptor(familyname);\n    fam.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);\n    table.addFamily(fam);\n\n    Connection connection1 = ConnectionFactory.createConnection(conf1);\n    Connection connection2 = ConnectionFactory.createConnection(conf2);\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.createTable(table);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.createTable(table);\n    }\n    utility1.waitUntilAllRegionsAssigned(tablename);\n    utility2.waitUntilAllRegionsAssigned(tablename);\n\n    Table lHtable1 = utility1.getConnection().getTable(tablename);\n\n    // now suspend replication\n    admin.disablePeer(\"2\");\n\n    // put some data (lead with 0 so the edit gets sorted before the other table's edits\n    //   in the replication batch)\n    // write a bunch of edits, making sure we fill a batch\n    byte[] rowKey = Bytes.toBytes(0 + \" put on table to be dropped\");\n    Put put = new Put(rowKey);\n    put.addColumn(familyname, row, row);\n    lHtable1.put(put);\n\n    for (int i = 0; i < 1000; i++) {\n      rowKey = Bytes.toBytes(\"NormalPut\" + i);\n      put = new Put(rowKey).addColumn(famName, row, row);\n      htable1.put(put);\n    }\n\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.disableTable(tablename);\n      admin1.deleteTable(tablename);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.disableTable(tablename);\n      admin2.deleteTable(tablename);\n    }\n\n    admin.enablePeer(\"2\");\n    if (allowProceeding) {\n      // in this we'd expect the key to make it over\n      verifyReplicationProceeded(rowKey);\n    } else {\n      verifyReplicationStuck(rowKey);\n    }\n    // just to be safe\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n  }"
        ],
        [
            "TestReplicationDroppedTables::testEditsBehindDroppedTableTiming()",
            " 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  \n 235 -\n 236 -\n 237  \n 238  \n 239  \n 240 -\n 241 -\n 242 -\n 243 -\n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 -\n 256  \n 257  \n 258  \n 259 -\n 260  \n 261  \n 262  \n 263  \n 264 -\n 265  \n 266  \n 267  \n 268  ",
            "  @Test\n  public void testEditsBehindDroppedTableTiming() throws Exception {\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, true);\n    conf1.setInt(HConstants.REPLICATION_SOURCE_MAXTHREADS_KEY, 1);\n\n    // make sure we have a single region server only, so that all\n    // edits for all tables go there\n    utility1.shutdownMiniHBaseCluster();\n    utility1.startMiniHBaseCluster(1, 1);\n\n    TableName tablename = TableName.valueOf(\"testdroppedtimed\");\n    byte[] familyname = Bytes.toBytes(\"fam\");\n    byte[] row = Bytes.toBytes(\"row\");\n\n    HTableDescriptor table = new HTableDescriptor(tablename);\n    HColumnDescriptor fam = new HColumnDescriptor(familyname);\n    fam.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);\n    table.addFamily(fam);\n\n    Connection connection1 = ConnectionFactory.createConnection(conf1);\n    Connection connection2 = ConnectionFactory.createConnection(conf2);\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.createTable(table);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.createTable(table);\n    }\n    utility1.waitUntilAllRegionsAssigned(tablename);\n    utility2.waitUntilAllRegionsAssigned(tablename);\n\n    Table lHtable1 = utility1.getConnection().getTable(tablename);\n\n    // now suspend replication\n    admin.disablePeer(\"2\");\n\n    // put some data (lead with 0 so the edit gets sorted before the other table's edits\n    //   in the replication batch)\n    // write a bunch of edits, making sure we fill a batch\n    byte[] rowkey = Bytes.toBytes(0+\" put on table to be dropped\");\n    Put put = new Put(rowkey);\n    put.addColumn(familyname, row, row);\n    lHtable1.put(put);\n\n    rowkey = Bytes.toBytes(\"normal put\");\n    put = new Put(rowkey);\n    put.addColumn(famName, row, row);\n    htable1.put(put);\n\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.disableTable(tablename);\n      admin2.deleteTable(tablename);\n    }\n\n    admin.enablePeer(\"2\");\n    // edit should still be stuck\n\n    try (Admin admin1 = connection1.getAdmin()) {\n      // the source table still exists, replication should be stalled\n      verifyReplicationStuck(rowkey);\n\n      admin1.disableTable(tablename);\n      // still stuck, source table still exists\n      verifyReplicationStuck(rowkey);\n\n      admin1.deleteTable(tablename);\n      // now the source table is gone, replication should proceed, the\n      // offending edits be dropped\n      verifyReplicationProceeded(rowkey);\n    }\n    // just to be safe\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n  }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242  \n 243 +\n 244 +\n 245  \n 246  \n 247  \n 248 +\n 249 +\n 250 +\n 251 +\n 252 +\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264 +\n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273 +\n 274  \n 275  \n 276  \n 277  ",
            "  @Test\n  public void testEditsBehindDroppedTableTiming() throws Exception {\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, true);\n    conf1.setInt(HConstants.REPLICATION_SOURCE_MAXTHREADS_KEY, 1);\n\n    // make sure we have a single region server only, so that all\n    // edits for all tables go there\n    utility1.shutdownMiniHBaseCluster();\n    utility1.startMiniHBaseCluster(1, 1);\n\n    TableName tablename = TableName.valueOf(\"testdroppedtimed\");\n    byte[] familyname = Bytes.toBytes(\"fam\");\n    byte[] row = Bytes.toBytes(\"row\");\n\n    HTableDescriptor table = new HTableDescriptor(tablename);\n    HColumnDescriptor fam = new HColumnDescriptor(familyname);\n    fam.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);\n    table.addFamily(fam);\n\n    Connection connection1 = ConnectionFactory.createConnection(conf1);\n    Connection connection2 = ConnectionFactory.createConnection(conf2);\n    try (Admin admin1 = connection1.getAdmin()) {\n      admin1.createTable(table);\n    }\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.createTable(table);\n    }\n    utility1.waitUntilAllRegionsAssigned(tablename);\n    utility2.waitUntilAllRegionsAssigned(tablename);\n\n    Table lHtable1 = utility1.getConnection().getTable(tablename);\n\n    // now suspend replication\n    admin.disablePeer(\"2\");\n\n    // put some data (lead with 0 so the edit gets sorted before the other table's edits\n    // in the replication batch)\n    // write a bunch of edits, making sure we fill a batch\n    byte[] rowKey = Bytes.toBytes(0 + \" put on table to be dropped\");\n    Put put = new Put(rowKey);\n    put.addColumn(familyname, row, row);\n    lHtable1.put(put);\n\n    for (int i = 0; i < 1000; i++) {\n      rowKey = Bytes.toBytes(\"NormalPut\" + i);\n      put = new Put(rowKey).addColumn(famName, row, row);\n      htable1.put(put);\n    }\n\n    try (Admin admin2 = connection2.getAdmin()) {\n      admin2.disableTable(tablename);\n      admin2.deleteTable(tablename);\n    }\n\n    admin.enablePeer(\"2\");\n    // edit should still be stuck\n\n    try (Admin admin1 = connection1.getAdmin()) {\n      // the source table still exists, replication should be stalled\n      verifyReplicationStuck(rowKey);\n\n      admin1.disableTable(tablename);\n      // still stuck, source table still exists\n      verifyReplicationStuck(rowKey);\n\n      admin1.deleteTable(tablename);\n      // now the source table is gone, replication should proceed, the\n      // offending edits be dropped\n      verifyReplicationProceeded(rowKey);\n    }\n    // just to be safe\n    conf1.setBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n  }"
        ],
        [
            "TestReplicationDroppedTables::setUp()",
            "  58  \n  59  \n  60  \n  61  \n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  @Before\n  public void setUp() throws Exception {\n    // Starting and stopping replication can make us miss new logs,\n    // rolling like this makes sure the most recent one gets added to the queue\n    for ( JVMClusterUtil.RegionServerThread r :\n        utility1.getHBaseCluster().getRegionServerThreads()) {\n      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n    }\n    int rowCount = utility1.countRows(tableName);\n    utility1.deleteTableData(tableName);\n    // truncating the table will send one Delete per row to the slave cluster\n    // in an async fashion, which is why we cannot just call deleteTableData on\n    // utility2 since late writes could make it to the slave in some way.\n    // Instead, we truncate the first table and wait for all the Deletes to\n    // make it to the slave.\n    Scan scan = new Scan();\n    int lastCount = 0;\n    for (int i = 0; i < NB_RETRIES; i++) {\n      if (i==NB_RETRIES-1) {\n        fail(\"Waited too much time for truncate\");\n      }\n      ResultScanner scanner = htable2.getScanner(scan);\n      Result[] res = scanner.next(rowCount);\n      scanner.close();\n      if (res.length != 0) {\n        if (res.length < lastCount) {\n          i--; // Don't increment timeout if we make progress\n        }\n        lastCount = res.length;\n        LOG.info(\"Still got \" + res.length + \" rows\");\n        Thread.sleep(SLEEP_TIME);\n      } else {\n        break;\n      }\n    }\n  }",
            "  59  \n  60  \n  61  \n  62  \n  63 +\n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99 +\n 100  ",
            "  @Before\n  public void setUp() throws Exception {\n    // Starting and stopping replication can make us miss new logs,\n    // rolling like this makes sure the most recent one gets added to the queue\n    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n        .getRegionServerThreads()) {\n      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n    }\n    int rowCount = utility1.countRows(tableName);\n    utility1.deleteTableData(tableName);\n    // truncating the table will send one Delete per row to the slave cluster\n    // in an async fashion, which is why we cannot just call deleteTableData on\n    // utility2 since late writes could make it to the slave in some way.\n    // Instead, we truncate the first table and wait for all the Deletes to\n    // make it to the slave.\n    Scan scan = new Scan();\n    int lastCount = 0;\n    for (int i = 0; i < NB_RETRIES; i++) {\n      if (i == NB_RETRIES - 1) {\n        fail(\"Waited too much time for truncate\");\n      }\n      ResultScanner scanner = htable2.getScanner(scan);\n      Result[] res = scanner.next(rowCount);\n      scanner.close();\n      if (res.length != 0) {\n        if (res.length < lastCount) {\n          i--; // Don't increment timeout if we make progress\n        }\n        lastCount = res.length;\n        LOG.info(\"Still got \" + res.length + \" rows\");\n        Thread.sleep(SLEEP_TIME);\n      } else {\n        break;\n      }\n    }\n    // Set the max request size to a tiny 10K for dividing the replication WAL entries into multiple\n    // batches. the default max request size is 256M, so all replication entries are in a batch, but\n    // when replicate at sink side, it'll apply to rs group by table name, so the WAL of test table\n    // may apply first, and then test_dropped table, and we will believe that the replication is not\n    // got stuck (HBASE-20475).\n    conf1.setInt(RpcServer.MAX_REQUEST_SIZE, 10 * 1024);\n  }"
        ]
    ],
    "7357b0ce9fb4ae0700538c167edbf2c320783490": [
        [
            "Export::processData(Region,Configuration,UserProvider,Scan,Token,List)",
            " 208 -\n 209 -\n 210 -\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "  private static ExportProtos.ExportResponse processData(final Region region, final Configuration conf,\n    final UserProvider userProvider, final Scan scan, final Token userToken,\n    final List<SequenceFile.Writer.Option> opts) throws IOException {\n    ScanCoprocessor cp = new ScanCoprocessor(region);\n    RegionScanner scanner = null;\n    try (RegionOp regionOp = new RegionOp(region);\n            SecureWriter out = new SecureWriter(conf, userProvider, userToken, opts)) {\n      scanner = cp.checkScannerOpen(scan);\n      ImmutableBytesWritable key = new ImmutableBytesWritable();\n      long rowCount = 0;\n      long cellCount = 0;\n      List<Result> results = new ArrayList<>();\n      List<Cell> cells = new ArrayList<>();\n      boolean hasMore;\n      do {\n        boolean bypass = cp.preScannerNext(scanner, results, scan.getBatch());\n        if (bypass) {\n          hasMore = false;\n        } else {\n          hasMore = scanner.nextRaw(cells);\n          if (cells.isEmpty()) {\n            continue;\n          }\n          Cell firstCell = cells.get(0);\n          for (Cell cell : cells) {\n            if (Bytes.compareTo(firstCell.getRowArray(), firstCell.getRowOffset(), firstCell.getRowLength(),\n                    cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()) != 0) {\n              throw new IOException(\"Why the RegionScanner#nextRaw returns the data of different rows??\"\n                      + \" first row=\" + Bytes.toHex(firstCell.getRowArray(), firstCell.getRowOffset(), firstCell.getRowLength())\n                      + \", current row=\" + Bytes.toHex(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()));\n            }\n          }\n          results.add(Result.create(cells));\n          cells.clear();\n          cp.postScannerNext(scanner, results, scan.getBatch(), hasMore);\n        }\n        for (Result r : results) {\n          key.set(r.getRow());\n          out.append(key, r);\n          ++rowCount;\n          cellCount += r.size();\n        }\n        results.clear();\n      } while (hasMore);\n      return ExportProtos.ExportResponse.newBuilder()\n              .setRowCount(rowCount)\n              .setCellCount(cellCount)\n              .build();\n    } finally {\n      cp.checkScannerClose(scanner);\n    }\n  }",
            " 214 +\n 215 +\n 216 +\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "  private static ExportProtos.ExportResponse processData(final Region region,\n      final Configuration conf, final UserProvider userProvider, final Scan scan,\n      final Token userToken, final List<SequenceFile.Writer.Option> opts) throws IOException {\n    ScanCoprocessor cp = new ScanCoprocessor(region);\n    RegionScanner scanner = null;\n    try (RegionOp regionOp = new RegionOp(region);\n            SecureWriter out = new SecureWriter(conf, userProvider, userToken, opts)) {\n      scanner = cp.checkScannerOpen(scan);\n      ImmutableBytesWritable key = new ImmutableBytesWritable();\n      long rowCount = 0;\n      long cellCount = 0;\n      List<Result> results = new ArrayList<>();\n      List<Cell> cells = new ArrayList<>();\n      boolean hasMore;\n      do {\n        boolean bypass = cp.preScannerNext(scanner, results, scan.getBatch());\n        if (bypass) {\n          hasMore = false;\n        } else {\n          hasMore = scanner.nextRaw(cells);\n          if (cells.isEmpty()) {\n            continue;\n          }\n          Cell firstCell = cells.get(0);\n          for (Cell cell : cells) {\n            if (Bytes.compareTo(firstCell.getRowArray(), firstCell.getRowOffset(),\n                firstCell.getRowLength(), cell.getRowArray(), cell.getRowOffset(),\n                cell.getRowLength()) != 0) {\n              throw new IOException(\"Why the RegionScanner#nextRaw returns the data of different\"\n                  + \" rows?? first row=\"\n                  + Bytes.toHex(firstCell.getRowArray(), firstCell.getRowOffset(),\n                    firstCell.getRowLength())\n                  + \", current row=\"\n                  + Bytes.toHex(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()));\n            }\n          }\n          results.add(Result.create(cells));\n          cells.clear();\n          cp.postScannerNext(scanner, results, scan.getBatch(), hasMore);\n        }\n        for (Result r : results) {\n          key.set(r.getRow());\n          out.append(key, r);\n          ++rowCount;\n          cellCount += r.size();\n        }\n        results.clear();\n      } while (hasMore);\n      return ExportProtos.ExportResponse.newBuilder()\n              .setRowCount(rowCount)\n              .setCellCount(cellCount)\n              .build();\n    } finally {\n      cp.checkScannerClose(scanner);\n    }\n  }"
        ],
        [
            "AggregationClient::getMedianArgs(Table,ColumnInterpreter,Scan)",
            " 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679 -\n 680 -\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  ",
            "  /**\n   * It helps locate the region with median for a given column whose weight\n   * is specified in an optional column.\n   * From individual regions, it obtains sum of values and sum of weights.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return pair whose first element is a map between start row of the region\n   *  and (sum of values, sum of weights) for the region, the second element is\n   *  (sum of values, sum of weights) for all the regions chosen\n   * @throws Throwable\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n  Pair<NavigableMap<byte[], List<S>>, List<S>>\n  getMedianArgs(final Table table,\n      final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    final NavigableMap<byte[], List<S>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n    class StdCallback implements Batch.Callback<List<S>> {\n      S sumVal = null, sumWeights = null;\n\n      public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {\n        List<S> l = new ArrayList<>(2);\n        l.add(sumVal);\n        l.add(sumWeights);\n        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);\n        return p;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, List<S> result) {\n        map.put(row, result);\n        sumVal = ci.add(sumVal, result.get(0));\n        sumWeights = ci.add(sumWeights, result.get(1));\n      }\n    }\n    StdCallback stdCallback = new StdCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, List<S>>() {\n          @Override\n          public List<S> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMedian(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n\n            List<S> list = new ArrayList<>();\n            for (int i = 0; i < response.getFirstPartCount(); i++) {\n              ByteString b = response.getFirstPart(i);\n              T t = getParsedGenericInstance(ci.getClass(), 4, b);\n              S s = ci.getPromotedValueFromProto(t);\n              list.add(s);\n            }\n            return list;\n          }\n\n        }, stdCallback);\n    return stdCallback.getMedianParams();\n  }",
            " 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690 +\n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  ",
            "  /**\n   * It helps locate the region with median for a given column whose weight\n   * is specified in an optional column.\n   * From individual regions, it obtains sum of values and sum of weights.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return pair whose first element is a map between start row of the region\n   *   and (sum of values, sum of weights) for the region, the second element is\n   *   (sum of values, sum of weights) for all the regions chosen\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n    Pair<NavigableMap<byte[], List<S>>, List<S>>\n    getMedianArgs(final Table table,\n      final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    final NavigableMap<byte[], List<S>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n    class StdCallback implements Batch.Callback<List<S>> {\n      S sumVal = null, sumWeights = null;\n\n      public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {\n        List<S> l = new ArrayList<>(2);\n        l.add(sumVal);\n        l.add(sumWeights);\n        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);\n        return p;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, List<S> result) {\n        map.put(row, result);\n        sumVal = ci.add(sumVal, result.get(0));\n        sumWeights = ci.add(sumWeights, result.get(1));\n      }\n    }\n    StdCallback stdCallback = new StdCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, List<S>>() {\n          @Override\n          public List<S> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMedian(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n\n            List<S> list = new ArrayList<>();\n            for (int i = 0; i < response.getFirstPartCount(); i++) {\n              ByteString b = response.getFirstPart(i);\n              T t = getParsedGenericInstance(ci.getClass(), 4, b);\n              S s = ci.getPromotedValueFromProto(t);\n              list.add(s);\n            }\n            return list;\n          }\n\n        }, stdCallback);\n    return stdCallback.getMedianParams();\n  }"
        ],
        [
            "AggregateImplementation::getMin(RpcController,AggregateRequest,RpcCallback)",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "  /**\n   * Gives the minimum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, minimum value for the\n   * entire column family will be returned.\n   */\n  @Override\n  public void getMin(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    T min = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      List<Cell> results = new ArrayList<>();\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          min = (min == null || (temp != null && ci.compare(temp, min) < 0)) ? temp : min;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (min != null) {\n        response = AggregateResponse.newBuilder().addFirstPart(\n          ci.getProtoForCellType(min).toByteString()).build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Minimum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + min);\n    done.run(response);\n  }",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "  /**\n   * Gives the minimum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, minimum value for the\n   * entire column family will be returned.\n   */\n  @Override\n  public void getMin(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    T min = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      List<Cell> results = new ArrayList<>();\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          min = (min == null || (temp != null && ci.compare(temp, min) < 0)) ? temp : min;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (min != null) {\n        response = AggregateResponse.newBuilder().addFirstPart(\n          ci.getProtoForCellType(min).toByteString()).build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Minimum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + min);\n    done.run(response);\n  }"
        ],
        [
            "AggregationClient::median(TableName,ColumnInterpreter,Scan)",
            " 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741 -\n 742  \n 743  \n 744 -\n 745  \n 746  ",
            "  /**\n   * This is the client side interface/handler for calling the median method for a\n   * given cf-cq combination. This method collects the necessary parameters\n   * to compute the median and returns the median.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return R the median\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  R median(final TableName tableName, ColumnInterpreter<R, S, P, Q, T> ci,\n      Scan scan) throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n        return median(table, ci, scan);\n    }\n  }",
            " 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753 +\n 754  \n 755  \n 756 +\n 757  \n 758  ",
            "  /**\n   * This is the client side interface/handler for calling the median method for a\n   * given cf-cq combination. This method collects the necessary parameters\n   * to compute the median and returns the median.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return R the median\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    R median(final TableName tableName, ColumnInterpreter<R, S, P, Q, T> ci,\n      Scan scan) throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return median(table, ci, scan);\n    }\n  }"
        ],
        [
            "AggregationClient::min(TableName,ColumnInterpreter,Scan)",
            " 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 -\n 242 -\n 243  \n 244  \n 245  \n 246  ",
            "  /**\n   * It gives the minimum value of a column for a given column family for the\n   * given range. In case qualifier is null, a min of all values for the given\n   * family is returned.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return min val &lt;R&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> R min(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n  throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return min(table, ci, scan);\n    }\n  }",
            " 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237 +\n 238  \n 239  \n 240  \n 241  ",
            "  /**\n   * It gives the minimum value of a column for a given column family for the\n   * given range. In case qualifier is null, a min of all values for the given\n   * family is returned.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return min val &lt;R&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> R min(\n          final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return min(table, ci, scan);\n    }\n  }"
        ],
        [
            "ProtobufCoprocessorService::ping(RpcController,TestProtos,RpcCallback)",
            "  52  \n  53  \n  54 -\n  55  \n  56  ",
            "  @Override\n  public void ping(RpcController controller, TestProtos.EmptyRequestProto request,\n      RpcCallback<TestProtos.EmptyResponseProto> done) {\n    done.run(TestProtos.EmptyResponseProto.getDefaultInstance());\n  }",
            "  52  \n  53  \n  54 +\n  55  \n  56  ",
            "  @Override\n  public void ping(RpcController controller, TestProtos.EmptyRequestProto request,\n          RpcCallback<TestProtos.EmptyResponseProto> done) {\n    done.run(TestProtos.EmptyResponseProto.getDefaultInstance());\n  }"
        ],
        [
            "ProtobufCoprocessorService::addr(RpcController,EmptyRequestProto,RpcCallback)",
            "  79  \n  80  \n  81 -\n  82  \n  83  \n  84  ",
            "  @Override\n  public void addr(RpcController controller, EmptyRequestProto request,\n      RpcCallback<AddrResponseProto> done) {\n    done.run(AddrResponseProto.newBuilder()\n        .setAddr(RpcServer.getRemoteAddress().get().getHostAddress()).build());\n  }",
            "  79  \n  80  \n  81 +\n  82  \n  83  \n  84  ",
            "  @Override\n  public void addr(RpcController controller, EmptyRequestProto request,\n          RpcCallback<AddrResponseProto> done) {\n    done.run(AddrResponseProto.newBuilder()\n        .setAddr(RpcServer.getRemoteAddress().get().getHostAddress()).build());\n  }"
        ],
        [
            "TestServerCustomProtocol::doPing(PingProtos)",
            " 410  \n 411  \n 412  \n 413 -\n 414 -\n 415  ",
            "  private static String doPing(PingProtos.PingService instance) throws IOException {\n    CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse> rpcCallback =\n        new CoprocessorRpcUtils.BlockingRpcCallback<>();\n      instance.ping(null, PingProtos.PingRequest.newBuilder().build(), rpcCallback);\n      return rpcCallback.get().getPong();\n  }",
            " 414  \n 415  \n 416  \n 417 +\n 418 +\n 419  ",
            "  private static String doPing(PingProtos.PingService instance) throws IOException {\n    CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.PingResponse> rpcCallback =\n        new CoprocessorRpcUtils.BlockingRpcCallback<>();\n    instance.ping(null, PingProtos.PingRequest.newBuilder().build(), rpcCallback);\n    return rpcCallback.get().getPong();\n  }"
        ],
        [
            "SecureBulkLoadEndpointClient::bulkLoadHFiles(List,Token,String,byte)",
            " 113  \n 114 -\n 115 -\n 116 -\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  public boolean bulkLoadHFiles(final List<Pair<byte[], String>> familyPaths,\n                         final Token<?> userToken,\n                         final String bulkToken,\n                         final byte[] startRow) throws IOException {\n    // we never want to send a batch of HFiles to all regions, thus cannot call\n    // HTable#coprocessorService methods that take start and end rowkeys; see HBASE-9639\n    try {\n      CoprocessorRpcChannel channel = table.coprocessorService(startRow);\n      SecureBulkLoadProtos.SecureBulkLoadService instance =\n          ProtobufUtil.newServiceStub(SecureBulkLoadProtos.SecureBulkLoadService.class, channel);\n\n      DelegationToken protoDT =\n          DelegationToken.newBuilder().build();\n      if(userToken != null) {\n        protoDT =\n            DelegationToken.newBuilder()\n              .setIdentifier(ByteStringer.wrap(userToken.getIdentifier()))\n              .setPassword(ByteStringer.wrap(userToken.getPassword()))\n              .setKind(userToken.getKind().toString())\n              .setService(userToken.getService().toString()).build();\n      }\n\n      List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths =\n          new ArrayList<>(familyPaths.size());\n      for(Pair<byte[], String> el: familyPaths) {\n        protoFamilyPaths.add(ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder()\n          .setFamily(ByteStringer.wrap(el.getFirst()))\n          .setPath(el.getSecond()).build());\n      }\n\n      SecureBulkLoadProtos.SecureBulkLoadHFilesRequest request =\n          SecureBulkLoadProtos.SecureBulkLoadHFilesRequest.newBuilder()\n            .setFsToken(protoDT)\n            .addAllFamilyPath(protoFamilyPaths)\n            .setBulkToken(bulkToken).build();\n\n      ServerRpcController controller = new ServerRpcController();\n      CoprocessorRpcUtils.BlockingRpcCallback<SecureBulkLoadProtos.SecureBulkLoadHFilesResponse>\n            rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();\n      instance.secureBulkLoadHFiles(controller,\n        request,\n        rpcCallback);\n\n      SecureBulkLoadProtos.SecureBulkLoadHFilesResponse response = rpcCallback.get();\n      if (controller.failedOnException()) {\n        throw controller.getFailedOn();\n      }\n      return response.getLoaded();\n    } catch (Throwable throwable) {\n      throw new IOException(throwable);\n    }\n  }",
            " 112  \n 113 +\n 114 +\n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "  public boolean bulkLoadHFiles(final List<Pair<byte[], String>> familyPaths,\n          final Token<?> userToken, final String bulkToken, final byte[] startRow)\n          throws IOException {\n    // we never want to send a batch of HFiles to all regions, thus cannot call\n    // HTable#coprocessorService methods that take start and end rowkeys; see HBASE-9639\n    try {\n      CoprocessorRpcChannel channel = table.coprocessorService(startRow);\n      SecureBulkLoadProtos.SecureBulkLoadService instance =\n          ProtobufUtil.newServiceStub(SecureBulkLoadProtos.SecureBulkLoadService.class, channel);\n\n      DelegationToken protoDT =\n          DelegationToken.newBuilder().build();\n      if(userToken != null) {\n        protoDT =\n            DelegationToken.newBuilder()\n              .setIdentifier(ByteStringer.wrap(userToken.getIdentifier()))\n              .setPassword(ByteStringer.wrap(userToken.getPassword()))\n              .setKind(userToken.getKind().toString())\n              .setService(userToken.getService().toString()).build();\n      }\n\n      List<ClientProtos.BulkLoadHFileRequest.FamilyPath> protoFamilyPaths =\n          new ArrayList<>(familyPaths.size());\n      for(Pair<byte[], String> el: familyPaths) {\n        protoFamilyPaths.add(ClientProtos.BulkLoadHFileRequest.FamilyPath.newBuilder()\n          .setFamily(ByteStringer.wrap(el.getFirst()))\n          .setPath(el.getSecond()).build());\n      }\n\n      SecureBulkLoadProtos.SecureBulkLoadHFilesRequest request =\n          SecureBulkLoadProtos.SecureBulkLoadHFilesRequest.newBuilder()\n            .setFsToken(protoDT)\n            .addAllFamilyPath(protoFamilyPaths)\n            .setBulkToken(bulkToken).build();\n\n      ServerRpcController controller = new ServerRpcController();\n      CoprocessorRpcUtils.BlockingRpcCallback<SecureBulkLoadProtos.SecureBulkLoadHFilesResponse>\n            rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();\n      instance.secureBulkLoadHFiles(controller,\n        request,\n        rpcCallback);\n\n      SecureBulkLoadProtos.SecureBulkLoadHFilesResponse response = rpcCallback.get();\n      if (controller.failedOnException()) {\n        throw controller.getFailedOn();\n      }\n      return response.getLoaded();\n    } catch (Throwable throwable) {\n      throw new IOException(throwable);\n    }\n  }"
        ],
        [
            "TestCoprocessorEndpoint::sum(Table,byte,byte,byte,byte)",
            " 121  \n 122 -\n 123 -\n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  ",
            "  private Map<byte [], Long> sum(final Table table, final byte [] family,\n      final byte [] qualifier, final byte [] start, final byte [] end)\n  throws ServiceException, Throwable {\n    return table.coprocessorService(ColumnAggregationProtos.ColumnAggregationService.class,\n        start, end,\n      new Batch.Call<ColumnAggregationProtos.ColumnAggregationService, Long>() {\n        @Override\n        public Long call(ColumnAggregationProtos.ColumnAggregationService instance)\n        throws IOException {\n          CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n          ColumnAggregationProtos.SumRequest.Builder builder =\n            ColumnAggregationProtos.SumRequest.newBuilder();\n          builder.setFamily(ByteStringer.wrap(family));\n          if (qualifier != null && qualifier.length > 0) {\n            builder.setQualifier(ByteStringer.wrap(qualifier));\n          }\n          instance.sum(null, builder.build(), rpcCallback);\n          return rpcCallback.get().getSum();\n        }\n      });\n  }",
            " 120  \n 121 +\n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  private Map<byte [], Long> sum(final Table table, final byte [] family,\n          final byte [] qualifier, final byte [] start, final byte [] end)\n          throws ServiceException, Throwable {\n    return table.coprocessorService(ColumnAggregationProtos.ColumnAggregationService.class,\n        start, end,\n      new Batch.Call<ColumnAggregationProtos.ColumnAggregationService, Long>() {\n        @Override\n        public Long call(ColumnAggregationProtos.ColumnAggregationService instance)\n          throws IOException {\n          CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n          ColumnAggregationProtos.SumRequest.Builder builder =\n            ColumnAggregationProtos.SumRequest.newBuilder();\n          builder.setFamily(ByteStringer.wrap(family));\n          if (qualifier != null && qualifier.length > 0) {\n            builder.setQualifier(ByteStringer.wrap(qualifier));\n          }\n          instance.sum(null, builder.build(), rpcCallback);\n          return rpcCallback.get().getSum();\n        }\n      });\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::convert(PrepareBulkLoadRequest)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest\n    convert(PrepareBulkLoadRequest request)\n  throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest.Builder\n          builder =\n        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest.\n        newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest\n    convert(PrepareBulkLoadRequest request)\n    throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest.Builder\n          builder =\n        org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest.\n        newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }"
        ],
        [
            "AggregationClient::sum(TableName,ColumnInterpreter,Scan)",
            " 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385  \n 386 -\n 387  \n 388  ",
            "  /**\n   * It sums up the value returned from various regions. In case qualifier is\n   * null, summation of all the column qualifiers in the given family is done.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return sum &lt;S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> S sum(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n  throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n        return sum(table, ci, scan);\n    }\n  }",
            " 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383 +\n 384  \n 385 +\n 386  \n 387  ",
            "  /**\n   * It sums up the value returned from various regions. In case qualifier is\n   * null, summation of all the column qualifiers in the given family is done.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return sum &lt;S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> S sum(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n    throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return sum(table, ci, scan);\n    }\n  }"
        ],
        [
            "TestRowProcessorEndpoint::concurrentExec(Runnable,int)",
            " 222 -\n 223 -\n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  private void concurrentExec(\n      final Runnable task, final int numThreads) throws Throwable {\n    startSignal = new CountDownLatch(numThreads);\n    doneSignal = new CountDownLatch(numThreads);\n    for (int i = 0; i < numThreads; ++i) {\n      new Thread(new Runnable() {\n        @Override\n        public void run() {\n          try {\n            startSignal.countDown();\n            startSignal.await();\n            task.run();\n          } catch (Throwable e) {\n            failures.incrementAndGet();\n            e.printStackTrace();\n          }\n          doneSignal.countDown();\n        }\n      }).start();\n    }\n    doneSignal.await();\n  }",
            " 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "  private void concurrentExec(final Runnable task, final int numThreads) throws Throwable {\n    startSignal = new CountDownLatch(numThreads);\n    doneSignal = new CountDownLatch(numThreads);\n    for (int i = 0; i < numThreads; ++i) {\n      new Thread(new Runnable() {\n        @Override\n        public void run() {\n          try {\n            startSignal.countDown();\n            startSignal.await();\n            task.run();\n          } catch (Throwable e) {\n            failures.incrementAndGet();\n            e.printStackTrace();\n          }\n          doneSignal.countDown();\n        }\n      }).start();\n    }\n    doneSignal.await();\n  }"
        ],
        [
            "AsyncAggregationClient::sum(AsyncTable,ColumnInterpreter,Scan)",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246 -\n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message> CompletableFuture<S>\n      sum(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<S> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<S> callback = new AbstractAggregationCallback<S>(future) {\n\n      private S sum;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          S s = getPromotedValueFromProto(ci, resp, 0);\n          sum = ci.add(sum, s);\n        }\n      }\n\n      @Override\n      protected S getFinalResult() {\n        return sum;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getSum(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message> CompletableFuture<S>\n      sum(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<S> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<S> callback = new AbstractAggregationCallback<S>(future) {\n      private S sum;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          S s = getPromotedValueFromProto(ci, resp, 0);\n          sum = ci.add(sum, s);\n        }\n      }\n\n      @Override\n      protected S getFinalResult() {\n        return sum;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getSum(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "AggregationClient::rowCount(TableName,ColumnInterpreter,Scan)",
            " 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316 -\n 317  \n 318 -\n 319  \n 320  ",
            "  /**\n   * It gives the row count, by summing up the individual results obtained from\n   * regions. In case the qualifier is null, FirstKeyValueFilter is used to\n   * optimised the operation. In case qualifier is provided, I can't use the\n   * filter as it may set the flag to skip to next row, but the value read is\n   * not of the given filter: in this case, this particular row will not be\n   * counted ==&gt; an error.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return &lt;R, S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> long rowCount(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n  throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n        return rowCount(table, ci, scan);\n    }\n  }",
            " 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 +\n 312 +\n 313  \n 314 +\n 315  \n 316  ",
            "  /**\n   * It gives the row count, by summing up the individual results obtained from\n   * regions. In case the qualifier is null, FirstKeyValueFilter is used to\n   * optimised the operation. In case qualifier is provided, I can't use the\n   * filter as it may set the flag to skip to next row, but the value read is\n   * not of the given filter: in this case, this particular row will not be\n   * counted ==&gt; an error.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return &lt;R, S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> long rowCount(\n          final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return rowCount(table, ci, scan);\n    }\n  }"
        ],
        [
            "Export::getCompressionType(ExportProtos)",
            " 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "  private static SequenceFile.CompressionType getCompressionType(final ExportProtos.ExportRequest request) {\n    if (request.hasCompressType()) {\n      return SequenceFile.CompressionType.valueOf(request.getCompressType());\n    } else {\n      return DEFAULT_TYPE;\n    }\n  }",
            " 163 +\n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "  private static SequenceFile.CompressionType getCompressionType(\n      final ExportProtos.ExportRequest request) {\n    if (request.hasCompressType()) {\n      return SequenceFile.CompressionType.valueOf(request.getCompressType());\n    } else {\n      return DEFAULT_TYPE;\n    }\n  }"
        ],
        [
            "TestCoprocessorEndpoint::testCoprocessorService()",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205 -\n 206 -\n 207 -\n 208 -\n 209 -\n 210 -\n 211 -\n 212 -\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242 -\n 243 -\n 244 -\n 245 -\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  @Test\n  public void testCoprocessorService() throws Throwable {\n    Table table = util.getConnection().getTable(TEST_TABLE);\n\n    List<HRegionLocation> regions;\n    try(RegionLocator rl = util.getConnection().getRegionLocator(TEST_TABLE)) {\n      regions = rl.getAllRegionLocations();\n    }\n    final TestProtos.EchoRequestProto request =\n        TestProtos.EchoRequestProto.newBuilder().setMessage(\"hello\").build();\n    final Map<byte[], String> results = Collections.synchronizedMap(\n        new TreeMap<byte[], String>(Bytes.BYTES_COMPARATOR));\n    try {\n      // scan: for all regions\n      final RpcController controller = new ServerRpcController();\n      table.coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto.class,\n          ROWS[0], ROWS[ROWS.length - 1],\n          new Batch.Call<TestRpcServiceProtos.TestProtobufRpcProto, TestProtos.EchoResponseProto>() {\n            public TestProtos.EchoResponseProto call(TestRpcServiceProtos.TestProtobufRpcProto instance)\n                throws IOException {\n              LOG.debug(\"Default response is \" + TestProtos.EchoRequestProto.getDefaultInstance());\n              CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =\n                  new CoprocessorRpcUtils.BlockingRpcCallback<>();\n              instance.echo(controller, request, callback);\n              TestProtos.EchoResponseProto response = callback.get();\n              LOG.debug(\"Batch.Call returning result \" + response);\n              return response;\n            }\n          },\n          new Batch.Callback<TestProtos.EchoResponseProto>() {\n            public void update(byte[] region, byte[] row, TestProtos.EchoResponseProto result) {\n              assertNotNull(result);\n              assertEquals(\"hello\", result.getMessage());\n              results.put(region, result.getMessage());\n            }\n          }\n      );\n      for (Map.Entry<byte[], String> e : results.entrySet()) {\n        LOG.info(\"Got value \"+e.getValue()+\" for region \"+Bytes.toStringBinary(e.getKey()));\n      }\n      assertEquals(3, results.size());\n      for (HRegionLocation info : regions) {\n        LOG.info(\"Region info is \"+info.getRegionInfo().getRegionNameAsString());\n        assertTrue(results.containsKey(info.getRegionInfo().getRegionName()));\n      }\n      results.clear();\n\n      // scan: for region 2 and region 3\n      table.coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto.class,\n          ROWS[rowSeperator1], ROWS[ROWS.length - 1],\n          new Batch.Call<TestRpcServiceProtos.TestProtobufRpcProto, TestProtos.EchoResponseProto>() {\n            public TestProtos.EchoResponseProto call(TestRpcServiceProtos.TestProtobufRpcProto instance)\n                throws IOException {\n              LOG.debug(\"Default response is \" + TestProtos.EchoRequestProto.getDefaultInstance());\n              CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =\n                  new CoprocessorRpcUtils.BlockingRpcCallback<>();\n              instance.echo(controller, request, callback);\n              TestProtos.EchoResponseProto response = callback.get();\n              LOG.debug(\"Batch.Call returning result \" + response);\n              return response;\n            }\n          },\n          new Batch.Callback<TestProtos.EchoResponseProto>() {\n            public void update(byte[] region, byte[] row, TestProtos.EchoResponseProto result) {\n              assertNotNull(result);\n              assertEquals(\"hello\", result.getMessage());\n              results.put(region, result.getMessage());\n            }\n          }\n      );\n      for (Map.Entry<byte[], String> e : results.entrySet()) {\n        LOG.info(\"Got value \"+e.getValue()+\" for region \"+Bytes.toStringBinary(e.getKey()));\n      }\n      assertEquals(2, results.size());\n    } finally {\n      table.close();\n    }\n  }",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213  \n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248  \n 249 +\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "  @Test\n  public void testCoprocessorService() throws Throwable {\n    Table table = util.getConnection().getTable(TEST_TABLE);\n\n    List<HRegionLocation> regions;\n    try(RegionLocator rl = util.getConnection().getRegionLocator(TEST_TABLE)) {\n      regions = rl.getAllRegionLocations();\n    }\n    final TestProtos.EchoRequestProto request =\n        TestProtos.EchoRequestProto.newBuilder().setMessage(\"hello\").build();\n    final Map<byte[], String> results = Collections.synchronizedMap(\n        new TreeMap<byte[], String>(Bytes.BYTES_COMPARATOR));\n    try {\n      // scan: for all regions\n      final RpcController controller = new ServerRpcController();\n      table.coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto.class,\n        ROWS[0], ROWS[ROWS.length - 1],\n        new Batch.Call<TestRpcServiceProtos.TestProtobufRpcProto, TestProtos.EchoResponseProto>() {\n          @Override\n          public TestProtos.EchoResponseProto call(\n              TestRpcServiceProtos.TestProtobufRpcProto instance) throws IOException {\n            LOG.debug(\"Default response is \" + TestProtos.EchoRequestProto.getDefaultInstance());\n            CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.echo(controller, request, callback);\n            TestProtos.EchoResponseProto response = callback.get();\n            LOG.debug(\"Batch.Call returning result \" + response);\n            return response;\n          }\n        },\n        new Batch.Callback<TestProtos.EchoResponseProto>() {\n          @Override\n          public void update(byte[] region, byte[] row, TestProtos.EchoResponseProto result) {\n            assertNotNull(result);\n            assertEquals(\"hello\", result.getMessage());\n            results.put(region, result.getMessage());\n          }\n        }\n      );\n      for (Map.Entry<byte[], String> e : results.entrySet()) {\n        LOG.info(\"Got value \"+e.getValue()+\" for region \"+Bytes.toStringBinary(e.getKey()));\n      }\n      assertEquals(3, results.size());\n      for (HRegionLocation info : regions) {\n        LOG.info(\"Region info is \"+info.getRegionInfo().getRegionNameAsString());\n        assertTrue(results.containsKey(info.getRegionInfo().getRegionName()));\n      }\n      results.clear();\n\n      // scan: for region 2 and region 3\n      table.coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto.class,\n        ROWS[rowSeperator1], ROWS[ROWS.length - 1],\n        new Batch.Call<TestRpcServiceProtos.TestProtobufRpcProto, TestProtos.EchoResponseProto>() {\n          @Override\n          public TestProtos.EchoResponseProto call(\n              TestRpcServiceProtos.TestProtobufRpcProto instance) throws IOException {\n            LOG.debug(\"Default response is \" + TestProtos.EchoRequestProto.getDefaultInstance());\n            CoprocessorRpcUtils.BlockingRpcCallback<TestProtos.EchoResponseProto> callback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.echo(controller, request, callback);\n            TestProtos.EchoResponseProto response = callback.get();\n            LOG.debug(\"Batch.Call returning result \" + response);\n            return response;\n          }\n        },\n        new Batch.Callback<TestProtos.EchoResponseProto>() {\n          @Override\n          public void update(byte[] region, byte[] row, TestProtos.EchoResponseProto result) {\n            assertNotNull(result);\n            assertEquals(\"hello\", result.getMessage());\n            results.put(region, result.getMessage());\n          }\n        }\n      );\n      for (Map.Entry<byte[], String> e : results.entrySet()) {\n        LOG.info(\"Got value \"+e.getValue()+\" for region \"+Bytes.toStringBinary(e.getKey()));\n      }\n      assertEquals(2, results.size());\n    } finally {\n      table.close();\n    }\n  }"
        ],
        [
            "TestCoprocessorTableEndpoint::testCoprocessorTableEndpoint()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  ",
            "  @Test\n  public void testCoprocessorTableEndpoint() throws Throwable {\n    final TableName tableName = TableName.valueOf(name.getMethodName());\n\n    HTableDescriptor desc = new HTableDescriptor(tableName);\n    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));\n    desc.addCoprocessor(org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.class.getName());\n\n    createTable(desc);\n    verifyTable(tableName);\n  }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  ",
            "  @Test\n  public void testCoprocessorTableEndpoint() throws Throwable {\n    final TableName tableName = TableName.valueOf(name.getMethodName());\n\n    HTableDescriptor desc = new HTableDescriptor(tableName);\n    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));\n    desc.addCoprocessor(ColumnAggregationEndpoint.class.getName());\n\n    createTable(desc);\n    verifyTable(tableName);\n  }"
        ],
        [
            "TestHRegionServerBulkLoadWithOldSecureEndpoint::runAtomicBulkloadTest(TableName,int,int)",
            " 158  \n 159 -\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "  void runAtomicBulkloadTest(TableName tableName, int millisToRun, int numScanners)\n      throws Exception {\n    setupTable(tableName, 10);\n\n    TestContext ctx = new TestContext(UTIL.getConfiguration());\n\n    AtomicHFileLoader loader = new AtomicHFileLoader(tableName, ctx, null);\n    ctx.addThread(loader);\n\n    List<AtomicScanReader> scanners = Lists.newArrayList();\n    for (int i = 0; i < numScanners; i++) {\n      AtomicScanReader scanner = new AtomicScanReader(tableName, ctx, families);\n      scanners.add(scanner);\n      ctx.addThread(scanner);\n    }\n\n    ctx.startThreads();\n    ctx.waitFor(millisToRun);\n    ctx.stop();\n\n    LOG.info(\"Loaders:\");\n    LOG.info(\"  loaded \" + loader.numBulkLoads.get());\n    LOG.info(\"  compations \" + loader.numCompactions.get());\n\n    LOG.info(\"Scanners:\");\n    for (AtomicScanReader scanner : scanners) {\n      LOG.info(\"  scanned \" + scanner.numScans.get());\n      LOG.info(\"  verified \" + scanner.numRowsScanned.get() + \" rows\");\n    }\n  }",
            " 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "  void runAtomicBulkloadTest(TableName tableName, int millisToRun, int numScanners)\n          throws Exception {\n    setupTable(tableName, 10);\n\n    TestContext ctx = new TestContext(UTIL.getConfiguration());\n\n    AtomicHFileLoader loader = new AtomicHFileLoader(tableName, ctx, null);\n    ctx.addThread(loader);\n\n    List<AtomicScanReader> scanners = Lists.newArrayList();\n    for (int i = 0; i < numScanners; i++) {\n      AtomicScanReader scanner = new AtomicScanReader(tableName, ctx, families);\n      scanners.add(scanner);\n      ctx.addThread(scanner);\n    }\n\n    ctx.startThreads();\n    ctx.waitFor(millisToRun);\n    ctx.stop();\n\n    LOG.info(\"Loaders:\");\n    LOG.info(\"  loaded \" + loader.numBulkLoads.get());\n    LOG.info(\"  compations \" + loader.numCompactions.get());\n\n    LOG.info(\"Scanners:\");\n    for (AtomicScanReader scanner : scanners) {\n      LOG.info(\"  scanned \" + scanner.numScans.get());\n      LOG.info(\"  verified \" + scanner.numRowsScanned.get() + \" rows\");\n    }\n  }"
        ],
        [
            "TestAsyncCoprocessorEndpoint::testRegionServerCoprocessorServiceError()",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "  @Test\n  public void testRegionServerCoprocessorServiceError() throws Exception {\n    final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();\n    DummyRegionServerEndpointProtos.DummyRequest request =\n        DummyRegionServerEndpointProtos.DummyRequest.getDefaultInstance();\n    try {\n      admin\n          .<DummyRegionServerEndpointProtos.DummyService.Stub, DummyRegionServerEndpointProtos.DummyResponse> coprocessorService(\n            DummyRegionServerEndpointProtos.DummyService::newStub,\n            (s, c, done) -> s.dummyThrow(c, request, done), serverName).get();\n      fail(\"Should have thrown an exception\");\n    } catch (Exception e) {\n      assertTrue(e.getCause() instanceof RetriesExhaustedException);\n      assertTrue(e.getCause().getMessage().contains(WHAT_TO_THROW.getClass().getName().trim()));\n    }\n  }",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  ",
            "  @Test\n  public void testRegionServerCoprocessorServiceError() throws Exception {\n    final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();\n    DummyRegionServerEndpointProtos.DummyRequest request =\n        DummyRegionServerEndpointProtos.DummyRequest.getDefaultInstance();\n    try {\n      admin\n          .<DummyRegionServerEndpointProtos.DummyService.Stub,\n              DummyRegionServerEndpointProtos.DummyResponse> coprocessorService(\n            DummyRegionServerEndpointProtos.DummyService::newStub,\n            (s, c, done) -> s.dummyThrow(c, request, done), serverName).get();\n      fail(\"Should have thrown an exception\");\n    } catch (Exception e) {\n      assertTrue(e.getCause() instanceof RetriesExhaustedException);\n      assertTrue(e.getCause().getMessage().contains(WHAT_TO_THROW.getClass().getName().trim()));\n    }\n  }"
        ],
        [
            "Export::run(Configuration,TableName,Scan,Path)",
            " 117 -\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "  public static Map<byte[], Response> run(final Configuration conf, TableName tableName, Scan scan, Path dir) throws Throwable {\n    FileSystem fs = dir.getFileSystem(conf);\n    UserProvider userProvider = UserProvider.instantiate(conf);\n    checkDir(fs, dir);\n    FsDelegationToken fsDelegationToken = new FsDelegationToken(userProvider, \"renewer\");\n    fsDelegationToken.acquireDelegationToken(fs);\n    try {\n      final ExportProtos.ExportRequest request = getConfiguredRequest(conf, dir,\n        scan, fsDelegationToken.getUserToken());\n      try (Connection con = ConnectionFactory.createConnection(conf);\n              Table table = con.getTable(tableName)) {\n        Map<byte[], Response> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n        table.coprocessorService(ExportProtos.ExportService.class,\n          scan.getStartRow(),\n          scan.getStopRow(),\n          (ExportProtos.ExportService service) -> {\n            ServerRpcController controller = new ServerRpcController();\n            Map<byte[], ExportProtos.ExportResponse> rval = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n            CoprocessorRpcUtils.BlockingRpcCallback<ExportProtos.ExportResponse>\n              rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            service.export(controller, request, rpcCallback);\n            if (controller.failedOnException()) {\n              throw controller.getFailedOn();\n            }\n            return rpcCallback.get();\n          }).forEach((k, v) -> result.put(k, new Response(v)));\n        return result;\n      } catch (Throwable e) {\n        fs.delete(dir, true);\n        throw e;\n      }\n    } finally {\n      fsDelegationToken.releaseDelegationToken();\n    }\n  }",
            " 118 +\n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  public static Map<byte[], Response> run(final Configuration conf, TableName tableName,\n      Scan scan, Path dir) throws Throwable {\n    FileSystem fs = dir.getFileSystem(conf);\n    UserProvider userProvider = UserProvider.instantiate(conf);\n    checkDir(fs, dir);\n    FsDelegationToken fsDelegationToken = new FsDelegationToken(userProvider, \"renewer\");\n    fsDelegationToken.acquireDelegationToken(fs);\n    try {\n      final ExportProtos.ExportRequest request = getConfiguredRequest(conf, dir,\n        scan, fsDelegationToken.getUserToken());\n      try (Connection con = ConnectionFactory.createConnection(conf);\n              Table table = con.getTable(tableName)) {\n        Map<byte[], Response> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n        table.coprocessorService(ExportProtos.ExportService.class,\n          scan.getStartRow(),\n          scan.getStopRow(),\n          (ExportProtos.ExportService service) -> {\n            ServerRpcController controller = new ServerRpcController();\n            Map<byte[], ExportProtos.ExportResponse> rval = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n            CoprocessorRpcUtils.BlockingRpcCallback<ExportProtos.ExportResponse>\n              rpcCallback = new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            service.export(controller, request, rpcCallback);\n            if (controller.failedOnException()) {\n              throw controller.getFailedOn();\n            }\n            return rpcCallback.get();\n          }).forEach((k, v) -> result.put(k, new Response(v)));\n        return result;\n      } catch (Throwable e) {\n        fs.delete(dir, true);\n        throw e;\n      }\n    } finally {\n      fsDelegationToken.releaseDelegationToken();\n    }\n  }"
        ],
        [
            "AsyncAggregationClient::avg(AsyncTable,ColumnInterpreter,Scan)",
            " 270  \n 271 -\n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Double>\n      avg(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<Double> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Double> callback = new AbstractAggregationCallback<Double>(future) {\n\n      private S sum;\n\n      long count = 0L;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          sum = ci.add(sum, getPromotedValueFromProto(ci, resp, 0));\n          count += resp.getSecondPart().asReadOnlyByteBuffer().getLong();\n        }\n      }\n\n      @Override\n      protected Double getFinalResult() {\n        return ci.divideForAvg(sum, count);\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getAvg(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 270  \n 271 +\n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Double> avg(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci,\n          Scan scan) {\n    CompletableFuture<Double> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Double> callback = new AbstractAggregationCallback<Double>(future) {\n      private S sum;\n\n      long count = 0L;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          sum = ci.add(sum, getPromotedValueFromProto(ci, resp, 0));\n          count += resp.getSecondPart().asReadOnlyByteBuffer().getLong();\n        }\n      }\n\n      @Override\n      protected Double getFinalResult() {\n        return ci.divideForAvg(sum, count);\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getAvg(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "AsyncAggregationClient::sumByRegion(AsyncTable,ColumnInterpreter,Scan)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  \n 370 -\n 371 -\n 372 -\n 373 -\n 374 -\n 375  \n 376  \n 377 -\n 378 -\n 379 -\n 380 -\n 381 -\n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  ",
            "  private static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<NavigableMap<byte[], S>>\n      sumByRegion(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<NavigableMap<byte[], S>> future =\n        new CompletableFuture<NavigableMap<byte[], S>>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    int firstPartIndex = scan.getFamilyMap().get(scan.getFamilies()[0]).size() - 1;\n    AbstractAggregationCallback<NavigableMap<byte[], S>> callback =\n        new AbstractAggregationCallback<NavigableMap<byte[], S>>(future) {\n\n          private final NavigableMap<byte[], S> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n\n          @Override\n          protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n            if (resp.getFirstPartCount() > 0) {\n              map.put(region.getStartKey(), getPromotedValueFromProto(ci, resp, firstPartIndex));\n            }\n          }\n\n          @Override\n          protected NavigableMap<byte[], S> getFinalResult() {\n            return map;\n          }\n        };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getMedian(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367 +\n 368  \n 369 +\n 370 +\n 371 +\n 372 +\n 373  \n 374 +\n 375  \n 376 +\n 377 +\n 378 +\n 379 +\n 380 +\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  ",
            "  private static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<NavigableMap<byte[], S>>\n      sumByRegion(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<NavigableMap<byte[], S>> future =\n        new CompletableFuture<NavigableMap<byte[], S>>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    int firstPartIndex = scan.getFamilyMap().get(scan.getFamilies()[0]).size() - 1;\n    AbstractAggregationCallback<NavigableMap<byte[], S>> callback =\n        new AbstractAggregationCallback<NavigableMap<byte[], S>>(future) {\n\n      private final NavigableMap<byte[], S> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n\n        @Override\n        protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n          if (resp.getFirstPartCount() > 0) {\n            map.put(region.getStartKey(), getPromotedValueFromProto(ci, resp, firstPartIndex));\n          }\n        }\n\n        @Override\n        protected NavigableMap<byte[], S> getFinalResult() {\n          return map;\n        }\n      };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getMedian(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "TestSecureExport::testVisibilityLabels()",
            " 342  \n 343  \n 344  \n 345  \n 346 -\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 -\n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  ",
            "  @Test\n  public void testVisibilityLabels() throws IOException, Throwable {\n    final String exportTable = name.getMethodName() + \"_export\";\n    final String importTable = name.getMethodName() + \"_import\";\n    final TableDescriptor exportHtd = TableDescriptorBuilder.newBuilder(TableName.valueOf(exportTable))\n            .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYA))\n            .setOwnerString(USER_OWNER)\n            .build();\n    SecureTestUtil.createTable(UTIL, exportHtd, new byte[][]{Bytes.toBytes(\"s\")});\n    AccessTestAction putAction = () -> {\n      Put p1 = new Put(ROW1);\n      p1.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p1.setCellVisibility(new CellVisibility(SECRET));\n      Put p2 = new Put(ROW2);\n      p2.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p2.setCellVisibility(new CellVisibility(PRIVATE + \" & \" + CONFIDENTIAL));\n      Put p3 = new Put(ROW3);\n      p3.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p3.setCellVisibility(new CellVisibility(\"!\" + CONFIDENTIAL + \" & \" + TOPSECRET));\n      try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n              Table t = conn.getTable(TableName.valueOf(exportTable))) {\n        t.put(p1);\n        t.put(p2);\n        t.put(p3);\n      }\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(putAction, getUserByLogin(USER_OWNER));\n    List<Pair<List<String>, Integer>> labelsAndRowCounts = new LinkedList<>();\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(SECRET), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(PRIVATE, CONFIDENTIAL), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET, CONFIDENTIAL), 0));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET, CONFIDENTIAL, PRIVATE, SECRET), 2));\n    for (final Pair<List<String>, Integer> labelsAndRowCount : labelsAndRowCounts) {\n      final List<String> labels = labelsAndRowCount.getFirst();\n      final int rowCount = labelsAndRowCount.getSecond();\n      //create a open permission directory.\n      final Path openDir = new Path(\"testAccessCase\");\n      final FileSystem fs = openDir.getFileSystem(UTIL.getConfiguration());\n      fs.mkdirs(openDir);\n      fs.setPermission(openDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL));\n      final Path output = fs.makeQualified(new Path(openDir, \"output\"));\n      AccessTestAction exportAction = () -> {\n        StringBuilder buf = new StringBuilder();\n        labels.forEach(v -> buf.append(v).append(\",\"));\n        buf.deleteCharAt(buf.length() - 1);\n        try {\n          String[] args = new String[]{\n            \"-D \" + ExportUtils.EXPORT_VISIBILITY_LABELS + \"=\" + buf.toString(),\n            exportTable,\n            output.toString(),};\n          Export.run(new Configuration(UTIL.getConfiguration()), args);\n          return null;\n        } catch (ServiceException | IOException ex) {\n          throw ex;\n        } catch (Throwable ex) {\n          throw new Exception(ex);\n        }\n      };\n      SecureTestUtil.verifyAllowed(exportAction, getUserByLogin(USER_OWNER));\n      final TableDescriptor importHtd = TableDescriptorBuilder.newBuilder(TableName.valueOf(importTable))\n              .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYB))\n              .setOwnerString(USER_OWNER)\n              .build();\n      SecureTestUtil.createTable(UTIL, importHtd, new byte[][]{Bytes.toBytes(\"s\")});\n      AccessTestAction importAction = () -> {\n        String[] args = new String[]{\n          \"-D\" + Import.CF_RENAME_PROP + \"=\" + FAMILYA_STRING + \":\" + FAMILYB_STRING,\n          importTable,\n          output.toString()\n        };\n        assertEquals(0, ToolRunner.run(new Configuration(UTIL.getConfiguration()), new Import(), args));\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(importAction, getUserByLogin(USER_OWNER));\n      AccessTestAction scanAction = () -> {\n        Scan scan = new Scan();\n        scan.setAuthorizations(new Authorizations(labels));\n        try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n                Table table = conn.getTable(importHtd.getTableName());\n                ResultScanner scanner = table.getScanner(scan)) {\n          int count = 0;\n          for (Result r : scanner) {\n            ++count;\n          }\n          assertEquals(rowCount, count);\n        }\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(scanAction, getUserByLogin(USER_OWNER));\n      AccessTestAction deleteAction = () -> {\n        UTIL.deleteTable(importHtd.getTableName());\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n      clearOutput(output);\n    }\n    AccessTestAction deleteAction = () -> {\n      UTIL.deleteTable(exportHtd.getTableName());\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n  }",
            " 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420 +\n 421 +\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432 +\n 433 +\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "  @Test\n  public void testVisibilityLabels() throws IOException, Throwable {\n    final String exportTable = name.getMethodName() + \"_export\";\n    final String importTable = name.getMethodName() + \"_import\";\n    final TableDescriptor exportHtd = TableDescriptorBuilder\n            .newBuilder(TableName.valueOf(exportTable))\n            .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYA))\n            .setOwnerString(USER_OWNER)\n            .build();\n    SecureTestUtil.createTable(UTIL, exportHtd, new byte[][]{Bytes.toBytes(\"s\")});\n    AccessTestAction putAction = () -> {\n      Put p1 = new Put(ROW1);\n      p1.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p1.setCellVisibility(new CellVisibility(SECRET));\n      Put p2 = new Put(ROW2);\n      p2.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p2.setCellVisibility(new CellVisibility(PRIVATE + \" & \" + CONFIDENTIAL));\n      Put p3 = new Put(ROW3);\n      p3.addColumn(FAMILYA, QUAL, NOW, QUAL);\n      p3.setCellVisibility(new CellVisibility(\"!\" + CONFIDENTIAL + \" & \" + TOPSECRET));\n      try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n              Table t = conn.getTable(TableName.valueOf(exportTable))) {\n        t.put(p1);\n        t.put(p2);\n        t.put(p3);\n      }\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(putAction, getUserByLogin(USER_OWNER));\n    List<Pair<List<String>, Integer>> labelsAndRowCounts = new LinkedList<>();\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(SECRET), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(PRIVATE, CONFIDENTIAL), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET), 1));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET, CONFIDENTIAL), 0));\n    labelsAndRowCounts.add(new Pair<>(Arrays.asList(TOPSECRET, CONFIDENTIAL, PRIVATE, SECRET), 2));\n    for (final Pair<List<String>, Integer> labelsAndRowCount : labelsAndRowCounts) {\n      final List<String> labels = labelsAndRowCount.getFirst();\n      final int rowCount = labelsAndRowCount.getSecond();\n      //create a open permission directory.\n      final Path openDir = new Path(\"testAccessCase\");\n      final FileSystem fs = openDir.getFileSystem(UTIL.getConfiguration());\n      fs.mkdirs(openDir);\n      fs.setPermission(openDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL));\n      final Path output = fs.makeQualified(new Path(openDir, \"output\"));\n      AccessTestAction exportAction = () -> {\n        StringBuilder buf = new StringBuilder();\n        labels.forEach(v -> buf.append(v).append(\",\"));\n        buf.deleteCharAt(buf.length() - 1);\n        try {\n          String[] args = new String[]{\n            \"-D \" + ExportUtils.EXPORT_VISIBILITY_LABELS + \"=\" + buf.toString(),\n            exportTable,\n            output.toString(),};\n          Export.run(new Configuration(UTIL.getConfiguration()), args);\n          return null;\n        } catch (ServiceException | IOException ex) {\n          throw ex;\n        } catch (Throwable ex) {\n          throw new Exception(ex);\n        }\n      };\n      SecureTestUtil.verifyAllowed(exportAction, getUserByLogin(USER_OWNER));\n      final TableDescriptor importHtd = TableDescriptorBuilder\n              .newBuilder(TableName.valueOf(importTable))\n              .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYB))\n              .setOwnerString(USER_OWNER)\n              .build();\n      SecureTestUtil.createTable(UTIL, importHtd, new byte[][]{Bytes.toBytes(\"s\")});\n      AccessTestAction importAction = () -> {\n        String[] args = new String[]{\n          \"-D\" + Import.CF_RENAME_PROP + \"=\" + FAMILYA_STRING + \":\" + FAMILYB_STRING,\n          importTable,\n          output.toString()\n        };\n        assertEquals(0, ToolRunner.run(\n            new Configuration(UTIL.getConfiguration()), new Import(), args));\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(importAction, getUserByLogin(USER_OWNER));\n      AccessTestAction scanAction = () -> {\n        Scan scan = new Scan();\n        scan.setAuthorizations(new Authorizations(labels));\n        try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n                Table table = conn.getTable(importHtd.getTableName());\n                ResultScanner scanner = table.getScanner(scan)) {\n          int count = 0;\n          for (Result r : scanner) {\n            ++count;\n          }\n          assertEquals(rowCount, count);\n        }\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(scanAction, getUserByLogin(USER_OWNER));\n      AccessTestAction deleteAction = () -> {\n        UTIL.deleteTable(importHtd.getTableName());\n        return null;\n      };\n      SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n      clearOutput(output);\n    }\n    AccessTestAction deleteAction = () -> {\n      UTIL.deleteTable(exportHtd.getTableName());\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n  }"
        ],
        [
            "TestServerCustomProtocol::compoundOfHelloAndPing(Table,byte,byte)",
            " 269  \n 270 -\n 271 -\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  ",
            "  private Map<byte [], String> compoundOfHelloAndPing(final Table table, final byte [] start,\n      final byte [] end)\n  throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class,\n        start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();\n            // Call ping on same instance.  Use result calling hello on same instance.\n            builder.setName(doPing(instance));\n            instance.hello(null, builder.build(), rpcCallback);\n            PingProtos.HelloResponse r = rpcCallback.get();\n            return r != null && r.hasResponse()? r.getResponse(): null;\n          }\n        });\n  }",
            " 275  \n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  ",
            "  private Map<byte [], String> compoundOfHelloAndPing(final Table table, final byte [] start,\n          final byte [] end) throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class,\n        start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();\n            // Call ping on same instance.  Use result calling hello on same instance.\n            builder.setName(doPing(instance));\n            instance.hello(null, builder.build(), rpcCallback);\n            PingProtos.HelloResponse r = rpcCallback.get();\n            return r != null && r.hasResponse()? r.getResponse(): null;\n          }\n        });\n  }"
        ],
        [
            "AggregationClient::getStdArgs(Table,ColumnInterpreter,Scan)",
            " 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 -\n 565 -\n 566  \n 567  \n 568 -\n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  ",
            "  /**\n   * It computes a global standard deviation for a given column and its value.\n   * Standard deviation is square root of (average of squares -\n   * average*average). From individual regions, it obtains sum, square sum and\n   * number of rows. With these, the above values are computed to get the global\n   * std.\n   * @param table\n   * @param scan\n   * @return standard deviations\n   * @throws Throwable\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n  Pair<List<S>, Long> getStdArgs(final Table table,\n      final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class StdCallback implements Batch.Callback<Pair<List<S>, Long>> {\n      long rowCountVal = 0l;\n      S sumVal = null, sumSqVal = null;\n\n      public synchronized Pair<List<S>, Long> getStdParams() {\n        List<S> l = new ArrayList<>(2);\n        l.add(sumVal);\n        l.add(sumSqVal);\n        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);\n        return p;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, Pair<List<S>, Long> result) {\n        if (result.getFirst().size() > 0) {\n          sumVal = ci.add(sumVal, result.getFirst().get(0));\n          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));\n          rowCountVal += result.getSecond();\n        }\n      }\n    }\n    StdCallback stdCallback = new StdCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Pair<List<S>, Long>>() {\n          @Override\n          public Pair<List<S>, Long> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getStd(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            Pair<List<S>, Long> pair = new Pair<>(new ArrayList<>(), 0L);\n            if (response.getFirstPartCount() == 0) {\n              return pair;\n            }\n            List<S> list = new ArrayList<>();\n            for (int i = 0; i < response.getFirstPartCount(); i++) {\n              ByteString b = response.getFirstPart(i);\n              T t = getParsedGenericInstance(ci.getClass(), 4, b);\n              S s = ci.getPromotedValueFromProto(t);\n              list.add(s);\n            }\n            pair.setFirst(list);\n            ByteBuffer bb = ByteBuffer.allocate(8).put(\n                getBytesFromResponse(response.getSecondPart()));\n            bb.rewind();\n            pair.setSecond(bb.getLong());\n            return pair;\n          }\n        }, stdCallback);\n    return stdCallback.getStdParams();\n  }",
            " 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571 +\n 572 +\n 573  \n 574  \n 575 +\n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  ",
            "  /**\n   * It computes a global standard deviation for a given column and its value.\n   * Standard deviation is square root of (average of squares -\n   * average*average). From individual regions, it obtains sum, square sum and\n   * number of rows. With these, the above values are computed to get the global\n   * std.\n   * @param table table to scan.\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return standard deviations\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n    Pair<List<S>, Long> getStdArgs(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci,\n          final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class StdCallback implements Batch.Callback<Pair<List<S>, Long>> {\n      long rowCountVal = 0L;\n      S sumVal = null, sumSqVal = null;\n\n      public synchronized Pair<List<S>, Long> getStdParams() {\n        List<S> l = new ArrayList<>(2);\n        l.add(sumVal);\n        l.add(sumSqVal);\n        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);\n        return p;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, Pair<List<S>, Long> result) {\n        if (result.getFirst().size() > 0) {\n          sumVal = ci.add(sumVal, result.getFirst().get(0));\n          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));\n          rowCountVal += result.getSecond();\n        }\n      }\n    }\n\n    StdCallback stdCallback = new StdCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Pair<List<S>, Long>>() {\n          @Override\n          public Pair<List<S>, Long> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getStd(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            Pair<List<S>, Long> pair = new Pair<>(new ArrayList<>(), 0L);\n            if (response.getFirstPartCount() == 0) {\n              return pair;\n            }\n            List<S> list = new ArrayList<>();\n            for (int i = 0; i < response.getFirstPartCount(); i++) {\n              ByteString b = response.getFirstPart(i);\n              T t = getParsedGenericInstance(ci.getClass(), 4, b);\n              S s = ci.getPromotedValueFromProto(t);\n              list.add(s);\n            }\n            pair.setFirst(list);\n            ByteBuffer bb = ByteBuffer.allocate(8).put(\n                getBytesFromResponse(response.getSecondPart()));\n            bb.rewind();\n            pair.setSecond(bb.getLong());\n            return pair;\n          }\n        }, stdCallback);\n    return stdCallback.getStdParams();\n  }"
        ],
        [
            "Export::SecureWriter::SecureWriter(Configuration,UserProvider,Token,List)",
            " 442 -\n 443 -\n 444  \n 445 -\n 446  ",
            "    SecureWriter(final Configuration conf, final UserProvider userProvider, final Token userToken,\n            final List<SequenceFile.Writer.Option> opts) throws IOException {\n      privilegedWriter = new PrivilegedWriter(getActiveUser(userProvider, userToken),\n        SequenceFile.createWriter(conf, opts.toArray(new SequenceFile.Writer.Option[opts.size()])));\n    }",
            " 451 +\n 452 +\n 453 +\n 454  \n 455 +\n 456 +\n 457  ",
            "    SecureWriter(final Configuration conf, final UserProvider userProvider,\n        final Token userToken, final List<SequenceFile.Writer.Option> opts)\n        throws IOException {\n      privilegedWriter = new PrivilegedWriter(getActiveUser(userProvider, userToken),\n        SequenceFile.createWriter(conf,\n            opts.toArray(new SequenceFile.Writer.Option[opts.size()])));\n    }"
        ],
        [
            "AggregationHelper::validateArgAndGetPB(Scan,ColumnInterpreter,boolean)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 -\n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  ",
            "  static <R, S, P extends Message, Q extends Message, T extends Message> AggregateRequest\n      validateArgAndGetPB(Scan scan, ColumnInterpreter<R, S, P, Q, T> ci, boolean canFamilyBeAbsent)\n          throws IOException {\n    validateParameters(scan, canFamilyBeAbsent);\n    final AggregateRequest.Builder requestBuilder = AggregateRequest.newBuilder();\n    requestBuilder.setInterpreterClassName(ci.getClass().getCanonicalName());\n    P columnInterpreterSpecificData = null;\n    if ((columnInterpreterSpecificData = ci.getRequestData()) != null) {\n      requestBuilder.setInterpreterSpecificBytes(columnInterpreterSpecificData.toByteString());\n    }\n    requestBuilder.setScan(ProtobufUtil.toScan(scan));\n    return requestBuilder.build();\n  }",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  ",
            "  static <R, S, P extends Message, Q extends Message, T extends Message> AggregateRequest\n      validateArgAndGetPB(Scan scan, ColumnInterpreter<R, S, P, Q, T> ci, boolean canFamilyBeAbsent)\n          throws IOException {\n    validateParameters(scan, canFamilyBeAbsent);\n    final AggregateRequest.Builder requestBuilder = AggregateRequest.newBuilder();\n    requestBuilder.setInterpreterClassName(ci.getClass().getCanonicalName());\n    P columnInterpreterSpecificData = ci.getRequestData();\n    if (columnInterpreterSpecificData != null) {\n      requestBuilder.setInterpreterSpecificBytes(columnInterpreterSpecificData.toByteString());\n    }\n    requestBuilder.setScan(ProtobufUtil.toScan(scan));\n    return requestBuilder.build();\n  }"
        ],
        [
            "AggregationClient::std(TableName,ColumnInterpreter,Scan)",
            " 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 -\n 637  \n 638  \n 639 -\n 640  \n 641  ",
            "  /**\n   * This is the client side interface/handle for calling the std method for a\n   * given cf-cq combination. It was necessary to add one more call stack as its\n   * return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the std and returns the double value.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return &lt;R, S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  double std(final TableName tableName, ColumnInterpreter<R, S, P, Q, T> ci,\n      Scan scan) throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n        return std(table, ci, scan);\n    }\n  }",
            " 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645 +\n 646  \n 647  \n 648 +\n 649  \n 650  ",
            "  /**\n   * This is the client side interface/handle for calling the std method for a\n   * given cf-cq combination. It was necessary to add one more call stack as its\n   * return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the std and returns the double value.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return &lt;R, S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    double std(final TableName tableName, ColumnInterpreter<R, S, P, Q, T> ci,\n      Scan scan) throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return std(table, ci, scan);\n    }\n  }"
        ],
        [
            "AggregationClient::sum(Table,ColumnInterpreter,Scan)",
            " 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 -\n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  ",
            "  /**\n   * It sums up the value returned from various regions. In case qualifier is\n   * null, summation of all the column qualifiers in the given family is done.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return sum &lt;S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  S sum(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci,\n      final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n\n    class SumCallBack implements Batch.Callback<S> {\n      S sumVal = null;\n\n      public S getSumResult() {\n        return sumVal;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, S result) {\n        sumVal = ci.add(sumVal, result);\n      }\n    }\n    SumCallBack sumCallBack = new SumCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, S>() {\n          @Override\n          public S call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            // Not sure what is going on here why I have to do these casts. TODO.\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getSum(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() == 0) {\n              return null;\n            }\n            ByteString b = response.getFirstPart(0);\n            T t = getParsedGenericInstance(ci.getClass(), 4, b);\n            S s = ci.getPromotedValueFromProto(t);\n            return s;\n          }\n        }, sumCallBack);\n    return sumCallBack.getSumResult();\n  }",
            " 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400 +\n 401 +\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  ",
            "  /**\n   * It sums up the value returned from various regions. In case qualifier is\n   * null, summation of all the column qualifiers in the given family is done.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return sum &lt;S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    S sum(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n\n    class SumCallBack implements Batch.Callback<S> {\n      S sumVal = null;\n\n      public S getSumResult() {\n        return sumVal;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, S result) {\n        sumVal = ci.add(sumVal, result);\n      }\n    }\n    SumCallBack sumCallBack = new SumCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, S>() {\n          @Override\n          public S call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            // Not sure what is going on here why I have to do these casts. TODO.\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getSum(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() == 0) {\n              return null;\n            }\n            ByteString b = response.getFirstPart(0);\n            T t = getParsedGenericInstance(ci.getClass(), 4, b);\n            S s = ci.getPromotedValueFromProto(t);\n            return s;\n          }\n        }, sumCallBack);\n    return sumCallBack.getSumResult();\n  }"
        ],
        [
            "TestSecureExport::beforeClass()",
            " 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "  /**\n   * Sets the security firstly for getting the correct default realm.\n   * @throws Exception\n   */\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    UserProvider.setUserProviderForTesting(UTIL.getConfiguration(), HadoopSecurityEnabledUserProviderForTesting.class);\n    setUpKdcServer();\n    SecureTestUtil.enableSecurity(UTIL.getConfiguration());\n    UTIL.getConfiguration().setBoolean(AccessControlConstants.EXEC_PERMISSION_CHECKS_KEY, true);\n    VisibilityTestUtil.enableVisiblityLabels(UTIL.getConfiguration());\n    SecureTestUtil.verifyConfiguration(UTIL.getConfiguration());\n    setUpClusterKdc();\n    UTIL.startMiniCluster();\n    UTIL.waitUntilAllRegionsAssigned(AccessControlLists.ACL_TABLE_NAME);\n    UTIL.waitUntilAllRegionsAssigned(VisibilityConstants.LABELS_TABLE_NAME);\n    UTIL.waitTableEnabled(AccessControlLists.ACL_TABLE_NAME, 50000);\n    UTIL.waitTableEnabled(VisibilityConstants.LABELS_TABLE_NAME, 50000);\n    SecureTestUtil.grantGlobal(UTIL, USER_ADMIN,\n            Permission.Action.ADMIN,\n            Permission.Action.CREATE,\n            Permission.Action.EXEC,\n            Permission.Action.READ,\n            Permission.Action.WRITE);\n    addLabels(UTIL.getConfiguration(), Arrays.asList(USER_OWNER),\n            Arrays.asList(PRIVATE, CONFIDENTIAL, SECRET, TOPSECRET));\n  }",
            " 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239 +\n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "  /**\n   * Sets the security firstly for getting the correct default realm.\n   */\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    UserProvider.setUserProviderForTesting(UTIL.getConfiguration(),\n        HadoopSecurityEnabledUserProviderForTesting.class);\n    setUpKdcServer();\n    SecureTestUtil.enableSecurity(UTIL.getConfiguration());\n    UTIL.getConfiguration().setBoolean(AccessControlConstants.EXEC_PERMISSION_CHECKS_KEY, true);\n    VisibilityTestUtil.enableVisiblityLabels(UTIL.getConfiguration());\n    SecureTestUtil.verifyConfiguration(UTIL.getConfiguration());\n    setUpClusterKdc();\n    UTIL.startMiniCluster();\n    UTIL.waitUntilAllRegionsAssigned(AccessControlLists.ACL_TABLE_NAME);\n    UTIL.waitUntilAllRegionsAssigned(VisibilityConstants.LABELS_TABLE_NAME);\n    UTIL.waitTableEnabled(AccessControlLists.ACL_TABLE_NAME, 50000);\n    UTIL.waitTableEnabled(VisibilityConstants.LABELS_TABLE_NAME, 50000);\n    SecureTestUtil.grantGlobal(UTIL, USER_ADMIN,\n            Permission.Action.ADMIN,\n            Permission.Action.CREATE,\n            Permission.Action.EXEC,\n            Permission.Action.READ,\n            Permission.Action.WRITE);\n    addLabels(UTIL.getConfiguration(), Arrays.asList(USER_OWNER),\n            Arrays.asList(PRIVATE, CONFIDENTIAL, SECRET, TOPSECRET));\n  }"
        ],
        [
            "TestServerCustomProtocol::PingHandler::start(CoprocessorEnvironment)",
            "  85  \n  86  \n  87 -\n  88  \n  89  ",
            "    @Override\n    public void start(CoprocessorEnvironment env) throws IOException {\n      if (env instanceof RegionCoprocessorEnvironment) return;\n      throw new CoprocessorException(\"Must be loaded on a table region!\");\n    }",
            "  84  \n  85  \n  86 +\n  87 +\n  88 +\n  89  \n  90  ",
            "    @Override\n    public void start(CoprocessorEnvironment env) throws IOException {\n      if (env instanceof RegionCoprocessorEnvironment) {\n        return;\n      }\n      throw new CoprocessorException(\"Must be loaded on a table region!\");\n    }"
        ],
        [
            "TestServerCustomProtocol::hello(Table,String)",
            " 245  \n 246 -\n 247  \n 248  ",
            "  private Map<byte [], String> hello(final Table table, final String send)\n  throws ServiceException, Throwable {\n    return hello(table, send, null, null);\n  }",
            " 250  \n 251 +\n 252  \n 253  ",
            "  private Map<byte [], String> hello(final Table table, final String send)\n          throws ServiceException, Throwable {\n    return hello(table, send, null, null);\n  }"
        ],
        [
            "AggregationClient::avg(TableName,ColumnInterpreter,Scan)",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528 -\n 529 -\n 530  \n 531  \n 532  ",
            "  /**\n   * This is the client side interface/handle for calling the average method for\n   * a given cf-cq combination. It was necessary to add one more call stack as\n   * its return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the average and returs the double value.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return &lt;R, S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  double avg(final TableName tableName,\n      final ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) throws Throwable {\n    Pair<S, Long> p = getAvgArgs(tableName, ci, scan);\n    return ci.divideForAvg(p.getFirst(), p.getSecond());\n  }",
            " 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532 +\n 533 +\n 534  \n 535  \n 536  ",
            "  /**\n   * This is the client side interface/handle for calling the average method for\n   * a given cf-cq combination. It was necessary to add one more call stack as\n   * its return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the average and returs the double value.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return &lt;R, S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    double avg(final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci,\n          Scan scan) throws Throwable {\n    Pair<S, Long> p = getAvgArgs(tableName, ci, scan);\n    return ci.divideForAvg(p.getFirst(), p.getSecond());\n  }"
        ],
        [
            "AggregationClient::max(TableName,ColumnInterpreter,Scan)",
            " 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 -\n 170 -\n 171  \n 172  \n 173  \n 174  ",
            "  /**\n   * It gives the maximum value of a column for a given column family for the\n   * given range. In case qualifier is null, a max of all values for the given\n   * family is returned.\n   * @param tableName\n   * @param ci\n   * @param scan\n   * @return max val &lt;R&gt;\n   * @throws Throwable\n   *           The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> R max(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n  throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return max(table, ci, scan);\n    }\n  }",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168  \n 169  \n 170  \n 171  ",
            "  /**\n   * It gives the maximum value of a column for a given column family for the\n   * given range. In case qualifier is null, a max of all values for the given\n   * family is returned.\n   * @param tableName the name of the table to scan\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return max val &lt;R&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> R max(\n          final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return max(table, ci, scan);\n    }\n  }"
        ],
        [
            "TestClassLoading::testClassLoadingFromHDFS()",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  ",
            "  @Test\n  // HBASE-3516: Test CP Class loading from HDFS\n  public void testClassLoadingFromHDFS() throws Exception {\n    FileSystem fs = cluster.getFileSystem();\n\n    File jarFile1 = buildCoprocessorJar(cpName1);\n    File jarFile2 = buildCoprocessorJar(cpName2);\n\n    // copy the jars into dfs\n    fs.copyFromLocalFile(new Path(jarFile1.getPath()),\n      new Path(fs.getUri().toString() + Path.SEPARATOR));\n    String jarFileOnHDFS1 = fs.getUri().toString() + Path.SEPARATOR +\n      jarFile1.getName();\n    Path pathOnHDFS1 = new Path(jarFileOnHDFS1);\n    assertTrue(\"Copy jar file to HDFS failed.\",\n      fs.exists(pathOnHDFS1));\n    LOG.info(\"Copied jar file to HDFS: \" + jarFileOnHDFS1);\n\n    fs.copyFromLocalFile(new Path(jarFile2.getPath()),\n        new Path(fs.getUri().toString() + Path.SEPARATOR));\n    String jarFileOnHDFS2 = fs.getUri().toString() + Path.SEPARATOR +\n      jarFile2.getName();\n    Path pathOnHDFS2 = new Path(jarFileOnHDFS2);\n    assertTrue(\"Copy jar file to HDFS failed.\",\n      fs.exists(pathOnHDFS2));\n    LOG.info(\"Copied jar file to HDFS: \" + jarFileOnHDFS2);\n\n    // create a table that references the coprocessors\n    HTableDescriptor htd = new HTableDescriptor(tableName);\n    htd.addFamily(new HColumnDescriptor(\"test\"));\n      // without configuration values\n    htd.setValue(\"COPROCESSOR$1\", jarFileOnHDFS1.toString() + \"|\" + cpName1 +\n      \"|\" + Coprocessor.PRIORITY_USER);\n      // with configuration values\n    htd.setValue(\"COPROCESSOR$2\", jarFileOnHDFS2.toString() + \"|\" + cpName2 +\n      \"|\" + Coprocessor.PRIORITY_USER + \"|k1=v1,k2=v2,k3=v3\");\n    Admin admin = TEST_UTIL.getAdmin();\n    if (admin.tableExists(tableName)) {\n      if (admin.isTableEnabled(tableName)) {\n        admin.disableTable(tableName);\n      }\n      admin.deleteTable(tableName);\n    }\n    CoprocessorClassLoader.clearCache();\n    byte[] startKey = {10, 63};\n    byte[] endKey = {12, 43};\n    admin.createTable(htd, startKey, endKey, 4);\n    waitForTable(htd.getTableName());\n\n    // verify that the coprocessors were loaded\n    boolean foundTableRegion=false;\n    boolean found1 = true, found2 = true, found2_k1 = true, found2_k2 = true, found2_k3 = true;\n    Map<Region, Set<ClassLoader>> regionsActiveClassLoaders = new HashMap<>();\n    MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();\n    for (HRegion region:\n        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {\n      if (region.getRegionInfo().getRegionNameAsString().startsWith(tableName.getNameAsString())) {\n        foundTableRegion = true;\n        CoprocessorEnvironment env;\n        env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName1);\n        found1 = found1 && (env != null);\n        env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName2);\n        found2 = found2 && (env != null);\n        if (env != null) {\n          Configuration conf = env.getConfiguration();\n          found2_k1 = found2_k1 && (conf.get(\"k1\") != null);\n          found2_k2 = found2_k2 && (conf.get(\"k2\") != null);\n          found2_k3 = found2_k3 && (conf.get(\"k3\") != null);\n        } else {\n          found2_k1 = found2_k2 = found2_k3 = false;\n        }\n        regionsActiveClassLoaders\n            .put(region, ((CoprocessorHost) region.getCoprocessorHost()).getExternalClassLoaders());\n      }\n    }\n\n    assertTrue(\"No region was found for table \" + tableName, foundTableRegion);\n    assertTrue(\"Class \" + cpName1 + \" was missing on a region\", found1);\n    assertTrue(\"Class \" + cpName2 + \" was missing on a region\", found2);\n    assertTrue(\"Configuration key 'k1' was missing on a region\", found2_k1);\n    assertTrue(\"Configuration key 'k2' was missing on a region\", found2_k2);\n    assertTrue(\"Configuration key 'k3' was missing on a region\", found2_k3);\n    // check if CP classloaders are cached\n    assertNotNull(jarFileOnHDFS1 + \" was not cached\",\n      CoprocessorClassLoader.getIfCached(pathOnHDFS1));\n    assertNotNull(jarFileOnHDFS2 + \" was not cached\",\n      CoprocessorClassLoader.getIfCached(pathOnHDFS2));\n    //two external jar used, should be one classloader per jar\n    assertEquals(\"The number of cached classloaders should be equal to the number\" +\n      \" of external jar files\",\n      2, CoprocessorClassLoader.getAllCached().size());\n    //check if region active classloaders are shared across all RS regions\n    Set<ClassLoader> externalClassLoaders = new HashSet<>(\n      CoprocessorClassLoader.getAllCached());\n    for (Map.Entry<Region, Set<ClassLoader>> regionCP : regionsActiveClassLoaders.entrySet()) {\n      assertTrue(\"Some CP classloaders for region \" + regionCP.getKey() + \" are not cached.\"\n        + \" ClassLoader Cache:\" + externalClassLoaders\n        + \" Region ClassLoaders:\" + regionCP.getValue(),\n        externalClassLoaders.containsAll(regionCP.getValue()));\n    }\n  }",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212 +\n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  ",
            "  @Test\n  // HBASE-3516: Test CP Class loading from HDFS\n  public void testClassLoadingFromHDFS() throws Exception {\n    FileSystem fs = cluster.getFileSystem();\n\n    File jarFile1 = buildCoprocessorJar(cpName1);\n    File jarFile2 = buildCoprocessorJar(cpName2);\n\n    // copy the jars into dfs\n    fs.copyFromLocalFile(new Path(jarFile1.getPath()),\n      new Path(fs.getUri().toString() + Path.SEPARATOR));\n    String jarFileOnHDFS1 = fs.getUri().toString() + Path.SEPARATOR +\n      jarFile1.getName();\n    Path pathOnHDFS1 = new Path(jarFileOnHDFS1);\n    assertTrue(\"Copy jar file to HDFS failed.\",\n      fs.exists(pathOnHDFS1));\n    LOG.info(\"Copied jar file to HDFS: \" + jarFileOnHDFS1);\n\n    fs.copyFromLocalFile(new Path(jarFile2.getPath()),\n        new Path(fs.getUri().toString() + Path.SEPARATOR));\n    String jarFileOnHDFS2 = fs.getUri().toString() + Path.SEPARATOR +\n      jarFile2.getName();\n    Path pathOnHDFS2 = new Path(jarFileOnHDFS2);\n    assertTrue(\"Copy jar file to HDFS failed.\",\n      fs.exists(pathOnHDFS2));\n    LOG.info(\"Copied jar file to HDFS: \" + jarFileOnHDFS2);\n\n    // create a table that references the coprocessors\n    HTableDescriptor htd = new HTableDescriptor(tableName);\n    htd.addFamily(new HColumnDescriptor(\"test\"));\n      // without configuration values\n    htd.setValue(\"COPROCESSOR$1\", jarFileOnHDFS1.toString() + \"|\" + cpName1 +\n      \"|\" + Coprocessor.PRIORITY_USER);\n      // with configuration values\n    htd.setValue(\"COPROCESSOR$2\", jarFileOnHDFS2.toString() + \"|\" + cpName2 +\n      \"|\" + Coprocessor.PRIORITY_USER + \"|k1=v1,k2=v2,k3=v3\");\n    Admin admin = TEST_UTIL.getAdmin();\n    if (admin.tableExists(tableName)) {\n      if (admin.isTableEnabled(tableName)) {\n        admin.disableTable(tableName);\n      }\n      admin.deleteTable(tableName);\n    }\n    CoprocessorClassLoader.clearCache();\n    byte[] startKey = {10, 63};\n    byte[] endKey = {12, 43};\n    admin.createTable(htd, startKey, endKey, 4);\n    waitForTable(htd.getTableName());\n\n    // verify that the coprocessors were loaded\n    boolean foundTableRegion=false;\n    boolean found1 = true, found2 = true, found2_k1 = true, found2_k2 = true, found2_k3 = true;\n    Map<Region, Set<ClassLoader>> regionsActiveClassLoaders = new HashMap<>();\n    MiniHBaseCluster hbase = TEST_UTIL.getHBaseCluster();\n    for (HRegion region:\n        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {\n      if (region.getRegionInfo().getRegionNameAsString().startsWith(tableName.getNameAsString())) {\n        foundTableRegion = true;\n        CoprocessorEnvironment env;\n        env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName1);\n        found1 = found1 && (env != null);\n        env = region.getCoprocessorHost().findCoprocessorEnvironment(cpName2);\n        found2 = found2 && (env != null);\n        if (env != null) {\n          Configuration conf = env.getConfiguration();\n          found2_k1 = found2_k1 && (conf.get(\"k1\") != null);\n          found2_k2 = found2_k2 && (conf.get(\"k2\") != null);\n          found2_k3 = found2_k3 && (conf.get(\"k3\") != null);\n        } else {\n          found2_k1 = false;\n          found2_k2 = false;\n          found2_k3 = false;\n        }\n        regionsActiveClassLoaders\n            .put(region, ((CoprocessorHost) region.getCoprocessorHost()).getExternalClassLoaders());\n      }\n    }\n\n    assertTrue(\"No region was found for table \" + tableName, foundTableRegion);\n    assertTrue(\"Class \" + cpName1 + \" was missing on a region\", found1);\n    assertTrue(\"Class \" + cpName2 + \" was missing on a region\", found2);\n    assertTrue(\"Configuration key 'k1' was missing on a region\", found2_k1);\n    assertTrue(\"Configuration key 'k2' was missing on a region\", found2_k2);\n    assertTrue(\"Configuration key 'k3' was missing on a region\", found2_k3);\n    // check if CP classloaders are cached\n    assertNotNull(jarFileOnHDFS1 + \" was not cached\",\n      CoprocessorClassLoader.getIfCached(pathOnHDFS1));\n    assertNotNull(jarFileOnHDFS2 + \" was not cached\",\n      CoprocessorClassLoader.getIfCached(pathOnHDFS2));\n    //two external jar used, should be one classloader per jar\n    assertEquals(\"The number of cached classloaders should be equal to the number\" +\n      \" of external jar files\",\n      2, CoprocessorClassLoader.getAllCached().size());\n    //check if region active classloaders are shared across all RS regions\n    Set<ClassLoader> externalClassLoaders = new HashSet<>(\n      CoprocessorClassLoader.getAllCached());\n    for (Map.Entry<Region, Set<ClassLoader>> regionCP : regionsActiveClassLoaders.entrySet()) {\n      assertTrue(\"Some CP classloaders for region \" + regionCP.getKey() + \" are not cached.\"\n        + \" ClassLoader Cache:\" + externalClassLoaders\n        + \" Region ClassLoaders:\" + regionCP.getValue(),\n        externalClassLoaders.containsAll(regionCP.getValue()));\n    }\n  }"
        ],
        [
            "AggregationClient::getAvgArgs(Table,ColumnInterpreter,Scan)",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467 -\n 468 -\n 469  \n 470  \n 471  \n 472 -\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  ",
            "  /**\n   * It computes average while fetching sum and row count from all the\n   * corresponding regions. Approach is to compute a global sum of region level\n   * sum and rowcount and then compute the average.\n   * @param table\n   * @param scan\n   * @throws Throwable\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n  Pair<S, Long> getAvgArgs(final Table table,\n      final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class AvgCallBack implements Batch.Callback<Pair<S, Long>> {\n      S sum = null;\n      Long rowCount = 0l;\n\n      public synchronized Pair<S, Long> getAvgArgs() {\n        return new Pair<>(sum, rowCount);\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, Pair<S, Long> result) {\n        sum = ci.add(sum, result.getFirst());\n        rowCount += result.getSecond();\n      }\n    }\n    AvgCallBack avgCallBack = new AvgCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Pair<S, Long>>() {\n          @Override\n          public Pair<S, Long> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getAvg(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            Pair<S, Long> pair = new Pair<>(null, 0L);\n            if (response.getFirstPartCount() == 0) {\n              return pair;\n            }\n            ByteString b = response.getFirstPart(0);\n            T t = getParsedGenericInstance(ci.getClass(), 4, b);\n            S s = ci.getPromotedValueFromProto(t);\n            pair.setFirst(s);\n            ByteBuffer bb = ByteBuffer.allocate(8).put(\n                getBytesFromResponse(response.getSecondPart()));\n            bb.rewind();\n            pair.setSecond(bb.getLong());\n            return pair;\n          }\n        }, avgCallBack);\n    return avgCallBack.getAvgArgs();\n  }",
            " 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 +\n 470 +\n 471  \n 472  \n 473  \n 474 +\n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  ",
            "  /**\n   * It computes average while fetching sum and row count from all the\n   * corresponding regions. Approach is to compute a global sum of region level\n   * sum and rowcount and then compute the average.\n   * @param table table to scan.\n   * @param scan the HBase scan object to use to read data from HBase\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message>\n    Pair<S, Long> getAvgArgs(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci,\n          final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class AvgCallBack implements Batch.Callback<Pair<S, Long>> {\n      S sum = null;\n      Long rowCount = 0L;\n\n      public synchronized Pair<S, Long> getAvgArgs() {\n        return new Pair<>(sum, rowCount);\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, Pair<S, Long> result) {\n        sum = ci.add(sum, result.getFirst());\n        rowCount += result.getSecond();\n      }\n    }\n\n    AvgCallBack avgCallBack = new AvgCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Pair<S, Long>>() {\n          @Override\n          public Pair<S, Long> call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getAvg(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            Pair<S, Long> pair = new Pair<>(null, 0L);\n            if (response.getFirstPartCount() == 0) {\n              return pair;\n            }\n            ByteString b = response.getFirstPart(0);\n            T t = getParsedGenericInstance(ci.getClass(), 4, b);\n            S s = ci.getPromotedValueFromProto(t);\n            pair.setFirst(s);\n            ByteBuffer bb = ByteBuffer.allocate(8).put(\n                getBytesFromResponse(response.getSecondPart()));\n            bb.rewind();\n            pair.setSecond(bb.getLong());\n            return pair;\n          }\n        }, avgCallBack);\n    return avgCallBack.getAvgArgs();\n  }"
        ],
        [
            "AggregateImplementation::getAvg(RpcController,AggregateRequest,RpcCallback)",
            " 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 -\n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "  /**\n   * Gives a Pair with first object as Sum and second object as row count,\n   * computed for a given combination of column qualifier and column family in\n   * the given row range as defined in the Scan object. In its current\n   * implementation, it takes one column family and one column qualifier (if\n   * provided). In case of null column qualifier, an aggregate sum over all the\n   * entire column family will be returned.\n   * <p>\n   * The average is computed in\n   * AggregationClient#avg(byte[], ColumnInterpreter, Scan) by\n   * processing results from all regions, so its \"ok\" to pass sum and a Long\n   * type.\n   */\n  @Override\n  public void getAvg(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null;\n      Long rowCountVal = 0l;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n      boolean hasMoreRows = false;\n\n      do {\n        results.clear();\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          sumVal = ci.add(sumVal, ci.castToReturnType(ci.getValue(colFamily,\n              qualifier, results.get(i))));\n        }\n        rowCountVal++;\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        ByteString first = ci.getProtoForPromotedType(sumVal).toByteString();\n        AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n        pair.addFirstPart(first);\n        ByteBuffer bb = ByteBuffer.allocate(8).putLong(rowCountVal);\n        bb.rewind();\n        pair.setSecondPart(ByteString.copyFrom(bb));\n        response = pair.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "  /**\n   * Gives a Pair with first object as Sum and second object as row count,\n   * computed for a given combination of column qualifier and column family in\n   * the given row range as defined in the Scan object. In its current\n   * implementation, it takes one column family and one column qualifier (if\n   * provided). In case of null column qualifier, an aggregate sum over all the\n   * entire column family will be returned.\n   * <p>\n   * The average is computed in\n   * AggregationClient#avg(byte[], ColumnInterpreter, Scan) by\n   * processing results from all regions, so its \"ok\" to pass sum and a Long\n   * type.\n   */\n  @Override\n  public void getAvg(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null;\n      Long rowCountVal = 0L;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n      boolean hasMoreRows = false;\n\n      do {\n        results.clear();\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          sumVal = ci.add(sumVal, ci.castToReturnType(ci.getValue(colFamily,\n              qualifier, results.get(i))));\n        }\n        rowCountVal++;\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        ByteString first = ci.getProtoForPromotedType(sumVal).toByteString();\n        AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n        pair.addFirstPart(first);\n        ByteBuffer bb = ByteBuffer.allocate(8).putLong(rowCountVal);\n        bb.rewind();\n        pair.setSecondPart(ByteString.copyFrom(bb));\n        response = pair.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }"
        ],
        [
            "TestRowProcessorEndpoint::RowProcessorEndpoint::doScan(HRegion,Scan,List)",
            " 646 -\n 647 -\n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655 -\n 656  \n 657  ",
            "    public static void doScan(\n        HRegion region, Scan scan, List<Cell> result) throws IOException {\n      InternalScanner scanner = null;\n      try {\n        scan.setIsolationLevel(IsolationLevel.READ_UNCOMMITTED);\n        scanner = region.getScanner(scan);\n        result.clear();\n        scanner.next(result);\n      } finally {\n        if (scanner != null) scanner.close();\n      }\n    }",
            " 643 +\n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 +\n 652 +\n 653 +\n 654  \n 655  ",
            "    public static void doScan(HRegion region, Scan scan, List<Cell> result) throws IOException {\n      InternalScanner scanner = null;\n      try {\n        scan.setIsolationLevel(IsolationLevel.READ_UNCOMMITTED);\n        scanner = region.getScanner(scan);\n        result.clear();\n        scanner.next(result);\n      } finally {\n        if (scanner != null) {\n          scanner.close();\n        }\n      }\n    }"
        ],
        [
            "ProtobufCoprocessorService::echo(RpcController,TestProtos,RpcCallback)",
            "  58  \n  59  \n  60 -\n  61  \n  62  \n  63  ",
            "  @Override\n  public void echo(RpcController controller, TestProtos.EchoRequestProto request,\n      RpcCallback<TestProtos.EchoResponseProto> done) {\n    String message = request.getMessage();\n    done.run(TestProtos.EchoResponseProto.newBuilder().setMessage(message).build());\n  }",
            "  58  \n  59  \n  60 +\n  61  \n  62  \n  63  ",
            "  @Override\n  public void echo(RpcController controller, TestProtos.EchoRequestProto request,\n          RpcCallback<TestProtos.EchoResponseProto> done) {\n    String message = request.getMessage();\n    done.run(TestProtos.EchoResponseProto.newBuilder().setMessage(message).build());\n  }"
        ],
        [
            "AsyncAggregationClient::std(AsyncTable,ColumnInterpreter,Scan)",
            " 308  \n 309 -\n 310 -\n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Double>\n      std(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<Double> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Double> callback = new AbstractAggregationCallback<Double>(future) {\n\n      private S sum;\n\n      private S sumSq;\n\n      private long count;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          sum = ci.add(sum, getPromotedValueFromProto(ci, resp, 0));\n          sumSq = ci.add(sumSq, getPromotedValueFromProto(ci, resp, 1));\n          count += resp.getSecondPart().asReadOnlyByteBuffer().getLong();\n        }\n      }\n\n      @Override\n      protected Double getFinalResult() {\n        double avg = ci.divideForAvg(sum, count);\n        double avgSq = ci.divideForAvg(sumSq, count);\n        return Math.sqrt(avgSq - avg * avg);\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getStd(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 307  \n 308 +\n 309 +\n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Double> std(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci,\n          Scan scan) {\n    CompletableFuture<Double> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Double> callback = new AbstractAggregationCallback<Double>(future) {\n\n      private S sum;\n\n      private S sumSq;\n\n      private long count;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          sum = ci.add(sum, getPromotedValueFromProto(ci, resp, 0));\n          sumSq = ci.add(sumSq, getPromotedValueFromProto(ci, resp, 1));\n          count += resp.getSecondPart().asReadOnlyByteBuffer().getLong();\n        }\n      }\n\n      @Override\n      protected Double getFinalResult() {\n        double avg = ci.divideForAvg(sum, count);\n        double avgSq = ci.divideForAvg(sumSq, count);\n        return Math.sqrt(avgSq - avg * avg);\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getStd(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "TestHRegionServerBulkLoadWithOldSecureEndpoint::AtomicHFileLoader::doAnAction()",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118 -\n 119 -\n 120 -\n 121 -\n 122 -\n 123 -\n 124 -\n 125 -\n 126 -\n 127 -\n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  ",
            "    public void doAnAction() throws Exception {\n      long iteration = numBulkLoads.getAndIncrement();\n      Path dir =  UTIL.getDataTestDirOnTestFS(String.format(\"bulkLoad_%08d\",\n          iteration));\n\n      // create HFiles for different column families\n      FileSystem fs = UTIL.getTestFileSystem();\n      byte[] val = Bytes.toBytes(String.format(\"%010d\", iteration));\n      final List<Pair<byte[], String>> famPaths = new ArrayList<>(NUM_CFS);\n      for (int i = 0; i < NUM_CFS; i++) {\n        Path hfile = new Path(dir, family(i));\n        byte[] fam = Bytes.toBytes(family(i));\n        createHFile(fs, hfile, fam, QUAL, val, 1000);\n        famPaths.add(new Pair<>(fam, hfile.toString()));\n      }\n\n      // bulk load HFiles\n      final ClusterConnection conn = (ClusterConnection) UTIL.getAdmin().getConnection();\n      Table table = conn.getTable(tableName);\n      final String bulkToken = new SecureBulkLoadEndpointClient(table).prepareBulkLoad(tableName);\n      RpcControllerFactory rpcControllerFactory = new RpcControllerFactory(UTIL.getConfiguration());\n      ClientServiceCallable<Void> callable =\n          new ClientServiceCallable<Void>(conn, tableName, Bytes.toBytes(\"aaa\"),\n              rpcControllerFactory.newController(), HConstants.PRIORITY_UNSET) {\n            @Override\n            protected Void rpcCall() throws Exception {\n              LOG.debug(\"Going to connect to server \" + getLocation() + \" for row \" +\n                  Bytes.toStringBinary(getRow()));\n              try (Table table = conn.getTable(getTableName())) {\n                boolean loaded = new SecureBulkLoadEndpointClient(table).bulkLoadHFiles(famPaths,\n                    null, bulkToken, getLocation().getRegionInfo().getStartKey());\n              }\n              return null;\n            }\n          };\n      RpcRetryingCallerFactory factory = new RpcRetryingCallerFactory(conf);\n      RpcRetryingCaller<Void> caller = factory.<Void> newCaller();\n      caller.callWithRetries(callable, Integer.MAX_VALUE);\n\n      // Periodically do compaction to reduce the number of open file handles.\n      if (numBulkLoads.get() % 5 == 0) {\n        // 5 * 50 = 250 open file handles!\n        callable = new ClientServiceCallable<Void>(conn, tableName, Bytes.toBytes(\"aaa\"),\n            rpcControllerFactory.newController(), HConstants.PRIORITY_UNSET) {\n          @Override\n          protected Void rpcCall() throws Exception {\n            LOG.debug(\"compacting \" + getLocation() + \" for row \"\n                + Bytes.toStringBinary(getRow()));\n            AdminProtos.AdminService.BlockingInterface server =\n              conn.getAdmin(getLocation().getServerName());\n            CompactRegionRequest request =\n              RequestConverter.buildCompactRegionRequest(\n                getLocation().getRegionInfo().getRegionName(), true, null);\n            server.compactRegion(null, request);\n            numCompactions.incrementAndGet();\n            return null;\n          }\n        };\n        caller.callWithRetries(callable, Integer.MAX_VALUE);\n      }\n    }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125  \n 126 +\n 127 +\n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "    public void doAnAction() throws Exception {\n      long iteration = numBulkLoads.getAndIncrement();\n      Path dir =  UTIL.getDataTestDirOnTestFS(String.format(\"bulkLoad_%08d\",\n          iteration));\n\n      // create HFiles for different column families\n      FileSystem fs = UTIL.getTestFileSystem();\n      byte[] val = Bytes.toBytes(String.format(\"%010d\", iteration));\n      final List<Pair<byte[], String>> famPaths = new ArrayList<>(NUM_CFS);\n      for (int i = 0; i < NUM_CFS; i++) {\n        Path hfile = new Path(dir, family(i));\n        byte[] fam = Bytes.toBytes(family(i));\n        createHFile(fs, hfile, fam, QUAL, val, 1000);\n        famPaths.add(new Pair<>(fam, hfile.toString()));\n      }\n\n      // bulk load HFiles\n      final ClusterConnection conn = (ClusterConnection) UTIL.getAdmin().getConnection();\n      Table table = conn.getTable(tableName);\n      final String bulkToken = new SecureBulkLoadEndpointClient(table).prepareBulkLoad(tableName);\n      RpcControllerFactory rpcControllerFactory = new RpcControllerFactory(UTIL.getConfiguration());\n      ClientServiceCallable<Void> callable =\n        new ClientServiceCallable<Void>(conn, tableName, Bytes.toBytes(\"aaa\"),\n            rpcControllerFactory.newController(), HConstants.PRIORITY_UNSET) {\n          @Override\n          protected Void rpcCall() throws Exception {\n            LOG.debug(\"Going to connect to server \" + getLocation() + \" for row \" +\n                Bytes.toStringBinary(getRow()));\n            try (Table table = conn.getTable(getTableName())) {\n              boolean loaded = new SecureBulkLoadEndpointClient(table).bulkLoadHFiles(famPaths,\n                  null, bulkToken, getLocation().getRegionInfo().getStartKey());\n            }\n            return null;\n          }\n        };\n      RpcRetryingCallerFactory factory = new RpcRetryingCallerFactory(conf);\n      RpcRetryingCaller<Void> caller = factory.<Void> newCaller();\n      caller.callWithRetries(callable, Integer.MAX_VALUE);\n\n      // Periodically do compaction to reduce the number of open file handles.\n      if (numBulkLoads.get() % 5 == 0) {\n        // 5 * 50 = 250 open file handles!\n        callable = new ClientServiceCallable<Void>(conn, tableName, Bytes.toBytes(\"aaa\"),\n            rpcControllerFactory.newController(), HConstants.PRIORITY_UNSET) {\n          @Override\n          protected Void rpcCall() throws Exception {\n            LOG.debug(\"compacting \" + getLocation() + \" for row \"\n                + Bytes.toStringBinary(getRow()));\n            AdminProtos.AdminService.BlockingInterface server =\n              conn.getAdmin(getLocation().getServerName());\n            CompactRegionRequest request =\n              RequestConverter.buildCompactRegionRequest(\n                getLocation().getRegionInfo().getRegionName(), true, null);\n            server.compactRegion(null, request);\n            numCompactions.incrementAndGet();\n            return null;\n          }\n        };\n        caller.callWithRetries(callable, Integer.MAX_VALUE);\n      }\n    }"
        ],
        [
            "AggregationClient::avg(Table,ColumnInterpreter,Scan)",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547 -\n 548  \n 549  \n 550  ",
            "  /**\n   * This is the client side interface/handle for calling the average method for\n   * a given cf-cq combination. It was necessary to add one more call stack as\n   * its return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the average and returs the double value.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return &lt;R, S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> double avg(\n      final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) throws Throwable {\n    Pair<S, Long> p = getAvgArgs(table, ci, scan);\n    return ci.divideForAvg(p.getFirst(), p.getSecond());\n  }",
            " 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 +\n 553 +\n 554  \n 555  \n 556  ",
            "  /**\n   * This is the client side interface/handle for calling the average method for\n   * a given cf-cq combination. It was necessary to add one more call stack as\n   * its return type should be a decimal value, irrespective of what\n   * columninterpreter says. So, this methods collects the necessary parameters\n   * to compute the average and returs the double value.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return &lt;R, S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message> double avg(\n          final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, Scan scan)\n          throws Throwable {\n    Pair<S, Long> p = getAvgArgs(table, ci, scan);\n    return ci.divideForAvg(p.getFirst(), p.getSecond());\n  }"
        ],
        [
            "ColumnAggregationEndpointWithErrors::sum(RpcController,ColumnAggregationWithErrorsSumRequest,RpcCallback)",
            "  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "  @Override\n  public void sum(RpcController controller, ColumnAggregationWithErrorsSumRequest request,\n      RpcCallback<ColumnAggregationWithErrorsSumResponse> done) {\n    // aggregate at each region\n    Scan scan = new Scan();\n    // Family is required in pb. Qualifier is not.\n    byte[] family = request.getFamily().toByteArray();\n    byte[] qualifier = request.hasQualifier() ? request.getQualifier().toByteArray() : null;\n    if (request.hasQualifier()) {\n      scan.addColumn(family, qualifier);\n    } else {\n      scan.addFamily(family);\n    }\n    int sumResult = 0;\n    InternalScanner scanner = null;\n    try {\n      Region region = this.env.getRegion();\n      // throw an exception for requests to the last region in the table, to test error handling\n      if (Bytes.equals(region.getRegionInfo().getEndKey(), HConstants.EMPTY_END_ROW)) {\n        throw new DoNotRetryIOException(\"An expected exception\");\n      }\n      scanner = region.getScanner(scan);\n      List<Cell> curVals = new ArrayList<>();\n      boolean hasMore = false;\n      do {\n        curVals.clear();\n        hasMore = scanner.next(curVals);\n        for (Cell kv : curVals) {\n          if (CellUtil.matchingQualifier(kv, qualifier)) {\n            sumResult += Bytes.toInt(kv.getValueArray(), kv.getValueOffset());\n          }\n        }\n      } while (hasMore);\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n      // Set result to -1 to indicate error.\n      sumResult = -1;\n      LOG.info(\"Setting sum result to -1 to indicate error\", e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException e) {\n          CoprocessorRpcUtils.setControllerException(controller, e);\n          sumResult = -1;\n          LOG.info(\"Setting sum result to -1 to indicate error\", e);\n        }\n      }\n    }\n    done.run(ColumnAggregationWithErrorsSumResponse.newBuilder().setSum(sumResult).build());\n  }",
            "  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "  @Override\n  public void sum(RpcController controller, ColumnAggregationWithErrorsSumRequest request,\n          RpcCallback<ColumnAggregationWithErrorsSumResponse> done) {\n    // aggregate at each region\n    Scan scan = new Scan();\n    // Family is required in pb. Qualifier is not.\n    byte[] family = request.getFamily().toByteArray();\n    byte[] qualifier = request.hasQualifier() ? request.getQualifier().toByteArray() : null;\n    if (request.hasQualifier()) {\n      scan.addColumn(family, qualifier);\n    } else {\n      scan.addFamily(family);\n    }\n    int sumResult = 0;\n    InternalScanner scanner = null;\n    try {\n      Region region = this.env.getRegion();\n      // throw an exception for requests to the last region in the table, to test error handling\n      if (Bytes.equals(region.getRegionInfo().getEndKey(), HConstants.EMPTY_END_ROW)) {\n        throw new DoNotRetryIOException(\"An expected exception\");\n      }\n      scanner = region.getScanner(scan);\n      List<Cell> curVals = new ArrayList<>();\n      boolean hasMore = false;\n      do {\n        curVals.clear();\n        hasMore = scanner.next(curVals);\n        for (Cell kv : curVals) {\n          if (CellUtil.matchingQualifier(kv, qualifier)) {\n            sumResult += Bytes.toInt(kv.getValueArray(), kv.getValueOffset());\n          }\n        }\n      } while (hasMore);\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n      // Set result to -1 to indicate error.\n      sumResult = -1;\n      LOG.info(\"Setting sum result to -1 to indicate error\", e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException e) {\n          CoprocessorRpcUtils.setControllerException(controller, e);\n          sumResult = -1;\n          LOG.info(\"Setting sum result to -1 to indicate error\", e);\n        }\n      }\n    }\n    done.run(ColumnAggregationWithErrorsSumResponse.newBuilder().setSum(sumResult).build());\n  }"
        ],
        [
            "AsyncAggregationClient::findMedian(CompletableFuture,AsyncTable,ColumnInterpreter,Scan,NavigableMap)",
            " 390  \n 391 -\n 392 -\n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  ",
            "  private static <R, S, P extends Message, Q extends Message, T extends Message> void findMedian(\n      CompletableFuture<R> future, AsyncTable<AdvancedScanResultConsumer> table,\n      ColumnInterpreter<R, S, P, Q, T> ci, Scan scan, NavigableMap<byte[], S> sumByRegion) {\n    double halfSum = ci.divideForAvg(sumByRegion.values().stream().reduce(ci::add).get(), 2L);\n    S movingSum = null;\n    byte[] startRow = null;\n    for (Map.Entry<byte[], S> entry : sumByRegion.entrySet()) {\n      startRow = entry.getKey();\n      S newMovingSum = ci.add(movingSum, entry.getValue());\n      if (ci.divideForAvg(newMovingSum, 1L) > halfSum) {\n        break;\n      }\n      movingSum = newMovingSum;\n    }\n    if (startRow != null) {\n      scan.withStartRow(startRow);\n    }\n    // we can not pass movingSum directly to an anonymous class as it is not final.\n    S baseSum = movingSum;\n    byte[] family = scan.getFamilies()[0];\n    NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(family);\n    byte[] weightQualifier = qualifiers.last();\n    byte[] valueQualifier = qualifiers.first();\n    table.scan(scan, new AdvancedScanResultConsumer() {\n\n      private S sum = baseSum;\n\n      private R value = null;\n\n      @Override\n      public void onNext(Result[] results, ScanController controller) {\n        try {\n          for (Result result : results) {\n            Cell weightCell = result.getColumnLatestCell(family, weightQualifier);\n            R weight = ci.getValue(family, weightQualifier, weightCell);\n            sum = ci.add(sum, ci.castToReturnType(weight));\n            if (ci.divideForAvg(sum, 1L) > halfSum) {\n              if (value != null) {\n                future.complete(value);\n              } else {\n                future.completeExceptionally(new NoSuchElementException());\n              }\n              controller.terminate();\n              return;\n            }\n            Cell valueCell = result.getColumnLatestCell(family, valueQualifier);\n            value = ci.getValue(family, valueQualifier, valueCell);\n          }\n        } catch (IOException e) {\n          future.completeExceptionally(e);\n          controller.terminate();\n        }\n      }\n\n      @Override\n      public void onError(Throwable error) {\n        future.completeExceptionally(error);\n      }\n\n      @Override\n      public void onComplete() {\n        if (!future.isDone()) {\n          // we should not reach here as the future should be completed in onNext.\n          future.completeExceptionally(new NoSuchElementException());\n        }\n      }\n    });\n  }",
            " 389  \n 390 +\n 391 +\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  ",
            "  private static <R, S, P extends Message, Q extends Message, T extends Message> void findMedian(\n          CompletableFuture<R> future, AsyncTable<AdvancedScanResultConsumer> table,\n          ColumnInterpreter<R, S, P, Q, T> ci, Scan scan, NavigableMap<byte[], S> sumByRegion) {\n    double halfSum = ci.divideForAvg(sumByRegion.values().stream().reduce(ci::add).get(), 2L);\n    S movingSum = null;\n    byte[] startRow = null;\n    for (Map.Entry<byte[], S> entry : sumByRegion.entrySet()) {\n      startRow = entry.getKey();\n      S newMovingSum = ci.add(movingSum, entry.getValue());\n      if (ci.divideForAvg(newMovingSum, 1L) > halfSum) {\n        break;\n      }\n      movingSum = newMovingSum;\n    }\n    if (startRow != null) {\n      scan.withStartRow(startRow);\n    }\n    // we can not pass movingSum directly to an anonymous class as it is not final.\n    S baseSum = movingSum;\n    byte[] family = scan.getFamilies()[0];\n    NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(family);\n    byte[] weightQualifier = qualifiers.last();\n    byte[] valueQualifier = qualifiers.first();\n    table.scan(scan, new AdvancedScanResultConsumer() {\n      private S sum = baseSum;\n\n      private R value = null;\n\n      @Override\n      public void onNext(Result[] results, ScanController controller) {\n        try {\n          for (Result result : results) {\n            Cell weightCell = result.getColumnLatestCell(family, weightQualifier);\n            R weight = ci.getValue(family, weightQualifier, weightCell);\n            sum = ci.add(sum, ci.castToReturnType(weight));\n            if (ci.divideForAvg(sum, 1L) > halfSum) {\n              if (value != null) {\n                future.complete(value);\n              } else {\n                future.completeExceptionally(new NoSuchElementException());\n              }\n              controller.terminate();\n              return;\n            }\n            Cell valueCell = result.getColumnLatestCell(family, valueQualifier);\n            value = ci.getValue(family, valueQualifier, valueCell);\n          }\n        } catch (IOException e) {\n          future.completeExceptionally(e);\n          controller.terminate();\n        }\n      }\n\n      @Override\n      public void onError(Throwable error) {\n        future.completeExceptionally(error);\n      }\n\n      @Override\n      public void onComplete() {\n        if (!future.isDone()) {\n          // we should not reach here as the future should be completed in onNext.\n          future.completeExceptionally(new NoSuchElementException());\n        }\n      }\n    });\n  }"
        ],
        [
            "AggregationClient::max(Table,ColumnInterpreter,Scan)",
            " 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190 -\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "  /**\n   * It gives the maximum value of a column for a given column family for the\n   * given range. In case qualifier is null, a max of all values for the given\n   * family is returned.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return max val &lt;&gt;\n   * @throws Throwable\n   *           The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  R max(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci,\n      final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class MaxCallBack implements Batch.Callback<R> {\n      R max = null;\n\n      R getMax() {\n        return max;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, R result) {\n        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;\n      }\n    }\n    MaxCallBack aMaxCallBack = new MaxCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, R>() {\n          @Override\n          public R call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMax(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() > 0) {\n              ByteString b = response.getFirstPart(0);\n              Q q = getParsedGenericInstance(ci.getClass(), 3, b);\n              return ci.getCellValueFromProto(q);\n            }\n            return null;\n          }\n        }, aMaxCallBack);\n    return aMaxCallBack.getMax();\n  }",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186 +\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "  /**\n   * It gives the maximum value of a column for a given column family for the\n   * given range. In case qualifier is null, a max of all values for the given\n   * family is returned.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return max val &lt;&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    R max(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class MaxCallBack implements Batch.Callback<R> {\n      R max = null;\n\n      R getMax() {\n        return max;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, R result) {\n        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;\n      }\n    }\n    MaxCallBack aMaxCallBack = new MaxCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, R>() {\n          @Override\n          public R call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMax(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() > 0) {\n              ByteString b = response.getFirstPart(0);\n              Q q = getParsedGenericInstance(ci.getClass(), 3, b);\n              return ci.getCellValueFromProto(q);\n            }\n            return null;\n          }\n        }, aMaxCallBack);\n    return aMaxCallBack.getMax();\n  }"
        ],
        [
            "TestSecureExport::addLabels(Configuration,List,List)",
            " 186 -\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "  private static void addLabels(final Configuration conf, final List<String> users, final List<String> labels) throws Exception {\n    PrivilegedExceptionAction<VisibilityLabelsProtos.VisibilityLabelsResponse> action\n      = () -> {\n        try (Connection conn = ConnectionFactory.createConnection(conf)) {\n          VisibilityClient.addLabels(conn, labels.toArray(new String[labels.size()]));\n          for (String user : users) {\n            VisibilityClient.setAuths(conn, labels.toArray(new String[labels.size()]), user);\n          }\n        } catch (Throwable t) {\n          throw new IOException(t);\n        }\n        return null;\n      };\n    getUserByLogin(USER_ADMIN).runAs(action);\n  }",
            " 200 +\n 201 +\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  ",
            "  private static void addLabels(final Configuration conf, final List<String> users,\n      final List<String> labels) throws Exception {\n    PrivilegedExceptionAction<VisibilityLabelsProtos.VisibilityLabelsResponse> action\n      = () -> {\n        try (Connection conn = ConnectionFactory.createConnection(conf)) {\n          VisibilityClient.addLabels(conn, labels.toArray(new String[labels.size()]));\n          for (String user : users) {\n            VisibilityClient.setAuths(conn, labels.toArray(new String[labels.size()]), user);\n          }\n        } catch (Throwable t) {\n          throw new IOException(t);\n        }\n        return null;\n      };\n    getUserByLogin(USER_ADMIN).runAs(action);\n  }"
        ],
        [
            "TestSecureExport::testAccessCase()",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  ",
            "  /**\n   * Test the ExportEndpoint's access levels. The {@link Export} test is ignored\n   * since the access exceptions cannot be collected from the mappers.\n   *\n   * @throws java.io.IOException\n   */\n  @Test\n  public void testAccessCase() throws IOException, Throwable {\n    final String exportTable = name.getMethodName();\n    TableDescriptor exportHtd = TableDescriptorBuilder\n            .newBuilder(TableName.valueOf(name.getMethodName()))\n            .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYA))\n            .setOwnerString(USER_OWNER)\n            .build();\n    SecureTestUtil.createTable(UTIL, exportHtd, new byte[][]{Bytes.toBytes(\"s\")});\n    SecureTestUtil.grantOnTable(UTIL, USER_RO,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.READ);\n    SecureTestUtil.grantOnTable(UTIL, USER_RX,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.READ,\n            Permission.Action.EXEC);\n    SecureTestUtil.grantOnTable(UTIL, USER_XO,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.EXEC);\n    assertEquals(4, AccessControlLists.getTablePermissions(UTIL.getConfiguration(),\n            TableName.valueOf(exportTable)).size());\n    AccessTestAction putAction = () -> {\n      Put p = new Put(ROW1);\n      p.addColumn(FAMILYA, Bytes.toBytes(\"qual_0\"), NOW, QUAL);\n      p.addColumn(FAMILYA, Bytes.toBytes(\"qual_1\"), NOW, QUAL);\n      try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n              Table t = conn.getTable(TableName.valueOf(exportTable))) {\n        t.put(p);\n      }\n      return null;\n    };\n    // no hdfs access.\n    SecureTestUtil.verifyAllowed(putAction,\n      getUserByLogin(USER_ADMIN),\n      getUserByLogin(USER_OWNER));\n    SecureTestUtil.verifyDenied(putAction,\n      getUserByLogin(USER_RO),\n      getUserByLogin(USER_XO),\n      getUserByLogin(USER_RX),\n      getUserByLogin(USER_NONE));\n\n    final FileSystem fs = UTIL.getDFSCluster().getFileSystem();\n    final Path openDir = fs.makeQualified(new Path(\"testAccessCase\"));\n    fs.mkdirs(openDir);\n    fs.setPermission(openDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL));\n    final Path output = fs.makeQualified(new Path(openDir, \"output\"));\n    AccessTestAction exportAction = () -> {\n      try {\n        String[] args = new String[]{exportTable, output.toString()};\n        Map<byte[], Export.Response> result\n                = Export.run(new Configuration(UTIL.getConfiguration()), args);\n        long rowCount = 0;\n        long cellCount = 0;\n        for (Export.Response r : result.values()) {\n          rowCount += r.getRowCount();\n          cellCount += r.getCellCount();\n        }\n        assertEquals(1, rowCount);\n        assertEquals(2, cellCount);\n        return null;\n      } catch (ServiceException | IOException ex) {\n        throw ex;\n      } catch (Throwable ex) {\n        LOG.error(ex.toString(), ex);\n        throw new Exception(ex);\n      } finally {\n        clearOutput(output);\n      }\n    };\n    SecureTestUtil.verifyDenied(exportAction,\n      getUserByLogin(USER_RO),\n      getUserByLogin(USER_XO),\n      getUserByLogin(USER_NONE));\n    SecureTestUtil.verifyAllowed(exportAction,\n      getUserByLogin(USER_ADMIN),\n      getUserByLogin(USER_OWNER),\n      getUserByLogin(USER_RX));\n    AccessTestAction deleteAction = () -> {\n      UTIL.deleteTable(TableName.valueOf(exportTable));\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n    fs.delete(openDir, true);\n  }",
            " 269  \n 270  \n 271  \n 272  \n 273  \n 274 +\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "  /**\n   * Test the ExportEndpoint's access levels. The {@link Export} test is ignored\n   * since the access exceptions cannot be collected from the mappers.\n   */\n  @Test\n  public void testAccessCase() throws Throwable {\n    final String exportTable = name.getMethodName();\n    TableDescriptor exportHtd = TableDescriptorBuilder\n            .newBuilder(TableName.valueOf(name.getMethodName()))\n            .setColumnFamily(ColumnFamilyDescriptorBuilder.of(FAMILYA))\n            .setOwnerString(USER_OWNER)\n            .build();\n    SecureTestUtil.createTable(UTIL, exportHtd, new byte[][]{Bytes.toBytes(\"s\")});\n    SecureTestUtil.grantOnTable(UTIL, USER_RO,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.READ);\n    SecureTestUtil.grantOnTable(UTIL, USER_RX,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.READ,\n            Permission.Action.EXEC);\n    SecureTestUtil.grantOnTable(UTIL, USER_XO,\n            TableName.valueOf(exportTable), null, null,\n            Permission.Action.EXEC);\n    assertEquals(4, AccessControlLists.getTablePermissions(UTIL.getConfiguration(),\n            TableName.valueOf(exportTable)).size());\n    AccessTestAction putAction = () -> {\n      Put p = new Put(ROW1);\n      p.addColumn(FAMILYA, Bytes.toBytes(\"qual_0\"), NOW, QUAL);\n      p.addColumn(FAMILYA, Bytes.toBytes(\"qual_1\"), NOW, QUAL);\n      try (Connection conn = ConnectionFactory.createConnection(UTIL.getConfiguration());\n              Table t = conn.getTable(TableName.valueOf(exportTable))) {\n        t.put(p);\n      }\n      return null;\n    };\n    // no hdfs access.\n    SecureTestUtil.verifyAllowed(putAction,\n      getUserByLogin(USER_ADMIN),\n      getUserByLogin(USER_OWNER));\n    SecureTestUtil.verifyDenied(putAction,\n      getUserByLogin(USER_RO),\n      getUserByLogin(USER_XO),\n      getUserByLogin(USER_RX),\n      getUserByLogin(USER_NONE));\n\n    final FileSystem fs = UTIL.getDFSCluster().getFileSystem();\n    final Path openDir = fs.makeQualified(new Path(\"testAccessCase\"));\n    fs.mkdirs(openDir);\n    fs.setPermission(openDir, new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.ALL));\n    final Path output = fs.makeQualified(new Path(openDir, \"output\"));\n    AccessTestAction exportAction = () -> {\n      try {\n        String[] args = new String[]{exportTable, output.toString()};\n        Map<byte[], Export.Response> result\n                = Export.run(new Configuration(UTIL.getConfiguration()), args);\n        long rowCount = 0;\n        long cellCount = 0;\n        for (Export.Response r : result.values()) {\n          rowCount += r.getRowCount();\n          cellCount += r.getCellCount();\n        }\n        assertEquals(1, rowCount);\n        assertEquals(2, cellCount);\n        return null;\n      } catch (ServiceException | IOException ex) {\n        throw ex;\n      } catch (Throwable ex) {\n        LOG.error(ex.toString(), ex);\n        throw new Exception(ex);\n      } finally {\n        clearOutput(output);\n      }\n    };\n    SecureTestUtil.verifyDenied(exportAction,\n      getUserByLogin(USER_RO),\n      getUserByLogin(USER_XO),\n      getUserByLogin(USER_NONE));\n    SecureTestUtil.verifyAllowed(exportAction,\n      getUserByLogin(USER_ADMIN),\n      getUserByLogin(USER_OWNER),\n      getUserByLogin(USER_RX));\n    AccessTestAction deleteAction = () -> {\n      UTIL.deleteTable(TableName.valueOf(exportTable));\n      return null;\n    };\n    SecureTestUtil.verifyAllowed(deleteAction, getUserByLogin(USER_OWNER));\n    fs.delete(openDir, true);\n  }"
        ],
        [
            "TestHRegionServerBulkLoadWithOldSecureEndpoint::AtomicHFileLoader::AtomicHFileLoader(TableName,TestContext,byte)",
            "  89 -\n  90 -\n  91  \n  92  \n  93  ",
            "    public AtomicHFileLoader(TableName tableName, TestContext ctx,\n        byte targetFamilies[][]) throws IOException {\n      super(ctx);\n      this.tableName = tableName;\n    }",
            "  88 +\n  89 +\n  90  \n  91  \n  92  ",
            "    public AtomicHFileLoader(TableName tableName, TestContext ctx, byte[][] targetFamilies)\n            throws IOException {\n      super(ctx);\n      this.tableName = tableName;\n    }"
        ],
        [
            "TestServerCustomProtocol::ping(Table,byte,byte)",
            " 399  \n 400 -\n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  ",
            "  private Map<byte [], String> ping(final Table table, final byte [] start, final byte [] end)\n  throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class, start, end,\n      new Batch.Call<PingProtos.PingService, String>() {\n        @Override\n        public String call(PingProtos.PingService instance) throws IOException {\n          return doPing(instance);\n        }\n      });\n  }",
            " 403  \n 404 +\n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  ",
            "  private Map<byte [], String> ping(final Table table, final byte [] start, final byte [] end)\n          throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class, start, end,\n      new Batch.Call<PingProtos.PingService, String>() {\n        @Override\n        public String call(PingProtos.PingService instance) throws IOException {\n          return doPing(instance);\n        }\n      });\n  }"
        ],
        [
            "AggregateImplementation::getSum(RpcController,AggregateRequest,RpcCallback)",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186 -\n 187  \n 188  \n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  /**\n   * Gives the sum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, sum for the entire column\n   * family will be returned.\n   */\n  @Override\n  public void getSum(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    long sum = 0l;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null;\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          if (temp != null)\n            sumVal = ci.add(sumVal, ci.castToReturnType(temp));\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        response = AggregateResponse.newBuilder().addFirstPart(\n          ci.getProtoForPromotedType(sumVal).toByteString()).build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.debug(\"Sum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + sum);\n    done.run(response);\n  }",
            " 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209  \n 210 +\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  /**\n   * Gives the sum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, sum for the entire column\n   * family will be returned.\n   */\n  @Override\n  public void getSum(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    long sum = 0L;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null;\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          if (temp != null) {\n            sumVal = ci.add(sumVal, ci.castToReturnType(temp));\n          }\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        response = AggregateResponse.newBuilder().addFirstPart(\n          ci.getProtoForPromotedType(sumVal).toByteString()).build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.debug(\"Sum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + sum);\n    done.run(response);\n  }"
        ],
        [
            "AsyncAggregationClient::min(AsyncTable,ColumnInterpreter,Scan)",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message> CompletableFuture<R>\n      min(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<R> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<R> callback = new AbstractAggregationCallback<R>(future) {\n\n      private R min;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          R result = getCellValueFromProto(ci, resp, 0);\n          if (min == null || (result != null && ci.compare(min, result) > 0)) {\n            min = result;\n          }\n        }\n      }\n\n      @Override\n      protected R getFinalResult() {\n        return min;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getMin(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175 +\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message> CompletableFuture<R>\n      min(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<R> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, false);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n\n    AbstractAggregationCallback<R> callback = new AbstractAggregationCallback<R>(future) {\n\n      private R min;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        if (resp.getFirstPartCount() > 0) {\n          R result = getCellValueFromProto(ci, resp, 0);\n          if (min == null || (result != null && ci.compare(min, result) > 0)) {\n            min = result;\n          }\n        }\n      }\n\n      @Override\n      protected R getFinalResult() {\n        return min;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getMin(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "AsyncAggregationClient::rowCount(AsyncTable,ColumnInterpreter,Scan)",
            " 202  \n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Long>\n      rowCount(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<Long> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, true);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Long> callback = new AbstractAggregationCallback<Long>(future) {\n\n      private long count;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        count += resp.getFirstPart(0).asReadOnlyByteBuffer().getLong();\n      }\n\n      @Override\n      protected Long getFinalResult() {\n        return count;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getRowNum(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }",
            " 203  \n 204 +\n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<Long> rowCount(AsyncTable<?> table, ColumnInterpreter<R, S, P, Q, T> ci,\n          Scan scan) {\n    CompletableFuture<Long> future = new CompletableFuture<>();\n    AggregateRequest req;\n    try {\n      req = validateArgAndGetPB(scan, ci, true);\n    } catch (IOException e) {\n      future.completeExceptionally(e);\n      return future;\n    }\n    AbstractAggregationCallback<Long> callback = new AbstractAggregationCallback<Long>(future) {\n\n      private long count;\n\n      @Override\n      protected void aggregate(RegionInfo region, AggregateResponse resp) throws IOException {\n        count += resp.getFirstPart(0).asReadOnlyByteBuffer().getLong();\n      }\n\n      @Override\n      protected Long getFinalResult() {\n        return count;\n      }\n    };\n    table\n        .<AggregateService, AggregateResponse> coprocessorService(AggregateService::newStub,\n          (stub, controller, rpcCallback) -> stub.getRowNum(controller, req, rpcCallback), callback)\n        .fromRow(nullToEmpty(scan.getStartRow()), scan.includeStartRow())\n        .toRow(nullToEmpty(scan.getStopRow()), scan.includeStopRow()).execute();\n    return future;\n  }"
        ],
        [
            "AggregateImplementation::getStd(RpcController,AggregateRequest,RpcCallback)",
            " 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 -\n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  ",
            "  /**\n   * Gives a Pair with first object a List containing Sum and sum of squares,\n   * and the second object as row count. It is computed for a given combination of\n   * column qualifier and column family in the given row range as defined in the\n   * Scan object. In its current implementation, it takes one column family and\n   * one column qualifier (if provided). The idea is get the value of variance first:\n   * the average of the squares less the square of the average a standard\n   * deviation is square root of variance.\n   */\n  @Override\n  public void getStd(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    InternalScanner scanner = null;\n    AggregateResponse response = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null, sumSqVal = null, tempVal = null;\n      long rowCountVal = 0l;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n\n      boolean hasMoreRows = false;\n\n      do {\n        tempVal = null;\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          tempVal = ci.add(tempVal, ci.castToReturnType(ci.getValue(colFamily,\n              qualifier, results.get(i))));\n        }\n        results.clear();\n        sumVal = ci.add(sumVal, tempVal);\n        sumSqVal = ci.add(sumSqVal, ci.multiply(tempVal, tempVal));\n        rowCountVal++;\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        ByteString first_sumVal = ci.getProtoForPromotedType(sumVal).toByteString();\n        ByteString first_sumSqVal = ci.getProtoForPromotedType(sumSqVal).toByteString();\n        AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n        pair.addFirstPart(first_sumVal);\n        pair.addFirstPart(first_sumSqVal);\n        ByteBuffer bb = ByteBuffer.allocate(8).putLong(rowCountVal);\n        bb.rewind();\n        pair.setSecondPart(ByteString.copyFrom(bb));\n        response = pair.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }",
            " 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  ",
            "  /**\n   * Gives a Pair with first object a List containing Sum and sum of squares,\n   * and the second object as row count. It is computed for a given combination of\n   * column qualifier and column family in the given row range as defined in the\n   * Scan object. In its current implementation, it takes one column family and\n   * one column qualifier (if provided). The idea is get the value of variance first:\n   * the average of the squares less the square of the average a standard\n   * deviation is square root of variance.\n   */\n  @Override\n  public void getStd(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    InternalScanner scanner = null;\n    AggregateResponse response = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null, sumSqVal = null, tempVal = null;\n      long rowCountVal = 0L;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      List<Cell> results = new ArrayList<>();\n\n      boolean hasMoreRows = false;\n\n      do {\n        tempVal = null;\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          tempVal = ci.add(tempVal, ci.castToReturnType(ci.getValue(colFamily,\n              qualifier, results.get(i))));\n        }\n        results.clear();\n        sumVal = ci.add(sumVal, tempVal);\n        sumSqVal = ci.add(sumSqVal, ci.multiply(tempVal, tempVal));\n        rowCountVal++;\n      } while (hasMoreRows);\n      if (sumVal != null) {\n        ByteString first_sumVal = ci.getProtoForPromotedType(sumVal).toByteString();\n        ByteString first_sumSqVal = ci.getProtoForPromotedType(sumSqVal).toByteString();\n        AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n        pair.addFirstPart(first_sumVal);\n        pair.addFirstPart(first_sumSqVal);\n        ByteBuffer bb = ByteBuffer.allocate(8).putLong(rowCountVal);\n        bb.rewind();\n        pair.setSecondPart(ByteString.copyFrom(bb));\n        response = pair.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::cleanupBulkLoad(RpcController,CleanupBulkLoadRequest,RpcCallback)",
            " 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  ",
            "  @Override\n  public void cleanupBulkLoad(RpcController controller, CleanupBulkLoadRequest request,\n      RpcCallback<CleanupBulkLoadResponse> done) {\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n      secureBulkLoadManager.cleanupBulkLoad((HRegion) this.env.getRegion(), convert(request));\n      done.run(CleanupBulkLoadResponse.newBuilder().build());\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(null);\n  }",
            " 113  \n 114  \n 115 +\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  ",
            "  @Override\n  public void cleanupBulkLoad(RpcController controller, CleanupBulkLoadRequest request,\n          RpcCallback<CleanupBulkLoadResponse> done) {\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n      secureBulkLoadManager.cleanupBulkLoad((HRegion) this.env.getRegion(), convert(request));\n      done.run(CleanupBulkLoadResponse.newBuilder().build());\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(null);\n  }"
        ],
        [
            "TestServerCustomProtocol::noop(Table,byte,byte)",
            " 289 -\n 290 -\n 291 -\n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  ",
            "  private Map<byte [], String> noop(final Table table, final byte [] start,\n      final byte [] end)\n  throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class, start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.NoopResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.NoopRequest.Builder builder = PingProtos.NoopRequest.newBuilder();\n            instance.noop(null, builder.build(), rpcCallback);\n            rpcCallback.get();\n            // Looks like null is expected when void.  That is what the test below is looking for\n            return null;\n          }\n        });\n  }",
            " 294 +\n 295 +\n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  ",
            "  private Map<byte [], String> noop(final Table table, final byte [] start, final byte [] end)\n          throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class, start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.NoopResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.NoopRequest.Builder builder = PingProtos.NoopRequest.newBuilder();\n            instance.noop(null, builder.build(), rpcCallback);\n            rpcCallback.get();\n            // Looks like null is expected when void.  That is what the test below is looking for\n            return null;\n          }\n        });\n  }"
        ],
        [
            "AggregationClient::median(Table,ColumnInterpreter,Scan)",
            " 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759 -\n 760 -\n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786 -\n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 -\n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  ",
            "  /**\n   * This is the client side interface/handler for calling the median method for a\n   * given cf-cq combination. This method collects the necessary parameters\n   * to compute the median and returns the median.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return R the median\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  R median(final Table table, ColumnInterpreter<R, S, P, Q, T> ci,\n      Scan scan) throws Throwable {\n    Pair<NavigableMap<byte[], List<S>>, List<S>> p = getMedianArgs(table, ci, scan);\n    byte[] startRow = null;\n    byte[] colFamily = scan.getFamilies()[0];\n    NavigableSet<byte[]> quals = scan.getFamilyMap().get(colFamily);\n    NavigableMap<byte[], List<S>> map = p.getFirst();\n    S sumVal = p.getSecond().get(0);\n    S sumWeights = p.getSecond().get(1);\n    double halfSumVal = ci.divideForAvg(sumVal, 2L);\n    double movingSumVal = 0;\n    boolean weighted = false;\n    if (quals.size() > 1) {\n      weighted = true;\n      halfSumVal = ci.divideForAvg(sumWeights, 2L);\n    }\n\n    for (Map.Entry<byte[], List<S>> entry : map.entrySet()) {\n      S s = weighted ? entry.getValue().get(1) : entry.getValue().get(0);\n      double newSumVal = movingSumVal + ci.divideForAvg(s, 1L);\n      if (newSumVal > halfSumVal) break;  // we found the region with the median\n      movingSumVal = newSumVal;\n      startRow = entry.getKey();\n    }\n    // scan the region with median and find it\n    Scan scan2 = new Scan(scan);\n    // inherit stop row from method parameter\n    if (startRow != null) scan2.setStartRow(startRow);\n    ResultScanner scanner = null;\n    try {\n      int cacheSize = scan2.getCaching();\n      if (!scan2.getCacheBlocks() || scan2.getCaching() < 2) {\n        scan2.setCacheBlocks(true);\n        cacheSize = 5;\n        scan2.setCaching(cacheSize);\n      }\n      scanner = table.getScanner(scan2);\n      Result[] results = null;\n      byte[] qualifier = quals.pollFirst();\n      // qualifier for the weight column\n      byte[] weightQualifier = weighted ? quals.pollLast() : qualifier;\n      R value = null;\n      do {\n        results = scanner.next(cacheSize);\n        if (results != null && results.length > 0) {\n          for (int i = 0; i < results.length; i++) {\n            Result r = results[i];\n            // retrieve weight\n            Cell kv = r.getColumnLatestCell(colFamily, weightQualifier);\n            R newValue = ci.getValue(colFamily, weightQualifier, kv);\n            S s = ci.castToReturnType(newValue);\n            double newSumVal = movingSumVal + ci.divideForAvg(s, 1L);\n            // see if we have moved past the median\n            if (newSumVal > halfSumVal) {\n              return value;\n            }\n            movingSumVal = newSumVal;\n            kv = r.getColumnLatestCell(colFamily, qualifier);\n            value = ci.getValue(colFamily, qualifier, kv);\n            }\n          }\n      } while (results != null && results.length > 0);\n    } finally {\n      if (scanner != null) {\n        scanner.close();\n      }\n    }\n    return null;\n  }",
            " 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772 +\n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791 +\n 792 +\n 793 +\n 794 +\n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801 +\n 802 +\n 803 +\n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836 +\n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  ",
            "  /**\n   * This is the client side interface/handler for calling the median method for a\n   * given cf-cq combination. This method collects the necessary parameters\n   * to compute the median and returns the median.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return R the median\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    R median(final Table table, ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) throws Throwable {\n    Pair<NavigableMap<byte[], List<S>>, List<S>> p = getMedianArgs(table, ci, scan);\n    byte[] startRow = null;\n    byte[] colFamily = scan.getFamilies()[0];\n    NavigableSet<byte[]> quals = scan.getFamilyMap().get(colFamily);\n    NavigableMap<byte[], List<S>> map = p.getFirst();\n    S sumVal = p.getSecond().get(0);\n    S sumWeights = p.getSecond().get(1);\n    double halfSumVal = ci.divideForAvg(sumVal, 2L);\n    double movingSumVal = 0;\n    boolean weighted = false;\n    if (quals.size() > 1) {\n      weighted = true;\n      halfSumVal = ci.divideForAvg(sumWeights, 2L);\n    }\n\n    for (Map.Entry<byte[], List<S>> entry : map.entrySet()) {\n      S s = weighted ? entry.getValue().get(1) : entry.getValue().get(0);\n      double newSumVal = movingSumVal + ci.divideForAvg(s, 1L);\n      if (newSumVal > halfSumVal) {\n        // we found the region with the median\n        break;\n      }\n      movingSumVal = newSumVal;\n      startRow = entry.getKey();\n    }\n    // scan the region with median and find it\n    Scan scan2 = new Scan(scan);\n    // inherit stop row from method parameter\n    if (startRow != null) {\n      scan2.setStartRow(startRow);\n    }\n    ResultScanner scanner = null;\n    try {\n      int cacheSize = scan2.getCaching();\n      if (!scan2.getCacheBlocks() || scan2.getCaching() < 2) {\n        scan2.setCacheBlocks(true);\n        cacheSize = 5;\n        scan2.setCaching(cacheSize);\n      }\n      scanner = table.getScanner(scan2);\n      Result[] results = null;\n      byte[] qualifier = quals.pollFirst();\n      // qualifier for the weight column\n      byte[] weightQualifier = weighted ? quals.pollLast() : qualifier;\n      R value = null;\n      do {\n        results = scanner.next(cacheSize);\n        if (results != null && results.length > 0) {\n          for (int i = 0; i < results.length; i++) {\n            Result r = results[i];\n            // retrieve weight\n            Cell kv = r.getColumnLatestCell(colFamily, weightQualifier);\n            R newValue = ci.getValue(colFamily, weightQualifier, kv);\n            S s = ci.castToReturnType(newValue);\n            double newSumVal = movingSumVal + ci.divideForAvg(s, 1L);\n            // see if we have moved past the median\n            if (newSumVal > halfSumVal) {\n              return value;\n            }\n            movingSumVal = newSumVal;\n            kv = r.getColumnLatestCell(colFamily, qualifier);\n            value = ci.getValue(colFamily, qualifier, kv);\n          }\n        }\n      } while (results != null && results.length > 0);\n    } finally {\n      if (scanner != null) {\n        scanner.close();\n      }\n    }\n    return null;\n  }"
        ],
        [
            "AsyncAggregationClient::median(AsyncTable,ColumnInterpreter,Scan)",
            " 459  \n 460 -\n 461 -\n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n      CompletableFuture<R> median(AsyncTable<AdvancedScanResultConsumer> table,\n      ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<R> future = new CompletableFuture<>();\n    sumByRegion(table, ci, scan).whenComplete((sumByRegion, error) -> {\n      if (error != null) {\n        future.completeExceptionally(error);\n      } else if (sumByRegion.isEmpty()) {\n        future.completeExceptionally(new NoSuchElementException());\n      } else {\n        findMedian(future, table, ci, ReflectionUtils.newInstance(scan.getClass(), scan),\n          sumByRegion);\n      }\n    });\n    return future;\n  }",
            " 457  \n 458 +\n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "  public static <R, S, P extends Message, Q extends Message, T extends Message>\n          CompletableFuture<R> median(AsyncTable<AdvancedScanResultConsumer> table,\n          ColumnInterpreter<R, S, P, Q, T> ci, Scan scan) {\n    CompletableFuture<R> future = new CompletableFuture<>();\n    sumByRegion(table, ci, scan).whenComplete((sumByRegion, error) -> {\n      if (error != null) {\n        future.completeExceptionally(error);\n      } else if (sumByRegion.isEmpty()) {\n        future.completeExceptionally(new NoSuchElementException());\n      } else {\n        findMedian(future, table, ci, ReflectionUtils.newInstance(scan.getClass(), scan),\n          sumByRegion);\n      }\n    });\n    return future;\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::convert(CleanupBulkLoadRequest)",
            " 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest\n  convert(CleanupBulkLoadRequest request)\n      throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest.Builder\n        builder =\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest.\n      newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }",
            " 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest\n    convert(CleanupBulkLoadRequest request)\n      throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest.Builder\n        builder =\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest.\n      newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }"
        ],
        [
            "TestCoprocessorTableEndpoint::sum(Table,byte,byte,byte,byte)",
            " 113  \n 114  \n 115  \n 116 -\n 117  \n 118 -\n 119 -\n 120 -\n 121 -\n 122 -\n 123 -\n 124 -\n 125 -\n 126 -\n 127 -\n 128 -\n 129  \n 130 -\n 131 -\n 132 -\n 133 -\n 134  ",
            "  private static Map<byte [], Long> sum(final Table table, final byte [] family,\n    final byte [] qualifier, final byte [] start, final byte [] end)\n      throws ServiceException, Throwable {\n  return table.coprocessorService(ColumnAggregationProtos.ColumnAggregationService.class,\n      start, end,\n    new Batch.Call<ColumnAggregationProtos.ColumnAggregationService, Long>() {\n      @Override\n      public Long call(ColumnAggregationProtos.ColumnAggregationService instance)\n      throws IOException {\n        CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =\n            new CoprocessorRpcUtils.BlockingRpcCallback<>();\n        ColumnAggregationProtos.SumRequest.Builder builder =\n          ColumnAggregationProtos.SumRequest.newBuilder();\n        builder.setFamily(ByteString.copyFrom(family));\n        if (qualifier != null && qualifier.length > 0) {\n          builder.setQualifier(ByteString.copyFrom(qualifier));\n        }\n        instance.sum(null, builder.build(), rpcCallback);\n        return rpcCallback.get().getSum();\n      }\n    });\n  }",
            " 112  \n 113  \n 114  \n 115 +\n 116  \n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131  \n 132 +\n 133  ",
            "  private static Map<byte [], Long> sum(final Table table, final byte [] family,\n    final byte [] qualifier, final byte [] start, final byte [] end)\n      throws ServiceException, Throwable {\n    return table.coprocessorService(ColumnAggregationProtos.ColumnAggregationService.class,\n      start, end,\n      new Batch.Call<ColumnAggregationProtos.ColumnAggregationService, Long>() {\n        @Override\n        public Long call(ColumnAggregationProtos.ColumnAggregationService instance)\n          throws IOException {\n          CoprocessorRpcUtils.BlockingRpcCallback<ColumnAggregationProtos.SumResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n          ColumnAggregationProtos.SumRequest.Builder builder =\n            ColumnAggregationProtos.SumRequest.newBuilder();\n          builder.setFamily(ByteString.copyFrom(family));\n          if (qualifier != null && qualifier.length > 0) {\n            builder.setQualifier(ByteString.copyFrom(qualifier));\n          }\n          instance.sum(null, builder.build(), rpcCallback);\n          return rpcCallback.get().getSum();\n        }\n      });\n  }"
        ],
        [
            "TestAsyncCoprocessorEndpoint::testMasterCoprocessorService()",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84 -\n  85  \n  86  \n  87  ",
            "  @Test\n  public void testMasterCoprocessorService() throws Exception {\n    TestProtos.EchoRequestProto request =\n        TestProtos.EchoRequestProto.newBuilder().setMessage(\"hello\").build();\n    TestProtos.EchoResponseProto response =\n        admin\n            .<TestRpcServiceProtos.TestProtobufRpcProto.Stub, TestProtos.EchoResponseProto> coprocessorService(\n              TestRpcServiceProtos.TestProtobufRpcProto::newStub,\n              (s, c, done) -> s.echo(c, request, done)).get();\n    assertEquals(\"hello\", response.getMessage());\n  }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85  \n  86  ",
            "  @Test\n  public void testMasterCoprocessorService() throws Exception {\n    TestProtos.EchoRequestProto request =\n        TestProtos.EchoRequestProto.newBuilder().setMessage(\"hello\").build();\n    TestProtos.EchoResponseProto response =\n        admin\n            .<TestRpcServiceProtos.TestProtobufRpcProto.Stub, TestProtos.EchoResponseProto>\n                coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto::newStub,\n              (s, c, done) -> s.echo(c, request, done)).get();\n    assertEquals(\"hello\", response.getMessage());\n  }"
        ],
        [
            "ProtobufCoprocessorService::error(RpcController,TestProtos,RpcCallback)",
            "  65  \n  66  \n  67 -\n  68  \n  69  \n  70  ",
            "  @Override\n  public void error(RpcController controller, TestProtos.EmptyRequestProto request,\n      RpcCallback<TestProtos.EmptyResponseProto> done) {\n    CoprocessorRpcUtils.setControllerException(controller, new IOException(\"Test exception\"));\n    done.run(null);\n  }",
            "  65  \n  66  \n  67 +\n  68  \n  69  \n  70  ",
            "  @Override\n  public void error(RpcController controller, TestProtos.EmptyRequestProto request,\n          RpcCallback<TestProtos.EmptyResponseProto> done) {\n    CoprocessorRpcUtils.setControllerException(controller, new IOException(\"Test exception\"));\n    done.run(null);\n  }"
        ],
        [
            "ProtobufCoprocessorService::pause(RpcController,PauseRequestProto,RpcCallback)",
            "  72  \n  73  \n  74 -\n  75  \n  76  \n  77  ",
            "  @Override\n  public void pause(RpcController controller, PauseRequestProto request,\n      RpcCallback<EmptyResponseProto> done) {\n    Threads.sleepWithoutInterrupt(request.getMs());\n    done.run(EmptyResponseProto.getDefaultInstance());\n  }",
            "  72  \n  73  \n  74 +\n  75  \n  76  \n  77  ",
            "  @Override\n  public void pause(RpcController controller, PauseRequestProto request,\n          RpcCallback<EmptyResponseProto> done) {\n    Threads.sleepWithoutInterrupt(request.getMs());\n    done.run(EmptyResponseProto.getDefaultInstance());\n  }"
        ],
        [
            "Export::run(Configuration,String)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  ",
            "  @VisibleForTesting\n  static Map<byte[], Response> run(final Configuration conf, final String[] args) throws Throwable {\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n    if (!ExportUtils.isValidArguements(args)) {\n      ExportUtils.usage(\"Wrong number of arguments: \" + ArrayUtils.getLength(otherArgs));\n      return null;\n    }\n    Triple<TableName, Scan, Path> arguments = ExportUtils.getArgumentsFromCommandLine(conf, otherArgs);\n    return run(conf, arguments.getFirst(), arguments.getSecond(), arguments.getThird());\n  }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114 +\n 115  \n 116  ",
            "  @VisibleForTesting\n  static Map<byte[], Response> run(final Configuration conf, final String[] args) throws Throwable {\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n    if (!ExportUtils.isValidArguements(args)) {\n      ExportUtils.usage(\"Wrong number of arguments: \" + ArrayUtils.getLength(otherArgs));\n      return null;\n    }\n    Triple<TableName, Scan, Path> arguments =\n        ExportUtils.getArgumentsFromCommandLine(conf, otherArgs);\n    return run(conf, arguments.getFirst(), arguments.getSecond(), arguments.getThird());\n  }"
        ],
        [
            "AggregationClient::rowCount(Table,ColumnInterpreter,Scan)",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337 -\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  ",
            "  /**\n   * It gives the row count, by summing up the individual results obtained from\n   * regions. In case the qualifier is null, FirstKeyValueFilter is used to\n   * optimised the operation. In case qualifier is provided, I can't use the\n   * filter as it may set the flag to skip to next row, but the value read is\n   * not of the given filter: in this case, this particular row will not be\n   * counted ==&gt; an error.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return &lt;R, S&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  long rowCount(final Table table,\n      final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, true);\n    class RowNumCallback implements Batch.Callback<Long> {\n      private final AtomicLong rowCountL = new AtomicLong(0);\n\n      public long getRowNumCount() {\n        return rowCountL.get();\n      }\n\n      @Override\n      public void update(byte[] region, byte[] row, Long result) {\n        rowCountL.addAndGet(result.longValue());\n      }\n    }\n    RowNumCallback rowNum = new RowNumCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Long>() {\n          @Override\n          public Long call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getRowNum(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            byte[] bytes = getBytesFromResponse(response.getFirstPart(0));\n            ByteBuffer bb = ByteBuffer.allocate(8).put(bytes);\n            bb.rewind();\n            return bb.getLong();\n          }\n        }, rowNum);\n    return rowNum.getRowNumCount();\n  }",
            " 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 +\n 334 +\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348 +\n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  ",
            "  /**\n   * It gives the row count, by summing up the individual results obtained from\n   * regions. In case the qualifier is null, FirstKeyValueFilter is used to\n   * optimised the operation. In case qualifier is provided, I can't use the\n   * filter as it may set the flag to skip to next row, but the value read is\n   * not of the given filter: in this case, this particular row will not be\n   * counted ==&gt; an error.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return &lt;R, S&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    long rowCount(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, true);\n    class RowNumCallback implements Batch.Callback<Long> {\n      private final AtomicLong rowCountL = new AtomicLong(0);\n\n      public long getRowNumCount() {\n        return rowCountL.get();\n      }\n\n      @Override\n      public void update(byte[] region, byte[] row, Long result) {\n        rowCountL.addAndGet(result.longValue());\n      }\n    }\n\n    RowNumCallback rowNum = new RowNumCallback();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, Long>() {\n          @Override\n          public Long call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getRowNum(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            byte[] bytes = getBytesFromResponse(response.getFirstPart(0));\n            ByteBuffer bb = ByteBuffer.allocate(8).put(bytes);\n            bb.rewind();\n            return bb.getLong();\n          }\n        }, rowNum);\n    return rowNum.getRowNumCount();\n  }"
        ],
        [
            "TestSecureExport::getUserByLogin(String)",
            " 149  \n 150 -\n 151  ",
            "  private static User getUserByLogin(final String user) throws IOException {\n    return User.create(UserGroupInformation.loginUserFromKeytabAndReturnUGI(getPrinciple(user), KEYTAB_FILE.getAbsolutePath()));\n  }",
            " 149  \n 150 +\n 151 +\n 152  ",
            "  private static User getUserByLogin(final String user) throws IOException {\n    return User.create(UserGroupInformation.loginUserFromKeytabAndReturnUGI(\n        getPrinciple(user), KEYTAB_FILE.getAbsolutePath()));\n  }"
        ],
        [
            "TestSecureExport::setUpClusterKdc()",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 -\n 164 -\n 165 -\n 166 -\n 167  \n 168 -\n 169 -\n 170 -\n 171  \n 172 -\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179 -\n 180  \n 181  \n 182  \n 183 -\n 184 -\n 185  ",
            "  private static void setUpClusterKdc() throws Exception {\n    HBaseKerberosUtils.setKeytabFileForTesting(KEYTAB_FILE.getAbsolutePath());\n    HBaseKerberosUtils.setPrincipalForTesting(SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    HBaseKerberosUtils.setSecuredConfiguration(UTIL.getConfiguration());\n    // if we drop support for hadoop-2.4.0 and hadoop-2.4.1,\n    // the following key should be changed.\n    // 1) DFS_NAMENODE_USER_NAME_KEY -> DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY\n    // 2) DFS_DATANODE_USER_NAME_KEY -> DFS_DATANODE_KERBEROS_PRINCIPAL_KEY\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY, SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, KEYTAB_FILE.getAbsolutePath());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_KEYTAB_FILE_KEY, KEYTAB_FILE.getAbsolutePath());\n    // set yarn principal\n    UTIL.getConfiguration().set(YarnConfiguration.RM_PRINCIPAL, SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(YarnConfiguration.NM_PRINCIPAL, SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY, HTTP_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().setBoolean(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, true);\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_HTTP_POLICY_KEY, HttpConfig.Policy.HTTPS_ONLY.name());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY, LOCALHOST + \":0\");\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_KEY, LOCALHOST + \":0\");\n\n    File keystoresDir = new File(UTIL.getDataTestDir(\"keystore\").toUri().getPath());\n    keystoresDir.mkdirs();\n    String sslConfDir = KeyStoreTestUtil.getClasspathDir(TestSecureExport.class);\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir.getAbsolutePath(), sslConfDir, UTIL.getConfiguration(), false);\n\n    UTIL.getConfiguration().setBoolean(\"ignore.secure.ports.for.testing\", true);\n    UserGroupInformation.setConfiguration(UTIL.getConfiguration());\n    UTIL.getConfiguration().set(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY, UTIL.getConfiguration().get(\n      CoprocessorHost.REGION_COPROCESSOR_CONF_KEY) + \",\" + Export.class.getName());\n  }",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174  \n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180 +\n 181  \n 182 +\n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190 +\n 191 +\n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197 +\n 198  ",
            "  private static void setUpClusterKdc() throws Exception {\n    HBaseKerberosUtils.setKeytabFileForTesting(KEYTAB_FILE.getAbsolutePath());\n    HBaseKerberosUtils.setPrincipalForTesting(SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    HBaseKerberosUtils.setSecuredConfiguration(UTIL.getConfiguration());\n    // if we drop support for hadoop-2.4.0 and hadoop-2.4.1,\n    // the following key should be changed.\n    // 1) DFS_NAMENODE_USER_NAME_KEY -> DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY\n    // 2) DFS_DATANODE_USER_NAME_KEY -> DFS_DATANODE_KERBEROS_PRINCIPAL_KEY\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY,\n        SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY,\n        SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n        KEYTAB_FILE.getAbsolutePath());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_KEYTAB_FILE_KEY,\n        KEYTAB_FILE.getAbsolutePath());\n    // set yarn principal\n    UTIL.getConfiguration().set(YarnConfiguration.RM_PRINCIPAL,\n        SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(YarnConfiguration.NM_PRINCIPAL,\n        SERVER_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY,\n        HTTP_PRINCIPAL + \"@\" + KDC.getRealm());\n    UTIL.getConfiguration().setBoolean(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY, true);\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_HTTP_POLICY_KEY,\n        HttpConfig.Policy.HTTPS_ONLY.name());\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY, LOCALHOST + \":0\");\n    UTIL.getConfiguration().set(DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_KEY, LOCALHOST + \":0\");\n\n    File keystoresDir = new File(UTIL.getDataTestDir(\"keystore\").toUri().getPath());\n    keystoresDir.mkdirs();\n    String sslConfDir = KeyStoreTestUtil.getClasspathDir(TestSecureExport.class);\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir.getAbsolutePath(), sslConfDir,\n        UTIL.getConfiguration(), false);\n\n    UTIL.getConfiguration().setBoolean(\"ignore.secure.ports.for.testing\", true);\n    UserGroupInformation.setConfiguration(UTIL.getConfiguration());\n    UTIL.getConfiguration().set(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY,\n        UTIL.getConfiguration().get(\n            CoprocessorHost.REGION_COPROCESSOR_CONF_KEY) + \",\" + Export.class.getName());\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::prepareBulkLoad(RpcController,PrepareBulkLoadRequest,RpcCallback)",
            "  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  @Override\n  public void prepareBulkLoad(RpcController controller, PrepareBulkLoadRequest request,\n      RpcCallback<PrepareBulkLoadResponse> done) {\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n\n      String bulkToken = secureBulkLoadManager.prepareBulkLoad((HRegion) this.env.getRegion(),\n          convert(request));\n      done.run(PrepareBulkLoadResponse.newBuilder().setBulkToken(bulkToken).build());\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(null);\n  }",
            "  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  @Override\n  public void prepareBulkLoad(RpcController controller, PrepareBulkLoadRequest request,\n          RpcCallback<PrepareBulkLoadResponse> done) {\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n\n      String bulkToken = secureBulkLoadManager.prepareBulkLoad((HRegion) this.env.getRegion(),\n          convert(request));\n      done.run(PrepareBulkLoadResponse.newBuilder().setBulkToken(bulkToken).build());\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(null);\n  }"
        ],
        [
            "AggregateImplementation::getRowNum(RpcController,AggregateRequest,RpcCallback)",
            " 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 -\n 239  \n 240 -\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 -\n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  /**\n   * Gives the row count for the given column family and column qualifier, in\n   * the given row range as defined in the Scan object.\n   */\n  @Override\n  public void getRowNum(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    long counter = 0l;\n    List<Cell> results = new ArrayList<>();\n    InternalScanner scanner = null;\n    try {\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      byte[][] colFamilies = scan.getFamilies();\n      byte[] colFamily = colFamilies != null ? colFamilies[0] : null;\n      NavigableSet<byte[]> qualifiers = colFamilies != null ?\n          scan.getFamilyMap().get(colFamily) : null;\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      if (scan.getFilter() == null && qualifier == null)\n        scan.setFilter(new FirstKeyOnlyFilter());\n      scanner = env.getRegion().getScanner(scan);\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        if (results.size() > 0) {\n          counter++;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      ByteBuffer bb = ByteBuffer.allocate(8).putLong(counter);\n      bb.rewind();\n      response = AggregateResponse.newBuilder().addFirstPart(\n          ByteString.copyFrom(bb)).build();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Row counter from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + counter);\n    done.run(response);\n  }",
            " 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239  \n 240 +\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 +\n 254  \n 255 +\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  ",
            "  /**\n   * Gives the row count for the given column family and column qualifier, in\n   * the given row range as defined in the Scan object.\n   */\n  @Override\n  public void getRowNum(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    long counter = 0L;\n    List<Cell> results = new ArrayList<>();\n    InternalScanner scanner = null;\n    try {\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      byte[][] colFamilies = scan.getFamilies();\n      byte[] colFamily = colFamilies != null ? colFamilies[0] : null;\n      NavigableSet<byte[]> qualifiers = colFamilies != null ?\n          scan.getFamilyMap().get(colFamily) : null;\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      if (scan.getFilter() == null && qualifier == null) {\n        scan.setFilter(new FirstKeyOnlyFilter());\n      }\n      scanner = env.getRegion().getScanner(scan);\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        if (results.size() > 0) {\n          counter++;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      ByteBuffer bb = ByteBuffer.allocate(8).putLong(counter);\n      bb.rewind();\n      response = AggregateResponse.newBuilder().addFirstPart(\n          ByteString.copyFrom(bb)).build();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Row counter from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + counter);\n    done.run(response);\n  }"
        ],
        [
            "Export::getWriterOptions(Configuration,RegionInfo,ExportProtos)",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 -\n 202  \n 203  \n 204  \n 205  \n 206  ",
            "  private static List<SequenceFile.Writer.Option> getWriterOptions(final Configuration conf,\n          final RegionInfo info, final ExportProtos.ExportRequest request) throws IOException {\n    List<SequenceFile.Writer.Option> rval = new LinkedList<>();\n    rval.add(SequenceFile.Writer.keyClass(ImmutableBytesWritable.class));\n    rval.add(SequenceFile.Writer.valueClass(Result.class));\n    rval.add(getOutputPath(conf, info, request));\n    if (getCompression(request)) {\n      rval.add(SequenceFile.Writer.compression(getCompressionType(request), getCompressionCodec(conf, request)));\n    } else {\n      rval.add(SequenceFile.Writer.compression(SequenceFile.CompressionType.NONE));\n    }\n    return rval;\n  }",
            " 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 +\n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  ",
            "  private static List<SequenceFile.Writer.Option> getWriterOptions(final Configuration conf,\n          final RegionInfo info, final ExportProtos.ExportRequest request) throws IOException {\n    List<SequenceFile.Writer.Option> rval = new LinkedList<>();\n    rval.add(SequenceFile.Writer.keyClass(ImmutableBytesWritable.class));\n    rval.add(SequenceFile.Writer.valueClass(Result.class));\n    rval.add(getOutputPath(conf, info, request));\n    if (getCompression(request)) {\n      rval.add(SequenceFile.Writer.compression(getCompressionType(request),\n          getCompressionCodec(conf, request)));\n    } else {\n      rval.add(SequenceFile.Writer.compression(SequenceFile.CompressionType.NONE));\n    }\n    return rval;\n  }"
        ],
        [
            "TestServerCustomProtocol::before()",
            " 155  \n 156 -\n 157  \n 158  \n 159  \n 160 -\n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168 -\n 169  \n 170  \n 171  ",
            "  @Before\n  public void before()  throws Exception {\n    final byte[][] SPLIT_KEYS = new byte[][] { ROW_B, ROW_C };\n    Table table = util.createTable(TEST_TABLE, TEST_FAMILY, SPLIT_KEYS);\n\n    Put puta = new Put( ROW_A );\n    puta.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(puta);\n\n    Put putb = new Put( ROW_B );\n    putb.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(putb);\n\n    Put putc = new Put( ROW_C );\n    putc.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(putc);\n  }",
            " 160  \n 161 +\n 162  \n 163  \n 164  \n 165 +\n 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  ",
            "  @Before\n  public void before() throws Exception {\n    final byte[][] SPLIT_KEYS = new byte[][] { ROW_B, ROW_C };\n    Table table = util.createTable(TEST_TABLE, TEST_FAMILY, SPLIT_KEYS);\n\n    Put puta = new Put(ROW_A);\n    puta.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(puta);\n\n    Put putb = new Put(ROW_B);\n    putb.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(putb);\n\n    Put putc = new Put(ROW_C);\n    putc.addColumn(TEST_FAMILY, Bytes.toBytes(\"col1\"), Bytes.toBytes(1));\n    table.put(putc);\n  }"
        ],
        [
            "AggregateImplementation::getMax(RpcController,AggregateRequest,RpcCallback)",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 -\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  ",
            "  /**\n   * Gives the maximum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, maximum value for the\n   * entire column family will be returned.\n   */\n  @Override\n  public void getMax(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    InternalScanner scanner = null;\n    AggregateResponse response = null;\n    T max = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      List<Cell> results = new ArrayList<>();\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      // qualifier can be null.\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          max = (max == null || (temp != null && ci.compare(temp, max) > 0)) ? temp : max;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (max != null) {\n        AggregateResponse.Builder builder = AggregateResponse.newBuilder();\n        builder.addFirstPart(ci.getProtoForCellType(max).toByteString());\n        response = builder.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Maximum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + max);\n    done.run(response);\n  }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "  /**\n   * Gives the maximum for a given combination of column qualifier and column\n   * family, in the given row range as defined in the Scan object. In its\n   * current implementation, it takes one column family and one column qualifier\n   * (if provided). In case of null column qualifier, maximum value for the\n   * entire column family will be returned.\n   */\n  @Override\n  public void getMax(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    InternalScanner scanner = null;\n    AggregateResponse response = null;\n    T max = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      T temp;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      List<Cell> results = new ArrayList<>();\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] qualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        qualifier = qualifiers.pollFirst();\n      }\n      // qualifier can be null.\n      boolean hasMoreRows = false;\n      do {\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          temp = ci.getValue(colFamily, qualifier, results.get(i));\n          max = (max == null || (temp != null && ci.compare(temp, max) > 0)) ? temp : max;\n        }\n        results.clear();\n      } while (hasMoreRows);\n      if (max != null) {\n        AggregateResponse.Builder builder = AggregateResponse.newBuilder();\n        builder.addFirstPart(ci.getProtoForCellType(max).toByteString());\n        response = builder.build();\n      }\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    log.info(\"Maximum from this region is \"\n        + env.getRegion().getRegionInfo().getRegionNameAsString() + \": \" + max);\n    done.run(response);\n  }"
        ],
        [
            "AggregationClient::getAvgArgs(TableName,ColumnInterpreter,Scan)",
            " 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 -\n 455  \n 456  ",
            "  /**\n   * It computes average while fetching sum and row count from all the\n   * corresponding regions. Approach is to compute a global sum of region level\n   * sum and rowcount and then compute the average.\n   * @param tableName\n   * @param scan\n   * @throws Throwable\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message> Pair<S, Long> getAvgArgs(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n      throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n        return getAvgArgs(table, ci, scan);\n    }\n  }",
            " 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455 +\n 456  \n 457  ",
            "  /**\n   * It computes average while fetching sum and row count from all the\n   * corresponding regions. Approach is to compute a global sum of region level\n   * sum and rowcount and then compute the average.\n   * @param tableName the name of the table to scan\n   * @param scan the HBase scan object to use to read data from HBase\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  private <R, S, P extends Message, Q extends Message, T extends Message> Pair<S, Long> getAvgArgs(\n      final TableName tableName, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n      throws Throwable {\n    try (Table table = connection.getTable(tableName)) {\n      return getAvgArgs(table, ci, scan);\n    }\n  }"
        ],
        [
            "TestAsyncCoprocessorEndpoint::testRegionServerCoprocessorService()",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  ",
            "  @Test\n  public void testRegionServerCoprocessorService() throws Exception {\n    final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();\n    DummyRegionServerEndpointProtos.DummyRequest request =\n        DummyRegionServerEndpointProtos.DummyRequest.getDefaultInstance();\n    DummyRegionServerEndpointProtos.DummyResponse response =\n        admin\n            .<DummyRegionServerEndpointProtos.DummyService.Stub, DummyRegionServerEndpointProtos.DummyResponse> coprocessorService(\n              DummyRegionServerEndpointProtos.DummyService::newStub,\n              (s, c, done) -> s.dummyCall(c, request, done), serverName).get();\n    assertEquals(DUMMY_VALUE, response.getValue());\n  }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109 +\n 110  \n 111  \n 112  \n 113  ",
            "  @Test\n  public void testRegionServerCoprocessorService() throws Exception {\n    final ServerName serverName = TEST_UTIL.getHBaseCluster().getRegionServer(0).getServerName();\n    DummyRegionServerEndpointProtos.DummyRequest request =\n        DummyRegionServerEndpointProtos.DummyRequest.getDefaultInstance();\n    DummyRegionServerEndpointProtos.DummyResponse response =\n        admin\n            .<DummyRegionServerEndpointProtos.DummyService.Stub,\n                DummyRegionServerEndpointProtos.DummyResponse> coprocessorService(\n              DummyRegionServerEndpointProtos.DummyService::newStub,\n              (s, c, done) -> s.dummyCall(c, request, done), serverName).get();\n    assertEquals(DUMMY_VALUE, response.getValue());\n  }"
        ],
        [
            "TestAsyncCoprocessorEndpoint::testMasterCoprocessorError()",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  @Test\n  public void testMasterCoprocessorError() throws Exception {\n    TestProtos.EmptyRequestProto emptyRequest = TestProtos.EmptyRequestProto.getDefaultInstance();\n    try {\n      admin\n          .<TestRpcServiceProtos.TestProtobufRpcProto.Stub, TestProtos.EmptyResponseProto> coprocessorService(\n            TestRpcServiceProtos.TestProtobufRpcProto::newStub,\n            (s, c, done) -> s.error(c, emptyRequest, done)).get();\n      fail(\"Should have thrown an exception\");\n    } catch (Exception e) {\n    }\n  }",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  ",
            "  @Test\n  public void testMasterCoprocessorError() throws Exception {\n    TestProtos.EmptyRequestProto emptyRequest = TestProtos.EmptyRequestProto.getDefaultInstance();\n    try {\n      admin\n          .<TestRpcServiceProtos.TestProtobufRpcProto.Stub, TestProtos.EmptyResponseProto>\n              coprocessorService(TestRpcServiceProtos.TestProtobufRpcProto::newStub,\n            (s, c, done) -> s.error(c, emptyRequest, done)).get();\n      fail(\"Should have thrown an exception\");\n    } catch (Exception e) {\n    }\n  }"
        ],
        [
            "Export::export(RpcController,ExportProtos,RpcCallback)",
            " 321  \n 322  \n 323  \n 324  \n 325  \n 326 -\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "  @Override\n  public void export(RpcController controller, ExportProtos.ExportRequest request,\n          RpcCallback<ExportProtos.ExportResponse> done) {\n    Region region = env.getRegion();\n    Configuration conf = HBaseConfiguration.create(env.getConfiguration());\n    conf.setStrings(\"io.serializations\", conf.get(\"io.serializations\"), ResultSerialization.class.getName());\n    try {\n      Scan scan = validateKey(region.getRegionInfo(), request);\n      Token userToken = null;\n      if (userProvider.isHadoopSecurityEnabled() && !request.hasFsToken()) {\n        LOG.warn(\"Hadoop security is enable, but no found of user token\");\n      } else if (userProvider.isHadoopSecurityEnabled()) {\n        userToken = new Token(request.getFsToken().getIdentifier().toByteArray(),\n                request.getFsToken().getPassword().toByteArray(),\n                new Text(request.getFsToken().getKind()),\n                new Text(request.getFsToken().getService()));\n      }\n      ExportProtos.ExportResponse response = processData(region, conf, userProvider,\n        scan, userToken, getWriterOptions(conf, region.getRegionInfo(), request));\n      done.run(response);\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n      LOG.error(e.toString(), e);\n    }\n  }",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  @Override\n  public void export(RpcController controller, ExportProtos.ExportRequest request,\n          RpcCallback<ExportProtos.ExportResponse> done) {\n    Region region = env.getRegion();\n    Configuration conf = HBaseConfiguration.create(env.getConfiguration());\n    conf.setStrings(\"io.serializations\", conf.get(\"io.serializations\"),\n        ResultSerialization.class.getName());\n    try {\n      Scan scan = validateKey(region.getRegionInfo(), request);\n      Token userToken = null;\n      if (userProvider.isHadoopSecurityEnabled() && !request.hasFsToken()) {\n        LOG.warn(\"Hadoop security is enable, but no found of user token\");\n      } else if (userProvider.isHadoopSecurityEnabled()) {\n        userToken = new Token(request.getFsToken().getIdentifier().toByteArray(),\n                request.getFsToken().getPassword().toByteArray(),\n                new Text(request.getFsToken().getKind()),\n                new Text(request.getFsToken().getService()));\n      }\n      ExportProtos.ExportResponse response = processData(region, conf, userProvider,\n        scan, userToken, getWriterOptions(conf, region.getRegionInfo(), request));\n      done.run(response);\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n      LOG.error(e.toString(), e);\n    }\n  }"
        ],
        [
            "TestServerCustomProtocol::verifyRegionResults(RegionLocator,Map,byte)",
            " 462 -\n 463 -\n 464  \n 465  ",
            "  private void verifyRegionResults(RegionLocator table,\n      Map<byte[],String> results, byte[] row) throws Exception {\n    verifyRegionResults(table, results, \"pong\", row);\n  }",
            " 466 +\n 467 +\n 468  \n 469  ",
            "  private void verifyRegionResults(RegionLocator table, Map<byte[],String> results, byte[] row)\n          throws Exception {\n    verifyRegionResults(table, results, \"pong\", row);\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::ConvertSecureBulkLoadHFilesRequest(SecureBulkLoadHFilesRequest)",
            " 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "  private BulkLoadHFileRequest ConvertSecureBulkLoadHFilesRequest(\n      SecureBulkLoadHFilesRequest request) {\n    BulkLoadHFileRequest.Builder bulkLoadHFileRequest = BulkLoadHFileRequest.newBuilder();\n    RegionSpecifier region =\n        ProtobufUtil.buildRegionSpecifier(RegionSpecifierType.REGION_NAME, this.env\n            .getRegionInfo().getRegionName());\n    bulkLoadHFileRequest.setRegion(region).setFsToken(request.getFsToken())\n        .setBulkToken(request.getBulkToken()).setAssignSeqNum(request.getAssignSeqNum())\n        .addAllFamilyPath(request.getFamilyPathList());\n    return bulkLoadHFileRequest.build();\n  }",
            " 173  \n 174 +\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "  private BulkLoadHFileRequest ConvertSecureBulkLoadHFilesRequest(\n          SecureBulkLoadHFilesRequest request) {\n    BulkLoadHFileRequest.Builder bulkLoadHFileRequest = BulkLoadHFileRequest.newBuilder();\n    RegionSpecifier region =\n        ProtobufUtil.buildRegionSpecifier(RegionSpecifierType.REGION_NAME, this.env\n            .getRegionInfo().getRegionName());\n    bulkLoadHFileRequest.setRegion(region).setFsToken(request.getFsToken())\n        .setBulkToken(request.getBulkToken()).setAssignSeqNum(request.getAssignSeqNum())\n        .addAllFamilyPath(request.getFamilyPathList());\n    return bulkLoadHFileRequest.build();\n  }"
        ],
        [
            "TestServerCustomProtocol::PingHandler::hello(RpcController,HelloRequest,RpcCallback)",
            " 116  \n 117  \n 118  \n 119 -\n 120 -\n 121 -\n 122  ",
            "    @Override\n    public void hello(RpcController controller, HelloRequest request,\n        RpcCallback<HelloResponse> done) {\n      if (!request.hasName()) done.run(HelloResponse.newBuilder().setResponse(WHOAREYOU).build());\n      else if (request.getName().equals(NOBODY)) done.run(HelloResponse.newBuilder().build());\n      else done.run(HelloResponse.newBuilder().setResponse(HELLO + request.getName()).build());\n    }",
            " 117  \n 118  \n 119  \n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127  ",
            "    @Override\n    public void hello(RpcController controller, HelloRequest request,\n        RpcCallback<HelloResponse> done) {\n      if (!request.hasName()) {\n        done.run(HelloResponse.newBuilder().setResponse(WHOAREYOU).build());\n      } else if (request.getName().equals(NOBODY)) {\n        done.run(HelloResponse.newBuilder().build());\n      } else {\n        done.run(HelloResponse.newBuilder().setResponse(HELLO + request.getName()).build());\n      }\n    }"
        ],
        [
            "TestServerCustomProtocol::hello(Table,String,String)",
            " 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  private Map<byte [], String> hello(final Table table, final String send, final String response)\n  throws ServiceException, Throwable {\n    Map<byte [], String> results = hello(table, send);\n    for (Map.Entry<byte [], String> e: results.entrySet()) {\n      assertEquals(\"Invalid custom protocol response\", response, e.getValue());\n    }\n    return results;\n  }",
            " 241  \n 242 +\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  ",
            "  private Map<byte [], String> hello(final Table table, final String send, final String response)\n          throws ServiceException, Throwable {\n    Map<byte [], String> results = hello(table, send);\n    for (Map.Entry<byte [], String> e: results.entrySet()) {\n      assertEquals(\"Invalid custom protocol response\", response, e.getValue());\n    }\n    return results;\n  }"
        ],
        [
            "Export::validateKey(RegionInfo,ExportProtos)",
            " 347 -\n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "  private Scan validateKey(final RegionInfo region, final ExportProtos.ExportRequest request) throws IOException {\n    Scan scan = ProtobufUtil.toScan(request.getScan());\n    byte[] regionStartKey = region.getStartKey();\n    byte[] originStartKey = scan.getStartRow();\n    if (originStartKey == null\n            || Bytes.compareTo(originStartKey, regionStartKey) < 0) {\n      scan.setStartRow(regionStartKey);\n    }\n    byte[] regionEndKey = region.getEndKey();\n    byte[] originEndKey = scan.getStopRow();\n    if (originEndKey == null\n            || Bytes.compareTo(originEndKey, regionEndKey) > 0) {\n      scan.setStartRow(regionEndKey);\n    }\n    return scan;\n  }",
            " 357 +\n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "  private Scan validateKey(final RegionInfo region, final ExportProtos.ExportRequest request)\n      throws IOException {\n    Scan scan = ProtobufUtil.toScan(request.getScan());\n    byte[] regionStartKey = region.getStartKey();\n    byte[] originStartKey = scan.getStartRow();\n    if (originStartKey == null\n            || Bytes.compareTo(originStartKey, regionStartKey) < 0) {\n      scan.setStartRow(regionStartKey);\n    }\n    byte[] regionEndKey = region.getEndKey();\n    byte[] originEndKey = scan.getStopRow();\n    if (originEndKey == null\n            || Bytes.compareTo(originEndKey, regionEndKey) > 0) {\n      scan.setStartRow(regionEndKey);\n    }\n    return scan;\n  }"
        ],
        [
            "TestServerCustomProtocol::hello(Table,String,byte,byte)",
            " 250  \n 251 -\n 252 -\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  \n 267  ",
            "  private Map<byte [], String> hello(final Table table, final String send, final byte [] start,\n      final byte [] end)\n  throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class,\n        start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();\n            if (send != null) builder.setName(send);\n            instance.hello(null, builder.build(), rpcCallback);\n            PingProtos.HelloResponse r = rpcCallback.get();\n            return r != null && r.hasResponse()? r.getResponse(): null;\n          }\n        });\n  }",
            " 255  \n 256 +\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 +\n 266 +\n 267 +\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "  private Map<byte [], String> hello(final Table table, final String send, final byte [] start,\n          final byte [] end) throws ServiceException, Throwable {\n    return table.coprocessorService(PingProtos.PingService.class,\n        start, end,\n        new Batch.Call<PingProtos.PingService, String>() {\n          @Override\n          public String call(PingProtos.PingService instance) throws IOException {\n            CoprocessorRpcUtils.BlockingRpcCallback<PingProtos.HelloResponse> rpcCallback =\n              new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            PingProtos.HelloRequest.Builder builder = PingProtos.HelloRequest.newBuilder();\n            if (send != null) {\n              builder.setName(send);\n            }\n            instance.hello(null, builder.build(), rpcCallback);\n            PingProtos.HelloResponse r = rpcCallback.get();\n            return r != null && r.hasResponse()? r.getResponse(): null;\n          }\n        });\n  }"
        ],
        [
            "TestCoprocessorTableEndpoint::testDynamicCoprocessorTableEndpoint()",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  ",
            "  @Test\n  public void testDynamicCoprocessorTableEndpoint() throws Throwable {\n    final TableName tableName = TableName.valueOf(name.getMethodName());\n\n    HTableDescriptor desc = new HTableDescriptor(tableName);\n    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));\n\n    createTable(desc);\n\n    desc.addCoprocessor(org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.class.getName());\n    updateTable(desc);\n\n    verifyTable(tableName);\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 +\n  99  \n 100  \n 101  \n 102  ",
            "  @Test\n  public void testDynamicCoprocessorTableEndpoint() throws Throwable {\n    final TableName tableName = TableName.valueOf(name.getMethodName());\n\n    HTableDescriptor desc = new HTableDescriptor(tableName);\n    desc.addFamily(new HColumnDescriptor(TEST_FAMILY));\n\n    createTable(desc);\n\n    desc.addCoprocessor(ColumnAggregationEndpoint.class.getName());\n    updateTable(desc);\n\n    verifyTable(tableName);\n  }"
        ],
        [
            "AggregateImplementation::getMedian(RpcController,AggregateRequest,RpcCallback)",
            " 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422 -\n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "  /**\n   * Gives a List containing sum of values and sum of weights.\n   * It is computed for the combination of column\n   * family and column qualifier(s) in the given row range as defined in the\n   * Scan object. In its current implementation, it takes one column family and\n   * two column qualifiers. The first qualifier is for values column and\n   * the second qualifier (optional) is for weight column.\n   */\n  @Override\n  public void getMedian(RpcController controller, AggregateRequest request,\n      RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null, sumWeights = null, tempVal = null, tempWeight = null;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] valQualifier = null, weightQualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        valQualifier = qualifiers.pollFirst();\n        // if weighted median is requested, get qualifier for the weight column\n        weightQualifier = qualifiers.pollLast();\n      }\n      List<Cell> results = new ArrayList<>();\n\n      boolean hasMoreRows = false;\n\n      do {\n        tempVal = null;\n        tempWeight = null;\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          Cell kv = results.get(i);\n          tempVal = ci.add(tempVal, ci.castToReturnType(ci.getValue(colFamily,\n              valQualifier, kv)));\n          if (weightQualifier != null) {\n            tempWeight = ci.add(tempWeight,\n                ci.castToReturnType(ci.getValue(colFamily, weightQualifier, kv)));\n          }\n        }\n        results.clear();\n        sumVal = ci.add(sumVal, tempVal);\n        sumWeights = ci.add(sumWeights, tempWeight);\n      } while (hasMoreRows);\n      ByteString first_sumVal = ci.getProtoForPromotedType(sumVal).toByteString();\n      S s = sumWeights == null ? ci.castToReturnType(ci.getMinValue()) : sumWeights;\n      ByteString first_sumWeights = ci.getProtoForPromotedType(s).toByteString();\n      AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n      pair.addFirstPart(first_sumVal);\n      pair.addFirstPart(first_sumWeights);\n      response = pair.build();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }",
            " 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  ",
            "  /**\n   * Gives a List containing sum of values and sum of weights.\n   * It is computed for the combination of column\n   * family and column qualifier(s) in the given row range as defined in the\n   * Scan object. In its current implementation, it takes one column family and\n   * two column qualifiers. The first qualifier is for values column and\n   * the second qualifier (optional) is for weight column.\n   */\n  @Override\n  public void getMedian(RpcController controller, AggregateRequest request,\n          RpcCallback<AggregateResponse> done) {\n    AggregateResponse response = null;\n    InternalScanner scanner = null;\n    try {\n      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);\n      S sumVal = null, sumWeights = null, tempVal = null, tempWeight = null;\n      Scan scan = ProtobufUtil.toScan(request.getScan());\n      scanner = env.getRegion().getScanner(scan);\n      byte[] colFamily = scan.getFamilies()[0];\n      NavigableSet<byte[]> qualifiers = scan.getFamilyMap().get(colFamily);\n      byte[] valQualifier = null, weightQualifier = null;\n      if (qualifiers != null && !qualifiers.isEmpty()) {\n        valQualifier = qualifiers.pollFirst();\n        // if weighted median is requested, get qualifier for the weight column\n        weightQualifier = qualifiers.pollLast();\n      }\n      List<Cell> results = new ArrayList<>();\n\n      boolean hasMoreRows = false;\n\n      do {\n        tempVal = null;\n        tempWeight = null;\n        hasMoreRows = scanner.next(results);\n        int listSize = results.size();\n        for (int i = 0; i < listSize; i++) {\n          Cell kv = results.get(i);\n          tempVal = ci.add(tempVal, ci.castToReturnType(ci.getValue(colFamily,\n              valQualifier, kv)));\n          if (weightQualifier != null) {\n            tempWeight = ci.add(tempWeight,\n                ci.castToReturnType(ci.getValue(colFamily, weightQualifier, kv)));\n          }\n        }\n        results.clear();\n        sumVal = ci.add(sumVal, tempVal);\n        sumWeights = ci.add(sumWeights, tempWeight);\n      } while (hasMoreRows);\n      ByteString first_sumVal = ci.getProtoForPromotedType(sumVal).toByteString();\n      S s = sumWeights == null ? ci.castToReturnType(ci.getMinValue()) : sumWeights;\n      ByteString first_sumWeights = ci.getProtoForPromotedType(s).toByteString();\n      AggregateResponse.Builder pair = AggregateResponse.newBuilder();\n      pair.addFirstPart(first_sumVal);\n      pair.addFirstPart(first_sumWeights);\n      response = pair.build();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    } finally {\n      if (scanner != null) {\n        try {\n          scanner.close();\n        } catch (IOException ignored) {}\n      }\n    }\n    done.run(response);\n  }"
        ],
        [
            "Export::getCompressionCodec(Configuration,ExportProtos)",
            " 169 -\n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "  private static CompressionCodec getCompressionCodec(final Configuration conf, final ExportProtos.ExportRequest request) {\n    try {\n      Class<? extends CompressionCodec> codecClass;\n      if (request.hasCompressCodec()) {\n        codecClass = conf.getClassByName(request.getCompressCodec()).asSubclass(CompressionCodec.class);\n      } else {\n        codecClass = DEFAULT_CODEC;\n      }\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } catch (ClassNotFoundException e) {\n      throw new IllegalArgumentException(\"Compression codec \"\n              + request.getCompressCodec() + \" was not found.\", e);\n    }\n  }",
            " 172 +\n 173 +\n 174  \n 175  \n 176  \n 177 +\n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "  private static CompressionCodec getCompressionCodec(final Configuration conf,\n      final ExportProtos.ExportRequest request) {\n    try {\n      Class<? extends CompressionCodec> codecClass;\n      if (request.hasCompressCodec()) {\n        codecClass = conf.getClassByName(request.getCompressCodec())\n            .asSubclass(CompressionCodec.class);\n      } else {\n        codecClass = DEFAULT_CODEC;\n      }\n      return ReflectionUtils.newInstance(codecClass, conf);\n    } catch (ClassNotFoundException e) {\n      throw new IllegalArgumentException(\"Compression codec \"\n              + request.getCompressCodec() + \" was not found.\", e);\n    }\n  }"
        ],
        [
            "Export::SecureWriter::getActiveUser(UserProvider,Token)",
            " 452 -\n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  ",
            "    private static User getActiveUser(final UserProvider userProvider, final Token userToken) throws IOException {\n      User user = RpcServer.getRequestUser().orElse(userProvider.getCurrent());\n      if (user == null && userToken != null) {\n        LOG.warn(\"No found of user credentials, but a token was got from user request\");\n      } else if (user != null && userToken != null) {\n        user.addToken(userToken);\n      }\n      return user;\n    }",
            " 463 +\n 464 +\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "    private static User getActiveUser(final UserProvider userProvider, final Token userToken)\n        throws IOException {\n      User user = RpcServer.getRequestUser().orElse(userProvider.getCurrent());\n      if (user == null && userToken != null) {\n        LOG.warn(\"No found of user credentials, but a token was got from user request\");\n      } else if (user != null && userToken != null) {\n        user.addToken(userToken);\n      }\n      return user;\n    }"
        ],
        [
            "AggregationClient::min(Table,ColumnInterpreter,Scan)",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260 -\n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 -\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  ",
            "  /**\n   * It gives the minimum value of a column for a given column family for the\n   * given range. In case qualifier is null, a min of all values for the given\n   * family is returned.\n   * @param table\n   * @param ci\n   * @param scan\n   * @return min val &lt;R&gt;\n   * @throws Throwable\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n  R min(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci,\n      final Scan scan) throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class MinCallBack implements Batch.Callback<R> {\n\n      private R min = null;\n\n      public R getMinimum() {\n        return min;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, R result) {\n        min = (min == null || (result != null && ci.compare(result, min) < 0)) ? result : min;\n      }\n    }\n    MinCallBack minCallBack = new MinCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, R>() {\n\n          @Override\n          public R call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMin(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() > 0) {\n              ByteString b = response.getFirstPart(0);\n              Q q = getParsedGenericInstance(ci.getClass(), 3, b);\n              return ci.getCellValueFromProto(q);\n            }\n            return null;\n          }\n        }, minCallBack);\n    log.debug(\"Min fom all regions is: \" + minCallBack.getMinimum());\n    return minCallBack.getMinimum();\n  }",
            " 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270 +\n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "  /**\n   * It gives the minimum value of a column for a given column family for the\n   * given range. In case qualifier is null, a min of all values for the given\n   * family is returned.\n   * @param table table to scan.\n   * @param ci the user's ColumnInterpreter implementation\n   * @param scan the HBase scan object to use to read data from HBase\n   * @return min val &lt;R&gt;\n   * @throws Throwable The caller is supposed to handle the exception as they are thrown\n   *           &amp; propagated to it.\n   */\n  public <R, S, P extends Message, Q extends Message, T extends Message>\n    R min(final Table table, final ColumnInterpreter<R, S, P, Q, T> ci, final Scan scan)\n          throws Throwable {\n    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);\n    class MinCallBack implements Batch.Callback<R> {\n      private R min = null;\n\n      public R getMinimum() {\n        return min;\n      }\n\n      @Override\n      public synchronized void update(byte[] region, byte[] row, R result) {\n        min = (min == null || (result != null && ci.compare(result, min) < 0)) ? result : min;\n      }\n    }\n\n    MinCallBack minCallBack = new MinCallBack();\n    table.coprocessorService(AggregateService.class, scan.getStartRow(), scan.getStopRow(),\n        new Batch.Call<AggregateService, R>() {\n          @Override\n          public R call(AggregateService instance) throws IOException {\n            RpcController controller = new AggregationClientRpcController();\n            CoprocessorRpcUtils.BlockingRpcCallback<AggregateResponse> rpcCallback =\n                new CoprocessorRpcUtils.BlockingRpcCallback<>();\n            instance.getMin(controller, requestArg, rpcCallback);\n            AggregateResponse response = rpcCallback.get();\n            if (controller.failed()) {\n              throw new IOException(controller.errorText());\n            }\n            if (response.getFirstPartCount() > 0) {\n              ByteString b = response.getFirstPart(0);\n              Q q = getParsedGenericInstance(ci.getClass(), 3, b);\n              return ci.getCellValueFromProto(q);\n            }\n            return null;\n          }\n        }, minCallBack);\n    log.debug(\"Min fom all regions is: \" + minCallBack.getMinimum());\n    return minCallBack.getMinimum();\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::secureBulkLoadHFiles(RpcController,SecureBulkLoadHFilesRequest,RpcCallback)",
            " 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "  @Override\n  public void secureBulkLoadHFiles(RpcController controller, SecureBulkLoadHFilesRequest request,\n      RpcCallback<SecureBulkLoadHFilesResponse> done) {\n    boolean loaded = false;\n    Map<byte[], List<Path>> map = null;\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n      BulkLoadHFileRequest bulkLoadHFileRequest = ConvertSecureBulkLoadHFilesRequest(request);\n      map = secureBulkLoadManager.secureBulkLoadHFiles((HRegion) this.env.getRegion(),\n          convert(bulkLoadHFileRequest));\n      loaded = map != null && !map.isEmpty();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(SecureBulkLoadHFilesResponse.newBuilder().setLoaded(loaded).build());\n  }",
            " 141  \n 142  \n 143 +\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "  @Override\n  public void secureBulkLoadHFiles(RpcController controller, SecureBulkLoadHFilesRequest request,\n          RpcCallback<SecureBulkLoadHFilesResponse> done) {\n    boolean loaded = false;\n    Map<byte[], List<Path>> map = null;\n    try {\n      SecureBulkLoadManager secureBulkLoadManager = this.rsServices.getSecureBulkLoadManager();\n      BulkLoadHFileRequest bulkLoadHFileRequest = ConvertSecureBulkLoadHFilesRequest(request);\n      map = secureBulkLoadManager.secureBulkLoadHFiles((HRegion) this.env.getRegion(),\n          convert(bulkLoadHFileRequest));\n      loaded = map != null && !map.isEmpty();\n    } catch (IOException e) {\n      CoprocessorRpcUtils.setControllerException(controller, e);\n    }\n    done.run(SecureBulkLoadHFilesResponse.newBuilder().setLoaded(loaded).build());\n  }"
        ],
        [
            "SecureBulkLoadEndpoint::convert(BulkLoadHFileRequest)",
            " 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest\n  convert(BulkLoadHFileRequest request)\n      throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.Builder\n        builder =\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.\n        newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }",
            " 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "  /**\n   *  Convert from CPEP protobuf 2.5 to internal protobuf 3.3.\n   */\n  org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest\n    convert(BulkLoadHFileRequest request)\n      throws org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException {\n    byte [] bytes = request.toByteArray();\n    org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.Builder\n        builder =\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.BulkLoadHFileRequest.\n        newBuilder();\n    builder.mergeFrom(bytes);\n    return builder.build();\n  }"
        ],
        [
            "TestServerCustomProtocol::verifyRegionResults(RegionLocator,Map,String,byte)",
            " 467 -\n 468 -\n 469 -\n 470  \n 471  \n 472 -\n 473 -\n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "  private void verifyRegionResults(RegionLocator regionLocator,\n      Map<byte[], String> results, String expected, byte[] row)\n  throws Exception {\n    for (Map.Entry<byte [], String> e: results.entrySet()) {\n      LOG.info(\"row=\" + Bytes.toString(row) + \", expected=\" + expected +\n       \", result key=\" + Bytes.toString(e.getKey()) +\n       \", value=\" + e.getValue());\n    }\n    HRegionLocation loc = regionLocator.getRegionLocation(row, true);\n    byte[] region = loc.getRegionInfo().getRegionName();\n    assertTrue(\"Results should contain region \" +\n      Bytes.toStringBinary(region) + \" for row '\" + Bytes.toStringBinary(row)+ \"'\",\n      results.containsKey(region));\n    assertEquals(\"Invalid result for row '\"+Bytes.toStringBinary(row)+\"'\",\n      expected, results.get(region));\n  }",
            " 471 +\n 472 +\n 473  \n 474  \n 475 +\n 476 +\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  ",
            "  private void verifyRegionResults(RegionLocator regionLocator, Map<byte[], String> results,\n          String expected, byte[] row) throws Exception {\n    for (Map.Entry<byte [], String> e: results.entrySet()) {\n      LOG.info(\"row=\" + Bytes.toString(row) + \", expected=\" + expected +\n        \", result key=\" + Bytes.toString(e.getKey()) +\n        \", value=\" + e.getValue());\n    }\n    HRegionLocation loc = regionLocator.getRegionLocation(row, true);\n    byte[] region = loc.getRegionInfo().getRegionName();\n    assertTrue(\"Results should contain region \" +\n      Bytes.toStringBinary(region) + \" for row '\" + Bytes.toStringBinary(row)+ \"'\",\n      results.containsKey(region));\n    assertEquals(\"Invalid result for row '\"+Bytes.toStringBinary(row)+\"'\",\n      expected, results.get(region));\n  }"
        ]
    ],
    "0db2b628d6db90b5ad300773d81e9cdb7dd0ebcd": [
        [
            "RegionLocationFinder::getTableDescriptor(TableName)",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 -\n 224 -\n 225  \n 226  \n 227  \n 228  ",
            "  /**\n   * return TableDescriptor for a given tableName\n   *\n   * @param tableName the table name\n   * @return TableDescriptor\n   * @throws IOException\n   */\n  protected TableDescriptor getTableDescriptor(TableName tableName) throws IOException {\n    TableDescriptor tableDescriptor = null;\n    try {\n      if (this.services != null && this.services.getTableDescriptors() != null) {\n        tableDescriptor = this.services.getTableDescriptors().get(tableName);\n      }\n    } catch (FileNotFoundException fnfe) {\n      LOG.debug(\"FileNotFoundException during getTableDescriptors.\" + \" Current table name = \"\n          + tableName, fnfe);\n    }\n\n    return tableDescriptor;\n  }",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224  \n 225  \n 226  \n 227  ",
            "  /**\n   * return TableDescriptor for a given tableName\n   *\n   * @param tableName the table name\n   * @return TableDescriptor\n   * @throws IOException\n   */\n  protected TableDescriptor getTableDescriptor(TableName tableName) throws IOException {\n    TableDescriptor tableDescriptor = null;\n    try {\n      if (this.services != null && this.services.getTableDescriptors() != null) {\n        tableDescriptor = this.services.getTableDescriptors().get(tableName);\n      }\n    } catch (FileNotFoundException fnfe) {\n      LOG.debug(\"tableName={}\", tableName, fnfe);\n    }\n\n    return tableDescriptor;\n  }"
        ],
        [
            "CleanerChore::CleanerTask::compute()",
            " 473  \n 474  \n 475 -\n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  ",
            "    @Override\n    protected Boolean compute() {\n      LOG.debug(\"Cleaning under {}\", dir);\n      List<FileStatus> subDirs;\n      List<FileStatus> files;\n      try {\n        // if dir doesn't exist, we'll get null back for both of these\n        // which will fall through to succeeding.\n        subDirs = getFilteredStatus(status -> status.isDirectory());\n        files = getFilteredStatus(status -> status.isFile());\n      } catch (IOException ioe) {\n        LOG.warn(\"failed to get FileStatus for contents of '{}'\", dir, ioe);\n        return false;\n      }\n\n      boolean nullSubDirs = subDirs == null;\n      if (nullSubDirs) {\n        LOG.trace(\"There is no subdir under {}\", dir);\n      }\n      if (files == null) {\n        LOG.trace(\"There is no file under {}\", dir);\n      }\n\n      int capacity = nullSubDirs ? 0 : subDirs.size();\n      List<CleanerTask> tasks = Lists.newArrayListWithCapacity(capacity);\n      if (!nullSubDirs) {\n        sortByConsumedSpace(subDirs);\n        for (FileStatus subdir : subDirs) {\n          CleanerTask task = new CleanerTask(subdir, false);\n          tasks.add(task);\n          task.fork();\n        }\n      }\n\n      boolean result = true;\n      result &= deleteAction(() -> checkAndDeleteFiles(files), \"files\");\n      result &= deleteAction(() -> getCleanResult(tasks), \"subdirs\");\n      // if and only if files and subdirs under current dir are deleted successfully, and\n      // it is not the root dir, then task will try to delete it.\n      if (result && !root) {\n        result &= deleteAction(() -> fs.delete(dir, false), \"dir\");\n      }\n      return result;\n    }",
            " 467  \n 468  \n 469 +\n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "    @Override\n    protected Boolean compute() {\n      LOG.trace(\"Cleaning under {}\", dir);\n      List<FileStatus> subDirs;\n      List<FileStatus> files;\n      try {\n        // if dir doesn't exist, we'll get null back for both of these\n        // which will fall through to succeeding.\n        subDirs = getFilteredStatus(status -> status.isDirectory());\n        files = getFilteredStatus(status -> status.isFile());\n      } catch (IOException ioe) {\n        LOG.warn(\"failed to get FileStatus for contents of '{}'\", dir, ioe);\n        return false;\n      }\n\n      boolean nullSubDirs = subDirs == null;\n      if (nullSubDirs) {\n        LOG.trace(\"There is no subdir under {}\", dir);\n      }\n      if (files == null) {\n        LOG.trace(\"There is no file under {}\", dir);\n      }\n\n      int capacity = nullSubDirs ? 0 : subDirs.size();\n      List<CleanerTask> tasks = Lists.newArrayListWithCapacity(capacity);\n      if (!nullSubDirs) {\n        sortByConsumedSpace(subDirs);\n        for (FileStatus subdir : subDirs) {\n          CleanerTask task = new CleanerTask(subdir, false);\n          tasks.add(task);\n          task.fork();\n        }\n      }\n\n      boolean result = true;\n      result &= deleteAction(() -> checkAndDeleteFiles(files), \"files\");\n      result &= deleteAction(() -> getCleanResult(tasks), \"subdirs\");\n      // if and only if files and subdirs under current dir are deleted successfully, and\n      // it is not the root dir, then task will try to delete it.\n      if (result && !root) {\n        result &= deleteAction(() -> fs.delete(dir, false), \"dir\");\n      }\n      return result;\n    }"
        ],
        [
            "RegionLocationFinder::getBlockDistribution(RegionInfo)",
            " 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 -\n 281 -\n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  ",
            "  public HDFSBlocksDistribution getBlockDistribution(RegionInfo hri) {\n    HDFSBlocksDistribution blockDistbn = null;\n    try {\n      if (cache.asMap().containsKey(hri)) {\n        blockDistbn = cache.get(hri);\n        return blockDistbn;\n      } else {\n        LOG.debug(\"HDFSBlocksDistribution not found in cache for region \"\n            + hri.getRegionNameAsString());\n        blockDistbn = internalGetTopBlockLocation(hri);\n        cache.put(hri, blockDistbn);\n        return blockDistbn;\n      }\n    } catch (ExecutionException e) {\n      LOG.warn(\"Error while fetching cache entry \", e);\n      blockDistbn = internalGetTopBlockLocation(hri);\n      cache.put(hri, blockDistbn);\n      return blockDistbn;\n    }\n  }",
            " 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  ",
            "  public HDFSBlocksDistribution getBlockDistribution(RegionInfo hri) {\n    HDFSBlocksDistribution blockDistbn = null;\n    try {\n      if (cache.asMap().containsKey(hri)) {\n        blockDistbn = cache.get(hri);\n        return blockDistbn;\n      } else {\n        LOG.trace(\"HDFSBlocksDistribution not found in cache for {}\", hri.getRegionNameAsString());\n        blockDistbn = internalGetTopBlockLocation(hri);\n        cache.put(hri, blockDistbn);\n        return blockDistbn;\n      }\n    } catch (ExecutionException e) {\n      LOG.warn(\"Error while fetching cache entry \", e);\n      blockDistbn = internalGetTopBlockLocation(hri);\n      cache.put(hri, blockDistbn);\n      return blockDistbn;\n    }\n  }"
        ],
        [
            "NettyHBaseSaslRpcClient::setupSaslHandler(ChannelPipeline)",
            "  46  \n  47  \n  48 -\n  49 -\n  50 -\n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "  public void setupSaslHandler(ChannelPipeline p) {\n    String qop = (String) saslClient.getNegotiatedProperty(Sasl.QOP);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"SASL client context established. Negotiated QoP: \" + qop);\n    }\n\n    if (qop == null || \"auth\".equalsIgnoreCase(qop)) {\n      return;\n    }\n    // add wrap and unwrap handlers to pipeline.\n    p.addFirst(new SaslWrapHandler(saslClient),\n      new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, 4, 0, 4),\n      new SaslUnwrapHandler(saslClient));\n  }",
            "  46  \n  47  \n  48 +\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  ",
            "  public void setupSaslHandler(ChannelPipeline p) {\n    String qop = (String) saslClient.getNegotiatedProperty(Sasl.QOP);\n    LOG.trace(\"SASL client context established. Negotiated QoP {}\", qop);\n    if (qop == null || \"auth\".equalsIgnoreCase(qop)) {\n      return;\n    }\n    // add wrap and unwrap handlers to pipeline.\n    p.addFirst(new SaslWrapHandler(saslClient),\n      new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, 4, 0, 4),\n      new SaslUnwrapHandler(saslClient));\n  }"
        ],
        [
            "AssignProcedure::reportTransition(MasterProcedureEnv,RegionStateNode,TransitionCode,long)",
            " 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295 -\n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  ",
            "  @Override\n  protected void reportTransition(final MasterProcedureEnv env, final RegionStateNode regionNode,\n      final TransitionCode code, final long openSeqNum) throws UnexpectedStateException {\n    switch (code) {\n      case OPENED:\n        if (openSeqNum < 0) {\n          throw new UnexpectedStateException(\"Received report unexpected \" + code +\n              \" transition openSeqNum=\" + openSeqNum + \", \" + regionNode);\n        }\n        if (openSeqNum < regionNode.getOpenSeqNum()) {\n          LOG.warn(\"Skipping update of open seqnum with \" + openSeqNum +\n              \" because current seqnum=\" + regionNode.getOpenSeqNum());\n        } else {\n          regionNode.setOpenSeqNum(openSeqNum);\n        }\n        // Leave the state here as OPENING for now. We set it to OPEN in\n        // REGION_TRANSITION_FINISH section where we do a bunch of checks.\n        // regionNode.setState(RegionState.State.OPEN, RegionState.State.OPENING);\n        setTransitionState(RegionTransitionState.REGION_TRANSITION_FINISH);\n        break;\n      case FAILED_OPEN:\n        handleFailure(env, regionNode);\n        break;\n      default:\n        throw new UnexpectedStateException(\"Received report unexpected \" + code +\n            \" transition openSeqNum=\" + openSeqNum + \", \" + regionNode.toShortString() +\n            \", \" + this + \", expected OPENED or FAILED_OPEN.\");\n    }\n  }",
            " 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295 +\n 296 +\n 297 +\n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "  @Override\n  protected void reportTransition(final MasterProcedureEnv env, final RegionStateNode regionNode,\n      final TransitionCode code, final long openSeqNum) throws UnexpectedStateException {\n    switch (code) {\n      case OPENED:\n        if (openSeqNum < 0) {\n          throw new UnexpectedStateException(\"Received report unexpected \" + code +\n              \" transition openSeqNum=\" + openSeqNum + \", \" + regionNode);\n        }\n        if (openSeqNum < regionNode.getOpenSeqNum()) {\n          // Don't bother logging if openSeqNum == 0\n          if (openSeqNum != 0) {\n            LOG.warn(\"Skipping update of open seqnum with \" + openSeqNum +\n                \" because current seqnum=\" + regionNode.getOpenSeqNum());\n          }\n        } else {\n          regionNode.setOpenSeqNum(openSeqNum);\n        }\n        // Leave the state here as OPENING for now. We set it to OPEN in\n        // REGION_TRANSITION_FINISH section where we do a bunch of checks.\n        // regionNode.setState(RegionState.State.OPEN, RegionState.State.OPENING);\n        setTransitionState(RegionTransitionState.REGION_TRANSITION_FINISH);\n        break;\n      case FAILED_OPEN:\n        handleFailure(env, regionNode);\n        break;\n      default:\n        throw new UnexpectedStateException(\"Received report unexpected \" + code +\n            \" transition openSeqNum=\" + openSeqNum + \", \" + regionNode.toShortString() +\n            \", \" + this + \", expected OPENED or FAILED_OPEN.\");\n    }\n  }"
        ],
        [
            "HFileCleaner::consumerLoop(BlockingQueue)",
            " 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 -\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n    try {\n      while (running) {\n        HFileDeleteTask task = null;\n        try {\n          task = queue.take();\n        } catch (InterruptedException e) {\n          LOG.trace(\"Interrupted while trying to take a task from queue\", e);\n          break;\n        }\n        if (task != null) {\n          LOG.debug(\"Removing {}\", task.filePath);\n          boolean succeed;\n          try {\n            succeed = this.fs.delete(task.filePath, false);\n          } catch (IOException e) {\n            LOG.warn(\"Failed to delete {}\", task.filePath, e);\n            succeed = false;\n          }\n          task.setResult(succeed);\n          if (succeed) {\n            countDeletedFiles(task.fileLength >= throttlePoint, queue == largeFileQueue);\n          }\n        }\n      }\n    } finally {\n      LOG.debug(\"Exit {}\", Thread.currentThread());\n    }\n  }",
            " 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n    try {\n      while (running) {\n        HFileDeleteTask task = null;\n        try {\n          task = queue.take();\n        } catch (InterruptedException e) {\n          LOG.trace(\"Interrupted while trying to take a task from queue\", e);\n          break;\n        }\n        if (task != null) {\n          LOG.trace(\"Removing {}\", task.filePath);\n          boolean succeed;\n          try {\n            succeed = this.fs.delete(task.filePath, false);\n          } catch (IOException e) {\n            LOG.warn(\"Failed to delete {}\", task.filePath, e);\n            succeed = false;\n          }\n          task.setResult(succeed);\n          if (succeed) {\n            countDeletedFiles(task.fileLength >= throttlePoint, queue == largeFileQueue);\n          }\n        }\n      }\n    } finally {\n      LOG.debug(\"Exit {}\", Thread.currentThread());\n    }\n  }"
        ],
        [
            "CleanerChore::chore()",
            " 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275 -\n 276 -\n 277 -\n 278  \n 279 -\n 280 -\n 281 -\n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295 -\n 296 -\n 297  \n 298  ",
            "  @Override\n  protected void chore() {\n    if (getEnabled()) {\n      try {\n        POOL.latchCountUp();\n        if (runCleaner()) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Cleaned all WALs under {}\", oldFileDir);\n          }\n        } else {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"WALs outstanding under {}\", oldFileDir);\n          }\n        }\n      } finally {\n        POOL.latchCountDown();\n      }\n      // After each cleaner chore, checks if received reconfigure notification while cleaning.\n      // First in cleaner turns off notification, to avoid another cleaner updating pool again.\n      if (POOL.reconfigNotification.compareAndSet(true, false)) {\n        // This cleaner is waiting for other cleaners finishing their jobs.\n        // To avoid missing next chore, only wait 0.8 * period, then shutdown.\n        POOL.updatePool((long) (0.8 * getTimeUnit().toMillis(getPeriod())));\n      }\n    } else {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Cleaner chore disabled! Not cleaning.\");\n      }\n    }\n  }",
            " 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275 +\n 276  \n 277 +\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 +\n 291  \n 292  ",
            "  @Override\n  protected void chore() {\n    if (getEnabled()) {\n      try {\n        POOL.latchCountUp();\n        if (runCleaner()) {\n          LOG.trace(\"Cleaned all WALs under {}\", oldFileDir);\n        } else {\n          LOG.trace(\"WALs outstanding under {}\", oldFileDir);\n        }\n      } finally {\n        POOL.latchCountDown();\n      }\n      // After each cleaner chore, checks if received reconfigure notification while cleaning.\n      // First in cleaner turns off notification, to avoid another cleaner updating pool again.\n      if (POOL.reconfigNotification.compareAndSet(true, false)) {\n        // This cleaner is waiting for other cleaners finishing their jobs.\n        // To avoid missing next chore, only wait 0.8 * period, then shutdown.\n        POOL.updatePool((long) (0.8 * getTimeUnit().toMillis(getPeriod())));\n      }\n    } else {\n      LOG.trace(\"Cleaner chore disabled! Not cleaning.\");\n    }\n  }"
        ],
        [
            "HFileCleaner::countDeletedFiles(boolean,boolean)",
            " 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  \n 267 -\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  private void countDeletedFiles(boolean isLargeFile, boolean fromLargeQueue) {\n    if (isLargeFile) {\n      if (deletedLargeFiles.get() == Long.MAX_VALUE) {\n        LOG.info(\"Deleted more than Long.MAX_VALUE large files, reset counter to 0\");\n        deletedLargeFiles.set(0L);\n      }\n      deletedLargeFiles.incrementAndGet();\n    } else {\n      if (deletedSmallFiles.get() == Long.MAX_VALUE) {\n        LOG.info(\"Deleted more than Long.MAX_VALUE small files, reset counter to 0\");\n        deletedSmallFiles.set(0L);\n      }\n      if (fromLargeQueue) {\n        LOG.trace(\"Stolen a small file deletion task in large file thread\");\n      }\n      deletedSmallFiles.incrementAndGet();\n    }\n  }",
            " 258  \n 259  \n 260  \n 261 +\n 262  \n 263  \n 264  \n 265  \n 266  \n 267 +\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  private void countDeletedFiles(boolean isLargeFile, boolean fromLargeQueue) {\n    if (isLargeFile) {\n      if (deletedLargeFiles.get() == Long.MAX_VALUE) {\n        LOG.debug(\"Deleted more than Long.MAX_VALUE large files, reset counter to 0\");\n        deletedLargeFiles.set(0L);\n      }\n      deletedLargeFiles.incrementAndGet();\n    } else {\n      if (deletedSmallFiles.get() == Long.MAX_VALUE) {\n        LOG.debug(\"Deleted more than Long.MAX_VALUE small files, reset counter to 0\");\n        deletedSmallFiles.set(0L);\n      }\n      if (fromLargeQueue) {\n        LOG.trace(\"Stolen a small file deletion task in large file thread\");\n      }\n      deletedSmallFiles.incrementAndGet();\n    }\n  }"
        ],
        [
            "NettyHBaseSaslRpcClientHandler::writeResponse(ChannelHandlerContext,byte)",
            "  73  \n  74 -\n  75 -\n  76 -\n  77  \n  78  \n  79  ",
            "  private void writeResponse(ChannelHandlerContext ctx, byte[] response) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Will send token of size \" + response.length + \" from initSASLContext.\");\n    }\n    ctx.writeAndFlush(\n      ctx.alloc().buffer(4 + response.length).writeInt(response.length).writeBytes(response));\n  }",
            "  73  \n  74 +\n  75  \n  76  \n  77  ",
            "  private void writeResponse(ChannelHandlerContext ctx, byte[] response) {\n    LOG.trace(\"Sending token size={} from initSASLContext.\", response.length);\n    ctx.writeAndFlush(\n      ctx.alloc().buffer(4 + response.length).writeInt(response.length).writeBytes(response));\n  }"
        ],
        [
            "NettyHBaseSaslRpcClientHandler::channelRead0(ChannelHandlerContext,ByteBuf)",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137 -\n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @Override\n  protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception {\n    int len = msg.readInt();\n    if (len == SaslUtil.SWITCH_TO_SIMPLE_AUTH) {\n      saslRpcClient.dispose();\n      if (saslRpcClient.fallbackAllowed) {\n        saslPromise.trySuccess(false);\n      } else {\n        saslPromise.tryFailure(new FallbackDisallowedException());\n      }\n      return;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Will read input token of size \" + len + \" for processing by initSASLContext\");\n    }\n    final byte[] challenge = new byte[len];\n    msg.readBytes(challenge);\n    byte[] response = ugi.doAs(new PrivilegedExceptionAction<byte[]>() {\n\n      @Override\n      public byte[] run() throws Exception {\n        return saslRpcClient.evaluateChallenge(challenge);\n      }\n    });\n    if (response != null) {\n      writeResponse(ctx, response);\n    }\n    tryComplete(ctx);\n  }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "  @Override\n  protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception {\n    int len = msg.readInt();\n    if (len == SaslUtil.SWITCH_TO_SIMPLE_AUTH) {\n      saslRpcClient.dispose();\n      if (saslRpcClient.fallbackAllowed) {\n        saslPromise.trySuccess(false);\n      } else {\n        saslPromise.tryFailure(new FallbackDisallowedException());\n      }\n      return;\n    }\n    LOG.trace(\"Reading input token size={} for processing by initSASLContext\", len);\n    final byte[] challenge = new byte[len];\n    msg.readBytes(challenge);\n    byte[] response = ugi.doAs(new PrivilegedExceptionAction<byte[]>() {\n\n      @Override\n      public byte[] run() throws Exception {\n        return saslRpcClient.evaluateChallenge(challenge);\n      }\n    });\n    if (response != null) {\n      writeResponse(ctx, response);\n    }\n    tryComplete(ctx);\n  }"
        ]
    ],
    "c365c4084e185cfd4bab711f9ce8924654445ac9": [
        [
            "DisableTableProcedure::executeFromState(MasterProcedureEnv,DisableTableState)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  ",
            "  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, final DisableTableState state)\n      throws InterruptedException {\n    LOG.trace(\"{} execute state={}\", this, state);\n    try {\n      switch (state) {\n        case DISABLE_TABLE_PREPARE:\n          if (prepareDisable(env)) {\n            setNextState(DisableTableState.DISABLE_TABLE_PRE_OPERATION);\n          } else {\n            assert isFailed() : \"disable should have an exception here\";\n            return Flow.NO_MORE_STATE;\n          }\n          break;\n        case DISABLE_TABLE_PRE_OPERATION:\n          preDisable(env, state);\n          setNextState(DisableTableState.DISABLE_TABLE_SET_DISABLING_TABLE_STATE);\n          break;\n        case DISABLE_TABLE_SET_DISABLING_TABLE_STATE:\n          setTableStateToDisabling(env, tableName);\n          setNextState(DisableTableState.DISABLE_TABLE_MARK_REGIONS_OFFLINE);\n          break;\n        case DISABLE_TABLE_MARK_REGIONS_OFFLINE:\n          addChildProcedure(env.getAssignmentManager().createUnassignProcedures(tableName));\n          setNextState(DisableTableState.DISABLE_TABLE_ADD_REPLICATION_BARRIER);\n          break;\n        case DISABLE_TABLE_ADD_REPLICATION_BARRIER:\n          if (env.getMasterServices().getTableDescriptors().get(tableName)\n            .hasGlobalReplicationScope()) {\n            MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n            try (BufferedMutator mutator = env.getMasterServices().getConnection()\n              .getBufferedMutator(TableName.META_TABLE_NAME)) {\n              for (RegionInfo region : env.getAssignmentManager().getRegionStates()\n                .getRegionsOfTable(tableName)) {\n                long maxSequenceId =\n                  WALSplitter.getMaxRegionSequenceId(mfs.getFileSystem(), mfs.getRegionDir(region));\n                long openSeqNum = maxSequenceId > 0 ? maxSequenceId + 1 : HConstants.NO_SEQNUM;\n                mutator.mutate(MetaTableAccessor.makePutForReplicationBarrier(region, openSeqNum,\n                  EnvironmentEdgeManager.currentTime()));\n              }\n            }\n          }\n          setNextState(DisableTableState.DISABLE_TABLE_SET_DISABLED_TABLE_STATE);\n          break;\n        case DISABLE_TABLE_SET_DISABLED_TABLE_STATE:\n          setTableStateToDisabled(env, tableName);\n          setNextState(DisableTableState.DISABLE_TABLE_POST_OPERATION);\n          break;\n        case DISABLE_TABLE_POST_OPERATION:\n          postDisable(env, state);\n          return Flow.NO_MORE_STATE;\n        default:\n          throw new UnsupportedOperationException(\"Unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-disable-table\", e);\n      } else {\n        LOG.warn(\"Retriable error trying to disable table={} (in state={})\", tableName, state, e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146  \n 147  \n 148  \n 149  ",
            "  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, final DisableTableState state)\n      throws InterruptedException {\n    LOG.trace(\"{} execute state={}\", this, state);\n    try {\n      switch (state) {\n        case DISABLE_TABLE_PREPARE:\n          if (prepareDisable(env)) {\n            setNextState(DisableTableState.DISABLE_TABLE_PRE_OPERATION);\n          } else {\n            assert isFailed() : \"disable should have an exception here\";\n            return Flow.NO_MORE_STATE;\n          }\n          break;\n        case DISABLE_TABLE_PRE_OPERATION:\n          preDisable(env, state);\n          setNextState(DisableTableState.DISABLE_TABLE_SET_DISABLING_TABLE_STATE);\n          break;\n        case DISABLE_TABLE_SET_DISABLING_TABLE_STATE:\n          setTableStateToDisabling(env, tableName);\n          setNextState(DisableTableState.DISABLE_TABLE_MARK_REGIONS_OFFLINE);\n          break;\n        case DISABLE_TABLE_MARK_REGIONS_OFFLINE:\n          addChildProcedure(env.getAssignmentManager().createUnassignProcedures(tableName));\n          setNextState(DisableTableState.DISABLE_TABLE_ADD_REPLICATION_BARRIER);\n          break;\n        case DISABLE_TABLE_ADD_REPLICATION_BARRIER:\n          if (env.getMasterServices().getTableDescriptors().get(tableName)\n            .hasGlobalReplicationScope()) {\n            MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n            try (BufferedMutator mutator = env.getMasterServices().getConnection()\n              .getBufferedMutator(TableName.META_TABLE_NAME)) {\n              for (RegionInfo region : env.getAssignmentManager().getRegionStates()\n                .getRegionsOfTable(tableName)) {\n                long maxSequenceId =\n                  WALSplitter.getMaxRegionSequenceId(mfs.getFileSystem(), mfs.getRegionDir(region));\n                long openSeqNum = maxSequenceId > 0 ? maxSequenceId + 1 : HConstants.NO_SEQNUM;\n                mutator.mutate(MetaTableAccessor.makePutForReplicationBarrier(region, openSeqNum,\n                  EnvironmentEdgeManager.currentTime()));\n              }\n            }\n          }\n          setNextState(DisableTableState.DISABLE_TABLE_SET_DISABLED_TABLE_STATE);\n          break;\n        case DISABLE_TABLE_SET_DISABLED_TABLE_STATE:\n          setTableStateToDisabled(env, tableName);\n          setNextState(DisableTableState.DISABLE_TABLE_POST_OPERATION);\n          break;\n        case DISABLE_TABLE_POST_OPERATION:\n          postDisable(env, state);\n          return Flow.NO_MORE_STATE;\n        default:\n          throw new UnsupportedOperationException(\"Unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-disable-table\", e);\n      } else {\n        LOG.warn(\"Retryable error in {}\", this, e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }"
        ],
        [
            "DisableTableProcedure::prepareDisable(MasterProcedureEnv)",
            " 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "  /**\n   * Action before any real action of disabling table. Set the exception in the procedure instead\n   * of throwing it.  This approach is to deal with backward compatible with 1.0.\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private boolean prepareDisable(final MasterProcedureEnv env) throws IOException {\n    boolean canTableBeDisabled = true;\n    if (tableName.equals(TableName.META_TABLE_NAME)) {\n      setFailure(\"master-disable-table\", new ConstraintException(\"Cannot disable catalog table\"));\n      canTableBeDisabled = false;\n    } else if (!MetaTableAccessor.tableExists(env.getMasterServices().getConnection(), tableName)) {\n      setFailure(\"master-disable-table\", new TableNotFoundException(tableName));\n      canTableBeDisabled = false;\n    } else if (!skipTableStateCheck) {\n      // There could be multiple client requests trying to disable or enable\n      // the table at the same time. Ensure only the first request is honored\n      // After that, no other requests can be accepted until the table reaches\n      // DISABLED or ENABLED.\n      //\n      // Note: in 1.0 release, we called TableStateManager.setTableStateIfInStates() to set\n      // the state to DISABLING from ENABLED. The implementation was done before table lock\n      // was implemented. With table lock, there is no need to set the state here (it will\n      // set the state later on). A quick state check should be enough for us to move forward.\n      TableStateManager tsm = env.getMasterServices().getTableStateManager();\n      TableState ts = tsm.getTableState(tableName);\n      if (!ts.isEnabled()) {\n        LOG.info(\"Not ENABLED tableState=\" + ts + \"; skipping disable\");\n        setFailure(\"master-disable-table\", new TableNotEnabledException(ts.toString()));\n        canTableBeDisabled = false;\n      }\n    }\n\n    // We are done the check. Future actions in this procedure could be done asynchronously.\n    releaseSyncLatch();\n\n    return canTableBeDisabled;\n  }",
            " 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "  /**\n   * Action before any real action of disabling table. Set the exception in the procedure instead\n   * of throwing it.  This approach is to deal with backward compatible with 1.0.\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private boolean prepareDisable(final MasterProcedureEnv env) throws IOException {\n    boolean canTableBeDisabled = true;\n    if (tableName.equals(TableName.META_TABLE_NAME)) {\n      setFailure(\"master-disable-table\", new ConstraintException(\"Cannot disable catalog table\"));\n      canTableBeDisabled = false;\n    } else if (!MetaTableAccessor.tableExists(env.getMasterServices().getConnection(), tableName)) {\n      setFailure(\"master-disable-table\", new TableNotFoundException(tableName));\n      canTableBeDisabled = false;\n    } else if (!skipTableStateCheck) {\n      // There could be multiple client requests trying to disable or enable\n      // the table at the same time. Ensure only the first request is honored\n      // After that, no other requests can be accepted until the table reaches\n      // DISABLED or ENABLED.\n      //\n      // Note: in 1.0 release, we called TableStateManager.setTableStateIfInStates() to set\n      // the state to DISABLING from ENABLED. The implementation was done before table lock\n      // was implemented. With table lock, there is no need to set the state here (it will\n      // set the state later on). A quick state check should be enough for us to move forward.\n      TableStateManager tsm = env.getMasterServices().getTableStateManager();\n      TableState ts = tsm.getTableState(tableName);\n      if (!ts.isEnabled()) {\n        LOG.info(\"Not ENABLED skipping {}\", this);\n        setFailure(\"master-disable-table\", new TableNotEnabledException(ts.toString()));\n        canTableBeDisabled = false;\n      }\n    }\n\n    // We are done the check. Future actions in this procedure could be done asynchronously.\n    releaseSyncLatch();\n\n    return canTableBeDisabled;\n  }"
        ],
        [
            "DeleteTableProcedure::executeFromState(MasterProcedureEnv,DeleteTableState)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 -\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 -\n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, DeleteTableState state)\n      throws InterruptedException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(this + \" execute state=\" + state);\n    }\n    try {\n      switch (state) {\n        case DELETE_TABLE_PRE_OPERATION:\n          // Verify if we can delete the table\n          boolean deletable = prepareDelete(env);\n          releaseSyncLatch();\n          if (!deletable) {\n            assert isFailed() : \"the delete should have an exception here\";\n            return Flow.NO_MORE_STATE;\n          }\n\n          // TODO: Move out... in the acquireLock()\n          LOG.debug(\"Waiting for '\" + getTableName() + \"' regions in transition\");\n          regions = env.getAssignmentManager().getRegionStates().getRegionsOfTable(getTableName());\n          assert regions != null && !regions.isEmpty() : \"unexpected 0 regions\";\n          ProcedureSyncWait.waitRegionInTransition(env, regions);\n\n          // Call coprocessors\n          preDelete(env);\n\n          setNextState(DeleteTableState.DELETE_TABLE_REMOVE_FROM_META);\n          break;\n        case DELETE_TABLE_REMOVE_FROM_META:\n          LOG.debug(\"delete '\" + getTableName() + \"' regions from META\");\n          DeleteTableProcedure.deleteFromMeta(env, getTableName(), regions);\n          setNextState(DeleteTableState.DELETE_TABLE_CLEAR_FS_LAYOUT);\n          break;\n        case DELETE_TABLE_CLEAR_FS_LAYOUT:\n          LOG.debug(\"delete '\" + getTableName() + \"' from filesystem\");\n          DeleteTableProcedure.deleteFromFs(env, getTableName(), regions, true);\n          setNextState(DeleteTableState.DELETE_TABLE_UPDATE_DESC_CACHE);\n          regions = null;\n          break;\n        case DELETE_TABLE_UPDATE_DESC_CACHE:\n          LOG.debug(\"delete '\" + getTableName() + \"' descriptor\");\n          DeleteTableProcedure.deleteTableDescriptorCache(env, getTableName());\n          setNextState(DeleteTableState.DELETE_TABLE_UNASSIGN_REGIONS);\n          break;\n        case DELETE_TABLE_UNASSIGN_REGIONS:\n          LOG.debug(\"delete '\" + getTableName() + \"' assignment state\");\n          DeleteTableProcedure.deleteAssignmentState(env, getTableName());\n          setNextState(DeleteTableState.DELETE_TABLE_POST_OPERATION);\n          break;\n        case DELETE_TABLE_POST_OPERATION:\n          postDelete(env);\n          LOG.debug(\"delete '\" + getTableName() + \"' completed\");\n          return Flow.NO_MORE_STATE;\n        default:\n          throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-delete-table\", e);\n      } else {\n        LOG.warn(\"Retriable error trying to delete table=\" + getTableName() + \" state=\" + state, e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  @Override\n  protected Flow executeFromState(final MasterProcedureEnv env, DeleteTableState state)\n      throws InterruptedException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(this + \" execute state=\" + state);\n    }\n    try {\n      switch (state) {\n        case DELETE_TABLE_PRE_OPERATION:\n          // Verify if we can delete the table\n          boolean deletable = prepareDelete(env);\n          releaseSyncLatch();\n          if (!deletable) {\n            assert isFailed() : \"the delete should have an exception here\";\n            return Flow.NO_MORE_STATE;\n          }\n\n          // TODO: Move out... in the acquireLock()\n          LOG.debug(\"Waiting for RIT for {}\", this);\n          regions = env.getAssignmentManager().getRegionStates().getRegionsOfTable(getTableName());\n          assert regions != null && !regions.isEmpty() : \"unexpected 0 regions\";\n          ProcedureSyncWait.waitRegionInTransition(env, regions);\n\n          // Call coprocessors\n          preDelete(env);\n\n          setNextState(DeleteTableState.DELETE_TABLE_REMOVE_FROM_META);\n          break;\n        case DELETE_TABLE_REMOVE_FROM_META:\n          LOG.debug(\"Deleting regions from META for {}\", this);\n          DeleteTableProcedure.deleteFromMeta(env, getTableName(), regions);\n          setNextState(DeleteTableState.DELETE_TABLE_CLEAR_FS_LAYOUT);\n          break;\n        case DELETE_TABLE_CLEAR_FS_LAYOUT:\n          LOG.debug(\"Deleting regions from filesystem for {}\", this);\n          DeleteTableProcedure.deleteFromFs(env, getTableName(), regions, true);\n          setNextState(DeleteTableState.DELETE_TABLE_UPDATE_DESC_CACHE);\n          regions = null;\n          break;\n        case DELETE_TABLE_UPDATE_DESC_CACHE:\n          LOG.debug(\"Deleting descriptor for {}\", this);\n          DeleteTableProcedure.deleteTableDescriptorCache(env, getTableName());\n          setNextState(DeleteTableState.DELETE_TABLE_UNASSIGN_REGIONS);\n          break;\n        case DELETE_TABLE_UNASSIGN_REGIONS:\n          LOG.debug(\"Deleting assignment state for {}\", this);\n          DeleteTableProcedure.deleteAssignmentState(env, getTableName());\n          setNextState(DeleteTableState.DELETE_TABLE_POST_OPERATION);\n          break;\n        case DELETE_TABLE_POST_OPERATION:\n          postDelete(env);\n          LOG.debug(\"Finished {}\", this);\n          return Flow.NO_MORE_STATE;\n        default:\n          throw new UnsupportedOperationException(\"unhandled state=\" + state);\n      }\n    } catch (IOException e) {\n      if (isRollbackSupported(state)) {\n        setFailure(\"master-delete-table\", e);\n      } else {\n        LOG.warn(\"Retriable error trying to delete table=\" + getTableName() + \" state=\" + state, e);\n      }\n    }\n    return Flow.HAS_MORE_STATE;\n  }"
        ],
        [
            "DisableTableProcedure::setTableStateToDisabled(MasterProcedureEnv,TableName)",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 -\n 310  ",
            "  /**\n   * Mark table state to Disabled\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  protected static void setTableStateToDisabled(\n      final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    // Flip the table to disabled\n    env.getMasterServices().getTableStateManager().setTableState(\n      tableName,\n      TableState.State.DISABLED);\n    LOG.info(\"Disabled table, \" + tableName + \", is completed.\");\n  }",
            " 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310 +\n 311  ",
            "  /**\n   * Mark table state to Disabled\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  protected static void setTableStateToDisabled(\n      final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    // Flip the table to disabled\n    env.getMasterServices().getTableStateManager().setTableState(\n      tableName,\n      TableState.State.DISABLED);\n    LOG.info(\"Set {} to state={}\", tableName, TableState.State.DISABLED);\n  }"
        ],
        [
            "ProcedureExecutor::execProcedure(RootProcedureState,Procedure)",
            "1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503 -\n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  ",
            "  /**\n   * Executes <code>procedure</code>\n   * <ul>\n   *  <li>Calls the doExecute() of the procedure\n   *  <li>If the procedure execution didn't fail (i.e. valid user input)\n   *  <ul>\n   *    <li>...and returned subprocedures\n   *    <ul><li>The subprocedures are initialized.\n   *      <li>The subprocedures are added to the store\n   *      <li>The subprocedures are added to the runnable queue\n   *      <li>The procedure is now in a WAITING state, waiting for the subprocedures to complete\n   *    </ul>\n   *    </li>\n   *   <li>...if there are no subprocedure\n   *    <ul><li>the procedure completed successfully\n   *      <li>if there is a parent (WAITING)\n   *      <li>the parent state will be set to RUNNABLE\n   *    </ul>\n   *   </li>\n   *  </ul>\n   *  </li>\n   *  <li>In case of failure\n   *  <ul>\n   *    <li>The store is updated with the new state</li>\n   *    <li>The executor (caller of this method) will start the rollback of the procedure</li>\n   *  </ul>\n   *  </li>\n   *  </ul>\n   */\n  private void execProcedure(RootProcedureState<TEnvironment> procStack,\n      Procedure<TEnvironment> procedure) {\n    Preconditions.checkArgument(procedure.getState() == ProcedureState.RUNNABLE,\n      procedure.toString());\n\n    // Procedures can suspend themselves. They skip out by throwing a ProcedureSuspendedException.\n    // The exception is caught below and then we hurry to the exit without disturbing state. The\n    // idea is that the processing of this procedure will be unsuspended later by an external event\n    // such the report of a region open.\n    boolean suspended = false;\n\n    // Whether to 're-' -execute; run through the loop again.\n    boolean reExecute = false;\n\n    Procedure<TEnvironment>[] subprocs = null;\n    do {\n      reExecute = false;\n      try {\n        subprocs = procedure.doExecute(getEnvironment());\n        if (subprocs != null && subprocs.length == 0) {\n          subprocs = null;\n        }\n      } catch (ProcedureSuspendedException e) {\n        LOG.trace(\"Suspend {}\", procedure);\n        suspended = true;\n      } catch (ProcedureYieldException e) {\n        LOG.trace(\"Yield {}\", procedure, e);\n        yieldProcedure(procedure);\n        return;\n      } catch (InterruptedException e) {\n        LOG.trace(\"Yield interrupt {}\", procedure, e);\n        handleInterruptedException(procedure, e);\n        yieldProcedure(procedure);\n        return;\n      } catch (Throwable e) {\n        // Catch NullPointerExceptions or similar errors...\n        String msg = \"CODE-BUG: Uncaught runtime exception: \" + procedure;\n        LOG.error(msg, e);\n        procedure.setFailure(new RemoteProcedureException(msg, e));\n      }\n\n      if (!procedure.isFailed()) {\n        if (subprocs != null) {\n          if (subprocs.length == 1 && subprocs[0] == procedure) {\n            // Procedure returned itself. Quick-shortcut for a state machine-like procedure;\n            // i.e. we go around this loop again rather than go back out on the scheduler queue.\n            subprocs = null;\n            reExecute = true;\n            LOG.trace(\"Short-circuit to next step on pid={}\", procedure.getProcId());\n          } else {\n            // Yield the current procedure, and make the subprocedure runnable\n            // subprocs may come back 'null'.\n            subprocs = initializeChildren(procStack, procedure, subprocs);\n            LOG.info(\"Initialized subprocedures=\" +\n              (subprocs == null? null:\n                Stream.of(subprocs).map(e -> \"{\" + e.toString() + \"}\").\n                collect(Collectors.toList()).toString()));\n          }\n        } else if (procedure.getState() == ProcedureState.WAITING_TIMEOUT) {\n          LOG.trace(\"Added to timeoutExecutor {}\", procedure);\n          timeoutExecutor.add(procedure);\n        } else if (!suspended) {\n          // No subtask, so we are done\n          procedure.setState(ProcedureState.SUCCESS);\n        }\n      }\n\n      // Add the procedure to the stack\n      procStack.addRollbackStep(procedure);\n\n      // allows to kill the executor before something is stored to the wal.\n      // useful to test the procedure recovery.\n      if (testing != null && testing.shouldKillBeforeStoreUpdate(suspended)) {\n        String msg = \"TESTING: Kill before store update: \" + procedure;\n        LOG.debug(msg);\n        stop();\n        throw new RuntimeException(msg);\n      }\n\n      // TODO: The code here doesn't check if store is running before persisting to the store as\n      // it relies on the method call below to throw RuntimeException to wind up the stack and\n      // executor thread to stop. The statement following the method call below seems to check if\n      // store is not running, to prevent scheduling children procedures, re-execution or yield\n      // of this procedure. This may need more scrutiny and subsequent cleanup in future\n      //\n      // Commit the transaction even if a suspend (state may have changed). Note this append\n      // can take a bunch of time to complete.\n      updateStoreOnExec(procStack, procedure, subprocs);\n\n      // if the store is not running we are aborting\n      if (!store.isRunning()) {\n        return;\n      }\n      // if the procedure is kind enough to pass the slot to someone else, yield\n      if (procedure.isRunnable() && !suspended &&\n          procedure.isYieldAfterExecutionStep(getEnvironment())) {\n        yieldProcedure(procedure);\n        return;\n      }\n\n      assert (reExecute && subprocs == null) || !reExecute;\n    } while (reExecute);\n    // Submit the new subprocedures\n    if (subprocs != null && !procedure.isFailed()) {\n      submitChildrenProcedures(subprocs);\n    }\n\n    // we need to log the release lock operation before waking up the parent procedure, as there\n    // could be race that the parent procedure may call updateStoreOnExec ahead of us and remove all\n    // the sub procedures from store and cause problems...\n    releaseLock(procedure, false);\n\n    // if the procedure is complete and has a parent, count down the children latch.\n    // If 'suspended', do nothing to change state -- let other threads handle unsuspend event.\n    if (!suspended && procedure.isFinished() && procedure.hasParent()) {\n      countDownChildren(procStack, procedure);\n    }\n  }",
            "1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503 +\n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  ",
            "  /**\n   * Executes <code>procedure</code>\n   * <ul>\n   *  <li>Calls the doExecute() of the procedure\n   *  <li>If the procedure execution didn't fail (i.e. valid user input)\n   *  <ul>\n   *    <li>...and returned subprocedures\n   *    <ul><li>The subprocedures are initialized.\n   *      <li>The subprocedures are added to the store\n   *      <li>The subprocedures are added to the runnable queue\n   *      <li>The procedure is now in a WAITING state, waiting for the subprocedures to complete\n   *    </ul>\n   *    </li>\n   *   <li>...if there are no subprocedure\n   *    <ul><li>the procedure completed successfully\n   *      <li>if there is a parent (WAITING)\n   *      <li>the parent state will be set to RUNNABLE\n   *    </ul>\n   *   </li>\n   *  </ul>\n   *  </li>\n   *  <li>In case of failure\n   *  <ul>\n   *    <li>The store is updated with the new state</li>\n   *    <li>The executor (caller of this method) will start the rollback of the procedure</li>\n   *  </ul>\n   *  </li>\n   *  </ul>\n   */\n  private void execProcedure(RootProcedureState<TEnvironment> procStack,\n      Procedure<TEnvironment> procedure) {\n    Preconditions.checkArgument(procedure.getState() == ProcedureState.RUNNABLE,\n        \"NOT RUNNABLE! \" + procedure.toString());\n\n    // Procedures can suspend themselves. They skip out by throwing a ProcedureSuspendedException.\n    // The exception is caught below and then we hurry to the exit without disturbing state. The\n    // idea is that the processing of this procedure will be unsuspended later by an external event\n    // such the report of a region open.\n    boolean suspended = false;\n\n    // Whether to 're-' -execute; run through the loop again.\n    boolean reExecute = false;\n\n    Procedure<TEnvironment>[] subprocs = null;\n    do {\n      reExecute = false;\n      try {\n        subprocs = procedure.doExecute(getEnvironment());\n        if (subprocs != null && subprocs.length == 0) {\n          subprocs = null;\n        }\n      } catch (ProcedureSuspendedException e) {\n        LOG.trace(\"Suspend {}\", procedure);\n        suspended = true;\n      } catch (ProcedureYieldException e) {\n        LOG.trace(\"Yield {}\", procedure, e);\n        yieldProcedure(procedure);\n        return;\n      } catch (InterruptedException e) {\n        LOG.trace(\"Yield interrupt {}\", procedure, e);\n        handleInterruptedException(procedure, e);\n        yieldProcedure(procedure);\n        return;\n      } catch (Throwable e) {\n        // Catch NullPointerExceptions or similar errors...\n        String msg = \"CODE-BUG: Uncaught runtime exception: \" + procedure;\n        LOG.error(msg, e);\n        procedure.setFailure(new RemoteProcedureException(msg, e));\n      }\n\n      if (!procedure.isFailed()) {\n        if (subprocs != null) {\n          if (subprocs.length == 1 && subprocs[0] == procedure) {\n            // Procedure returned itself. Quick-shortcut for a state machine-like procedure;\n            // i.e. we go around this loop again rather than go back out on the scheduler queue.\n            subprocs = null;\n            reExecute = true;\n            LOG.trace(\"Short-circuit to next step on pid={}\", procedure.getProcId());\n          } else {\n            // Yield the current procedure, and make the subprocedure runnable\n            // subprocs may come back 'null'.\n            subprocs = initializeChildren(procStack, procedure, subprocs);\n            LOG.info(\"Initialized subprocedures=\" +\n              (subprocs == null? null:\n                Stream.of(subprocs).map(e -> \"{\" + e.toString() + \"}\").\n                collect(Collectors.toList()).toString()));\n          }\n        } else if (procedure.getState() == ProcedureState.WAITING_TIMEOUT) {\n          LOG.trace(\"Added to timeoutExecutor {}\", procedure);\n          timeoutExecutor.add(procedure);\n        } else if (!suspended) {\n          // No subtask, so we are done\n          procedure.setState(ProcedureState.SUCCESS);\n        }\n      }\n\n      // Add the procedure to the stack\n      procStack.addRollbackStep(procedure);\n\n      // allows to kill the executor before something is stored to the wal.\n      // useful to test the procedure recovery.\n      if (testing != null && testing.shouldKillBeforeStoreUpdate(suspended)) {\n        String msg = \"TESTING: Kill before store update: \" + procedure;\n        LOG.debug(msg);\n        stop();\n        throw new RuntimeException(msg);\n      }\n\n      // TODO: The code here doesn't check if store is running before persisting to the store as\n      // it relies on the method call below to throw RuntimeException to wind up the stack and\n      // executor thread to stop. The statement following the method call below seems to check if\n      // store is not running, to prevent scheduling children procedures, re-execution or yield\n      // of this procedure. This may need more scrutiny and subsequent cleanup in future\n      //\n      // Commit the transaction even if a suspend (state may have changed). Note this append\n      // can take a bunch of time to complete.\n      updateStoreOnExec(procStack, procedure, subprocs);\n\n      // if the store is not running we are aborting\n      if (!store.isRunning()) {\n        return;\n      }\n      // if the procedure is kind enough to pass the slot to someone else, yield\n      if (procedure.isRunnable() && !suspended &&\n          procedure.isYieldAfterExecutionStep(getEnvironment())) {\n        yieldProcedure(procedure);\n        return;\n      }\n\n      assert (reExecute && subprocs == null) || !reExecute;\n    } while (reExecute);\n    // Submit the new subprocedures\n    if (subprocs != null && !procedure.isFailed()) {\n      submitChildrenProcedures(subprocs);\n    }\n\n    // we need to log the release lock operation before waking up the parent procedure, as there\n    // could be race that the parent procedure may call updateStoreOnExec ahead of us and remove all\n    // the sub procedures from store and cause problems...\n    releaseLock(procedure, false);\n\n    // if the procedure is complete and has a parent, count down the children latch.\n    // If 'suspended', do nothing to change state -- let other threads handle unsuspend event.\n    if (!suspended && procedure.isFinished() && procedure.hasParent()) {\n      countDownChildren(procStack, procedure);\n    }\n  }"
        ],
        [
            "AbstractStateMachineTableProcedure::checkOnline(MasterProcedureEnv,RegionInfo)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "  /**\n   * Check region is online.\n   */\n  protected static void checkOnline(MasterProcedureEnv env, final RegionInfo ri)\n      throws DoNotRetryRegionException {\n    RegionStates regionStates = env.getAssignmentManager().getRegionStates();\n    RegionState rs = regionStates.getRegionState(ri);\n    if (rs == null) {\n      throw new UnknownRegionException(\"No RegionState found for \" + ri.getEncodedName());\n    }\n    if (!rs.isOpened()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() + \" is not OPEN\");\n    }\n    if (ri.isSplitParent()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() +\n          \" is not online (splitParent=true)\");\n    }\n    if (ri.isSplit()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() + \" has split=true\");\n    }\n    if (ri.isOffline()) {\n      // RegionOfflineException is not instance of DNRIOE so wrap it.\n      throw new DoNotRetryRegionException(new RegionOfflineException(ri.getEncodedName()));\n    }\n  }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "  /**\n   * Check region is online.\n   */\n  protected static void checkOnline(MasterProcedureEnv env, final RegionInfo ri)\n      throws DoNotRetryRegionException {\n    RegionStates regionStates = env.getAssignmentManager().getRegionStates();\n    RegionState rs = regionStates.getRegionState(ri);\n    if (rs == null) {\n      throw new UnknownRegionException(\"No RegionState found for \" + ri.getEncodedName());\n    }\n    if (!rs.isOpened()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() + \" is not OPEN; regionState=\" + rs);\n    }\n    if (ri.isSplitParent()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() +\n          \" is not online (splitParent=true)\");\n    }\n    if (ri.isSplit()) {\n      throw new DoNotRetryRegionException(ri.getEncodedName() + \" has split=true\");\n    }\n    if (ri.isOffline()) {\n      // RegionOfflineException is not instance of DNRIOE so wrap it.\n      throw new DoNotRetryRegionException(new RegionOfflineException(ri.getEncodedName()));\n    }\n  }"
        ],
        [
            "DisableTableProcedure::setTableStateToDisabling(MasterProcedureEnv,TableName)",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "  /**\n   * Mark table state to Disabling\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  protected static void setTableStateToDisabling(\n      final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    // Set table disabling flag up in zk.\n    env.getMasterServices().getTableStateManager().setTableState(\n      tableName,\n      TableState.State.DISABLING);\n  }",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 +\n 296  ",
            "  /**\n   * Mark table state to Disabling\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  protected static void setTableStateToDisabling(\n      final MasterProcedureEnv env,\n      final TableName tableName) throws IOException {\n    // Set table disabling flag up in zk.\n    env.getMasterServices().getTableStateManager().setTableState(\n      tableName,\n      TableState.State.DISABLING);\n    LOG.info(\"Set {} to state={}\", tableName, TableState.State.DISABLING);\n  }"
        ],
        [
            "AsyncRequestFutureImpl::createLog(int,int,int,ServerName,Throwable,long,boolean,String,int,int)",
            " 951  \n 952  \n 953  \n 954  \n 955 -\n 956 -\n 957 -\n 958  \n 959  \n 960 -\n 961 -\n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970 -\n 971  \n 972  \n 973 -\n 974  \n 975  \n 976 -\n 977  \n 978 -\n 979  \n 980  \n 981  \n 982  ",
            "  private String createLog(int numAttempt, int failureCount, int replaySize, ServerName sn,\n                           Throwable error, long backOffTime, boolean willRetry, String startTime,\n                           int failed, int stopped) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"id=\").append(asyncProcess.id).append(\", table=\").append(tableName).append(\", \")\n        .append(\"attempt=\").append(numAttempt)\n        .append(\"/\").append(asyncProcess.numTries).append(\", \");\n\n    if (failureCount > 0 || error != null){\n      sb.append(\"failed=\").append(failureCount).append(\"ops\").append(\", last exception=\").\n          append(error == null ? \"null\" : error);\n    } else {\n      sb.append(\"succeeded\");\n    }\n\n    sb.append(\" on \").append(sn).append(\", tracking started \").append(startTime);\n\n    if (willRetry) {\n      sb.append(\", retrying after=\").append(backOffTime).append(\"ms\").\n          append(\", replay=\").append(replaySize).append(\"ops\");\n    } else if (failureCount > 0) {\n      if (stopped > 0) {\n        sb.append(\"; not retrying \").append(stopped).append(\" due to success from other replica\");\n      }\n      if (failed > 0) {\n        sb.append(\"; not retrying \").append(failed).append(\" - final failure\");\n      }\n\n    }\n\n    return sb.toString();\n  }",
            " 951  \n 952  \n 953  \n 954  \n 955 +\n 956 +\n 957 +\n 958  \n 959  \n 960 +\n 961 +\n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970 +\n 971  \n 972  \n 973 +\n 974 +\n 975  \n 976  \n 977 +\n 978  \n 979  \n 980  \n 981  \n 982  ",
            "  private String createLog(int numAttempt, int failureCount, int replaySize, ServerName sn,\n                           Throwable error, long backOffTime, boolean willRetry, String startTime,\n                           int failed, int stopped) {\n    StringBuilder sb = new StringBuilder();\n    sb.append(\"id=\").append(asyncProcess.id).append(\", table=\").append(tableName).\n        append(\", attempt=\").append(numAttempt).append(\"/\").append(asyncProcess.numTries).\n        append(\", \");\n\n    if (failureCount > 0 || error != null){\n      sb.append(\"failureCount=\").append(failureCount).append(\"ops\").append(\", last exception=\").\n          append(error);\n    } else {\n      sb.append(\"succeeded\");\n    }\n\n    sb.append(\" on \").append(sn).append(\", tracking started \").append(startTime);\n\n    if (willRetry) {\n      sb.append(\", retrying after=\").append(backOffTime).append(\"ms\").\n          append(\", operationsToReplay=\").append(replaySize);\n    } else if (failureCount > 0) {\n      if (stopped > 0) {\n        sb.append(\"; NOT retrying, stopped=\").append(stopped).\n            append(\" because successful operation on other replica\");\n      }\n      if (failed > 0) {\n        sb.append(\"; NOT retrying, failed=\").append(failed).append(\" -- final attempt!\");\n      }\n    }\n\n    return sb.toString();\n  }"
        ],
        [
            "HRegion::doClose(boolean,MonitoredTask)",
            "1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539 -\n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"UL_UNRELEASED_LOCK_EXCEPTION_PATH\",\n      justification=\"I think FindBugs is confused\")\n  private Map<byte[], List<HStoreFile>> doClose(boolean abort, MonitoredTask status)\n      throws IOException {\n    if (isClosed()) {\n      LOG.warn(\"Region \" + this + \" already closed\");\n      return null;\n    }\n\n    if (coprocessorHost != null) {\n      status.setStatus(\"Running coprocessor pre-close hooks\");\n      this.coprocessorHost.preClose(abort);\n    }\n    status.setStatus(\"Disabling compacts and flushes for region\");\n    boolean canFlush = true;\n    synchronized (writestate) {\n      // Disable compacting and flushing by background threads for this\n      // region.\n      canFlush = !writestate.readOnly;\n      writestate.writesEnabled = false;\n      LOG.debug(\"Closing {}, disabling compactions & flushes\",\n          this.getRegionInfo().getEncodedName());\n      waitForFlushesAndCompactions();\n    }\n    // If we were not just flushing, is it worth doing a preflush...one\n    // that will clear out of the bulk of the memstore before we put up\n    // the close flag?\n    if (!abort && worthPreFlushing() && canFlush) {\n      status.setStatus(\"Pre-flushing region before close\");\n      LOG.info(\"Running close preflush of {}\" + this.getRegionInfo().getEncodedName());\n      try {\n        internalFlushcache(status);\n      } catch (IOException ioe) {\n        // Failed to flush the region. Keep going.\n        status.setStatus(\"Failed pre-flush \" + this + \"; \" + ioe.getMessage());\n      }\n    }\n\n    if (timeoutForWriteLock == null\n        || timeoutForWriteLock == Long.MAX_VALUE) {\n      // block waiting for the lock for closing\n      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n    } else {\n      try {\n        boolean succeed = lock.writeLock().tryLock(timeoutForWriteLock, TimeUnit.SECONDS);\n        if (!succeed) {\n          throw new IOException(\"Failed to get write lock when closing region\");\n        }\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      }\n    }\n    this.closing.set(true);\n    status.setStatus(\"Disabling writes for close\");\n    try {\n      if (this.isClosed()) {\n        status.abort(\"Already got closed by another process\");\n        // SplitTransaction handles the null\n        return null;\n      }\n      LOG.debug(\"Updates disabled for region \" + this);\n      // Don't flush the cache if we are aborting\n      if (!abort && canFlush) {\n        int failedfFlushCount = 0;\n        int flushCount = 0;\n        long tmp = 0;\n        long remainingSize = this.memStoreSizing.getDataSize();\n        while (remainingSize > 0) {\n          try {\n            internalFlushcache(status);\n            if(flushCount >0) {\n              LOG.info(\"Running extra flush, \" + flushCount +\n                  \" (carrying snapshot?) \" + this);\n            }\n            flushCount++;\n            tmp = this.memStoreSizing.getDataSize();\n            if (tmp >= remainingSize) {\n              failedfFlushCount++;\n            }\n            remainingSize = tmp;\n            if (failedfFlushCount > 5) {\n              // If we failed 5 times and are unable to clear memory, abort\n              // so we do not lose data\n              throw new DroppedSnapshotException(\"Failed clearing memory after \" +\n                  flushCount + \" attempts on region: \" +\n                  Bytes.toStringBinary(getRegionInfo().getRegionName()));\n            }\n          } catch (IOException ioe) {\n            status.setStatus(\"Failed flush \" + this + \", putting online again\");\n            synchronized (writestate) {\n              writestate.writesEnabled = true;\n            }\n            // Have to throw to upper layers.  I can't abort server from here.\n            throw ioe;\n          }\n        }\n      }\n\n      Map<byte[], List<HStoreFile>> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n      if (!stores.isEmpty()) {\n        // initialize the thread pool for closing stores in parallel.\n        ThreadPoolExecutor storeCloserThreadPool =\n          getStoreOpenAndCloseThreadPool(\"StoreCloserThread-\" +\n            getRegionInfo().getRegionNameAsString());\n        CompletionService<Pair<byte[], Collection<HStoreFile>>> completionService =\n          new ExecutorCompletionService<>(storeCloserThreadPool);\n\n        // close each store in parallel\n        for (HStore store : stores.values()) {\n          MemStoreSize mss = store.getFlushableSize();\n          if (!(abort || mss.getDataSize() == 0 || writestate.readOnly)) {\n            if (getRegionServerServices() != null) {\n              getRegionServerServices().abort(\"Assertion failed while closing store \"\n                + getRegionInfo().getRegionNameAsString() + \" \" + store\n                + \". flushableSize expected=0, actual={\" + mss\n                + \"}. Current memStoreSize=\" + this.memStoreSizing.getMemStoreSize() +\n                  \". Maybe a coprocessor \"\n                + \"operation failed and left the memstore in a partially updated state.\", null);\n            }\n          }\n          completionService\n              .submit(new Callable<Pair<byte[], Collection<HStoreFile>>>() {\n                @Override\n                public Pair<byte[], Collection<HStoreFile>> call() throws IOException {\n                  return new Pair<>(store.getColumnFamilyDescriptor().getName(), store.close());\n                }\n              });\n        }\n        try {\n          for (int i = 0; i < stores.size(); i++) {\n            Future<Pair<byte[], Collection<HStoreFile>>> future = completionService.take();\n            Pair<byte[], Collection<HStoreFile>> storeFiles = future.get();\n            List<HStoreFile> familyFiles = result.get(storeFiles.getFirst());\n            if (familyFiles == null) {\n              familyFiles = new ArrayList<>();\n              result.put(storeFiles.getFirst(), familyFiles);\n            }\n            familyFiles.addAll(storeFiles.getSecond());\n          }\n        } catch (InterruptedException e) {\n          throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          if (cause instanceof IOException) {\n            throw (IOException) cause;\n          }\n          throw new IOException(cause);\n        } finally {\n          storeCloserThreadPool.shutdownNow();\n        }\n      }\n\n      status.setStatus(\"Writing region close event to WAL\");\n      // Always write close marker to wal even for read only table. This is not a big problem as we\n      // do not write any data into the region.\n      if (!abort && wal != null && getRegionServerServices() != null &&\n        RegionReplicaUtil.isDefaultReplica(getRegionInfo())) {\n        writeRegionCloseMarker(wal);\n      }\n\n      this.closed.set(true);\n      if (!canFlush) {\n        decrMemStoreSize(this.memStoreSizing.getMemStoreSize());\n      } else if (this.memStoreSizing.getDataSize() != 0) {\n        LOG.error(\"Memstore data size is {}\", this.memStoreSizing.getDataSize());\n      }\n      if (coprocessorHost != null) {\n        status.setStatus(\"Running coprocessor post-close hooks\");\n        this.coprocessorHost.postClose(abort);\n      }\n      if (this.metricsRegion != null) {\n        this.metricsRegion.close();\n      }\n      if (this.metricsRegionWrapper != null) {\n        Closeables.close(this.metricsRegionWrapper, true);\n      }\n      status.markComplete(\"Closed\");\n      LOG.info(\"Closed \" + this);\n      return result;\n    } finally {\n      lock.writeLock().unlock();\n    }\n  }",
            "1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539 +\n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"UL_UNRELEASED_LOCK_EXCEPTION_PATH\",\n      justification=\"I think FindBugs is confused\")\n  private Map<byte[], List<HStoreFile>> doClose(boolean abort, MonitoredTask status)\n      throws IOException {\n    if (isClosed()) {\n      LOG.warn(\"Region \" + this + \" already closed\");\n      return null;\n    }\n\n    if (coprocessorHost != null) {\n      status.setStatus(\"Running coprocessor pre-close hooks\");\n      this.coprocessorHost.preClose(abort);\n    }\n    status.setStatus(\"Disabling compacts and flushes for region\");\n    boolean canFlush = true;\n    synchronized (writestate) {\n      // Disable compacting and flushing by background threads for this\n      // region.\n      canFlush = !writestate.readOnly;\n      writestate.writesEnabled = false;\n      LOG.debug(\"Closing {}, disabling compactions & flushes\",\n          this.getRegionInfo().getEncodedName());\n      waitForFlushesAndCompactions();\n    }\n    // If we were not just flushing, is it worth doing a preflush...one\n    // that will clear out of the bulk of the memstore before we put up\n    // the close flag?\n    if (!abort && worthPreFlushing() && canFlush) {\n      status.setStatus(\"Pre-flushing region before close\");\n      LOG.info(\"Running close preflush of {}\", this.getRegionInfo().getEncodedName());\n      try {\n        internalFlushcache(status);\n      } catch (IOException ioe) {\n        // Failed to flush the region. Keep going.\n        status.setStatus(\"Failed pre-flush \" + this + \"; \" + ioe.getMessage());\n      }\n    }\n\n    if (timeoutForWriteLock == null\n        || timeoutForWriteLock == Long.MAX_VALUE) {\n      // block waiting for the lock for closing\n      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n    } else {\n      try {\n        boolean succeed = lock.writeLock().tryLock(timeoutForWriteLock, TimeUnit.SECONDS);\n        if (!succeed) {\n          throw new IOException(\"Failed to get write lock when closing region\");\n        }\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      }\n    }\n    this.closing.set(true);\n    status.setStatus(\"Disabling writes for close\");\n    try {\n      if (this.isClosed()) {\n        status.abort(\"Already got closed by another process\");\n        // SplitTransaction handles the null\n        return null;\n      }\n      LOG.debug(\"Updates disabled for region \" + this);\n      // Don't flush the cache if we are aborting\n      if (!abort && canFlush) {\n        int failedfFlushCount = 0;\n        int flushCount = 0;\n        long tmp = 0;\n        long remainingSize = this.memStoreSizing.getDataSize();\n        while (remainingSize > 0) {\n          try {\n            internalFlushcache(status);\n            if(flushCount >0) {\n              LOG.info(\"Running extra flush, \" + flushCount +\n                  \" (carrying snapshot?) \" + this);\n            }\n            flushCount++;\n            tmp = this.memStoreSizing.getDataSize();\n            if (tmp >= remainingSize) {\n              failedfFlushCount++;\n            }\n            remainingSize = tmp;\n            if (failedfFlushCount > 5) {\n              // If we failed 5 times and are unable to clear memory, abort\n              // so we do not lose data\n              throw new DroppedSnapshotException(\"Failed clearing memory after \" +\n                  flushCount + \" attempts on region: \" +\n                  Bytes.toStringBinary(getRegionInfo().getRegionName()));\n            }\n          } catch (IOException ioe) {\n            status.setStatus(\"Failed flush \" + this + \", putting online again\");\n            synchronized (writestate) {\n              writestate.writesEnabled = true;\n            }\n            // Have to throw to upper layers.  I can't abort server from here.\n            throw ioe;\n          }\n        }\n      }\n\n      Map<byte[], List<HStoreFile>> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n      if (!stores.isEmpty()) {\n        // initialize the thread pool for closing stores in parallel.\n        ThreadPoolExecutor storeCloserThreadPool =\n          getStoreOpenAndCloseThreadPool(\"StoreCloserThread-\" +\n            getRegionInfo().getRegionNameAsString());\n        CompletionService<Pair<byte[], Collection<HStoreFile>>> completionService =\n          new ExecutorCompletionService<>(storeCloserThreadPool);\n\n        // close each store in parallel\n        for (HStore store : stores.values()) {\n          MemStoreSize mss = store.getFlushableSize();\n          if (!(abort || mss.getDataSize() == 0 || writestate.readOnly)) {\n            if (getRegionServerServices() != null) {\n              getRegionServerServices().abort(\"Assertion failed while closing store \"\n                + getRegionInfo().getRegionNameAsString() + \" \" + store\n                + \". flushableSize expected=0, actual={\" + mss\n                + \"}. Current memStoreSize=\" + this.memStoreSizing.getMemStoreSize() +\n                  \". Maybe a coprocessor \"\n                + \"operation failed and left the memstore in a partially updated state.\", null);\n            }\n          }\n          completionService\n              .submit(new Callable<Pair<byte[], Collection<HStoreFile>>>() {\n                @Override\n                public Pair<byte[], Collection<HStoreFile>> call() throws IOException {\n                  return new Pair<>(store.getColumnFamilyDescriptor().getName(), store.close());\n                }\n              });\n        }\n        try {\n          for (int i = 0; i < stores.size(); i++) {\n            Future<Pair<byte[], Collection<HStoreFile>>> future = completionService.take();\n            Pair<byte[], Collection<HStoreFile>> storeFiles = future.get();\n            List<HStoreFile> familyFiles = result.get(storeFiles.getFirst());\n            if (familyFiles == null) {\n              familyFiles = new ArrayList<>();\n              result.put(storeFiles.getFirst(), familyFiles);\n            }\n            familyFiles.addAll(storeFiles.getSecond());\n          }\n        } catch (InterruptedException e) {\n          throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n        } catch (ExecutionException e) {\n          Throwable cause = e.getCause();\n          if (cause instanceof IOException) {\n            throw (IOException) cause;\n          }\n          throw new IOException(cause);\n        } finally {\n          storeCloserThreadPool.shutdownNow();\n        }\n      }\n\n      status.setStatus(\"Writing region close event to WAL\");\n      // Always write close marker to wal even for read only table. This is not a big problem as we\n      // do not write any data into the region.\n      if (!abort && wal != null && getRegionServerServices() != null &&\n        RegionReplicaUtil.isDefaultReplica(getRegionInfo())) {\n        writeRegionCloseMarker(wal);\n      }\n\n      this.closed.set(true);\n      if (!canFlush) {\n        decrMemStoreSize(this.memStoreSizing.getMemStoreSize());\n      } else if (this.memStoreSizing.getDataSize() != 0) {\n        LOG.error(\"Memstore data size is {}\", this.memStoreSizing.getDataSize());\n      }\n      if (coprocessorHost != null) {\n        status.setStatus(\"Running coprocessor post-close hooks\");\n        this.coprocessorHost.postClose(abort);\n      }\n      if (this.metricsRegion != null) {\n        this.metricsRegion.close();\n      }\n      if (this.metricsRegionWrapper != null) {\n        Closeables.close(this.metricsRegionWrapper, true);\n      }\n      status.markComplete(\"Closed\");\n      LOG.info(\"Closed \" + this);\n      return result;\n    } finally {\n      lock.writeLock().unlock();\n    }\n  }"
        ]
    ],
    "e7f6c2972dba2bc1eff8a5ae39893603508336ea": [
        [
            "TestServerCrashProcedure::setupConf(Configuration)",
            "  63  \n  64  \n  65  \n  66 -\n  67  ",
            "  private void setupConf(Configuration conf) {\n    conf.setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n    conf.set(\"hbase.balancer.tablesOnMaster\", \"none\");\n    conf.setInt(\"hbase.client.retries.number\", 3);\n  }",
            "  64  \n  65  \n  66  \n  67 +\n  68 +\n  69  ",
            "  private void setupConf(Configuration conf) {\n    conf.setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n    conf.set(\"hbase.balancer.tablesOnMaster\", \"none\");\n    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 3);\n    conf.setInt(HConstants.HBASE_CLIENT_SERVERSIDE_RETRIES_MULTIPLIER, 3);\n  }"
        ],
        [
            "ProcedureTestingUtility::restart(ProcedureExecutor,boolean,boolean,Callable,Callable,Callable,boolean,boolean)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143 -\n 144 -\n 145 -\n 146  ",
            "  public static <TEnv> void restart(ProcedureExecutor<TEnv> procExecutor,\n      boolean avoidTestKillDuringRestart, boolean failOnCorrupted, Callable<Void> stopAction,\n      Callable<Void> actionBeforeStartWorker, Callable<Void> startAction, boolean abort,\n      boolean startWorkers) throws Exception {\n    final ProcedureStore procStore = procExecutor.getStore();\n    final int storeThreads = procExecutor.getCorePoolSize();\n    final int execThreads = procExecutor.getCorePoolSize();\n\n    final ProcedureExecutor.Testing testing = procExecutor.testing;\n    if (avoidTestKillDuringRestart) {\n      procExecutor.testing = null;\n    }\n\n    // stop\n    LOG.info(\"RESTART - Stop\");\n    procExecutor.stop();\n    procStore.stop(abort);\n    if (stopAction != null) {\n      stopAction.call();\n    }\n    procExecutor.join();\n    procExecutor.getScheduler().clear();\n\n    // nothing running...\n\n    // re-start\n    LOG.info(\"RESTART - Start\");\n    procStore.start(storeThreads);\n    procExecutor.init(execThreads, failOnCorrupted);\n    if (actionBeforeStartWorker != null) {\n      actionBeforeStartWorker.call();\n    }\n    if (startWorkers) {\n      procExecutor.startWorkers();\n    }\n    if (startAction != null) {\n      startAction.call();\n    }\n\n    if (avoidTestKillDuringRestart) {\n      procExecutor.testing = testing;\n    }\n  }",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137 +\n 138 +\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  ",
            "  public static <TEnv> void restart(ProcedureExecutor<TEnv> procExecutor,\n      boolean avoidTestKillDuringRestart, boolean failOnCorrupted, Callable<Void> stopAction,\n      Callable<Void> actionBeforeStartWorker, Callable<Void> startAction, boolean abort,\n      boolean startWorkers) throws Exception {\n    final ProcedureStore procStore = procExecutor.getStore();\n    final int storeThreads = procExecutor.getCorePoolSize();\n    final int execThreads = procExecutor.getCorePoolSize();\n\n    final ProcedureExecutor.Testing testing = procExecutor.testing;\n    if (avoidTestKillDuringRestart) {\n      procExecutor.testing = null;\n    }\n\n    // stop\n    LOG.info(\"RESTART - Stop\");\n    procExecutor.stop();\n    procStore.stop(abort);\n    if (stopAction != null) {\n      stopAction.call();\n    }\n    procExecutor.join();\n    procExecutor.getScheduler().clear();\n\n    // nothing running...\n\n    // re-start\n    LOG.info(\"RESTART - Start\");\n    procStore.start(storeThreads);\n    procExecutor.init(execThreads, failOnCorrupted);\n    if (actionBeforeStartWorker != null) {\n      actionBeforeStartWorker.call();\n    }\n    if (avoidTestKillDuringRestart) {\n      procExecutor.testing = testing;\n    }\n    if (startWorkers) {\n      procExecutor.startWorkers();\n    }\n    if (startAction != null) {\n      startAction.call();\n    }\n  }"
        ],
        [
            "TestMergeTableRegionsProcedure::testMergeWithoutPONR()",
            " 276  \n 277  \n 278 -\n 279 -\n 280 -\n 281  \n 282 -\n 283  \n 284 -\n 285 -\n 286  \n 287 -\n 288 -\n 289 -\n 290  \n 291 -\n 292 -\n 293  \n 294 -\n 295 -\n 296 -\n 297  \n 298 -\n 299 -\n 300 -\n 301 -\n 302  \n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308  ",
            "  @Test\n  public void testMergeWithoutPONR() throws Exception {\n    try {\n      final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n      final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n\n      List<RegionInfo> tableRegions = createTable(tableName);\n\n      ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n\n      RegionInfo[] regionsToMerge = new RegionInfo[2];\n      regionsToMerge[0] = tableRegions.get(0);\n      regionsToMerge[1] = tableRegions.get(1);\n\n      long procId = procExec.submitProcedure(\n        new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n\n      // Execute until step 9 of split procedure\n      // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n      MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n\n      // Unset Toggle Kill and make ProcExec work correctly\n      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n      MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n      ProcedureTestingUtility.waitProcedure(procExec, procId);\n\n      assertRegionCount(tableName, initialRegionCount - 1);\n    } catch (Throwable t) {\n      LOG.error(\"error!\", t);\n      throw t;\n    }\n  }",
            " 276  \n 277  \n 278 +\n 279 +\n 280  \n 281 +\n 282  \n 283 +\n 284 +\n 285  \n 286 +\n 287 +\n 288 +\n 289  \n 290 +\n 291 +\n 292  \n 293 +\n 294 +\n 295 +\n 296  \n 297 +\n 298 +\n 299 +\n 300 +\n 301  \n 302 +\n 303  ",
            "  @Test\n  public void testMergeWithoutPONR() throws Exception {\n    final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n\n    List<RegionInfo> tableRegions = createTable(tableName);\n\n    ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n\n    RegionInfo[] regionsToMerge = new RegionInfo[2];\n    regionsToMerge[0] = tableRegions.get(0);\n    regionsToMerge[1] = tableRegions.get(1);\n\n    long procId = procExec.submitProcedure(\n      new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n\n    // Execute until step 9 of split procedure\n    // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n    MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n\n    // Unset Toggle Kill and make ProcExec work correctly\n    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n    MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n    ProcedureTestingUtility.waitProcedure(procExec, procId);\n\n    assertRegionCount(tableName, initialRegionCount - 1);\n  }"
        ],
        [
            "MasterProcedureTestingUtility::restartMasterProcedureExecutor(ProcedureExecutor)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114 -\n 115  \n 116  \n 117  \n 118  ",
            "  public static void restartMasterProcedureExecutor(ProcedureExecutor<MasterProcedureEnv> procExec)\n      throws Exception {\n    final MasterProcedureEnv env = procExec.getEnvironment();\n    final HMaster master = (HMaster) env.getMasterServices();\n    ProcedureTestingUtility.restart(procExec, true, true,\n      // stop services\n      new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          // try to simulate a master restart by removing the ServerManager states about seqIDs\n          for (RegionState regionState: am.getRegionStates().getRegionStates()) {\n            env.getMasterServices().getServerManager().removeRegion(regionState.getRegion());\n          }\n          am.stop();\n          master.setInitialized(false);\n          return null;\n        }\n      },\n      // setup RIT before starting workers\n      new Callable<Void>() {\n\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          am.start();\n          // just follow the same way with HMaster.finishActiveMasterInitialization. See the\n          // comments there\n          am.setupRIT(procExec.getActiveProceduresNoCopy().stream().filter(p -> !p.isSuccess())\n            .filter(p -> p instanceof TransitRegionStateProcedure)\n            .map(p -> (TransitRegionStateProcedure) p).collect(Collectors.toList()));\n          return null;\n        }\n      },\n      // restart services\n      new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          am.joinCluster();\n          master.setInitialized(true);\n          return null;\n        }\n      });\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119  \n 120  \n 121  \n 122  ",
            "  public static void restartMasterProcedureExecutor(ProcedureExecutor<MasterProcedureEnv> procExec)\n      throws Exception {\n    final MasterProcedureEnv env = procExec.getEnvironment();\n    final HMaster master = (HMaster) env.getMasterServices();\n    ProcedureTestingUtility.restart(procExec, true, true,\n      // stop services\n      new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          // try to simulate a master restart by removing the ServerManager states about seqIDs\n          for (RegionState regionState: am.getRegionStates().getRegionStates()) {\n            env.getMasterServices().getServerManager().removeRegion(regionState.getRegion());\n          }\n          am.stop();\n          master.setInitialized(false);\n          return null;\n        }\n      },\n      // setup RIT before starting workers\n      new Callable<Void>() {\n\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          am.start();\n          // just follow the same way with HMaster.finishActiveMasterInitialization. See the\n          // comments there\n          am.setupRIT(procExec.getActiveProceduresNoCopy().stream().filter(p -> !p.isSuccess())\n            .filter(p -> p instanceof TransitRegionStateProcedure)\n            .map(p -> (TransitRegionStateProcedure) p).collect(Collectors.toList()));\n          return null;\n        }\n      },\n      // restart services\n      new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n          AssignmentManager am = env.getAssignmentManager();\n          try {\n            am.joinCluster();\n            master.setInitialized(true);\n          } catch (Exception e) {\n            LOG.warn(\"Failed to load meta\", e);\n          }\n          return null;\n        }\n      });\n  }"
        ]
    ],
    "a551149ca7af79c7a3958f5f450310ca6b3d7aae": [
        [
            "AsyncRegionLocatorHelper::updateCachedLocationOnError(HRegionLocation,Throwable,Function,Consumer,Consumer)",
            "  52  \n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  ",
            "  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,\n      Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,\n      Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache) {\n    HRegionLocation oldLoc = cachedLocationSupplier.apply(loc);\n    LOG.debug(\"Try updating {} , the old value is {}\", loc, oldLoc, exception);\n    if (!canUpdateOnError(loc, oldLoc)) {\n      return;\n    }\n    Throwable cause = findException(exception);\n    LOG.debug(\"The actual exception when updating {}\", loc, cause);\n    if (cause == null || !isMetaClearingException(cause)) {\n      LOG.debug(\"Will not update {} because the exception is null or not the one we care about\",\n        loc);\n      return;\n    }\n    if (cause instanceof RegionMovedException) {\n      RegionMovedException rme = (RegionMovedException) cause;\n      HRegionLocation newLoc =\n        new HRegionLocation(loc.getRegion(), rme.getServerName(), rme.getLocationSeqNum());\n      LOG.debug(\"Try updating {} with the new location {} constructed by {}\", loc, newLoc, rme);\n      addToCache.accept(newLoc);\n    } else {\n      LOG.debug(\"Try removing {} from cache\", loc);\n      removeFromCache.accept(loc);\n    }\n  }",
            "  52  \n  53  \n  54  \n  55  \n  56 +\n  57 +\n  58 +\n  59 +\n  60  \n  61  \n  62  \n  63  \n  64 +\n  65 +\n  66 +\n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,\n      Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,\n      Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache) {\n    HRegionLocation oldLoc = cachedLocationSupplier.apply(loc);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Try updating {} , the old value is {}, error={}\", loc, oldLoc,\n        exception != null ? exception.toString() : \"none\");\n    }\n    if (!canUpdateOnError(loc, oldLoc)) {\n      return;\n    }\n    Throwable cause = findException(exception);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"The actual exception when updating {} is {}\", loc,\n        cause != null ? cause.toString() : \"none\");\n    }\n    if (cause == null || !isMetaClearingException(cause)) {\n      LOG.debug(\"Will not update {} because the exception is null or not the one we care about\",\n        loc);\n      return;\n    }\n    if (cause instanceof RegionMovedException) {\n      RegionMovedException rme = (RegionMovedException) cause;\n      HRegionLocation newLoc =\n        new HRegionLocation(loc.getRegion(), rme.getServerName(), rme.getLocationSeqNum());\n      LOG.debug(\"Try updating {} with the new location {} constructed by {}\", loc, newLoc, rme);\n      addToCache.accept(newLoc);\n    } else {\n      LOG.debug(\"Try removing {} from cache\", loc);\n      removeFromCache.accept(loc);\n    }\n  }"
        ],
        [
            "AsyncNonMetaRegionLocator::complete(TableName,LocateRequest,RegionLocations,Throwable)",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 -\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  ",
            "  private void complete(TableName tableName, LocateRequest req, RegionLocations locs,\n      Throwable error) {\n    if (error != null) {\n      LOG.warn(\"Failed to locate region in '\" + tableName + \"', row='\" +\n        Bytes.toStringBinary(req.row) + \"', locateType=\" + req.locateType, error);\n    }\n    Optional<LocateRequest> toSend = Optional.empty();\n    TableCache tableCache = getTableCache(tableName);\n    if (locs != null) {\n      if (!addToCache(tableCache, locs)) {\n        // someone is ahead of us.\n        synchronized (tableCache) {\n          tableCache.pendingRequests.remove(req);\n          tableCache.clearCompletedRequests(Optional.empty());\n          // Remove a complete locate request in a synchronized block, so the table cache must have\n          // quota to send a candidate request.\n          toSend = tableCache.getCandidate();\n          toSend.ifPresent(r -> tableCache.send(r));\n        }\n        toSend.ifPresent(r -> locateInMeta(tableName, r));\n        return;\n      }\n    }\n    synchronized (tableCache) {\n      tableCache.pendingRequests.remove(req);\n      if (error instanceof DoNotRetryIOException) {\n        CompletableFuture<?> future = tableCache.allRequests.remove(req);\n        if (future != null) {\n          future.completeExceptionally(error);\n        }\n      }\n      tableCache.clearCompletedRequests(Optional.ofNullable(locs));\n      // Remove a complete locate request in a synchronized block, so the table cache must have\n      // quota to send a candidate request.\n      toSend = tableCache.getCandidate();\n      toSend.ifPresent(r -> tableCache.send(r));\n    }\n    toSend.ifPresent(r -> locateInMeta(tableName, r));\n  }",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 +\n 278 +\n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  ",
            "  private void complete(TableName tableName, LocateRequest req, RegionLocations locs,\n      Throwable error) {\n    if (error != null) {\n      LOG.warn(\"Failed to locate region in '\" + tableName + \"', row='\" +\n        Bytes.toStringBinary(req.row) + \"', locateType=\" + req.locateType, error);\n    }\n    Optional<LocateRequest> toSend = Optional.empty();\n    TableCache tableCache = getTableCache(tableName);\n    if (locs != null) {\n      if (!addToCache(tableCache, locs)) {\n        // someone is ahead of us.\n        synchronized (tableCache) {\n          tableCache.pendingRequests.remove(req);\n          tableCache.clearCompletedRequests(Optional.empty());\n          // Remove a complete locate request in a synchronized block, so the table cache must have\n          // quota to send a candidate request.\n          toSend = tableCache.getCandidate();\n          toSend.ifPresent(r -> tableCache.send(r));\n        }\n        toSend.ifPresent(r -> locateInMeta(tableName, r));\n        return;\n      }\n    }\n    synchronized (tableCache) {\n      tableCache.pendingRequests.remove(req);\n      if (error != null) {\n        // fail the request itself, no matter whether it is a DoNotRetryIOException, as we have\n        // already retried several times\n        CompletableFuture<?> future = tableCache.allRequests.remove(req);\n        if (future != null) {\n          future.completeExceptionally(error);\n        }\n      }\n      tableCache.clearCompletedRequests(Optional.ofNullable(locs));\n      // Remove a complete locate request in a synchronized block, so the table cache must have\n      // quota to send a candidate request.\n      toSend = tableCache.getCandidate();\n      toSend.ifPresent(r -> tableCache.send(r));\n    }\n    toSend.ifPresent(r -> locateInMeta(tableName, r));\n  }"
        ],
        [
            "AsyncMetaRegionLocator::getRegionLocations(int,boolean)",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  ",
            "  /**\n   * Get the region locations for meta region. If the location for the given replica is not\n   * available in the cached locations, then fetch from the HBase cluster.\n   * <p/>\n   * The <code>replicaId</code> parameter is important. If the region replication config for meta\n   * region is changed, then the cached region locations may not have the locations for new\n   * replicas. If we do not check the location for the given replica, we will always return the\n   * cached region locations and cause an infinite loop.\n   */\n  CompletableFuture<RegionLocations> getRegionLocations(int replicaId, boolean reload) {\n    for (;;) {\n      if (!reload) {\n        RegionLocations locs = this.metaRegionLocations.get();\n        if (isGood(locs, replicaId)) {\n          return CompletableFuture.completedFuture(locs);\n        }\n      }\n      LOG.trace(\"Meta region location cache is null, try fetching from registry.\");\n      if (metaRelocateFuture.compareAndSet(null, new CompletableFuture<>())) {\n        LOG.debug(\"Start fetching meta region location from registry.\");\n        CompletableFuture<RegionLocations> future = metaRelocateFuture.get();\n        addListener(registry.getMetaRegionLocation(), (locs, error) -> {\n          if (error != null) {\n            LOG.debug(\"Failed to fetch meta region location from registry\", error);\n            metaRelocateFuture.getAndSet(null).completeExceptionally(error);\n            return;\n          }\n          LOG.debug(\"The fetched meta region location is {}\" + locs);\n          // Here we update cache before reset future, so it is possible that someone can get a\n          // stale value. Consider this:\n          // 1. update cache\n          // 2. someone clear the cache and relocate again\n          // 3. the metaRelocateFuture is not null so the old future is used.\n          // 4. we clear metaRelocateFuture and complete the future in it with the value being\n          // cleared in step 2.\n          // But we do not think it is a big deal as it rarely happens, and even if it happens, the\n          // caller will retry again later, no correctness problems.\n          this.metaRegionLocations.set(locs);\n          metaRelocateFuture.set(null);\n          future.complete(locs);\n        });\n        return future;\n      } else {\n        CompletableFuture<RegionLocations> future = metaRelocateFuture.get();\n        if (future != null) {\n          return future;\n        }\n      }\n    }\n  }",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  ",
            "  /**\n   * Get the region locations for meta region. If the location for the given replica is not\n   * available in the cached locations, then fetch from the HBase cluster.\n   * <p/>\n   * The <code>replicaId</code> parameter is important. If the region replication config for meta\n   * region is changed, then the cached region locations may not have the locations for new\n   * replicas. If we do not check the location for the given replica, we will always return the\n   * cached region locations and cause an infinite loop.\n   */\n  CompletableFuture<RegionLocations> getRegionLocations(int replicaId, boolean reload) {\n    for (;;) {\n      if (!reload) {\n        RegionLocations locs = this.metaRegionLocations.get();\n        if (isGood(locs, replicaId)) {\n          return CompletableFuture.completedFuture(locs);\n        }\n      }\n      LOG.trace(\"Meta region location cache is null, try fetching from registry.\");\n      if (metaRelocateFuture.compareAndSet(null, new CompletableFuture<>())) {\n        LOG.debug(\"Start fetching meta region location from registry.\");\n        CompletableFuture<RegionLocations> future = metaRelocateFuture.get();\n        addListener(registry.getMetaRegionLocation(), (locs, error) -> {\n          if (error != null) {\n            LOG.debug(\"Failed to fetch meta region location from registry\", error);\n            metaRelocateFuture.getAndSet(null).completeExceptionally(error);\n            return;\n          }\n          LOG.debug(\"The fetched meta region location is {}\", locs);\n          // Here we update cache before reset future, so it is possible that someone can get a\n          // stale value. Consider this:\n          // 1. update cache\n          // 2. someone clear the cache and relocate again\n          // 3. the metaRelocateFuture is not null so the old future is used.\n          // 4. we clear metaRelocateFuture and complete the future in it with the value being\n          // cleared in step 2.\n          // But we do not think it is a big deal as it rarely happens, and even if it happens, the\n          // caller will retry again later, no correctness problems.\n          this.metaRegionLocations.set(locs);\n          metaRelocateFuture.set(null);\n          future.complete(locs);\n        });\n        return future;\n      } else {\n        CompletableFuture<RegionLocations> future = metaRelocateFuture.get();\n        if (future != null) {\n          return future;\n        }\n      }\n    }\n  }"
        ]
    ]
}