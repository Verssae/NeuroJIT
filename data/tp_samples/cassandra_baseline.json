{
    "d306144282595be6818fa386a3fbb4aece040884": [
        [
            "RepairJob::run()",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119 -\n 120 -\n 121 -\n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints) throws Exception\n                {\n                    logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, endpoints));\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, allEndpoints));\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees) throws Exception\n            {\n                // Unregister from FailureDetector once we've completed synchronizing Merkle trees.\n                // After this point, we rely on tcp_keepalive for individual sockets to notify us when a connection is down.\n                // See CASSANDRA-3569\n                FailureDetector.instance.unregisterFailureDetectionEventListener(session);\n\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints) throws Exception\n                {\n                    logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, endpoints));\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, allEndpoints));\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees) throws Exception\n            {\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }"
        ],
        [
            "RepairSession::RepairSession(UUID,UUID,Range,String,RepairParallelism,Set,long,String)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    /**\n     * Create new repair session.\n     *\n     * @param parentRepairSession the parent sessions id\n     * @param id this sessions id\n     * @param range range to repair\n     * @param keyspace name of keyspace\n     * @param parallelismDegree specifies the degree of parallelism when calculating the merkle trees\n     * @param endpoints the data centers that should be part of the repair; null for all DCs\n     * @param repairedAt when the repair occurred (millis)\n     * @param cfnames names of columnfamilies\n     */\n    public RepairSession(UUID parentRepairSession,\n                         UUID id,\n                         Range<Token> range,\n                         String keyspace,\n                         RepairParallelism parallelismDegree,\n                         Set<InetAddress> endpoints,\n                         long repairedAt,\n                         String... cfnames)\n    {\n        assert cfnames.length > 0 : \"Repairing no column families seems pointless, doesn't it\";\n\n        this.parentRepairSession = parentRepairSession;\n        this.id = id;\n        this.parallelismDegree = parallelismDegree;\n        this.keyspace = keyspace;\n        this.cfnames = cfnames;\n        this.range = range;\n        this.endpoints = endpoints;\n        this.repairedAt = repairedAt;\n    }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  ",
            "    /**\n     * Create new repair session.\n     *\n     * @param parentRepairSession the parent sessions id\n     * @param id this sessions id\n     * @param range range to repair\n     * @param keyspace name of keyspace\n     * @param parallelismDegree specifies the degree of parallelism when calculating the merkle trees\n     * @param endpoints the data centers that should be part of the repair; null for all DCs\n     * @param repairedAt when the repair occurred (millis)\n     * @param cfnames names of columnfamilies\n     */\n    public RepairSession(UUID parentRepairSession,\n                         UUID id,\n                         Range<Token> range,\n                         String keyspace,\n                         RepairParallelism parallelismDegree,\n                         Set<InetAddress> endpoints,\n                         long repairedAt,\n                         String... cfnames)\n    {\n        assert cfnames.length > 0 : \"Repairing no column families seems pointless, doesn't it\";\n\n        this.parentRepairSession = parentRepairSession;\n        this.id = id;\n        this.parallelismDegree = parallelismDegree;\n        this.keyspace = keyspace;\n        this.cfnames = cfnames;\n        this.range = range;\n        this.endpoints = endpoints;\n        this.repairedAt = repairedAt;\n        this.validationRemaining = new AtomicInteger(cfnames.length);\n    }"
        ],
        [
            "RepairSession::validationComplete(RepairJobDesc,InetAddress,MerkleTree)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "    /**\n     * Receive merkle tree response or failed response from {@code endpoint} for current repair job.\n     *\n     * @param desc repair job description\n     * @param endpoint endpoint that sent merkle tree\n     * @param tree calculated merkle tree, or null if validation failed\n     */\n    public void validationComplete(RepairJobDesc desc, InetAddress endpoint, MerkleTree tree)\n    {\n        ValidationTask task = validating.remove(Pair.create(desc, endpoint));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        String message = String.format(\"Received merkle tree for %s from %s\", desc.columnFamily, endpoint);\n        logger.info(\"[repair #{}] {}\", getId(), message);\n        Tracing.traceRepair(message);\n        task.treeReceived(tree);\n    }",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192  ",
            "    /**\n     * Receive merkle tree response or failed response from {@code endpoint} for current repair job.\n     *\n     * @param desc repair job description\n     * @param endpoint endpoint that sent merkle tree\n     * @param tree calculated merkle tree, or null if validation failed\n     */\n    public void validationComplete(RepairJobDesc desc, InetAddress endpoint, MerkleTree tree)\n    {\n        ValidationTask task = validating.remove(Pair.create(desc, endpoint));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        String message = String.format(\"Received merkle tree for %s from %s\", desc.columnFamily, endpoint);\n        logger.info(\"[repair #{}] {}\", getId(), message);\n        Tracing.traceRepair(message);\n        task.treeReceived(tree);\n\n        // Unregister from FailureDetector once we've completed synchronizing Merkle trees.\n        // After this point, we rely on tcp_keepalive for individual sockets to notify us when a connection is down.\n        // See CASSANDRA-3569\n        if (validationRemaining.decrementAndGet() == 0)\n        {\n            FailureDetector.instance.unregisterFailureDetectionEventListener(this);\n        }\n    }"
        ]
    ],
    "9ddce25ff79efae6edd42c1f2f4c78deba19b4e7": [
        [
            "AbstractSSTableIterator::IndexState::isPastCurrentBlock()",
            " 457  \n 458  \n 459 -\n 460  ",
            "        public boolean isPastCurrentBlock()\n        {\n            return currentIndexIdx < indexes.size() && reader.file.bytesPastMark(mark) >= currentIndex().width;\n        }",
            " 457  \n 458  \n 459 +\n 460  ",
            "        public boolean isPastCurrentBlock()\n        {\n            return reader.file.bytesPastMark(mark) >= currentIndex().width;\n        }"
        ],
        [
            "SSTableIterator::ForwardIndexedReader::computeNext()",
            " 251  \n 252  \n 253  \n 254  \n 255 -\n 256 -\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "        @Override\n        protected Unfiltered computeNext() throws IOException\n        {\n            // Our previous read might have made us cross an index block boundary. If so, update our informations.\n            if (indexState.isPastCurrentBlock())\n                indexState.setToBlock(indexState.currentBlockIdx() + 1);\n\n            // Return the next unfiltered unless we've reached the end, or we're beyond our slice\n            // end (note that unless we're on the last block for the slice, there is no point\n            // in checking the slice end).\n            if (indexState.isDone()\n                || indexState.currentBlockIdx() > lastBlockIdx\n                || !deserializer.hasNext()\n                || (indexState.currentBlockIdx() == lastBlockIdx && deserializer.compareNextTo(end) > 0))\n                return null;\n\n\n            Unfiltered next = deserializer.readNext();\n            if (next.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)\n                updateOpenMarker((RangeTombstoneMarker)next);\n            return next;\n        }",
            " 251  \n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "        @Override\n        protected Unfiltered computeNext() throws IOException\n        {\n            // Our previous read might have made us cross an index block boundary. If so, update our informations.\n            int currentBlockIdx = indexState.currentBlockIdx();\n            if (indexState.isPastCurrentBlock() && currentBlockIdx + 1 < indexState.blocksCount())\n                indexState.setToBlock(currentBlockIdx + 1);\n\n            // Return the next unfiltered unless we've reached the end, or we're beyond our slice\n            // end (note that unless we're on the last block for the slice, there is no point\n            // in checking the slice end).\n            if (indexState.isDone()\n                || indexState.currentBlockIdx() > lastBlockIdx\n                || !deserializer.hasNext()\n                || (indexState.currentBlockIdx() == lastBlockIdx && deserializer.compareNextTo(end) > 0))\n                return null;\n\n\n            Unfiltered next = deserializer.readNext();\n            if (next.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)\n                updateOpenMarker((RangeTombstoneMarker)next);\n            return next;\n        }"
        ]
    ],
    "eca7cbb2e20858625c39cf0f9a5a76c6cc905dc0": [
        [
            "AntiCompactionTest::shouldSkipAntiCompactionForNonIntersectingRange()",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 -\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  ",
            "    @Test\n    public void shouldSkipAntiCompactionForNonIntersectingRange() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore store = keyspace.getColumnFamilyStore(CF);\n        store.disableAutoCompaction();\n\n        for (int table = 0; table < 10; table++)\n        {\n            generateSStable(store,Integer.toString(table));\n        }\n        Collection<SSTableReader> sstables = getUnrepairedSSTables(store);\n        assertEquals(store.getLiveSSTables().size(), sstables.size());\n        \n        Range<Token> range = new Range<Token>(new BytesToken(\"-10\".getBytes()), new BytesToken(\"-1\".getBytes()));\n        List<Range<Token>> ranges = Arrays.asList(range);\n\n\n        try (LifecycleTransaction txn = store.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(store, ranges, refs, txn, 0);\n        }\n\n        assertThat(store.getLiveSSTables().size(), is(10));\n        assertThat(Iterables.get(store.getLiveSSTables(), 0).isRepaired(), is(false));\n    }",
            " 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301 +\n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  ",
            "    @Test\n    public void shouldSkipAntiCompactionForNonIntersectingRange() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore store = keyspace.getColumnFamilyStore(CF);\n        store.disableAutoCompaction();\n\n        for (int table = 0; table < 10; table++)\n        {\n            generateSStable(store,Integer.toString(table));\n        }\n        Collection<SSTableReader> sstables = getUnrepairedSSTables(store);\n        assertEquals(store.getLiveSSTables().size(), sstables.size());\n\n        Range<Token> range = new Range<Token>(new BytesToken(\"-10\".getBytes()), new BytesToken(\"-1\".getBytes()));\n        List<Range<Token>> ranges = Arrays.asList(range);\n\n\n        try (LifecycleTransaction txn = store.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(store, ranges, refs, txn, 0);\n        }\n\n        assertThat(store.getLiveSSTables().size(), is(10));\n        assertThat(Iterables.get(store.getLiveSSTables(), 0).isRepaired(), is(false));\n    }"
        ],
        [
            "AntiCompactionTest::antiCompactionSizeTest()",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  ",
            "    @Test\n    public void antiCompactionSizeTest() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.disableAutoCompaction();\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long origSize = s.bytesOnDisk();\n        Range<Token> range = new Range<Token>(new BytesToken(ByteBufferUtil.bytes(0)), new BytesToken(ByteBufferUtil.bytes(500)));\n        Collection<SSTableReader> sstables = cfs.getLiveSSTables();\n        try (LifecycleTransaction txn = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(cfs, Arrays.asList(range), refs, txn, 12345);\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(origSize, cfs.metric.liveDiskSpaceUsed.getCount(), 100000);\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153  \n 154 +\n 155  \n 156 +\n 157 +\n 158  \n 159 +\n 160 +\n 161  ",
            "    @Test\n    public void antiCompactionSizeTest() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.disableAutoCompaction();\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long origSize = s.bytesOnDisk();\n        Range<Token> range = new Range<Token>(new BytesToken(ByteBufferUtil.bytes(0)), new BytesToken(ByteBufferUtil.bytes(500)));\n        Collection<SSTableReader> sstables = cfs.getLiveSSTables();\n        try (LifecycleTransaction txn = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(cfs, Arrays.asList(range), refs, txn, 12345);\n        }\n        long sum = 0;\n        long rows = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n        {\n            sum += x.bytesOnDisk();\n            rows += x.getTotalRows();\n        }\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(rows, 1000 * (1000 * 5));//See writeFile for how this number is derived\n        assertEquals(origSize, cfs.metric.liveDiskSpaceUsed.getCount(), 16000000);\n    }"
        ]
    ],
    "e769324220ccfb2e48063d639e378a8a34814651": [
        [
            "StorageProxy::mutateMV(ByteBuffer,Collection)",
            " 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678 -\n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687 -\n 688  \n 689  \n 690 -\n 691 -\n 692  \n 693 -\n 694 -\n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  ",
            "    /**\n     * Use this method to have these Mutations applied\n     * across all replicas.\n     *\n     * @param mutations the mutations to be applied across the replicas\n     */\n    public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations)\n    throws UnavailableException, OverloadedException, WriteTimeoutException\n    {\n        Tracing.trace(\"Determining replicas for mutation\");\n        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n\n        long startTime = System.nanoTime();\n        List<WriteResponseHandlerWrapper> wrappers = new ArrayList<>(mutations.size());\n\n        try\n        {\n            Token baseToken = StorageService.instance.getTokenMetadata().partitioner.getToken(dataKey);\n\n            ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;\n\n            //Since the base -> view replication is 1:1 we only need to store the BL locally\n            final Collection<InetAddress> batchlogEndpoints = Collections.singleton(FBUtilities.getBroadcastAddress());\n            final UUID batchUUID = UUIDGen.getTimeUUID();\n            BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(),\n                                                                                                          () -> asyncRemoveFromBatchlog(batchlogEndpoints, batchUUID));\n\n            // add a handler for each mutation - includes checking availability, but doesn't initiate any writes, yet\n            for (Mutation mutation : mutations)\n            {\n                String keyspaceName = mutation.getKeyspaceName();\n                Token tk = mutation.key().getToken();\n                List<InetAddress> naturalEndpoints = Lists.newArrayList(MaterializedViewUtils.getViewNaturalEndpoint(keyspaceName, baseToken, tk));\n\n                WriteResponseHandlerWrapper wrapper = wrapMVBatchResponseHandler(mutation,\n                                                                                 consistencyLevel,\n                                                                                 consistencyLevel,\n                                                                                 naturalEndpoints,\n                                                                                 WriteType.BATCH,\n                                                                                 cleanup);\n\n                wrappers.add(wrapper);\n            }\n\n            //Apply to local batchlog memtable in this thread\n            BatchlogManager.getBatchlogMutationFor(mutations, batchUUID, MessagingService.current_version).apply();\n\n            // now actually perform the writes and wait for them to complete\n            asyncWriteBatchedMutations(wrappers, localDataCenter, Stage.MATERIALIZED_VIEW_MUTATION);\n        }\n        catch (WriteTimeoutException ex)\n        {\n            mvWriteMetrics.timeouts.mark();\n            Tracing.trace(\"Write timeout; received {} of {} required replies\", ex.received, ex.blockFor);\n            throw ex;\n        }\n        catch (UnavailableException e)\n        {\n            mvWriteMetrics.unavailables.mark();\n            Tracing.trace(\"Unavailable\");\n            throw e;\n        }\n        catch (OverloadedException e)\n        {\n            mvWriteMetrics.unavailables.mark();\n            Tracing.trace(\"Overloaded\");\n            throw e;\n        }\n        finally\n        {\n            mvWriteMetrics.addNano(System.nanoTime() - startTime);\n        }\n    }",
            " 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681 +\n 682 +\n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691 +\n 692 +\n 693 +\n 694 +\n 695 +\n 696 +\n 697 +\n 698 +\n 699 +\n 700 +\n 701 +\n 702  \n 703  \n 704 +\n 705 +\n 706 +\n 707 +\n 708  \n 709 +\n 710 +\n 711 +\n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  ",
            "    /**\n     * Use this method to have these Mutations applied\n     * across all replicas.\n     *\n     * @param mutations the mutations to be applied across the replicas\n     */\n    public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations)\n    throws UnavailableException, OverloadedException, WriteTimeoutException\n    {\n        Tracing.trace(\"Determining replicas for mutation\");\n        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n\n        long startTime = System.nanoTime();\n        List<WriteResponseHandlerWrapper> wrappers = new ArrayList<>(mutations.size());\n\n        try\n        {\n            Token baseToken = StorageService.instance.getTokenMetadata().partitioner.getToken(dataKey);\n\n            ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;\n\n            //Since the base -> view replication is 1:1 we only need to store the BL locally\n            final Collection<InetAddress> batchlogEndpoints = Collections.singleton(FBUtilities.getBroadcastAddress());\n            final UUID batchUUID = UUIDGen.getTimeUUID();\n            BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(),\n                                                                                                          () -> asyncRemoveFromBatchlog(batchlogEndpoints, batchUUID));\n\n            // add a handler for each mutation - includes checking availability, but doesn't initiate any writes, yet\n            for (Mutation mutation : mutations)\n            {\n                String keyspaceName = mutation.getKeyspaceName();\n                Token tk = mutation.key().getToken();\n                InetAddress pairedEndpoint = MaterializedViewUtils.getViewNaturalEndpoint(keyspaceName, baseToken, tk);\n                List<InetAddress> naturalEndpoints = Lists.newArrayList(pairedEndpoint);\n\n                WriteResponseHandlerWrapper wrapper = wrapMVBatchResponseHandler(mutation,\n                                                                                 consistencyLevel,\n                                                                                 consistencyLevel,\n                                                                                 naturalEndpoints,\n                                                                                 WriteType.BATCH,\n                                                                                 cleanup);\n\n                //When local node is the endpoint and there are no pending nodes we can\n                // Just apply the mutation locally.\n                if (pairedEndpoint.equals(FBUtilities.getBroadcastAddress()) &&\n                    wrapper.handler.pendingEndpoints.isEmpty())\n                {\n                    mutation.apply();\n                }\n                else\n                {\n                    wrappers.add(wrapper);\n                }\n            }\n\n            if (!wrappers.isEmpty())\n            {\n                //Apply to local batchlog memtable in this thread\n                BatchlogManager.getBatchlogMutationFor(Lists.transform(wrappers, w -> w.mutation), batchUUID, MessagingService.current_version).apply();\n\n                // now actually perform the writes and wait for them to complete\n                asyncWriteBatchedMutations(wrappers, localDataCenter, Stage.MATERIALIZED_VIEW_MUTATION);\n            }\n        }\n        catch (WriteTimeoutException ex)\n        {\n            mvWriteMetrics.timeouts.mark();\n            Tracing.trace(\"Write timeout; received {} of {} required replies\", ex.received, ex.blockFor);\n            throw ex;\n        }\n        catch (UnavailableException e)\n        {\n            mvWriteMetrics.unavailables.mark();\n            Tracing.trace(\"Unavailable\");\n            throw e;\n        }\n        catch (OverloadedException e)\n        {\n            mvWriteMetrics.unavailables.mark();\n            Tracing.trace(\"Overloaded\");\n            throw e;\n        }\n        finally\n        {\n            mvWriteMetrics.addNano(System.nanoTime() - startTime);\n        }\n    }"
        ]
    ],
    "83fcc926eb6e5f8667f31dfec3605d7358e0cfd0": [
        [
            "DatabaseDescriptor::applyConfig(Config)",
            " 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "    public static void applyConfig(Config config) throws ConfigurationException\n    {\n        conf = config;\n\n        if (conf.commitlog_sync == null)\n        {\n            throw new ConfigurationException(\"Missing required directive CommitLogSync\", false);\n        }\n\n        if (conf.commitlog_sync == Config.CommitLogSync.batch)\n        {\n            if (conf.commitlog_sync_batch_window_in_ms == null)\n            {\n                throw new ConfigurationException(\"Missing value for commitlog_sync_batch_window_in_ms: Double expected.\", false);\n            }\n            else if (conf.commitlog_sync_period_in_ms != null)\n            {\n                throw new ConfigurationException(\"Batch sync specified, but commitlog_sync_period_in_ms found. Only specify commitlog_sync_batch_window_in_ms when using batch sync\", false);\n            }\n            logger.debug(\"Syncing log with a batch window of {}\", conf.commitlog_sync_batch_window_in_ms);\n        }\n        else\n        {\n            if (conf.commitlog_sync_period_in_ms == null)\n            {\n                throw new ConfigurationException(\"Missing value for commitlog_sync_period_in_ms: Integer expected\", false);\n            }\n            else if (conf.commitlog_sync_batch_window_in_ms != null)\n            {\n                throw new ConfigurationException(\"commitlog_sync_period_in_ms specified, but commitlog_sync_batch_window_in_ms found.  Only specify commitlog_sync_period_in_ms when using periodic sync.\", false);\n            }\n            logger.debug(\"Syncing log with a period of {}\", conf.commitlog_sync_period_in_ms);\n        }\n\n        /* evaluate the DiskAccessMode Config directive, which also affects indexAccessMode selection */\n        if (conf.disk_access_mode == Config.DiskAccessMode.auto)\n        {\n            conf.disk_access_mode = hasLargeAddressSpace() ? Config.DiskAccessMode.mmap : Config.DiskAccessMode.standard;\n            indexAccessMode = conf.disk_access_mode;\n            logger.info(\"DiskAccessMode 'auto' determined to be {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n        else if (conf.disk_access_mode == Config.DiskAccessMode.mmap_index_only)\n        {\n            conf.disk_access_mode = Config.DiskAccessMode.standard;\n            indexAccessMode = Config.DiskAccessMode.mmap;\n            logger.info(\"DiskAccessMode is {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n        else\n        {\n            indexAccessMode = conf.disk_access_mode;\n            logger.info(\"DiskAccessMode is {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n\n        /* Authentication, authorization and role management backend, implementing IAuthenticator, IAuthorizer & IRoleMapper*/\n        if (conf.authenticator != null)\n            authenticator = FBUtilities.newAuthenticator(conf.authenticator);\n\n        if (conf.authorizer != null)\n            authorizer = FBUtilities.newAuthorizer(conf.authorizer);\n\n        if (authenticator instanceof AllowAllAuthenticator && !(authorizer instanceof AllowAllAuthorizer))\n            throw new ConfigurationException(\"AllowAllAuthenticator can't be used with \" +  conf.authorizer, false);\n\n        if (conf.role_manager != null)\n            roleManager = FBUtilities.newRoleManager(conf.role_manager);\n        else\n            roleManager = new CassandraRoleManager();\n\n        if (authenticator instanceof PasswordAuthenticator && !(roleManager instanceof CassandraRoleManager))\n            throw new ConfigurationException(\"CassandraRoleManager must be used with PasswordAuthenticator\", false);\n\n        if (conf.internode_authenticator != null)\n            internodeAuthenticator = FBUtilities.construct(conf.internode_authenticator, \"internode_authenticator\");\n        else\n            internodeAuthenticator = new AllowAllInternodeAuthenticator();\n\n        authenticator.validateConfiguration();\n        authorizer.validateConfiguration();\n        roleManager.validateConfiguration();\n        internodeAuthenticator.validateConfiguration();\n\n        /* Hashing strategy */\n        if (conf.partitioner == null)\n        {\n            throw new ConfigurationException(\"Missing directive: partitioner\", false);\n        }\n        try\n        {\n            partitioner = FBUtilities.newPartitioner(System.getProperty(\"cassandra.partitioner\", conf.partitioner));\n        }\n        catch (Exception e)\n        {\n            throw new ConfigurationException(\"Invalid partitioner class \" + conf.partitioner, false);\n        }\n        paritionerName = partitioner.getClass().getCanonicalName();\n\n        if (conf.gc_warn_threshold_in_ms < 0)\n        {\n            throw new ConfigurationException(\"gc_warn_threshold_in_ms must be a positive integer\");\n        }\n\n        if (conf.max_hint_window_in_ms == null)\n        {\n            throw new ConfigurationException(\"max_hint_window_in_ms cannot be set to null\", false);\n        }\n\n        /* phi convict threshold for FailureDetector */\n        if (conf.phi_convict_threshold < 5 || conf.phi_convict_threshold > 16)\n        {\n            throw new ConfigurationException(\"phi_convict_threshold must be between 5 and 16\", false);\n        }\n\n        /* Thread per pool */\n        if (conf.concurrent_reads != null && conf.concurrent_reads < 2)\n        {\n            throw new ConfigurationException(\"concurrent_reads must be at least 2\", false);\n        }\n\n        if (conf.concurrent_writes != null && conf.concurrent_writes < 2)\n        {\n            throw new ConfigurationException(\"concurrent_writes must be at least 2\", false);\n        }\n\n        if (conf.concurrent_counter_writes != null && conf.concurrent_counter_writes < 2)\n            throw new ConfigurationException(\"concurrent_counter_writes must be at least 2\", false);\n\n        if (conf.concurrent_replicates != null)\n            logger.warn(\"concurrent_replicates has been deprecated and should be removed from cassandra.yaml\");\n\n        if (conf.file_cache_size_in_mb == null)\n            conf.file_cache_size_in_mb = Math.min(512, (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576)));\n\n        if (conf.memtable_offheap_space_in_mb == null)\n            conf.memtable_offheap_space_in_mb = (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576));\n        if (conf.memtable_offheap_space_in_mb < 0)\n            throw new ConfigurationException(\"memtable_offheap_space_in_mb must be positive\", false);\n        // for the moment, we default to twice as much on-heap space as off-heap, as heap overhead is very large\n        if (conf.memtable_heap_space_in_mb == null)\n            conf.memtable_heap_space_in_mb = (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576));\n        if (conf.memtable_heap_space_in_mb <= 0)\n            throw new ConfigurationException(\"memtable_heap_space_in_mb must be positive\", false);\n        logger.info(\"Global memtable on-heap threshold is enabled at {}MB\", conf.memtable_heap_space_in_mb);\n        if (conf.memtable_offheap_space_in_mb == 0)\n            logger.info(\"Global memtable off-heap threshold is disabled, HeapAllocator will be used instead\");\n        else\n            logger.info(\"Global memtable off-heap threshold is enabled at {}MB\", conf.memtable_offheap_space_in_mb);\n\n        applyAddressConfig(config);\n\n        if (conf.thrift_framed_transport_size_in_mb <= 0)\n            throw new ConfigurationException(\"thrift_framed_transport_size_in_mb must be positive\", false);\n\n        if (conf.native_transport_max_frame_size_in_mb <= 0)\n            throw new ConfigurationException(\"native_transport_max_frame_size_in_mb must be positive\", false);\n\n        // fail early instead of OOMing (see CASSANDRA-8116)\n        if (ThriftServer.HSHA.equals(conf.rpc_server_type) && conf.rpc_max_threads == Integer.MAX_VALUE)\n            throw new ConfigurationException(\"The hsha rpc_server_type is not compatible with an rpc_max_threads \" +\n                                             \"setting of 'unlimited'.  Please see the comments in cassandra.yaml \" +\n                                             \"for rpc_server_type and rpc_max_threads.\",\n                                             false);\n        if (ThriftServer.HSHA.equals(conf.rpc_server_type) && conf.rpc_max_threads > (FBUtilities.getAvailableProcessors() * 2 + 1024))\n            logger.warn(\"rpc_max_threads setting of {} may be too high for the hsha server and cause unnecessary thread contention, reducing performance\", conf.rpc_max_threads);\n\n        /* end point snitch */\n        if (conf.endpoint_snitch == null)\n        {\n            throw new ConfigurationException(\"Missing endpoint_snitch directive\", false);\n        }\n        snitch = createEndpointSnitch(conf.endpoint_snitch);\n        EndpointSnitchInfo.create();\n\n        localDC = snitch.getDatacenter(FBUtilities.getBroadcastAddress());\n        localComparator = new Comparator<InetAddress>()\n        {\n            public int compare(InetAddress endpoint1, InetAddress endpoint2)\n            {\n                boolean local1 = localDC.equals(snitch.getDatacenter(endpoint1));\n                boolean local2 = localDC.equals(snitch.getDatacenter(endpoint2));\n                if (local1 && !local2)\n                    return -1;\n                if (local2 && !local1)\n                    return 1;\n                return 0;\n            }\n        };\n\n        /* Request Scheduler setup */\n        requestSchedulerOptions = conf.request_scheduler_options;\n        if (conf.request_scheduler != null)\n        {\n            try\n            {\n                if (requestSchedulerOptions == null)\n                {\n                    requestSchedulerOptions = new RequestSchedulerOptions();\n                }\n                Class<?> cls = Class.forName(conf.request_scheduler);\n                requestScheduler = (IRequestScheduler) cls.getConstructor(RequestSchedulerOptions.class).newInstance(requestSchedulerOptions);\n            }\n            catch (ClassNotFoundException e)\n            {\n                throw new ConfigurationException(\"Invalid Request Scheduler class \" + conf.request_scheduler, false);\n            }\n            catch (Exception e)\n            {\n                throw new ConfigurationException(\"Unable to instantiate request scheduler\", e);\n            }\n        }\n        else\n        {\n            requestScheduler = new NoScheduler();\n        }\n\n        if (conf.request_scheduler_id == RequestSchedulerId.keyspace)\n        {\n            requestSchedulerId = conf.request_scheduler_id;\n        }\n        else\n        {\n            // Default to Keyspace\n            requestSchedulerId = RequestSchedulerId.keyspace;\n        }\n\n        // if data dirs, commitlog dir, or saved caches dir are set in cassandra.yaml, use that.  Otherwise,\n        // use -Dcassandra.storagedir (set in cassandra-env.sh) as the parent dir for data/, commitlog/, and saved_caches/\n        if (conf.commitlog_directory == null)\n        {\n            conf.commitlog_directory = System.getProperty(\"cassandra.storagedir\", null);\n            if (conf.commitlog_directory == null)\n                throw new ConfigurationException(\"commitlog_directory is missing and -Dcassandra.storagedir is not set\", false);\n            conf.commitlog_directory += File.separator + \"commitlog\";\n        }\n\n        if (conf.commitlog_total_space_in_mb == null)\n        {\n            int preferredSize = 8192;\n            int minSize = 0;\n            try\n            {\n                // use 1/4 of available space.  See discussion on #10013 and #10199\n                minSize = Ints.checkedCast((guessFileStore(conf.commitlog_directory).getTotalSpace() / 1048576) / 4);\n            }\n            catch (IOException e)\n            {\n                logger.debug(\"Error checking disk space\", e);\n                throw new ConfigurationException(String.format(\"Unable to check disk space available to %s. Perhaps the Cassandra user does not have the necessary permissions\",\n                                                               conf.commitlog_directory), e);\n            }\n            if (minSize < preferredSize)\n            {\n                logger.warn(\"Small commitlog volume detected at {}; setting commitlog_total_space_in_mb to {}.  You can override this in cassandra.yaml\",\n                            conf.commitlog_directory, minSize);\n                conf.commitlog_total_space_in_mb = minSize;\n            }\n            else\n            {\n                conf.commitlog_total_space_in_mb = preferredSize;\n            }\n        }\n\n        if (conf.saved_caches_directory == null)\n        {\n            conf.saved_caches_directory = System.getProperty(\"cassandra.storagedir\", null);\n            if (conf.saved_caches_directory == null)\n                throw new ConfigurationException(\"saved_caches_directory is missing and -Dcassandra.storagedir is not set\", false);\n            conf.saved_caches_directory += File.separator + \"saved_caches\";\n        }\n        if (conf.data_file_directories == null)\n        {\n            String defaultDataDir = System.getProperty(\"cassandra.storagedir\", null);\n            if (defaultDataDir == null)\n                throw new ConfigurationException(\"data_file_directories is not missing and -Dcassandra.storagedir is not set\", false);\n            conf.data_file_directories = new String[]{ defaultDataDir + File.separator + \"data\" };\n        }\n\n        long dataFreeBytes = 0;\n        /* data file and commit log directories. they get created later, when they're needed. */\n        for (String datadir : conf.data_file_directories)\n        {\n            if (datadir.equals(conf.commitlog_directory))\n                throw new ConfigurationException(\"commitlog_directory must not be the same as any data_file_directories\", false);\n            if (datadir.equals(conf.saved_caches_directory))\n                throw new ConfigurationException(\"saved_caches_directory must not be the same as any data_file_directories\", false);\n\n            try\n            {\n                dataFreeBytes += guessFileStore(datadir).getUnallocatedSpace();\n            }\n            catch (IOException e)\n            {\n                logger.debug(\"Error checking disk space\", e);\n                throw new ConfigurationException(String.format(\"Unable to check disk space available to %s. Perhaps the Cassandra user does not have the necessary permissions\",\n                                                               datadir), e);\n            }\n        }\n        if (dataFreeBytes < 64L * 1024 * 1048576) // 64 GB\n            logger.warn(\"Only {} MB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots\",\n                        dataFreeBytes / 1048576);\n\n\n        if (conf.commitlog_directory.equals(conf.saved_caches_directory))\n            throw new ConfigurationException(\"saved_caches_directory must not be the same as the commitlog_directory\", false);\n\n        if (conf.memtable_flush_writers == null)\n            conf.memtable_flush_writers = Math.min(8, Math.max(2, Math.min(FBUtilities.getAvailableProcessors(), conf.data_file_directories.length)));\n\n        if (conf.memtable_flush_writers < 1)\n            throw new ConfigurationException(\"memtable_flush_writers must be at least 1\", false);\n\n        if (conf.memtable_cleanup_threshold == null)\n            conf.memtable_cleanup_threshold = (float) (1.0 / (1 + conf.memtable_flush_writers));\n\n        if (conf.memtable_cleanup_threshold < 0.01f)\n            throw new ConfigurationException(\"memtable_cleanup_threshold must be >= 0.01\", false);\n        if (conf.memtable_cleanup_threshold > 0.99f)\n            throw new ConfigurationException(\"memtable_cleanup_threshold must be <= 0.99\", false);\n        if (conf.memtable_cleanup_threshold < 0.1f)\n            logger.warn(\"memtable_cleanup_threshold is set very low, which may cause performance degradation\");\n\n        if (conf.concurrent_compactors == null)\n            conf.concurrent_compactors = Math.min(8, Math.max(2, Math.min(FBUtilities.getAvailableProcessors(), conf.data_file_directories.length)));\n\n        if (conf.concurrent_compactors <= 0)\n            throw new ConfigurationException(\"concurrent_compactors should be strictly greater than 0\", false);\n\n        if (conf.initial_token != null)\n            for (String token : tokensFromString(conf.initial_token))\n                partitioner.getTokenFactory().validate(token);\n\n        if (conf.num_tokens == null)\n        \tconf.num_tokens = 1;\n        else if (conf.num_tokens > MAX_NUM_TOKENS)\n            throw new ConfigurationException(String.format(\"A maximum number of %d tokens per node is supported\", MAX_NUM_TOKENS), false);\n\n        try\n        {\n            // if key_cache_size_in_mb option was set to \"auto\" then size of the cache should be \"min(5% of Heap (in MB), 100MB)\n            keyCacheSizeInMB = (conf.key_cache_size_in_mb == null)\n                ? Math.min(Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.05 / 1024 / 1024)), 100)\n                : conf.key_cache_size_in_mb;\n\n            if (keyCacheSizeInMB < 0)\n                throw new NumberFormatException(); // to escape duplicating error message\n        }\n        catch (NumberFormatException e)\n        {\n            throw new ConfigurationException(\"key_cache_size_in_mb option was set incorrectly to '\"\n                    + conf.key_cache_size_in_mb + \"', supported values are <integer> >= 0.\", false);\n        }\n\n        try\n        {\n            // if counter_cache_size_in_mb option was set to \"auto\" then size of the cache should be \"min(2.5% of Heap (in MB), 50MB)\n            counterCacheSizeInMB = (conf.counter_cache_size_in_mb == null)\n                    ? Math.min(Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.025 / 1024 / 1024)), 50)\n                    : conf.counter_cache_size_in_mb;\n\n            if (counterCacheSizeInMB < 0)\n                throw new NumberFormatException(); // to escape duplicating error message\n        }\n        catch (NumberFormatException e)\n        {\n            throw new ConfigurationException(\"counter_cache_size_in_mb option was set incorrectly to '\"\n                    + conf.counter_cache_size_in_mb + \"', supported values are <integer> >= 0.\", false);\n        }\n\n        // if set to empty/\"auto\" then use 5% of Heap size\n        indexSummaryCapacityInMB = (conf.index_summary_capacity_in_mb == null)\n            ? Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.05 / 1024 / 1024))\n            : conf.index_summary_capacity_in_mb;\n\n        if (indexSummaryCapacityInMB < 0)\n            throw new ConfigurationException(\"index_summary_capacity_in_mb option was set incorrectly to '\"\n                    + conf.index_summary_capacity_in_mb + \"', it should be a non-negative integer.\", false);\n\n        if(conf.encryption_options != null)\n        {\n            logger.warn(\"Please rename encryption_options as server_encryption_options in the yaml\");\n            //operate under the assumption that server_encryption_options is not set in yaml rather than both\n            conf.server_encryption_options = conf.encryption_options;\n        }\n\n        // load the seeds for node contact points\n        if (conf.seed_provider == null)\n        {\n            throw new ConfigurationException(\"seeds configuration is missing; a minimum of one seed is required.\", false);\n        }\n        try\n        {\n            Class<?> seedProviderClass = Class.forName(conf.seed_provider.class_name);\n            seedProvider = (SeedProvider)seedProviderClass.getConstructor(Map.class).newInstance(conf.seed_provider.parameters);\n        }\n        // there are about 5 checked exceptions that could be thrown here.\n        catch (Exception e)\n        {\n            throw new ConfigurationException(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\", false);\n        }\n        if (seedProvider.getSeeds().size() == 0)\n            throw new ConfigurationException(\"The seed provider lists no seeds.\", false);\n    }",
            " 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 +\n 398 +\n 399 +\n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  ",
            "    public static void applyConfig(Config config) throws ConfigurationException\n    {\n        conf = config;\n\n        if (conf.commitlog_sync == null)\n        {\n            throw new ConfigurationException(\"Missing required directive CommitLogSync\", false);\n        }\n\n        if (conf.commitlog_sync == Config.CommitLogSync.batch)\n        {\n            if (conf.commitlog_sync_batch_window_in_ms == null)\n            {\n                throw new ConfigurationException(\"Missing value for commitlog_sync_batch_window_in_ms: Double expected.\", false);\n            }\n            else if (conf.commitlog_sync_period_in_ms != null)\n            {\n                throw new ConfigurationException(\"Batch sync specified, but commitlog_sync_period_in_ms found. Only specify commitlog_sync_batch_window_in_ms when using batch sync\", false);\n            }\n            logger.debug(\"Syncing log with a batch window of {}\", conf.commitlog_sync_batch_window_in_ms);\n        }\n        else\n        {\n            if (conf.commitlog_sync_period_in_ms == null)\n            {\n                throw new ConfigurationException(\"Missing value for commitlog_sync_period_in_ms: Integer expected\", false);\n            }\n            else if (conf.commitlog_sync_batch_window_in_ms != null)\n            {\n                throw new ConfigurationException(\"commitlog_sync_period_in_ms specified, but commitlog_sync_batch_window_in_ms found.  Only specify commitlog_sync_period_in_ms when using periodic sync.\", false);\n            }\n            logger.debug(\"Syncing log with a period of {}\", conf.commitlog_sync_period_in_ms);\n        }\n\n        /* evaluate the DiskAccessMode Config directive, which also affects indexAccessMode selection */\n        if (conf.disk_access_mode == Config.DiskAccessMode.auto)\n        {\n            conf.disk_access_mode = hasLargeAddressSpace() ? Config.DiskAccessMode.mmap : Config.DiskAccessMode.standard;\n            indexAccessMode = conf.disk_access_mode;\n            logger.info(\"DiskAccessMode 'auto' determined to be {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n        else if (conf.disk_access_mode == Config.DiskAccessMode.mmap_index_only)\n        {\n            conf.disk_access_mode = Config.DiskAccessMode.standard;\n            indexAccessMode = Config.DiskAccessMode.mmap;\n            logger.info(\"DiskAccessMode is {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n        else\n        {\n            indexAccessMode = conf.disk_access_mode;\n            logger.info(\"DiskAccessMode is {}, indexAccessMode is {}\", conf.disk_access_mode, indexAccessMode);\n        }\n\n        /* Authentication, authorization and role management backend, implementing IAuthenticator, IAuthorizer & IRoleMapper*/\n        if (conf.authenticator != null)\n            authenticator = FBUtilities.newAuthenticator(conf.authenticator);\n\n        if (conf.authorizer != null)\n            authorizer = FBUtilities.newAuthorizer(conf.authorizer);\n\n        if (authenticator instanceof AllowAllAuthenticator && !(authorizer instanceof AllowAllAuthorizer))\n            throw new ConfigurationException(\"AllowAllAuthenticator can't be used with \" +  conf.authorizer, false);\n\n        if (conf.role_manager != null)\n            roleManager = FBUtilities.newRoleManager(conf.role_manager);\n        else\n            roleManager = new CassandraRoleManager();\n\n        if (authenticator instanceof PasswordAuthenticator && !(roleManager instanceof CassandraRoleManager))\n            throw new ConfigurationException(\"CassandraRoleManager must be used with PasswordAuthenticator\", false);\n\n        if (conf.internode_authenticator != null)\n            internodeAuthenticator = FBUtilities.construct(conf.internode_authenticator, \"internode_authenticator\");\n        else\n            internodeAuthenticator = new AllowAllInternodeAuthenticator();\n\n        authenticator.validateConfiguration();\n        authorizer.validateConfiguration();\n        roleManager.validateConfiguration();\n        internodeAuthenticator.validateConfiguration();\n\n        /* Hashing strategy */\n        if (conf.partitioner == null)\n        {\n            throw new ConfigurationException(\"Missing directive: partitioner\", false);\n        }\n        try\n        {\n            partitioner = FBUtilities.newPartitioner(System.getProperty(\"cassandra.partitioner\", conf.partitioner));\n        }\n        catch (Exception e)\n        {\n            throw new ConfigurationException(\"Invalid partitioner class \" + conf.partitioner, false);\n        }\n        paritionerName = partitioner.getClass().getCanonicalName();\n\n        if (conf.gc_warn_threshold_in_ms < 0)\n        {\n            throw new ConfigurationException(\"gc_warn_threshold_in_ms must be a positive integer\");\n        }\n\n        if (conf.max_hint_window_in_ms == null)\n        {\n            throw new ConfigurationException(\"max_hint_window_in_ms cannot be set to null\", false);\n        }\n\n        /* phi convict threshold for FailureDetector */\n        if (conf.phi_convict_threshold < 5 || conf.phi_convict_threshold > 16)\n        {\n            throw new ConfigurationException(\"phi_convict_threshold must be between 5 and 16\", false);\n        }\n\n        /* Thread per pool */\n        if (conf.concurrent_reads != null && conf.concurrent_reads < 2)\n        {\n            throw new ConfigurationException(\"concurrent_reads must be at least 2\", false);\n        }\n\n        if (conf.concurrent_writes != null && conf.concurrent_writes < 2)\n        {\n            throw new ConfigurationException(\"concurrent_writes must be at least 2\", false);\n        }\n\n        if (conf.concurrent_counter_writes != null && conf.concurrent_counter_writes < 2)\n            throw new ConfigurationException(\"concurrent_counter_writes must be at least 2\", false);\n\n        if (conf.concurrent_replicates != null)\n            logger.warn(\"concurrent_replicates has been deprecated and should be removed from cassandra.yaml\");\n\n        if (conf.file_cache_size_in_mb == null)\n            conf.file_cache_size_in_mb = Math.min(512, (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576)));\n\n        if (conf.memory_allocator != null)\n            logger.warn(\"memory_allocator has been deprecated and should be removed from cassandra.yaml\");\n\n        if (conf.memtable_offheap_space_in_mb == null)\n            conf.memtable_offheap_space_in_mb = (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576));\n        if (conf.memtable_offheap_space_in_mb < 0)\n            throw new ConfigurationException(\"memtable_offheap_space_in_mb must be positive\", false);\n        // for the moment, we default to twice as much on-heap space as off-heap, as heap overhead is very large\n        if (conf.memtable_heap_space_in_mb == null)\n            conf.memtable_heap_space_in_mb = (int) (Runtime.getRuntime().maxMemory() / (4 * 1048576));\n        if (conf.memtable_heap_space_in_mb <= 0)\n            throw new ConfigurationException(\"memtable_heap_space_in_mb must be positive\", false);\n        logger.info(\"Global memtable on-heap threshold is enabled at {}MB\", conf.memtable_heap_space_in_mb);\n        if (conf.memtable_offheap_space_in_mb == 0)\n            logger.info(\"Global memtable off-heap threshold is disabled, HeapAllocator will be used instead\");\n        else\n            logger.info(\"Global memtable off-heap threshold is enabled at {}MB\", conf.memtable_offheap_space_in_mb);\n\n        applyAddressConfig(config);\n\n        if (conf.thrift_framed_transport_size_in_mb <= 0)\n            throw new ConfigurationException(\"thrift_framed_transport_size_in_mb must be positive\", false);\n\n        if (conf.native_transport_max_frame_size_in_mb <= 0)\n            throw new ConfigurationException(\"native_transport_max_frame_size_in_mb must be positive\", false);\n\n        // fail early instead of OOMing (see CASSANDRA-8116)\n        if (ThriftServer.HSHA.equals(conf.rpc_server_type) && conf.rpc_max_threads == Integer.MAX_VALUE)\n            throw new ConfigurationException(\"The hsha rpc_server_type is not compatible with an rpc_max_threads \" +\n                                             \"setting of 'unlimited'.  Please see the comments in cassandra.yaml \" +\n                                             \"for rpc_server_type and rpc_max_threads.\",\n                                             false);\n        if (ThriftServer.HSHA.equals(conf.rpc_server_type) && conf.rpc_max_threads > (FBUtilities.getAvailableProcessors() * 2 + 1024))\n            logger.warn(\"rpc_max_threads setting of {} may be too high for the hsha server and cause unnecessary thread contention, reducing performance\", conf.rpc_max_threads);\n\n        /* end point snitch */\n        if (conf.endpoint_snitch == null)\n        {\n            throw new ConfigurationException(\"Missing endpoint_snitch directive\", false);\n        }\n        snitch = createEndpointSnitch(conf.endpoint_snitch);\n        EndpointSnitchInfo.create();\n\n        localDC = snitch.getDatacenter(FBUtilities.getBroadcastAddress());\n        localComparator = new Comparator<InetAddress>()\n        {\n            public int compare(InetAddress endpoint1, InetAddress endpoint2)\n            {\n                boolean local1 = localDC.equals(snitch.getDatacenter(endpoint1));\n                boolean local2 = localDC.equals(snitch.getDatacenter(endpoint2));\n                if (local1 && !local2)\n                    return -1;\n                if (local2 && !local1)\n                    return 1;\n                return 0;\n            }\n        };\n\n        /* Request Scheduler setup */\n        requestSchedulerOptions = conf.request_scheduler_options;\n        if (conf.request_scheduler != null)\n        {\n            try\n            {\n                if (requestSchedulerOptions == null)\n                {\n                    requestSchedulerOptions = new RequestSchedulerOptions();\n                }\n                Class<?> cls = Class.forName(conf.request_scheduler);\n                requestScheduler = (IRequestScheduler) cls.getConstructor(RequestSchedulerOptions.class).newInstance(requestSchedulerOptions);\n            }\n            catch (ClassNotFoundException e)\n            {\n                throw new ConfigurationException(\"Invalid Request Scheduler class \" + conf.request_scheduler, false);\n            }\n            catch (Exception e)\n            {\n                throw new ConfigurationException(\"Unable to instantiate request scheduler\", e);\n            }\n        }\n        else\n        {\n            requestScheduler = new NoScheduler();\n        }\n\n        if (conf.request_scheduler_id == RequestSchedulerId.keyspace)\n        {\n            requestSchedulerId = conf.request_scheduler_id;\n        }\n        else\n        {\n            // Default to Keyspace\n            requestSchedulerId = RequestSchedulerId.keyspace;\n        }\n\n        // if data dirs, commitlog dir, or saved caches dir are set in cassandra.yaml, use that.  Otherwise,\n        // use -Dcassandra.storagedir (set in cassandra-env.sh) as the parent dir for data/, commitlog/, and saved_caches/\n        if (conf.commitlog_directory == null)\n        {\n            conf.commitlog_directory = System.getProperty(\"cassandra.storagedir\", null);\n            if (conf.commitlog_directory == null)\n                throw new ConfigurationException(\"commitlog_directory is missing and -Dcassandra.storagedir is not set\", false);\n            conf.commitlog_directory += File.separator + \"commitlog\";\n        }\n\n        if (conf.commitlog_total_space_in_mb == null)\n        {\n            int preferredSize = 8192;\n            int minSize = 0;\n            try\n            {\n                // use 1/4 of available space.  See discussion on #10013 and #10199\n                minSize = Ints.checkedCast((guessFileStore(conf.commitlog_directory).getTotalSpace() / 1048576) / 4);\n            }\n            catch (IOException e)\n            {\n                logger.debug(\"Error checking disk space\", e);\n                throw new ConfigurationException(String.format(\"Unable to check disk space available to %s. Perhaps the Cassandra user does not have the necessary permissions\",\n                                                               conf.commitlog_directory), e);\n            }\n            if (minSize < preferredSize)\n            {\n                logger.warn(\"Small commitlog volume detected at {}; setting commitlog_total_space_in_mb to {}.  You can override this in cassandra.yaml\",\n                            conf.commitlog_directory, minSize);\n                conf.commitlog_total_space_in_mb = minSize;\n            }\n            else\n            {\n                conf.commitlog_total_space_in_mb = preferredSize;\n            }\n        }\n\n        if (conf.saved_caches_directory == null)\n        {\n            conf.saved_caches_directory = System.getProperty(\"cassandra.storagedir\", null);\n            if (conf.saved_caches_directory == null)\n                throw new ConfigurationException(\"saved_caches_directory is missing and -Dcassandra.storagedir is not set\", false);\n            conf.saved_caches_directory += File.separator + \"saved_caches\";\n        }\n        if (conf.data_file_directories == null)\n        {\n            String defaultDataDir = System.getProperty(\"cassandra.storagedir\", null);\n            if (defaultDataDir == null)\n                throw new ConfigurationException(\"data_file_directories is not missing and -Dcassandra.storagedir is not set\", false);\n            conf.data_file_directories = new String[]{ defaultDataDir + File.separator + \"data\" };\n        }\n\n        long dataFreeBytes = 0;\n        /* data file and commit log directories. they get created later, when they're needed. */\n        for (String datadir : conf.data_file_directories)\n        {\n            if (datadir.equals(conf.commitlog_directory))\n                throw new ConfigurationException(\"commitlog_directory must not be the same as any data_file_directories\", false);\n            if (datadir.equals(conf.saved_caches_directory))\n                throw new ConfigurationException(\"saved_caches_directory must not be the same as any data_file_directories\", false);\n\n            try\n            {\n                dataFreeBytes += guessFileStore(datadir).getUnallocatedSpace();\n            }\n            catch (IOException e)\n            {\n                logger.debug(\"Error checking disk space\", e);\n                throw new ConfigurationException(String.format(\"Unable to check disk space available to %s. Perhaps the Cassandra user does not have the necessary permissions\",\n                                                               datadir), e);\n            }\n        }\n        if (dataFreeBytes < 64L * 1024 * 1048576) // 64 GB\n            logger.warn(\"Only {} MB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots\",\n                        dataFreeBytes / 1048576);\n\n\n        if (conf.commitlog_directory.equals(conf.saved_caches_directory))\n            throw new ConfigurationException(\"saved_caches_directory must not be the same as the commitlog_directory\", false);\n\n        if (conf.memtable_flush_writers == null)\n            conf.memtable_flush_writers = Math.min(8, Math.max(2, Math.min(FBUtilities.getAvailableProcessors(), conf.data_file_directories.length)));\n\n        if (conf.memtable_flush_writers < 1)\n            throw new ConfigurationException(\"memtable_flush_writers must be at least 1\", false);\n\n        if (conf.memtable_cleanup_threshold == null)\n            conf.memtable_cleanup_threshold = (float) (1.0 / (1 + conf.memtable_flush_writers));\n\n        if (conf.memtable_cleanup_threshold < 0.01f)\n            throw new ConfigurationException(\"memtable_cleanup_threshold must be >= 0.01\", false);\n        if (conf.memtable_cleanup_threshold > 0.99f)\n            throw new ConfigurationException(\"memtable_cleanup_threshold must be <= 0.99\", false);\n        if (conf.memtable_cleanup_threshold < 0.1f)\n            logger.warn(\"memtable_cleanup_threshold is set very low, which may cause performance degradation\");\n\n        if (conf.concurrent_compactors == null)\n            conf.concurrent_compactors = Math.min(8, Math.max(2, Math.min(FBUtilities.getAvailableProcessors(), conf.data_file_directories.length)));\n\n        if (conf.concurrent_compactors <= 0)\n            throw new ConfigurationException(\"concurrent_compactors should be strictly greater than 0\", false);\n\n        if (conf.initial_token != null)\n            for (String token : tokensFromString(conf.initial_token))\n                partitioner.getTokenFactory().validate(token);\n\n        if (conf.num_tokens == null)\n        \tconf.num_tokens = 1;\n        else if (conf.num_tokens > MAX_NUM_TOKENS)\n            throw new ConfigurationException(String.format(\"A maximum number of %d tokens per node is supported\", MAX_NUM_TOKENS), false);\n\n        try\n        {\n            // if key_cache_size_in_mb option was set to \"auto\" then size of the cache should be \"min(5% of Heap (in MB), 100MB)\n            keyCacheSizeInMB = (conf.key_cache_size_in_mb == null)\n                ? Math.min(Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.05 / 1024 / 1024)), 100)\n                : conf.key_cache_size_in_mb;\n\n            if (keyCacheSizeInMB < 0)\n                throw new NumberFormatException(); // to escape duplicating error message\n        }\n        catch (NumberFormatException e)\n        {\n            throw new ConfigurationException(\"key_cache_size_in_mb option was set incorrectly to '\"\n                    + conf.key_cache_size_in_mb + \"', supported values are <integer> >= 0.\", false);\n        }\n\n        try\n        {\n            // if counter_cache_size_in_mb option was set to \"auto\" then size of the cache should be \"min(2.5% of Heap (in MB), 50MB)\n            counterCacheSizeInMB = (conf.counter_cache_size_in_mb == null)\n                    ? Math.min(Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.025 / 1024 / 1024)), 50)\n                    : conf.counter_cache_size_in_mb;\n\n            if (counterCacheSizeInMB < 0)\n                throw new NumberFormatException(); // to escape duplicating error message\n        }\n        catch (NumberFormatException e)\n        {\n            throw new ConfigurationException(\"counter_cache_size_in_mb option was set incorrectly to '\"\n                    + conf.counter_cache_size_in_mb + \"', supported values are <integer> >= 0.\", false);\n        }\n\n        // if set to empty/\"auto\" then use 5% of Heap size\n        indexSummaryCapacityInMB = (conf.index_summary_capacity_in_mb == null)\n            ? Math.max(1, (int) (Runtime.getRuntime().totalMemory() * 0.05 / 1024 / 1024))\n            : conf.index_summary_capacity_in_mb;\n\n        if (indexSummaryCapacityInMB < 0)\n            throw new ConfigurationException(\"index_summary_capacity_in_mb option was set incorrectly to '\"\n                    + conf.index_summary_capacity_in_mb + \"', it should be a non-negative integer.\", false);\n\n        if(conf.encryption_options != null)\n        {\n            logger.warn(\"Please rename encryption_options as server_encryption_options in the yaml\");\n            //operate under the assumption that server_encryption_options is not set in yaml rather than both\n            conf.server_encryption_options = conf.encryption_options;\n        }\n\n        // load the seeds for node contact points\n        if (conf.seed_provider == null)\n        {\n            throw new ConfigurationException(\"seeds configuration is missing; a minimum of one seed is required.\", false);\n        }\n        try\n        {\n            Class<?> seedProviderClass = Class.forName(conf.seed_provider.class_name);\n            seedProvider = (SeedProvider)seedProviderClass.getConstructor(Map.class).newInstance(conf.seed_provider.parameters);\n        }\n        // there are about 5 checked exceptions that could be thrown here.\n        catch (Exception e)\n        {\n            throw new ConfigurationException(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\", false);\n        }\n        if (seedProvider.getSeeds().size() == 0)\n            throw new ConfigurationException(\"The seed provider lists no seeds.\", false);\n    }"
        ]
    ],
    "a55fd76ddd96e3ed2d967910f0572804fcfacc2f": [
        [
            "OutboundTcpConnectionPool::newSocket(InetAddress)",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135 -\n 136 -\n 137 -\n 138  \n 139  ",
            "    public static Socket newSocket(InetAddress endpoint) throws IOException\n    {\n        // zero means 'bind on any available port.'\n        if (isEncryptedChannel(endpoint))\n        {\n            if (Config.getOutboundBindAny())\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort());\n            else\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort(), FBUtilities.getLocalAddress(), 0);\n        }\n        else\n        {\n            Socket socket = SocketChannel.open(new InetSocketAddress(endpoint, DatabaseDescriptor.getStoragePort())).socket();\n            if (Config.getOutboundBindAny() && !socket.isBound())\n                socket.bind(new InetSocketAddress(FBUtilities.getLocalAddress(), 0));\n            return socket;\n        }\n    }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139  \n 140  ",
            "    public static Socket newSocket(InetAddress endpoint) throws IOException\n    {\n        // zero means 'bind on any available port.'\n        if (isEncryptedChannel(endpoint))\n        {\n            if (Config.getOutboundBindAny())\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort());\n            else\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort(), FBUtilities.getLocalAddress(), 0);\n        }\n        else\n        {\n            SocketChannel channel = SocketChannel.open();\n            if (!Config.getOutboundBindAny())\n                channel.bind(new InetSocketAddress(FBUtilities.getLocalAddress(), 0));\n            channel.connect(new InetSocketAddress(endpoint, DatabaseDescriptor.getStoragePort()));\n            return channel.socket();\n        }\n    }"
        ]
    ],
    "3c6dfa4aa0b9ffb0a48a02b949bff2a8406764e6": [
        [
            "ReadCommand::CheckForAbort::applyToRow(Row)",
            " 486  \n 487  \n 488 -\n 489 -\n 490  ",
            "        protected Row applyToRow(Row row)\n        {\n            maybeAbort();\n            return row;\n        }",
            " 491  \n 492  \n 493 +\n 494  ",
            "        protected Row applyToRow(Row row)\n        {\n            return maybeAbort() ? null : row;\n        }"
        ],
        [
            "ReadCommand::CheckForAbort::applyToPartition(BaseRowIterator)",
            " 480  \n 481  \n 482 -\n 483  \n 484  ",
            "        protected BaseRowIterator<?> applyToPartition(BaseRowIterator partition)\n        {\n            maybeAbort();\n            return partition;\n        }",
            " 480  \n 481  \n 482 +\n 483 +\n 484 +\n 485 +\n 486 +\n 487 +\n 488  \n 489  ",
            "        protected BaseRowIterator<?> applyToPartition(BaseRowIterator partition)\n        {\n            if (maybeAbort())\n            {\n                partition.close();\n                return null;\n            }\n\n            return partition;\n        }"
        ],
        [
            "ReadCommand::CheckForAbort::maybeAbort()",
            " 492 -\n 493  \n 494  \n 495  \n 496  \n 497 -\n 498 -\n 499  ",
            "        private void maybeAbort()\n        {\n            if (isAborted())\n                stop();\n\n            if (TEST_ITERATION_DELAY_MILLIS > 0)\n                maybeDelayForTesting();\n        }",
            " 496 +\n 497  \n 498 +\n 499 +\n 500 +\n 501  \n 502 +\n 503  \n 504 +\n 505 +\n 506  \n 507 +\n 508  ",
            "        private boolean maybeAbort()\n        {\n            if (TEST_ITERATION_DELAY_MILLIS > 0)\n                maybeDelayForTesting();\n\n            if (isAborted())\n            {\n                stop();\n                return true;\n            }\n\n            return false;\n        }"
        ]
    ],
    "98cc2c8d6cc27f1a2e675030a13b46fd336812f8": [
        [
            "ActiveRepairService::ParentRepairSession::ParentRepairSession(List,Collection,long)",
            " 434  \n 435  \n 436  \n 437  \n 438  \n 439 -\n 440  \n 441  ",
            "        public ParentRepairSession(List<ColumnFamilyStore> columnFamilyStores, Collection<Range<Token>> ranges, long repairedAt)\n        {\n            for (ColumnFamilyStore cfs : columnFamilyStores)\n                this.columnFamilyStores.put(cfs.metadata.cfId, cfs);\n            this.ranges = ranges;\n            this.sstableMap = new HashMap<>();\n            this.repairedAt = repairedAt;\n        }",
            " 434  \n 435  \n 436  \n 437 +\n 438  \n 439 +\n 440 +\n 441  \n 442  \n 443  ",
            "        public ParentRepairSession(List<ColumnFamilyStore> columnFamilyStores, Collection<Range<Token>> ranges, long repairedAt)\n        {\n            for (ColumnFamilyStore cfs : columnFamilyStores)\n            {\n                this.columnFamilyStores.put(cfs.metadata.cfId, cfs);\n                sstableMap.put(cfs.metadata.cfId, new HashSet<SSTableReader>());\n            }\n            this.ranges = ranges;\n            this.repairedAt = repairedAt;\n        }"
        ],
        [
            "ActiveRepairService::ParentRepairSession::addSSTables(UUID,Collection)",
            " 467  \n 468  \n 469 -\n 470 -\n 471 -\n 472 -\n 473 -\n 474  ",
            "        public void addSSTables(UUID cfId, Collection<SSTableReader> sstables)\n        {\n            Set<SSTableReader> existingSSTables = this.sstableMap.get(cfId);\n            if (existingSSTables == null)\n                existingSSTables = new HashSet<>();\n            existingSSTables.addAll(sstables);\n            this.sstableMap.put(cfId, existingSSTables);\n        }",
            " 469  \n 470  \n 471 +\n 472  ",
            "        public void addSSTables(UUID cfId, Collection<SSTableReader> sstables)\n        {\n            sstableMap.get(cfId).addAll(sstables);\n        }"
        ]
    ],
    "cb21f28ecadceaac78aba1c2de4670afc2c38daa": [
        [
            "Schema::addTable(CFMetaData)",
            " 605  \n 606  \n 607  \n 608  \n 609 -\n 610 -\n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  ",
            "    public void addTable(CFMetaData cfm)\n    {\n        assert getCFMetaData(cfm.ksName, cfm.cfName) == null;\n\n        // Make sure the keyspace is initialized and initialize the table.\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        // Update the keyspaces map with the updated metadata\n        update(cfm.ksName, ks -> ks.withSwapped(ks.tables.with(cfm)));\n        // Update the table ID <-> table name map (cfIdMap)\n        load(cfm);\n\n        // init the new CF before switching the KSM to the new one\n        // to avoid races as in CASSANDRA-10761\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        MigrationManager.instance.notifyCreateColumnFamily(cfm);\n    }",
            " 605  \n 606  \n 607  \n 608  \n 609 +\n 610 +\n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  ",
            "    public void addTable(CFMetaData cfm)\n    {\n        assert getCFMetaData(cfm.ksName, cfm.cfName) == null;\n\n        // Make sure the keyspace is initialized\n        Keyspace.open(cfm.ksName);\n        // Update the keyspaces map with the updated metadata\n        update(cfm.ksName, ks -> ks.withSwapped(ks.tables.with(cfm)));\n        // Update the table ID <-> table name map (cfIdMap)\n        load(cfm);\n\n        // init the new CF before switching the KSM to the new one\n        // to avoid races as in CASSANDRA-10761\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        MigrationManager.instance.notifyCreateColumnFamily(cfm);\n    }"
        ]
    ],
    "c00395155e2420002db493ca69784d0fe723b55d": [
        [
            "CompactionManager::performCleanup(ColumnFamilyStore)",
            " 400  \n 401  \n 402  \n 403  \n 404 -\n 405 -\n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  ",
            "    public AllSSTableOpStatus performCleanup(final ColumnFamilyStore cfStore) throws InterruptedException, ExecutionException\n    {\n        assert !cfStore.isIndex();\n        Keyspace keyspace = cfStore.keyspace;\n        final Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());\n        if (ranges.isEmpty())\n        {\n            logger.info(\"Cleanup cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n        final boolean hasIndexes = cfStore.indexManager.hasIndexes();\n\n        return parallelAllSSTableOperation(cfStore, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());\n                Collections.sort(sortedSSTables, new SSTableReader.SizeComparator());\n                return sortedSSTables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                CleanupStrategy cleanupStrategy = CleanupStrategy.get(cfStore, ranges);\n                doCleanupOne(cfStore, txn, cleanupStrategy, ranges, hasIndexes);\n            }\n        }, OperationType.CLEANUP);\n    }",
            " 400  \n 401  \n 402  \n 403  \n 404 +\n 405  \n 406  \n 407  \n 408  \n 409 +\n 410 +\n 411 +\n 412 +\n 413 +\n 414 +\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  ",
            "    public AllSSTableOpStatus performCleanup(final ColumnFamilyStore cfStore) throws InterruptedException, ExecutionException\n    {\n        assert !cfStore.isIndex();\n        Keyspace keyspace = cfStore.keyspace;\n        if (!StorageService.instance.isJoined())\n        {\n            logger.info(\"Cleanup cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n        final Collection<Range<Token>> ranges = StorageService.instance.getLocalRanges(keyspace.getName());\n        if (ranges.isEmpty())\n        {\n            logger.info(\"Node owns no data for keyspace {}\", keyspace.getName());\n            return AllSSTableOpStatus.SUCCESSFUL;\n        }\n        final boolean hasIndexes = cfStore.indexManager.hasIndexes();\n\n        return parallelAllSSTableOperation(cfStore, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());\n                Collections.sort(sortedSSTables, new SSTableReader.SizeComparator());\n                return sortedSSTables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                CleanupStrategy cleanupStrategy = CleanupStrategy.get(cfStore, ranges);\n                doCleanupOne(cfStore, txn, cleanupStrategy, ranges, hasIndexes);\n            }\n        }, OperationType.CLEANUP);\n    }"
        ]
    ],
    "2217695166a61f576b36993b36a6bde8c8952fde": [
        [
            "ReadResponse::LegacyRemoteDataResponse::makeIterator(ReadCommand)",
            " 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270 -\n 271  \n 272  \n 273  \n 274  \n 275  \n 276 -\n 277  \n 278 -\n 279  \n 280  \n 281  ",
            "        public UnfilteredPartitionIterator makeIterator(final ReadCommand command)\n        {\n            // Due to a bug in the serialization of AbstractBounds, anything that isn't a Range is understood by pre-3.0 nodes\n            // as a Bound, which means IncludingExcludingBounds and ExcludingBounds responses may include keys they shouldn't.\n            // So filter partitions that shouldn't be included here.\n            boolean skipFirst = false;\n            boolean skipLast = false;\n            if (!partitions.isEmpty() && command instanceof PartitionRangeReadCommand)\n            {\n                AbstractBounds<PartitionPosition> keyRange = ((PartitionRangeReadCommand)command).dataRange().keyRange();\n                boolean isExcludingBounds = keyRange instanceof ExcludingBounds;\n                skipFirst = isExcludingBounds && !keyRange.contains(partitions.get(0).partitionKey());\n                skipLast = (isExcludingBounds || keyRange instanceof IncludingExcludingBounds) && !keyRange.contains(partitions.get(partitions.size() - 1).partitionKey());\n            }\n\n            final List<ImmutableBTreePartition> toReturn;\n            if (skipFirst || skipLast)\n            {\n                toReturn = partitions.size() == 1\n                         ? Collections.emptyList()\n                         : partitions.subList(skipFirst ? 1 : 0, skipLast ? partitions.size() - 1 : partitions.size());\n            }\n            else\n            {\n                toReturn = partitions;\n            }\n\n            return new AbstractUnfilteredPartitionIterator()\n            {\n                private int idx;\n\n                public boolean isForThrift()\n                {\n                    return true;\n                }\n\n                public CFMetaData metadata()\n                {\n                    return command.metadata();\n                }\n\n                public boolean hasNext()\n                {\n                    return idx < toReturn.size();\n                }\n\n                public UnfilteredRowIterator next()\n                {\n                    ImmutableBTreePartition partition = toReturn.get(idx++);\n\n\n                    ClusteringIndexFilter filter = command.clusteringIndexFilter(partition.partitionKey());\n\n                    // Pre-3.0, we didn't have a way to express exclusivity for non-composite comparators, so all slices were\n                    // inclusive on both ends. If we have exclusive slice ends, we need to filter the results here.\n                    if (!command.metadata().isCompound())\n                        return filter.filter(partition.sliceableUnfilteredIterator(command.columnFilter(), filter.isReversed()));\n\n                    return partition.unfilteredIterator(command.columnFilter(), Slices.ALL, filter.isReversed());\n                }\n            };\n        }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276 +\n 277 +\n 278  \n 279 +\n 280 +\n 281  \n 282  \n 283  ",
            "        public UnfilteredPartitionIterator makeIterator(final ReadCommand command)\n        {\n            // Due to a bug in the serialization of AbstractBounds, anything that isn't a Range is understood by pre-3.0 nodes\n            // as a Bound, which means IncludingExcludingBounds and ExcludingBounds responses may include keys they shouldn't.\n            // So filter partitions that shouldn't be included here.\n            boolean skipFirst = false;\n            boolean skipLast = false;\n            if (!partitions.isEmpty() && command instanceof PartitionRangeReadCommand)\n            {\n                AbstractBounds<PartitionPosition> keyRange = ((PartitionRangeReadCommand)command).dataRange().keyRange();\n                boolean isExcludingBounds = keyRange instanceof ExcludingBounds;\n                skipFirst = isExcludingBounds && !keyRange.contains(partitions.get(0).partitionKey());\n                skipLast = (isExcludingBounds || keyRange instanceof IncludingExcludingBounds) && !keyRange.contains(partitions.get(partitions.size() - 1).partitionKey());\n            }\n\n            final List<ImmutableBTreePartition> toReturn;\n            if (skipFirst || skipLast)\n            {\n                toReturn = partitions.size() == 1\n                         ? Collections.emptyList()\n                         : partitions.subList(skipFirst ? 1 : 0, skipLast ? partitions.size() - 1 : partitions.size());\n            }\n            else\n            {\n                toReturn = partitions;\n            }\n\n            return new AbstractUnfilteredPartitionIterator()\n            {\n                private int idx;\n\n                public boolean isForThrift()\n                {\n                    return true;\n                }\n\n                public CFMetaData metadata()\n                {\n                    return command.metadata();\n                }\n\n                public boolean hasNext()\n                {\n                    return idx < toReturn.size();\n                }\n\n                public UnfilteredRowIterator next()\n                {\n                    ImmutableBTreePartition partition = toReturn.get(idx++);\n\n                    ClusteringIndexFilter filter = command.clusteringIndexFilter(partition.partitionKey());\n\n                    // Pre-3.0, we didn't have a way to express exclusivity for non-composite comparators, so all slices were\n                    // inclusive on both ends. If we have exclusive slice ends, we need to filter the results here.\n                    if (!command.metadata().isCompound())\n                        return ThriftResultsMerger.maybeWrap(\n                                filter.filter(partition.sliceableUnfilteredIterator(command.columnFilter(), filter.isReversed())), command.nowInSec());\n\n                    return ThriftResultsMerger.maybeWrap(\n                            partition.unfilteredIterator(command.columnFilter(), Slices.ALL, filter.isReversed()), command.nowInSec());\n                }\n            };\n        }"
        ]
    ],
    "eaa06942a6b5f54fb72bc7fe53d469cd034c2106": [
        [
            "LegacySSTableTest::copyFile(File,File)",
            " 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475 -\n 476 -\n 477 -\n 478 -\n 479  \n 480  ",
            "    private static void copyFile(File cfDir, File file) throws IOException\n    {\n        byte[] buf = new byte[65536];\n        if (file.isFile())\n        {\n            File target = new File(cfDir, file.getName());\n            int rd;\n            FileInputStream is = new FileInputStream(file);\n            FileOutputStream os = new FileOutputStream(target);\n            while ((rd = is.read(buf)) >= 0)\n                os.write(buf, 0, rd);\n        }\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482 +\n 483 +\n 484 +\n 485  \n 486  ",
            "    private static void copyFile(File cfDir, File file) throws IOException\n    {\n        byte[] buf = new byte[65536];\n        if (file.isFile())\n        {\n            File target = new File(cfDir, file.getName());\n            int rd;\n            try (FileInputStream is = new FileInputStream(file);\n                 FileOutputStream os = new FileOutputStream(target);) {\n                while ((rd = is.read(buf)) >= 0)\n                    os.write(buf, 0, rd);\n                }\n        }\n    }"
        ],
        [
            "LegacySSTableTest::defineSchema()",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102 -\n 103 -\n 104 -\n 105  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n        StorageService.instance.initServer();\n        Keyspace.setInitialized();\n        createKeyspace();\n        for (String legacyVersion : legacyVersions)\n        {\n            createTables(legacyVersion);\n        }\n        String scp = System.getProperty(LEGACY_SSTABLE_PROP);\n        assert scp != null;\n        LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();\n        assert LEGACY_SSTABLE_ROOT.isDirectory();\n    }",
            "  90  \n  91  \n  92  \n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        String scp = System.getProperty(LEGACY_SSTABLE_PROP);\n        Assert.assertNotNull(\"System property \" + LEGACY_SSTABLE_ROOT + \" not set\", scp);\n        \n        LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();\n        Assert.assertTrue(\"System property \" + LEGACY_SSTABLE_ROOT + \" does not specify a directory\", LEGACY_SSTABLE_ROOT.isDirectory());\n\n        SchemaLoader.prepareServer();\n        StorageService.instance.initServer();\n        Keyspace.setInitialized();\n        createKeyspace();\n        for (String legacyVersion : legacyVersions)\n        {\n            createTables(legacyVersion);\n        }\n\n    }"
        ],
        [
            "LegacySSTableTest::copySstablesToTestData(String,String,File)",
            " 455  \n 456  \n 457 -\n 458  \n 459  \n 460  \n 461  ",
            "    private static void copySstablesToTestData(String legacyVersion, String table, File cfDir) throws IOException\n    {\n        for (File file : getTableDir(legacyVersion, table).listFiles())\n        {\n            copyFile(cfDir, file);\n        }\n    }",
            " 458  \n 459  \n 460 +\n 461 +\n 462 +\n 463  \n 464  \n 465  \n 466  ",
            "    private static void copySstablesToTestData(String legacyVersion, String table, File cfDir) throws IOException\n    {\n        File tableDir = getTableDir(legacyVersion, table);\n        Assert.assertTrue(\"The table directory \" + tableDir + \" was not found\", tableDir.isDirectory());\n        for (File file : tableDir.listFiles())\n        {\n            copyFile(cfDir, file);\n        }\n    }"
        ]
    ],
    "5847222d9b2428c201a534876f86a0ec6f6f436f": [
        [
            "ReadCommand::LegacyRangeSliceCommandSerializer::serialize(ReadCommand,DataOutputPlus,int)",
            " 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 -\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            assert version < MessagingService.VERSION_30;\n\n            PartitionRangeReadCommand rangeCommand = (PartitionRangeReadCommand) command;\n            assert !rangeCommand.dataRange().isPaging();\n\n            // convert pre-3.0 incompatible names filters to slice filters\n            rangeCommand = maybeConvertNamesToSlice(rangeCommand);\n\n            CFMetaData metadata = rangeCommand.metadata();\n\n            out.writeUTF(metadata.ksName);\n            out.writeUTF(metadata.cfName);\n            out.writeLong(rangeCommand.nowInSec() * 1000L);  // convert from seconds to millis\n\n            // begin DiskAtomFilterSerializer.serialize()\n            if (rangeCommand.isNamesQuery())\n            {\n                out.writeByte(1);  // 0 for slices, 1 for names\n                ClusteringIndexNamesFilter filter = (ClusteringIndexNamesFilter) rangeCommand.dataRange().clusteringIndexFilter;\n                LegacyReadCommandSerializer.serializeNamesFilter(rangeCommand, filter, out);\n            }\n            else\n            {\n                out.writeByte(0);  // 0 for slices, 1 for names\n\n                // slice filter serialization\n                ClusteringIndexSliceFilter filter = (ClusteringIndexSliceFilter) rangeCommand.dataRange().clusteringIndexFilter;\n\n                boolean makeStaticSlice = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && !filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                LegacyReadCommandSerializer.serializeSlices(out, filter.requestedSlices(), filter.isReversed(), makeStaticSlice, metadata);\n\n                out.writeBoolean(filter.isReversed());\n\n                // limit\n                DataLimits limits = rangeCommand.limits();\n                if (limits.isDistinct())\n                    out.writeInt(1);\n                else\n                    out.writeInt(LegacyReadCommandSerializer.updateLimitForQuery(rangeCommand.limits().count(), filter.requestedSlices()));\n\n                int compositesToGroup;\n                boolean selectsStatics = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() || filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                if (limits.kind() == DataLimits.Kind.THRIFT_LIMIT)\n                    compositesToGroup = -1;\n                else if (limits.isDistinct() && !selectsStatics)\n                    compositesToGroup = -2;  // for DISTINCT queries (CASSANDRA-8490)\n                else\n                    compositesToGroup = metadata.isDense() ? -1 : metadata.clusteringColumns().size();\n\n                out.writeInt(compositesToGroup);\n            }\n\n            serializeRowFilter(out, rangeCommand.rowFilter());\n            AbstractBounds.rowPositionSerializer.serialize(rangeCommand.dataRange().keyRange(), out, version);\n\n            // maxResults\n            out.writeInt(rangeCommand.limits().count());\n\n            // countCQL3Rows\n            if (rangeCommand.isForThrift() || rangeCommand.limits().perPartitionCount() == 1)  // if for Thrift or DISTINCT\n                out.writeBoolean(false);\n            else\n                out.writeBoolean(true);\n\n            // isPaging\n            out.writeBoolean(false);\n        }",
            " 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 +\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            assert version < MessagingService.VERSION_30;\n\n            PartitionRangeReadCommand rangeCommand = (PartitionRangeReadCommand) command;\n            assert !rangeCommand.dataRange().isPaging();\n\n            // convert pre-3.0 incompatible names filters to slice filters\n            rangeCommand = maybeConvertNamesToSlice(rangeCommand);\n\n            CFMetaData metadata = rangeCommand.metadata();\n\n            out.writeUTF(metadata.ksName);\n            out.writeUTF(metadata.cfName);\n            out.writeLong(rangeCommand.nowInSec() * 1000L);  // convert from seconds to millis\n\n            // begin DiskAtomFilterSerializer.serialize()\n            if (rangeCommand.isNamesQuery())\n            {\n                out.writeByte(1);  // 0 for slices, 1 for names\n                ClusteringIndexNamesFilter filter = (ClusteringIndexNamesFilter) rangeCommand.dataRange().clusteringIndexFilter;\n                LegacyReadCommandSerializer.serializeNamesFilter(rangeCommand, filter, out);\n            }\n            else\n            {\n                out.writeByte(0);  // 0 for slices, 1 for names\n\n                // slice filter serialization\n                ClusteringIndexSliceFilter filter = (ClusteringIndexSliceFilter) rangeCommand.dataRange().clusteringIndexFilter;\n\n                boolean makeStaticSlice = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && !filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                LegacyReadCommandSerializer.serializeSlices(out, filter.requestedSlices(), filter.isReversed(), makeStaticSlice, metadata);\n\n                out.writeBoolean(filter.isReversed());\n\n                // limit\n                DataLimits limits = rangeCommand.limits();\n                if (limits.isDistinct())\n                    out.writeInt(1);\n                else\n                    out.writeInt(LegacyReadCommandSerializer.updateLimitForQuery(rangeCommand.limits().count(), filter.requestedSlices()));\n\n                int compositesToGroup;\n                boolean selectsStatics = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                if (limits.kind() == DataLimits.Kind.THRIFT_LIMIT)\n                    compositesToGroup = -1;\n                else if (limits.isDistinct() && !selectsStatics)\n                    compositesToGroup = -2;  // for DISTINCT queries (CASSANDRA-8490)\n                else\n                    compositesToGroup = metadata.isDense() ? -1 : metadata.clusteringColumns().size();\n\n                out.writeInt(compositesToGroup);\n            }\n\n            serializeRowFilter(out, rangeCommand.rowFilter());\n            AbstractBounds.rowPositionSerializer.serialize(rangeCommand.dataRange().keyRange(), out, version);\n\n            // maxResults\n            out.writeInt(rangeCommand.limits().count());\n\n            // countCQL3Rows\n            if (rangeCommand.isForThrift() || rangeCommand.limits().perPartitionCount() == 1)  // if for Thrift or DISTINCT\n                out.writeBoolean(false);\n            else\n                out.writeBoolean(true);\n\n            // isPaging\n            out.writeBoolean(false);\n        }"
        ],
        [
            "PagingState::RowMark::toString()",
            " 258  \n 259  \n 260  \n 261 -\n 262  ",
            "        @Override\n        public String toString()\n        {\n            return ByteBufferUtil.bytesToHex(mark);\n        }",
            " 261  \n 262  \n 263  \n 264 +\n 265  ",
            "        @Override\n        public String toString()\n        {\n            return mark == null ? \"null\" : ByteBufferUtil.bytesToHex(mark);\n        }"
        ],
        [
            "PagingState::RowMark::create(CFMetaData,Row,int)",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "        public static RowMark create(CFMetaData metadata, Row row, int protocolVersion)\n        {\n            ByteBuffer mark;\n            if (protocolVersion <= Server.VERSION_3)\n            {\n                // We need to be backward compatible with 2.1/2.2 nodes paging states. Which means we have to send\n                // the full cellname of the \"last\" cell in the row we get (since that's how 2.1/2.2 nodes will start after\n                // that last row if they get that paging state).\n                Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator();\n                if (!cells.hasNext())\n                {\n                    mark = LegacyLayout.encodeClustering(metadata, row.clustering());\n                }\n                else\n                {\n                    Cell cell = cells.next();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null);\n                }\n            }\n            else\n            {\n                // We froze the serialization version to 3.0 as we need to make this this doesn't change (that is, it has to be\n                // fix for a given version of the protocol).\n                mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata));\n            }\n            return new RowMark(mark, protocolVersion);\n        }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 +\n 217 +\n 218 +\n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "        public static RowMark create(CFMetaData metadata, Row row, int protocolVersion)\n        {\n            ByteBuffer mark;\n            if (protocolVersion <= Server.VERSION_3)\n            {\n                // We need to be backward compatible with 2.1/2.2 nodes paging states. Which means we have to send\n                // the full cellname of the \"last\" cell in the row we get (since that's how 2.1/2.2 nodes will start after\n                // that last row if they get that paging state).\n                Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator();\n                if (!cells.hasNext())\n                {\n                    // If the last returned row has no cell, this means in 2.1/2.2 terms that we stopped on the row\n                    // marker. Note that this shouldn't happen if the table is COMPACT.\n                    assert !metadata.isCompactTable();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), ByteBufferUtil.EMPTY_BYTE_BUFFER, null);\n                }\n                else\n                {\n                    Cell cell = cells.next();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null);\n                }\n            }\n            else\n            {\n                // We froze the serialization version to 3.0 as we need to make this this doesn't change (that is, it has to be\n                // fix for a given version of the protocol).\n                mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata));\n            }\n            return new RowMark(mark, protocolVersion);\n        }"
        ]
    ],
    "c92928bb9c2441254b51e2ea4dc742c9245b9f4c": [
        [
            "RepairRunnable::runMayThrow()",
            " 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies;\n        try\n        {\n            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              options.isPullRepair(),\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results)\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }",
            " 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies;\n        try\n        {\n            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d (%s), repairing keyspace %s with %s\", cmd, parentSession, keyspace,\n                                       options);\n        logger.info(message);\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            message = message + \" tracing with \" + sessionId;\n            fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              options.isPullRepair(),\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results)\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }"
        ]
    ],
    "52bf7acb0520411f420ccf36b9a3770674f604f6": [
        [
            "UnfilteredDeserializer::OldFormatDeserializer::TombstoneTracker::openNew(LegacyLayout)",
            " 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650 -\n 651  \n 652  \n 653  \n 654  ",
            "            /**\n             * Update the tracker given the provided newly open tombstone. This return the Unfiltered corresponding to the opening\n             * of said tombstone: this can be a simple open mark, a boundary (if there was an open tombstone superseded by this new one)\n             * or even null (if the new tombston start is supersedes by the currently open tombstone).\n             *\n             * Note that this method assume the added tombstone is not fully shadowed, i.e. that !isShadowed(tombstone). It also\n             * assumes no opened tombstone closes before that tombstone (so !hasClosingMarkerBefore(tombstone)).\n             */\n            public Unfiltered openNew(LegacyLayout.LegacyRangeTombstone tombstone)\n            {\n                if (openTombstones.isEmpty())\n                {\n                    openTombstones.add(tombstone);\n                    return new RangeTombstoneBoundMarker(tombstone.start.bound, tombstone.deletionTime);\n                }\n\n                Iterator<LegacyLayout.LegacyRangeTombstone> iter = openTombstones.iterator();\n                LegacyLayout.LegacyRangeTombstone first = iter.next();\n                if (tombstone.deletionTime.supersedes(first.deletionTime))\n                {\n                    // We're supperseding the currently open tombstone, so we should produce a boundary that close the currently open\n                    // one and open the new one. We should also add the tombstone, but if it stop after the first one, we should\n                    // also remove that first tombstone as it won't be useful anymore.\n                    if (metadata.comparator.compare(tombstone.stop.bound, first.stop.bound) >= 0)\n                        iter.remove();\n\n                    openTombstones.add(tombstone);\n                    return RangeTombstoneBoundaryMarker.makeBoundary(false, tombstone.start.bound.invert(), tombstone.start.bound, first.deletionTime, tombstone.deletionTime);\n                }\n                else\n                {\n                    // If the new tombstone don't supersedes the currently open tombstone, we don't have anything to return, we\n                    // just add the new tombstone (because we know tombstone is not fully shadowed, this imply the new tombstone\n                    // simply extend after the first one and we'll deal with it later)\n                    assert metadata.comparator.compare(tombstone.start.bound, first.stop.bound) > 0;\n                    openTombstones.add(tombstone);\n                    return null;\n                }\n            }",
            " 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650 +\n 651  \n 652  \n 653  \n 654  ",
            "            /**\n             * Update the tracker given the provided newly open tombstone. This return the Unfiltered corresponding to the opening\n             * of said tombstone: this can be a simple open mark, a boundary (if there was an open tombstone superseded by this new one)\n             * or even null (if the new tombston start is supersedes by the currently open tombstone).\n             *\n             * Note that this method assume the added tombstone is not fully shadowed, i.e. that !isShadowed(tombstone). It also\n             * assumes no opened tombstone closes before that tombstone (so !hasClosingMarkerBefore(tombstone)).\n             */\n            public Unfiltered openNew(LegacyLayout.LegacyRangeTombstone tombstone)\n            {\n                if (openTombstones.isEmpty())\n                {\n                    openTombstones.add(tombstone);\n                    return new RangeTombstoneBoundMarker(tombstone.start.bound, tombstone.deletionTime);\n                }\n\n                Iterator<LegacyLayout.LegacyRangeTombstone> iter = openTombstones.iterator();\n                LegacyLayout.LegacyRangeTombstone first = iter.next();\n                if (tombstone.deletionTime.supersedes(first.deletionTime))\n                {\n                    // We're supperseding the currently open tombstone, so we should produce a boundary that close the currently open\n                    // one and open the new one. We should also add the tombstone, but if it stop after the first one, we should\n                    // also remove that first tombstone as it won't be useful anymore.\n                    if (metadata.comparator.compare(tombstone.stop.bound, first.stop.bound) >= 0)\n                        iter.remove();\n\n                    openTombstones.add(tombstone);\n                    return RangeTombstoneBoundaryMarker.makeBoundary(false, tombstone.start.bound.invert(), tombstone.start.bound, first.deletionTime, tombstone.deletionTime);\n                }\n                else\n                {\n                    // If the new tombstone don't supersedes the currently open tombstone, we don't have anything to return, we\n                    // just add the new tombstone (because we know tombstone is not fully shadowed, this imply the new tombstone\n                    // simply extend after the first one and we'll deal with it later)\n                    assert metadata.comparator.compare(tombstone.start.bound, first.stop.bound) <= 0;\n                    openTombstones.add(tombstone);\n                    return null;\n                }\n            }"
        ]
    ],
    "883c9f0f743139d78996f5faf191508a9be338b5": [
        [
            "JsonTransformer::serializePartition(UnfilteredRowIterator)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  \n 196 -\n 197 -\n 198 -\n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205 -\n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n            {\n                serializeDeletion(partition.partitionLevelDeletion());\n                json.writeEndObject();\n            }\n            else\n            {\n                json.writeEndObject();\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                {\n                    serializeRow(partition.staticRow());\n                }\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n            }\n\n            json.writeEndObject();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197 +\n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n                serializeDeletion(partition.partitionLevelDeletion());\n\n            json.writeEndObject();\n\n            if (partition.hasNext() || partition.staticRow() != null)\n            {\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                    serializeRow(partition.staticRow());\n\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n\n                json.writeEndObject();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }"
        ]
    ],
    "a6237bf65a95d654b7e702e81fd0d353460d0c89": [
        [
            "OutboundTcpConnection::run()",
            " 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186 -\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "    public void run()\n    {\n        final int drainedMessageSize = 128;\n        // keeping list (batch) size small for now; that way we don't have an unbounded array (that we never resize)\n        final List<QueuedMessage> drainedMessages = new ArrayList<>(drainedMessageSize);\n\n        outer:\n        while (true)\n        {\n            try\n            {\n                cs.coalesce(backlog, drainedMessages, drainedMessageSize);\n            }\n            catch (InterruptedException e)\n            {\n                throw new AssertionError(e);\n            }\n\n            currentMsgBufferCount = drainedMessages.size();\n\n            int count = drainedMessages.size();\n            //The timestamp of the first message has already been provided to the coalescing strategy\n            //so skip logging it.\n            for (QueuedMessage qm : drainedMessages)\n            {\n                try\n                {\n                    MessageOut<?> m = qm.message;\n                    if (m == CLOSE_SENTINEL)\n                    {\n                        disconnect();\n                        if (isStopped)\n                            break outer;\n                        continue;\n                    }\n\n                    if (qm.isTimedOut())\n                        dropped.incrementAndGet();\n                    else if (socket != null || connect())\n                        writeConnected(qm, count == 1 && backlog.isEmpty());\n                    else\n                        // clear out the queue, else gossip messages back up.\n                        backlog.clear();\n                }\n                catch (Exception e)\n                {\n                    JVMStabilityInspector.inspectThrowable(e);\n                    // really shouldn't get here, as exception handling in writeConnected() is reasonably robust\n                    // but we want to catch anything bad we don't drop the messages in the current batch\n                    logger.error(\"error processing a message intended for {}\", poolReference.endPoint(), e);\n                }\n                currentMsgBufferCount = --count;\n            }\n            drainedMessages.clear();\n        }\n    }",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 +\n 225  \n 226 +\n 227  \n 228 +\n 229 +\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "    public void run()\n    {\n        final int drainedMessageSize = 128;\n        // keeping list (batch) size small for now; that way we don't have an unbounded array (that we never resize)\n        final List<QueuedMessage> drainedMessages = new ArrayList<>(drainedMessageSize);\n\n        outer:\n        while (!isStopped)\n        {\n            try\n            {\n                cs.coalesce(backlog, drainedMessages, drainedMessageSize);\n            }\n            catch (InterruptedException e)\n            {\n                throw new AssertionError(e);\n            }\n\n            currentMsgBufferCount = drainedMessages.size();\n\n            int count = drainedMessages.size();\n            //The timestamp of the first message has already been provided to the coalescing strategy\n            //so skip logging it.\n            inner:\n            for (QueuedMessage qm : drainedMessages)\n            {\n                try\n                {\n                    MessageOut<?> m = qm.message;\n                    if (m == CLOSE_SENTINEL)\n                    {\n                        disconnect();\n                        if (isStopped)\n                            break outer;\n                        continue;\n                    }\n\n                    if (qm.isTimedOut())\n                        dropped.incrementAndGet();\n                    else if (socket != null || connect())\n                        writeConnected(qm, count == 1 && backlog.isEmpty());\n                    else\n                    {\n                        // clear out the queue, else gossip messages back up.\n                        drainedMessages.clear();\n                        backlog.clear();\n                        break inner;\n                    }\n                }\n                catch (Exception e)\n                {\n                    JVMStabilityInspector.inspectThrowable(e);\n                    // really shouldn't get here, as exception handling in writeConnected() is reasonably robust\n                    // but we want to catch anything bad we don't drop the messages in the current batch\n                    logger.error(\"error processing a message intended for {}\", poolReference.endPoint(), e);\n                }\n                currentMsgBufferCount = --count;\n            }\n            drainedMessages.clear();\n        }\n    }"
        ],
        [
            "OutboundTcpConnection::closeSocket(boolean)",
            " 162  \n 163  \n 164 -\n 165  \n 166  \n 167  ",
            "    void closeSocket(boolean destroyThread)\n    {\n        backlog.clear();\n        isStopped = destroyThread; // Exit loop to stop the thread\n        enqueue(CLOSE_SENTINEL, -1);\n    }",
            " 162  \n 163  \n 164  \n 165 +\n 166 +\n 167 +\n 168 +\n 169  \n 170  ",
            "    void closeSocket(boolean destroyThread)\n    {\n        isStopped = destroyThread; // Exit loop to stop the thread\n        backlog.clear();\n        // in the \"destroyThread = true\" case, enqueuing the sentinel is important mostly to unblock the backlog.take()\n        // (via the CoalescingStrategy) in case there's a data race between this method enqueuing the sentinel\n        // and run() clearing the backlog on connection failure.\n        enqueue(CLOSE_SENTINEL, -1);\n    }"
        ]
    ],
    "6ed9134336bb48d04284cefd303d8374ed901c0a": [
        [
            "StorageService::excise(Collection,InetAddress)",
            "2246  \n2247  \n2248  \n2249  \n2250 -\n2251 -\n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  ",
            "    private void excise(Collection<Token> tokens, InetAddress endpoint)\n    {\n        logger.info(\"Removing tokens {} for {}\", tokens, endpoint);\n\n        if (tokenMetadata.isMember(endpoint))\n            HintsService.instance.excise(tokenMetadata.getHostId(endpoint));\n\n        removeEndpoint(endpoint);\n        tokenMetadata.removeEndpoint(endpoint);\n        if (!tokens.isEmpty())\n            tokenMetadata.removeBootstrapTokens(tokens);\n        notifyLeft(endpoint);\n        PendingRangeCalculatorService.instance.update();\n    }",
            "2246  \n2247  \n2248  \n2249  \n2250 +\n2251 +\n2252 +\n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  ",
            "    private void excise(Collection<Token> tokens, InetAddress endpoint)\n    {\n        logger.info(\"Removing tokens {} for {}\", tokens, endpoint);\n\n        UUID hostId = tokenMetadata.getHostId(endpoint);\n        if (hostId != null && tokenMetadata.isMember(endpoint))\n            HintsService.instance.excise(hostId);\n\n        removeEndpoint(endpoint);\n        tokenMetadata.removeEndpoint(endpoint);\n        if (!tokens.isEmpty())\n            tokenMetadata.removeBootstrapTokens(tokens);\n        notifyLeft(endpoint);\n        PendingRangeCalculatorService.instance.update();\n    }"
        ]
    ],
    "e885886d5c92bfd8d2fa1596bfa86d6a5a8d89bb": [
        [
            "ThreadAwareSecurityManager::SMAwareReconfigureOnChangeFilter::SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter)",
            " 105  \n 106  \n 107  \n 108  ",
            "        SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter reconfigureOnChangeFilter)\n        {\n            setRefreshPeriod(reconfigureOnChangeFilter.getRefreshPeriod());\n        }",
            " 105  \n 106  \n 107  \n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115  ",
            "        SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter reconfigureOnChangeFilter)\n        {\n            setRefreshPeriod(reconfigureOnChangeFilter.getRefreshPeriod());\n            setName(reconfigureOnChangeFilter.getName());\n            setContext(reconfigureOnChangeFilter.getContext());\n            if (reconfigureOnChangeFilter.isStarted())\n            {\n                reconfigureOnChangeFilter.stop();\n                start();\n            }\n        }"
        ]
    ],
    "17d43fa55eca29be492a716f04d9ceff1989762d": [
        [
            "CqlRecordWriter::RangeClient::run()",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302 -\n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308 -\n 309 -\n 310 -\n 311 -\n 312 -\n 313 -\n 314 -\n 315 -\n 316 -\n 317 -\n 318 -\n 319 -\n 320 -\n 321 -\n 322 -\n 323 -\n 324 -\n 325 -\n 326 -\n 327 -\n 328 -\n 329 -\n 330 -\n 331 -\n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 -\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        //There are too many ways for the Thread.interrupted() state to be cleared, so\n                        //we can't rely on that here. Until the java driver gives us a better way of knowing\n                        //that this exception came from an InterruptedException, this is the best solution.\n                        if (canRetryDriverConnection(e))\n                        {\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                        continue;\n                    }\n\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n                            \n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337 +\n 338 +\n 339 +\n 340 +\n 341 +\n 342 +\n 343 +\n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349 +\n 350 +\n 351 +\n 352 +\n 353 +\n 354 +\n 355 +\n 356 +\n 357 +\n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n\n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        //There are too many ways for the Thread.interrupted() state to be cleared, so\n                        //we can't rely on that here. Until the java driver gives us a better way of knowing\n                        //that this exception came from an InterruptedException, this is the best solution.\n                        if (canRetryDriverConnection(e))\n                        {\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }"
        ],
        [
            "CqlRecordWriter::RangeClient::closeInternal()",
            " 408  \n 409  \n 410  \n 411  \n 412 -\n 413  \n 414  ",
            "        protected void closeInternal()\n        {\n            if (client != null)\n            {\n                client.close();;\n            }\n        }",
            " 405  \n 406  \n 407  \n 408  \n 409 +\n 410  \n 411  ",
            "        protected void closeInternal()\n        {\n            if (client != null)\n            {\n                client.close();\n            }\n        }"
        ]
    ],
    "c1a113a9e3381d5278ca2254b0d0b062cfa7551b": [
        [
            "LegacyLayout::readLegacyAtom(CFMetaData,DataInputPlus,boolean)",
            " 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964 -\n 965  \n 966  \n 967  ",
            "    public static LegacyAtom readLegacyAtom(CFMetaData metadata, DataInputPlus in, boolean readAllAsDynamic) throws IOException\n    {\n        while (true)\n        {\n            ByteBuffer cellname = ByteBufferUtil.readWithShortLength(in);\n            if (!cellname.hasRemaining())\n                return null; // END_OF_ROW\n\n            try\n            {\n                int b = in.readUnsignedByte();\n                return (b & RANGE_TOMBSTONE_MASK) != 0\n                    ? readLegacyRangeTombstoneBody(metadata, in, cellname)\n                    : readLegacyCellBody(metadata, in, cellname, b, SerializationHelper.Flag.LOCAL, readAllAsDynamic);\n            }\n            catch (UnknownColumnException e)\n            {\n                // We can get there if we read a cell for a dropped column, and ff that is the case,\n                // then simply ignore the cell is fine. But also not that we ignore if it's the\n                // system keyspace because for those table we actually remove columns without registering\n                // them in the dropped columns\n                assert metadata.ksName.equals(SystemKeyspace.NAME) || metadata.getDroppedColumnDefinition(cellname) != null : e.getMessage();\n            }\n        }\n    }",
            " 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964 +\n 965  \n 966  \n 967  ",
            "    public static LegacyAtom readLegacyAtom(CFMetaData metadata, DataInputPlus in, boolean readAllAsDynamic) throws IOException\n    {\n        while (true)\n        {\n            ByteBuffer cellname = ByteBufferUtil.readWithShortLength(in);\n            if (!cellname.hasRemaining())\n                return null; // END_OF_ROW\n\n            try\n            {\n                int b = in.readUnsignedByte();\n                return (b & RANGE_TOMBSTONE_MASK) != 0\n                    ? readLegacyRangeTombstoneBody(metadata, in, cellname)\n                    : readLegacyCellBody(metadata, in, cellname, b, SerializationHelper.Flag.LOCAL, readAllAsDynamic);\n            }\n            catch (UnknownColumnException e)\n            {\n                // We can get there if we read a cell for a dropped column, and ff that is the case,\n                // then simply ignore the cell is fine. But also not that we ignore if it's the\n                // system keyspace because for those table we actually remove columns without registering\n                // them in the dropped columns\n                assert metadata.ksName.equals(SystemKeyspace.NAME) || metadata.getDroppedColumnDefinition(e.columnName) != null : e.getMessage();\n            }\n        }\n    }"
        ]
    ],
    "a8e2dc52409ce0dc7476d60b3adee34c547f0b14": [
        [
            "Selection::ResultSetBuilder::rowToJson(List,int)",
            " 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380 -\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  ",
            "        private List<ByteBuffer> rowToJson(List<ByteBuffer> row, int protocolVersion)\n        {\n            StringBuilder sb = new StringBuilder(\"{\");\n            for (int i = 0; i < metadata.names.size(); i++)\n            {\n                if (i > 0)\n                    sb.append(\", \");\n\n                ColumnSpecification spec = metadata.names.get(i);\n                String columnName = spec.name.toString();\n                if (!columnName.equals(columnName.toLowerCase(Locale.US)))\n                    columnName = \"\\\"\" + columnName + \"\\\"\";\n\n                ByteBuffer buffer = row.get(i);\n                sb.append('\"');\n                sb.append(Json.quoteAsJsonString(columnName));\n                sb.append(\"\\\": \");\n                if (buffer == null)\n                    sb.append(\"null\");\n                else\n                    sb.append(spec.type.toJSONString(buffer, protocolVersion));\n            }\n            sb.append(\"}\");\n            return Collections.singletonList(UTF8Type.instance.getSerializer().serialize(sb.toString()));\n        }",
            " 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380 +\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  ",
            "        private List<ByteBuffer> rowToJson(List<ByteBuffer> row, int protocolVersion)\n        {\n            StringBuilder sb = new StringBuilder(\"{\");\n            for (int i = 0; i < metadata.names.size(); i++)\n            {\n                if (i > 0)\n                    sb.append(\", \");\n\n                ColumnSpecification spec = metadata.names.get(i);\n                String columnName = spec.name.toString();\n                if (!columnName.equals(columnName.toLowerCase(Locale.US)))\n                    columnName = \"\\\"\" + columnName + \"\\\"\";\n\n                ByteBuffer buffer = row.get(i);\n                sb.append('\"');\n                sb.append(Json.quoteAsJsonString(columnName));\n                sb.append(\"\\\": \");\n                if (buffer == null || !buffer.hasRemaining())\n                    sb.append(\"null\");\n                else\n                    sb.append(spec.type.toJSONString(buffer, protocolVersion));\n            }\n            sb.append(\"}\");\n            return Collections.singletonList(UTF8Type.instance.getSerializer().serialize(sb.toString()));\n        }"
        ]
    ],
    "0a4728f62b51095706bf7155e8f60b39ec5fa082": [
        [
            "StreamSession::addTransferFiles(Collection)",
            " 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 -\n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  ",
            "    public void addTransferFiles(Collection<SSTableStreamingSections> sstableDetails)\n    {\n        Iterator<SSTableStreamingSections> iter = sstableDetails.iterator();\n        while (iter.hasNext())\n        {\n            SSTableStreamingSections details = iter.next();\n            if (details.sections.isEmpty())\n            {\n                // A reference was acquired on the sstable and we won't stream it\n                details.ref.release();\n                iter.remove();\n                continue;\n            }\n\n            UUID cfId = details.ref.get().metadata.cfId;\n            StreamTransferTask task = transfers.get(cfId);\n            if (task == null)\n            {\n                task = new StreamTransferTask(this, cfId);\n                transfers.put(cfId, task);\n            }\n            task.addTransferFile(details.ref, details.estimatedKeys, details.sections, details.repairedAt);\n            iter.remove();\n        }\n    }",
            " 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 +\n 373 +\n 374 +\n 375 +\n 376 +\n 377  \n 378  \n 379  \n 380  \n 381  ",
            "    public void addTransferFiles(Collection<SSTableStreamingSections> sstableDetails)\n    {\n        Iterator<SSTableStreamingSections> iter = sstableDetails.iterator();\n        while (iter.hasNext())\n        {\n            SSTableStreamingSections details = iter.next();\n            if (details.sections.isEmpty())\n            {\n                // A reference was acquired on the sstable and we won't stream it\n                details.ref.release();\n                iter.remove();\n                continue;\n            }\n\n            UUID cfId = details.ref.get().metadata.cfId;\n            StreamTransferTask task = transfers.get(cfId);\n            if (task == null)\n            {\n                //guarantee atomicity\n                StreamTransferTask newTask = new StreamTransferTask(this, cfId);\n                task = transfers.putIfAbsent(cfId, newTask);\n                if (task == null)\n                    task = newTask;\n            }\n            task.addTransferFile(details.ref, details.estimatedKeys, details.sections, details.repairedAt);\n            iter.remove();\n        }\n    }"
        ],
        [
            "ThriftSessionManager::currentSession()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 -\n  61 -\n  62  \n  63  \n  64  ",
            "    /**\n     * @return the current session for the most recently given socket on this thread\n     */\n    public ThriftClientState currentSession()\n    {\n        SocketAddress socket = remoteSocket.get();\n        assert socket != null;\n\n        ThriftClientState cState = activeSocketSessions.get(socket);\n        if (cState == null)\n        {\n            cState = new ThriftClientState(socket);\n            activeSocketSessions.put(socket, cState);\n        }\n        return cState;\n    }",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65  \n  66  \n  67  ",
            "    /**\n     * @return the current session for the most recently given socket on this thread\n     */\n    public ThriftClientState currentSession()\n    {\n        SocketAddress socket = remoteSocket.get();\n        assert socket != null;\n\n        ThriftClientState cState = activeSocketSessions.get(socket);\n        if (cState == null)\n        {\n            //guarantee atomicity\n            ThriftClientState newState = new ThriftClientState(socket);\n            cState = activeSocketSessions.putIfAbsent(socket, newState);\n            if (cState == null)\n                cState = newState;\n        }\n        return cState;\n    }"
        ]
    ],
    "6ff1cbb3ee1b7e6f261aeb454854dd249ab605df": [
        [
            "TokenMetadata::printPendingRanges()",
            "1028  \n1029  \n1030  \n1031  \n1032 -\n1033  \n1034 -\n1035 -\n1036 -\n1037 -\n1038 -\n1039  \n1040  \n1041  \n1042  ",
            "    private String printPendingRanges()\n    {\n        StringBuilder sb = new StringBuilder();\n\n        for (Map.Entry<String, Multimap<Range<Token>, InetAddress>> entry : pendingRanges.entrySet())\n        {\n            for (Map.Entry<Range<Token>, InetAddress> rmap : entry.getValue().entries())\n            {\n                sb.append(rmap.getValue()).append(':').append(rmap.getKey());\n                sb.append(System.getProperty(\"line.separator\"));\n            }\n        }\n\n        return sb.toString();\n    }",
            "1040  \n1041  \n1042  \n1043  \n1044 +\n1045  \n1046 +\n1047  \n1048  \n1049  \n1050  ",
            "    private String printPendingRanges()\n    {\n        StringBuilder sb = new StringBuilder();\n\n        for (PendingRangeMaps pendingRangeMaps : pendingRanges.values())\n        {\n            sb.append(pendingRangeMaps.printPendingRanges());\n        }\n\n        return sb.toString();\n    }"
        ],
        [
            "TokenMetadata::getPendingRanges(String)",
            " 689  \n 690 -\n 691  \n 692 -\n 693  ",
            "    /** a mutable map may be returned but caller should not modify it */\n    public Map<Range<Token>, Collection<InetAddress>> getPendingRanges(String keyspaceName)\n    {\n        return getPendingRangesMM(keyspaceName).asMap();\n    }",
            " 696  \n 697 +\n 698  \n 699 +\n 700  ",
            "    /** a mutable map may be returned but caller should not modify it */\n    public PendingRangeMaps getPendingRanges(String keyspaceName)\n    {\n        return this.pendingRanges.get(keyspaceName);\n    }"
        ],
        [
            "StorageService::getPendingRangeToEndpointMap(String)",
            "1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381 -\n1382  \n1383  \n1384  \n1385  \n1386  \n1387  ",
            "    public Map<List<String>, List<String>> getPendingRangeToEndpointMap(String keyspace)\n    {\n        // some people just want to get a visual representation of things. Allow null and set it to the first\n        // non-system keyspace.\n        if (keyspace == null)\n            keyspace = Schema.instance.getNonSystemKeyspaces().get(0);\n\n        Map<List<String>, List<String>> map = new HashMap<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata.getPendingRanges(keyspace).entrySet())\n        {\n            List<InetAddress> l = new ArrayList<>(entry.getValue());\n            map.put(entry.getKey().asList(), stringify(l));\n        }\n        return map;\n    }",
            "1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381 +\n1382  \n1383  \n1384  \n1385  \n1386  \n1387  ",
            "    public Map<List<String>, List<String>> getPendingRangeToEndpointMap(String keyspace)\n    {\n        // some people just want to get a visual representation of things. Allow null and set it to the first\n        // non-system keyspace.\n        if (keyspace == null)\n            keyspace = Schema.instance.getNonSystemKeyspaces().get(0);\n\n        Map<List<String>, List<String>> map = new HashMap<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata.getPendingRangesMM(keyspace).asMap().entrySet())\n        {\n            List<InetAddress> l = new ArrayList<>(entry.getValue());\n            map.put(entry.getKey().asList(), stringify(l));\n        }\n        return map;\n    }"
        ],
        [
            "TokenMetadata::calculatePendingRanges(AbstractReplicationStrategy,String)",
            " 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736 -\n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764 -\n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797 -\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            Multimap<Range<Token>, InetAddress> newPendingRanges = HashMultimap.create();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                newPendingRanges.putAll(range, Sets.difference(newEndpoints, currentEndpoints));\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                    newPendingRanges.put(range, endpoint);\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.put(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }",
            " 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743 +\n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771 +\n 772 +\n 773 +\n 774 +\n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789 +\n 790 +\n 791 +\n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809 +\n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            PendingRangeMaps newPendingRanges = new PendingRangeMaps();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                for (InetAddress address : Sets.difference(newEndpoints, currentEndpoints))\n                {\n                    newPendingRanges.addPendingRange(range, address);\n                }\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.addPendingRange(range, endpoint);\n                }\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.addPendingRange(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }"
        ],
        [
            "TokenMetadata::getPendingRangesMM(String)",
            " 676 -\n 677  \n 678 -\n 679 -\n 680  \n 681 -\n 682 -\n 683 -\n 684 -\n 685  \n 686  \n 687  ",
            "    private Multimap<Range<Token>, InetAddress> getPendingRangesMM(String keyspaceName)\n    {\n        Multimap<Range<Token>, InetAddress> map = pendingRanges.get(keyspaceName);\n        if (map == null)\n        {\n            map = HashMultimap.create();\n            Multimap<Range<Token>, InetAddress> priorMap = pendingRanges.putIfAbsent(keyspaceName, map);\n            if (priorMap != null)\n                map = priorMap;\n        }\n        return map;\n    }",
            " 676 +\n 677  \n 678 +\n 679 +\n 680 +\n 681 +\n 682  \n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690 +\n 691  \n 692 +\n 693  \n 694  ",
            "    public Multimap<Range<Token>, InetAddress> getPendingRangesMM(String keyspaceName)\n    {\n        Multimap<Range<Token>, InetAddress> map = HashMultimap.create();\n        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);\n\n        if (pendingRangeMaps != null)\n        {\n            for (Map.Entry<Range<Token>, List<InetAddress>> entry : pendingRangeMaps)\n            {\n                Range<Token> range = entry.getKey();\n                for (InetAddress address : entry.getValue())\n                {\n                    map.put(range, address);\n                }\n            }\n        }\n\n        return map;\n    }"
        ],
        [
            "TokenMetadata::pendingEndpointsFor(Token,String)",
            "1044  \n1045  \n1046 -\n1047 -\n1048  \n1049  \n1050 -\n1051 -\n1052 -\n1053 -\n1054 -\n1055 -\n1056 -\n1057 -\n1058  ",
            "    public Collection<InetAddress> pendingEndpointsFor(Token token, String keyspaceName)\n    {\n        Map<Range<Token>, Collection<InetAddress>> ranges = getPendingRanges(keyspaceName);\n        if (ranges.isEmpty())\n            return Collections.emptyList();\n\n        Set<InetAddress> endpoints = new HashSet<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : ranges.entrySet())\n        {\n            if (entry.getKey().contains(token))\n                endpoints.addAll(entry.getValue());\n        }\n\n        return endpoints;\n    }",
            "1052  \n1053  \n1054 +\n1055 +\n1056  \n1057  \n1058 +\n1059  ",
            "    public Collection<InetAddress> pendingEndpointsFor(Token token, String keyspaceName)\n    {\n        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);\n        if (pendingRangeMaps == null)\n            return Collections.emptyList();\n\n        return pendingRangeMaps.pendingEndpointsFor(token);\n    }"
        ]
    ],
    "e04efab3f9a60e5e8c34c845548b6ab6d0570376": [
        [
            "MetadataCollector::updateClusteringValues(ClusteringPrefix)",
            " 229  \n 230  \n 231 -\n 232 -\n 233  \n 234  ",
            "    public MetadataCollector updateClusteringValues(ClusteringPrefix clustering)\n    {\n        minClustering = comparator.compare(clustering, minClustering) < 0 ? clustering : minClustering;\n        maxClustering = comparator.compare(clustering, maxClustering) > 0 ? clustering : maxClustering;\n        return this;\n    }",
            " 230  \n 231  \n 232 +\n 233 +\n 234  \n 235  ",
            "    public MetadataCollector updateClusteringValues(ClusteringPrefix clustering)\n    {\n        minClustering = minClustering == null || comparator.compare(clustering, minClustering) < 0 ? clustering : minClustering;\n        maxClustering = maxClustering == null || comparator.compare(clustering, maxClustering) > 0 ? clustering : maxClustering;\n        return this;\n    }"
        ],
        [
            "MetadataCollector::finalizeMetadata(String,double,long,SerializationHeader)",
            " 272  \n 273  \n 274 -\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289 -\n 290 -\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  ",
            "    public Map<MetadataType, MetadataComponent> finalizeMetadata(String partitioner, double bloomFilterFPChance, long repairedAt, SerializationHeader header)\n    {\n        Preconditions.checkState(comparator.compare(maxClustering, minClustering) >= 0);\n        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();\n        components.put(MetadataType.VALIDATION, new ValidationMetadata(partitioner, bloomFilterFPChance));\n        components.put(MetadataType.STATS, new StatsMetadata(estimatedPartitionSize,\n                                                             estimatedCellPerPartitionCount,\n                                                             commitLogIntervals,\n                                                             timestampTracker.min(),\n                                                             timestampTracker.max(),\n                                                             localDeletionTimeTracker.min(),\n                                                             localDeletionTimeTracker.max(),\n                                                             ttlTracker.min(),\n                                                             ttlTracker.max(),\n                                                             compressionRatio,\n                                                             estimatedTombstoneDropTime.build(),\n                                                             sstableLevel,\n                                                             makeList(minClustering.getRawValues()),\n                                                             makeList(maxClustering.getRawValues()),\n                                                             hasLegacyCounterShards,\n                                                             repairedAt,\n                                                             totalColumnsSet,\n                                                             totalRows));\n        components.put(MetadataType.COMPACTION, new CompactionMetadata(cardinality));\n        components.put(MetadataType.HEADER, header.toComponent());\n        return components;\n    }",
            " 273  \n 274  \n 275 +\n 276 +\n 277 +\n 278 +\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293 +\n 294 +\n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  ",
            "    public Map<MetadataType, MetadataComponent> finalizeMetadata(String partitioner, double bloomFilterFPChance, long repairedAt, SerializationHeader header)\n    {\n        Preconditions.checkState((minClustering == null && maxClustering == null)\n                                 || comparator.compare(maxClustering, minClustering) >= 0);\n        ByteBuffer[] minValues = minClustering != null ? minClustering.getRawValues() : EMPTY_CLUSTERING;\n        ByteBuffer[] maxValues = maxClustering != null ? maxClustering.getRawValues() : EMPTY_CLUSTERING;\n        Map<MetadataType, MetadataComponent> components = Maps.newHashMap();\n        components.put(MetadataType.VALIDATION, new ValidationMetadata(partitioner, bloomFilterFPChance));\n        components.put(MetadataType.STATS, new StatsMetadata(estimatedPartitionSize,\n                                                             estimatedCellPerPartitionCount,\n                                                             commitLogIntervals,\n                                                             timestampTracker.min(),\n                                                             timestampTracker.max(),\n                                                             localDeletionTimeTracker.min(),\n                                                             localDeletionTimeTracker.max(),\n                                                             ttlTracker.min(),\n                                                             ttlTracker.max(),\n                                                             compressionRatio,\n                                                             estimatedTombstoneDropTime.build(),\n                                                             sstableLevel,\n                                                             makeList(minValues),\n                                                             makeList(maxValues),\n                                                             hasLegacyCounterShards,\n                                                             repairedAt,\n                                                             totalColumnsSet,\n                                                             totalRows));\n        components.put(MetadataType.COMPACTION, new CompactionMetadata(cardinality));\n        components.put(MetadataType.HEADER, header.toComponent());\n        return components;\n    }"
        ]
    ],
    "2cd18ef5a01a06d90e13e61971e5601c7de61e7c": [
        [
            "SSTableMetadataViewer::main(String)",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "    /**\n     * @param args a list of sstables whose metadata we're interested in\n     */\n    public static void main(String[] args) throws IOException\n    {\n        PrintStream out = System.out;\n        if (args.length == 0)\n        {\n            out.println(\"Usage: sstablemetadata <sstable filenames>\");\n            System.exit(1);\n        }\n\n        Util.initDatabaseDescriptor();\n\n        for (String fname : args)\n        {\n            if (new File(fname).exists())\n            {\n                Descriptor descriptor = Descriptor.fromFilename(fname);\n                Map<MetadataType, MetadataComponent> metadata = descriptor.getMetadataSerializer().deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n                ValidationMetadata validation = (ValidationMetadata) metadata.get(MetadataType.VALIDATION);\n                StatsMetadata stats = (StatsMetadata) metadata.get(MetadataType.STATS);\n                CompactionMetadata compaction = (CompactionMetadata) metadata.get(MetadataType.COMPACTION);\n\n                out.printf(\"SSTable: %s%n\", descriptor);\n                if (validation != null)\n                {\n                    out.printf(\"Partitioner: %s%n\", validation.partitioner);\n                    out.printf(\"Bloom Filter FP chance: %f%n\", validation.bloomFilterFPChance);\n                }\n                if (stats != null)\n                {\n                    out.printf(\"Minimum timestamp: %s%n\", stats.minTimestamp);\n                    out.printf(\"Maximum timestamp: %s%n\", stats.maxTimestamp);\n                    out.printf(\"SSTable max local deletion time: %s%n\", stats.maxLocalDeletionTime);\n                    out.printf(\"Compression ratio: %s%n\", stats.compressionRatio);\n                    out.printf(\"Estimated droppable tombstones: %s%n\", stats.getEstimatedDroppableTombstoneRatio((int) (System.currentTimeMillis() / 1000)));\n                    out.printf(\"SSTable Level: %d%n\", stats.sstableLevel);\n                    out.printf(\"Repaired at: %d%n\", stats.repairedAt);\n                    out.println(stats.replayPosition);\n                    out.println(\"Estimated tombstone drop times:\");\n                    for (Map.Entry<Double, Long> entry : stats.estimatedTombstoneDropTime.getAsMap().entrySet())\n                    {\n                        out.printf(\"%-10s:%10s%n\",entry.getKey().intValue(), entry.getValue());\n                    }\n                    printHistograms(stats, out);\n                }\n                if (compaction != null)\n                {\n                    out.printf(\"Estimated cardinality: %s%n\", compaction.cardinalityEstimator.cardinality());\n                }\n            }\n            else\n            {\n                out.println(\"No such file: \" + fname);\n            }\n        }\n    }",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "    /**\n     * @param args a list of sstables whose metadata we're interested in\n     */\n    public static void main(String[] args) throws IOException\n    {\n        PrintStream out = System.out;\n        if (args.length == 0)\n        {\n            out.println(\"Usage: sstablemetadata <sstable filenames>\");\n            System.exit(1);\n        }\n\n        Util.initDatabaseDescriptor();\n\n        for (String fname : args)\n        {\n            if (new File(fname).exists())\n            {\n                Descriptor descriptor = Descriptor.fromFilename(fname);\n                Map<MetadataType, MetadataComponent> metadata = descriptor.getMetadataSerializer().deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n                ValidationMetadata validation = (ValidationMetadata) metadata.get(MetadataType.VALIDATION);\n                StatsMetadata stats = (StatsMetadata) metadata.get(MetadataType.STATS);\n                CompactionMetadata compaction = (CompactionMetadata) metadata.get(MetadataType.COMPACTION);\n                CompressionMetadata compression = null;\n                File compressionFile = new File(descriptor.filenameFor(Component.COMPRESSION_INFO));\n                if (compressionFile.exists())\n                    compression = CompressionMetadata.create(fname);\n\n                out.printf(\"SSTable: %s%n\", descriptor);\n                if (validation != null)\n                {\n                    out.printf(\"Partitioner: %s%n\", validation.partitioner);\n                    out.printf(\"Bloom Filter FP chance: %f%n\", validation.bloomFilterFPChance);\n                }\n                if (stats != null)\n                {\n                    out.printf(\"Minimum timestamp: %s%n\", stats.minTimestamp);\n                    out.printf(\"Maximum timestamp: %s%n\", stats.maxTimestamp);\n                    out.printf(\"SSTable max local deletion time: %s%n\", stats.maxLocalDeletionTime);\n                    out.printf(\"Compressor: %s%n\", compression != null ? compression.compressor().getClass().getName() : \"-\");\n                    if (compression != null)\n                        out.printf(\"Compression ratio: %s%n\", stats.compressionRatio);\n                    out.printf(\"Estimated droppable tombstones: %s%n\", stats.getEstimatedDroppableTombstoneRatio((int) (System.currentTimeMillis() / 1000)));\n                    out.printf(\"SSTable Level: %d%n\", stats.sstableLevel);\n                    out.printf(\"Repaired at: %d%n\", stats.repairedAt);\n                    out.println(stats.replayPosition);\n                    out.println(\"Estimated tombstone drop times:\");\n                    for (Map.Entry<Double, Long> entry : stats.estimatedTombstoneDropTime.getAsMap().entrySet())\n                    {\n                        out.printf(\"%-10s:%10s%n\",entry.getKey().intValue(), entry.getValue());\n                    }\n                    printHistograms(stats, out);\n                }\n                if (compaction != null)\n                {\n                    out.printf(\"Estimated cardinality: %s%n\", compaction.cardinalityEstimator.cardinality());\n                }\n            }\n            else\n            {\n                out.println(\"No such file: \" + fname);\n            }\n        }\n    }"
        ]
    ]
}