{
    "d306144282595be6818fa386a3fbb4aece040884": [
        [
            "RepairSession::RepairSession(UUID,UUID,Range,String,RepairParallelism,Set,long,String)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    /**\n     * Create new repair session.\n     *\n     * @param parentRepairSession the parent sessions id\n     * @param id this sessions id\n     * @param range range to repair\n     * @param keyspace name of keyspace\n     * @param parallelismDegree specifies the degree of parallelism when calculating the merkle trees\n     * @param endpoints the data centers that should be part of the repair; null for all DCs\n     * @param repairedAt when the repair occurred (millis)\n     * @param cfnames names of columnfamilies\n     */\n    public RepairSession(UUID parentRepairSession,\n                         UUID id,\n                         Range<Token> range,\n                         String keyspace,\n                         RepairParallelism parallelismDegree,\n                         Set<InetAddress> endpoints,\n                         long repairedAt,\n                         String... cfnames)\n    {\n        assert cfnames.length > 0 : \"Repairing no column families seems pointless, doesn't it\";\n\n        this.parentRepairSession = parentRepairSession;\n        this.id = id;\n        this.parallelismDegree = parallelismDegree;\n        this.keyspace = keyspace;\n        this.cfnames = cfnames;\n        this.range = range;\n        this.endpoints = endpoints;\n        this.repairedAt = repairedAt;\n    }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  ",
            "    /**\n     * Create new repair session.\n     *\n     * @param parentRepairSession the parent sessions id\n     * @param id this sessions id\n     * @param range range to repair\n     * @param keyspace name of keyspace\n     * @param parallelismDegree specifies the degree of parallelism when calculating the merkle trees\n     * @param endpoints the data centers that should be part of the repair; null for all DCs\n     * @param repairedAt when the repair occurred (millis)\n     * @param cfnames names of columnfamilies\n     */\n    public RepairSession(UUID parentRepairSession,\n                         UUID id,\n                         Range<Token> range,\n                         String keyspace,\n                         RepairParallelism parallelismDegree,\n                         Set<InetAddress> endpoints,\n                         long repairedAt,\n                         String... cfnames)\n    {\n        assert cfnames.length > 0 : \"Repairing no column families seems pointless, doesn't it\";\n\n        this.parentRepairSession = parentRepairSession;\n        this.id = id;\n        this.parallelismDegree = parallelismDegree;\n        this.keyspace = keyspace;\n        this.cfnames = cfnames;\n        this.range = range;\n        this.endpoints = endpoints;\n        this.repairedAt = repairedAt;\n        this.validationRemaining = new AtomicInteger(cfnames.length);\n    }"
        ],
        [
            "RepairJob::run()",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119 -\n 120 -\n 121 -\n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints) throws Exception\n                {\n                    logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, endpoints));\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, allEndpoints));\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees) throws Exception\n            {\n                // Unregister from FailureDetector once we've completed synchronizing Merkle trees.\n                // After this point, we rely on tcp_keepalive for individual sockets to notify us when a connection is down.\n                // See CASSANDRA-3569\n                FailureDetector.instance.unregisterFailureDetectionEventListener(session);\n\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints) throws Exception\n                {\n                    logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, endpoints));\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            logger.info(String.format(\"[repair #%s] requesting merkle trees for %s (to %s)\", desc.sessionId, desc.columnFamily, allEndpoints));\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees) throws Exception\n            {\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }"
        ],
        [
            "RepairSession::validationComplete(RepairJobDesc,InetAddress,MerkleTree)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "    /**\n     * Receive merkle tree response or failed response from {@code endpoint} for current repair job.\n     *\n     * @param desc repair job description\n     * @param endpoint endpoint that sent merkle tree\n     * @param tree calculated merkle tree, or null if validation failed\n     */\n    public void validationComplete(RepairJobDesc desc, InetAddress endpoint, MerkleTree tree)\n    {\n        ValidationTask task = validating.remove(Pair.create(desc, endpoint));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        String message = String.format(\"Received merkle tree for %s from %s\", desc.columnFamily, endpoint);\n        logger.info(\"[repair #{}] {}\", getId(), message);\n        Tracing.traceRepair(message);\n        task.treeReceived(tree);\n    }",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192  ",
            "    /**\n     * Receive merkle tree response or failed response from {@code endpoint} for current repair job.\n     *\n     * @param desc repair job description\n     * @param endpoint endpoint that sent merkle tree\n     * @param tree calculated merkle tree, or null if validation failed\n     */\n    public void validationComplete(RepairJobDesc desc, InetAddress endpoint, MerkleTree tree)\n    {\n        ValidationTask task = validating.remove(Pair.create(desc, endpoint));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        String message = String.format(\"Received merkle tree for %s from %s\", desc.columnFamily, endpoint);\n        logger.info(\"[repair #{}] {}\", getId(), message);\n        Tracing.traceRepair(message);\n        task.treeReceived(tree);\n\n        // Unregister from FailureDetector once we've completed synchronizing Merkle trees.\n        // After this point, we rely on tcp_keepalive for individual sockets to notify us when a connection is down.\n        // See CASSANDRA-3569\n        if (validationRemaining.decrementAndGet() == 0)\n        {\n            FailureDetector.instance.unregisterFailureDetectionEventListener(this);\n        }\n    }"
        ]
    ],
    "9ddce25ff79efae6edd42c1f2f4c78deba19b4e7": [
        [
            "SSTableIterator::ForwardIndexedReader::computeNext()",
            " 251  \n 252  \n 253  \n 254  \n 255 -\n 256 -\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "        @Override\n        protected Unfiltered computeNext() throws IOException\n        {\n            // Our previous read might have made us cross an index block boundary. If so, update our informations.\n            if (indexState.isPastCurrentBlock())\n                indexState.setToBlock(indexState.currentBlockIdx() + 1);\n\n            // Return the next unfiltered unless we've reached the end, or we're beyond our slice\n            // end (note that unless we're on the last block for the slice, there is no point\n            // in checking the slice end).\n            if (indexState.isDone()\n                || indexState.currentBlockIdx() > lastBlockIdx\n                || !deserializer.hasNext()\n                || (indexState.currentBlockIdx() == lastBlockIdx && deserializer.compareNextTo(end) > 0))\n                return null;\n\n\n            Unfiltered next = deserializer.readNext();\n            if (next.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)\n                updateOpenMarker((RangeTombstoneMarker)next);\n            return next;\n        }",
            " 251  \n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "        @Override\n        protected Unfiltered computeNext() throws IOException\n        {\n            // Our previous read might have made us cross an index block boundary. If so, update our informations.\n            int currentBlockIdx = indexState.currentBlockIdx();\n            if (indexState.isPastCurrentBlock() && currentBlockIdx + 1 < indexState.blocksCount())\n                indexState.setToBlock(currentBlockIdx + 1);\n\n            // Return the next unfiltered unless we've reached the end, or we're beyond our slice\n            // end (note that unless we're on the last block for the slice, there is no point\n            // in checking the slice end).\n            if (indexState.isDone()\n                || indexState.currentBlockIdx() > lastBlockIdx\n                || !deserializer.hasNext()\n                || (indexState.currentBlockIdx() == lastBlockIdx && deserializer.compareNextTo(end) > 0))\n                return null;\n\n\n            Unfiltered next = deserializer.readNext();\n            if (next.kind() == Unfiltered.Kind.RANGE_TOMBSTONE_MARKER)\n                updateOpenMarker((RangeTombstoneMarker)next);\n            return next;\n        }"
        ],
        [
            "AbstractSSTableIterator::IndexState::isPastCurrentBlock()",
            " 457  \n 458  \n 459 -\n 460  ",
            "        public boolean isPastCurrentBlock()\n        {\n            return currentIndexIdx < indexes.size() && reader.file.bytesPastMark(mark) >= currentIndex().width;\n        }",
            " 457  \n 458  \n 459 +\n 460  ",
            "        public boolean isPastCurrentBlock()\n        {\n            return reader.file.bytesPastMark(mark) >= currentIndex().width;\n        }"
        ]
    ],
    "eca7cbb2e20858625c39cf0f9a5a76c6cc905dc0": [
        [
            "AntiCompactionTest::antiCompactionSizeTest()",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  ",
            "    @Test\n    public void antiCompactionSizeTest() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.disableAutoCompaction();\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long origSize = s.bytesOnDisk();\n        Range<Token> range = new Range<Token>(new BytesToken(ByteBufferUtil.bytes(0)), new BytesToken(ByteBufferUtil.bytes(500)));\n        Collection<SSTableReader> sstables = cfs.getLiveSSTables();\n        try (LifecycleTransaction txn = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(cfs, Arrays.asList(range), refs, txn, 12345);\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(origSize, cfs.metric.liveDiskSpaceUsed.getCount(), 100000);\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153  \n 154 +\n 155  \n 156 +\n 157 +\n 158  \n 159 +\n 160 +\n 161  ",
            "    @Test\n    public void antiCompactionSizeTest() throws InterruptedException, IOException\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.disableAutoCompaction();\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long origSize = s.bytesOnDisk();\n        Range<Token> range = new Range<Token>(new BytesToken(ByteBufferUtil.bytes(0)), new BytesToken(ByteBufferUtil.bytes(500)));\n        Collection<SSTableReader> sstables = cfs.getLiveSSTables();\n        try (LifecycleTransaction txn = cfs.getTracker().tryModify(sstables, OperationType.ANTICOMPACTION);\n             Refs<SSTableReader> refs = Refs.ref(sstables))\n        {\n            CompactionManager.instance.performAnticompaction(cfs, Arrays.asList(range), refs, txn, 12345);\n        }\n        long sum = 0;\n        long rows = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n        {\n            sum += x.bytesOnDisk();\n            rows += x.getTotalRows();\n        }\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(rows, 1000 * (1000 * 5));//See writeFile for how this number is derived\n        assertEquals(origSize, cfs.metric.liveDiskSpaceUsed.getCount(), 16000000);\n    }"
        ]
    ],
    "0a4ce4449f137a959b02fee14e1fdd427ccd520e": [
        [
            "RepairMessageVerbHandler::doVerb(MessageIn,int)",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    public void doVerb(final MessageIn<RepairMessage> message, final int id)\n    {\n        // TODO add cancel/interrupt message\n        RepairJobDesc desc = message.payload.desc;\n        try\n        {\n            switch (message.payload.messageType)\n            {\n                case PREPARE_GLOBAL_MESSAGE:\n                case PREPARE_MESSAGE:\n                    PrepareMessage prepareMessage = (PrepareMessage) message.payload;\n                    logger.debug(\"Preparing, {}\", prepareMessage);\n                    List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>(prepareMessage.cfIds.size());\n                    for (UUID cfId : prepareMessage.cfIds)\n                    {\n                        Pair<String, String> kscf = Schema.instance.getCF(cfId);\n                        ColumnFamilyStore columnFamilyStore = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right);\n                        columnFamilyStores.add(columnFamilyStore);\n                    }\n                    CassandraVersion peerVersion = SystemKeyspace.getReleaseVersion(message.from);\n                    // note that we default isGlobal to true since old version always default to true:\n                    boolean isGlobal = peerVersion == null ||\n                                       peerVersion.compareTo(ActiveRepairService.SUPPORTS_GLOBAL_PREPARE_FLAG_VERSION) < 0 ||\n                                       message.payload.messageType.equals(RepairMessage.Type.PREPARE_GLOBAL_MESSAGE);\n                    logger.debug(\"Received prepare message: global message = {}, peerVersion = {},\", message.payload.messageType.equals(RepairMessage.Type.PREPARE_GLOBAL_MESSAGE), peerVersion);\n                    ActiveRepairService.instance.registerParentRepairSession(prepareMessage.parentRepairSession,\n                            columnFamilyStores,\n                            prepareMessage.ranges,\n                            prepareMessage.isIncremental,\n                            isGlobal);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                case SNAPSHOT:\n                    logger.debug(\"Snapshotting {}\", desc);\n                    ColumnFamilyStore cfs = Keyspace.open(desc.keyspace).getColumnFamilyStore(desc.columnFamily);\n                    final Range<Token> repairingRange = desc.range;\n                    Set<SSTableReader> snapshottedSSSTables = cfs.snapshot(desc.sessionId.toString(), new Predicate<SSTableReader>()\n                    {\n                        public boolean apply(SSTableReader sstable)\n                        {\n                            return sstable != null &&\n                                    !(sstable.partitioner instanceof LocalPartitioner) && // exclude SSTables from 2i\n                                    new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(Collections.singleton(repairingRange));\n                        }\n                    }, true); //ephemeral snapshot, if repair fails, it will be cleaned next startup\n\n                    Set<SSTableReader> currentlyRepairing = ActiveRepairService.instance.currentlyRepairing(cfs.metadata.cfId, desc.parentSessionId);\n                    if (!Sets.intersection(currentlyRepairing, snapshottedSSSTables).isEmpty())\n                    {\n                        logger.error(\"Cannot start multiple repair sessions over the same sstables\");\n                        throw new RuntimeException(\"Cannot start multiple repair sessions over the same sstables\");\n                    }\n                    ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId).addSSTables(cfs.metadata.cfId, snapshottedSSSTables);\n                    logger.debug(\"Enqueuing response to snapshot request {} to {}\", desc.sessionId, message.from);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                case VALIDATION_REQUEST:\n                    ValidationRequest validationRequest = (ValidationRequest) message.payload;\n                    logger.debug(\"Validating {}\", validationRequest);\n                    // trigger read-only compaction\n                    ColumnFamilyStore store = Keyspace.open(desc.keyspace).getColumnFamilyStore(desc.columnFamily);\n\n                    Validator validator = new Validator(desc, message.from, validationRequest.gcBefore);\n                    CompactionManager.instance.submitValidation(store, validator);\n                    break;\n\n                case SYNC_REQUEST:\n                    // forwarded sync request\n                    SyncRequest request = (SyncRequest) message.payload;\n                    logger.debug(\"Syncing {}\", request);\n                    long repairedAt = ActiveRepairService.UNREPAIRED_SSTABLE;\n                    if (desc.parentSessionId != null && ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId) != null)\n                        repairedAt = ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId).getRepairedAt();\n\n                    StreamingRepairTask task = new StreamingRepairTask(desc, request, repairedAt);\n                    task.run();\n                    break;\n\n                case ANTICOMPACTION_REQUEST:\n                    AnticompactionRequest anticompactionRequest = (AnticompactionRequest) message.payload;\n                    logger.debug(\"Got anticompaction request {}\", anticompactionRequest);\n                    ListenableFuture<?> compactionDone = ActiveRepairService.instance.doAntiCompaction(anticompactionRequest.parentRepairSession, anticompactionRequest.successfulRanges);\n                    compactionDone.addListener(new Runnable()\n                    {\n                        @Override\n                        public void run()\n                        {\n                            MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                        }\n                    }, MoreExecutors.sameThreadExecutor());\n                    break;\n\n                case CLEANUP:\n                    logger.debug(\"cleaning up repair\");\n                    CleanupMessage cleanup = (CleanupMessage) message.payload;\n                    ActiveRepairService.instance.removeParentRepairSession(cleanup.parentRepairSession);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                default:\n                    ActiveRepairService.instance.handleMessage(message.from, message.payload);\n                    break;\n            }\n        }\n        catch (Exception e)\n        {\n            logger.error(\"Got error, removing parent repair session\");\n            if (desc != null && desc.parentSessionId != null)\n                ActiveRepairService.instance.removeParentRepairSession(desc.parentSessionId);\n            throw new RuntimeException(e);\n        }\n    }",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107 +\n 108  \n 109 +\n 110 +\n 111 +\n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "    public void doVerb(final MessageIn<RepairMessage> message, final int id)\n    {\n        // TODO add cancel/interrupt message\n        RepairJobDesc desc = message.payload.desc;\n        try\n        {\n            switch (message.payload.messageType)\n            {\n                case PREPARE_GLOBAL_MESSAGE:\n                case PREPARE_MESSAGE:\n                    PrepareMessage prepareMessage = (PrepareMessage) message.payload;\n                    logger.debug(\"Preparing, {}\", prepareMessage);\n                    List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>(prepareMessage.cfIds.size());\n                    for (UUID cfId : prepareMessage.cfIds)\n                    {\n                        Pair<String, String> kscf = Schema.instance.getCF(cfId);\n                        ColumnFamilyStore columnFamilyStore = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right);\n                        columnFamilyStores.add(columnFamilyStore);\n                    }\n                    CassandraVersion peerVersion = SystemKeyspace.getReleaseVersion(message.from);\n                    // note that we default isGlobal to true since old version always default to true:\n                    boolean isGlobal = peerVersion == null ||\n                                       peerVersion.compareTo(ActiveRepairService.SUPPORTS_GLOBAL_PREPARE_FLAG_VERSION) < 0 ||\n                                       message.payload.messageType.equals(RepairMessage.Type.PREPARE_GLOBAL_MESSAGE);\n                    logger.debug(\"Received prepare message: global message = {}, peerVersion = {},\", message.payload.messageType.equals(RepairMessage.Type.PREPARE_GLOBAL_MESSAGE), peerVersion);\n                    ActiveRepairService.instance.registerParentRepairSession(prepareMessage.parentRepairSession,\n                            columnFamilyStores,\n                            prepareMessage.ranges,\n                            prepareMessage.isIncremental,\n                            isGlobal);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                case SNAPSHOT:\n                    logger.debug(\"Snapshotting {}\", desc);\n                    final ColumnFamilyStore cfs = Keyspace.open(desc.keyspace).getColumnFamilyStore(desc.columnFamily);\n                    final Range<Token> repairingRange = desc.range;\n                    Set<SSTableReader> snapshottedSSSTables = cfs.snapshot(desc.sessionId.toString(), new Predicate<SSTableReader>()\n                    {\n                        public boolean apply(SSTableReader sstable)\n                        {\n                            return sstable != null &&\n                                    !(sstable.partitioner instanceof LocalPartitioner) && // exclude SSTables from 2i\n                                    new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(Collections.singleton(repairingRange));\n                        }\n                    }, true); //ephemeral snapshot, if repair fails, it will be cleaned next startup\n\n                    Set<SSTableReader> currentlyRepairing = ActiveRepairService.instance.currentlyRepairing(cfs.metadata.cfId, desc.parentSessionId);\n                    if (!Sets.intersection(currentlyRepairing, snapshottedSSSTables).isEmpty())\n                    {\n                        // clear snapshot that we just created\n                        cfs.clearSnapshot(desc.sessionId.toString());\n                        logger.error(\"Cannot start multiple repair sessions over the same sstables\");\n                        MessageOut reply = new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE)\n                                               .withParameter(MessagingService.FAILURE_RESPONSE_PARAM, MessagingService.ONE_BYTE);\n                        MessagingService.instance().sendReply(reply, id, message.from);\n                        return;\n                    }\n                    ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId).addSSTables(cfs.metadata.cfId, snapshottedSSSTables);\n                    logger.debug(\"Enqueuing response to snapshot request {} to {}\", desc.sessionId, message.from);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                case VALIDATION_REQUEST:\n                    ValidationRequest validationRequest = (ValidationRequest) message.payload;\n                    logger.debug(\"Validating {}\", validationRequest);\n                    // trigger read-only compaction\n                    ColumnFamilyStore store = Keyspace.open(desc.keyspace).getColumnFamilyStore(desc.columnFamily);\n\n                    Validator validator = new Validator(desc, message.from, validationRequest.gcBefore);\n                    CompactionManager.instance.submitValidation(store, validator);\n                    break;\n\n                case SYNC_REQUEST:\n                    // forwarded sync request\n                    SyncRequest request = (SyncRequest) message.payload;\n                    logger.debug(\"Syncing {}\", request);\n                    long repairedAt = ActiveRepairService.UNREPAIRED_SSTABLE;\n                    if (desc.parentSessionId != null && ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId) != null)\n                        repairedAt = ActiveRepairService.instance.getParentRepairSession(desc.parentSessionId).getRepairedAt();\n\n                    StreamingRepairTask task = new StreamingRepairTask(desc, request, repairedAt);\n                    task.run();\n                    break;\n\n                case ANTICOMPACTION_REQUEST:\n                    AnticompactionRequest anticompactionRequest = (AnticompactionRequest) message.payload;\n                    logger.debug(\"Got anticompaction request {}\", anticompactionRequest);\n                    ListenableFuture<?> compactionDone = ActiveRepairService.instance.doAntiCompaction(anticompactionRequest.parentRepairSession, anticompactionRequest.successfulRanges);\n                    compactionDone.addListener(new Runnable()\n                    {\n                        @Override\n                        public void run()\n                        {\n                            MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                        }\n                    }, MoreExecutors.sameThreadExecutor());\n                    break;\n\n                case CLEANUP:\n                    logger.debug(\"cleaning up repair\");\n                    CleanupMessage cleanup = (CleanupMessage) message.payload;\n                    ActiveRepairService.instance.removeParentRepairSession(cleanup.parentRepairSession);\n                    MessagingService.instance().sendReply(new MessageOut(MessagingService.Verb.INTERNAL_RESPONSE), id, message.from);\n                    break;\n\n                default:\n                    ActiveRepairService.instance.handleMessage(message.from, message.payload);\n                    break;\n            }\n        }\n        catch (Exception e)\n        {\n            logger.error(\"Got error, removing parent repair session\");\n            if (desc != null && desc.parentSessionId != null)\n                ActiveRepairService.instance.removeParentRepairSession(desc.parentSessionId);\n            throw new RuntimeException(e);\n        }\n    }"
        ]
    ],
    "e1fb18a00b598431f52b88d12e2eddbe07233e88": [
        [
            "ColumnFamilyStore::ColumnFamilyStore(Keyspace,String,int,CFMetaData,Directories,boolean,boolean)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366 -\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391 -\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        this.directories = directories;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.params.crcCheckChance);\n        indexManager = new SecondaryIndexManager(this);\n        viewManager = keyspace.viewManager.forTable(metadata.cfId);\n        metric = new TableMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n        this.directories = this.compactionStrategyManager.getDirectories();\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (IndexMetadata info : metadata.getIndexes())\n            indexManager.addIndex(info);\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            mbeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,table=%s\",\n                                         isIndex() ? \"IndexTables\" : \"Tables\",\n                                         keyspace.getName(), name);\n            oldMBeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,columnfamily=%s\",\n                                         isIndex() ? \"IndexColumnFamilies\" : \"ColumnFamilies\",\n                                         keyspace.getName(), name);\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName[] objectNames = {new ObjectName(mbeanName), new ObjectName(oldMBeanName)};\n                for (ObjectName objectName : objectNames)\n                {\n                    mbs.registerMBean(this, objectName);\n                }\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.trace(\"retryPolicy for {} is {}\", name, this.metadata.params.speculativeRetry);\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetryParam retryPolicy = ColumnFamilyStore.this.metadata.params.speculativeRetry;\n                    switch (retryPolicy.kind())\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.threshold()) * 1000d);\n                            break;\n                        case CUSTOM:\n                            sampleLatencyNanos = (long) retryPolicy.threshold();\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n    }",
            " 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.params.crcCheckChance);\n        indexManager = new SecondaryIndexManager(this);\n        viewManager = keyspace.viewManager.forTable(metadata.cfId);\n        metric = new TableMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n        this.directories = compactionStrategyManager.getDirectories();\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (IndexMetadata info : metadata.getIndexes())\n            indexManager.addIndex(info);\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            mbeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,table=%s\",\n                                         isIndex() ? \"IndexTables\" : \"Tables\",\n                                         keyspace.getName(), name);\n            oldMBeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,columnfamily=%s\",\n                                         isIndex() ? \"IndexColumnFamilies\" : \"ColumnFamilies\",\n                                         keyspace.getName(), name);\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName[] objectNames = {new ObjectName(mbeanName), new ObjectName(oldMBeanName)};\n                for (ObjectName objectName : objectNames)\n                {\n                    mbs.registerMBean(this, objectName);\n                }\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.trace(\"retryPolicy for {} is {}\", name, this.metadata.params.speculativeRetry);\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetryParam retryPolicy = ColumnFamilyStore.this.metadata.params.speculativeRetry;\n                    switch (retryPolicy.kind())\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.threshold()) * 1000d);\n                            break;\n                        case CUSTOM:\n                            sampleLatencyNanos = (long) retryPolicy.threshold();\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n    }"
        ],
        [
            "AbstractCompactionStrategy::getDirectories()",
            " 122  \n 123  \n 124 -\n 125  ",
            "    public Directories getDirectories()\n    {\n        return cfs.getDirectories();\n    }",
            " 126  \n 127  \n 128 +\n 129  ",
            "    public Directories getDirectories()\n    {\n        return directories;\n    }"
        ],
        [
            "AbstractCompactionStrategy::AbstractCompactionStrategy(ColumnFamilyStore,Map)",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        assert cfs != null;\n        this.cfs = cfs;\n        this.options = ImmutableMap.copyOf(options);\n\n        /* checks must be repeated here, as user supplied strategies might not call validateOptions directly */\n\n        try\n        {\n            validateOptions(options);\n            String optionValue = options.get(TOMBSTONE_THRESHOLD_OPTION);\n            tombstoneThreshold = optionValue == null ? DEFAULT_TOMBSTONE_THRESHOLD : Float.parseFloat(optionValue);\n            optionValue = options.get(TOMBSTONE_COMPACTION_INTERVAL_OPTION);\n            tombstoneCompactionInterval = optionValue == null ? DEFAULT_TOMBSTONE_COMPACTION_INTERVAL : Long.parseLong(optionValue);\n            optionValue = options.get(UNCHECKED_TOMBSTONE_COMPACTION_OPTION);\n            uncheckedTombstoneCompaction = optionValue == null ? DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION : Boolean.parseBoolean(optionValue);\n            if (!shouldBeEnabled())\n                this.disable();\n        }\n        catch (ConfigurationException e)\n        {\n            logger.warn(\"Error setting compaction strategy options ({}), defaults will be used\", e.getMessage());\n            tombstoneThreshold = DEFAULT_TOMBSTONE_THRESHOLD;\n            tombstoneCompactionInterval = DEFAULT_TOMBSTONE_COMPACTION_INTERVAL;\n            uncheckedTombstoneCompaction = DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION;\n        }\n    }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  ",
            "    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        assert cfs != null;\n        this.cfs = cfs;\n        this.options = ImmutableMap.copyOf(options);\n\n        /* checks must be repeated here, as user supplied strategies might not call validateOptions directly */\n\n        try\n        {\n            validateOptions(options);\n            String optionValue = options.get(TOMBSTONE_THRESHOLD_OPTION);\n            tombstoneThreshold = optionValue == null ? DEFAULT_TOMBSTONE_THRESHOLD : Float.parseFloat(optionValue);\n            optionValue = options.get(TOMBSTONE_COMPACTION_INTERVAL_OPTION);\n            tombstoneCompactionInterval = optionValue == null ? DEFAULT_TOMBSTONE_COMPACTION_INTERVAL : Long.parseLong(optionValue);\n            optionValue = options.get(UNCHECKED_TOMBSTONE_COMPACTION_OPTION);\n            uncheckedTombstoneCompaction = optionValue == null ? DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION : Boolean.parseBoolean(optionValue);\n            if (!shouldBeEnabled())\n                this.disable();\n        }\n        catch (ConfigurationException e)\n        {\n            logger.warn(\"Error setting compaction strategy options ({}), defaults will be used\", e.getMessage());\n            tombstoneThreshold = DEFAULT_TOMBSTONE_THRESHOLD;\n            tombstoneCompactionInterval = DEFAULT_TOMBSTONE_COMPACTION_INTERVAL;\n            uncheckedTombstoneCompaction = DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION;\n        }\n\n        directories = new Directories(cfs.metadata, Directories.dataDirectories);\n    }"
        ]
    ],
    "a55fd76ddd96e3ed2d967910f0572804fcfacc2f": [
        [
            "OutboundTcpConnectionPool::newSocket(InetAddress)",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135 -\n 136 -\n 137 -\n 138  \n 139  ",
            "    public static Socket newSocket(InetAddress endpoint) throws IOException\n    {\n        // zero means 'bind on any available port.'\n        if (isEncryptedChannel(endpoint))\n        {\n            if (Config.getOutboundBindAny())\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort());\n            else\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort(), FBUtilities.getLocalAddress(), 0);\n        }\n        else\n        {\n            Socket socket = SocketChannel.open(new InetSocketAddress(endpoint, DatabaseDescriptor.getStoragePort())).socket();\n            if (Config.getOutboundBindAny() && !socket.isBound())\n                socket.bind(new InetSocketAddress(FBUtilities.getLocalAddress(), 0));\n            return socket;\n        }\n    }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139  \n 140  ",
            "    public static Socket newSocket(InetAddress endpoint) throws IOException\n    {\n        // zero means 'bind on any available port.'\n        if (isEncryptedChannel(endpoint))\n        {\n            if (Config.getOutboundBindAny())\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort());\n            else\n                return SSLFactory.getSocket(DatabaseDescriptor.getServerEncryptionOptions(), endpoint, DatabaseDescriptor.getSSLStoragePort(), FBUtilities.getLocalAddress(), 0);\n        }\n        else\n        {\n            SocketChannel channel = SocketChannel.open();\n            if (!Config.getOutboundBindAny())\n                channel.bind(new InetSocketAddress(FBUtilities.getLocalAddress(), 0));\n            channel.connect(new InetSocketAddress(endpoint, DatabaseDescriptor.getStoragePort()));\n            return channel.socket();\n        }\n    }"
        ]
    ],
    "3c6dfa4aa0b9ffb0a48a02b949bff2a8406764e6": [
        [
            "ReadCommand::CheckForAbort::applyToPartition(BaseRowIterator)",
            " 480  \n 481  \n 482 -\n 483  \n 484  ",
            "        protected BaseRowIterator<?> applyToPartition(BaseRowIterator partition)\n        {\n            maybeAbort();\n            return partition;\n        }",
            " 480  \n 481  \n 482 +\n 483 +\n 484 +\n 485 +\n 486 +\n 487 +\n 488  \n 489  ",
            "        protected BaseRowIterator<?> applyToPartition(BaseRowIterator partition)\n        {\n            if (maybeAbort())\n            {\n                partition.close();\n                return null;\n            }\n\n            return partition;\n        }"
        ],
        [
            "ReadCommand::CheckForAbort::applyToRow(Row)",
            " 486  \n 487  \n 488 -\n 489 -\n 490  ",
            "        protected Row applyToRow(Row row)\n        {\n            maybeAbort();\n            return row;\n        }",
            " 491  \n 492  \n 493 +\n 494  ",
            "        protected Row applyToRow(Row row)\n        {\n            return maybeAbort() ? null : row;\n        }"
        ],
        [
            "ReadCommand::CheckForAbort::maybeAbort()",
            " 492 -\n 493  \n 494  \n 495  \n 496  \n 497 -\n 498 -\n 499  ",
            "        private void maybeAbort()\n        {\n            if (isAborted())\n                stop();\n\n            if (TEST_ITERATION_DELAY_MILLIS > 0)\n                maybeDelayForTesting();\n        }",
            " 496 +\n 497  \n 498 +\n 499 +\n 500 +\n 501  \n 502 +\n 503  \n 504 +\n 505 +\n 506  \n 507 +\n 508  ",
            "        private boolean maybeAbort()\n        {\n            if (TEST_ITERATION_DELAY_MILLIS > 0)\n                maybeDelayForTesting();\n\n            if (isAborted())\n            {\n                stop();\n                return true;\n            }\n\n            return false;\n        }"
        ]
    ],
    "101cd72e55168f6a97bbb2556aed9f23cef80f42": [
        [
            "CompactionManager::parallelAllSSTableOperation(ColumnFamilyStore,OneSSTableOperation,OperationType)",
            " 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "    @SuppressWarnings(\"resource\")\n    private AllSSTableOpStatus parallelAllSSTableOperation(final ColumnFamilyStore cfs, final OneSSTableOperation operation, OperationType operationType) throws ExecutionException, InterruptedException\n    {\n        List<LifecycleTransaction> transactions = new ArrayList<>();\n        try (LifecycleTransaction compacting = cfs.markAllCompacting(operationType);)\n        {\n            Iterable<SSTableReader> sstables = Lists.newArrayList(operation.filterSSTables(compacting));\n            if (Iterables.isEmpty(sstables))\n            {\n                logger.info(\"No sstables for {}.{}\", cfs.keyspace.getName(), cfs.name);\n                return AllSSTableOpStatus.SUCCESSFUL;\n            }\n\n            List<Future<Object>> futures = new ArrayList<>();\n\n            for (final SSTableReader sstable : sstables)\n            {\n                if (executor.isShutdown())\n                {\n                    logger.info(\"Executor has shut down, not submitting task\");\n                    return AllSSTableOpStatus.ABORTED;\n                }\n\n                final LifecycleTransaction txn = compacting.split(singleton(sstable));\n                transactions.add(txn);\n                futures.add(executor.submit(new Callable<Object>()\n                {\n                    @Override\n                    public Object call() throws Exception\n                    {\n                        operation.execute(txn);\n                        return this;\n                    }\n                }));\n            }\n\n            assert compacting.originals().isEmpty();\n\n            FBUtilities.waitOnFutures(futures);\n            return AllSSTableOpStatus.SUCCESSFUL;\n        }\n        finally\n        {\n            Throwable fail = Throwables.close(null, transactions);\n            if (fail != null)\n                logger.error(\"Failed to cleanup lifecycle transactions {}\", fail);\n        }\n    }",
            " 257  \n 258  \n 259  \n 260  \n 261 +\n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "    @SuppressWarnings(\"resource\")\n    private AllSSTableOpStatus parallelAllSSTableOperation(final ColumnFamilyStore cfs, final OneSSTableOperation operation, OperationType operationType) throws ExecutionException, InterruptedException\n    {\n        List<LifecycleTransaction> transactions = new ArrayList<>();\n        try (LifecycleTransaction compacting = cfs.markAllCompacting(operationType))\n        {\n            Iterable<SSTableReader> sstables = compacting != null ? Lists.newArrayList(operation.filterSSTables(compacting)) : Collections.<SSTableReader>emptyList();\n            if (Iterables.isEmpty(sstables))\n            {\n                logger.info(\"No sstables for {}.{}\", cfs.keyspace.getName(), cfs.name);\n                return AllSSTableOpStatus.SUCCESSFUL;\n            }\n\n            List<Future<Object>> futures = new ArrayList<>();\n\n            for (final SSTableReader sstable : sstables)\n            {\n                if (executor.isShutdown())\n                {\n                    logger.info(\"Executor has shut down, not submitting task\");\n                    return AllSSTableOpStatus.ABORTED;\n                }\n\n                final LifecycleTransaction txn = compacting.split(singleton(sstable));\n                transactions.add(txn);\n                futures.add(executor.submit(new Callable<Object>()\n                {\n                    @Override\n                    public Object call() throws Exception\n                    {\n                        operation.execute(txn);\n                        return this;\n                    }\n                }));\n            }\n\n            assert compacting.originals().isEmpty();\n\n            FBUtilities.waitOnFutures(futures);\n            return AllSSTableOpStatus.SUCCESSFUL;\n        }\n        finally\n        {\n            Throwable fail = Throwables.close(null, transactions);\n            if (fail != null)\n                logger.error(\"Failed to cleanup lifecycle transactions {}\", fail);\n        }\n    }"
        ]
    ],
    "98cc2c8d6cc27f1a2e675030a13b46fd336812f8": [
        [
            "ActiveRepairService::ParentRepairSession::ParentRepairSession(List,Collection,long)",
            " 434  \n 435  \n 436  \n 437  \n 438  \n 439 -\n 440  \n 441  ",
            "        public ParentRepairSession(List<ColumnFamilyStore> columnFamilyStores, Collection<Range<Token>> ranges, long repairedAt)\n        {\n            for (ColumnFamilyStore cfs : columnFamilyStores)\n                this.columnFamilyStores.put(cfs.metadata.cfId, cfs);\n            this.ranges = ranges;\n            this.sstableMap = new HashMap<>();\n            this.repairedAt = repairedAt;\n        }",
            " 434  \n 435  \n 436  \n 437 +\n 438  \n 439 +\n 440 +\n 441  \n 442  \n 443  ",
            "        public ParentRepairSession(List<ColumnFamilyStore> columnFamilyStores, Collection<Range<Token>> ranges, long repairedAt)\n        {\n            for (ColumnFamilyStore cfs : columnFamilyStores)\n            {\n                this.columnFamilyStores.put(cfs.metadata.cfId, cfs);\n                sstableMap.put(cfs.metadata.cfId, new HashSet<SSTableReader>());\n            }\n            this.ranges = ranges;\n            this.repairedAt = repairedAt;\n        }"
        ],
        [
            "ActiveRepairService::ParentRepairSession::addSSTables(UUID,Collection)",
            " 467  \n 468  \n 469 -\n 470 -\n 471 -\n 472 -\n 473 -\n 474  ",
            "        public void addSSTables(UUID cfId, Collection<SSTableReader> sstables)\n        {\n            Set<SSTableReader> existingSSTables = this.sstableMap.get(cfId);\n            if (existingSSTables == null)\n                existingSSTables = new HashSet<>();\n            existingSSTables.addAll(sstables);\n            this.sstableMap.put(cfId, existingSSTables);\n        }",
            " 469  \n 470  \n 471 +\n 472  ",
            "        public void addSSTables(UUID cfId, Collection<SSTableReader> sstables)\n        {\n            sstableMap.get(cfId).addAll(sstables);\n        }"
        ]
    ],
    "cb21f28ecadceaac78aba1c2de4670afc2c38daa": [
        [
            "Schema::addTable(CFMetaData)",
            " 605  \n 606  \n 607  \n 608  \n 609 -\n 610 -\n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  ",
            "    public void addTable(CFMetaData cfm)\n    {\n        assert getCFMetaData(cfm.ksName, cfm.cfName) == null;\n\n        // Make sure the keyspace is initialized and initialize the table.\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        // Update the keyspaces map with the updated metadata\n        update(cfm.ksName, ks -> ks.withSwapped(ks.tables.with(cfm)));\n        // Update the table ID <-> table name map (cfIdMap)\n        load(cfm);\n\n        // init the new CF before switching the KSM to the new one\n        // to avoid races as in CASSANDRA-10761\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        MigrationManager.instance.notifyCreateColumnFamily(cfm);\n    }",
            " 605  \n 606  \n 607  \n 608  \n 609 +\n 610 +\n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  ",
            "    public void addTable(CFMetaData cfm)\n    {\n        assert getCFMetaData(cfm.ksName, cfm.cfName) == null;\n\n        // Make sure the keyspace is initialized\n        Keyspace.open(cfm.ksName);\n        // Update the keyspaces map with the updated metadata\n        update(cfm.ksName, ks -> ks.withSwapped(ks.tables.with(cfm)));\n        // Update the table ID <-> table name map (cfIdMap)\n        load(cfm);\n\n        // init the new CF before switching the KSM to the new one\n        // to avoid races as in CASSANDRA-10761\n        Keyspace.open(cfm.ksName).initCf(cfm, true);\n        MigrationManager.instance.notifyCreateColumnFamily(cfm);\n    }"
        ]
    ],
    "60b23b1252e600189d11c614d971a4c2c3055edc": [
        [
            "AbstractQueryPager::Pager::onClose()",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "        @Override\n        public void onClose()\n        {\n            recordLast(lastKey, lastRow);\n\n            int counted = counter.counted();\n            remaining -= counted;\n            // If the clustering of the last row returned is a static one, it means that the partition was only\n            // containing data within the static columns. Therefore, there are not data remaining within the partition.\n            if (lastRow != null && lastRow.clustering() == Clustering.STATIC_CLUSTERING)\n            {\n                remainingInPartition = 0;\n            }\n            else\n            {\n                remainingInPartition -= counter.countedInCurrentPartition();\n            }\n            exhausted = counted < pageLimits.count();\n        }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "        @Override\n        public void onClose()\n        {\n            recordLast(lastKey, lastRow);\n\n            int counted = counter.counted();\n            remaining -= counted;\n            // If the clustering of the last row returned is a static one, it means that the partition was only\n            // containing data within the static columns. If the clustering of the last row returned is empty\n            // it means that there is only one row per partition. Therefore, in both cases there are no data remaining\n            // within the partition.\n            if (lastRow != null && (lastRow.clustering() == Clustering.STATIC_CLUSTERING\n                    || lastRow.clustering() == Clustering.EMPTY))\n            {\n                remainingInPartition = 0;\n            }\n            else\n            {\n                remainingInPartition -= counter.countedInCurrentPartition();\n            }\n            exhausted = counted < pageLimits.count();\n        }"
        ],
        [
            "PagingState::toString()",
            " 161  \n 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public String toString()\n    {\n        return String.format(\"PagingState(key=%s, cellname=%s, remaining=%d, remainingInPartition=%d\",\n                             ByteBufferUtil.bytesToHex(partitionKey),\n                             rowMark,\n                             remaining,\n                             remainingInPartition);\n    }",
            " 161  \n 162  \n 163  \n 164  \n 165 +\n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public String toString()\n    {\n        return String.format(\"PagingState(key=%s, cellname=%s, remaining=%d, remainingInPartition=%d\",\n                             partitionKey != null ? ByteBufferUtil.bytesToHex(partitionKey) : null,\n                             rowMark,\n                             remaining,\n                             remainingInPartition);\n    }"
        ]
    ],
    "eb5a59a311a14cc0a3c37a13d10abc8c5a0f6d1b": [
        [
            "DataOutputBuffer::recycle()",
            "  79  \n  80  \n  81  \n  82 -\n  83  \n  84  \n  85  ",
            "    public void recycle()\n    {\n        assert handle != null;\n        buffer.rewind();\n\n        RECYCLER.recycle(this, handle);\n    }",
            "  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96  \n  97  ",
            "    public void recycle()\n    {\n        assert handle != null;\n\n        // Avoid throwing away instances that are too large, trim large buffers to default size instead.\n        // See CASSANDRA-11838 for details.\n        if (buffer().capacity() > MAX_RECYCLE_BUFFER_SIZE)\n            buffer = ByteBuffer.allocate(DEFAULT_INITIAL_BUFFER_SIZE);\n\n        buffer.rewind();\n        RECYCLER.recycle(this, handle);\n    }"
        ],
        [
            "DataOutputBuffer::DataOutputBuffer(Recycler)",
            "  53  \n  54  \n  55 -\n  56  ",
            "    private DataOutputBuffer(Recycler.Handle handle)\n    {\n        this(128, handle);\n    }",
            "  60  \n  61  \n  62 +\n  63  ",
            "    private DataOutputBuffer(Recycler.Handle handle)\n    {\n        this(DEFAULT_INITIAL_BUFFER_SIZE, handle);\n    }"
        ],
        [
            "DataOutputBuffer::DataOutputBuffer()",
            "  58  \n  59  \n  60 -\n  61  ",
            "    public DataOutputBuffer()\n    {\n        this(128);\n    }",
            "  65  \n  66  \n  67 +\n  68  ",
            "    public DataOutputBuffer()\n    {\n        this(DEFAULT_INITIAL_BUFFER_SIZE);\n    }"
        ],
        [
            "BTree::Builder::reuse(Comparator)",
            " 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  ",
            "        private void reuse(Comparator<? super V> comparator)\n        {\n            this.comparator = comparator;\n            quickResolver = null;\n            count = 0;\n            detected = true;\n            auto = true;\n        }",
            " 837  \n 838  \n 839  \n 840  \n 841 +\n 842  \n 843  \n 844  \n 845  ",
            "        private void reuse(Comparator<? super V> comparator)\n        {\n            this.comparator = comparator;\n            quickResolver = null;\n            Arrays.fill(values, 0, count, null);\n            count = 0;\n            detected = true;\n            auto = true;\n        }"
        ]
    ],
    "2217695166a61f576b36993b36a6bde8c8952fde": [
        [
            "ReadResponse::LegacyRemoteDataResponse::makeIterator(ReadCommand)",
            " 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270 -\n 271  \n 272  \n 273  \n 274  \n 275  \n 276 -\n 277  \n 278 -\n 279  \n 280  \n 281  ",
            "        public UnfilteredPartitionIterator makeIterator(final ReadCommand command)\n        {\n            // Due to a bug in the serialization of AbstractBounds, anything that isn't a Range is understood by pre-3.0 nodes\n            // as a Bound, which means IncludingExcludingBounds and ExcludingBounds responses may include keys they shouldn't.\n            // So filter partitions that shouldn't be included here.\n            boolean skipFirst = false;\n            boolean skipLast = false;\n            if (!partitions.isEmpty() && command instanceof PartitionRangeReadCommand)\n            {\n                AbstractBounds<PartitionPosition> keyRange = ((PartitionRangeReadCommand)command).dataRange().keyRange();\n                boolean isExcludingBounds = keyRange instanceof ExcludingBounds;\n                skipFirst = isExcludingBounds && !keyRange.contains(partitions.get(0).partitionKey());\n                skipLast = (isExcludingBounds || keyRange instanceof IncludingExcludingBounds) && !keyRange.contains(partitions.get(partitions.size() - 1).partitionKey());\n            }\n\n            final List<ImmutableBTreePartition> toReturn;\n            if (skipFirst || skipLast)\n            {\n                toReturn = partitions.size() == 1\n                         ? Collections.emptyList()\n                         : partitions.subList(skipFirst ? 1 : 0, skipLast ? partitions.size() - 1 : partitions.size());\n            }\n            else\n            {\n                toReturn = partitions;\n            }\n\n            return new AbstractUnfilteredPartitionIterator()\n            {\n                private int idx;\n\n                public boolean isForThrift()\n                {\n                    return true;\n                }\n\n                public CFMetaData metadata()\n                {\n                    return command.metadata();\n                }\n\n                public boolean hasNext()\n                {\n                    return idx < toReturn.size();\n                }\n\n                public UnfilteredRowIterator next()\n                {\n                    ImmutableBTreePartition partition = toReturn.get(idx++);\n\n\n                    ClusteringIndexFilter filter = command.clusteringIndexFilter(partition.partitionKey());\n\n                    // Pre-3.0, we didn't have a way to express exclusivity for non-composite comparators, so all slices were\n                    // inclusive on both ends. If we have exclusive slice ends, we need to filter the results here.\n                    if (!command.metadata().isCompound())\n                        return filter.filter(partition.sliceableUnfilteredIterator(command.columnFilter(), filter.isReversed()));\n\n                    return partition.unfilteredIterator(command.columnFilter(), Slices.ALL, filter.isReversed());\n                }\n            };\n        }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276 +\n 277 +\n 278  \n 279 +\n 280 +\n 281  \n 282  \n 283  ",
            "        public UnfilteredPartitionIterator makeIterator(final ReadCommand command)\n        {\n            // Due to a bug in the serialization of AbstractBounds, anything that isn't a Range is understood by pre-3.0 nodes\n            // as a Bound, which means IncludingExcludingBounds and ExcludingBounds responses may include keys they shouldn't.\n            // So filter partitions that shouldn't be included here.\n            boolean skipFirst = false;\n            boolean skipLast = false;\n            if (!partitions.isEmpty() && command instanceof PartitionRangeReadCommand)\n            {\n                AbstractBounds<PartitionPosition> keyRange = ((PartitionRangeReadCommand)command).dataRange().keyRange();\n                boolean isExcludingBounds = keyRange instanceof ExcludingBounds;\n                skipFirst = isExcludingBounds && !keyRange.contains(partitions.get(0).partitionKey());\n                skipLast = (isExcludingBounds || keyRange instanceof IncludingExcludingBounds) && !keyRange.contains(partitions.get(partitions.size() - 1).partitionKey());\n            }\n\n            final List<ImmutableBTreePartition> toReturn;\n            if (skipFirst || skipLast)\n            {\n                toReturn = partitions.size() == 1\n                         ? Collections.emptyList()\n                         : partitions.subList(skipFirst ? 1 : 0, skipLast ? partitions.size() - 1 : partitions.size());\n            }\n            else\n            {\n                toReturn = partitions;\n            }\n\n            return new AbstractUnfilteredPartitionIterator()\n            {\n                private int idx;\n\n                public boolean isForThrift()\n                {\n                    return true;\n                }\n\n                public CFMetaData metadata()\n                {\n                    return command.metadata();\n                }\n\n                public boolean hasNext()\n                {\n                    return idx < toReturn.size();\n                }\n\n                public UnfilteredRowIterator next()\n                {\n                    ImmutableBTreePartition partition = toReturn.get(idx++);\n\n                    ClusteringIndexFilter filter = command.clusteringIndexFilter(partition.partitionKey());\n\n                    // Pre-3.0, we didn't have a way to express exclusivity for non-composite comparators, so all slices were\n                    // inclusive on both ends. If we have exclusive slice ends, we need to filter the results here.\n                    if (!command.metadata().isCompound())\n                        return ThriftResultsMerger.maybeWrap(\n                                filter.filter(partition.sliceableUnfilteredIterator(command.columnFilter(), filter.isReversed())), command.nowInSec());\n\n                    return ThriftResultsMerger.maybeWrap(\n                            partition.unfilteredIterator(command.columnFilter(), Slices.ALL, filter.isReversed()), command.nowInSec());\n                }\n            };\n        }"
        ]
    ],
    "eaa06942a6b5f54fb72bc7fe53d469cd034c2106": [
        [
            "LegacySSTableTest::copyFile(File,File)",
            " 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475 -\n 476 -\n 477 -\n 478 -\n 479  \n 480  ",
            "    private static void copyFile(File cfDir, File file) throws IOException\n    {\n        byte[] buf = new byte[65536];\n        if (file.isFile())\n        {\n            File target = new File(cfDir, file.getName());\n            int rd;\n            FileInputStream is = new FileInputStream(file);\n            FileOutputStream os = new FileOutputStream(target);\n            while ((rd = is.read(buf)) >= 0)\n                os.write(buf, 0, rd);\n        }\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482 +\n 483 +\n 484 +\n 485  \n 486  ",
            "    private static void copyFile(File cfDir, File file) throws IOException\n    {\n        byte[] buf = new byte[65536];\n        if (file.isFile())\n        {\n            File target = new File(cfDir, file.getName());\n            int rd;\n            try (FileInputStream is = new FileInputStream(file);\n                 FileOutputStream os = new FileOutputStream(target);) {\n                while ((rd = is.read(buf)) >= 0)\n                    os.write(buf, 0, rd);\n                }\n        }\n    }"
        ],
        [
            "LegacySSTableTest::copySstablesToTestData(String,String,File)",
            " 455  \n 456  \n 457 -\n 458  \n 459  \n 460  \n 461  ",
            "    private static void copySstablesToTestData(String legacyVersion, String table, File cfDir) throws IOException\n    {\n        for (File file : getTableDir(legacyVersion, table).listFiles())\n        {\n            copyFile(cfDir, file);\n        }\n    }",
            " 458  \n 459  \n 460 +\n 461 +\n 462 +\n 463  \n 464  \n 465  \n 466  ",
            "    private static void copySstablesToTestData(String legacyVersion, String table, File cfDir) throws IOException\n    {\n        File tableDir = getTableDir(legacyVersion, table);\n        Assert.assertTrue(\"The table directory \" + tableDir + \" was not found\", tableDir.isDirectory());\n        for (File file : tableDir.listFiles())\n        {\n            copyFile(cfDir, file);\n        }\n    }"
        ],
        [
            "LegacySSTableTest::defineSchema()",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102 -\n 103 -\n 104 -\n 105  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n        StorageService.instance.initServer();\n        Keyspace.setInitialized();\n        createKeyspace();\n        for (String legacyVersion : legacyVersions)\n        {\n            createTables(legacyVersion);\n        }\n        String scp = System.getProperty(LEGACY_SSTABLE_PROP);\n        assert scp != null;\n        LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();\n        assert LEGACY_SSTABLE_ROOT.isDirectory();\n    }",
            "  90  \n  91  \n  92  \n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        String scp = System.getProperty(LEGACY_SSTABLE_PROP);\n        Assert.assertNotNull(\"System property \" + LEGACY_SSTABLE_ROOT + \" not set\", scp);\n        \n        LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();\n        Assert.assertTrue(\"System property \" + LEGACY_SSTABLE_ROOT + \" does not specify a directory\", LEGACY_SSTABLE_ROOT.isDirectory());\n\n        SchemaLoader.prepareServer();\n        StorageService.instance.initServer();\n        Keyspace.setInitialized();\n        createKeyspace();\n        for (String legacyVersion : legacyVersions)\n        {\n            createTables(legacyVersion);\n        }\n\n    }"
        ]
    ],
    "da07130e4e08cd8645001c6f7fa9ea2acb2072ca": [
        [
            "LogReplicaSet::maybeCreateReplica(File,String,Set)",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "    void maybeCreateReplica(File folder, String fileName, Set<LogRecord> records)\n    {\n        if (replicasByFile.containsKey(folder))\n            return;\n\n        final LogReplica replica = LogReplica.create(folder, fileName);\n\n        records.forEach(replica::append);\n        replicasByFile.put(folder, replica);\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"Created new file replica {}\", replica);\n    }",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    void maybeCreateReplica(File folder, String fileName, Set<LogRecord> records)\n    {\n        if (replicasByFile.containsKey(folder))\n            return;\n\n        @SuppressWarnings(\"resource\")  // LogReplicas are closed in LogReplicaSet::close\n        final LogReplica replica = LogReplica.create(folder, fileName);\n\n        records.forEach(replica::append);\n        replicasByFile.put(folder, replica);\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"Created new file replica {}\", replica);\n    }"
        ],
        [
            "LogFile::close()",
            " 377 -\n 378  \n 379  \n 380  ",
            "    void close()\n    {\n        replicas.close();\n    }",
            " 377 +\n 378  \n 379  \n 380  ",
            "    public void close()\n    {\n        replicas.close();\n    }"
        ],
        [
            "LogReplicaSet::close()",
            " 204 -\n 205  \n 206  \n 207  ",
            "    void close()\n    {\n        Throwables.maybeFail(Throwables.perform(null, replicas().stream().map(r -> r::close)));\n    }",
            " 205 +\n 206  \n 207  \n 208  ",
            "    public void close()\n    {\n        Throwables.maybeFail(Throwables.perform(null, replicas().stream().map(r -> r::close)));\n    }"
        ],
        [
            "LogReplica::close()",
            "  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "    void close()\n    {\n        if (folderDescriptor >= 0)\n        {\n            CLibrary.tryCloseFD(folderDescriptor);\n            folderDescriptor = -1;\n        }\n    }",
            "  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "    public void close()\n    {\n        if (folderDescriptor >= 0)\n        {\n            CLibrary.tryCloseFD(folderDescriptor);\n            folderDescriptor = -1;\n        }\n    }"
        ],
        [
            "LogAwareFileLister::classifyFiles(File)",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126 -\n 127 -\n 128 -\n 129  ",
            "    /**\n     * We read txn log files, if we fail we throw only if the user has specified\n     * OnTxnErr.THROW, else we log an error and apply the txn log anyway\n     */\n    void classifyFiles(File txnFile)\n    {\n        LogFile txn = LogFile.make(txnFile);\n        readTxnLog(txn);\n        classifyFiles(txn);\n        files.put(txnFile, FileType.TXN_LOG);\n    }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131  ",
            "    /**\n     * We read txn log files, if we fail we throw only if the user has specified\n     * OnTxnErr.THROW, else we log an error and apply the txn log anyway\n     */\n    void classifyFiles(File txnFile)\n    {\n        try (LogFile txn = LogFile.make(txnFile))\n        {\n            readTxnLog(txn);\n            classifyFiles(txn);\n            files.put(txnFile, FileType.TXN_LOG);\n        }\n    }"
        ],
        [
            "LogTransaction::LogFilesByName::removeUnfinishedLeftovers(String,List)",
            " 426  \n 427  \n 428 -\n 429 -\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 -\n 443 -\n 444 -\n 445 -\n 446  ",
            "        static void removeUnfinishedLeftovers(String name, List<File> logFiles)\n        {\n            LogFile txn = LogFile.make(name, logFiles);\n            try\n            {\n                if (txn.verify())\n                {\n                    Throwable failure = txn.removeUnfinishedLeftovers(null);\n                    if (failure != null)\n                        logger.error(\"Failed to remove unfinished transaction leftovers for txn {}\", txn, failure);\n                }\n                else\n                {\n                    logger.error(\"Unexpected disk state: failed to read transaction txn {}\", txn);\n                }\n            }\n            finally\n            {\n                txn.close();\n            }\n        }",
            " 426  \n 427  \n 428 +\n 429 +\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  ",
            "        static void removeUnfinishedLeftovers(String name, List<File> logFiles)\n        {\n\n            try(LogFile txn = LogFile.make(name, logFiles))\n            {\n                if (txn.verify())\n                {\n                    Throwable failure = txn.removeUnfinishedLeftovers(null);\n                    if (failure != null)\n                        logger.error(\"Failed to remove unfinished transaction leftovers for txn {}\", txn, failure);\n                }\n                else\n                {\n                    logger.error(\"Unexpected disk state: failed to read transaction txn {}\", txn);\n                }\n            }\n        }"
        ]
    ],
    "5847222d9b2428c201a534876f86a0ec6f6f436f": [
        [
            "PagingState::RowMark::toString()",
            " 258  \n 259  \n 260  \n 261 -\n 262  ",
            "        @Override\n        public String toString()\n        {\n            return ByteBufferUtil.bytesToHex(mark);\n        }",
            " 261  \n 262  \n 263  \n 264 +\n 265  ",
            "        @Override\n        public String toString()\n        {\n            return mark == null ? \"null\" : ByteBufferUtil.bytesToHex(mark);\n        }"
        ],
        [
            "PagingState::RowMark::create(CFMetaData,Row,int)",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "        public static RowMark create(CFMetaData metadata, Row row, int protocolVersion)\n        {\n            ByteBuffer mark;\n            if (protocolVersion <= Server.VERSION_3)\n            {\n                // We need to be backward compatible with 2.1/2.2 nodes paging states. Which means we have to send\n                // the full cellname of the \"last\" cell in the row we get (since that's how 2.1/2.2 nodes will start after\n                // that last row if they get that paging state).\n                Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator();\n                if (!cells.hasNext())\n                {\n                    mark = LegacyLayout.encodeClustering(metadata, row.clustering());\n                }\n                else\n                {\n                    Cell cell = cells.next();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null);\n                }\n            }\n            else\n            {\n                // We froze the serialization version to 3.0 as we need to make this this doesn't change (that is, it has to be\n                // fix for a given version of the protocol).\n                mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata));\n            }\n            return new RowMark(mark, protocolVersion);\n        }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 +\n 217 +\n 218 +\n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "        public static RowMark create(CFMetaData metadata, Row row, int protocolVersion)\n        {\n            ByteBuffer mark;\n            if (protocolVersion <= Server.VERSION_3)\n            {\n                // We need to be backward compatible with 2.1/2.2 nodes paging states. Which means we have to send\n                // the full cellname of the \"last\" cell in the row we get (since that's how 2.1/2.2 nodes will start after\n                // that last row if they get that paging state).\n                Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, true).iterator();\n                if (!cells.hasNext())\n                {\n                    // If the last returned row has no cell, this means in 2.1/2.2 terms that we stopped on the row\n                    // marker. Note that this shouldn't happen if the table is COMPACT.\n                    assert !metadata.isCompactTable();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), ByteBufferUtil.EMPTY_BYTE_BUFFER, null);\n                }\n                else\n                {\n                    Cell cell = cells.next();\n                    mark = LegacyLayout.encodeCellName(metadata, row.clustering(), cell.column().name.bytes, cell.column().isComplex() ? cell.path().get(0) : null);\n                }\n            }\n            else\n            {\n                // We froze the serialization version to 3.0 as we need to make this this doesn't change (that is, it has to be\n                // fix for a given version of the protocol).\n                mark = Clustering.serializer.serialize(row.clustering(), MessagingService.VERSION_30, makeClusteringTypes(metadata));\n            }\n            return new RowMark(mark, protocolVersion);\n        }"
        ],
        [
            "ReadCommand::LegacyRangeSliceCommandSerializer::serialize(ReadCommand,DataOutputPlus,int)",
            " 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 -\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            assert version < MessagingService.VERSION_30;\n\n            PartitionRangeReadCommand rangeCommand = (PartitionRangeReadCommand) command;\n            assert !rangeCommand.dataRange().isPaging();\n\n            // convert pre-3.0 incompatible names filters to slice filters\n            rangeCommand = maybeConvertNamesToSlice(rangeCommand);\n\n            CFMetaData metadata = rangeCommand.metadata();\n\n            out.writeUTF(metadata.ksName);\n            out.writeUTF(metadata.cfName);\n            out.writeLong(rangeCommand.nowInSec() * 1000L);  // convert from seconds to millis\n\n            // begin DiskAtomFilterSerializer.serialize()\n            if (rangeCommand.isNamesQuery())\n            {\n                out.writeByte(1);  // 0 for slices, 1 for names\n                ClusteringIndexNamesFilter filter = (ClusteringIndexNamesFilter) rangeCommand.dataRange().clusteringIndexFilter;\n                LegacyReadCommandSerializer.serializeNamesFilter(rangeCommand, filter, out);\n            }\n            else\n            {\n                out.writeByte(0);  // 0 for slices, 1 for names\n\n                // slice filter serialization\n                ClusteringIndexSliceFilter filter = (ClusteringIndexSliceFilter) rangeCommand.dataRange().clusteringIndexFilter;\n\n                boolean makeStaticSlice = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && !filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                LegacyReadCommandSerializer.serializeSlices(out, filter.requestedSlices(), filter.isReversed(), makeStaticSlice, metadata);\n\n                out.writeBoolean(filter.isReversed());\n\n                // limit\n                DataLimits limits = rangeCommand.limits();\n                if (limits.isDistinct())\n                    out.writeInt(1);\n                else\n                    out.writeInt(LegacyReadCommandSerializer.updateLimitForQuery(rangeCommand.limits().count(), filter.requestedSlices()));\n\n                int compositesToGroup;\n                boolean selectsStatics = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() || filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                if (limits.kind() == DataLimits.Kind.THRIFT_LIMIT)\n                    compositesToGroup = -1;\n                else if (limits.isDistinct() && !selectsStatics)\n                    compositesToGroup = -2;  // for DISTINCT queries (CASSANDRA-8490)\n                else\n                    compositesToGroup = metadata.isDense() ? -1 : metadata.clusteringColumns().size();\n\n                out.writeInt(compositesToGroup);\n            }\n\n            serializeRowFilter(out, rangeCommand.rowFilter());\n            AbstractBounds.rowPositionSerializer.serialize(rangeCommand.dataRange().keyRange(), out, version);\n\n            // maxResults\n            out.writeInt(rangeCommand.limits().count());\n\n            // countCQL3Rows\n            if (rangeCommand.isForThrift() || rangeCommand.limits().perPartitionCount() == 1)  // if for Thrift or DISTINCT\n                out.writeBoolean(false);\n            else\n                out.writeBoolean(true);\n\n            // isPaging\n            out.writeBoolean(false);\n        }",
            " 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 +\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            assert version < MessagingService.VERSION_30;\n\n            PartitionRangeReadCommand rangeCommand = (PartitionRangeReadCommand) command;\n            assert !rangeCommand.dataRange().isPaging();\n\n            // convert pre-3.0 incompatible names filters to slice filters\n            rangeCommand = maybeConvertNamesToSlice(rangeCommand);\n\n            CFMetaData metadata = rangeCommand.metadata();\n\n            out.writeUTF(metadata.ksName);\n            out.writeUTF(metadata.cfName);\n            out.writeLong(rangeCommand.nowInSec() * 1000L);  // convert from seconds to millis\n\n            // begin DiskAtomFilterSerializer.serialize()\n            if (rangeCommand.isNamesQuery())\n            {\n                out.writeByte(1);  // 0 for slices, 1 for names\n                ClusteringIndexNamesFilter filter = (ClusteringIndexNamesFilter) rangeCommand.dataRange().clusteringIndexFilter;\n                LegacyReadCommandSerializer.serializeNamesFilter(rangeCommand, filter, out);\n            }\n            else\n            {\n                out.writeByte(0);  // 0 for slices, 1 for names\n\n                // slice filter serialization\n                ClusteringIndexSliceFilter filter = (ClusteringIndexSliceFilter) rangeCommand.dataRange().clusteringIndexFilter;\n\n                boolean makeStaticSlice = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && !filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                LegacyReadCommandSerializer.serializeSlices(out, filter.requestedSlices(), filter.isReversed(), makeStaticSlice, metadata);\n\n                out.writeBoolean(filter.isReversed());\n\n                // limit\n                DataLimits limits = rangeCommand.limits();\n                if (limits.isDistinct())\n                    out.writeInt(1);\n                else\n                    out.writeInt(LegacyReadCommandSerializer.updateLimitForQuery(rangeCommand.limits().count(), filter.requestedSlices()));\n\n                int compositesToGroup;\n                boolean selectsStatics = !rangeCommand.columnFilter().fetchedColumns().statics.isEmpty() && filter.requestedSlices().selects(Clustering.STATIC_CLUSTERING);\n                if (limits.kind() == DataLimits.Kind.THRIFT_LIMIT)\n                    compositesToGroup = -1;\n                else if (limits.isDistinct() && !selectsStatics)\n                    compositesToGroup = -2;  // for DISTINCT queries (CASSANDRA-8490)\n                else\n                    compositesToGroup = metadata.isDense() ? -1 : metadata.clusteringColumns().size();\n\n                out.writeInt(compositesToGroup);\n            }\n\n            serializeRowFilter(out, rangeCommand.rowFilter());\n            AbstractBounds.rowPositionSerializer.serialize(rangeCommand.dataRange().keyRange(), out, version);\n\n            // maxResults\n            out.writeInt(rangeCommand.limits().count());\n\n            // countCQL3Rows\n            if (rangeCommand.isForThrift() || rangeCommand.limits().perPartitionCount() == 1)  // if for Thrift or DISTINCT\n                out.writeBoolean(false);\n            else\n                out.writeBoolean(true);\n\n            // isPaging\n            out.writeBoolean(false);\n        }"
        ]
    ],
    "52bf7acb0520411f420ccf36b9a3770674f604f6": [
        [
            "UnfilteredDeserializer::OldFormatDeserializer::TombstoneTracker::openNew(LegacyLayout)",
            " 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650 -\n 651  \n 652  \n 653  \n 654  ",
            "            /**\n             * Update the tracker given the provided newly open tombstone. This return the Unfiltered corresponding to the opening\n             * of said tombstone: this can be a simple open mark, a boundary (if there was an open tombstone superseded by this new one)\n             * or even null (if the new tombston start is supersedes by the currently open tombstone).\n             *\n             * Note that this method assume the added tombstone is not fully shadowed, i.e. that !isShadowed(tombstone). It also\n             * assumes no opened tombstone closes before that tombstone (so !hasClosingMarkerBefore(tombstone)).\n             */\n            public Unfiltered openNew(LegacyLayout.LegacyRangeTombstone tombstone)\n            {\n                if (openTombstones.isEmpty())\n                {\n                    openTombstones.add(tombstone);\n                    return new RangeTombstoneBoundMarker(tombstone.start.bound, tombstone.deletionTime);\n                }\n\n                Iterator<LegacyLayout.LegacyRangeTombstone> iter = openTombstones.iterator();\n                LegacyLayout.LegacyRangeTombstone first = iter.next();\n                if (tombstone.deletionTime.supersedes(first.deletionTime))\n                {\n                    // We're supperseding the currently open tombstone, so we should produce a boundary that close the currently open\n                    // one and open the new one. We should also add the tombstone, but if it stop after the first one, we should\n                    // also remove that first tombstone as it won't be useful anymore.\n                    if (metadata.comparator.compare(tombstone.stop.bound, first.stop.bound) >= 0)\n                        iter.remove();\n\n                    openTombstones.add(tombstone);\n                    return RangeTombstoneBoundaryMarker.makeBoundary(false, tombstone.start.bound.invert(), tombstone.start.bound, first.deletionTime, tombstone.deletionTime);\n                }\n                else\n                {\n                    // If the new tombstone don't supersedes the currently open tombstone, we don't have anything to return, we\n                    // just add the new tombstone (because we know tombstone is not fully shadowed, this imply the new tombstone\n                    // simply extend after the first one and we'll deal with it later)\n                    assert metadata.comparator.compare(tombstone.start.bound, first.stop.bound) > 0;\n                    openTombstones.add(tombstone);\n                    return null;\n                }\n            }",
            " 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650 +\n 651  \n 652  \n 653  \n 654  ",
            "            /**\n             * Update the tracker given the provided newly open tombstone. This return the Unfiltered corresponding to the opening\n             * of said tombstone: this can be a simple open mark, a boundary (if there was an open tombstone superseded by this new one)\n             * or even null (if the new tombston start is supersedes by the currently open tombstone).\n             *\n             * Note that this method assume the added tombstone is not fully shadowed, i.e. that !isShadowed(tombstone). It also\n             * assumes no opened tombstone closes before that tombstone (so !hasClosingMarkerBefore(tombstone)).\n             */\n            public Unfiltered openNew(LegacyLayout.LegacyRangeTombstone tombstone)\n            {\n                if (openTombstones.isEmpty())\n                {\n                    openTombstones.add(tombstone);\n                    return new RangeTombstoneBoundMarker(tombstone.start.bound, tombstone.deletionTime);\n                }\n\n                Iterator<LegacyLayout.LegacyRangeTombstone> iter = openTombstones.iterator();\n                LegacyLayout.LegacyRangeTombstone first = iter.next();\n                if (tombstone.deletionTime.supersedes(first.deletionTime))\n                {\n                    // We're supperseding the currently open tombstone, so we should produce a boundary that close the currently open\n                    // one and open the new one. We should also add the tombstone, but if it stop after the first one, we should\n                    // also remove that first tombstone as it won't be useful anymore.\n                    if (metadata.comparator.compare(tombstone.stop.bound, first.stop.bound) >= 0)\n                        iter.remove();\n\n                    openTombstones.add(tombstone);\n                    return RangeTombstoneBoundaryMarker.makeBoundary(false, tombstone.start.bound.invert(), tombstone.start.bound, first.deletionTime, tombstone.deletionTime);\n                }\n                else\n                {\n                    // If the new tombstone don't supersedes the currently open tombstone, we don't have anything to return, we\n                    // just add the new tombstone (because we know tombstone is not fully shadowed, this imply the new tombstone\n                    // simply extend after the first one and we'll deal with it later)\n                    assert metadata.comparator.compare(tombstone.start.bound, first.stop.bound) <= 0;\n                    openTombstones.add(tombstone);\n                    return null;\n                }\n            }"
        ]
    ],
    "883c9f0f743139d78996f5faf191508a9be338b5": [
        [
            "JsonTransformer::serializePartition(UnfilteredRowIterator)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  \n 196 -\n 197 -\n 198 -\n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205 -\n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n            {\n                serializeDeletion(partition.partitionLevelDeletion());\n                json.writeEndObject();\n            }\n            else\n            {\n                json.writeEndObject();\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                {\n                    serializeRow(partition.staticRow());\n                }\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n            }\n\n            json.writeEndObject();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197 +\n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n                serializeDeletion(partition.partitionLevelDeletion());\n\n            json.writeEndObject();\n\n            if (partition.hasNext() || partition.staticRow() != null)\n            {\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                    serializeRow(partition.staticRow());\n\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n\n                json.writeEndObject();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }"
        ]
    ],
    "1ba68a1e5d681c091e2c53e7720029f10591e7ef": [
        [
            "LogTransactionTest::testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second()",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 -\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord);\n            FileUtils.append(logFiles.get(1), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 +\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc-\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord);\n            FileUtils.append(logFiles.get(1), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }"
        ],
        [
            "LogRecord::LogRecord(Type,String,long,int,long,String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    private LogRecord(Type type,\n                      String absolutePath,\n                      long updateTime,\n                      int numFiles,\n                      long checksum,\n                      String raw)\n    {\n        assert !type.hasFile() || absolutePath != null : \"Expected file path for file records\";\n\n        this.type = type;\n        this.absolutePath = type.hasFile() ? Optional.of(absolutePath) : Optional.<String>empty();\n        this.updateTime = type == Type.REMOVE ? updateTime : 0;\n        this.numFiles = type.hasFile() ? numFiles : 0;\n        this.status = new Status();\n        if (raw == null)\n        {\n            assert checksum == 0;\n            this.checksum = computeChecksum();\n            this.raw = format();\n        }\n        else\n        {\n            this.checksum = checksum;\n            this.raw = raw;\n        }\n    }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "    private LogRecord(Type type,\n                      String absolutePath,\n                      long updateTime,\n                      int numFiles,\n                      long checksum,\n                      String raw)\n    {\n        assert !type.hasFile() || absolutePath != null : \"Expected file path for file records\";\n\n        this.type = type;\n        this.absolutePath = type.hasFile() ? Optional.of(absolutePath) : Optional.empty();\n        this.updateTime = type == Type.REMOVE ? updateTime : 0;\n        this.numFiles = type.hasFile() ? numFiles : 0;\n        this.status = new Status();\n        if (raw == null)\n        {\n            assert checksum == 0;\n            this.checksum = computeChecksum();\n            this.raw = format();\n        }\n        else\n        {\n            this.checksum = checksum;\n            this.raw = raw;\n        }\n    }"
        ],
        [
            "LogTransactionTest::testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first()",
            " 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 -\n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            FileUtils.append(logFiles.get(1), sstableRecord);\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }",
            " 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 +\n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc-\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            FileUtils.append(logFiles.get(1), sstableRecord);\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }"
        ],
        [
            "LogRecord::absolutePath()",
            " 290 -\n 291  \n 292 -\n 293  ",
            "    String absolutePath()\n    {\n        return absolutePath.isPresent() ? absolutePath.get() : \"\";\n    }",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308 +\n 309 +\n 310 +\n 311 +\n 312 +\n 313 +\n 314  ",
            "    /**\n     * Return the absolute path, if present, except for the last character (the descriptor separator), or\n     * the empty string if the record has no path. This method is only to be used internally for writing\n     * the record to file or computing the checksum.\n     *\n     * CASSANDRA-13294: the last character of the absolute path is the descriptor separator, it is removed\n     * from the absolute path for backward compatibility, to make sure that on upgrade from 3.0.x to 3.0.y\n     * or to 3.y or to 4.0, the checksum of existing txn files still matches (in case of non clean shutdown\n     * some txn files may be present). By removing the last character here, it means that\n     * it will never be written to txn files, but it is added after reading a txn file in LogFile.make().\n     */\n    private String absolutePath()\n    {\n        if (!absolutePath.isPresent())\n            return \"\";\n\n        String ret = absolutePath.get();\n        assert ret.charAt(ret.length() -1) == Component.separator : \"Invalid absolute path, should end with '-'\";\n        return ret.substring(0, ret.length() - 1);\n    }"
        ],
        [
            "LogRecord::make(Type,SSTable)",
            " 147  \n 148  \n 149 -\n 150  \n 151  ",
            "    public static LogRecord make(Type type, SSTable table)\n    {\n        String absoluteTablePath = FileUtils.getCanonicalPath(table.descriptor.baseFilename());\n        return make(type, getExistingFiles(absoluteTablePath), table.getAllFilePaths().size(), absoluteTablePath);\n    }",
            " 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153 +\n 154 +\n 155  \n 156  ",
            "    public static LogRecord make(Type type, SSTable table)\n    {\n        // CASSANDRA-13294: add the sstable component separator because for legacy (2.1) files\n        // there is no separator after the generation number, and this would cause files of sstables with\n        // a higher generation number that starts with the same number, to be incorrectly classified as files\n        // of this record sstable\n        String absoluteTablePath = FileUtils.getCanonicalPath(table.descriptor.baseFilename() + Component.separator);\n        return make(type, getExistingFiles(absoluteTablePath), table.getAllFilePaths().size(), absoluteTablePath);\n    }"
        ],
        [
            "LogRecord::make(String)",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "    public static LogRecord make(String line)\n    {\n        try\n        {\n            Matcher matcher = REGEX.matcher(line);\n            if (!matcher.matches())\n                return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line)\n                       .setError(String.format(\"Failed to parse [%s]\", line));\n\n            Type type = Type.fromPrefix(matcher.group(1));\n            return new LogRecord(type,\n                                 matcher.group(2),\n                                 Long.valueOf(matcher.group(3)),\n                                 Integer.valueOf(matcher.group(4)),\n                                 Long.valueOf(matcher.group(5)), line);\n        }\n        catch (Throwable t)\n        {\n            return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line).setError(t);\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "    public static LogRecord make(String line)\n    {\n        try\n        {\n            Matcher matcher = REGEX.matcher(line);\n            if (!matcher.matches())\n                return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line)\n                       .setError(String.format(\"Failed to parse [%s]\", line));\n\n            Type type = Type.fromPrefix(matcher.group(1));\n            return new LogRecord(type,\n                                 matcher.group(2) + Component.separator, // see comment on CASSANDRA-13294 below\n                                 Long.valueOf(matcher.group(3)),\n                                 Integer.valueOf(matcher.group(4)),\n                                 Long.valueOf(matcher.group(5)), line);\n        }\n        catch (Throwable t)\n        {\n            return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line).setError(t);\n        }\n    }"
        ]
    ],
    "e885886d5c92bfd8d2fa1596bfa86d6a5a8d89bb": [
        [
            "ThreadAwareSecurityManager::SMAwareReconfigureOnChangeFilter::SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter)",
            " 105  \n 106  \n 107  \n 108  ",
            "        SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter reconfigureOnChangeFilter)\n        {\n            setRefreshPeriod(reconfigureOnChangeFilter.getRefreshPeriod());\n        }",
            " 105  \n 106  \n 107  \n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115  ",
            "        SMAwareReconfigureOnChangeFilter(ReconfigureOnChangeFilter reconfigureOnChangeFilter)\n        {\n            setRefreshPeriod(reconfigureOnChangeFilter.getRefreshPeriod());\n            setName(reconfigureOnChangeFilter.getName());\n            setContext(reconfigureOnChangeFilter.getContext());\n            if (reconfigureOnChangeFilter.isStarted())\n            {\n                reconfigureOnChangeFilter.stop();\n                start();\n            }\n        }"
        ]
    ],
    "6ed9134336bb48d04284cefd303d8374ed901c0a": [
        [
            "StorageService::excise(Collection,InetAddress)",
            "2246  \n2247  \n2248  \n2249  \n2250 -\n2251 -\n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  ",
            "    private void excise(Collection<Token> tokens, InetAddress endpoint)\n    {\n        logger.info(\"Removing tokens {} for {}\", tokens, endpoint);\n\n        if (tokenMetadata.isMember(endpoint))\n            HintsService.instance.excise(tokenMetadata.getHostId(endpoint));\n\n        removeEndpoint(endpoint);\n        tokenMetadata.removeEndpoint(endpoint);\n        if (!tokens.isEmpty())\n            tokenMetadata.removeBootstrapTokens(tokens);\n        notifyLeft(endpoint);\n        PendingRangeCalculatorService.instance.update();\n    }",
            "2246  \n2247  \n2248  \n2249  \n2250 +\n2251 +\n2252 +\n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  ",
            "    private void excise(Collection<Token> tokens, InetAddress endpoint)\n    {\n        logger.info(\"Removing tokens {} for {}\", tokens, endpoint);\n\n        UUID hostId = tokenMetadata.getHostId(endpoint);\n        if (hostId != null && tokenMetadata.isMember(endpoint))\n            HintsService.instance.excise(hostId);\n\n        removeEndpoint(endpoint);\n        tokenMetadata.removeEndpoint(endpoint);\n        if (!tokens.isEmpty())\n            tokenMetadata.removeBootstrapTokens(tokens);\n        notifyLeft(endpoint);\n        PendingRangeCalculatorService.instance.update();\n    }"
        ]
    ],
    "17d43fa55eca29be492a716f04d9ceff1989762d": [
        [
            "CqlRecordWriter::RangeClient::run()",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302 -\n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308 -\n 309 -\n 310 -\n 311 -\n 312 -\n 313 -\n 314 -\n 315 -\n 316 -\n 317 -\n 318 -\n 319 -\n 320 -\n 321 -\n 322 -\n 323 -\n 324 -\n 325 -\n 326 -\n 327 -\n 328 -\n 329 -\n 330 -\n 331 -\n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 -\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        //There are too many ways for the Thread.interrupted() state to be cleared, so\n                        //we can't rely on that here. Until the java driver gives us a better way of knowing\n                        //that this exception came from an InterruptedException, this is the best solution.\n                        if (canRetryDriverConnection(e))\n                        {\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                        continue;\n                    }\n\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n                            \n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337 +\n 338 +\n 339 +\n 340 +\n 341 +\n 342 +\n 343 +\n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349 +\n 350 +\n 351 +\n 352 +\n 353 +\n 354 +\n 355 +\n 356 +\n 357 +\n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n\n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        //There are too many ways for the Thread.interrupted() state to be cleared, so\n                        //we can't rely on that here. Until the java driver gives us a better way of knowing\n                        //that this exception came from an InterruptedException, this is the best solution.\n                        if (canRetryDriverConnection(e))\n                        {\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }"
        ],
        [
            "CqlRecordWriter::RangeClient::closeInternal()",
            " 408  \n 409  \n 410  \n 411  \n 412 -\n 413  \n 414  ",
            "        protected void closeInternal()\n        {\n            if (client != null)\n            {\n                client.close();;\n            }\n        }",
            " 405  \n 406  \n 407  \n 408  \n 409 +\n 410  \n 411  ",
            "        protected void closeInternal()\n        {\n            if (client != null)\n            {\n                client.close();\n            }\n        }"
        ]
    ],
    "4c6411f083b9448114a0ba349fc02e60299f6541": [
        [
            "OutboundTcpConnection::expireMessages()",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  ",
            "    private void expireMessages()\n    {\n        Iterator<QueuedMessage> iter = backlog.iterator();\n        while (iter.hasNext())\n        {\n            QueuedMessage qm = iter.next();\n            if (qm.timestampNanos >= System.nanoTime() - qm.message.getTimeout())\n                return;\n            iter.remove();\n            dropped.incrementAndGet();\n        }\n    }",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521 +\n 522 +\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "    private void expireMessages()\n    {\n        Iterator<QueuedMessage> iter = backlog.iterator();\n        while (iter.hasNext())\n        {\n            QueuedMessage qm = iter.next();\n            if (!qm.droppable)\n                continue;\n            if (qm.timestampNanos >= System.nanoTime() - qm.message.getTimeout())\n                return;\n            iter.remove();\n            dropped.incrementAndGet();\n        }\n    }"
        ]
    ],
    "fa14804543e9ed2cc781d9f8511ab7a5c22f8dd7": [
        [
            "JsonTransformer::serializePartition(UnfilteredRowIterator)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n            {\n                serializeDeletion(partition.partitionLevelDeletion());\n            }\n            else\n            {\n                json.writeEndObject();\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                {\n                    serializeRow(partition.staticRow());\n                }\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n            }\n\n            json.writeEndObject();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    private void serializePartition(UnfilteredRowIterator partition)\n    {\n        String key = metadata.getKeyValidator().getString(partition.partitionKey().getKey());\n        try\n        {\n            json.writeStartObject();\n\n            json.writeFieldName(\"partition\");\n            json.writeStartObject();\n            json.writeFieldName(\"key\");\n            serializePartitionKey(partition.partitionKey());\n            json.writeNumberField(\"position\", this.currentScanner.getCurrentPosition());\n\n            if (!partition.partitionLevelDeletion().isLive())\n            {\n                serializeDeletion(partition.partitionLevelDeletion());\n                json.writeEndObject();\n            }\n            else\n            {\n                json.writeEndObject();\n                json.writeFieldName(\"rows\");\n                json.writeStartArray();\n                updatePosition();\n                if (!partition.staticRow().isEmpty())\n                {\n                    serializeRow(partition.staticRow());\n                }\n                Unfiltered unfiltered;\n                updatePosition();\n                while (partition.hasNext())\n                {\n                    unfiltered = partition.next();\n                    if (unfiltered instanceof Row)\n                    {\n                        serializeRow((Row) unfiltered);\n                    }\n                    else if (unfiltered instanceof RangeTombstoneMarker)\n                    {\n                        serializeTombstone((RangeTombstoneMarker) unfiltered);\n                    }\n                    updatePosition();\n                }\n                json.writeEndArray();\n            }\n\n            json.writeEndObject();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Fatal error parsing partition: {}\", key, e);\n        }\n    }"
        ]
    ],
    "7e362e78c342d8ca016de12218732a3e5f7dcc36": [
        [
            "TableHistograms::execute(NodeProbe)",
            "  40  \n  41  \n  42  \n  43 -\n  44  \n  45  \n  46 -\n  47 -\n  48  \n  49  \n  50  \n  51  \n  52  \n  53 -\n  54 -\n  55  \n  56  \n  57  \n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63 -\n  64 -\n  65 -\n  66 -\n  67 -\n  68 -\n  69 -\n  70 -\n  71 -\n  72 -\n  73 -\n  74 -\n  75  \n  76 -\n  77 -\n  78  \n  79  \n  80 -\n  81 -\n  82 -\n  83 -\n  84  \n  85 -\n  86  \n  87 -\n  88 -\n  89  \n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95 -\n  96  \n  97 -\n  98 -\n  99 -\n 100 -\n 101 -\n 102 -\n 103 -\n 104 -\n 105 -\n 106 -\n 107 -\n 108  \n 109 -\n 110 -\n 111 -\n 112 -\n 113 -\n 114 -\n 115 -\n 116  \n 117 -\n 118 -\n 119 -\n 120 -\n 121  \n 122 -\n 123 -\n 124 -\n 125 -\n 126 -\n 127  \n 128 -\n 129 -\n 130 -\n 131 -\n 132 -\n 133 -\n 134 -\n 135 -\n 136 -\n 137  \n 138 -\n 139  ",
            "    @Override\n    public void execute(NodeProbe probe)\n    {\n        String keyspace = null, table = null;\n        if (args.size() == 2)\n        {\n            keyspace = args.get(0);\n            table = args.get(1);\n        }\n        else if (args.size() == 1)\n        {\n            String[] input = args.get(0).split(\"\\\\.\");\n            checkArgument(input.length == 2, \"tablehistograms requires keyspace and table name arguments\");\n            keyspace = input[0];\n            table = input[1];\n        }\n        else\n        {\n            checkArgument(false, \"tablehistograms requires keyspace and table name arguments\");\n        }\n\n        // calculate percentile of row size and column count\n        long[] estimatedPartitionSize = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedPartitionSizeHistogram\");\n        long[] estimatedColumnCount = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedColumnCountHistogram\");\n\n        // build arrays to store percentile values\n        double[] estimatedRowSizePercentiles = new double[7];\n        double[] estimatedColumnCountPercentiles = new double[7];\n        double[] offsetPercentiles = new double[]{0.5, 0.75, 0.95, 0.98, 0.99};\n\n        if (ArrayUtils.isEmpty(estimatedPartitionSize) || ArrayUtils.isEmpty(estimatedColumnCount))\n        {\n            System.err.println(\"No SSTables exists, unable to calculate 'Partition Size' and 'Cell Count' percentiles\");\n\n            for (int i = 0; i < 7; i++)\n            {\n                estimatedRowSizePercentiles[i] = Double.NaN;\n                estimatedColumnCountPercentiles[i] = Double.NaN;\n            }\n        }\n        else\n        {\n            EstimatedHistogram partitionSizeHist = new EstimatedHistogram(estimatedPartitionSize);\n            EstimatedHistogram columnCountHist = new EstimatedHistogram(estimatedColumnCount);\n\n            if (partitionSizeHist.isOverflowed())\n            {\n                System.err.println(String.format(\"Row sizes are larger than %s, unable to calculate percentiles\", partitionSizeHist.getLargestBucketOffset()));\n                for (int i = 0; i < offsetPercentiles.length; i++)\n                        estimatedRowSizePercentiles[i] = Double.NaN;\n            }\n            else\n            {\n                for (int i = 0; i < offsetPercentiles.length; i++)\n                    estimatedRowSizePercentiles[i] = partitionSizeHist.percentile(offsetPercentiles[i]);\n            }\n\n            if (columnCountHist.isOverflowed())\n            {\n                System.err.println(String.format(\"Column counts are larger than %s, unable to calculate percentiles\", columnCountHist.getLargestBucketOffset()));\n                for (int i = 0; i < estimatedColumnCountPercentiles.length; i++)\n                    estimatedColumnCountPercentiles[i] = Double.NaN;\n            }\n            else\n            {\n                for (int i = 0; i < offsetPercentiles.length; i++)\n                    estimatedColumnCountPercentiles[i] = columnCountHist.percentile(offsetPercentiles[i]);\n            }\n\n            // min value\n            estimatedRowSizePercentiles[5] = partitionSizeHist.min();\n            estimatedColumnCountPercentiles[5] = columnCountHist.min();\n            // max value\n            estimatedRowSizePercentiles[6] = partitionSizeHist.max();\n            estimatedColumnCountPercentiles[6] = columnCountHist.max();\n        }\n\n        String[] percentiles = new String[]{\"50%\", \"75%\", \"95%\", \"98%\", \"99%\", \"Min\", \"Max\"};\n        double[] readLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"ReadLatency\"));\n        double[] writeLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"WriteLatency\"));\n        double[] sstablesPerRead = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxHistogramMBean) probe.getColumnFamilyMetric(keyspace, table, \"SSTablesPerReadHistogram\"));\n\n        System.out.println(format(\"%s/%s histograms\", keyspace, table));\n        System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",\n                \"Percentile\", \"SSTables\", \"Write Latency\", \"Read Latency\", \"Partition Size\", \"Cell Count\"));\n        System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",\n                \"\", \"\", \"(micros)\", \"(micros)\", \"(bytes)\", \"\"));\n\n        for (int i = 0; i < percentiles.length; i++)\n        {\n            System.out.println(format(\"%-10s%10.2f%18.2f%18.2f%18.0f%18.0f\",\n                    percentiles[i],\n                    sstablesPerRead[i],\n                    writeLatency[i],\n                    readLatency[i],\n                    estimatedRowSizePercentiles[i],\n                    estimatedColumnCountPercentiles[i]));\n        }\n        System.out.println();\n    }",
            "  45  \n  46  \n  47  \n  48 +\n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  \n  65 +\n  66 +\n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73  \n  74  \n  75  \n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82  \n  83 +\n  84 +\n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98  \n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106  \n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118  \n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130  \n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136 +\n 137 +\n 138  \n 139 +\n 140 +\n 141 +\n 142 +\n 143  \n 144 +\n 145 +\n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151 +\n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158 +\n 159 +\n 160 +\n 161 +\n 162  \n 163  ",
            "    @Override\n    public void execute(NodeProbe probe)\n    {\n        Map<String, List<String>> tablesList = new HashMap<>();\n        if (args.size() == 2)\n        {\n            tablesList.put(args.get(0), new ArrayList<String>(Arrays.asList(args.get(1))));\n        }\n        else if (args.size() == 1)\n        {\n            String[] input = args.get(0).split(\"\\\\.\");\n            checkArgument(input.length == 2, \"tablehistograms requires keyspace and table name arguments\");\n            tablesList.put(input[0], new ArrayList<String>(Arrays.asList(input[1])));\n        }\n        else\n        {\n            // get a list of table stores\n            Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> tableMBeans = probe.getColumnFamilyStoreMBeanProxies();\n            while (tableMBeans.hasNext())\n            {\n                Map.Entry<String, ColumnFamilyStoreMBean> entry = tableMBeans.next();\n                String keyspaceName = entry.getKey();\n                ColumnFamilyStoreMBean tableProxy = entry.getValue();\n                if (!tablesList.containsKey(keyspaceName))\n                {\n                    tablesList.put(keyspaceName, new ArrayList<String>());\n                }\n                tablesList.get(keyspaceName).add(tableProxy.getTableName());\n            }\n        }\n\n        Iterator<Map.Entry<String, List<String>>> iter = tablesList.entrySet().iterator();\n        while(iter.hasNext())\n        {\n            Map.Entry<String, List<String>> entry = iter.next();\n            String keyspace = entry.getKey();\n            for (String table : entry.getValue())\n            {\n                // calculate percentile of row size and column count\n                long[] estimatedPartitionSize = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedPartitionSizeHistogram\");\n                long[] estimatedColumnCount = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedColumnCountHistogram\");\n\n                // build arrays to store percentile values\n                double[] estimatedRowSizePercentiles = new double[7];\n                double[] estimatedColumnCountPercentiles = new double[7];\n                double[] offsetPercentiles = new double[]{0.5, 0.75, 0.95, 0.98, 0.99};\n\n                if (ArrayUtils.isEmpty(estimatedPartitionSize) || ArrayUtils.isEmpty(estimatedColumnCount))\n                {\n                    System.out.println(\"No SSTables exists, unable to calculate 'Partition Size' and 'Cell Count' percentiles\");\n\n                    for (int i = 0; i < 7; i++)\n                    {\n                        estimatedRowSizePercentiles[i] = Double.NaN;\n                        estimatedColumnCountPercentiles[i] = Double.NaN;\n                    }\n                }\n                else\n                {\n                    EstimatedHistogram partitionSizeHist = new EstimatedHistogram(estimatedPartitionSize);\n                    EstimatedHistogram columnCountHist = new EstimatedHistogram(estimatedColumnCount);\n\n                    if (partitionSizeHist.isOverflowed())\n                    {\n                        System.out.println(String.format(\"Row sizes are larger than %s, unable to calculate percentiles\", partitionSizeHist.getLargestBucketOffset()));\n                        for (int i = 0; i < offsetPercentiles.length; i++)\n                            estimatedRowSizePercentiles[i] = Double.NaN;\n                    }\n                    else\n                    {\n                        for (int i = 0; i < offsetPercentiles.length; i++)\n                            estimatedRowSizePercentiles[i] = partitionSizeHist.percentile(offsetPercentiles[i]);\n                    }\n\n                    if (columnCountHist.isOverflowed())\n                    {\n                        System.out.println(String.format(\"Column counts are larger than %s, unable to calculate percentiles\", columnCountHist.getLargestBucketOffset()));\n                        for (int i = 0; i < estimatedColumnCountPercentiles.length; i++)\n                            estimatedColumnCountPercentiles[i] = Double.NaN;\n                    }\n                    else\n                    {\n                        for (int i = 0; i < offsetPercentiles.length; i++)\n                            estimatedColumnCountPercentiles[i] = columnCountHist.percentile(offsetPercentiles[i]);\n                    }\n\n                    // min value\n                    estimatedRowSizePercentiles[5] = partitionSizeHist.min();\n                    estimatedColumnCountPercentiles[5] = columnCountHist.min();\n                    // max value\n                    estimatedRowSizePercentiles[6] = partitionSizeHist.max();\n                    estimatedColumnCountPercentiles[6] = columnCountHist.max();\n                }\n\n                String[] percentiles = new String[]{\"50%\", \"75%\", \"95%\", \"98%\", \"99%\", \"Min\", \"Max\"};\n                double[] readLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"ReadLatency\"));\n                double[] writeLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"WriteLatency\"));\n                double[] sstablesPerRead = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxHistogramMBean) probe.getColumnFamilyMetric(keyspace, table, \"SSTablesPerReadHistogram\"));\n\n                System.out.println(format(\"%s/%s histograms\", keyspace, table));\n                System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",\n                        \"Percentile\", \"SSTables\", \"Write Latency\", \"Read Latency\", \"Partition Size\", \"Cell Count\"));\n                System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",\n                        \"\", \"\", \"(micros)\", \"(micros)\", \"(bytes)\", \"\"));\n\n                for (int i = 0; i < percentiles.length; i++)\n                {\n                    System.out.println(format(\"%-10s%10.2f%18.2f%18.2f%18.0f%18.0f\",\n                            percentiles[i],\n                            sstablesPerRead[i],\n                            writeLatency[i],\n                            readLatency[i],\n                            estimatedRowSizePercentiles[i],\n                            estimatedColumnCountPercentiles[i]));\n                }\n                System.out.println();\n            }\n        }\n    }"
        ]
    ],
    "0a4728f62b51095706bf7155e8f60b39ec5fa082": [
        [
            "ThriftSessionManager::currentSession()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 -\n  61 -\n  62  \n  63  \n  64  ",
            "    /**\n     * @return the current session for the most recently given socket on this thread\n     */\n    public ThriftClientState currentSession()\n    {\n        SocketAddress socket = remoteSocket.get();\n        assert socket != null;\n\n        ThriftClientState cState = activeSocketSessions.get(socket);\n        if (cState == null)\n        {\n            cState = new ThriftClientState(socket);\n            activeSocketSessions.put(socket, cState);\n        }\n        return cState;\n    }",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65  \n  66  \n  67  ",
            "    /**\n     * @return the current session for the most recently given socket on this thread\n     */\n    public ThriftClientState currentSession()\n    {\n        SocketAddress socket = remoteSocket.get();\n        assert socket != null;\n\n        ThriftClientState cState = activeSocketSessions.get(socket);\n        if (cState == null)\n        {\n            //guarantee atomicity\n            ThriftClientState newState = new ThriftClientState(socket);\n            cState = activeSocketSessions.putIfAbsent(socket, newState);\n            if (cState == null)\n                cState = newState;\n        }\n        return cState;\n    }"
        ],
        [
            "StreamSession::addTransferFiles(Collection)",
            " 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 -\n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  ",
            "    public void addTransferFiles(Collection<SSTableStreamingSections> sstableDetails)\n    {\n        Iterator<SSTableStreamingSections> iter = sstableDetails.iterator();\n        while (iter.hasNext())\n        {\n            SSTableStreamingSections details = iter.next();\n            if (details.sections.isEmpty())\n            {\n                // A reference was acquired on the sstable and we won't stream it\n                details.ref.release();\n                iter.remove();\n                continue;\n            }\n\n            UUID cfId = details.ref.get().metadata.cfId;\n            StreamTransferTask task = transfers.get(cfId);\n            if (task == null)\n            {\n                task = new StreamTransferTask(this, cfId);\n                transfers.put(cfId, task);\n            }\n            task.addTransferFile(details.ref, details.estimatedKeys, details.sections, details.repairedAt);\n            iter.remove();\n        }\n    }",
            " 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 +\n 373 +\n 374 +\n 375 +\n 376 +\n 377  \n 378  \n 379  \n 380  \n 381  ",
            "    public void addTransferFiles(Collection<SSTableStreamingSections> sstableDetails)\n    {\n        Iterator<SSTableStreamingSections> iter = sstableDetails.iterator();\n        while (iter.hasNext())\n        {\n            SSTableStreamingSections details = iter.next();\n            if (details.sections.isEmpty())\n            {\n                // A reference was acquired on the sstable and we won't stream it\n                details.ref.release();\n                iter.remove();\n                continue;\n            }\n\n            UUID cfId = details.ref.get().metadata.cfId;\n            StreamTransferTask task = transfers.get(cfId);\n            if (task == null)\n            {\n                //guarantee atomicity\n                StreamTransferTask newTask = new StreamTransferTask(this, cfId);\n                task = transfers.putIfAbsent(cfId, newTask);\n                if (task == null)\n                    task = newTask;\n            }\n            task.addTransferFile(details.ref, details.estimatedKeys, details.sections, details.repairedAt);\n            iter.remove();\n        }\n    }"
        ]
    ],
    "6ff1cbb3ee1b7e6f261aeb454854dd249ab605df": [
        [
            "TokenMetadata::pendingEndpointsFor(Token,String)",
            "1044  \n1045  \n1046 -\n1047 -\n1048  \n1049  \n1050 -\n1051 -\n1052 -\n1053 -\n1054 -\n1055 -\n1056 -\n1057 -\n1058  ",
            "    public Collection<InetAddress> pendingEndpointsFor(Token token, String keyspaceName)\n    {\n        Map<Range<Token>, Collection<InetAddress>> ranges = getPendingRanges(keyspaceName);\n        if (ranges.isEmpty())\n            return Collections.emptyList();\n\n        Set<InetAddress> endpoints = new HashSet<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : ranges.entrySet())\n        {\n            if (entry.getKey().contains(token))\n                endpoints.addAll(entry.getValue());\n        }\n\n        return endpoints;\n    }",
            "1052  \n1053  \n1054 +\n1055 +\n1056  \n1057  \n1058 +\n1059  ",
            "    public Collection<InetAddress> pendingEndpointsFor(Token token, String keyspaceName)\n    {\n        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);\n        if (pendingRangeMaps == null)\n            return Collections.emptyList();\n\n        return pendingRangeMaps.pendingEndpointsFor(token);\n    }"
        ],
        [
            "TokenMetadata::calculatePendingRanges(AbstractReplicationStrategy,String)",
            " 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736 -\n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764 -\n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797 -\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            Multimap<Range<Token>, InetAddress> newPendingRanges = HashMultimap.create();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                newPendingRanges.putAll(range, Sets.difference(newEndpoints, currentEndpoints));\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                    newPendingRanges.put(range, endpoint);\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.put(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }",
            " 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743 +\n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771 +\n 772 +\n 773 +\n 774 +\n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789 +\n 790 +\n 791 +\n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809 +\n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            PendingRangeMaps newPendingRanges = new PendingRangeMaps();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                for (InetAddress address : Sets.difference(newEndpoints, currentEndpoints))\n                {\n                    newPendingRanges.addPendingRange(range, address);\n                }\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.addPendingRange(range, endpoint);\n                }\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.addPendingRange(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }"
        ],
        [
            "StorageService::getPendingRangeToEndpointMap(String)",
            "1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381 -\n1382  \n1383  \n1384  \n1385  \n1386  \n1387  ",
            "    public Map<List<String>, List<String>> getPendingRangeToEndpointMap(String keyspace)\n    {\n        // some people just want to get a visual representation of things. Allow null and set it to the first\n        // non-system keyspace.\n        if (keyspace == null)\n            keyspace = Schema.instance.getNonSystemKeyspaces().get(0);\n\n        Map<List<String>, List<String>> map = new HashMap<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata.getPendingRanges(keyspace).entrySet())\n        {\n            List<InetAddress> l = new ArrayList<>(entry.getValue());\n            map.put(entry.getKey().asList(), stringify(l));\n        }\n        return map;\n    }",
            "1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381 +\n1382  \n1383  \n1384  \n1385  \n1386  \n1387  ",
            "    public Map<List<String>, List<String>> getPendingRangeToEndpointMap(String keyspace)\n    {\n        // some people just want to get a visual representation of things. Allow null and set it to the first\n        // non-system keyspace.\n        if (keyspace == null)\n            keyspace = Schema.instance.getNonSystemKeyspaces().get(0);\n\n        Map<List<String>, List<String>> map = new HashMap<>();\n        for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata.getPendingRangesMM(keyspace).asMap().entrySet())\n        {\n            List<InetAddress> l = new ArrayList<>(entry.getValue());\n            map.put(entry.getKey().asList(), stringify(l));\n        }\n        return map;\n    }"
        ],
        [
            "TokenMetadata::getPendingRanges(String)",
            " 689  \n 690 -\n 691  \n 692 -\n 693  ",
            "    /** a mutable map may be returned but caller should not modify it */\n    public Map<Range<Token>, Collection<InetAddress>> getPendingRanges(String keyspaceName)\n    {\n        return getPendingRangesMM(keyspaceName).asMap();\n    }",
            " 696  \n 697 +\n 698  \n 699 +\n 700  ",
            "    /** a mutable map may be returned but caller should not modify it */\n    public PendingRangeMaps getPendingRanges(String keyspaceName)\n    {\n        return this.pendingRanges.get(keyspaceName);\n    }"
        ],
        [
            "TokenMetadata::getPendingRangesMM(String)",
            " 676 -\n 677  \n 678 -\n 679 -\n 680  \n 681 -\n 682 -\n 683 -\n 684 -\n 685  \n 686  \n 687  ",
            "    private Multimap<Range<Token>, InetAddress> getPendingRangesMM(String keyspaceName)\n    {\n        Multimap<Range<Token>, InetAddress> map = pendingRanges.get(keyspaceName);\n        if (map == null)\n        {\n            map = HashMultimap.create();\n            Multimap<Range<Token>, InetAddress> priorMap = pendingRanges.putIfAbsent(keyspaceName, map);\n            if (priorMap != null)\n                map = priorMap;\n        }\n        return map;\n    }",
            " 676 +\n 677  \n 678 +\n 679 +\n 680 +\n 681 +\n 682  \n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690 +\n 691  \n 692 +\n 693  \n 694  ",
            "    public Multimap<Range<Token>, InetAddress> getPendingRangesMM(String keyspaceName)\n    {\n        Multimap<Range<Token>, InetAddress> map = HashMultimap.create();\n        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);\n\n        if (pendingRangeMaps != null)\n        {\n            for (Map.Entry<Range<Token>, List<InetAddress>> entry : pendingRangeMaps)\n            {\n                Range<Token> range = entry.getKey();\n                for (InetAddress address : entry.getValue())\n                {\n                    map.put(range, address);\n                }\n            }\n        }\n\n        return map;\n    }"
        ],
        [
            "TokenMetadata::printPendingRanges()",
            "1028  \n1029  \n1030  \n1031  \n1032 -\n1033  \n1034 -\n1035 -\n1036 -\n1037 -\n1038 -\n1039  \n1040  \n1041  \n1042  ",
            "    private String printPendingRanges()\n    {\n        StringBuilder sb = new StringBuilder();\n\n        for (Map.Entry<String, Multimap<Range<Token>, InetAddress>> entry : pendingRanges.entrySet())\n        {\n            for (Map.Entry<Range<Token>, InetAddress> rmap : entry.getValue().entries())\n            {\n                sb.append(rmap.getValue()).append(':').append(rmap.getKey());\n                sb.append(System.getProperty(\"line.separator\"));\n            }\n        }\n\n        return sb.toString();\n    }",
            "1040  \n1041  \n1042  \n1043  \n1044 +\n1045  \n1046 +\n1047  \n1048  \n1049  \n1050  ",
            "    private String printPendingRanges()\n    {\n        StringBuilder sb = new StringBuilder();\n\n        for (PendingRangeMaps pendingRangeMaps : pendingRanges.values())\n        {\n            sb.append(pendingRangeMaps.printPendingRanges());\n        }\n\n        return sb.toString();\n    }"
        ]
    ],
    "c92928bb9c2441254b51e2ea4dc742c9245b9f4c": [
        [
            "RepairRunnable::runMayThrow()",
            " 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies;\n        try\n        {\n            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              options.isPullRepair(),\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results)\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }",
            " 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies;\n        try\n        {\n            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d (%s), repairing keyspace %s with %s\", cmd, parentSession, keyspace,\n                                       options);\n        logger.info(message);\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            message = message + \" tracing with \" + sessionId;\n            fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              options.isPullRepair(),\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results)\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }"
        ]
    ],
    "2cd18ef5a01a06d90e13e61971e5601c7de61e7c": [
        [
            "SSTableMetadataViewer::main(String)",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "    /**\n     * @param args a list of sstables whose metadata we're interested in\n     */\n    public static void main(String[] args) throws IOException\n    {\n        PrintStream out = System.out;\n        if (args.length == 0)\n        {\n            out.println(\"Usage: sstablemetadata <sstable filenames>\");\n            System.exit(1);\n        }\n\n        Util.initDatabaseDescriptor();\n\n        for (String fname : args)\n        {\n            if (new File(fname).exists())\n            {\n                Descriptor descriptor = Descriptor.fromFilename(fname);\n                Map<MetadataType, MetadataComponent> metadata = descriptor.getMetadataSerializer().deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n                ValidationMetadata validation = (ValidationMetadata) metadata.get(MetadataType.VALIDATION);\n                StatsMetadata stats = (StatsMetadata) metadata.get(MetadataType.STATS);\n                CompactionMetadata compaction = (CompactionMetadata) metadata.get(MetadataType.COMPACTION);\n\n                out.printf(\"SSTable: %s%n\", descriptor);\n                if (validation != null)\n                {\n                    out.printf(\"Partitioner: %s%n\", validation.partitioner);\n                    out.printf(\"Bloom Filter FP chance: %f%n\", validation.bloomFilterFPChance);\n                }\n                if (stats != null)\n                {\n                    out.printf(\"Minimum timestamp: %s%n\", stats.minTimestamp);\n                    out.printf(\"Maximum timestamp: %s%n\", stats.maxTimestamp);\n                    out.printf(\"SSTable max local deletion time: %s%n\", stats.maxLocalDeletionTime);\n                    out.printf(\"Compression ratio: %s%n\", stats.compressionRatio);\n                    out.printf(\"Estimated droppable tombstones: %s%n\", stats.getEstimatedDroppableTombstoneRatio((int) (System.currentTimeMillis() / 1000)));\n                    out.printf(\"SSTable Level: %d%n\", stats.sstableLevel);\n                    out.printf(\"Repaired at: %d%n\", stats.repairedAt);\n                    out.println(stats.replayPosition);\n                    out.println(\"Estimated tombstone drop times:\");\n                    for (Map.Entry<Double, Long> entry : stats.estimatedTombstoneDropTime.getAsMap().entrySet())\n                    {\n                        out.printf(\"%-10s:%10s%n\",entry.getKey().intValue(), entry.getValue());\n                    }\n                    printHistograms(stats, out);\n                }\n                if (compaction != null)\n                {\n                    out.printf(\"Estimated cardinality: %s%n\", compaction.cardinalityEstimator.cardinality());\n                }\n            }\n            else\n            {\n                out.println(\"No such file: \" + fname);\n            }\n        }\n    }",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "    /**\n     * @param args a list of sstables whose metadata we're interested in\n     */\n    public static void main(String[] args) throws IOException\n    {\n        PrintStream out = System.out;\n        if (args.length == 0)\n        {\n            out.println(\"Usage: sstablemetadata <sstable filenames>\");\n            System.exit(1);\n        }\n\n        Util.initDatabaseDescriptor();\n\n        for (String fname : args)\n        {\n            if (new File(fname).exists())\n            {\n                Descriptor descriptor = Descriptor.fromFilename(fname);\n                Map<MetadataType, MetadataComponent> metadata = descriptor.getMetadataSerializer().deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n                ValidationMetadata validation = (ValidationMetadata) metadata.get(MetadataType.VALIDATION);\n                StatsMetadata stats = (StatsMetadata) metadata.get(MetadataType.STATS);\n                CompactionMetadata compaction = (CompactionMetadata) metadata.get(MetadataType.COMPACTION);\n                CompressionMetadata compression = null;\n                File compressionFile = new File(descriptor.filenameFor(Component.COMPRESSION_INFO));\n                if (compressionFile.exists())\n                    compression = CompressionMetadata.create(fname);\n\n                out.printf(\"SSTable: %s%n\", descriptor);\n                if (validation != null)\n                {\n                    out.printf(\"Partitioner: %s%n\", validation.partitioner);\n                    out.printf(\"Bloom Filter FP chance: %f%n\", validation.bloomFilterFPChance);\n                }\n                if (stats != null)\n                {\n                    out.printf(\"Minimum timestamp: %s%n\", stats.minTimestamp);\n                    out.printf(\"Maximum timestamp: %s%n\", stats.maxTimestamp);\n                    out.printf(\"SSTable max local deletion time: %s%n\", stats.maxLocalDeletionTime);\n                    out.printf(\"Compressor: %s%n\", compression != null ? compression.compressor().getClass().getName() : \"-\");\n                    if (compression != null)\n                        out.printf(\"Compression ratio: %s%n\", stats.compressionRatio);\n                    out.printf(\"Estimated droppable tombstones: %s%n\", stats.getEstimatedDroppableTombstoneRatio((int) (System.currentTimeMillis() / 1000)));\n                    out.printf(\"SSTable Level: %d%n\", stats.sstableLevel);\n                    out.printf(\"Repaired at: %d%n\", stats.repairedAt);\n                    out.println(stats.replayPosition);\n                    out.println(\"Estimated tombstone drop times:\");\n                    for (Map.Entry<Double, Long> entry : stats.estimatedTombstoneDropTime.getAsMap().entrySet())\n                    {\n                        out.printf(\"%-10s:%10s%n\",entry.getKey().intValue(), entry.getValue());\n                    }\n                    printHistograms(stats, out);\n                }\n                if (compaction != null)\n                {\n                    out.printf(\"Estimated cardinality: %s%n\", compaction.cardinalityEstimator.cardinality());\n                }\n            }\n            else\n            {\n                out.println(\"No such file: \" + fname);\n            }\n        }\n    }"
        ]
    ],
    "566799f567b319fdc62c94adfb8ffe4b96085649": [
        [
            "DatabaseDescriptor::getMemtableAllocatorPool()",
            "1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827 -\n1828  \n1829  \n1830  \n1831  ",
            "    public static MemtablePool getMemtableAllocatorPool()\n    {\n        long heapLimit = ((long) conf.memtable_heap_space_in_mb) << 20;\n        long offHeapLimit = ((long) conf.memtable_offheap_space_in_mb) << 20;\n        switch (conf.memtable_allocation_type)\n        {\n            case unslabbed_heap_buffers:\n                return new HeapPool(heapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case heap_buffers:\n                return new SlabPool(heapLimit, 0, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case offheap_buffers:\n                if (!FileUtils.isCleanerAvailable())\n                {\n                    throw new IllegalStateException(\"Could not free direct byte buffer: offheap_buffers is not a safe memtable_allocation_type without this ability, please adjust your config. This feature is only guaranteed to work on an Oracle JVM. Refusing to start.\");\n                }\n                return new SlabPool(heapLimit, offHeapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case offheap_objects:\n                return new NativePool(heapLimit, offHeapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            default:\n                throw new AssertionError();\n        }\n    }",
            "1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827 +\n1828 +\n1829  \n1830  \n1831  \n1832  ",
            "    public static MemtablePool getMemtableAllocatorPool()\n    {\n        long heapLimit = ((long) conf.memtable_heap_space_in_mb) << 20;\n        long offHeapLimit = ((long) conf.memtable_offheap_space_in_mb) << 20;\n        switch (conf.memtable_allocation_type)\n        {\n            case unslabbed_heap_buffers:\n                return new HeapPool(heapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case heap_buffers:\n                return new SlabPool(heapLimit, 0, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case offheap_buffers:\n                if (!FileUtils.isCleanerAvailable())\n                {\n                    throw new IllegalStateException(\"Could not free direct byte buffer: offheap_buffers is not a safe memtable_allocation_type without this ability, please adjust your config. This feature is only guaranteed to work on an Oracle JVM. Refusing to start.\");\n                }\n                return new SlabPool(heapLimit, offHeapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            case offheap_objects:\n                throw new ConfigurationException(\"offheap_objects are not available in 3.0. They should be re-introduced in a future release, see https://issues.apache.org/jira/browse/CASSANDRA-9472 for details\");\n                // return new NativePool(heapLimit, offHeapLimit, conf.memtable_cleanup_threshold, new ColumnFamilyStore.FlushLargestColumnFamily());\n            default:\n                throw new AssertionError();\n        }\n    }"
        ]
    ]
}