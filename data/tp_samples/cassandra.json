{
    "f56244d21a331cec7da5b751a4de9effad49952b": [
        [
            "UpdateParameters::getPrefetchedList(ByteBuffer,ColumnIdentifier)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101  ",
            "    public List<Cell> getPrefetchedList(ByteBuffer rowKey, ColumnIdentifier cql3ColumnName)\n    {\n        if (prefetchedLists == null)\n            return Collections.emptyList();\n\n        CQL3Row row = prefetchedLists.get(rowKey);\n        return row == null ? Collections.<Cell>emptyList() : row.getMultiCellColumn(cql3ColumnName);\n    }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105  ",
            "    public List<Cell> getPrefetchedList(ByteBuffer rowKey, ColumnIdentifier cql3ColumnName)\n    {\n        if (prefetchedLists == null)\n            return Collections.emptyList();\n\n        CQL3Row row = prefetchedLists.get(rowKey);\n        if (row == null)\n            return Collections.<Cell>emptyList();\n\n        List<Cell> cql3List = row.getMultiCellColumn(cql3ColumnName);\n        return (cql3List == null) ? Collections.<Cell>emptyList() : cql3List;\n    }"
        ],
        [
            "Lists::DiscarderByIndex::execute(ByteBuffer,ColumnFamily,Composite,UpdateParameters)",
            " 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503 -\n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "        public void execute(ByteBuffer rowKey, ColumnFamily cf, Composite prefix, UpdateParameters params) throws InvalidRequestException\n        {\n            assert column.type.isMultiCell() : \"Attempted to delete an item by index from a frozen list\";\n            Term.Terminal index = t.bind(params.options);\n            if (index == null)\n                throw new InvalidRequestException(\"Invalid null value for list index\");\n\n            List<Cell> existingList = params.getPrefetchedList(rowKey, column.name);\n            int idx = ByteBufferUtil.toInt(index.get(params.options));\n            if (existingList == null)\n                throw new InvalidRequestException(\"Attempted to delete an element from a list which is null\");\n            if (idx < 0 || idx >= existingList.size())\n                throw new InvalidRequestException(String.format(\"List index %d out of bound, list has size %d\", idx, existingList.size()));\n\n            CellName elementName = existingList.get(idx).name();\n            cf.addColumn(params.makeTombstone(elementName));\n        }",
            " 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503 +\n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "        public void execute(ByteBuffer rowKey, ColumnFamily cf, Composite prefix, UpdateParameters params) throws InvalidRequestException\n        {\n            assert column.type.isMultiCell() : \"Attempted to delete an item by index from a frozen list\";\n            Term.Terminal index = t.bind(params.options);\n            if (index == null)\n                throw new InvalidRequestException(\"Invalid null value for list index\");\n\n            List<Cell> existingList = params.getPrefetchedList(rowKey, column.name);\n            int idx = ByteBufferUtil.toInt(index.get(params.options));\n            if (existingList == null || existingList.size() == 0)\n                throw new InvalidRequestException(\"Attempted to delete an element from a list which is null\");\n            if (idx < 0 || idx >= existingList.size())\n                throw new InvalidRequestException(String.format(\"List index %d out of bound, list has size %d\", idx, existingList.size()));\n\n            CellName elementName = existingList.get(idx).name();\n            cf.addColumn(params.makeTombstone(elementName));\n        }"
        ],
        [
            "CollectionsTest::testLists()",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207 -\n 208 -\n 209  \n 210  \n 211  ",
            "    @Test\n    public void testLists() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, l list<text>)\");\n\n        execute(\"INSERT INTO %s(k, l) VALUES (0, ?)\", list(\"v1\", \"v2\", \"v3\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v2\", \"v3\")));\n\n        execute(\"DELETE l[?] FROM %s WHERE k = 0\", 1);\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v3\")));\n\n        execute(\"UPDATE %s SET l[?] = ? WHERE k = 0\", 1, \"v4\");\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v4\")));\n\n        // Full overwrite\n        execute(\"UPDATE %s SET l = ? WHERE k = 0\", list(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v6\", \"v5\")));\n\n        execute(\"UPDATE %s SET l = l + ? WHERE k = 0\", list(\"v7\", \"v8\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v6\", \"v5\", \"v7\", \"v8\")));\n\n        execute(\"UPDATE %s SET l = ? + l WHERE k = 0\", list(\"v9\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v9\", \"v6\", \"v5\", \"v7\", \"v8\")));\n\n        execute(\"UPDATE %s SET l = l - ? WHERE k = 0\", list(\"v5\", \"v8\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v9\", \"v6\", \"v7\")));\n\n        execute(\"DELETE l FROM %s WHERE k = 0\");\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row((Object) null));\n\n        assertInvalidMessage(\"Attempted to delete an element from a list which is null\",\n                             \"DELETE l[0] FROM %s WHERE k=0 \");\n\n        assertInvalidMessage(\"Attempted to set an element on a list which is null\",\n                             \"UPDATE %s SET l[0] = ? WHERE k=0\", list(\"v10\"));\n\n        assertInvalidMessage(\"Attempted to delete an element from a list which is null\",\n                             \"UPDATE %s SET l = l - ? WHERE k=0 \",\n                             list(\"v11\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row((Object) null));\n    }",
            " 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237  \n 238  \n 239  ",
            "    @Test\n    public void testLists() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, l list<text>)\");\n\n        execute(\"INSERT INTO %s(k, l) VALUES (0, ?)\", list(\"v1\", \"v2\", \"v3\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v2\", \"v3\")));\n\n        execute(\"DELETE l[?] FROM %s WHERE k = 0\", 1);\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v3\")));\n\n        execute(\"UPDATE %s SET l[?] = ? WHERE k = 0\", 1, \"v4\");\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v1\", \"v4\")));\n\n        // Full overwrite\n        execute(\"UPDATE %s SET l = ? WHERE k = 0\", list(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v6\", \"v5\")));\n\n        execute(\"UPDATE %s SET l = l + ? WHERE k = 0\", list(\"v7\", \"v8\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v6\", \"v5\", \"v7\", \"v8\")));\n\n        execute(\"UPDATE %s SET l = ? + l WHERE k = 0\", list(\"v9\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v9\", \"v6\", \"v5\", \"v7\", \"v8\")));\n\n        execute(\"UPDATE %s SET l = l - ? WHERE k = 0\", list(\"v5\", \"v8\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row(list(\"v9\", \"v6\", \"v7\")));\n\n        execute(\"DELETE l FROM %s WHERE k = 0\");\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row((Object) null));\n\n        assertInvalidMessage(\"Attempted to delete an element from a list which is null\",\n                             \"DELETE l[0] FROM %s WHERE k=0 \");\n\n        assertInvalidMessage(\"Attempted to set an element on a list which is null\",\n                             \"UPDATE %s SET l[0] = ? WHERE k=0\", list(\"v10\"));\n\n        execute(\"UPDATE %s SET l = l - ? WHERE k=0 \", list(\"v11\"));\n\n        assertRows(execute(\"SELECT l FROM %s WHERE k = 0\"), row((Object) null));\n    }"
        ],
        [
            "Lists::SetterByIndex::execute(ByteBuffer,ColumnFamily,Composite,UpdateParameters)",
            " 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 -\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  ",
            "        public void execute(ByteBuffer rowKey, ColumnFamily cf, Composite prefix, UpdateParameters params) throws InvalidRequestException\n        {\n            // we should not get here for frozen lists\n            assert column.type.isMultiCell() : \"Attempted to set an individual element on a frozen list\";\n\n            ByteBuffer index = idx.bindAndGet(params.options);\n            ByteBuffer value = t.bindAndGet(params.options);\n\n            if (index == null)\n                throw new InvalidRequestException(\"Invalid null value for list index\");\n\n            List<Cell> existingList = params.getPrefetchedList(rowKey, column.name);\n            int idx = ByteBufferUtil.toInt(index);\n            if (existingList == null)\n                throw new InvalidRequestException(\"Attempted to set an element on a list which is null\");\n            if (idx < 0 || idx >= existingList.size())\n                throw new InvalidRequestException(String.format(\"List index %d out of bound, list has size %d\", idx, existingList.size()));\n\n            CellName elementName = existingList.get(idx).name();\n            if (value == null)\n            {\n                cf.addColumn(params.makeTombstone(elementName));\n            }\n            else\n            {\n                // We don't support value > 64K because the serialization format encode the length as an unsigned short.\n                if (value.remaining() > FBUtilities.MAX_UNSIGNED_SHORT)\n                    throw new InvalidRequestException(String.format(\"List value is too long. List values are limited to %d bytes but %d bytes value provided\",\n                                                                    FBUtilities.MAX_UNSIGNED_SHORT,\n                                                                    value.remaining()));\n\n                cf.addColumn(params.makeColumn(elementName, value));\n            }\n        }",
            " 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 +\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  ",
            "        public void execute(ByteBuffer rowKey, ColumnFamily cf, Composite prefix, UpdateParameters params) throws InvalidRequestException\n        {\n            // we should not get here for frozen lists\n            assert column.type.isMultiCell() : \"Attempted to set an individual element on a frozen list\";\n\n            ByteBuffer index = idx.bindAndGet(params.options);\n            ByteBuffer value = t.bindAndGet(params.options);\n\n            if (index == null)\n                throw new InvalidRequestException(\"Invalid null value for list index\");\n\n            List<Cell> existingList = params.getPrefetchedList(rowKey, column.name);\n            int idx = ByteBufferUtil.toInt(index);\n            if (existingList == null || existingList.size() == 0)\n                throw new InvalidRequestException(\"Attempted to set an element on a list which is null\");\n            if (idx < 0 || idx >= existingList.size())\n                throw new InvalidRequestException(String.format(\"List index %d out of bound, list has size %d\", idx, existingList.size()));\n\n            CellName elementName = existingList.get(idx).name();\n            if (value == null)\n            {\n                cf.addColumn(params.makeTombstone(elementName));\n            }\n            else\n            {\n                // We don't support value > 64K because the serialization format encode the length as an unsigned short.\n                if (value.remaining() > FBUtilities.MAX_UNSIGNED_SHORT)\n                    throw new InvalidRequestException(String.format(\"List value is too long. List values are limited to %d bytes but %d bytes value provided\",\n                                                                    FBUtilities.MAX_UNSIGNED_SHORT,\n                                                                    value.remaining()));\n\n                cf.addColumn(params.makeColumn(elementName, value));\n            }\n        }"
        ],
        [
            "CollectionsTest::testMaps()",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "    @Test\n    public void testMaps() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, m map<text, int>)\");\n\n        execute(\"INSERT INTO %s(k, m) VALUES (0, ?)\", map(\"v1\", 1, \"v2\", 2));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v1\", 1, \"v2\", 2))\n        );\n\n        execute(\"UPDATE %s SET m[?] = ?, m[?] = ? WHERE k = 0\", \"v3\", 3, \"v4\", 4);\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v1\", 1, \"v2\", 2, \"v3\", 3, \"v4\", 4))\n        );\n\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v1\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v2\", 2, \"v3\", 3, \"v4\", 4))\n        );\n\n        // Full overwrite\n        execute(\"UPDATE %s SET m = ? WHERE k = 0\", map(\"v6\", 6, \"v5\", 5));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5, \"v6\", 6))\n        );\n\n        execute(\"UPDATE %s SET m = m + ? WHERE k = 0\", map(\"v7\", 7));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5, \"v6\", 6, \"v7\", 7))\n        );\n\n        // The empty map is parsed as an empty set (because we don't have enough info at parsing\n        // time when we see a {}) and special cased later. This test checks this work properly\n        execute(\"UPDATE %s SET m = {} WHERE k = 0\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row((Object)null)",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159 +\n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "    @Test\n    public void testMaps() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, m map<text, int>)\");\n\n        execute(\"INSERT INTO %s(k, m) VALUES (0, ?)\", map(\"v1\", 1, \"v2\", 2));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v1\", 1, \"v2\", 2))\n        );\n\n        execute(\"UPDATE %s SET m[?] = ?, m[?] = ? WHERE k = 0\", \"v3\", 3, \"v4\", 4);\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v1\", 1, \"v2\", 2, \"v3\", 3, \"v4\", 4))\n        );\n\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v1\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v2\", 2, \"v3\", 3, \"v4\", 4))\n        );\n\n        // Full overwrite\n        execute(\"UPDATE %s SET m = ? WHERE k = 0\", map(\"v6\", 6, \"v5\", 5));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5, \"v6\", 6))\n        );\n\n        execute(\"UPDATE %s SET m = m + ? WHERE k = 0\", map(\"v7\", 7));\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5, \"v6\", 6, \"v7\", 7))\n        );\n\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v7\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5, \"v6\", 6))\n        );\n\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v6\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row(map(\"v5\", 5))\n        );\n\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v5\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row((Object)null)\n        );\n\n        // Deleting a non-existing key should succeed\n        execute(\"DELETE m[?] FROM %s WHERE k = 0\", \"v5\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row((Object) null)\n        );\n\n        // The empty map is parsed as an empty set (because we don't have enough info at parsing\n        // time when we see a {}) and special cased later. This test checks this work properly\n        execute(\"UPDATE %s SET m = {} WHERE k = 0\");\n\n        assertRows(execute(\"SELECT m FROM %s WHERE k = 0\"),\n            row((Object)null)"
        ],
        [
            "CollectionsTest::testSets()",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "    @Test\n    public void testSets() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, s set<text>)\");\n\n        execute(\"INSERT INTO %s(k, s) VALUES (0, ?)\", set(\"v1\", \"v2\", \"v3\", \"v4\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v1\", \"v2\", \"v3\", \"v4\"))\n        );\n\n        execute(\"DELETE s[?] FROM %s WHERE k = 0\", \"v1\");\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v2\", \"v3\", \"v4\"))\n        );\n\n        // Full overwrite\n        execute(\"UPDATE %s SET s = ? WHERE k = 0\", set(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v5\", \"v6\"))\n        );\n\n        execute(\"UPDATE %s SET s = s + ? WHERE k = 0\", set(\"v7\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v5\", \"v6\", \"v7\"))\n        );\n\n        execute(\"UPDATE %s SET s = s - ? WHERE k = 0\", set(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v7\"))\n        );\n\n        execute(\"DELETE s FROM %s WHERE k = 0\");\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row((Object)null)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115  \n 116  \n 117  \n 118  ",
            "    @Test\n    public void testSets() throws Throwable\n    {\n        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, s set<text>)\");\n\n        execute(\"INSERT INTO %s(k, s) VALUES (0, ?)\", set(\"v1\", \"v2\", \"v3\", \"v4\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v1\", \"v2\", \"v3\", \"v4\"))\n        );\n\n        execute(\"DELETE s[?] FROM %s WHERE k = 0\", \"v1\");\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v2\", \"v3\", \"v4\"))\n        );\n\n        // Full overwrite\n        execute(\"UPDATE %s SET s = ? WHERE k = 0\", set(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v5\", \"v6\"))\n        );\n\n        execute(\"UPDATE %s SET s = s + ? WHERE k = 0\", set(\"v7\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v5\", \"v6\", \"v7\"))\n        );\n\n        execute(\"UPDATE %s SET s = s - ? WHERE k = 0\", set(\"v6\", \"v5\"));\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row(set(\"v7\"))\n        );\n\n        execute(\"DELETE s[?] FROM %s WHERE k = 0\", set(\"v7\"));\n\n        // Deleting an element that does not exist will succeed\n        execute(\"DELETE s[?] FROM %s WHERE k = 0\", set(\"v7\"));\n\n        execute(\"DELETE s FROM %s WHERE k = 0\");\n\n        assertRows(execute(\"SELECT s FROM %s WHERE k = 0\"),\n            row((Object)null)"
        ]
    ],
    "9b6f55bdec6d9b7c08d7cae267b2fefbf60d7afc": [
        [
            "SnapshotTask::run()",
            "  44  \n  45  \n  46 -\n  47  ",
            "    public void run()\n    {\n        MessagingService.instance().sendRRWithFailure(new SnapshotMessage(desc).createMessage(),\n                endpoint,",
            "  45  \n  46  \n  47 +\n  48  \n  49 +\n  50  ",
            "    public void run()\n    {\n        MessagingService.instance().sendRR(new SnapshotMessage(desc).createMessage(),\n                endpoint,\n                new SnapshotCallback(this), TimeUnit.HOURS.toMillis(1), true);\n    }"
        ],
        [
            "ActiveRepairService::prepareForRepair(Set,Collection,List)",
            " 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 -\n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "    public synchronized UUID prepareForRepair(Set<InetAddress> endpoints, Collection<Range<Token>> ranges, List<ColumnFamilyStore> columnFamilyStores)\n    {\n        UUID parentRepairSession = UUIDGen.getTimeUUID();\n        registerParentRepairSession(parentRepairSession, columnFamilyStores, ranges);\n        final CountDownLatch prepareLatch = new CountDownLatch(endpoints.size());\n        final AtomicBoolean status = new AtomicBoolean(true);\n        final Set<String> failedNodes = Collections.synchronizedSet(new HashSet<String>());\n        IAsyncCallbackWithFailure callback = new IAsyncCallbackWithFailure()\n        {\n            public void response(MessageIn msg)\n            {\n                prepareLatch.countDown();\n            }\n\n            public boolean isLatencyForSnitch()\n            {\n                return false;\n            }\n\n            public void onFailure(InetAddress from)\n            {\n                status.set(false);\n                failedNodes.add(from.getHostAddress());\n                prepareLatch.countDown();\n            }\n        };\n\n        List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());\n        for (ColumnFamilyStore cfs : columnFamilyStores)\n            cfIds.add(cfs.metadata.cfId);\n\n        for(InetAddress neighbour : endpoints)\n        {\n            PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);\n            MessageOut<RepairMessage> msg = message.createMessage();\n            MessagingService.instance().sendRRWithFailure(msg, neighbour, callback);\n        }\n        try\n        {\n            prepareLatch.await(1, TimeUnit.HOURS);\n        }\n        catch (InterruptedException e)\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString(), e);\n        }\n\n        if (!status.get())\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get positive replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString());\n        }\n\n        return parentRepairSession;\n    }",
            " 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 +\n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "    public synchronized UUID prepareForRepair(Set<InetAddress> endpoints, Collection<Range<Token>> ranges, List<ColumnFamilyStore> columnFamilyStores)\n    {\n        UUID parentRepairSession = UUIDGen.getTimeUUID();\n        registerParentRepairSession(parentRepairSession, columnFamilyStores, ranges);\n        final CountDownLatch prepareLatch = new CountDownLatch(endpoints.size());\n        final AtomicBoolean status = new AtomicBoolean(true);\n        final Set<String> failedNodes = Collections.synchronizedSet(new HashSet<String>());\n        IAsyncCallbackWithFailure callback = new IAsyncCallbackWithFailure()\n        {\n            public void response(MessageIn msg)\n            {\n                prepareLatch.countDown();\n            }\n\n            public boolean isLatencyForSnitch()\n            {\n                return false;\n            }\n\n            public void onFailure(InetAddress from)\n            {\n                status.set(false);\n                failedNodes.add(from.getHostAddress());\n                prepareLatch.countDown();\n            }\n        };\n\n        List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());\n        for (ColumnFamilyStore cfs : columnFamilyStores)\n            cfIds.add(cfs.metadata.cfId);\n\n        for(InetAddress neighbour : endpoints)\n        {\n            PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);\n            MessageOut<RepairMessage> msg = message.createMessage();\n            MessagingService.instance().sendRR(msg, neighbour, callback, TimeUnit.HOURS.toMillis(1), true);\n        }\n        try\n        {\n            prepareLatch.await(1, TimeUnit.HOURS);\n        }\n        catch (InterruptedException e)\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString(), e);\n        }\n\n        if (!status.get())\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get positive replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString());\n        }\n\n        return parentRepairSession;\n    }"
        ]
    ],
    "9dd847135ffdde384b48bdb8a147b27d8bb8b6a4": [
        [
            "AntiCompactionTest::shouldSkipAntiCompactionForNonIntersectingRange()",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "    @Test\n    public void shouldSkipAntiCompactionForNonIntersectingRange() throws InterruptedException, ExecutionException, IOException\n    {\n        ColumnFamilyStore store = prepareColumnFamilyStore();\n        Collection<SSTableReader> sstables = store.getUnrepairedSSTables();\n        assertEquals(store.getSSTables().size(), sstables.size());\n        Range<Token> range = new Range<Token>(new BytesToken(\"-10\".getBytes()), new BytesToken(\"-1\".getBytes()));\n        List<Range<Token>> ranges = Arrays.asList(range);\n\n        Refs<SSTableReader> refs = Refs.tryRef(sstables);\n        if (refs == null)\n            throw new IllegalStateException();\n        CompactionManager.instance.performAnticompaction(store, ranges, refs, 1);\n        assertThat(store.getSSTables().size(), is(1));\n        assertThat(Iterables.get(store.getSSTables(), 0).isRepaired(), is(false));\n        assertThat(Iterables.get(store.getSSTables(), 0).selfRef().globalCount(), is(1));\n        assertThat(store.getDataTracker().getCompacting().size(), is(0));\n    }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "    @Test\n    public void shouldSkipAntiCompactionForNonIntersectingRange() throws InterruptedException, ExecutionException, IOException\n    {\n        ColumnFamilyStore store = prepareColumnFamilyStore();\n        Collection<SSTableReader> sstables = store.getUnrepairedSSTables();\n        assertEquals(store.getSSTables().size(), sstables.size());\n        Range<Token> range = new Range<Token>(new BytesToken(\"-1\".getBytes()), new BytesToken(\"-10\".getBytes()));\n        List<Range<Token>> ranges = Arrays.asList(range);\n\n        Refs<SSTableReader> refs = Refs.tryRef(sstables);\n        if (refs == null)\n            throw new IllegalStateException();\n        CompactionManager.instance.performAnticompaction(store, ranges, refs, 1);\n        assertThat(store.getSSTables().size(), is(1));\n        assertThat(Iterables.get(store.getSSTables(), 0).isRepaired(), is(false));\n        assertThat(Iterables.get(store.getSSTables(), 0).selfRef().globalCount(), is(1));\n        assertThat(store.getDataTracker().getCompacting().size(), is(0));\n    }"
        ],
        [
            "CompactionManager::performAnticompaction(ColumnFamilyStore,Collection,Refs,long)",
            " 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447  \n 448 -\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 -\n 459 -\n 460 -\n 461 -\n 462 -\n 463 -\n 464 -\n 465 -\n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException, ExecutionException, IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      long repairedAt) throws InterruptedException, ExecutionException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.debug(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n                for (Range<Token> r : Range.normalize(ranges))\n                {\n                    Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken(), sstable.partitioner);\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        sstableIterator.remove();\n                        break;\n                    }\n                    else if (!sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) does not intersect repaired range {}, not touching repairedAt.\", sstable, sstableRange, r);\n                        nonAnticompacting.add(sstable);\n                        sstableIterator.remove();\n                        break;\n                    }\n                    else\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                    }\n                }\n            }\n            cfs.getDataTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatuses);\n            cfs.getDataTracker().unmarkCompacting(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, sstables, repairedAt);\n        }\n        finally\n        {\n            validatedForRepair.release();\n            cfs.getDataTracker().unmarkCompacting(sstables);\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }",
            " 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 +\n 444 +\n 445  \n 446  \n 447  \n 448 +\n 449 +\n 450 +\n 451 +\n 452 +\n 453 +\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462 +\n 463  \n 464  \n 465 +\n 466  \n 467  \n 468 +\n 469  \n 470  \n 471 +\n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException, ExecutionException, IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      long repairedAt) throws InterruptedException, ExecutionException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.debug(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken(), sstable.partitioner);\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            cfs.getDataTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatuses);\n            cfs.getDataTracker().unmarkCompacting(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, sstables, repairedAt);\n        }\n        finally\n        {\n            validatedForRepair.release();\n            cfs.getDataTracker().unmarkCompacting(sstables);\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }"
        ]
    ],
    "bd46463fbb7d6b0998c837450ce61df13eda041d": [
        [
            "CassandraDaemon::activate()",
            " 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536 -\n 537  \n 538 -\n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  ",
            "    /**\n     * A convenience method to initialize and start the daemon in one shot.\n     */\n    public void activate()\n    {\n        String pidFile = System.getProperty(\"cassandra-pidfile\");\n\n        if (FBUtilities.isWindows())\n        {\n            // We need to adjust the system timer on windows from the default 15ms down to the minimum of 1ms as this\n            // impacts timer intervals, thread scheduling, driver interrupts, etc.\n            WindowsTimer.startTimerPeriod(DatabaseDescriptor.getWindowsTimerInterval());\n        }\n\n        try\n        {\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                mbs.registerMBean(new StandardMBean(new NativeAccess(), NativeAccessMBean.class), new ObjectName(MBEAN_NAME));\n            }\n            catch (Exception e)\n            {\n                logger.error(\"error registering MBean {}\", MBEAN_NAME, e);\n                //Allow the server to start even if the bean can't be registered\n            }\n\n            try {\n                DatabaseDescriptor.forceStaticInitialization();\n            } catch (ExceptionInInitializerError e) {\n                throw e.getCause();\n            }\n\n            setup();\n\n            if (pidFile != null)\n            {\n                new File(pidFile).deleteOnExit();\n            }\n\n            if (System.getProperty(\"cassandra-foreground\") == null)\n            {\n                System.out.close();\n                System.err.close();\n            }\n\n            start();\n        }\n        catch (Throwable e)\n        {\n            boolean logStackTrace =\n                    e instanceof ConfigurationException ? ((ConfigurationException)e).logStackTrace : true;\n\n            System.out.println(\"Exception (\" + e.getClass().getName() + \") encountered during startup: \" + e.getMessage());\n\n            if (logStackTrace)\n            {\n                if (runManaged)\n                    logger.error(\"Exception encountered during startup\", e);\n                // try to warn user on stdout too, if we haven't already detached\n                e.printStackTrace();\n                exitOrFail(3, \"Exception encountered during startup\", e);\n            }\n            else\n            {\n                if (runManaged)\n                    logger.error(\"Exception encountered during startup: {}\", e.getMessage());\n                // try to warn user on stdout too, if we haven't already detached\n                System.err.println(e.getMessage());\n                exitOrFail(3, \"Exception encountered during startup: \" + e.getMessage());\n            }\n        }\n    }",
            " 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536 +\n 537 +\n 538  \n 539 +\n 540 +\n 541 +\n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  ",
            "    /**\n     * A convenience method to initialize and start the daemon in one shot.\n     */\n    public void activate()\n    {\n        String pidFile = System.getProperty(\"cassandra-pidfile\");\n\n        if (FBUtilities.isWindows())\n        {\n            // We need to adjust the system timer on windows from the default 15ms down to the minimum of 1ms as this\n            // impacts timer intervals, thread scheduling, driver interrupts, etc.\n            WindowsTimer.startTimerPeriod(DatabaseDescriptor.getWindowsTimerInterval());\n        }\n\n        try\n        {\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                mbs.registerMBean(new StandardMBean(new NativeAccess(), NativeAccessMBean.class), new ObjectName(MBEAN_NAME));\n            }\n            catch (Exception e)\n            {\n                logger.error(\"error registering MBean {}\", MBEAN_NAME, e);\n                //Allow the server to start even if the bean can't be registered\n            }\n\n            try\n            {\n                DatabaseDescriptor.forceStaticInitialization();\n            }\n            catch (ExceptionInInitializerError e)\n            {\n                throw e.getCause();\n            }\n\n            setup();\n\n            if (pidFile != null)\n            {\n                new File(pidFile).deleteOnExit();\n            }\n\n            if (System.getProperty(\"cassandra-foreground\") == null)\n            {\n                System.out.close();\n                System.err.close();\n            }\n\n            start();\n        }\n        catch (Throwable e)\n        {\n            boolean logStackTrace =\n                    e instanceof ConfigurationException ? ((ConfigurationException)e).logStackTrace : true;\n\n            System.out.println(\"Exception (\" + e.getClass().getName() + \") encountered during startup: \" + e.getMessage());\n\n            if (logStackTrace)\n            {\n                if (runManaged)\n                    logger.error(\"Exception encountered during startup\", e);\n                // try to warn user on stdout too, if we haven't already detached\n                e.printStackTrace();\n                exitOrFail(3, \"Exception encountered during startup\", e);\n            }\n            else\n            {\n                if (runManaged)\n                    logger.error(\"Exception encountered during startup: {}\", e.getMessage());\n                // try to warn user on stdout too, if we haven't already detached\n                System.err.println(e.getMessage());\n                exitOrFail(3, \"Exception encountered during startup: \" + e.getMessage());\n            }\n        }\n    }"
        ],
        [
            "CassandraDaemon::exitOrFail(int,String,Throwable)",
            " 675 -\n 676 -\n 677 -\n 678 -\n 679 -\n 680 -\n 681 -\n 682 -\n 683 -\n 684 -\n 685  ",
            "    private void exitOrFail(int code, String message, Throwable cause) {\n            if(runManaged) {\n                RuntimeException t = cause!=null ? new RuntimeException(message, cause) : new RuntimeException(message);\n                throw t;\n            }\n            else {\n                logger.error(message, cause);\n                System.exit(code);\n            }\n\n        }",
            " 680 +\n 681 +\n 682 +\n 683 +\n 684 +\n 685 +\n 686  \n 687 +\n 688 +\n 689 +\n 690 +\n 691 +\n 692 +",
            "    private void exitOrFail(int code, String message, Throwable cause)\n    {\n        if (runManaged)\n        {\n            RuntimeException t = cause!=null ? new RuntimeException(message, cause) : new RuntimeException(message);\n            throw t;\n        }\n        else\n        {\n            logger.error(message, cause);\n            System.exit(code);\n        }\n    }"
        ],
        [
            "CassandraDaemon::deactivate()",
            " 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611 -\n 612  \n 613  \n 614  ",
            "    /**\n     * A convenience method to stop and destroy the daemon in one shot.\n     */\n    public void deactivate()\n    {\n        stop();\n        destroy();\n        // completely shut down cassandra\n        if(!runManaged) {\n            System.exit(0);\n        }\n    }",
            " 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614 +\n 615 +\n 616  \n 617  \n 618  ",
            "    /**\n     * A convenience method to stop and destroy the daemon in one shot.\n     */\n    public void deactivate()\n    {\n        stop();\n        destroy();\n        // completely shut down cassandra\n        if(!runManaged)\n        {\n            System.exit(0);\n        }\n    }"
        ],
        [
            "CassandraDaemon::setup()",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 -\n 325 -\n 326 -\n 327 -\n 328 -\n 329 -\n 330 -\n 331 -\n 332 -\n 333 -\n 334 -\n 335 -\n 336 -\n 337 -\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "    /**\n     * This is a hook for concrete daemons to initialize themselves suitably.\n     *\n     * Subclasses should override this to finish the job (listening on ports, etc.)\n     */\n    protected void setup()\n    {\n        // Delete any failed snapshot deletions on Windows - see CASSANDRA-9658\n        if (FBUtilities.isWindows())\n            WindowsFailedSnapshotTracker.deleteOldSnapshots();\n\n        ThreadAwareSecurityManager.install();\n\n        logSystemInfo();\n\n        CLibrary.tryMlockall();\n\n        try\n        {\n            startupChecks.verify();\n        }\n        catch (StartupException e)\n        {\n            exitOrFail(e.returnCode, e.getMessage(), e.getCause());\n        }\n\n        try\n        {\n            if (SystemKeyspace.snapshotOnVersionChange())\n            {\n                SystemKeyspace.migrateDataDirs();\n            }\n        }\n        catch (IOException e)\n        {\n            exitOrFail(3, e.getMessage(), e.getCause());\n        }\n\n        maybeInitJmx();\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                StorageMetrics.exceptions.inc();\n                logger.error(\"Exception in thread {}\", t, e);\n                Tracing.trace(\"Exception in thread {}\", t, e);\n                for (Throwable e2 = e; e2 != null; e2 = e2.getCause())\n                {\n                    JVMStabilityInspector.inspectThrowable(e2);\n\n                    if (e2 instanceof FSError)\n                    {\n                        if (e2 != e) // make sure FSError gets logged exactly once.\n                            logger.error(\"Exception in thread {}\", t, e2);\n                        FileUtils.handleFSError((FSError) e2);\n                    }\n\n                    if (e2 instanceof CorruptSSTableException)\n                    {\n                        if (e2 != e)\n                            logger.error(\"Exception in thread \" + t, e2);\n                        FileUtils.handleCorruptSSTable((CorruptSSTableException) e2);\n                    }\n                }\n            }\n        });\n\n        /*\n         * Migrate pre-3.0 keyspaces, tables, types, functions, and aggregates, to their new 3.0 storage.\n         * We don't (and can't) wait for commit log replay here, but we don't need to - all schema changes force\n         * explicit memtable flushes.\n         */\n        LegacySchemaMigrator.migrate();\n\n        StorageService.instance.populateTokenMetadata();\n\n        // load schema from disk\n        Schema.instance.loadFromDisk();\n\n        // clean up debris in the rest of the keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspaceName.equals(SystemKeyspace.NAME))\n                continue;\n\n            for (CFMetaData cfm : Schema.instance.getTablesAndViews(keyspaceName))\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n        }\n\n        Keyspace.setInitialized();\n        // initialize keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"opening keyspace {}\", keyspaceName);\n            // disable auto compaction until commit log replay ends\n            for (ColumnFamilyStore cfs : Keyspace.open(keyspaceName).getColumnFamilyStores())\n            {\n                for (ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    store.disableAutoCompaction();\n                }\n            }\n        }\n\n\n        try\n        {\n            loadRowAndKeyCacheAsync().get();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Error loading key or row cache\", t);\n        }\n\n        try\n        {\n            GCInspector.register();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Unable to start GCInspector (currently only supported on the Sun JVM)\");\n        }\n\n        // replay the log if necessary\n        try\n        {\n            CommitLog.instance.recover();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        // migrate any legacy (pre-3.0) hints from system.hints table into the new store\n        new LegacyHintsMigrator(DatabaseDescriptor.getHintsDirectory(), DatabaseDescriptor.getMaxHintsFileSize()).migrate();\n\n        // migrate any legacy (pre-3.0) batch entries from system.batchlog to system.batches (new table format)\n        LegacyBatchlogMigrator.migrate();\n\n        // enable auto compaction\n        for (Keyspace keyspace : Keyspace.all())\n        {\n            for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())\n            {\n                for (final ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    if (store.getCompactionStrategyManager().shouldBeEnabled())\n                        store.enableAutoCompaction();\n                }\n            }\n        }\n\n        Runnable viewRebuild = new Runnable()\n        {\n            @Override\n            public void run()\n            {\n                for (Keyspace keyspace : Keyspace.all())\n                {\n                    keyspace.viewManager.buildAllViews();\n                }\n            }\n        };\n\n        ScheduledExecutors.optionalTasks.schedule(viewRebuild, StorageService.RING_DELAY, TimeUnit.MILLISECONDS);\n\n\n        SystemKeyspace.finishStartup();\n\n        // start server internals\n        StorageService.instance.registerDaemon(this);\n        try\n        {\n            StorageService.instance.initServer();\n        }\n        catch (ConfigurationException e)\n        {\n            System.err.println(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\");\n            exitOrFail(1, \"Fatal configuration error\", e);\n        }\n\n        Mx4jTool.maybeLoad();\n\n        // Metrics\n        String metricsReporterConfigFile = System.getProperty(\"cassandra.metricsReporterConfigFile\");\n        if (metricsReporterConfigFile != null)\n        {\n            logger.info(\"Trying to load metrics-reporter-config from file: {}\", metricsReporterConfigFile);\n            try\n            {\n                String reportFileLocation = CassandraDaemon.class.getClassLoader().getResource(metricsReporterConfigFile).getFile();\n                ReporterConfig.loadFromFile(reportFileLocation).enableAll(CassandraMetricsRegistry.Metrics);\n            }\n            catch (Exception e)\n            {\n                logger.warn(\"Failed to load metrics-reporter-config, metric sinks will not be activated\", e);\n            }\n        }\n\n        if (!FBUtilities.getBroadcastAddress().equals(InetAddress.getLoopbackAddress()))\n            waitForGossipToSettle();\n\n        // schedule periodic background compaction task submission. this is simply a backstop against compactions stalling\n        // due to scheduling errors or race conditions\n        ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(ColumnFamilyStore.getBackgroundCompactionTaskSubmitter(), 5, 1, TimeUnit.MINUTES);\n\n        // schedule periodic dumps of table size estimates into SystemKeyspace.SIZE_ESTIMATES_CF\n        // set cassandra.size_recorder_interval to 0 to disable\n        int sizeRecorderInterval = Integer.getInteger(\"cassandra.size_recorder_interval\", 5 * 60);\n        if (sizeRecorderInterval > 0)\n            ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(SizeEstimatesRecorder.instance, 30, sizeRecorderInterval, TimeUnit.SECONDS);\n\n        // Thrift\n        InetAddress rpcAddr = DatabaseDescriptor.getRpcAddress();\n        int rpcPort = DatabaseDescriptor.getRpcPort();\n        int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();\n        thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);\n\n        // Native transport\n        nativeTransportService = new NativeTransportService();\n\n        completeSetup();\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340 +\n 341 +\n 342 +\n 343 +\n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349 +\n 350 +\n 351 +\n 352 +\n 353 +\n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "    /**\n     * This is a hook for concrete daemons to initialize themselves suitably.\n     *\n     * Subclasses should override this to finish the job (listening on ports, etc.)\n     */\n    protected void setup()\n    {\n        // Delete any failed snapshot deletions on Windows - see CASSANDRA-9658\n        if (FBUtilities.isWindows())\n            WindowsFailedSnapshotTracker.deleteOldSnapshots();\n\n        ThreadAwareSecurityManager.install();\n\n        logSystemInfo();\n\n        CLibrary.tryMlockall();\n\n        try\n        {\n            startupChecks.verify();\n        }\n        catch (StartupException e)\n        {\n            exitOrFail(e.returnCode, e.getMessage(), e.getCause());\n        }\n\n        try\n        {\n            if (SystemKeyspace.snapshotOnVersionChange())\n            {\n                SystemKeyspace.migrateDataDirs();\n            }\n        }\n        catch (IOException e)\n        {\n            exitOrFail(3, e.getMessage(), e.getCause());\n        }\n\n        maybeInitJmx();\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                StorageMetrics.exceptions.inc();\n                logger.error(\"Exception in thread {}\", t, e);\n                Tracing.trace(\"Exception in thread {}\", t, e);\n                for (Throwable e2 = e; e2 != null; e2 = e2.getCause())\n                {\n                    JVMStabilityInspector.inspectThrowable(e2);\n\n                    if (e2 instanceof FSError)\n                    {\n                        if (e2 != e) // make sure FSError gets logged exactly once.\n                            logger.error(\"Exception in thread {}\", t, e2);\n                        FileUtils.handleFSError((FSError) e2);\n                    }\n\n                    if (e2 instanceof CorruptSSTableException)\n                    {\n                        if (e2 != e)\n                            logger.error(\"Exception in thread \" + t, e2);\n                        FileUtils.handleCorruptSSTable((CorruptSSTableException) e2);\n                    }\n                }\n            }\n        });\n\n        /*\n         * Migrate pre-3.0 keyspaces, tables, types, functions, and aggregates, to their new 3.0 storage.\n         * We don't (and can't) wait for commit log replay here, but we don't need to - all schema changes force\n         * explicit memtable flushes.\n         */\n        LegacySchemaMigrator.migrate();\n\n        StorageService.instance.populateTokenMetadata();\n\n        // load schema from disk\n        Schema.instance.loadFromDisk();\n\n        // clean up debris in the rest of the keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspaceName.equals(SystemKeyspace.NAME))\n                continue;\n\n            for (CFMetaData cfm : Schema.instance.getTablesAndViews(keyspaceName))\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n        }\n\n        Keyspace.setInitialized();\n        // initialize keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"opening keyspace {}\", keyspaceName);\n            // disable auto compaction until commit log replay ends\n            for (ColumnFamilyStore cfs : Keyspace.open(keyspaceName).getColumnFamilyStores())\n            {\n                for (ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    store.disableAutoCompaction();\n                }\n            }\n        }\n\n\n        try\n        {\n            loadRowAndKeyCacheAsync().get();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Error loading key or row cache\", t);\n        }\n\n        try\n        {\n            GCInspector.register();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Unable to start GCInspector (currently only supported on the Sun JVM)\");\n        }\n\n        // replay the log if necessary\n        try\n        {\n            CommitLog.instance.recover();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        // migrate any legacy (pre-3.0) hints from system.hints table into the new store\n        new LegacyHintsMigrator(DatabaseDescriptor.getHintsDirectory(), DatabaseDescriptor.getMaxHintsFileSize()).migrate();\n\n        // migrate any legacy (pre-3.0) batch entries from system.batchlog to system.batches (new table format)\n        LegacyBatchlogMigrator.migrate();\n\n        // enable auto compaction\n        for (Keyspace keyspace : Keyspace.all())\n        {\n            for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())\n            {\n                for (final ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    if (store.getCompactionStrategyManager().shouldBeEnabled())\n                        store.enableAutoCompaction();\n                }\n            }\n        }\n\n        Runnable viewRebuild = new Runnable()\n        {\n            @Override\n            public void run()\n            {\n                for (Keyspace keyspace : Keyspace.all())\n                {\n                    keyspace.viewManager.buildAllViews();\n                }\n            }\n        };\n\n        ScheduledExecutors.optionalTasks.schedule(viewRebuild, StorageService.RING_DELAY, TimeUnit.MILLISECONDS);\n\n\n        SystemKeyspace.finishStartup();\n\n        // Metrics\n        String metricsReporterConfigFile = System.getProperty(\"cassandra.metricsReporterConfigFile\");\n        if (metricsReporterConfigFile != null)\n        {\n            logger.info(\"Trying to load metrics-reporter-config from file: {}\", metricsReporterConfigFile);\n            try\n            {\n                String reportFileLocation = CassandraDaemon.class.getClassLoader().getResource(metricsReporterConfigFile).getFile();\n                ReporterConfig.loadFromFile(reportFileLocation).enableAll(CassandraMetricsRegistry.Metrics);\n            }\n            catch (Exception e)\n            {\n                logger.warn(\"Failed to load metrics-reporter-config, metric sinks will not be activated\", e);\n            }\n        }\n\n        // start server internals\n        StorageService.instance.registerDaemon(this);\n        try\n        {\n            StorageService.instance.initServer();\n        }\n        catch (ConfigurationException e)\n        {\n            System.err.println(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\");\n            exitOrFail(1, \"Fatal configuration error\", e);\n        }\n\n        Mx4jTool.maybeLoad();\n\n        if (!FBUtilities.getBroadcastAddress().equals(InetAddress.getLoopbackAddress()))\n            waitForGossipToSettle();\n\n        // schedule periodic background compaction task submission. this is simply a backstop against compactions stalling\n        // due to scheduling errors or race conditions\n        ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(ColumnFamilyStore.getBackgroundCompactionTaskSubmitter(), 5, 1, TimeUnit.MINUTES);\n\n        // schedule periodic dumps of table size estimates into SystemKeyspace.SIZE_ESTIMATES_CF\n        // set cassandra.size_recorder_interval to 0 to disable\n        int sizeRecorderInterval = Integer.getInteger(\"cassandra.size_recorder_interval\", 5 * 60);\n        if (sizeRecorderInterval > 0)\n            ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(SizeEstimatesRecorder.instance, 30, sizeRecorderInterval, TimeUnit.SECONDS);\n\n        // Thrift\n        InetAddress rpcAddr = DatabaseDescriptor.getRpcAddress();\n        int rpcPort = DatabaseDescriptor.getRpcPort();\n        int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();\n        thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);\n\n        // Native transport\n        nativeTransportService = new NativeTransportService();\n\n        completeSetup();\n    }"
        ],
        [
            "CassandraDaemon::exitOrFail(int,String)",
            " 671 -\n 672  \n 673  ",
            "    private void exitOrFail(int code, String message) {\n        exitOrFail(code, message, null);\n    }",
            " 675 +\n 676 +\n 677  \n 678  ",
            "    private void exitOrFail(int code, String message)\n    {\n        exitOrFail(code, message, null);\n    }"
        ]
    ],
    "d766f4fb20af4914b54420e22af0de909eb180ed": [
        [
            "CQLSSTableWriterTest::testForbidCounterUpdates()",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  ",
            "    @Test(expected = IllegalArgumentException.class)\n    public void testForbidCounterUpdates() throws Exception\n    {\n        String KS = \"cql_keyspace\";\n        String TABLE = \"counter1\";\n\n        File tempdir = Files.createTempDir();\n        File dataDir = new File(tempdir.getAbsolutePath() + File.separator + KS + File.separator + TABLE);\n        assert dataDir.mkdirs();\n\n        String schema = \"CREATE TABLE cql_keyspace.counter1 (\" +\n                        \"  my_id int, \" +\n                        \"  my_counter counter, \" +\n                        \"  PRIMARY KEY (my_id)\" +\n                        \")\";\n        String insert = String.format(\"UPDATE cql_keyspace.counter1 SET my_counter = my_counter - ? WHERE my_id = ?\");\n        CQLSSTableWriter.builder().inDirectory(dataDir)\n                        .forTable(schema)\n                        .withPartitioner(StorageService.instance.getPartitioner())\n                        .using(insert).build();\n    }",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  ",
            "    @Test(expected = IllegalArgumentException.class)\n    public void testForbidCounterUpdates() throws Exception\n    {\n        String KS = \"cql_keyspace\";\n        String TABLE = \"counter1\";\n\n        File tempdir = Files.createTempDir();\n        File dataDir = new File(tempdir.getAbsolutePath() + File.separator + KS + File.separator + TABLE);\n        assert dataDir.mkdirs();\n\n        String schema = \"CREATE TABLE cql_keyspace.counter1 (\" +\n                        \"  my_id int, \" +\n                        \"  my_counter counter, \" +\n                        \"  PRIMARY KEY (my_id)\" +\n                        \")\";\n        String insert = String.format(\"UPDATE cql_keyspace.counter1 SET my_counter = my_counter - ? WHERE my_id = ?\");\n        CQLSSTableWriter.builder().inDirectory(dataDir)\n                        .forTable(schema)\n                        .withPartitioner(Murmur3Partitioner.instance)\n                        .using(insert).build();\n    }"
        ]
    ],
    "ba926ff6d8c09834d5c45f4eae8d75d9051b1058": [
        [
            "JavaDriverClient::connect(ProtocolOptions)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123 -\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "    public void connect(ProtocolOptions.Compression compression) throws Exception\n    {\n\n        PoolingOptions poolingOpts = new PoolingOptions()\n                                     .setConnectionsPerHost(HostDistance.LOCAL, connectionsPerHost, connectionsPerHost)\n                                     .setMaxRequestsPerConnection(HostDistance.LOCAL, maxPendingPerConnection)\n                                     .setNewConnectionThreshold(HostDistance.LOCAL, 100);\n\n        Cluster.Builder clusterBuilder = Cluster.builder()\n                                                .addContactPoint(host)\n                                                .withPort(port)\n                                                .withPoolingOptions(poolingOpts)\n                                                .withoutJMXReporting()\n                                                .withProtocolVersion(protocolVersion)\n                                                .withoutMetrics(); // The driver uses metrics 3 with conflict with our version\n        if (whitelist != null)\n            clusterBuilder.withLoadBalancingPolicy(whitelist);\n        clusterBuilder.withCompression(compression);\n        if (encryptionOptions.enabled)\n        {\n            SSLContext sslContext;\n            sslContext = SSLFactory.createSSLContext(encryptionOptions, true);\n            SSLOptions sslOptions = JdkSSLOptions.builder()\n                                                 .withSSLContext(sslContext)\n                                                 .withCipherSuites(encryptionOptions.cipher_suites).build();\n            clusterBuilder.withSSL(sslOptions);\n        }\n\n        if (authProvider != null)\n        {\n            clusterBuilder.withAuthProvider(authProvider);\n        }\n        else if (username != null)\n        {\n            clusterBuilder.withCredentials(username, password);\n        }\n\n        cluster = clusterBuilder.build();\n        Metadata metadata = cluster.getMetadata();\n        System.out.printf(\n                \"Connected to cluster: %s, max pending requests per connection %d, max connections per host %d%n\",\n                metadata.getClusterName(),\n                maxPendingPerConnection,\n                connectionsPerHost);\n        for (Host host : metadata.getAllHosts())\n        {\n            System.out.printf(\"Datatacenter: %s; Host: %s; Rack: %s%n\",\n                    host.getDatacenter(), host.getAddress(), host.getRack());\n        }\n\n        session = cluster.connect();\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    public void connect(ProtocolOptions.Compression compression) throws Exception\n    {\n\n        PoolingOptions poolingOpts = new PoolingOptions()\n                                     .setConnectionsPerHost(HostDistance.LOCAL, connectionsPerHost, connectionsPerHost)\n                                     .setMaxRequestsPerConnection(HostDistance.LOCAL, maxPendingPerConnection)\n                                     .setNewConnectionThreshold(HostDistance.LOCAL, 100);\n\n        Cluster.Builder clusterBuilder = Cluster.builder()\n                                                .addContactPoint(host)\n                                                .withPort(port)\n                                                .withPoolingOptions(poolingOpts)\n                                                .withoutJMXReporting()\n                                                .withProtocolVersion(protocolVersion)\n                                                .withoutMetrics(); // The driver uses metrics 3 with conflict with our version\n        if (loadBalancingPolicy != null)\n            clusterBuilder.withLoadBalancingPolicy(loadBalancingPolicy);\n        clusterBuilder.withCompression(compression);\n        if (encryptionOptions.enabled)\n        {\n            SSLContext sslContext;\n            sslContext = SSLFactory.createSSLContext(encryptionOptions, true);\n            SSLOptions sslOptions = JdkSSLOptions.builder()\n                                                 .withSSLContext(sslContext)\n                                                 .withCipherSuites(encryptionOptions.cipher_suites).build();\n            clusterBuilder.withSSL(sslOptions);\n        }\n\n        if (authProvider != null)\n        {\n            clusterBuilder.withAuthProvider(authProvider);\n        }\n        else if (username != null)\n        {\n            clusterBuilder.withCredentials(username, password);\n        }\n\n        cluster = clusterBuilder.build();\n        Metadata metadata = cluster.getMetadata();\n        System.out.printf(\n                \"Connected to cluster: %s, max pending requests per connection %d, max connections per host %d%n\",\n                metadata.getClusterName(),\n                maxPendingPerConnection,\n                connectionsPerHost);\n        for (Host host : metadata.getAllHosts())\n        {\n            System.out.printf(\"Datatacenter: %s; Host: %s; Rack: %s%n\",\n                    host.getDatacenter(), host.getAddress(), host.getRack());\n        }\n\n        session = cluster.connect();\n    }"
        ],
        [
            "SettingsNode::Options::options()",
            " 142  \n 143  \n 144  \n 145 -\n 146  ",
            "        @Override\n        public List<? extends Option> options()\n        {\n            return Arrays.asList(whitelist, file, list);\n        }",
            " 148  \n 149  \n 150  \n 151 +\n 152  ",
            "        @Override\n        public List<? extends Option> options()\n        {\n            return Arrays.asList(datacenter, whitelist, file, list);\n        }"
        ],
        [
            "JavaDriverClient::JavaDriverClient(StressSettings,String,int,EncryptionOptions)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  ",
            "    public JavaDriverClient(StressSettings settings, String host, int port, EncryptionOptions.ClientEncryptionOptions encryptionOptions)\n    {\n        this.protocolVersion = settings.mode.protocolVersion;\n        this.host = host;\n        this.port = port;\n        this.username = settings.mode.username;\n        this.password = settings.mode.password;\n        this.authProvider = settings.mode.authProvider;\n        this.encryptionOptions = encryptionOptions;\n        if (settings.node.isWhiteList)\n            whitelist = new WhiteListPolicy(DCAwareRoundRobinPolicy.builder().build(), settings.node.resolveAll(settings.port.nativePort));\n        else\n            whitelist = null;\n        connectionsPerHost = settings.mode.connectionsPerHost == null ? 8 : settings.mode.connectionsPerHost;\n\n        int maxThreadCount = 0;\n        if (settings.rate.auto)\n            maxThreadCount = settings.rate.maxThreads;\n        else\n            maxThreadCount = settings.rate.threadCount;\n\n        //Always allow enough pending requests so every thread can have a request pending\n        //See https://issues.apache.org/jira/browse/CASSANDRA-7217\n        int requestsPerConnection = (maxThreadCount / connectionsPerHost) + connectionsPerHost;\n\n        maxPendingPerConnection = settings.mode.maxPendingPerConnection == null ? Math.max(128, requestsPerConnection ) : settings.mode.maxPendingPerConnection;\n    }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78  \n  79 +\n  80 +\n  81 +\n  82  \n  83 +\n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "    public JavaDriverClient(StressSettings settings, String host, int port, EncryptionOptions.ClientEncryptionOptions encryptionOptions)\n    {\n        this.protocolVersion = settings.mode.protocolVersion;\n        this.host = host;\n        this.port = port;\n        this.username = settings.mode.username;\n        this.password = settings.mode.password;\n        this.authProvider = settings.mode.authProvider;\n        this.encryptionOptions = encryptionOptions;\n\n        DCAwareRoundRobinPolicy.Builder policyBuilder = DCAwareRoundRobinPolicy.builder();\n        if (settings.node.datacenter != null)\n            policyBuilder.withLocalDc(settings.node.datacenter);\n\n        if (settings.node.isWhiteList)\n            loadBalancingPolicy = new WhiteListPolicy(policyBuilder.build(), settings.node.resolveAll(settings.port.nativePort));\n        else if (settings.node.datacenter != null)\n            loadBalancingPolicy = policyBuilder.build();\n        else\n            loadBalancingPolicy = null;\n\n        connectionsPerHost = settings.mode.connectionsPerHost == null ? 8 : settings.mode.connectionsPerHost;\n\n        int maxThreadCount = 0;\n        if (settings.rate.auto)\n            maxThreadCount = settings.rate.maxThreads;\n        else\n            maxThreadCount = settings.rate.threadCount;\n\n        //Always allow enough pending requests so every thread can have a request pending\n        //See https://issues.apache.org/jira/browse/CASSANDRA-7217\n        int requestsPerConnection = (maxThreadCount / connectionsPerHost) + connectionsPerHost;\n\n        maxPendingPerConnection = settings.mode.maxPendingPerConnection == null ? Math.max(128, requestsPerConnection ) : settings.mode.maxPendingPerConnection;\n    }"
        ],
        [
            "SettingsNode::SettingsNode(Options)",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "    public SettingsNode(Options options)\n    {\n        if (options.file.setByUser())\n        {\n            try\n            {\n                String node;\n                List<String> tmpNodes = new ArrayList<String>();\n                BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(options.file.value())));\n                try\n                {\n                    while ((node = in.readLine()) != null)\n                    {\n                        if (node.length() > 0)\n                            tmpNodes.add(node);\n                    }\n                    nodes = Arrays.asList(tmpNodes.toArray(new String[tmpNodes.size()]));\n                }\n                finally\n                {\n                    in.close();\n                }\n            }\n            catch(IOException ioe)\n            {\n                throw new RuntimeException(ioe);\n            }\n\n        }\n        else\n            nodes = Arrays.asList(options.list.value().split(\",\"));\n        isWhiteList = options.whitelist.setByUser();\n    }",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70 +\n  71 +\n  72  \n  73 +\n  74  ",
            "    public SettingsNode(Options options)\n    {\n        if (options.file.setByUser())\n        {\n            try\n            {\n                String node;\n                List<String> tmpNodes = new ArrayList<String>();\n                BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(options.file.value())));\n                try\n                {\n                    while ((node = in.readLine()) != null)\n                    {\n                        if (node.length() > 0)\n                            tmpNodes.add(node);\n                    }\n                    nodes = Arrays.asList(tmpNodes.toArray(new String[tmpNodes.size()]));\n                }\n                finally\n                {\n                    in.close();\n                }\n            }\n            catch(IOException ioe)\n            {\n                throw new RuntimeException(ioe);\n            }\n\n        }\n        else\n        {\n            nodes = Arrays.asList(options.list.value().split(\",\"));\n        }\n\n        isWhiteList = options.whitelist.setByUser();\n        datacenter = options.datacenter.value();\n    }"
        ]
    ],
    "3cda6829ccbbfaac08092773b55c9ee315dd5bbd": [
        [
            "CassandraDaemon::setup()",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  ",
            "    /**\n     * This is a hook for concrete daemons to initialize themselves suitably.\n     *\n     * Subclasses should override this to finish the job (listening on ports, etc.)\n     */\n    protected void setup()\n    {\n        FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());\n\n        // Delete any failed snapshot deletions on Windows - see CASSANDRA-9658\n        if (FBUtilities.isWindows())\n            WindowsFailedSnapshotTracker.deleteOldSnapshots();\n\n        logSystemInfo();\n\n        CLibrary.tryMlockall();\n\n        try\n        {\n            startupChecks.verify();\n        }\n        catch (StartupException e)\n        {\n            exitOrFail(e.returnCode, e.getMessage(), e.getCause());\n        }\n\n        try\n        {\n            SystemKeyspace.snapshotOnVersionChange();\n        }\n        catch (IOException e)\n        {\n            exitOrFail(3, e.getMessage(), e.getCause());\n        }\n\n        // We need to persist this as soon as possible after startup checks.\n        // This should be the first write to SystemKeyspace (CASSANDRA-11742)\n        SystemKeyspace.persistLocalMetadata();\n\n        maybeInitJmx();\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                StorageMetrics.exceptions.inc();\n                logger.error(\"Exception in thread {}\", t, e);\n                Tracing.trace(\"Exception in thread {}\", t, e);\n                for (Throwable e2 = e; e2 != null; e2 = e2.getCause())\n                {\n                    JVMStabilityInspector.inspectThrowable(e2);\n\n                    if (e2 instanceof FSError)\n                    {\n                        if (e2 != e) // make sure FSError gets logged exactly once.\n                            logger.error(\"Exception in thread {}\", t, e2);\n                        FileUtils.handleFSError((FSError) e2);\n                    }\n\n                    if (e2 instanceof CorruptSSTableException)\n                    {\n                        if (e2 != e)\n                            logger.error(\"Exception in thread \" + t, e2);\n                        FileUtils.handleCorruptSSTable((CorruptSSTableException) e2);\n                    }\n                }\n            }\n        });\n\n        // load schema from disk\n        Schema.instance.loadFromDisk();\n\n        // clean up compaction leftovers\n        Map<Pair<String, String>, Map<Integer, UUID>> unfinishedCompactions = SystemKeyspace.getUnfinishedCompactions();\n        for (Pair<String, String> kscf : unfinishedCompactions.keySet())\n        {\n            CFMetaData cfm = Schema.instance.getCFMetaData(kscf.left, kscf.right);\n            // CFMetaData can be null if CF is already dropped\n            if (cfm != null)\n                ColumnFamilyStore.removeUnfinishedCompactionLeftovers(cfm, unfinishedCompactions.get(kscf));\n        }\n        SystemKeyspace.discardCompactionsInProgress();\n\n        // clean up debris in the rest of the keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspaceName.equals(SystemKeyspace.NAME))\n                continue;\n\n            for (CFMetaData cfm : Schema.instance.getKeyspaceMetaData(keyspaceName).values())\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n        }\n\n        Keyspace.setInitialized();\n\n        // initialize keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"opening keyspace {}\", keyspaceName);\n            // disable auto compaction until commit log replay ends\n            for (ColumnFamilyStore cfs : Keyspace.open(keyspaceName).getColumnFamilyStores())\n            {\n                for (ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    store.disableAutoCompaction();\n                }\n            }\n        }\n\n\n        try\n        {\n            loadRowAndKeyCacheAsync().get();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Error loading key or row cache\", t);\n        }\n\n        try\n        {\n            GCInspector.register();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Unable to start GCInspector (currently only supported on the Sun JVM)\");\n        }\n\n        // replay the log if necessary\n        try\n        {\n            CommitLog.instance.recover();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        // enable auto compaction\n        for (Keyspace keyspace : Keyspace.all())\n        {\n            for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())\n            {\n                for (final ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    if (store.getCompactionStrategy().shouldBeEnabled())\n                        store.enableAutoCompaction();\n                }\n            }\n        }\n\n        SystemKeyspace.finishStartup();\n\n        // start server internals\n        StorageService.instance.registerDaemon(this);\n        try\n        {\n            StorageService.instance.initServer();\n        }\n        catch (ConfigurationException e)\n        {\n            System.err.println(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\");\n            exitOrFail(1, \"Fatal configuration error\", e);\n        }\n\n        Mx4jTool.maybeLoad();\n\n        // Metrics\n        String metricsReporterConfigFile = System.getProperty(\"cassandra.metricsReporterConfigFile\");\n        if (metricsReporterConfigFile != null)\n        {\n            logger.info(\"Trying to load metrics-reporter-config from file: {}\", metricsReporterConfigFile);\n            try\n            {\n                String reportFileLocation = CassandraDaemon.class.getClassLoader().getResource(metricsReporterConfigFile).getFile();\n                ReporterConfig.loadFromFile(reportFileLocation).enableAll(CassandraMetricsRegistry.Metrics);\n            }\n            catch (Exception e)\n            {\n                logger.warn(\"Failed to load metrics-reporter-config, metric sinks will not be activated\", e);\n            }\n        }\n\n        if (!FBUtilities.getBroadcastAddress().equals(InetAddress.getLoopbackAddress()))\n            waitForGossipToSettle();\n\n        // schedule periodic background compaction task submission. this is simply a backstop against compactions stalling\n        // due to scheduling errors or race conditions\n        ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(ColumnFamilyStore.getBackgroundCompactionTaskSubmitter(), 5, 1, TimeUnit.MINUTES);\n\n        // schedule periodic dumps of table size estimates into SystemKeyspace.SIZE_ESTIMATES_CF\n        // set cassandra.size_recorder_interval to 0 to disable\n        int sizeRecorderInterval = Integer.getInteger(\"cassandra.size_recorder_interval\", 5 * 60);\n        if (sizeRecorderInterval > 0)\n            ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(SizeEstimatesRecorder.instance, 30, sizeRecorderInterval, TimeUnit.SECONDS);\n\n        // Thrift\n        InetAddress rpcAddr = DatabaseDescriptor.getRpcAddress();\n        int rpcPort = DatabaseDescriptor.getRpcPort();\n        int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();\n        thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);\n\n        // Native transport\n        InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();\n        int nativePort = DatabaseDescriptor.getNativeTransportPort();\n        nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);\n\n        completeSetup();\n    }",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327 +\n 328 +\n 329 +\n 330 +\n 331 +\n 332 +\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  ",
            "    /**\n     * This is a hook for concrete daemons to initialize themselves suitably.\n     *\n     * Subclasses should override this to finish the job (listening on ports, etc.)\n     */\n    protected void setup()\n    {\n        FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());\n\n        // Delete any failed snapshot deletions on Windows - see CASSANDRA-9658\n        if (FBUtilities.isWindows())\n            WindowsFailedSnapshotTracker.deleteOldSnapshots();\n\n        logSystemInfo();\n\n        CLibrary.tryMlockall();\n\n        try\n        {\n            startupChecks.verify();\n        }\n        catch (StartupException e)\n        {\n            exitOrFail(e.returnCode, e.getMessage(), e.getCause());\n        }\n\n        try\n        {\n            SystemKeyspace.snapshotOnVersionChange();\n        }\n        catch (IOException e)\n        {\n            exitOrFail(3, e.getMessage(), e.getCause());\n        }\n\n        // We need to persist this as soon as possible after startup checks.\n        // This should be the first write to SystemKeyspace (CASSANDRA-11742)\n        SystemKeyspace.persistLocalMetadata();\n\n        maybeInitJmx();\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                StorageMetrics.exceptions.inc();\n                logger.error(\"Exception in thread {}\", t, e);\n                Tracing.trace(\"Exception in thread {}\", t, e);\n                for (Throwable e2 = e; e2 != null; e2 = e2.getCause())\n                {\n                    JVMStabilityInspector.inspectThrowable(e2);\n\n                    if (e2 instanceof FSError)\n                    {\n                        if (e2 != e) // make sure FSError gets logged exactly once.\n                            logger.error(\"Exception in thread {}\", t, e2);\n                        FileUtils.handleFSError((FSError) e2);\n                    }\n\n                    if (e2 instanceof CorruptSSTableException)\n                    {\n                        if (e2 != e)\n                            logger.error(\"Exception in thread \" + t, e2);\n                        FileUtils.handleCorruptSSTable((CorruptSSTableException) e2);\n                    }\n                }\n            }\n        });\n\n        // load schema from disk\n        Schema.instance.loadFromDisk();\n\n        // clean up compaction leftovers\n        Map<Pair<String, String>, Map<Integer, UUID>> unfinishedCompactions = SystemKeyspace.getUnfinishedCompactions();\n        for (Pair<String, String> kscf : unfinishedCompactions.keySet())\n        {\n            CFMetaData cfm = Schema.instance.getCFMetaData(kscf.left, kscf.right);\n            // CFMetaData can be null if CF is already dropped\n            if (cfm != null)\n                ColumnFamilyStore.removeUnfinishedCompactionLeftovers(cfm, unfinishedCompactions.get(kscf));\n        }\n        SystemKeyspace.discardCompactionsInProgress();\n\n        // clean up debris in the rest of the keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            // Skip system as we've already cleaned it\n            if (keyspaceName.equals(SystemKeyspace.NAME))\n                continue;\n\n            for (CFMetaData cfm : Schema.instance.getKeyspaceMetaData(keyspaceName).values())\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n        }\n\n        Keyspace.setInitialized();\n\n        // initialize keyspaces\n        for (String keyspaceName : Schema.instance.getKeyspaces())\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"opening keyspace {}\", keyspaceName);\n            // disable auto compaction until commit log replay ends\n            for (ColumnFamilyStore cfs : Keyspace.open(keyspaceName).getColumnFamilyStores())\n            {\n                for (ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    store.disableAutoCompaction();\n                }\n            }\n        }\n\n\n        try\n        {\n            loadRowAndKeyCacheAsync().get();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Error loading key or row cache\", t);\n        }\n\n        try\n        {\n            GCInspector.register();\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            logger.warn(\"Unable to start GCInspector (currently only supported on the Sun JVM)\");\n        }\n\n        // replay the log if necessary\n        try\n        {\n            CommitLog.instance.recover();\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        // enable auto compaction\n        for (Keyspace keyspace : Keyspace.all())\n        {\n            for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())\n            {\n                for (final ColumnFamilyStore store : cfs.concatWithIndexes())\n                {\n                    if (store.getCompactionStrategy().shouldBeEnabled())\n                        store.enableAutoCompaction();\n                }\n            }\n        }\n\n        SystemKeyspace.finishStartup();\n\n        // start server internals\n        StorageService.instance.registerDaemon(this);\n        try\n        {\n            StorageService.instance.initServer();\n        }\n        catch (ConfigurationException e)\n        {\n            System.err.println(e.getMessage() + \"\\nFatal configuration error; unable to start server.  See log for stacktrace.\");\n            exitOrFail(1, \"Fatal configuration error\", e);\n        }\n\n        Mx4jTool.maybeLoad();\n\n        // Metrics\n        String metricsReporterConfigFile = System.getProperty(\"cassandra.metricsReporterConfigFile\");\n        if (metricsReporterConfigFile != null)\n        {\n            logger.info(\"Trying to load metrics-reporter-config from file: {}\", metricsReporterConfigFile);\n            try\n            {\n                // enable metrics provided by metrics-jvm.jar\n                CassandraMetricsRegistry.Metrics.register(\"jvm.buffers.\", new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\n                CassandraMetricsRegistry.Metrics.register(\"jvm.gc.\", new GarbageCollectorMetricSet());\n                CassandraMetricsRegistry.Metrics.register(\"jvm.memory.\", new MemoryUsageGaugeSet());\n                CassandraMetricsRegistry.Metrics.register(\"jvm.fd.usage\", new FileDescriptorRatioGauge());\n                // initialize metrics-reporter-config from yaml file\n                String reportFileLocation = CassandraDaemon.class.getClassLoader().getResource(metricsReporterConfigFile).getFile();\n                ReporterConfig.loadFromFile(reportFileLocation).enableAll(CassandraMetricsRegistry.Metrics);\n            }\n            catch (Exception e)\n            {\n                logger.warn(\"Failed to load metrics-reporter-config, metric sinks will not be activated\", e);\n            }\n        }\n\n        if (!FBUtilities.getBroadcastAddress().equals(InetAddress.getLoopbackAddress()))\n            waitForGossipToSettle();\n\n        // schedule periodic background compaction task submission. this is simply a backstop against compactions stalling\n        // due to scheduling errors or race conditions\n        ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(ColumnFamilyStore.getBackgroundCompactionTaskSubmitter(), 5, 1, TimeUnit.MINUTES);\n\n        // schedule periodic dumps of table size estimates into SystemKeyspace.SIZE_ESTIMATES_CF\n        // set cassandra.size_recorder_interval to 0 to disable\n        int sizeRecorderInterval = Integer.getInteger(\"cassandra.size_recorder_interval\", 5 * 60);\n        if (sizeRecorderInterval > 0)\n            ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(SizeEstimatesRecorder.instance, 30, sizeRecorderInterval, TimeUnit.SECONDS);\n\n        // Thrift\n        InetAddress rpcAddr = DatabaseDescriptor.getRpcAddress();\n        int rpcPort = DatabaseDescriptor.getRpcPort();\n        int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();\n        thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);\n\n        // Native transport\n        InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();\n        int nativePort = DatabaseDescriptor.getNativeTransportPort();\n        nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);\n\n        completeSetup();\n    }"
        ]
    ],
    "acd46ab7c4e185e474026a0bfecdd2c5e77bf46d": [
        [
            "RepairRunnable::runMayThrow()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118 -\n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 3 + options.getRanges().size(); // calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace,\n                                                                                                columnFamilies);\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        Map<Range, Set<InetAddress>> rangeToNeighbors = new HashMap<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                    Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges,\n                                                                                  range, options.getDataCenters(),\n                                                                                  options.getHosts());\n                    rangeToNeighbors.put(range, neighbors);\n                    allNeighbors.addAll(neighbors);\n            }\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options.getRanges());\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Range<Token> range : options.getRanges())\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              range,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              rangeToNeighbors.get(range),\n                                                              repairedAt,\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRange().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRange().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results) throws Exception\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.add(sessionResult.range);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 +\n 116  \n 117  \n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 4 + options.getRanges().size(); // get valid column families, calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies;\n        try\n        {\n            validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace, columnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        Map<Range, Set<InetAddress>> rangeToNeighbors = new HashMap<>();\n\n        //pre-calculate output of getLocalRanges and pass it to getNeighbors to increase performance and prevent\n        //calculation multiple times\n        Collection<Range<Token>> keyspaceLocalRanges = storageService.getLocalRanges(keyspace);\n\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                    Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, keyspaceLocalRanges,\n                                                                                  range, options.getDataCenters(),\n                                                                                  options.getHosts());\n                    rangeToNeighbors.put(range, neighbors);\n                    allNeighbors.addAll(neighbors);\n            }\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options.getRanges());\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Range<Token> range : options.getRanges())\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              range,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              rangeToNeighbors.get(range),\n                                                              repairedAt,\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRange().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRange().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results) throws Exception\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.add(sessionResult.range);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }"
        ],
        [
            "RepairRunnable::fireErrorAndComplete(String,int,int,String)",
            " 102  \n 103  \n 104  \n 105 -\n 106  ",
            "    protected void fireErrorAndComplete(String tag, int progressCount, int totalProgress, String message)\n    {\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, message));\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress));\n    }",
            " 102  \n 103  \n 104  \n 105 +\n 106  ",
            "    protected void fireErrorAndComplete(String tag, int progressCount, int totalProgress, String message)\n    {\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progressCount, totalProgress, message));\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progressCount, totalProgress, String.format(\"Repair command #%d finished with error\", cmd)));\n    }"
        ]
    ],
    "adbe2cc4df0134955a2c83ae4ebd0086ea5e9164": [
        [
            "TTLExpiryTest::defineSchema()",
            "  57  \n  58  \n  59  \n  60 -\n  61 -\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    CFMetaData.Builder.create(KEYSPACE1, CF_STANDARD1)\n                                                      .addPartitionKey(\"pKey\", AsciiType.instance)\n                                                      .addRegularColumn(\"col1\", AsciiType.instance)\n                                                      .addRegularColumn(\"col\", AsciiType.instance)\n                                                      .addRegularColumn(\"col311\", AsciiType.instance)\n                                                      .addRegularColumn(\"col2\", AsciiType.instance)\n                                                      .addRegularColumn(\"col3\", AsciiType.instance)\n                                                      .addRegularColumn(\"col7\", AsciiType.instance)\n                                                      .addRegularColumn(\"col8\", MapType.getInstance(AsciiType.instance, AsciiType.instance, true))\n                                                      .addRegularColumn(\"shadow\", AsciiType.instance)\n                                                      .build().gcGraceSeconds(0));\n    }",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 +\n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    CFMetaData.Builder.create(KEYSPACE1, CF_STANDARD1)\n                                                      .addPartitionKey(\"pKey\", AsciiType.instance)\n                                                      .addRegularColumn(\"col1\", AsciiType.instance)\n                                                      .addRegularColumn(\"col\", AsciiType.instance)\n                                                      .addRegularColumn(\"col311\", AsciiType.instance)\n                                                      .addRegularColumn(\"col2\", AsciiType.instance)\n                                                      .addRegularColumn(\"col3\", AsciiType.instance)\n                                                      .addRegularColumn(\"col7\", AsciiType.instance)\n                                                      .addRegularColumn(\"col8\", MapType.getInstance(AsciiType.instance, AsciiType.instance, true))\n                                                      .addRegularColumn(\"shadow\", AsciiType.instance)\n                                                      .build().gcGraceSeconds(0));\n    }"
        ],
        [
            "CompactionsTest::defineSchema()",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        Map<String, String> compactionOptions = new HashMap<>();\n        compactionOptions.put(\"tombstone_compaction_interval\", \"1\");\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.denseCFMD(KEYSPACE1, CF_DENSE1)\n                                                .compaction(CompactionParams.scts(compactionOptions)),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1)\n                                                .compaction(CompactionParams.scts(compactionOptions)),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD2),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD3),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD4),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPER1, AsciiType.instance),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPER5, AsciiType.instance),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPERGC, AsciiType.instance)\n                                                .gcGraceSeconds(0));\n    }",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        Map<String, String> compactionOptions = new HashMap<>();\n        compactionOptions.put(\"tombstone_compaction_interval\", \"1\");\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.denseCFMD(KEYSPACE1, CF_DENSE1)\n                                                .compaction(CompactionParams.scts(compactionOptions)),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1)\n                                                .compaction(CompactionParams.scts(compactionOptions)),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD2),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD3),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD4),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPER1, AsciiType.instance),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPER5, AsciiType.instance),\n                                    SchemaLoader.superCFMD(KEYSPACE1, CF_SUPERGC, AsciiType.instance)\n                                                .gcGraceSeconds(0));\n    }"
        ],
        [
            "DateTieredCompactionStrategyTest::defineSchema()",
            "  53  \n  54  \n  55  \n  56 -\n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                KeyspaceParams.simple(1),\n                SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61  \n  62  \n  63  \n  64  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                KeyspaceParams.simple(1),\n                SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }"
        ],
        [
            "TimeWindowCompactionStrategyTest::defineSchema()",
            "  61  \n  62  \n  63  \n  64 -\n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69  \n  70  \n  71  \n  72  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }"
        ],
        [
            "StreamingHistogram::equals(Object)",
            " 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 -\n 275  ",
            "    @Override\n    public boolean equals(Object o)\n    {\n        if (this == o)\n            return true;\n\n        if (!(o instanceof StreamingHistogram))\n            return false;\n\n        StreamingHistogram that = (StreamingHistogram) o;\n        return maxBinSize == that.maxBinSize && bin.equals(that.bin);\n    }",
            " 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 +\n 275 +\n 276 +\n 277  ",
            "    @Override\n    public boolean equals(Object o)\n    {\n        if (this == o)\n            return true;\n\n        if (!(o instanceof StreamingHistogram))\n            return false;\n\n        StreamingHistogram that = (StreamingHistogram) o;\n        return maxBinSize == that.maxBinSize\n               && spool.equals(that.spool)\n               && bin.equals(that.bin);\n    }"
        ],
        [
            "LeveledCompactionStrategyTest::defineSchema()",
            "  73  \n  74  \n  75  \n  76 -\n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARDDLEVELED)\n                                                .compaction(CompactionParams.lcs(Collections.singletonMap(\"sstable_size_in_mb\", \"1\"))));\n        }",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80 +\n  81  \n  82  \n  83  \n  84  \n  85  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARDDLEVELED)\n                                                .compaction(CompactionParams.lcs(Collections.singletonMap(\"sstable_size_in_mb\", \"1\"))));\n        }"
        ],
        [
            "SizeTieredCompactionStrategyTest::defineSchema()",
            "  53  \n  54  \n  55  \n  56 -\n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        SchemaLoader.prepareServer();\n\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61  \n  62  \n  63  \n  64  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        // Disable tombstone histogram rounding for tests\n        System.setProperty(\"cassandra.streaminghistogram.roundseconds\", \"1\");\n\n        SchemaLoader.prepareServer();\n\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARD1));\n    }"
        ]
    ],
    "4ef0bf8861ee12f5c7e826298c42fe3ddb29c198": [
        [
            "LocalSessions::handleStatusRequest(InetAddress,StatusRequest)",
            " 657  \n 658  \n 659 -\n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670 -\n 671  ",
            "    public void handleStatusRequest(InetAddress from, StatusRequest request)\n    {\n        logger.debug(\"received {} from {}\", request, from);\n        UUID sessionID = request.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Received status response message for unknown session {}\", sessionID);\n            sendMessage(from, new StatusResponse(sessionID, FAILED));\n        }\n        else\n        {\n            sendMessage(from, new StatusResponse(sessionID, session.getState()));\n        }\n    }",
            " 660  \n 661  \n 662 +\n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 +\n 674 +\n 675  ",
            "    public void handleStatusRequest(InetAddress from, StatusRequest request)\n    {\n        logger.trace(\"received {} from {}\", request, from);\n        UUID sessionID = request.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Received status response message for unknown session {}\", sessionID);\n            sendMessage(from, new StatusResponse(sessionID, FAILED));\n        }\n        else\n        {\n            sendMessage(from, new StatusResponse(sessionID, session.getState()));\n            logger.debug(\"Responding to status response message for incremental repair session {} with local state {}\", sessionID, session.getState());\n       }\n    }"
        ],
        [
            "LocalSessions::deleteSession(UUID)",
            " 500  \n 501  \n 502 -\n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "    public synchronized void deleteSession(UUID sessionID)\n    {\n        logger.debug(\"deleting session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        Preconditions.checkArgument(session.isCompleted(), \"Cannot delete incomplete sessions\");\n\n        deleteRow(sessionID);\n        removeSession(sessionID);\n    }",
            " 500  \n 501  \n 502 +\n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "    public synchronized void deleteSession(UUID sessionID)\n    {\n        logger.debug(\"Deleting local repair session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        Preconditions.checkArgument(session.isCompleted(), \"Cannot delete incomplete sessions\");\n\n        deleteRow(sessionID);\n        removeSession(sessionID);\n    }"
        ],
        [
            "CoordinatorSession::sendMessage(InetAddress,RepairMessage)",
            " 131  \n 132  \n 133  \n 134  \n 135  ",
            "    protected void sendMessage(InetAddress destination, RepairMessage message)\n    {\n        MessageOut<RepairMessage> messageOut = new MessageOut<RepairMessage>(MessagingService.Verb.REPAIR_MESSAGE, message, RepairMessage.serializer);\n        MessagingService.instance().sendOneWay(messageOut, destination);\n    }",
            " 131  \n 132  \n 133 +\n 134  \n 135  \n 136  ",
            "    protected void sendMessage(InetAddress destination, RepairMessage message)\n    {\n        logger.trace(\"Sending {} to {}\", message, destination);\n        MessageOut<RepairMessage> messageOut = new MessageOut<RepairMessage>(MessagingService.Verb.REPAIR_MESSAGE, message, RepairMessage.serializer);\n        MessagingService.instance().sendOneWay(messageOut, destination);\n    }"
        ],
        [
            "CoordinatorSession::execute(Executor,Supplier,AtomicBoolean)",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249 -\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "    /**\n     * Runs the asynchronous consistent repair session. Actual repair sessions are scheduled via a submitter to make unit testing easier\n     */\n    public ListenableFuture execute(Executor executor, Supplier<ListenableFuture<List<RepairSessionResult>>> sessionSubmitter, AtomicBoolean hasFailure)\n    {\n        logger.debug(\"Executing consistent repair {}\", sessionID);\n\n        ListenableFuture<Boolean> prepareResult = prepare(executor);\n\n        // run repair sessions normally\n        ListenableFuture<List<RepairSessionResult>> repairSessionResults = Futures.transform(prepareResult, new AsyncFunction<Boolean, List<RepairSessionResult>>()\n        {\n            public ListenableFuture<List<RepairSessionResult>> apply(Boolean success) throws Exception\n            {\n                if (success)\n                {\n                    setRepairing();\n                    return sessionSubmitter.get();\n                }\n                else\n                {\n                    return Futures.immediateFuture(null);\n                }\n\n            }\n        });\n\n        // mark propose finalization\n        ListenableFuture<Boolean> proposeFuture = Futures.transform(repairSessionResults, new AsyncFunction<List<RepairSessionResult>, Boolean>()\n        {\n            public ListenableFuture<Boolean> apply(List<RepairSessionResult> results) throws Exception\n            {\n                if (results == null || results.isEmpty() || Iterables.any(results, r -> r == null))\n                {\n                    return Futures.immediateFailedFuture(new RuntimeException());\n                }\n                else\n                {\n                    return finalizePropose(executor);\n                }\n            }\n        });\n\n        // commit repaired data\n        Futures.addCallback(proposeFuture, new FutureCallback<Boolean>()\n        {\n            public void onSuccess(@Nullable Boolean result)\n            {\n                if (result != null && result)\n                {\n                    finalizeCommit(executor);\n                }\n                else\n                {\n                    hasFailure.set(true);\n                    fail(executor);\n                }\n            }\n\n            public void onFailure(Throwable t)\n            {\n                hasFailure.set(true);\n                fail(executor);\n            }\n        });\n\n        return proposeFuture;\n    }",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253 +\n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "    /**\n     * Runs the asynchronous consistent repair session. Actual repair sessions are scheduled via a submitter to make unit testing easier\n     */\n    public ListenableFuture execute(Executor executor, Supplier<ListenableFuture<List<RepairSessionResult>>> sessionSubmitter, AtomicBoolean hasFailure)\n    {\n        logger.info(\"Beginning coordination of incremental repair session {}\", sessionID);\n\n        ListenableFuture<Boolean> prepareResult = prepare(executor);\n\n        // run repair sessions normally\n        ListenableFuture<List<RepairSessionResult>> repairSessionResults = Futures.transform(prepareResult, new AsyncFunction<Boolean, List<RepairSessionResult>>()\n        {\n            public ListenableFuture<List<RepairSessionResult>> apply(Boolean success) throws Exception\n            {\n                if (success)\n                {\n                    setRepairing();\n                    return sessionSubmitter.get();\n                }\n                else\n                {\n                    return Futures.immediateFuture(null);\n                }\n\n            }\n        });\n\n        // mark propose finalization\n        ListenableFuture<Boolean> proposeFuture = Futures.transform(repairSessionResults, new AsyncFunction<List<RepairSessionResult>, Boolean>()\n        {\n            public ListenableFuture<Boolean> apply(List<RepairSessionResult> results) throws Exception\n            {\n                if (results == null || results.isEmpty() || Iterables.any(results, r -> r == null))\n                {\n                    return Futures.immediateFailedFuture(new RuntimeException());\n                }\n                else\n                {\n                    return finalizePropose(executor);\n                }\n            }\n        });\n\n        // commit repaired data\n        Futures.addCallback(proposeFuture, new FutureCallback<Boolean>()\n        {\n            public void onSuccess(@Nullable Boolean result)\n            {\n                if (result != null && result)\n                {\n                    finalizeCommit(executor);\n                }\n                else\n                {\n                    hasFailure.set(true);\n                    fail(executor);\n                }\n            }\n\n            public void onFailure(Throwable t)\n            {\n                hasFailure.set(true);\n                fail(executor);\n            }\n        });\n\n        return proposeFuture;\n    }"
        ],
        [
            "CoordinatorSession::handlePrepareResponse(InetAddress,boolean)",
            " 150  \n 151  \n 152  \n 153  \n 154 -\n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "    public synchronized void handlePrepareResponse(InetAddress participant, boolean success)\n    {\n        if (getState() == State.FAILED)\n        {\n            logger.debug(\"Consistent repair {} has failed, ignoring prepare response from {}\", sessionID, participant);\n        }\n        else if (!success)\n        {\n            logger.debug(\"Failed prepare response received from {} for session {}\", participant, sessionID);\n            fail();\n            prepareFuture.set(false);\n        }\n        else\n        {\n            logger.debug(\"Successful prepare response received from {} for session {}\", participant, sessionID);\n            setParticipantState(participant, State.PREPARED);\n            if (getState() == State.PREPARED)\n            {\n                prepareFuture.set(true);\n            }\n        }\n    }",
            " 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159 +\n 160  \n 161  \n 162  \n 163  \n 164  \n 165 +\n 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172  \n 173  ",
            "    public synchronized void handlePrepareResponse(InetAddress participant, boolean success)\n    {\n        if (getState() == State.FAILED)\n        {\n            logger.trace(\"Incremental repair session {} has failed, ignoring prepare response from {}\", sessionID, participant);\n        }\n        else if (!success)\n        {\n            logger.debug(\"{} failed the prepare phase for incremental repair session {}. Aborting session\", participant, sessionID);\n            fail();\n            prepareFuture.set(false);\n        }\n        else\n        {\n            logger.trace(\"Successful prepare response received from {} for repair session {}\", participant, sessionID);\n            setParticipantState(participant, State.PREPARED);\n            if (getState() == State.PREPARED)\n            {\n                logger.debug(\"Incremental repair session {} successfully prepared.\", sessionID);\n                prepareFuture.set(true);\n            }\n        }\n    }"
        ],
        [
            "CoordinatorSession::prepare(Executor)",
            " 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "    public ListenableFuture<Boolean> prepare(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.PREPARING));\n\n        logger.debug(\"Sending PrepareConsistentRequest message to {}\", participants);\n        PrepareConsistentRequest message = new PrepareConsistentRequest(sessionID, coordinator, participants);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        return prepareFuture;\n    }",
            " 138  \n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "    public ListenableFuture<Boolean> prepare(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.PREPARING));\n\n        logger.debug(\"Beginning prepare phase of incremental repair session {}\", sessionID);\n        PrepareConsistentRequest message = new PrepareConsistentRequest(sessionID, coordinator, participants);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        return prepareFuture;\n    }"
        ],
        [
            "CoordinatorSession::handleFinalizePromise(InetAddress,boolean)",
            " 190  \n 191  \n 192  \n 193  \n 194 -\n 195  \n 196  \n 197  \n 198 -\n 199  \n 200  \n 201  \n 202  \n 203  \n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "    public synchronized void handleFinalizePromise(InetAddress participant, boolean success)\n    {\n        if (getState() == State.FAILED)\n        {\n            logger.debug(\"Consistent repair {} has failed, ignoring finalize promise from {}\", sessionID, participant);\n        }\n        else if (!success)\n        {\n            logger.debug(\"Failed finalize promise received from {} for session {}\", participant, sessionID);\n            fail();\n            finalizeProposeFuture.set(false);\n        }\n        else\n        {\n            logger.debug(\"Successful finalize promise received from {} for session {}\", participant, sessionID);\n            setParticipantState(participant, State.FINALIZE_PROMISED);\n            if (getState() == State.FINALIZE_PROMISED)\n            {\n                finalizeProposeFuture.set(true);\n            }\n        }\n    }",
            " 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200 +\n 201  \n 202  \n 203  \n 204  \n 205  \n 206 +\n 207  \n 208  \n 209  \n 210 +\n 211  \n 212  \n 213  \n 214  ",
            "    public synchronized void handleFinalizePromise(InetAddress participant, boolean success)\n    {\n        if (getState() == State.FAILED)\n        {\n            logger.trace(\"Incremental repair {} has failed, ignoring finalize promise from {}\", sessionID, participant);\n        }\n        else if (!success)\n        {\n            logger.debug(\"Finalization proposal of session {} rejected by {}. Aborting session\", sessionID, participant);\n            fail();\n            finalizeProposeFuture.set(false);\n        }\n        else\n        {\n            logger.trace(\"Successful finalize promise received from {} for repair session {}\", participant, sessionID);\n            setParticipantState(participant, State.FINALIZE_PROMISED);\n            if (getState() == State.FINALIZE_PROMISED)\n            {\n                logger.debug(\"Finalization proposal for repair session {} accepted by all participants.\", sessionID);\n                finalizeProposeFuture.set(true);\n            }\n        }\n    }"
        ],
        [
            "LocalSessions::sendMessage(InetAddress,RepairMessage)",
            " 454  \n 455  \n 456 -\n 457  \n 458  \n 459  ",
            "    protected void sendMessage(InetAddress destination, RepairMessage message)\n    {\n        logger.debug(\"sending {} to {}\", message, destination);\n        MessageOut<RepairMessage> messageOut = new MessageOut<RepairMessage>(MessagingService.Verb.REPAIR_MESSAGE, message, RepairMessage.serializer);\n        MessagingService.instance().sendOneWay(messageOut, destination);\n    }",
            " 454  \n 455  \n 456 +\n 457  \n 458  \n 459  ",
            "    protected void sendMessage(InetAddress destination, RepairMessage message)\n    {\n        logger.trace(\"sending {} to {}\", message, destination);\n        MessageOut<RepairMessage> messageOut = new MessageOut<RepairMessage>(MessagingService.Verb.REPAIR_MESSAGE, message, RepairMessage.serializer);\n        MessagingService.instance().sendOneWay(messageOut, destination);\n    }"
        ],
        [
            "LocalSessions::handleFailSessionMessage(InetAddress,FailSession)",
            " 639  \n 640  \n 641 -\n 642  \n 643  ",
            "    public void handleFailSessionMessage(InetAddress from, FailSession msg)\n    {\n        logger.debug(\"received {} from {}\", msg, from);\n        failSession(msg.sessionID, false);\n    }",
            " 641  \n 642  \n 643 +\n 644  \n 645  ",
            "    public void handleFailSessionMessage(InetAddress from, FailSession msg)\n    {\n        logger.trace(\"received {} from {}\", msg, from);\n        failSession(msg.sessionID, false);\n    }"
        ],
        [
            "CoordinatorSession::fail(Executor)",
            " 230  \n 231  \n 232 -\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "    public synchronized void fail(Executor executor)\n    {\n        logger.debug(\"Failing session {}\", sessionID);\n        FailSession message = new FailSession(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            if (participantStates.get(participant) != State.FAILED)\n            {\n                executor.execute(() -> sendMessage(participant, message));\n            }\n        }\n        setAll(State.FAILED);\n    }",
            " 234  \n 235  \n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "    public synchronized void fail(Executor executor)\n    {\n        logger.info(\"Incremental repair session {} failed\", sessionID);\n        FailSession message = new FailSession(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            if (participantStates.get(participant) != State.FAILED)\n            {\n                executor.execute(() -> sendMessage(participant, message));\n            }\n        }\n        setAll(State.FAILED);\n    }"
        ],
        [
            "LocalSessions::handleFinalizeProposeMessage(InetAddress,FinalizePropose)",
            " 581  \n 582  \n 583 -\n 584  \n 585  \n 586  \n 587  \n 588 -\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600 -\n 601  \n 602  \n 603  ",
            "    public void handleFinalizeProposeMessage(InetAddress from, FinalizePropose propose)\n    {\n        logger.debug(\"received {} from {}\", propose, from);\n        UUID sessionID = propose.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.debug(\"No LocalSession found for session {}, responding with failure\", sessionID);\n            sendMessage(from, new FailSession(sessionID));\n            return;\n        }\n\n        try\n        {\n            setStateAndSave(session, FINALIZE_PROMISED);\n            sendMessage(from, new FinalizePromise(sessionID, getBroadcastAddress(), true));\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"error setting session to FINALIZE_PROMISED\", e);\n            failSession(sessionID);\n        }\n    }",
            " 581  \n 582  \n 583 +\n 584  \n 585  \n 586  \n 587  \n 588 +\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597 +\n 598  \n 599  \n 600  \n 601 +\n 602  \n 603  \n 604  ",
            "    public void handleFinalizeProposeMessage(InetAddress from, FinalizePropose propose)\n    {\n        logger.trace(\"received {} from {}\", propose, from);\n        UUID sessionID = propose.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.debug(\"Received FinalizePropose message for unknown repair session {}, responding with failure\");\n            sendMessage(from, new FailSession(sessionID));\n            return;\n        }\n\n        try\n        {\n            setStateAndSave(session, FINALIZE_PROMISED);\n            sendMessage(from, new FinalizePromise(sessionID, getBroadcastAddress(), true));\n            logger.debug(\"Received FinalizePropose message for incremental repair session {}, responded with FinalizePromise\");\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(String.format(\"Error handling FinalizePropose message for %s\", session), e);\n            failSession(sessionID);\n        }\n    }"
        ],
        [
            "LocalSessions::handlePrepareMessage(InetAddress,PrepareConsistentRequest)",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528 -\n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540 -\n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547 -\n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556 -\n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 -\n 565  \n 566  \n 567  \n 568  \n 569  ",
            "    /**\n     * The PrepareConsistentRequest effectively promotes the parent repair session to a consistent\n     * incremental session, and begins the 'pending anti compaction' which moves all sstable data\n     * that is to be repaired into it's own silo, preventing it from mixing with other data.\n     *\n     * No response is sent to the repair coordinator until the pending anti compaction has completed\n     * successfully. If the pending anti compaction fails, a failure message is sent to the coordinator,\n     * cancelling the session.\n     */\n    public void handlePrepareMessage(InetAddress from, PrepareConsistentRequest request)\n    {\n        logger.debug(\"received {} from {}\", request, from);\n        UUID sessionID = request.parentSession;\n        InetAddress coordinator = request.coordinator;\n        Set<InetAddress> peers = request.participants;\n\n        ActiveRepairService.ParentRepairSession parentSession;\n        try\n        {\n            parentSession = getParentRepairSession(sessionID);\n        }\n        catch (Throwable e)\n        {\n            logger.debug(\"Error retrieving ParentRepairSession for session {}, responding with failure\", sessionID);\n            sendMessage(coordinator, new FailSession(sessionID));\n            return;\n        }\n\n        LocalSession session = createSessionUnsafe(sessionID, parentSession, peers);\n        putSessionUnsafe(session);\n        logger.debug(\"created local session for {}\", sessionID);\n\n        ExecutorService executor = Executors.newFixedThreadPool(parentSession.getColumnFamilyStores().size());\n\n        ListenableFuture pendingAntiCompaction = submitPendingAntiCompaction(session, executor);\n        Futures.addCallback(pendingAntiCompaction, new FutureCallback()\n        {\n            public void onSuccess(@Nullable Object result)\n            {\n                logger.debug(\"pending anti-compaction for {} completed\", sessionID);\n                setStateAndSave(session, PREPARED);\n                sendMessage(coordinator, new PrepareConsistentResponse(sessionID, getBroadcastAddress(), true));\n                executor.shutdown();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                logger.debug(\"pending anti-compaction for {} failed\", sessionID);\n                failSession(sessionID);\n                executor.shutdown();\n            }\n        });\n    }",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528 +\n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540 +\n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547 +\n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556 +\n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 +\n 565  \n 566  \n 567  \n 568  \n 569  ",
            "    /**\n     * The PrepareConsistentRequest effectively promotes the parent repair session to a consistent\n     * incremental session, and begins the 'pending anti compaction' which moves all sstable data\n     * that is to be repaired into it's own silo, preventing it from mixing with other data.\n     *\n     * No response is sent to the repair coordinator until the pending anti compaction has completed\n     * successfully. If the pending anti compaction fails, a failure message is sent to the coordinator,\n     * cancelling the session.\n     */\n    public void handlePrepareMessage(InetAddress from, PrepareConsistentRequest request)\n    {\n        logger.trace(\"received {} from {}\", request, from);\n        UUID sessionID = request.parentSession;\n        InetAddress coordinator = request.coordinator;\n        Set<InetAddress> peers = request.participants;\n\n        ActiveRepairService.ParentRepairSession parentSession;\n        try\n        {\n            parentSession = getParentRepairSession(sessionID);\n        }\n        catch (Throwable e)\n        {\n            logger.trace(\"Error retrieving ParentRepairSession for session {}, responding with failure\", sessionID);\n            sendMessage(coordinator, new FailSession(sessionID));\n            return;\n        }\n\n        LocalSession session = createSessionUnsafe(sessionID, parentSession, peers);\n        putSessionUnsafe(session);\n        logger.info(\"Beginning local incremental repair session {}\", session);\n\n        ExecutorService executor = Executors.newFixedThreadPool(parentSession.getColumnFamilyStores().size());\n\n        ListenableFuture pendingAntiCompaction = submitPendingAntiCompaction(session, executor);\n        Futures.addCallback(pendingAntiCompaction, new FutureCallback()\n        {\n            public void onSuccess(@Nullable Object result)\n            {\n                logger.debug(\"Prepare phase for incremental repair session {} completed\", sessionID);\n                setStateAndSave(session, PREPARED);\n                sendMessage(coordinator, new PrepareConsistentResponse(sessionID, getBroadcastAddress(), true));\n                executor.shutdown();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                logger.error(String.format(\"Prepare phase for incremental repair session %s failed\", sessionID), t);\n                failSession(sessionID);\n                executor.shutdown();\n            }\n        });\n    }"
        ],
        [
            "CoordinatorSession::finalizeCommit(Executor)",
            " 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "    public synchronized void finalizeCommit(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.FINALIZE_PROMISED));\n        logger.debug(\"Sending FinalizeCommit message to {}\", participants);\n        FinalizeCommit message = new FinalizeCommit(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        setAll(State.FINALIZED);\n    }",
            " 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227  ",
            "    public synchronized void finalizeCommit(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.FINALIZE_PROMISED));\n        logger.debug(\"Committing finalization of repair session {}\", sessionID);\n        FinalizeCommit message = new FinalizeCommit(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        setAll(State.FINALIZED);\n        logger.info(\"Incremental repair session {} completed\", sessionID);\n    }"
        ],
        [
            "CoordinatorSession::setParticipantState(InetAddress,State)",
            "  95  \n  96  \n  97 -\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "    public synchronized void setParticipantState(InetAddress participant, State state)\n    {\n        logger.debug(\"Setting participant {} to state {} for repair {}\", participant, state, sessionID);\n        Preconditions.checkArgument(participantStates.containsKey(participant),\n                                    \"Session %s doesn't include %s\",\n                                    sessionID, participant);\n        Preconditions.checkArgument(participantStates.get(participant).canTransitionTo(state),\n                                    \"Invalid state transition %s -> %s\",\n                                    participantStates.get(participant), state);\n        participantStates.put(participant, state);\n\n        // update coordinator state if all participants are at the value being set\n        if (Iterables.all(participantStates.values(), s -> s == state))\n        {\n            setState(state);\n        }\n    }",
            "  95  \n  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "    public synchronized void setParticipantState(InetAddress participant, State state)\n    {\n        logger.trace(\"Setting participant {} to state {} for repair {}\", participant, state, sessionID);\n        Preconditions.checkArgument(participantStates.containsKey(participant),\n                                    \"Session %s doesn't include %s\",\n                                    sessionID, participant);\n        Preconditions.checkArgument(participantStates.get(participant).canTransitionTo(state),\n                                    \"Invalid state transition %s -> %s\",\n                                    participantStates.get(participant), state);\n        participantStates.put(participant, state);\n\n        // update coordinator state if all participants are at the value being set\n        if (Iterables.all(participantStates.values(), s -> s == state))\n        {\n            setState(state);\n        }\n    }"
        ],
        [
            "LocalSessions::failSession(UUID,boolean)",
            " 486  \n 487  \n 488 -\n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  ",
            "    public void failSession(UUID sessionID, boolean sendMessage)\n    {\n        logger.debug(\"failing session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        if (session != null)\n        {\n            setStateAndSave(session, FAILED);\n            if (sendMessage)\n            {\n                sendMessage(session.coordinator, new FailSession(sessionID));\n            }\n        }\n    }",
            " 486  \n 487  \n 488 +\n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  ",
            "    public void failSession(UUID sessionID, boolean sendMessage)\n    {\n        logger.info(\"Failing local repair session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        if (session != null)\n        {\n            setStateAndSave(session, FAILED);\n            if (sendMessage)\n            {\n                sendMessage(session.coordinator, new FailSession(sessionID));\n            }\n        }\n    }"
        ],
        [
            "CoordinatorSession::finalizePropose(Executor)",
            " 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "    public synchronized ListenableFuture<Boolean> finalizePropose(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.REPAIRING));\n        logger.debug(\"Sending FinalizePropose message to {}\", participants);\n        FinalizePropose message = new FinalizePropose(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        return finalizeProposeFuture;\n    }",
            " 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "    public synchronized ListenableFuture<Boolean> finalizePropose(Executor executor)\n    {\n        Preconditions.checkArgument(allStates(State.REPAIRING));\n        logger.debug(\"Proposing finalization of repair session {}\", sessionID);\n        FinalizePropose message = new FinalizePropose(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            executor.execute(() -> sendMessage(participant, message));\n        }\n        return finalizeProposeFuture;\n    }"
        ],
        [
            "LocalSessions::setStateAndSave(LocalSession,ConsistentSession)",
            " 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 -\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  ",
            "    private void setStateAndSave(LocalSession session, ConsistentSession.State state)\n    {\n        synchronized (session)\n        {\n            Preconditions.checkArgument(session.getState().canTransitionTo(state),\n                                        \"Invalid state transition %s -> %s\",\n                                        session.getState(), state);\n            logger.debug(\"Setting LocalSession state from {} -> {} for {}\", session.getState(), state, session.sessionID);\n            boolean wasCompleted = session.isCompleted();\n            session.setState(state);\n            session.setLastUpdate();\n            save(session);\n\n            if (session.isCompleted() && !wasCompleted)\n            {\n                sessionCompleted(session);\n            }\n        }\n    }",
            " 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 +\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  ",
            "    private void setStateAndSave(LocalSession session, ConsistentSession.State state)\n    {\n        synchronized (session)\n        {\n            Preconditions.checkArgument(session.getState().canTransitionTo(state),\n                                        \"Invalid state transition %s -> %s\",\n                                        session.getState(), state);\n            logger.trace(\"Changing LocalSession state from {} -> {} for {}\", session.getState(), state, session.sessionID);\n            boolean wasCompleted = session.isCompleted();\n            session.setState(state);\n            session.setLastUpdate();\n            save(session);\n\n            if (session.isCompleted() && !wasCompleted)\n            {\n                sessionCompleted(session);\n            }\n        }\n    }"
        ],
        [
            "LocalSessions::maybeSetRepairing(UUID)",
            " 571  \n 572  \n 573  \n 574  \n 575  \n 576 -\n 577  \n 578  \n 579  ",
            "    public void maybeSetRepairing(UUID sessionID)\n    {\n        LocalSession session = getSession(sessionID);\n        if (session != null && session.getState() != REPAIRING)\n        {\n            logger.debug(\"Setting local session {} to REPAIRING\", session);\n            setStateAndSave(session, REPAIRING);\n        }\n    }",
            " 571  \n 572  \n 573  \n 574  \n 575  \n 576 +\n 577  \n 578  \n 579  ",
            "    public void maybeSetRepairing(UUID sessionID)\n    {\n        LocalSession session = getSession(sessionID);\n        if (session != null && session.getState() != REPAIRING)\n        {\n            logger.debug(\"Setting local incremental repair session {} to REPAIRING\", session);\n            setStateAndSave(session, REPAIRING);\n        }\n    }"
        ],
        [
            "LocalSessions::cleanup()",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 -\n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "    /**\n     * Auto fails and auto deletes timed out and old sessions\n     * Compaction will clean up the sstables still owned by a deleted session\n     */\n    public void cleanup()\n    {\n        logger.debug(\"Running LocalSessions.cleanup\");\n        if (!isNodeInitialized())\n        {\n            logger.debug(\"node not initialized, aborting local session cleanup\");\n            return;\n        }\n        Set<LocalSession> currentSessions = new HashSet<>(sessions.values());\n        for (LocalSession session : currentSessions)\n        {\n            synchronized (session)\n            {\n                int now = FBUtilities.nowInSeconds();\n                if (shouldFail(session, now))\n                {\n                    logger.warn(\"Auto failing timed out repair session {}\", session);\n                    failSession(session.sessionID, false);\n                }\n                else if (shouldDelete(session, now))\n                {\n                    logger.warn(\"Auto deleting repair session {}\", session);\n                    deleteSession(session.sessionID);\n                }\n                else if (shouldCheckStatus(session, now))\n                {\n                    sendStatusRequest(session);\n                }\n            }\n        }\n    }",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245  \n 246  \n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "    /**\n     * Auto fails and auto deletes timed out and old sessions\n     * Compaction will clean up the sstables still owned by a deleted session\n     */\n    public void cleanup()\n    {\n        logger.trace(\"Running LocalSessions.cleanup\");\n        if (!isNodeInitialized())\n        {\n            logger.trace(\"node not initialized, aborting local session cleanup\");\n            return;\n        }\n        Set<LocalSession> currentSessions = new HashSet<>(sessions.values());\n        for (LocalSession session : currentSessions)\n        {\n            synchronized (session)\n            {\n                int now = FBUtilities.nowInSeconds();\n                if (shouldFail(session, now))\n                {\n                    logger.warn(\"Auto failing timed out repair session {}\", session);\n                    failSession(session.sessionID, false);\n                }\n                else if (shouldDelete(session, now))\n                {\n                    logger.debug(\"Auto deleting repair session {}\", session);\n                    deleteSession(session.sessionID);\n                }\n                else if (shouldCheckStatus(session, now))\n                {\n                    sendStatusRequest(session);\n                }\n            }\n        }\n    }"
        ],
        [
            "CoordinatorSession::setState(State)",
            "  89  \n  90  \n  91 -\n  92  \n  93  ",
            "    public void setState(State state)\n    {\n        logger.debug(\"Setting coordinator state to {} for repair {}\", state, sessionID);\n        super.setState(state);\n    }",
            "  89  \n  90  \n  91 +\n  92  \n  93  ",
            "    public void setState(State state)\n    {\n        logger.trace(\"Setting coordinator state to {} for repair {}\", state, sessionID);\n        super.setState(state);\n    }"
        ],
        [
            "LocalSessions::handleFinalizeCommitMessage(InetAddress,FinalizeCommit)",
            " 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627 -\n 628  \n 629  \n 630  \n 631  \n 632 -\n 633  \n 634  \n 635  \n 636  \n 637  ",
            "    /**\n     * Finalizes the repair session, completing it as successful.\n     *\n     * This only changes the state of the session, it doesn't promote the siloed sstables to repaired. That will happen\n     * as part of the compaction process, and avoids having to worry about in progress compactions interfering with the\n     * promotion.\n     */\n    public void handleFinalizeCommitMessage(InetAddress from, FinalizeCommit commit)\n    {\n        logger.debug(\"received {} from {}\", commit, from);\n        UUID sessionID = commit.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Received finalize commit message for unknown session {}\", sessionID);\n            return;\n        }\n\n        setStateAndSave(session, FINALIZED);\n    }",
            " 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628 +\n 629  \n 630  \n 631  \n 632  \n 633 +\n 634  \n 635  \n 636  \n 637  \n 638 +\n 639  ",
            "    /**\n     * Finalizes the repair session, completing it as successful.\n     *\n     * This only changes the state of the session, it doesn't promote the siloed sstables to repaired. That will happen\n     * as part of the compaction process, and avoids having to worry about in progress compactions interfering with the\n     * promotion.\n     */\n    public void handleFinalizeCommitMessage(InetAddress from, FinalizeCommit commit)\n    {\n        logger.trace(\"received {} from {}\", commit, from);\n        UUID sessionID = commit.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Ignoring FinalizeCommit message for unknown repair session {}\", sessionID);\n            return;\n        }\n\n        setStateAndSave(session, FINALIZED);\n        logger.info(\"Finalized local repair session {}\", sessionID);\n    }"
        ],
        [
            "LocalSession::toString()",
            "  81  \n  82  \n  83  \n  84 -\n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "    public String toString()\n    {\n        return \"LocalSession{\" +\n               \"state=\" + getState() +\n               \", sessionID=\" + sessionID +\n               \", coordinator=\" + coordinator +\n               \", tableIds=\" + tableIds +\n               \", repairedAt=\" + repairedAt +\n               \", ranges=\" + ranges +\n               \", participants=\" + participants +\n               \", startedAt=\" + startedAt +\n               \", lastUpdate=\" + lastUpdate +\n               '}';\n    }",
            "  81  \n  82  \n  83  \n  84 +\n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "    public String toString()\n    {\n        return \"LocalSession{\" +\n               \"sessionID=\" + sessionID +\n               \", state=\" + getState() +\n               \", coordinator=\" + coordinator +\n               \", tableIds=\" + tableIds +\n               \", repairedAt=\" + repairedAt +\n               \", ranges=\" + ranges +\n               \", participants=\" + participants +\n               \", startedAt=\" + startedAt +\n               \", lastUpdate=\" + lastUpdate +\n               '}';\n    }"
        ],
        [
            "LocalSessions::cancelSession(UUID,boolean)",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "    /**\n     * hook for operators to cancel sessions, cancelling from a non-coordinator is an error, unless\n     * force is set to true. Messages are sent out to other participants, but we don't wait for a response\n     */\n    public void cancelSession(UUID sessionID, boolean force)\n    {\n        logger.debug(\"cancelling session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        Preconditions.checkArgument(session != null, \"Session {} does not exist\", sessionID);\n        Preconditions.checkArgument(force || session.coordinator.equals(getBroadcastAddress()),\n                                    \"Cancel session %s from it's coordinator (%s) or use --force\",\n                                    sessionID, session.coordinator);\n\n        setStateAndSave(session, FAILED);\n        for (InetAddress participant : session.participants)\n        {\n            if (!participant.equals(getBroadcastAddress()))\n                sendMessage(participant, new FailSession(sessionID));\n        }\n    }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "    /**\n     * hook for operators to cancel sessions, cancelling from a non-coordinator is an error, unless\n     * force is set to true. Messages are sent out to other participants, but we don't wait for a response\n     */\n    public void cancelSession(UUID sessionID, boolean force)\n    {\n        logger.info(\"Cancelling local repair session {}\", sessionID);\n        LocalSession session = getSession(sessionID);\n        Preconditions.checkArgument(session != null, \"Session {} does not exist\", sessionID);\n        Preconditions.checkArgument(force || session.coordinator.equals(getBroadcastAddress()),\n                                    \"Cancel session %s from it's coordinator (%s) or use --force\",\n                                    sessionID, session.coordinator);\n\n        setStateAndSave(session, FAILED);\n        for (InetAddress participant : session.participants)\n        {\n            if (!participant.equals(getBroadcastAddress()))\n                sendMessage(participant, new FailSession(sessionID));\n        }\n    }"
        ],
        [
            "LocalSessions::handleStatusResponse(InetAddress,StatusResponse)",
            " 673  \n 674  \n 675 -\n 676  \n 677  \n 678  \n 679  \n 680 -\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692 -\n 693  \n 694  ",
            "    public void handleStatusResponse(InetAddress from, StatusResponse response)\n    {\n        logger.debug(\"received {} from {}\", response, from);\n        UUID sessionID = response.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Received status response message for unknown session {}\", sessionID);\n            return;\n        }\n\n        // only change local state if response state is FINALIZED or FAILED, since those are\n        // the only statuses that would indicate we've missed a message completing the session\n        if (response.state == FINALIZED || response.state == FAILED)\n        {\n            setStateAndSave(session, response.state);\n        }\n        else\n        {\n            logger.debug(\"{} is not actionable\");\n        }\n    }",
            " 677  \n 678  \n 679 +\n 680  \n 681  \n 682  \n 683  \n 684 +\n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693 +\n 694  \n 695  \n 696  \n 697 +\n 698  \n 699  ",
            "    public void handleStatusResponse(InetAddress from, StatusResponse response)\n    {\n        logger.trace(\"received {} from {}\", response, from);\n        UUID sessionID = response.sessionID;\n        LocalSession session = getSession(sessionID);\n        if (session == null)\n        {\n            logger.warn(\"Received StatusResponse message for unknown repair session {}\", sessionID);\n            return;\n        }\n\n        // only change local state if response state is FINALIZED or FAILED, since those are\n        // the only statuses that would indicate we've missed a message completing the session\n        if (response.state == FINALIZED || response.state == FAILED)\n        {\n            setStateAndSave(session, response.state);\n            logger.info(\"Unfinished local incremental repair session {} set to state {}\", sessionID, response.state);\n        }\n        else\n        {\n            logger.debug(\"Received StatusResponse for repair session {} with state {}, which is not actionable. Doing nothing.\", sessionID, response.state);\n        }\n    }"
        ],
        [
            "LocalSessions::sendStatusRequest(LocalSession)",
            " 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  ",
            "    public void sendStatusRequest(LocalSession session)\n    {\n        StatusRequest request = new StatusRequest(session.sessionID);\n        for (InetAddress participant : session.participants)\n        {\n            if (!getBroadcastAddress().equals(participant) && isAlive(participant))\n            {\n                sendMessage(participant, request);\n            }\n        }\n    }",
            " 647  \n 648  \n 649 +\n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  ",
            "    public void sendStatusRequest(LocalSession session)\n    {\n        logger.debug(\"Attempting to learn the outcome of unfinished local incremental repair session {}\", session.sessionID);\n        StatusRequest request = new StatusRequest(session.sessionID);\n        for (InetAddress participant : session.participants)\n        {\n            if (!getBroadcastAddress().equals(participant) && isAlive(participant))\n            {\n                sendMessage(participant, request);\n            }\n        }\n    }"
        ]
    ],
    "ebd0aaefe54d8a1349a54d904831e1d9e5e812bf": [
        [
            "CompactionManager::performAnticompaction(ColumnFamilyStore,Collection,Refs,LifecycleTransaction,long,UUID,UUID)",
            " 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631 -\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getActiveRepairedSSTableRefs(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @param parentRepairSession parent repair session ID\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt,\n                                      UUID pendingRepair,\n                                      UUID parentRepairSession) throws InterruptedException, IOException\n    {\n        ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(parentRepairSession);\n        Preconditions.checkArgument(!prs.isPreview(), \"Cannot anticompact for previews\");\n\n        logger.info(\"{} Starting anticompaction for {}.{} on {}/{} sstables\", PreviewKind.NONE.logPrefix(parentRepairSession), cfs.keyspace.getName(), cfs.getTableName(), validatedForRepair.size(), cfs.getLiveSSTables());\n        logger.trace(\"{} Starting anticompaction for ranges {}\", PreviewKind.NONE.logPrefix(parentRepairSession), ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        // we should only notify that repair status changed if it actually did:\n        Set<SSTableReader> mutatedRepairStatusToNotify = new HashSet<>();\n        Map<SSTableReader, Boolean> wasRepairedBefore = new HashMap<>();\n        for (SSTableReader sstable : sstables)\n            wasRepairedBefore.put(sstable, sstable.isRepaired());\n\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken());\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"{} SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, repairedAt, pendingRepair);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        if (!wasRepairedBefore.get(sstable))\n                            mutatedRepairStatusToNotify.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"{} SSTable {} ({}) will be anticompacted on range {}\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"{} SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            cfs.metric.bytesMutatedAnticompaction.inc(SSTableReader.getTotalBytes(mutatedRepairStatuses));\n            cfs.getTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatusToNotify);\n            txn.cancel(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt, pendingRepair);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"{} Completed anticompaction successfully\", PreviewKind.NONE.logPrefix(parentRepairSession));\n    }",
            " 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631 +\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getActiveRepairedSSTableRefs(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @param parentRepairSession parent repair session ID\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt,\n                                      UUID pendingRepair,\n                                      UUID parentRepairSession) throws InterruptedException, IOException\n    {\n        ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(parentRepairSession);\n        Preconditions.checkArgument(!prs.isPreview(), \"Cannot anticompact for previews\");\n\n        logger.info(\"{} Starting anticompaction for {}.{} on {}/{} sstables\", PreviewKind.NONE.logPrefix(parentRepairSession), cfs.keyspace.getName(), cfs.getTableName(), validatedForRepair.size(), cfs.getLiveSSTables().size());\n        logger.trace(\"{} Starting anticompaction for ranges {}\", PreviewKind.NONE.logPrefix(parentRepairSession), ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        // we should only notify that repair status changed if it actually did:\n        Set<SSTableReader> mutatedRepairStatusToNotify = new HashSet<>();\n        Map<SSTableReader, Boolean> wasRepairedBefore = new HashMap<>();\n        for (SSTableReader sstable : sstables)\n            wasRepairedBefore.put(sstable, sstable.isRepaired());\n\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken());\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"{} SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, repairedAt, pendingRepair);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        if (!wasRepairedBefore.get(sstable))\n                            mutatedRepairStatusToNotify.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"{} SSTable {} ({}) will be anticompacted on range {}\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"{} SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", PreviewKind.NONE.logPrefix(parentRepairSession), sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            cfs.metric.bytesMutatedAnticompaction.inc(SSTableReader.getTotalBytes(mutatedRepairStatuses));\n            cfs.getTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatusToNotify);\n            txn.cancel(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt, pendingRepair);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"{} Completed anticompaction successfully\", PreviewKind.NONE.logPrefix(parentRepairSession));\n    }"
        ]
    ],
    "6a1b1f26b7174e8c9bf86a96514ab626ce2a4117": [
        [
            "StressAction::warmup(OpDistributionFactory)",
            "  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "    private void warmup(OpDistributionFactory operations)\n    {\n        PrintStream warmupOutput = new PrintStream(new OutputStream() { @Override public void write(int b) throws IOException { } } );\n        // do 25% of iterations as warmup but no more than 50k (by default hotspot compiles methods after 10k invocations)\n        int iterations = (settings.command.count > 0\n                         ? Math.min(50000, (int)(settings.command.count * 0.25))\n                         : 50000) * settings.node.nodes.size();\n        int threads = 100;\n\n        if (settings.rate.maxThreads > 0)\n            threads = Math.min(threads, settings.rate.maxThreads);\n        if (settings.rate.threadCount > 0)\n            threads = Math.min(threads, settings.rate.threadCount);\n\n        for (OpDistributionFactory single : operations.each())\n        {\n            // we need to warm up all the nodes in the cluster ideally, but we may not be the only stress instance;\n            // so warm up all the nodes we're speaking to only.\n            output.println(String.format(\"Warming up %s with %d iterations...\", single.desc(), iterations));\n            run(single, threads, iterations, 0, null, null, warmupOutput, true);\n        }\n    }",
            "  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "    private void warmup(OpDistributionFactory operations)\n    {\n        PrintStream warmupOutput = new PrintStream(new OutputStream() { @Override public void write(int b) throws IOException { } } );\n        // do 25% of iterations as warmup but no more than 50k (by default hotspot compiles methods after 10k invocations)\n        int iterations = Math.min(50000, (int) (settings.command.count * 0.25)) * settings.node.nodes.size();\n        int threads = 100;\n\n        if (settings.rate.maxThreads > 0)\n            threads = Math.min(threads, settings.rate.maxThreads);\n        if (settings.rate.threadCount > 0)\n            threads = Math.min(threads, settings.rate.threadCount);\n\n        for (OpDistributionFactory single : operations.each())\n        {\n            // we need to warm up all the nodes in the cluster ideally, but we may not be the only stress instance;\n            // so warm up all the nodes we're speaking to only.\n            output.println(String.format(\"Warming up %s with %d iterations...\", single.desc(), iterations));\n            run(single, threads, iterations, 0, null, null, warmupOutput, true);\n        }\n    }"
        ],
        [
            "StressAction::run()",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    public void run()\n    {\n        // creating keyspace and column families\n        settings.maybeCreateKeyspaces();\n\n        output.println(\"Sleeping 2s...\");\n        Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);\n\n        if (!settings.command.noWarmup)\n            warmup(settings.command.getFactory(settings));\n        if (settings.command.truncate == SettingsCommand.TruncateWhen.ONCE)\n            settings.command.truncateTables(settings);\n\n        // TODO : move this to a new queue wrapper that gates progress based on a poisson (or configurable) distribution\n        RateLimiter rateLimiter = null;\n        if (settings.rate.opRateTargetPerSecond > 0)\n            rateLimiter = RateLimiter.create(settings.rate.opRateTargetPerSecond);\n\n        boolean success;\n        if (settings.rate.minThreads > 0)\n            success = runMulti(settings.rate.auto, rateLimiter);\n        else\n            success = null != run(settings.command.getFactory(settings), settings.rate.threadCount, settings.command.count,\n                                  settings.command.duration, rateLimiter, settings.command.durationUnits, output, false);\n\n        if (success)\n            output.println(\"END\");\n        else\n            output.println(\"FAILURE\");\n\n        settings.disconnect();\n    }",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "    public void run()\n    {\n        // creating keyspace and column families\n        settings.maybeCreateKeyspaces();\n\n        if (settings.command.count == 0)\n        {\n            output.println(\"N=0: SCHEMA CREATED, NOTHING ELSE DONE.\");\n            settings.disconnect();\n            return;\n        }\n\n        output.println(\"Sleeping 2s...\");\n        Uninterruptibles.sleepUninterruptibly(2, TimeUnit.SECONDS);\n\n        if (!settings.command.noWarmup)\n            warmup(settings.command.getFactory(settings));\n        if (settings.command.truncate == SettingsCommand.TruncateWhen.ONCE)\n            settings.command.truncateTables(settings);\n\n        // TODO : move this to a new queue wrapper that gates progress based on a poisson (or configurable) distribution\n        RateLimiter rateLimiter = null;\n        if (settings.rate.opRateTargetPerSecond > 0)\n            rateLimiter = RateLimiter.create(settings.rate.opRateTargetPerSecond);\n\n        boolean success;\n        if (settings.rate.minThreads > 0)\n            success = runMulti(settings.rate.auto, rateLimiter);\n        else\n            success = null != run(settings.command.getFactory(settings), settings.rate.threadCount, settings.command.count,\n                                  settings.command.duration, rateLimiter, settings.command.durationUnits, output, false);\n\n        if (success)\n            output.println(\"END\");\n        else\n            output.println(\"FAILURE\");\n\n        settings.disconnect();\n    }"
        ]
    ],
    "5d4335e8f8affc738cc72eacec2f01a8ea18a5b3": [
        [
            "DynamicCompositeType::validateComparator(int,ByteBuffer)",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204 -\n 205  \n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211 -\n 212  \n 213 -\n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "    protected AbstractType<?> validateComparator(int i, ByteBuffer bb) throws MarshalException\n    {\n        AbstractType<?> comparator = null;\n        if (bb.remaining() < 2)\n            throw new MarshalException(\"Not enough bytes to header of the comparator part of component \" + i);\n        int header = ByteBufferUtil.readShortLength(bb);\n        if ((header & 0x8000) == 0)\n        {\n            if (bb.remaining() < header)\n                throw new MarshalException(\"Not enough bytes to read comparator name of component \" + i);\n\n            ByteBuffer value = ByteBufferUtil.readBytes(bb, header);\n            String valueStr = null;\n            try\n            {\n                valueStr = ByteBufferUtil.string(value);\n                comparator = TypeParser.parse(valueStr);\n            }\n            catch (CharacterCodingException ce) \n            {\n                // ByteBufferUtil.string failed. \n                // Log it here and we'll further throw an exception below since comparator == null\n                logger.error(\"Failed with [{}] when decoding the byte buffer in ByteBufferUtil.string()\", \n                   ce.toString());\n            }\n            catch (Exception e)\n            {\n                // parse failed. \n                // Log it here and we'll further throw an exception below since comparator == null\n                logger.error(\"Failed to parse value string \\\"{}\\\" with exception: [{}]\", \n                   valueStr, e.toString());\n            }\n        }\n        else\n        {\n            comparator = aliases.get((byte)(header & 0xFF));\n        }\n\n        if (comparator == null)\n            throw new MarshalException(\"Cannot find comparator for component \" + i);\n        else\n            return comparator;\n    }",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204 +\n 205  \n 206 +\n 207 +\n 208  \n 209  \n 210  \n 211 +\n 212  \n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "    protected AbstractType<?> validateComparator(int i, ByteBuffer bb) throws MarshalException\n    {\n        AbstractType<?> comparator = null;\n        if (bb.remaining() < 2)\n            throw new MarshalException(\"Not enough bytes to header of the comparator part of component \" + i);\n        int header = ByteBufferUtil.readShortLength(bb);\n        if ((header & 0x8000) == 0)\n        {\n            if (bb.remaining() < header)\n                throw new MarshalException(\"Not enough bytes to read comparator name of component \" + i);\n\n            ByteBuffer value = ByteBufferUtil.readBytes(bb, header);\n            String valueStr = null;\n            try\n            {\n                valueStr = ByteBufferUtil.string(value);\n                comparator = TypeParser.parse(valueStr);\n            }\n            catch (CharacterCodingException ce)\n            {\n                // ByteBufferUtil.string failed.\n                // Log it here and we'll further throw an exception below since comparator == null\n                logger.error(\"Failed with [{}] when decoding the byte buffer in ByteBufferUtil.string()\",\n                   ce);\n            }\n            catch (Exception e)\n            {\n                // parse failed.\n                // Log it here and we'll further throw an exception below since comparator == null\n                logger.error(\"Failed to parse value string \\\"{}\\\" with exception: [{}]\",\n                   valueStr, e);\n            }\n        }\n        else\n        {\n            comparator = aliases.get((byte)(header & 0xFF));\n        }\n\n        if (comparator == null)\n            throw new MarshalException(\"Cannot find comparator for component \" + i);\n        else\n            return comparator;\n    }"
        ],
        [
            "DateTieredCompactionStrategyOptions::DateTieredCompactionStrategyOptions(Map)",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "    public DateTieredCompactionStrategyOptions(Map<String, String> options)\n    {\n        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);\n        timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);\n        if (timestampResolution != DEFAULT_TIMESTAMP_RESOLUTION)\n            logger.warn(\"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?\", timestampResolution.toString());\n        optionValue = options.get(MAX_SSTABLE_AGE_KEY);\n        double fractionalDays = optionValue == null ? DEFAULT_MAX_SSTABLE_AGE_DAYS : Double.parseDouble(optionValue);\n        maxSSTableAge = Math.round(fractionalDays * timestampResolution.convert(1, TimeUnit.DAYS));\n        optionValue = options.get(BASE_TIME_KEY);\n        baseTime = timestampResolution.convert(optionValue == null ? DEFAULT_BASE_TIME_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n        optionValue = options.get(EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS_KEY);\n        expiredSSTableCheckFrequency = TimeUnit.MILLISECONDS.convert(optionValue == null ? DEFAULT_EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n        optionValue = options.get(MAX_WINDOW_SIZE_KEY);\n        maxWindowSize = timestampResolution.convert(optionValue == null ? DEFAULT_MAX_WINDOW_SIZE_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n    }",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "    public DateTieredCompactionStrategyOptions(Map<String, String> options)\n    {\n        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);\n        timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);\n        if (timestampResolution != DEFAULT_TIMESTAMP_RESOLUTION)\n            logger.warn(\"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?\", timestampResolution);\n        optionValue = options.get(MAX_SSTABLE_AGE_KEY);\n        double fractionalDays = optionValue == null ? DEFAULT_MAX_SSTABLE_AGE_DAYS : Double.parseDouble(optionValue);\n        maxSSTableAge = Math.round(fractionalDays * timestampResolution.convert(1, TimeUnit.DAYS));\n        optionValue = options.get(BASE_TIME_KEY);\n        baseTime = timestampResolution.convert(optionValue == null ? DEFAULT_BASE_TIME_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n        optionValue = options.get(EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS_KEY);\n        expiredSSTableCheckFrequency = TimeUnit.MILLISECONDS.convert(optionValue == null ? DEFAULT_EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n        optionValue = options.get(MAX_WINDOW_SIZE_KEY);\n        maxWindowSize = timestampResolution.convert(optionValue == null ? DEFAULT_MAX_WINDOW_SIZE_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n    }"
        ],
        [
            "TimeWindowCompactionStrategyOptions::TimeWindowCompactionStrategyOptions(Map)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "    public TimeWindowCompactionStrategyOptions(Map<String, String> options)\n    {\n        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);\n        timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);\n        if (timestampResolution != DEFAULT_TIMESTAMP_RESOLUTION)\n            logger.warn(\"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?\", timestampResolution.toString());\n\n        optionValue = options.get(COMPACTION_WINDOW_UNIT_KEY);\n        sstableWindowUnit = optionValue == null ? DEFAULT_COMPACTION_WINDOW_UNIT : TimeUnit.valueOf(optionValue);\n\n        optionValue = options.get(COMPACTION_WINDOW_SIZE_KEY);\n        sstableWindowSize = optionValue == null ? DEFAULT_COMPACTION_WINDOW_SIZE : Integer.parseInt(optionValue);\n\n        optionValue = options.get(EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS_KEY);\n        expiredSSTableCheckFrequency = TimeUnit.MILLISECONDS.convert(optionValue == null ? DEFAULT_EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n\n        optionValue = options.get(UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION_KEY);\n        ignoreOverlaps = optionValue == null ? DEFAULT_UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION : (Boolean.getBoolean(UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION_PROPERTY) && Boolean.parseBoolean(optionValue));\n\n        stcsOptions = new SizeTieredCompactionStrategyOptions(options);\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "    public TimeWindowCompactionStrategyOptions(Map<String, String> options)\n    {\n        String optionValue = options.get(TIMESTAMP_RESOLUTION_KEY);\n        timestampResolution = optionValue == null ? DEFAULT_TIMESTAMP_RESOLUTION : TimeUnit.valueOf(optionValue);\n        if (timestampResolution != DEFAULT_TIMESTAMP_RESOLUTION)\n            logger.warn(\"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?\", timestampResolution);\n\n        optionValue = options.get(COMPACTION_WINDOW_UNIT_KEY);\n        sstableWindowUnit = optionValue == null ? DEFAULT_COMPACTION_WINDOW_UNIT : TimeUnit.valueOf(optionValue);\n\n        optionValue = options.get(COMPACTION_WINDOW_SIZE_KEY);\n        sstableWindowSize = optionValue == null ? DEFAULT_COMPACTION_WINDOW_SIZE : Integer.parseInt(optionValue);\n\n        optionValue = options.get(EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS_KEY);\n        expiredSSTableCheckFrequency = TimeUnit.MILLISECONDS.convert(optionValue == null ? DEFAULT_EXPIRED_SSTABLE_CHECK_FREQUENCY_SECONDS : Long.parseLong(optionValue), TimeUnit.SECONDS);\n\n        optionValue = options.get(UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION_KEY);\n        ignoreOverlaps = optionValue == null ? DEFAULT_UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION : (Boolean.getBoolean(UNSAFE_AGGRESSIVE_SSTABLE_EXPIRATION_PROPERTY) && Boolean.parseBoolean(optionValue));\n\n        stcsOptions = new SizeTieredCompactionStrategyOptions(options);\n    }"
        ],
        [
            "InboundHandshakeHandler::decode(ChannelHandlerContext,ByteBuf,List)",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "    @Override\n    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out)\n    {\n        try\n        {\n            if (!hasAuthenticated)\n            {\n                logSecureSocketDetails(ctx);\n                if (!handleAuthenticate(ctx.channel().remoteAddress(), ctx))\n                    return;\n            }\n\n            switch (state)\n            {\n                case START:\n                    state = handleStart(ctx, in);\n                    break;\n                case AWAIT_MESSAGING_START_RESPONSE:\n                    state = handleMessagingStartResponse(ctx, in);\n                    break;\n                case HANDSHAKE_FAIL:\n                    throw new IllegalStateException(\"channel should be closed after determining the handshake failed with peer: \" + ctx.channel().remoteAddress());\n                default:\n                    logger.error(\"unhandled state: \" + state);\n                    state = State.HANDSHAKE_FAIL;\n                    ctx.close();\n            }\n        }\n        catch (Exception e)\n        {\n            logger.error(\"unexpected error while negotiating internode messaging handshake\", e);\n            state = State.HANDSHAKE_FAIL;\n            ctx.close();\n        }\n    }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "    @Override\n    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out)\n    {\n        try\n        {\n            if (!hasAuthenticated)\n            {\n                logSecureSocketDetails(ctx);\n                if (!handleAuthenticate(ctx.channel().remoteAddress(), ctx))\n                    return;\n            }\n\n            switch (state)\n            {\n                case START:\n                    state = handleStart(ctx, in);\n                    break;\n                case AWAIT_MESSAGING_START_RESPONSE:\n                    state = handleMessagingStartResponse(ctx, in);\n                    break;\n                case HANDSHAKE_FAIL:\n                    throw new IllegalStateException(\"channel should be closed after determining the handshake failed with peer: \" + ctx.channel().remoteAddress());\n                default:\n                    logger.error(\"unhandled state: {}\", state);\n                    state = State.HANDSHAKE_FAIL;\n                    ctx.close();\n            }\n        }\n        catch (Exception e)\n        {\n            logger.error(\"unexpected error while negotiating internode messaging handshake\", e);\n            state = State.HANDSHAKE_FAIL;\n            ctx.close();\n        }\n    }"
        ],
        [
            "LogTransaction::delete(File)",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "    static void delete(File file)\n    {\n        try\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"Deleting {}\", file);\n\n            Files.delete(file.toPath());\n        }\n        catch (NoSuchFileException e)\n        {\n            logger.error(\"Unable to delete {} as it does not exist, see debug log file for stack trace\", file);\n            if (logger.isDebugEnabled())\n            {\n                ByteArrayOutputStream baos = new ByteArrayOutputStream();\n                try (PrintStream ps = new PrintStream(baos))\n                {\n                    e.printStackTrace(ps);\n                }\n                logger.debug(\"Unable to delete {} as it does not exist, stack trace:\\n {}\", file, baos.toString());\n            }\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Unable to delete {}\", file, e);\n            throw new RuntimeException(e);\n        }\n    }",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 +\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "    static void delete(File file)\n    {\n        try\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"Deleting {}\", file);\n\n            Files.delete(file.toPath());\n        }\n        catch (NoSuchFileException e)\n        {\n            logger.error(\"Unable to delete {} as it does not exist, see debug log file for stack trace\", file);\n            if (logger.isDebugEnabled())\n            {\n                ByteArrayOutputStream baos = new ByteArrayOutputStream();\n                try (PrintStream ps = new PrintStream(baos))\n                {\n                    e.printStackTrace(ps);\n                }\n                logger.debug(\"Unable to delete {} as it does not exist, stack trace:\\n {}\", file, baos);\n            }\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Unable to delete {}\", file, e);\n            throw new RuntimeException(e);\n        }\n    }"
        ],
        [
            "InboundHandshakeHandler::handleStart(ChannelHandlerContext,ByteBuf)",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    /**\n     * Handles receiving the first message in the internode messaging handshake protocol. If the sender's protocol version\n     * is accepted, we respond with the second message of the handshake protocol.\n     */\n    @VisibleForTesting\n    State handleStart(ChannelHandlerContext ctx, ByteBuf in) throws IOException\n    {\n        FirstHandshakeMessage msg = FirstHandshakeMessage.maybeDecode(in);\n        if (msg == null)\n            return State.START;\n\n        logger.trace(\"received first handshake message from peer {}, message = {}\", ctx.channel().remoteAddress(), msg);\n        version = msg.messagingVersion;\n\n        if (msg.mode == NettyFactory.Mode.STREAMING)\n        {\n            // streaming connections are per-session and have a fixed version.  we can't do anything with a wrong-version stream connection, so drop it.\n            if (version != StreamMessage.CURRENT_VERSION)\n            {\n                logger.warn(\"Received stream using protocol version %d (my version %d). Terminating connection\", version, MessagingService.current_version);\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            setupStreamingPipeline(ctx, version);\n            return State.HANDSHAKE_COMPLETE;\n        }\n        else\n        {\n            if (version < MessagingService.VERSION_30)\n            {\n                logger.error(\"Unable to read obsolete message version {} from {}; The earliest version supported is 3.0.0\", version, ctx.channel().remoteAddress());\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            logger.trace(\"Connection version {} from {}\", version, ctx.channel().remoteAddress());\n            compressed = msg.compressionEnabled;\n\n            // if this version is < the MS version the other node is trying\n            // to connect with, the other node will disconnect\n            ctx.writeAndFlush(new SecondHandshakeMessage(MessagingService.current_version).encode(ctx.alloc()))\n               .addListener(ChannelFutureListener.CLOSE_ON_FAILURE);\n\n            // outbound side will reconnect to change the version\n            if (version > MessagingService.current_version)\n            {\n                logger.info(\"peer wants to use a messaging version higher ({}) than what this node supports ({})\", version, MessagingService.current_version);\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getRpcTimeout());\n            handshakeTimeout = ctx.executor().schedule(() -> failHandshake(ctx), timeout, TimeUnit.MILLISECONDS);\n            return State.AWAIT_MESSAGING_START_RESPONSE;\n        }\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    /**\n     * Handles receiving the first message in the internode messaging handshake protocol. If the sender's protocol version\n     * is accepted, we respond with the second message of the handshake protocol.\n     */\n    @VisibleForTesting\n    State handleStart(ChannelHandlerContext ctx, ByteBuf in) throws IOException\n    {\n        FirstHandshakeMessage msg = FirstHandshakeMessage.maybeDecode(in);\n        if (msg == null)\n            return State.START;\n\n        logger.trace(\"received first handshake message from peer {}, message = {}\", ctx.channel().remoteAddress(), msg);\n        version = msg.messagingVersion;\n\n        if (msg.mode == NettyFactory.Mode.STREAMING)\n        {\n            // streaming connections are per-session and have a fixed version.  we can't do anything with a wrong-version stream connection, so drop it.\n            if (version != StreamMessage.CURRENT_VERSION)\n            {\n                logger.warn(\"Received stream using protocol version {} (my version {}). Terminating connection\", version, MessagingService.current_version);\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            setupStreamingPipeline(ctx, version);\n            return State.HANDSHAKE_COMPLETE;\n        }\n        else\n        {\n            if (version < MessagingService.VERSION_30)\n            {\n                logger.error(\"Unable to read obsolete message version {} from {}; The earliest version supported is 3.0.0\", version, ctx.channel().remoteAddress());\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            logger.trace(\"Connection version {} from {}\", version, ctx.channel().remoteAddress());\n            compressed = msg.compressionEnabled;\n\n            // if this version is < the MS version the other node is trying\n            // to connect with, the other node will disconnect\n            ctx.writeAndFlush(new SecondHandshakeMessage(MessagingService.current_version).encode(ctx.alloc()))\n               .addListener(ChannelFutureListener.CLOSE_ON_FAILURE);\n\n            // outbound side will reconnect to change the version\n            if (version > MessagingService.current_version)\n            {\n                logger.info(\"peer wants to use a messaging version higher ({}) than what this node supports ({})\", version, MessagingService.current_version);\n                ctx.close();\n                return State.HANDSHAKE_FAIL;\n            }\n\n            long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getRpcTimeout());\n            handshakeTimeout = ctx.executor().schedule(() -> failHandshake(ctx), timeout, TimeUnit.MILLISECONDS);\n            return State.AWAIT_MESSAGING_START_RESPONSE;\n        }\n    }"
        ]
    ],
    "59b8e171e91e4d714acac5ec195f40158e2e2971": [
        [
            "ConnectionHandler::MessageHandler::signalCloseDone()",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "        protected void signalCloseDone()\n        {\n            closeFuture.get().set(null);\n\n            // We can now close the socket\n            try\n            {\n                socket.close();\n            }",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218 +\n 219 +\n 220 +\n 221 +\n 222 +\n 223 +\n 224  ",
            "        protected void signalCloseDone()\n        {\n            closeFuture.get().set(null);\n\n            // We can now close the socket\n            try\n            {\n                socket.close();\n            }\n            catch (IOException e)\n            {\n                // Erroring out while closing shouldn't happen but is not really a big deal, so just log\n                // it at DEBUG and ignore otherwise.\n                logger.debug(\"Unexpected error while closing streaming connection\", e);\n            }\n        }"
        ]
    ],
    "b087897efff811950afd38bd88c08806e2933c38": [
        [
            "SliceQueryPager::SliceQueryPager(SliceFromReadCommand,ConsistencyLevel,ClientState,boolean,PagingState)",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  ",
            "    SliceQueryPager(SliceFromReadCommand command, ConsistencyLevel consistencyLevel, ClientState cstate, boolean localQuery, PagingState state)\n    {\n        this(command, consistencyLevel, cstate, localQuery);\n\n        if (state != null)\n        {\n            lastReturned = cfm.comparator.fromByteBuffer(state.cellName);\n            restoreState(state.remaining, true);\n        }\n    }",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62 +\n  63 +\n  64 +\n  65  \n  66  \n  67  ",
            "    SliceQueryPager(SliceFromReadCommand command, ConsistencyLevel consistencyLevel, ClientState cstate, boolean localQuery, PagingState state)\n    {\n        this(command, consistencyLevel, cstate, localQuery);\n\n        if (state != null)\n        {\n            // The only case where this could be a non-CellName Composite is if it's Composites.EMPTY, but that's not\n            // valid for PagingState.cellName, so we can safely cast to CellName.\n            lastReturned = (CellName) cfm.comparator.fromByteBuffer(state.cellName);\n            restoreState(state.remaining, true);\n        }\n    }"
        ],
        [
            "SliceQueryPager::containsPreviousLast(Row)",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  ",
            "    protected boolean containsPreviousLast(Row first)\n    {\n        if (lastReturned == null)\n            return false;\n\n        Cell firstCell = isReversed() ? lastCell(first.cf) : firstNonStaticCell(first.cf);\n        // Note: we only return true if the column is the lastReturned *and* it is live. If it is deleted, it is ignored by the\n        // rest of the paging code (it hasn't been counted as live in particular) and we want to act as if it wasn't there.\n        return !first.cf.deletionInfo().isDeleted(firstCell)\n            && firstCell.isLive(timestamp())\n            && lastReturned.equals(firstCell.name());\n    }",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  ",
            "    protected boolean containsPreviousLast(Row first)\n    {\n        if (lastReturned == null)\n            return false;\n\n        Cell firstCell = isReversed() ? lastCell(first.cf) : firstNonStaticCell(first.cf);\n        CFMetaData metadata = Schema.instance.getCFMetaData(command.getKeyspace(), command.getColumnFamilyName());\n        // Note: we only return true if the column is the lastReturned *and* it is live. If it is deleted, it is ignored by the\n        // rest of the paging code (it hasn't been counted as live in particular) and we want to act as if it wasn't there.\n        return !first.cf.deletionInfo().isDeleted(firstCell)\n            && firstCell.isLive(timestamp())\n            && firstCell.name().isSameCQL3RowAs(metadata.comparator, lastReturned);\n    }"
        ],
        [
            "RangeSliceQueryPager::containsPreviousLast(Row)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  ",
            "    protected boolean containsPreviousLast(Row first)\n    {\n        if (lastReturnedKey == null || !lastReturnedKey.equals(first.key))\n            return false;\n\n        // Same as SliceQueryPager, we ignore a deleted column\n        Cell firstCell = isReversed() ? lastCell(first.cf) : firstNonStaticCell(first.cf);\n        return !first.cf.deletionInfo().isDeleted(firstCell)\n            && firstCell.isLive(timestamp())\n            && lastReturnedName.equals(firstCell.name());\n    }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104 +\n 105  ",
            "    protected boolean containsPreviousLast(Row first)\n    {\n        if (lastReturnedKey == null || !lastReturnedKey.equals(first.key))\n            return false;\n\n        // Same as SliceQueryPager, we ignore a deleted column\n        Cell firstCell = isReversed() ? lastCell(first.cf) : firstNonStaticCell(first.cf);\n        CFMetaData metadata = Schema.instance.getCFMetaData(command.keyspace, command.columnFamily);\n        return !first.cf.deletionInfo().isDeleted(firstCell)\n            && firstCell.isLive(timestamp())\n            && firstCell.name().isSameCQL3RowAs(metadata.comparator, lastReturnedName);\n    }"
        ]
    ],
    "a0076e70eed5501ac9d8c3ff41ce8018710a1585": [
        [
            "SettingsMode::Cql3Options::options()",
            " 150  \n 151  \n 152  \n 153 -\n 154  ",
            "        @Override\n        public List<? extends Option> options()\n        {\n            return Arrays.asList(mode(), useUnPrepared, api, useCompression, port, user, password, authProvider);\n        }",
            " 161  \n 162  \n 163  \n 164 +\n 165 +\n 166  ",
            "        @Override\n        public List<? extends Option> options()\n        {\n            return Arrays.asList(mode(), useUnPrepared, api, useCompression, port, user, password, authProvider,\n                                 maxPendingPerConnection, connectionsPerHost);\n        }"
        ],
        [
            "JavaDriverClient::JavaDriverClient(StressSettings,String,int,EncryptionOptions)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "    public JavaDriverClient(StressSettings settings, String host, int port, EncryptionOptions.ClientEncryptionOptions encryptionOptions)\n    {\n        this.host = host;\n        this.port = port;\n        this.username = settings.mode.username;\n        this.password = settings.mode.password;\n        this.authProvider = settings.mode.authProvider;\n        this.encryptionOptions = encryptionOptions;\n        if (settings.node.isWhiteList)\n            whitelist = new WhiteListPolicy(new DCAwareRoundRobinPolicy(), settings.node.resolveAll(settings.port.nativePort));\n        else\n            whitelist = null;\n    }",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85 +\n  86 +\n  87  ",
            "    public JavaDriverClient(StressSettings settings, String host, int port, EncryptionOptions.ClientEncryptionOptions encryptionOptions)\n    {\n        this.host = host;\n        this.port = port;\n        this.username = settings.mode.username;\n        this.password = settings.mode.password;\n        this.authProvider = settings.mode.authProvider;\n        this.encryptionOptions = encryptionOptions;\n        if (settings.node.isWhiteList)\n            whitelist = new WhiteListPolicy(new DCAwareRoundRobinPolicy(), settings.node.resolveAll(settings.port.nativePort));\n        else\n            whitelist = null;\n        connectionsPerHost = settings.mode.connectionsPerHost == null ? 8 : settings.mode.connectionsPerHost;\n\n        int maxThreadCount = 0;\n        if (settings.rate.auto)\n            maxThreadCount = settings.rate.maxThreads;\n        else\n            maxThreadCount = settings.rate.threadCount;\n\n        //Always allow enough pending requests so every thread can have a request pending\n        //See https://issues.apache.org/jira/browse/CASSANDRA-7217\n        int requestsPerConnection = (maxThreadCount / connectionsPerHost) + connectionsPerHost;\n\n        maxPendingPerConnection = settings.mode.maxPendingPerConnection == null ? Math.max(128, requestsPerConnection ) : settings.mode.maxPendingPerConnection;\n    }"
        ],
        [
            "SettingsMode::SettingsMode(GroupedOptions)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "    public SettingsMode(GroupedOptions options)\n    {\n        if (options instanceof Cql3Options)\n        {\n            cqlVersion = CqlVersion.CQL3;\n            Cql3Options opts = (Cql3Options) options;\n            api = opts.mode().displayPrefix.equals(\"native\") ? ConnectionAPI.JAVA_DRIVER_NATIVE : ConnectionAPI.THRIFT;\n            style = opts.useUnPrepared.setByUser() ? ConnectionStyle.CQL :  ConnectionStyle.CQL_PREPARED;\n            compression = ProtocolOptions.Compression.valueOf(opts.useCompression.value().toUpperCase()).name();\n            username = opts.user.value();\n            password = opts.password.value();\n            authProviderClassname = opts.authProvider.value();\n            if (authProviderClassname != null)\n            {\n                try\n                {\n                    Class<?> clazz = Class.forName(authProviderClassname);\n                    if (!AuthProvider.class.isAssignableFrom(clazz))\n                        throw new IllegalArgumentException(clazz + \" is not a valid auth provider\");\n                    // check we can instantiate it\n                    if (PlainTextAuthProvider.class.equals(clazz))\n                    {\n                        authProvider = (AuthProvider) clazz.getConstructor(String.class, String.class)\n                            .newInstance(username, password);\n                    } else\n                    {\n                        authProvider = (AuthProvider) clazz.newInstance();\n                    }\n                }\n                catch (Exception e)\n                {\n                    throw new IllegalArgumentException(\"Invalid auth provider class: \" + opts.authProvider.value(), e);\n                }\n            }\n            else\n            {\n                authProvider = null;\n            }\n        }\n        else if (options instanceof Cql3SimpleNativeOptions)\n        {\n            cqlVersion = CqlVersion.CQL3;\n            Cql3SimpleNativeOptions opts = (Cql3SimpleNativeOptions) options;\n            api = ConnectionAPI.SIMPLE_NATIVE;\n            style = opts.usePrepared.setByUser() ? ConnectionStyle.CQL_PREPARED : ConnectionStyle.CQL;\n            compression = ProtocolOptions.Compression.NONE.name();\n            username = null;\n            password = null;\n            authProvider = null;\n            authProviderClassname = null;\n        }\n        else if (options instanceof ThriftOptions)\n        {\n            ThriftOptions opts = (ThriftOptions) options;\n            cqlVersion = CqlVersion.NOCQL;\n            api = opts.smart.setByUser() ? ConnectionAPI.THRIFT_SMART : ConnectionAPI.THRIFT;\n            style = ConnectionStyle.THRIFT;\n            compression = ProtocolOptions.Compression.NONE.name();\n            username = opts.user.value();\n            password = opts.password.value();\n            authProviderClassname = null;\n            authProvider = null;\n        }\n        else\n            throw new IllegalStateException();\n    }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118  \n 119  \n 120  \n 121  ",
            "    public SettingsMode(GroupedOptions options)\n    {\n        if (options instanceof Cql3Options)\n        {\n            cqlVersion = CqlVersion.CQL3;\n            Cql3Options opts = (Cql3Options) options;\n            api = opts.mode().displayPrefix.equals(\"native\") ? ConnectionAPI.JAVA_DRIVER_NATIVE : ConnectionAPI.THRIFT;\n            style = opts.useUnPrepared.setByUser() ? ConnectionStyle.CQL :  ConnectionStyle.CQL_PREPARED;\n            compression = ProtocolOptions.Compression.valueOf(opts.useCompression.value().toUpperCase()).name();\n            username = opts.user.value();\n            password = opts.password.value();\n            maxPendingPerConnection = opts.maxPendingPerConnection.value().isEmpty() ? null : Integer.valueOf(opts.maxPendingPerConnection.value());\n            connectionsPerHost = opts.connectionsPerHost.value().isEmpty() ? null : Integer.valueOf(opts.connectionsPerHost.value());\n            authProviderClassname = opts.authProvider.value();\n            if (authProviderClassname != null)\n            {\n                try\n                {\n                    Class<?> clazz = Class.forName(authProviderClassname);\n                    if (!AuthProvider.class.isAssignableFrom(clazz))\n                        throw new IllegalArgumentException(clazz + \" is not a valid auth provider\");\n                    // check we can instantiate it\n                    if (PlainTextAuthProvider.class.equals(clazz))\n                    {\n                        authProvider = (AuthProvider) clazz.getConstructor(String.class, String.class)\n                            .newInstance(username, password);\n                    } else\n                    {\n                        authProvider = (AuthProvider) clazz.newInstance();\n                    }\n                }\n                catch (Exception e)\n                {\n                    throw new IllegalArgumentException(\"Invalid auth provider class: \" + opts.authProvider.value(), e);\n                }\n            }\n            else\n            {\n                authProvider = null;\n            }\n        }\n        else if (options instanceof Cql3SimpleNativeOptions)\n        {\n            cqlVersion = CqlVersion.CQL3;\n            Cql3SimpleNativeOptions opts = (Cql3SimpleNativeOptions) options;\n            api = ConnectionAPI.SIMPLE_NATIVE;\n            style = opts.usePrepared.setByUser() ? ConnectionStyle.CQL_PREPARED : ConnectionStyle.CQL;\n            compression = ProtocolOptions.Compression.NONE.name();\n            username = null;\n            password = null;\n            authProvider = null;\n            authProviderClassname = null;\n            maxPendingPerConnection = null;\n            connectionsPerHost = null;\n        }\n        else if (options instanceof ThriftOptions)\n        {\n            ThriftOptions opts = (ThriftOptions) options;\n            cqlVersion = CqlVersion.NOCQL;\n            api = opts.smart.setByUser() ? ConnectionAPI.THRIFT_SMART : ConnectionAPI.THRIFT;\n            style = ConnectionStyle.THRIFT;\n            compression = ProtocolOptions.Compression.NONE.name();\n            username = opts.user.value();\n            password = opts.password.value();\n            authProviderClassname = null;\n            authProvider = null;\n            maxPendingPerConnection = null;\n            connectionsPerHost = null;\n        }\n        else\n            throw new IllegalStateException();\n    }"
        ],
        [
            "JavaDriverClient::connect(ProtocolOptions)",
            "  90  \n  91  \n  92  \n  93  \n  94 -\n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "    public void connect(ProtocolOptions.Compression compression) throws Exception\n    {\n\n        PoolingOptions poolingOpts = new PoolingOptions()\n                                     .setConnectionsPerHost(HostDistance.LOCAL, 8, 8)\n                                     .setMaxRequestsPerConnection(HostDistance.LOCAL, 128)\n                                     .setNewConnectionThreshold(HostDistance.LOCAL, 100);\n\n        Cluster.Builder clusterBuilder = Cluster.builder()\n                                                .addContactPoint(host)\n                                                .withPort(port)\n                                                .withPoolingOptions(poolingOpts)\n                                                .withoutJMXReporting()\n                                                .withoutMetrics(); // The driver uses metrics 3 with conflict with our version\n        if (whitelist != null)\n            clusterBuilder.withLoadBalancingPolicy(whitelist);\n        clusterBuilder.withCompression(compression);\n        if (encryptionOptions.enabled)\n        {\n            SSLContext sslContext;\n            sslContext = SSLFactory.createSSLContext(encryptionOptions, true);\n            SSLOptions sslOptions = new SSLOptions(sslContext, encryptionOptions.cipher_suites);\n            clusterBuilder.withSSL(sslOptions);\n        }\n\n        if (authProvider != null)\n        {\n            clusterBuilder.withAuthProvider(authProvider);\n        }\n        else if (username != null)\n        {\n            clusterBuilder.withCredentials(username, password);\n        }\n\n        cluster = clusterBuilder.build();\n        Metadata metadata = cluster.getMetadata();\n        System.out.printf(\"Connected to cluster: %s%n\",\n                metadata.getClusterName());\n        for (Host host : metadata.getAllHosts())\n        {\n            System.out.printf(\"Datatacenter: %s; Host: %s; Rack: %s%n\",\n                    host.getDatacenter(), host.getAddress(), host.getRack());\n        }\n\n        session = cluster.connect();\n    }",
            " 105  \n 106  \n 107  \n 108  \n 109 +\n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "    public void connect(ProtocolOptions.Compression compression) throws Exception\n    {\n\n        PoolingOptions poolingOpts = new PoolingOptions()\n                                     .setConnectionsPerHost(HostDistance.LOCAL, connectionsPerHost, connectionsPerHost)\n                                     .setMaxRequestsPerConnection(HostDistance.LOCAL, maxPendingPerConnection)\n                                     .setNewConnectionThreshold(HostDistance.LOCAL, 100);\n\n        Cluster.Builder clusterBuilder = Cluster.builder()\n                                                .addContactPoint(host)\n                                                .withPort(port)\n                                                .withPoolingOptions(poolingOpts)\n                                                .withoutJMXReporting()\n                                                .withoutMetrics(); // The driver uses metrics 3 with conflict with our version\n        if (whitelist != null)\n            clusterBuilder.withLoadBalancingPolicy(whitelist);\n        clusterBuilder.withCompression(compression);\n        if (encryptionOptions.enabled)\n        {\n            SSLContext sslContext;\n            sslContext = SSLFactory.createSSLContext(encryptionOptions, true);\n            SSLOptions sslOptions = new SSLOptions(sslContext, encryptionOptions.cipher_suites);\n            clusterBuilder.withSSL(sslOptions);\n        }\n\n        if (authProvider != null)\n        {\n            clusterBuilder.withAuthProvider(authProvider);\n        }\n        else if (username != null)\n        {\n            clusterBuilder.withCredentials(username, password);\n        }\n\n        cluster = clusterBuilder.build();\n        Metadata metadata = cluster.getMetadata();\n        System.out.printf(\n                \"Connected to cluster: %s, max pending requests per connection %d, max connections per host %d%n\",\n                metadata.getClusterName(),\n                maxPendingPerConnection,\n                connectionsPerHost);\n        for (Host host : metadata.getAllHosts())\n        {\n            System.out.printf(\"Datatacenter: %s; Host: %s; Rack: %s%n\",\n                    host.getDatacenter(), host.getAddress(), host.getRack());\n        }\n\n        session = cluster.connect();\n    }"
        ]
    ],
    "71bac92cf1df08de194a8283f382a7950b3a78ed": [
        [
            "CompactionManager::createWriter(ColumnFamilyStore,File,long,long,SSTableReader,LifecycleTransaction)",
            " 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968 -\n 969  \n 970  ",
            "    public static SSTableWriter createWriter(ColumnFamilyStore cfs,\n                                             File compactionFileLocation,\n                                             long expectedBloomFilterSize,\n                                             long repairedAt,\n                                             SSTableReader sstable,\n                                             LifecycleTransaction txn)\n    {\n        FileUtils.createDirectory(compactionFileLocation);\n\n        return SSTableWriter.create(cfs.metadata,\n                                    Descriptor.fromFilename(cfs.getSSTablePath(compactionFileLocation)),\n                                    expectedBloomFilterSize,\n                                    repairedAt,\n                                    sstable.getSSTableLevel(),\n                                    sstable.header,\n                                    txn);\n    }",
            " 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962 +\n 963 +\n 964 +\n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971 +\n 972  \n 973  ",
            "    public static SSTableWriter createWriter(ColumnFamilyStore cfs,\n                                             File compactionFileLocation,\n                                             long expectedBloomFilterSize,\n                                             long repairedAt,\n                                             SSTableReader sstable,\n                                             LifecycleTransaction txn)\n    {\n        FileUtils.createDirectory(compactionFileLocation);\n        SerializationHeader header = sstable.header;\n        if (header == null)\n            header = SerializationHeader.make(sstable.metadata, Collections.singleton(sstable));\n\n        return SSTableWriter.create(cfs.metadata,\n                                    Descriptor.fromFilename(cfs.getSSTablePath(compactionFileLocation)),\n                                    expectedBloomFilterSize,\n                                    repairedAt,\n                                    sstable.getSSTableLevel(),\n                                    header,\n                                    txn);\n    }"
        ],
        [
            "UnfilteredDeserializer::OldFormatDeserializer::AtomIterator::hasNext()",
            " 496  \n 497  \n 498  \n 499  \n 500  \n 501 -\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509 -\n 510 -\n 511 -\n 512  \n 513  \n 514  ",
            "            public boolean hasNext()\n            {\n                if (isDone)\n                    return false;\n\n                while (next == null)\n                {\n                    next = readAtom();\n                    if (next == null)\n                    {\n                        isDone = true;\n                        return false;\n                    }\n\n                    if (tombstoneTracker.isShadowed(next))\n                        next = null;\n                }\n                return true;\n            }",
            " 497  \n 498  \n 499  \n 500  \n 501  \n 502 +\n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  ",
            "            public boolean hasNext()\n            {\n                if (isDone)\n                    return false;\n\n                if (next == null)\n                {\n                    next = readAtom();\n                    if (next == null)\n                    {\n                        isDone = true;\n                        return false;\n                    }\n                }\n                return true;\n            }"
        ],
        [
            "UnfilteredDeserializer::OldFormatDeserializer::UnfilteredIterator::hasNext()",
            " 407  \n 408  \n 409 -\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422 -\n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "            public boolean hasNext()\n            {\n                // Note that we loop on next == null because TombstoneTracker.openNew() could return null below.\n                while (next == null)\n                {\n                    if (atoms.hasNext())\n                    {\n                        // If a range tombstone closes strictly before the next row/RT, we need to return that close (or boundary) marker first.\n                        if (tombstoneTracker.hasClosingMarkerBefore(atoms.peek()))\n                        {\n                            next = tombstoneTracker.popClosingMarker();\n                        }\n                        else\n                        {\n                            LegacyLayout.LegacyAtom atom = atoms.next();\n                            next = isRow(atom) ? readRow(atom) : tombstoneTracker.openNew(atom.asRangeTombstone());\n                        }\n                    }\n                    else if (tombstoneTracker.hasOpenTombstones())\n                    {\n                        next = tombstoneTracker.popClosingMarker();\n                    }\n                    else\n                    {\n                        return false;\n                    }\n                }\n                return next != null;\n            }",
            " 407  \n 408  \n 409 +\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422 +\n 423 +\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "            public boolean hasNext()\n            {\n                // Note that we loop on next == null because TombstoneTracker.openNew() could return null below or the atom might be shadowed.\n                while (next == null)\n                {\n                    if (atoms.hasNext())\n                    {\n                        // If a range tombstone closes strictly before the next row/RT, we need to return that close (or boundary) marker first.\n                        if (tombstoneTracker.hasClosingMarkerBefore(atoms.peek()))\n                        {\n                            next = tombstoneTracker.popClosingMarker();\n                        }\n                        else\n                        {\n                            LegacyLayout.LegacyAtom atom = atoms.next();\n                            if (!tombstoneTracker.isShadowed(atom))\n                                next = isRow(atom) ? readRow(atom) : tombstoneTracker.openNew(atom.asRangeTombstone());\n                        }\n                    }\n                    else if (tombstoneTracker.hasOpenTombstones())\n                    {\n                        next = tombstoneTracker.popClosingMarker();\n                    }\n                    else\n                    {\n                        return false;\n                    }\n                }\n                return next != null;\n            }"
        ],
        [
            "UnfilteredDeserializer::OldFormatDeserializer::TombstoneTracker::isShadowed(LegacyLayout)",
            " 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  ",
            "            /**\n             * Checks if the provided atom is fully shadowed by the open tombstones of this tracker (or the partition deletion).\n             */\n            public boolean isShadowed(LegacyLayout.LegacyAtom atom)\n            {\n                long timestamp = atom.isCell() ? atom.asCell().timestamp : atom.asRangeTombstone().deletionTime.markedForDeleteAt();\n\n                if (partitionDeletion.deletes(timestamp))\n                    return true;\n\n                SortedSet<LegacyLayout.LegacyRangeTombstone> coveringTombstones = isRow(atom) ? openTombstones : openTombstones.tailSet(atom.asRangeTombstone());\n                return Iterables.any(coveringTombstones, tombstone -> tombstone.deletionTime.deletes(timestamp));\n            }",
            " 572  \n 573  \n 574  \n 575  \n 576  \n 577 +\n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  ",
            "            /**\n             * Checks if the provided atom is fully shadowed by the open tombstones of this tracker (or the partition deletion).\n             */\n            public boolean isShadowed(LegacyLayout.LegacyAtom atom)\n            {\n                assert !hasClosingMarkerBefore(atom);\n                long timestamp = atom.isCell() ? atom.asCell().timestamp : atom.asRangeTombstone().deletionTime.markedForDeleteAt();\n\n                if (partitionDeletion.deletes(timestamp))\n                    return true;\n\n                SortedSet<LegacyLayout.LegacyRangeTombstone> coveringTombstones = isRow(atom) ? openTombstones : openTombstones.tailSet(atom.asRangeTombstone());\n                return Iterables.any(coveringTombstones, tombstone -> tombstone.deletionTime.deletes(timestamp));\n            }"
        ]
    ],
    "ad7e36b8ad52c56ea33376faf57afecbb2e58630": [
        [
            "CellTest::fakeColumn(String,AbstractType)",
            "  65  \n  66  \n  67 -\n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  ",
            "    private static ColumnDefinition fakeColumn(String name, AbstractType<?> type)\n    {\n        return new ColumnDefinition(fakeMetadata.ksName,\n                                    fakeMetadata.cfName,\n                                    ColumnIdentifier.getInterned(name, false),\n                                    type,\n                                    ColumnDefinition.NO_POSITION,\n                                    ColumnDefinition.Kind.REGULAR);\n    }",
            "  63  \n  64  \n  65 +\n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  ",
            "    private static ColumnDefinition fakeColumn(String name, AbstractType<?> type)\n    {\n        return new ColumnDefinition(\"fakeKs\",\n                                    \"fakeTable\",\n                                    ColumnIdentifier.getInterned(name, false),\n                                    type,\n                                    ColumnDefinition.NO_POSITION,\n                                    ColumnDefinition.Kind.REGULAR);\n    }"
        ],
        [
            "CellTest::testValidate()",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143  \n 144 -\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155  \n 156 -\n 157  ",
            "    @Test\n    public void testValidate()\n    {\n        ColumnDefinition c;\n\n        // Valid cells\n        c = fakeColumn(\"c\", Int32Type.instance);\n        assertValid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertValid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.bytes(4)));\n\n        assertValid(BufferCell.expiring(c, 0, 4, 4, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertValid(BufferCell.expiring(c, 0, 4, 4, ByteBufferUtil.bytes(4)));\n\n        assertValid(BufferCell.tombstone(c, 0, 4));\n\n        // Invalid value (we don't all empty values for smallint)\n        c = fakeColumn(\"c\", ShortType.instance);\n        assertInvalid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        // But this should be valid even though the underlying value is an empty BB (catches bug #11618)\n        assertValid(BufferCell.tombstone(c, 0, 4));\n        // And of course, this should be valid with a proper value\n        assertValid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.bytes((short)4)));\n\n        // Invalid ttl\n        assertInvalid(BufferCell.expiring(c, 0, -4, 4, ByteBufferUtil.bytes(4)));\n        // Invalid local deletion times\n        assertInvalid(BufferCell.expiring(c, 0, 4, -4, ByteBufferUtil.bytes(4)));\n        assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, ByteBufferUtil.bytes(4)));\n\n        c = fakeColumn(\"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        // Valid cell path\n        assertValid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes(4))));\n        // Invalid cell path (int values should be 0 or 4 bytes)\n        assertInvalid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes((long)4))));\n    }",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153  \n 154 +\n 155  ",
            "    @Test\n    public void testValidate()\n    {\n        ColumnDefinition c;\n\n        // Valid cells\n        c = fakeColumn(\"c\", Int32Type.instance);\n        assertValid(BufferCell.live(c, 0, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertValid(BufferCell.live(c, 0, ByteBufferUtil.bytes(4)));\n\n        assertValid(BufferCell.expiring(c, 0, 4, 4, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        assertValid(BufferCell.expiring(c, 0, 4, 4, ByteBufferUtil.bytes(4)));\n\n        assertValid(BufferCell.tombstone(c, 0, 4));\n\n        // Invalid value (we don't all empty values for smallint)\n        c = fakeColumn(\"c\", ShortType.instance);\n        assertInvalid(BufferCell.live(c, 0, ByteBufferUtil.EMPTY_BYTE_BUFFER));\n        // But this should be valid even though the underlying value is an empty BB (catches bug #11618)\n        assertValid(BufferCell.tombstone(c, 0, 4));\n        // And of course, this should be valid with a proper value\n        assertValid(BufferCell.live(c, 0, ByteBufferUtil.bytes((short)4)));\n\n        // Invalid ttl\n        assertInvalid(BufferCell.expiring(c, 0, -4, 4, ByteBufferUtil.bytes(4)));\n        // Invalid local deletion times\n        assertInvalid(BufferCell.expiring(c, 0, 4, -4, ByteBufferUtil.bytes(4)));\n        assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, ByteBufferUtil.bytes(4)));\n\n        c = fakeColumn(\"c\", MapType.getInstance(Int32Type.instance, Int32Type.instance, true));\n        // Valid cell path\n        assertValid(BufferCell.live(c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes(4))));\n        // Invalid cell path (int values should be 0 or 4 bytes)\n        assertInvalid(BufferCell.live(c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes((long)4))));\n    }"
        ]
    ],
    "7eb464734b0732ab3c6cd2a5c3409085c81f95a7": [
        [
            "LogTransaction::removeUnfinishedLeftovers(CFMetaData)",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 -\n 389  ",
            "    /**\n     * Called on startup to scan existing folders for any unfinished leftovers of\n     * operations that were ongoing when the process exited. Also called by the standalone\n     * sstableutil tool when the cleanup option is specified, @see StandaloneSSTableUtil.\n     *\n     */\n    static void removeUnfinishedLeftovers(CFMetaData metadata)\n    {\n        removeUnfinishedLeftovers(new Directories(metadata).getCFDirectories());\n    }",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 +\n 390  ",
            "    /**\n     * Called on startup to scan existing folders for any unfinished leftovers of\n     * operations that were ongoing when the process exited. Also called by the standalone\n     * sstableutil tool when the cleanup option is specified, @see StandaloneSSTableUtil.\n     *\n     */\n    static void removeUnfinishedLeftovers(CFMetaData metadata)\n    {\n        removeUnfinishedLeftovers(new Directories(metadata, ColumnFamilyStore.getInitialDirectories()).getCFDirectories());\n    }"
        ],
        [
            "ColumnFamilyStore::scrubDataDirectories(CFMetaData)",
            " 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583 -\n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  ",
            "    /**\n     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files\n     * and compacted sstables. Files that cannot be recognized will be ignored.\n     */\n    public static void scrubDataDirectories(CFMetaData metadata)\n    {\n        Directories directories = new Directories(metadata);\n\n         // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)\n        clearEphemeralSnapshots(directories);\n\n        directories.removeTemporaryDirectories();\n\n        logger.trace(\"Removing temporary or obsoleted files from unfinished operations for table {}\", metadata.cfName);\n        LifecycleTransaction.removeUnfinishedLeftovers(metadata);\n\n        logger.trace(\"Further extra check for orphan sstable files for {}\", metadata.cfName);\n        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister(Directories.OnTxnErr.IGNORE).list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            Set<Component> components = sstableFiles.getValue();\n\n            for (File tmpFile : desc.getTemporaryFiles())\n                tmpFile.delete();\n\n            File dataFile = new File(desc.filenameFor(Component.DATA));\n            if (components.contains(Component.DATA) && dataFile.length() > 0)\n                // everything appears to be in order... moving on.\n                continue;\n\n            // missing the DATA file! all components are orphaned\n            logger.warn(\"Removing orphans for {}: {}\", desc, components);\n            for (Component component : components)\n            {\n                File file = new File(desc.filenameFor(component));\n                if (file.exists())\n                    FileUtils.deleteWithConfirm(desc.filenameFor(component));\n            }\n        }\n\n        // cleanup incomplete saved caches\n        Pattern tmpCacheFilePattern = Pattern.compile(metadata.ksName + \"-\" + metadata.cfName + \"-(Key|Row)Cache.*\\\\.tmp$\");\n        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());\n\n        if (dir.exists())\n        {\n            assert dir.isDirectory();\n            for (File file : dir.listFiles())\n                if (tmpCacheFilePattern.matcher(file.getName()).matches())\n                    if (!file.delete())\n                        logger.warn(\"could not delete {}\", file.getAbsolutePath());\n        }\n\n        // also clean out any index leftovers.\n        for (IndexMetadata index : metadata.getIndexes())\n            if (!index.isCustom())\n            {\n                CFMetaData indexMetadata = CassandraIndex.indexCfsMetadata(metadata, index);\n                scrubDataDirectories(indexMetadata);\n            }\n    }",
            " 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583 +\n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  ",
            "    /**\n     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files\n     * and compacted sstables. Files that cannot be recognized will be ignored.\n     */\n    public static void scrubDataDirectories(CFMetaData metadata)\n    {\n        Directories directories = new Directories(metadata, initialDirectories);\n\n         // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)\n        clearEphemeralSnapshots(directories);\n\n        directories.removeTemporaryDirectories();\n\n        logger.trace(\"Removing temporary or obsoleted files from unfinished operations for table {}\", metadata.cfName);\n        LifecycleTransaction.removeUnfinishedLeftovers(metadata);\n\n        logger.trace(\"Further extra check for orphan sstable files for {}\", metadata.cfName);\n        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister(Directories.OnTxnErr.IGNORE).list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            Set<Component> components = sstableFiles.getValue();\n\n            for (File tmpFile : desc.getTemporaryFiles())\n                tmpFile.delete();\n\n            File dataFile = new File(desc.filenameFor(Component.DATA));\n            if (components.contains(Component.DATA) && dataFile.length() > 0)\n                // everything appears to be in order... moving on.\n                continue;\n\n            // missing the DATA file! all components are orphaned\n            logger.warn(\"Removing orphans for {}: {}\", desc, components);\n            for (Component component : components)\n            {\n                File file = new File(desc.filenameFor(component));\n                if (file.exists())\n                    FileUtils.deleteWithConfirm(desc.filenameFor(component));\n            }\n        }\n\n        // cleanup incomplete saved caches\n        Pattern tmpCacheFilePattern = Pattern.compile(metadata.ksName + \"-\" + metadata.cfName + \"-(Key|Row)Cache.*\\\\.tmp$\");\n        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());\n\n        if (dir.exists())\n        {\n            assert dir.isDirectory();\n            for (File file : dir.listFiles())\n                if (tmpCacheFilePattern.matcher(file.getName()).matches())\n                    if (!file.delete())\n                        logger.warn(\"could not delete {}\", file.getAbsolutePath());\n        }\n\n        // also clean out any index leftovers.\n        for (IndexMetadata index : metadata.getIndexes())\n            if (!index.isCustom())\n            {\n                CFMetaData indexMetadata = CassandraIndex.indexCfsMetadata(metadata, index);\n                scrubDataDirectories(indexMetadata);\n            }\n    }"
        ],
        [
            "StandaloneSSTableUtil::listFiles(Options,CFMetaData,OutputHandler)",
            "  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "    private static void listFiles(Options options, CFMetaData metadata, OutputHandler handler) throws IOException\n    {\n        Directories directories = new Directories(metadata);\n\n        for (File dir : directories.getCFDirectories())\n        {\n            for (File file : LifecycleTransaction.getFiles(dir.toPath(), getFilter(options), Directories.OnTxnErr.THROW))\n                handler.output(file.getCanonicalPath());\n        }\n    }",
            "  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "    private static void listFiles(Options options, CFMetaData metadata, OutputHandler handler) throws IOException\n    {\n        Directories directories = new Directories(metadata, ColumnFamilyStore.getInitialDirectories());\n\n        for (File dir : directories.getCFDirectories())\n        {\n            for (File file : LifecycleTransaction.getFiles(dir.toPath(), getFilter(options), Directories.OnTxnErr.THROW))\n                handler.output(file.getCanonicalPath());\n        }\n    }"
        ]
    ],
    "8206839328e665108e33de8b48926942d46cf12e": [
        [
            "DatabaseDescriptor::clientInitialization(boolean)",
            " 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "    /**\n     * Initializes this class as a client, which means that just an empty configuration will\n     * be used.\n     *\n     * @param failIfDaemonOrTool if {@code true} and a call to {@link #daemonInitialization()} or\n     *                           {@link #toolInitialization()} has been performed before, an\n     *                           {@link AssertionError} will be thrown.\n     */\n    public static void clientInitialization(boolean failIfDaemonOrTool)\n    {\n        if (!failIfDaemonOrTool && (daemonInitialized || toolInitialized))\n        {\n            return;\n        }\n        else\n        {\n            if (daemonInitialized)\n                throw new AssertionError(\"daemonInitialization() already called\");\n            if (toolInitialized)\n                throw new AssertionError(\"toolInitialization() already called\");\n        }\n\n        if (clientInitialized)\n            return;\n        clientInitialized = true;\n\n        Config.setClientMode(true);\n        conf = new Config();\n    }",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223 +\n 224  ",
            "    /**\n     * Initializes this class as a client, which means that just an empty configuration will\n     * be used.\n     *\n     * @param failIfDaemonOrTool if {@code true} and a call to {@link #daemonInitialization()} or\n     *                           {@link #toolInitialization()} has been performed before, an\n     *                           {@link AssertionError} will be thrown.\n     */\n    public static void clientInitialization(boolean failIfDaemonOrTool)\n    {\n        if (!failIfDaemonOrTool && (daemonInitialized || toolInitialized))\n        {\n            return;\n        }\n        else\n        {\n            if (daemonInitialized)\n                throw new AssertionError(\"daemonInitialization() already called\");\n            if (toolInitialized)\n                throw new AssertionError(\"toolInitialization() already called\");\n        }\n\n        if (clientInitialized)\n            return;\n        clientInitialized = true;\n\n        Config.setClientMode(true);\n        conf = new Config();\n        diskOptimizationStrategy = new SpinningDiskOptimizationStrategy();\n        partitioner = Murmur3Partitioner.instance;\n    }"
        ]
    ],
    "af37489092ca90bca336538adad02fb5ba859945": [
        [
            "LocalSessions::handlePrepareMessage(InetAddress,PrepareConsistentRequest)",
            " 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571 -\n 572 -\n 573  \n 574  \n 575  \n 576  ",
            "    /**\n     * The PrepareConsistentRequest effectively promotes the parent repair session to a consistent\n     * incremental session, and begins the 'pending anti compaction' which moves all sstable data\n     * that is to be repaired into it's own silo, preventing it from mixing with other data.\n     *\n     * No response is sent to the repair coordinator until the pending anti compaction has completed\n     * successfully. If the pending anti compaction fails, a failure message is sent to the coordinator,\n     * cancelling the session.\n     */\n    public void handlePrepareMessage(InetAddress from, PrepareConsistentRequest request)\n    {\n        logger.trace(\"received {} from {}\", request, from);\n        UUID sessionID = request.parentSession;\n        InetAddress coordinator = request.coordinator;\n        Set<InetAddress> peers = request.participants;\n\n        ActiveRepairService.ParentRepairSession parentSession;\n        try\n        {\n            parentSession = getParentRepairSession(sessionID);\n        }\n        catch (Throwable e)\n        {\n            logger.trace(\"Error retrieving ParentRepairSession for session {}, responding with failure\", sessionID);\n            sendMessage(coordinator, new FailSession(sessionID));\n            return;\n        }\n\n        LocalSession session = createSessionUnsafe(sessionID, parentSession, peers);\n        putSessionUnsafe(session);\n        logger.info(\"Beginning local incremental repair session {}\", session);\n\n        ExecutorService executor = Executors.newFixedThreadPool(parentSession.getColumnFamilyStores().size());\n\n        ListenableFuture pendingAntiCompaction = submitPendingAntiCompaction(session, executor);\n        Futures.addCallback(pendingAntiCompaction, new FutureCallback()\n        {\n            public void onSuccess(@Nullable Object result)\n            {\n                logger.debug(\"Prepare phase for incremental repair session {} completed\", sessionID);\n                setStateAndSave(session, PREPARED);\n                sendMessage(coordinator, new PrepareConsistentResponse(sessionID, getBroadcastAddress(), true));\n                executor.shutdown();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                logger.error(String.format(\"Prepare phase for incremental repair session %s failed\", sessionID), t);\n                failSession(sessionID);\n                executor.shutdown();\n            }\n        });\n    }",
            " 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571 +\n 572 +\n 573 +\n 574 +\n 575 +\n 576 +\n 577 +\n 578 +\n 579 +\n 580 +\n 581 +\n 582 +\n 583 +\n 584 +\n 585 +\n 586  \n 587  \n 588  \n 589  ",
            "    /**\n     * The PrepareConsistentRequest effectively promotes the parent repair session to a consistent\n     * incremental session, and begins the 'pending anti compaction' which moves all sstable data\n     * that is to be repaired into it's own silo, preventing it from mixing with other data.\n     *\n     * No response is sent to the repair coordinator until the pending anti compaction has completed\n     * successfully. If the pending anti compaction fails, a failure message is sent to the coordinator,\n     * cancelling the session.\n     */\n    public void handlePrepareMessage(InetAddress from, PrepareConsistentRequest request)\n    {\n        logger.trace(\"received {} from {}\", request, from);\n        UUID sessionID = request.parentSession;\n        InetAddress coordinator = request.coordinator;\n        Set<InetAddress> peers = request.participants;\n\n        ActiveRepairService.ParentRepairSession parentSession;\n        try\n        {\n            parentSession = getParentRepairSession(sessionID);\n        }\n        catch (Throwable e)\n        {\n            logger.trace(\"Error retrieving ParentRepairSession for session {}, responding with failure\", sessionID);\n            sendMessage(coordinator, new FailSession(sessionID));\n            return;\n        }\n\n        LocalSession session = createSessionUnsafe(sessionID, parentSession, peers);\n        putSessionUnsafe(session);\n        logger.info(\"Beginning local incremental repair session {}\", session);\n\n        ExecutorService executor = Executors.newFixedThreadPool(parentSession.getColumnFamilyStores().size());\n\n        ListenableFuture pendingAntiCompaction = submitPendingAntiCompaction(session, executor);\n        Futures.addCallback(pendingAntiCompaction, new FutureCallback()\n        {\n            public void onSuccess(@Nullable Object result)\n            {\n                logger.debug(\"Prepare phase for incremental repair session {} completed\", sessionID);\n                setStateAndSave(session, PREPARED);\n                sendMessage(coordinator, new PrepareConsistentResponse(sessionID, getBroadcastAddress(), true));\n                executor.shutdown();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                logger.error(\"Prepare phase for incremental repair session {} failed\", sessionID, t);\n                if (t instanceof PendingAntiCompaction.SSTableAcquisitionException)\n                {\n                    logger.warn(\"Prepare phase for incremental repair session {} was unable to \" +\n                                \"acquire exclusive access to the neccesary sstables. \" +\n                                \"This is usually caused by running multiple incremental repairs on nodes that share token ranges\",\n                                sessionID);\n\n                }\n                else\n                {\n                    logger.error(\"Prepare phase for incremental repair session {} failed\", sessionID, t);\n                }\n                sendMessage(coordinator, new PrepareConsistentResponse(sessionID, getBroadcastAddress(), false));\n                failSession(sessionID, false);\n                executor.shutdown();\n            }\n        });\n    }"
        ],
        [
            "CoordinatorSession::fail()",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    public synchronized void fail()\n    {\n        logger.info(\"Incremental repair session {} failed\", sessionID);\n        FailSession message = new FailSession(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            if (participantStates.get(participant) != State.FAILED)\n            {\n                sendMessage(participant, message);\n            }\n        }\n        setAll(State.FAILED);\n    }",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243 +\n 244 +\n 245 +\n 246 +\n 247  ",
            "    public synchronized void fail()\n    {\n        logger.info(\"Incremental repair session {} failed\", sessionID);\n        FailSession message = new FailSession(sessionID);\n        for (final InetAddress participant : participants)\n        {\n            if (participantStates.get(participant) != State.FAILED)\n            {\n                sendMessage(participant, message);\n            }\n        }\n        setAll(State.FAILED);\n\n        String exceptionMsg = String.format(\"Incremental repair session %s has failed\", sessionID);\n        finalizeProposeFuture.setException(new RuntimeException(exceptionMsg));\n        prepareFuture.setException(new RuntimeException(exceptionMsg));\n    }"
        ],
        [
            "LocalSessionTest::prepareAntiCompactFailure()",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307  \n 308  ",
            "    /**\n     * If anti compactionn fails, we should fail the session locally,\n     * and send a failure message back to the coordinator\n     */\n    @Test\n    public void prepareAntiCompactFailure()\n    {\n        UUID sessionID = registerSession();\n        InstrumentedLocalSessions sessions = new InstrumentedLocalSessions();\n        sessions.start();\n\n        // replacing future so we can inspect state before and after anti compaction callback\n        sessions.pendingAntiCompactionFuture = SettableFuture.create();\n        Assert.assertFalse(sessions.submitPendingAntiCompactionCalled);\n        sessions.handlePrepareMessage(PARTICIPANT1, new PrepareConsistentRequest(sessionID, COORDINATOR, PARTICIPANTS));\n        Assert.assertTrue(sessions.submitPendingAntiCompactionCalled);\n        Assert.assertTrue(sessions.sentMessages.isEmpty());\n\n        // anti compaction hasn't finished yet, so state in memory and on disk should be PREPARING\n        LocalSession session = sessions.getSession(sessionID);\n        Assert.assertNotNull(session);\n        Assert.assertEquals(PREPARING, session.getState());\n        Assert.assertEquals(session, sessions.loadUnsafe(sessionID));\n\n        // anti compaction has now finished, so state in memory and on disk should be PREPARED\n        sessions.pendingAntiCompactionFuture.setException(new RuntimeException());\n        session = sessions.getSession(sessionID);\n        Assert.assertNotNull(session);\n        Assert.assertEquals(FAILED, session.getState());\n        Assert.assertEquals(session, sessions.loadUnsafe(sessionID));\n\n        // ...and we should have sent a success message back to the coordinator\n        assertMessagesSent(sessions, COORDINATOR, new FailSession(sessionID));\n\n    }",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308  ",
            "    /**\n     * If anti compactionn fails, we should fail the session locally,\n     * and send a failure message back to the coordinator\n     */\n    @Test\n    public void prepareAntiCompactFailure()\n    {\n        UUID sessionID = registerSession();\n        InstrumentedLocalSessions sessions = new InstrumentedLocalSessions();\n        sessions.start();\n\n        // replacing future so we can inspect state before and after anti compaction callback\n        sessions.pendingAntiCompactionFuture = SettableFuture.create();\n        Assert.assertFalse(sessions.submitPendingAntiCompactionCalled);\n        sessions.handlePrepareMessage(PARTICIPANT1, new PrepareConsistentRequest(sessionID, COORDINATOR, PARTICIPANTS));\n        Assert.assertTrue(sessions.submitPendingAntiCompactionCalled);\n        Assert.assertTrue(sessions.sentMessages.isEmpty());\n\n        // anti compaction hasn't finished yet, so state in memory and on disk should be PREPARING\n        LocalSession session = sessions.getSession(sessionID);\n        Assert.assertNotNull(session);\n        Assert.assertEquals(PREPARING, session.getState());\n        Assert.assertEquals(session, sessions.loadUnsafe(sessionID));\n\n        // anti compaction has now finished, so state in memory and on disk should be PREPARED\n        sessions.pendingAntiCompactionFuture.setException(new RuntimeException());\n        session = sessions.getSession(sessionID);\n        Assert.assertNotNull(session);\n        Assert.assertEquals(FAILED, session.getState());\n        Assert.assertEquals(session, sessions.loadUnsafe(sessionID));\n\n        // ...and we should have sent a success message back to the coordinator\n        assertMessagesSent(sessions, COORDINATOR, new PrepareConsistentResponse(sessionID, PARTICIPANT1, false));\n\n    }"
        ],
        [
            "PendingAntiCompaction::AcquisitionCallback::apply(List)",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "        public ListenableFuture apply(List<AcquireResult> results) throws Exception\n        {\n            if (Iterables.any(results, t -> t == null))\n            {\n                // Release all sstables, and report failure back to coordinator\n                for (AcquireResult result : results)\n                {\n                    if (result != null)\n                    {\n                        logger.info(\"Releasing acquired sstables for {}.{}\", result.cfs.metadata.keyspace, result.cfs.metadata.name);\n                        result.abort();\n                    }\n                }\n                return Futures.immediateFailedFuture(new RuntimeException(\"unable to acquire sstables\"));\n            }\n            else\n            {\n                List<ListenableFuture<?>> pendingAntiCompactions = new ArrayList<>(results.size());\n                for (AcquireResult result : results)\n                {\n                    if (result.txn != null)\n                    {\n                        ListenableFuture<?> future = submitPendingAntiCompaction(result);\n                        pendingAntiCompactions.add(future);\n                    }\n                }\n\n                return Futures.allAsList(pendingAntiCompactions);\n            }\n        }",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "        public ListenableFuture apply(List<AcquireResult> results) throws Exception\n        {\n            if (Iterables.any(results, t -> t == null))\n            {\n                // Release all sstables, and report failure back to coordinator\n                for (AcquireResult result : results)\n                {\n                    if (result != null)\n                    {\n                        logger.info(\"Releasing acquired sstables for {}.{}\", result.cfs.metadata.keyspace, result.cfs.metadata.name);\n                        result.abort();\n                    }\n                }\n                return Futures.immediateFailedFuture(new SSTableAcquisitionException());\n            }\n            else\n            {\n                List<ListenableFuture<?>> pendingAntiCompactions = new ArrayList<>(results.size());\n                for (AcquireResult result : results)\n                {\n                    if (result.txn != null)\n                    {\n                        ListenableFuture<?> future = submitPendingAntiCompaction(result);\n                        pendingAntiCompactions.add(future);\n                    }\n                }\n\n                return Futures.allAsList(pendingAntiCompactions);\n            }\n        }"
        ]
    ],
    "f970de55746dd074843d8cfd5385b12acda56ee9": [
        [
            "CqlRecordWriter::RangeClient::run()",
            " 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 -\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        if (Thread.interrupted()) {\n                            lastException = new IOException(e);\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n                            \n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }",
            " 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326 +\n 327 +\n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 +\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "        /**\n         * Loops collecting cql binded variable values from the queue and sending to Cassandra\n         */\n        public void run()\n        {\n            outer:\n            while (run || !queue.isEmpty())\n            {\n                List<ByteBuffer> bindVariables;\n                try\n                {\n                    bindVariables = queue.take();\n                }\n                catch (InterruptedException e)\n                {\n                    // re-check loop condition after interrupt\n                    continue;\n                }\n\n                ListIterator<InetAddress> iter = endpoints.listIterator();\n                while (true)\n                {\n                    // send the mutation to the last-used endpoint.  first time through, this will NPE harmlessly.\n\n                    // attempt to connect to a different endpoint\n                    try\n                    {\n                        InetAddress address = iter.next();\n                        String host = address.getHostName();\n                        client = CqlConfigHelper.getOutputCluster(host, conf).connect();\n                    }\n                    catch (Exception e)\n                    {\n                        //If connection died due to Interrupt, just try connecting to the endpoint again.\n                        //There are too many ways for the Thread.interrupted() state to be cleared, so\n                        //we can't rely on that here. Until the java driver gives us a better way of knowing\n                        //that this exception came from an InterruptedException, this is the best solution.\n                        if (e instanceof DriverException && e.getMessage().contains(\"Connection thread interrupted\")) {\n                            lastException = new IOException(e);\n                            iter.previous();\n                        }\n                        closeInternal();\n\n                        // Most exceptions mean something unexpected went wrong to that endpoint, so\n                        // we should try again to another.  Other exceptions (auth or invalid request) are fatal.\n                        if ((e instanceof AuthenticationException || e instanceof InvalidQueryException) || !iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                        continue;\n                    }\n\n                    try\n                    {\n                        int i = 0;\n                        PreparedStatement statement = preparedStatement(client);\n                        while (bindVariables != null)\n                        {\n                            BoundStatement boundStatement = new BoundStatement(statement);\n                            for (int columnPosition = 0; columnPosition < bindVariables.size(); columnPosition++)\n                            {\n                                boundStatement.setBytesUnsafe(columnPosition, bindVariables.get(columnPosition));\n                            }\n                            client.execute(boundStatement);\n                            i++;\n                            \n                            if (i >= batchThreshold)\n                                break;\n                            bindVariables = queue.poll();\n                        }\n                        break;\n                    }\n                    catch (Exception e)\n                    {\n                        closeInternal();\n                        if (!iter.hasNext())\n                        {\n                            lastException = new IOException(e);\n                            break outer;\n                        }\n                    }\n\n                }\n            }\n            // close all our connections once we are done.\n            closeInternal();\n        }"
        ]
    ],
    "bb9aa098813b7f047f450086e18a78b149bb5349": [
        [
            "LoaderOptions::getCmdLineOptions()",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 -\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  ",
            "    private static CmdLineOptions getCmdLineOptions()\n    {\n        CmdLineOptions options = new CmdLineOptions();\n        options.addOption(\"v\", VERBOSE_OPTION, \"verbose output\");\n        options.addOption(\"h\", HELP_OPTION, \"display this help message\");\n        options.addOption(null, NOPROGRESS_OPTION, \"don't display progress\");\n        options.addOption(\"i\", IGNORE_NODES_OPTION, \"NODES\", \"don't stream to this (comma separated) list of nodes\");\n        options.addOption(\"d\", INITIAL_HOST_ADDRESS_OPTION, \"initial hosts\", \"Required. try to connect to these hosts (comma separated) initially for ring information\");\n        options.addOption(\"p\",  NATIVE_PORT_OPTION, \"native transport port\", \"port used for native connection (default 9042)\");\n        options.addOption(\"sp\",  STORAGE_PORT_OPTION, \"storage port\", \"port used for internode communication (default 7000)\");\n        options.addOption(\"ssp\",  SSL_STORAGE_PORT_OPTION, \"ssl storage port\", \"port used for TLS internode communication (default 7001)\");\n        options.addOption(\"t\", THROTTLE_MBITS, \"throttle\", \"throttle speed in Mbits (default unlimited)\");\n        options.addOption(\"idct\", INTER_DC_THROTTLE_MBITS, \"inter-dc-throttle\", \"inter-datacenter throttle speed in Mbits (default unlimited)\");\n        options.addOption(\"u\", USER_OPTION, \"username\", \"username for cassandra authentication\");\n        options.addOption(\"pw\", PASSWD_OPTION, \"password\", \"password for cassandra authentication\");\n        options.addOption(\"ap\", AUTH_PROVIDER_OPTION, \"auth provider\", \"custom AuthProvider class name for cassandra authentication\");\n        options.addOption(\"cph\", CONNECTIONS_PER_HOST, \"connectionsPerHost\", \"number of concurrent connections-per-host.\");\n        // ssl connection-related options\n        options.addOption(\"ts\", SSL_TRUSTSTORE, \"TRUSTSTORE\", \"Client SSL: full path to truststore\");\n        options.addOption(\"tspw\", SSL_TRUSTSTORE_PW, \"TRUSTSTORE-PASSWORD\", \"Client SSL: password of the truststore\");\n        options.addOption(\"ks\", SSL_KEYSTORE, \"KEYSTORE\", \"Client SSL: full path to keystore\");\n        options.addOption(\"kspw\", SSL_KEYSTORE_PW, \"KEYSTORE-PASSWORD\", \"Client SSL: password of the keystore\");\n        options.addOption(\"prtcl\", SSL_PROTOCOL, \"PROTOCOL\", \"Client SSL: connections protocol to use (default: TLS)\");\n        options.addOption(\"alg\", SSL_ALGORITHM, \"ALGORITHM\", \"Client SSL: algorithm (default: SunX509)\");\n        options.addOption(\"st\", SSL_STORE_TYPE, \"STORE-TYPE\", \"Client SSL: type of store\");\n        options.addOption(\"ciphers\", SSL_CIPHER_SUITES, \"CIPHER-SUITES\", \"Client SSL: comma-separated list of encryption suites to use\");\n        options.addOption(\"f\", CONFIG_PATH, \"path to config file\", \"cassandra.yaml file path for streaming throughput and client/server SSL.\");\n        options.addOption(\"spd\", ALLOW_SERVER_PORT_DISCOVERY_OPTION, \"allow server port discovery\", \"Use ports published by server to decide how to connect. With SSL requires StartTLS to be used.\");\n        return options;\n    }",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 +\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  ",
            "    private static CmdLineOptions getCmdLineOptions()\n    {\n        CmdLineOptions options = new CmdLineOptions();\n        options.addOption(\"v\", VERBOSE_OPTION, \"verbose output\");\n        options.addOption(\"h\", HELP_OPTION, \"display this help message\");\n        options.addOption(null, NOPROGRESS_OPTION, \"don't display progress\");\n        options.addOption(\"i\", IGNORE_NODES_OPTION, \"NODES\", \"don't stream to this (comma separated) list of nodes\");\n        options.addOption(\"d\", INITIAL_HOST_ADDRESS_OPTION, \"initial hosts\", \"Required. try to connect to these hosts (comma separated) initially for ring information\");\n        options.addOption(\"p\",  NATIVE_PORT_OPTION, \"native transport port\", \"port used for native connection (default 9042)\");\n        options.addOption(\"sp\",  STORAGE_PORT_OPTION, \"storage port\", \"port used for internode communication (default 7000)\");\n        options.addOption(\"ssp\",  SSL_STORAGE_PORT_OPTION, \"ssl storage port\", \"port used for TLS internode communication (default 7001)\");\n        options.addOption(\"t\", THROTTLE_MBITS, \"throttle\", \"throttle speed in Mbits (default unlimited)\");\n        options.addOption(\"idct\", INTER_DC_THROTTLE_MBITS, \"inter-dc-throttle\", \"inter-datacenter throttle speed in Mbits (default unlimited)\");\n        options.addOption(\"u\", USER_OPTION, \"username\", \"username for cassandra authentication\");\n        options.addOption(\"pw\", PASSWD_OPTION, \"password\", \"password for cassandra authentication\");\n        options.addOption(\"ap\", AUTH_PROVIDER_OPTION, \"auth provider\", \"custom AuthProvider class name for cassandra authentication\");\n        options.addOption(\"cph\", CONNECTIONS_PER_HOST, \"connectionsPerHost\", \"number of concurrent connections-per-host.\");\n        // ssl connection-related options\n        options.addOption(\"ts\", SSL_TRUSTSTORE, \"TRUSTSTORE\", \"Client SSL: full path to truststore\");\n        options.addOption(\"tspw\", SSL_TRUSTSTORE_PW, \"TRUSTSTORE-PASSWORD\", \"Client SSL: password of the truststore\");\n        options.addOption(\"ks\", SSL_KEYSTORE, \"KEYSTORE\", \"Client SSL: full path to keystore\");\n        options.addOption(\"kspw\", SSL_KEYSTORE_PW, \"KEYSTORE-PASSWORD\", \"Client SSL: password of the keystore\");\n        options.addOption(\"prtcl\", SSL_PROTOCOL, \"PROTOCOL\", \"Client SSL: connections protocol to use (default: TLS)\");\n        options.addOption(\"alg\", SSL_ALGORITHM, \"ALGORITHM\", \"Client SSL: algorithm\");\n        options.addOption(\"st\", SSL_STORE_TYPE, \"STORE-TYPE\", \"Client SSL: type of store\");\n        options.addOption(\"ciphers\", SSL_CIPHER_SUITES, \"CIPHER-SUITES\", \"Client SSL: comma-separated list of encryption suites to use\");\n        options.addOption(\"f\", CONFIG_PATH, \"path to config file\", \"cassandra.yaml file path for streaming throughput and client/server SSL.\");\n        options.addOption(\"spd\", ALLOW_SERVER_PORT_DISCOVERY_OPTION, \"allow server port discovery\", \"Use ports published by server to decide how to connect. With SSL requires StartTLS to be used.\");\n        return options;\n    }"
        ],
        [
            "SSLFactory::buildTrustManagerFactory(EncryptionOptions)",
            " 170  \n 171  \n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "    static TrustManagerFactory buildTrustManagerFactory(EncryptionOptions options) throws IOException\n    {\n        try (InputStream tsf = Files.newInputStream(Paths.get(options.truststore)))\n        {\n            TrustManagerFactory tmf = TrustManagerFactory.getInstance(options.algorithm);\n            KeyStore ts = KeyStore.getInstance(options.store_type);\n            ts.load(tsf, options.truststore_password.toCharArray());\n            tmf.init(ts);\n            return tmf;\n        }\n        catch (Exception e)\n        {\n            throw new IOException(\"failed to build trust manager store for secure connections\", e);\n        }\n    }",
            " 170  \n 171  \n 172  \n 173  \n 174 +\n 175 +\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "    static TrustManagerFactory buildTrustManagerFactory(EncryptionOptions options) throws IOException\n    {\n        try (InputStream tsf = Files.newInputStream(Paths.get(options.truststore)))\n        {\n            TrustManagerFactory tmf = TrustManagerFactory.getInstance(\n                options.algorithm == null ? TrustManagerFactory.getDefaultAlgorithm() : options.algorithm);\n            KeyStore ts = KeyStore.getInstance(options.store_type);\n            ts.load(tsf, options.truststore_password.toCharArray());\n            tmf.init(ts);\n            return tmf;\n        }\n        catch (Exception e)\n        {\n            throw new IOException(\"failed to build trust manager store for secure connections\", e);\n        }\n    }"
        ],
        [
            "SSLFactory::buildKeyManagerFactory(EncryptionOptions)",
            " 186  \n 187  \n 188  \n 189  \n 190 -\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  ",
            "    static KeyManagerFactory buildKeyManagerFactory(EncryptionOptions options) throws IOException\n    {\n        try (InputStream ksf = Files.newInputStream(Paths.get(options.keystore)))\n        {\n            KeyManagerFactory kmf = KeyManagerFactory.getInstance(options.algorithm);\n            KeyStore ks = KeyStore.getInstance(options.store_type);\n            ks.load(ksf, options.keystore_password.toCharArray());\n            if (!checkedExpiry)\n            {\n                for (Enumeration<String> aliases = ks.aliases(); aliases.hasMoreElements(); )\n                {\n                    String alias = aliases.nextElement();\n                    if (ks.getCertificate(alias).getType().equals(\"X.509\"))\n                    {\n                        Date expires = ((X509Certificate) ks.getCertificate(alias)).getNotAfter();\n                        if (expires.before(new Date()))\n                            logger.warn(\"Certificate for {} expired on {}\", alias, expires);\n                    }\n                }\n                checkedExpiry = true;\n            }\n            kmf.init(ks, options.keystore_password.toCharArray());\n            return kmf;\n        }\n        catch (Exception e)\n        {\n            throw new IOException(\"failed to build trust manager store for secure connections\", e);\n        }\n    }",
            " 187  \n 188  \n 189  \n 190  \n 191 +\n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  ",
            "    static KeyManagerFactory buildKeyManagerFactory(EncryptionOptions options) throws IOException\n    {\n        try (InputStream ksf = Files.newInputStream(Paths.get(options.keystore)))\n        {\n            KeyManagerFactory kmf = KeyManagerFactory.getInstance(\n                options.algorithm == null ? KeyManagerFactory.getDefaultAlgorithm() : options.algorithm);\n            KeyStore ks = KeyStore.getInstance(options.store_type);\n            ks.load(ksf, options.keystore_password.toCharArray());\n            if (!checkedExpiry)\n            {\n                for (Enumeration<String> aliases = ks.aliases(); aliases.hasMoreElements(); )\n                {\n                    String alias = aliases.nextElement();\n                    if (ks.getCertificate(alias).getType().equals(\"X.509\"))\n                    {\n                        Date expires = ((X509Certificate) ks.getCertificate(alias)).getNotAfter();\n                        if (expires.before(new Date()))\n                            logger.warn(\"Certificate for {} expired on {}\", alias, expires);\n                    }\n                }\n                checkedExpiry = true;\n            }\n            kmf.init(ks, options.keystore_password.toCharArray());\n            return kmf;\n        }\n        catch (Exception e)\n        {\n            throw new IOException(\"failed to build trust manager store for secure connections\", e);\n        }\n    }"
        ]
    ],
    "9b10928c159317160fb3049727679a48232b6041": [
        [
            "SSTableIdentityIterator::hasNext()",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "    public boolean hasNext()\n    {\n        try\n        {\n            return atomIterator.hasNext();\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            else\n                throw e;\n        }\n    }",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140 +\n 141 +\n 142  \n 143 +\n 144  \n 145 +\n 146  \n 147 +\n 148  \n 149  ",
            "    public boolean hasNext()\n    {\n        try\n        {\n            return atomIterator.hasNext();\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n            {\n                if (sstable != null)\n                    sstable.markSuspect();\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }"
        ],
        [
            "BlacklistingCompactionsTest::testBlacklisting(String)",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  ",
            "    public void testBlacklisting(String compactionStrategy) throws Exception\n    {\n        // this test does enough rows to force multiple block indexes to be used\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        final ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(\"Standard1\");\n\n        final int ROWS_PER_SSTABLE = 10;\n        final int SSTABLES = cfs.metadata.getIndexInterval() * 2 / ROWS_PER_SSTABLE;\n\n        cfs.setCompactionStrategyClass(compactionStrategy);\n\n        // disable compaction while flushing\n        cfs.disableAutoCompaction();\n        //test index corruption\n        //now create a few new SSTables\n        long maxTimestampExpected = Long.MIN_VALUE;\n        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();\n        for (int j = 0; j < SSTABLES; j++)\n        {\n            for (int i = 0; i < ROWS_PER_SSTABLE; i++)\n            {\n                DecoratedKey key = Util.dk(String.valueOf(i % 2));\n                RowMutation rm = new RowMutation(KEYSPACE, key.key);\n                long timestamp = j * ROWS_PER_SSTABLE + i;\n                rm.add(\"Standard1\", ByteBufferUtil.bytes(String.valueOf(i / 2)),\n                       ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                       timestamp);\n                maxTimestampExpected = Math.max(timestamp, maxTimestampExpected);\n                rm.apply();\n                inserted.add(key);\n            }\n            cfs.forceBlockingFlush();\n            CompactionsTest.assertMaxTimestamp(cfs, maxTimestampExpected);\n            assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(cfs).size());\n        }\n\n        Collection<SSTableReader> sstables = cfs.getSSTables();\n        int currentSSTable = 0;\n        int sstablesToCorrupt = 8;\n\n        // corrupt first 'sstablesToCorrupt' SSTables\n        for (SSTableReader sstable : sstables)\n        {\n            if(currentSSTable + 1 > sstablesToCorrupt)\n                break;\n\n            RandomAccessFile raf = null;\n\n            try\n            {\n                raf = new RandomAccessFile(sstable.getFilename(), \"rw\");\n                assertNotNull(raf);\n                raf.write(0xFFFFFF);\n            }\n            finally\n            {\n                FileUtils.closeQuietly(raf);\n            }\n\n            currentSSTable++;\n        }\n\n        int failures = 0;\n\n        // in case something will go wrong we don't want to loop forever using for (;;)\n        for (int i = 0; i < sstables.size(); i++)\n        {\n            try\n            {\n                cfs.forceMajorCompaction();\n            }\n            catch (Exception e)\n            {\n                // kind of a hack since we're not specifying just CorruptSSTableExceptions, or (what we actually expect)\n                // an ExecutionException wrapping a CSSTE.  This is probably Good Enough though, since if there are\n                // other errors in compaction presumably the other tests would bring that to light.\n                failures++;\n                continue;\n            }\n\n            assertEquals(sstablesToCorrupt + 1, cfs.getSSTables().size());\n            break;\n        }\n\n\n        cfs.truncateBlocking();\n        assertEquals(failures, sstablesToCorrupt);\n    }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  ",
            "    public void testBlacklisting(String compactionStrategy) throws Exception\n    {\n        // this test does enough rows to force multiple block indexes to be used\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        final ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(\"Standard1\");\n\n        final int ROWS_PER_SSTABLE = 10;\n        final int SSTABLES = cfs.metadata.getIndexInterval() * 2 / ROWS_PER_SSTABLE;\n\n        cfs.setCompactionStrategyClass(compactionStrategy);\n\n        // disable compaction while flushing\n        cfs.disableAutoCompaction();\n        //test index corruption\n        //now create a few new SSTables\n        long maxTimestampExpected = Long.MIN_VALUE;\n        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();\n        for (int j = 0; j < SSTABLES; j++)\n        {\n            for (int i = 0; i < ROWS_PER_SSTABLE; i++)\n            {\n                DecoratedKey key = Util.dk(String.valueOf(i % 2));\n                RowMutation rm = new RowMutation(KEYSPACE, key.key);\n                long timestamp = j * ROWS_PER_SSTABLE + i;\n                rm.add(\"Standard1\", ByteBufferUtil.bytes(String.valueOf(i / 2)),\n                       ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                       timestamp);\n                maxTimestampExpected = Math.max(timestamp, maxTimestampExpected);\n                rm.apply();\n                inserted.add(key);\n            }\n            cfs.forceBlockingFlush();\n            CompactionsTest.assertMaxTimestamp(cfs, maxTimestampExpected);\n            assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(cfs).size());\n        }\n\n        Collection<SSTableReader> sstables = cfs.getSSTables();\n        int currentSSTable = 0;\n        int sstablesToCorrupt = 8;\n\n        // corrupt first 'sstablesToCorrupt' SSTables\n        for (SSTableReader sstable : sstables)\n        {\n            if(currentSSTable + 1 > sstablesToCorrupt)\n                break;\n\n            RandomAccessFile raf = null;\n\n            try\n            {\n                raf = new RandomAccessFile(sstable.getFilename(), \"rw\");\n                assertNotNull(raf);\n                assertTrue(raf.length() > 20);\n                raf.seek(new Random().nextInt((int)(raf.length() - 20)));\n                // We want to write something large enough that the corruption cannot get undetected\n                // (even without compression)\n                byte[] corruption = new byte[20];\n                Arrays.fill(corruption, (byte)0xFF);\n                raf.write(corruption);\n\n            }\n            finally\n            {\n                FileUtils.closeQuietly(raf);\n            }\n\n            currentSSTable++;\n        }\n\n        int failures = 0;\n\n        // in case something will go wrong we don't want to loop forever using for (;;)\n        for (int i = 0; i < sstables.size(); i++)\n        {\n            try\n            {\n                cfs.forceMajorCompaction();\n            }\n            catch (Exception e)\n            {\n                // kind of a hack since we're not specifying just CorruptSSTableExceptions, or (what we actually expect)\n                // an ExecutionException wrapping a CSSTE.  This is probably Good Enough though, since if there are\n                // other errors in compaction presumably the other tests would bring that to light.\n                failures++;\n                continue;\n            }\n\n            assertEquals(sstablesToCorrupt + 1, cfs.getSSTables().size());\n            break;\n        }\n\n\n        cfs.truncateBlocking();\n        assertEquals(sstablesToCorrupt, failures);\n    }"
        ],
        [
            "SSTableIdentityIterator::SSTableIdentityIterator(CFMetaData,DataInput,String,DecoratedKey,long,boolean,SSTableReader,ColumnSerializer)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "    private SSTableIdentityIterator(CFMetaData metadata,\n                                    DataInput in,\n                                    String filename,\n                                    DecoratedKey key,\n                                    long dataSize,\n                                    boolean checkData,\n                                    SSTableReader sstable,\n                                    ColumnSerializer.Flag flag)\n    {\n        assert !checkData || (sstable != null);\n        this.in = in;\n        this.filename = filename;\n        this.key = key;\n        this.dataSize = dataSize;\n        this.expireBefore = (int)(System.currentTimeMillis() / 1000);\n        this.flag = flag;\n        this.validateColumns = checkData;\n        this.dataVersion = sstable == null ? Descriptor.Version.CURRENT : sstable.descriptor.version;\n\n        try\n        {\n            columnFamily = EmptyColumns.factory.create(metadata);\n            columnFamily.delete(DeletionTime.serializer.deserialize(in));\n            columnCount = dataVersion.hasRowSizeAndColumnCount ? in.readInt() : Integer.MAX_VALUE;\n            atomIterator = columnFamily.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        }\n        catch (IOException e)\n        {\n            if (sstable != null)\n                sstable.markSuspect();\n            throw new CorruptSSTableException(e, filename);\n        }\n    }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "    private SSTableIdentityIterator(CFMetaData metadata,\n                                    DataInput in,\n                                    String filename,\n                                    DecoratedKey key,\n                                    long dataSize,\n                                    boolean checkData,\n                                    SSTableReader sstable,\n                                    ColumnSerializer.Flag flag)\n    {\n        assert !checkData || (sstable != null);\n        this.in = in;\n        this.filename = filename;\n        this.key = key;\n        this.dataSize = dataSize;\n        this.expireBefore = (int)(System.currentTimeMillis() / 1000);\n        this.flag = flag;\n        this.validateColumns = checkData;\n        this.dataVersion = sstable == null ? Descriptor.Version.CURRENT : sstable.descriptor.version;\n        this.sstable = sstable;\n\n        try\n        {\n            columnFamily = EmptyColumns.factory.create(metadata);\n            columnFamily.delete(DeletionTime.serializer.deserialize(in));\n            columnCount = dataVersion.hasRowSizeAndColumnCount ? in.readInt() : Integer.MAX_VALUE;\n            atomIterator = columnFamily.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        }\n        catch (IOException e)\n        {\n            if (sstable != null)\n                sstable.markSuspect();\n            throw new CorruptSSTableException(e, filename);\n        }\n    }"
        ],
        [
            "SSTableIdentityIterator::getColumnFamilyWithColumns(ColumnFamily)",
            " 180  \n 181  \n 182  \n 183  \n 184 -\n 185 -\n 186 -\n 187  \n 188 -\n 189  \n 190 -\n 191  \n 192 -\n 193  \n 194 -\n 195  \n 196 -\n 197  \n 198  \n 199 -\n 200  ",
            "    public ColumnFamily getColumnFamilyWithColumns(ColumnFamily.Factory containerFactory)\n    {\n        ColumnFamily cf = columnFamily.cloneMeShallow(containerFactory, false);\n        // since we already read column count, just pass that value and continue deserialization\n        Iterator<OnDiskAtom> iter = cf.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        while (iter.hasNext())\n            cf.addAtom(iter.next());\n\n        if (validateColumns)\n        {\n            try\n            {\n                cf.metadata().validateColumns(cf);\n            }\n            catch (MarshalException e)\n            {\n                throw new RuntimeException(\"Error validating row \" + key, e);\n            }\n        }\n        return cf;\n    }",
            " 190  \n 191  \n 192  \n 193  \n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214  \n 215 +\n 216 +\n 217  \n 218 +\n 219 +\n 220 +\n 221  \n 222 +\n 223  \n 224 +\n 225  \n 226  \n 227  ",
            "    public ColumnFamily getColumnFamilyWithColumns(ColumnFamily.Factory containerFactory)\n    {\n        ColumnFamily cf = columnFamily.cloneMeShallow(containerFactory, false);\n        // since we already read column count, just pass that value and continue deserialization\n        try\n        {\n            Iterator<OnDiskAtom> iter = cf.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n            while (iter.hasNext())\n                cf.addAtom(iter.next());\n\n            if (validateColumns)\n            {\n                try\n                {\n                    cf.metadata().validateColumns(cf);\n                }\n                catch (MarshalException e)\n                {\n                    throw new RuntimeException(\"Error validating row \" + key, e);\n                }\n            }\n            return cf;\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n            {\n                if (sstable != null)\n                    sstable.markSuspect();\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }"
        ]
    ],
    "e1fb18a00b598431f52b88d12e2eddbe07233e88": [
        [
            "ColumnFamilyStore::ColumnFamilyStore(Keyspace,String,int,CFMetaData,Directories,boolean,boolean)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366 -\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391 -\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        this.directories = directories;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.params.crcCheckChance);\n        indexManager = new SecondaryIndexManager(this);\n        viewManager = keyspace.viewManager.forTable(metadata.cfId);\n        metric = new TableMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n        this.directories = this.compactionStrategyManager.getDirectories();\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (IndexMetadata info : metadata.getIndexes())\n            indexManager.addIndex(info);\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            mbeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,table=%s\",\n                                         isIndex() ? \"IndexTables\" : \"Tables\",\n                                         keyspace.getName(), name);\n            oldMBeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,columnfamily=%s\",\n                                         isIndex() ? \"IndexColumnFamilies\" : \"ColumnFamilies\",\n                                         keyspace.getName(), name);\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName[] objectNames = {new ObjectName(mbeanName), new ObjectName(oldMBeanName)};\n                for (ObjectName objectName : objectNames)\n                {\n                    mbs.registerMBean(this, objectName);\n                }\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.trace(\"retryPolicy for {} is {}\", name, this.metadata.params.speculativeRetry);\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetryParam retryPolicy = ColumnFamilyStore.this.metadata.params.speculativeRetry;\n                    switch (retryPolicy.kind())\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.threshold()) * 1000d);\n                            break;\n                        case CUSTOM:\n                            sampleLatencyNanos = (long) retryPolicy.threshold();\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n    }",
            " 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.params.crcCheckChance);\n        indexManager = new SecondaryIndexManager(this);\n        viewManager = keyspace.viewManager.forTable(metadata.cfId);\n        metric = new TableMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n        this.directories = compactionStrategyManager.getDirectories();\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (IndexMetadata info : metadata.getIndexes())\n            indexManager.addIndex(info);\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            mbeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,table=%s\",\n                                         isIndex() ? \"IndexTables\" : \"Tables\",\n                                         keyspace.getName(), name);\n            oldMBeanName = String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,columnfamily=%s\",\n                                         isIndex() ? \"IndexColumnFamilies\" : \"ColumnFamilies\",\n                                         keyspace.getName(), name);\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName[] objectNames = {new ObjectName(mbeanName), new ObjectName(oldMBeanName)};\n                for (ObjectName objectName : objectNames)\n                {\n                    mbs.registerMBean(this, objectName);\n                }\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.trace(\"retryPolicy for {} is {}\", name, this.metadata.params.speculativeRetry);\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetryParam retryPolicy = ColumnFamilyStore.this.metadata.params.speculativeRetry;\n                    switch (retryPolicy.kind())\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.threshold()) * 1000d);\n                            break;\n                        case CUSTOM:\n                            sampleLatencyNanos = (long) retryPolicy.threshold();\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n    }"
        ],
        [
            "AbstractCompactionStrategy::AbstractCompactionStrategy(ColumnFamilyStore,Map)",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        assert cfs != null;\n        this.cfs = cfs;\n        this.options = ImmutableMap.copyOf(options);\n\n        /* checks must be repeated here, as user supplied strategies might not call validateOptions directly */\n\n        try\n        {\n            validateOptions(options);\n            String optionValue = options.get(TOMBSTONE_THRESHOLD_OPTION);\n            tombstoneThreshold = optionValue == null ? DEFAULT_TOMBSTONE_THRESHOLD : Float.parseFloat(optionValue);\n            optionValue = options.get(TOMBSTONE_COMPACTION_INTERVAL_OPTION);\n            tombstoneCompactionInterval = optionValue == null ? DEFAULT_TOMBSTONE_COMPACTION_INTERVAL : Long.parseLong(optionValue);\n            optionValue = options.get(UNCHECKED_TOMBSTONE_COMPACTION_OPTION);\n            uncheckedTombstoneCompaction = optionValue == null ? DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION : Boolean.parseBoolean(optionValue);\n            if (!shouldBeEnabled())\n                this.disable();\n        }\n        catch (ConfigurationException e)\n        {\n            logger.warn(\"Error setting compaction strategy options ({}), defaults will be used\", e.getMessage());\n            tombstoneThreshold = DEFAULT_TOMBSTONE_THRESHOLD;\n            tombstoneCompactionInterval = DEFAULT_TOMBSTONE_COMPACTION_INTERVAL;\n            uncheckedTombstoneCompaction = DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION;\n        }\n    }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  ",
            "    protected AbstractCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        assert cfs != null;\n        this.cfs = cfs;\n        this.options = ImmutableMap.copyOf(options);\n\n        /* checks must be repeated here, as user supplied strategies might not call validateOptions directly */\n\n        try\n        {\n            validateOptions(options);\n            String optionValue = options.get(TOMBSTONE_THRESHOLD_OPTION);\n            tombstoneThreshold = optionValue == null ? DEFAULT_TOMBSTONE_THRESHOLD : Float.parseFloat(optionValue);\n            optionValue = options.get(TOMBSTONE_COMPACTION_INTERVAL_OPTION);\n            tombstoneCompactionInterval = optionValue == null ? DEFAULT_TOMBSTONE_COMPACTION_INTERVAL : Long.parseLong(optionValue);\n            optionValue = options.get(UNCHECKED_TOMBSTONE_COMPACTION_OPTION);\n            uncheckedTombstoneCompaction = optionValue == null ? DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION : Boolean.parseBoolean(optionValue);\n            if (!shouldBeEnabled())\n                this.disable();\n        }\n        catch (ConfigurationException e)\n        {\n            logger.warn(\"Error setting compaction strategy options ({}), defaults will be used\", e.getMessage());\n            tombstoneThreshold = DEFAULT_TOMBSTONE_THRESHOLD;\n            tombstoneCompactionInterval = DEFAULT_TOMBSTONE_COMPACTION_INTERVAL;\n            uncheckedTombstoneCompaction = DEFAULT_UNCHECKED_TOMBSTONE_COMPACTION_OPTION;\n        }\n\n        directories = new Directories(cfs.metadata, Directories.dataDirectories);\n    }"
        ],
        [
            "AbstractCompactionStrategy::getDirectories()",
            " 122  \n 123  \n 124 -\n 125  ",
            "    public Directories getDirectories()\n    {\n        return cfs.getDirectories();\n    }",
            " 126  \n 127  \n 128 +\n 129  ",
            "    public Directories getDirectories()\n    {\n        return directories;\n    }"
        ]
    ],
    "cba5ef609d6d25380afcb0dff06fe325101c727c": [
        [
            "LegacyLayout::LegacyBoundComparator::compare(LegacyBound,LegacyBound)",
            "1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  ",
            "        public int compare(LegacyBound a, LegacyBound b)\n        {\n            int result = this.clusteringComparator.compare(a.bound, b.bound);\n            if (result != 0)\n                return result;\n\n            return UTF8Type.instance.compare(a.collectionName.name.bytes, b.collectionName.name.bytes);\n        }",
            "1699  \n1700  \n1701 +\n1702 +\n1703 +\n1704 +\n1705 +\n1706 +\n1707 +\n1708 +\n1709 +\n1710 +\n1711  \n1712  \n1713  \n1714  \n1715 +\n1716 +\n1717 +\n1718 +\n1719 +\n1720 +\n1721  \n1722  ",
            "        public int compare(LegacyBound a, LegacyBound b)\n        {\n            // In the legacy sorting, BOTTOM comes before anything else\n            if (a == LegacyBound.BOTTOM)\n                return b == LegacyBound.BOTTOM ? 0 : -1;\n            if (b == LegacyBound.BOTTOM)\n                return 1;\n\n            // Excluding BOTTOM, statics are always before anything else.\n            if (a.isStatic != b.isStatic)\n                return a.isStatic ? -1 : 1;\n\n            int result = this.clusteringComparator.compare(a.bound, b.bound);\n            if (result != 0)\n                return result;\n\n            // If both have equal \"bound\" but one is a collection tombstone and not the other, then the other comes before as it points to the beginning of the row.\n            if (a.collectionName == null)\n                return b.collectionName == null ? 0 : 1;\n            if (b.collectionName == null)\n                return -1;\n\n            return UTF8Type.instance.compare(a.collectionName.name.bytes, b.collectionName.name.bytes);\n        }"
        ]
    ],
    "f3eb4ce40b3a57768c6368546ed0ca16d8634f54": [
        [
            "filterForQuery(Keyspace,List,ReadRepairDecision)",
            " 183  \n 184  \n 185 -\n 186 -\n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "    public List<InetAddress> filterForQuery(Keyspace keyspace, List<InetAddress> liveEndpoints, ReadRepairDecision readRepair)\n    {\n        // If we are doing an each quorum, we have to make sure that the endpoints we select provide a quorum for each\n        // data center\n        if (this == EACH_QUORUM)\n            return filterForEachQuorum(keyspace, liveEndpoints, readRepair);\n\n        /*\n         * Endpoints are expected to be restricted to live replicas, sorted by snitch preference.\n         * For LOCAL_QUORUM, move local-DC replicas in front first as we need them there whether\n         * we do read repair (since the first replica gets the data read) or not (since we'll take\n         * the blockFor first ones).\n         */\n        if (isDCLocal)\n            Collections.sort(liveEndpoints, DatabaseDescriptor.getLocalComparator());\n\n        switch (readRepair)\n        {\n            case NONE:\n                return liveEndpoints.subList(0, Math.min(liveEndpoints.size(), blockFor(keyspace)));\n            case GLOBAL:\n                return liveEndpoints;\n            case DC_LOCAL:\n                List<InetAddress> local = new ArrayList<InetAddress>();\n                List<InetAddress> other = new ArrayList<InetAddress>();\n                for (InetAddress add : liveEndpoints)\n                {\n                    if (isLocal(add))\n                        local.add(add);\n                    else\n                        other.add(add);\n                }\n                // check if blockfor more than we have localep's\n                int blockFor = blockFor(keyspace);\n                if (local.size() < blockFor)\n                    local.addAll(other.subList(0, Math.min(blockFor - local.size(), other.size())));\n                return local;\n            default:\n                throw new AssertionError();\n        }\n    }",
            " 183  \n 184  \n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "    public List<InetAddress> filterForQuery(Keyspace keyspace, List<InetAddress> liveEndpoints, ReadRepairDecision readRepair)\n    {\n        /*\n         * If we are doing an each quorum query, we have to make sure that the endpoints we select\n         * provide a quorum for each data center. If we are not using a NetworkTopologyStrategy,\n         * we should fall through and grab a quorum in the replication strategy.\n         */\n        if (this == EACH_QUORUM && keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)\n            return filterForEachQuorum(keyspace, liveEndpoints, readRepair);\n\n        /*\n         * Endpoints are expected to be restricted to live replicas, sorted by snitch preference.\n         * For LOCAL_QUORUM, move local-DC replicas in front first as we need them there whether\n         * we do read repair (since the first replica gets the data read) or not (since we'll take\n         * the blockFor first ones).\n         */\n        if (isDCLocal)\n            Collections.sort(liveEndpoints, DatabaseDescriptor.getLocalComparator());\n\n        switch (readRepair)\n        {\n            case NONE:\n                return liveEndpoints.subList(0, Math.min(liveEndpoints.size(), blockFor(keyspace)));\n            case GLOBAL:\n                return liveEndpoints;\n            case DC_LOCAL:\n                List<InetAddress> local = new ArrayList<InetAddress>();\n                List<InetAddress> other = new ArrayList<InetAddress>();\n                for (InetAddress add : liveEndpoints)\n                {\n                    if (isLocal(add))\n                        local.add(add);\n                    else\n                        other.add(add);\n                }\n                // check if blockfor more than we have localep's\n                int blockFor = blockFor(keyspace);\n                if (local.size() < blockFor)\n                    local.addAll(other.subList(0, Math.min(blockFor - local.size(), other.size())));\n                return local;\n            default:\n                throw new AssertionError();\n        }\n    }"
        ]
    ],
    "1ba68a1e5d681c091e2c53e7720029f10591e7ef": [
        [
            "LogTransactionTest::testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second()",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 -\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord);\n            FileUtils.append(logFiles.get(1), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }",
            " 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 +\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_second() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc-\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord);\n            FileUtils.append(logFiles.get(1), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }"
        ],
        [
            "LogRecord::LogRecord(Type,String,long,int,long,String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    private LogRecord(Type type,\n                      String absolutePath,\n                      long updateTime,\n                      int numFiles,\n                      long checksum,\n                      String raw)\n    {\n        assert !type.hasFile() || absolutePath != null : \"Expected file path for file records\";\n\n        this.type = type;\n        this.absolutePath = type.hasFile() ? Optional.of(absolutePath) : Optional.<String>empty();\n        this.updateTime = type == Type.REMOVE ? updateTime : 0;\n        this.numFiles = type.hasFile() ? numFiles : 0;\n        this.status = new Status();\n        if (raw == null)\n        {\n            assert checksum == 0;\n            this.checksum = computeChecksum();\n            this.raw = format();\n        }\n        else\n        {\n            this.checksum = checksum;\n            this.raw = raw;\n        }\n    }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "    private LogRecord(Type type,\n                      String absolutePath,\n                      long updateTime,\n                      int numFiles,\n                      long checksum,\n                      String raw)\n    {\n        assert !type.hasFile() || absolutePath != null : \"Expected file path for file records\";\n\n        this.type = type;\n        this.absolutePath = type.hasFile() ? Optional.of(absolutePath) : Optional.empty();\n        this.updateTime = type == Type.REMOVE ? updateTime : 0;\n        this.numFiles = type.hasFile() ? numFiles : 0;\n        this.status = new Status();\n        if (raw == null)\n        {\n            assert checksum == 0;\n            this.checksum = computeChecksum();\n            this.raw = format();\n        }\n        else\n        {\n            this.checksum = checksum;\n            this.raw = raw;\n        }\n    }"
        ],
        [
            "LogRecord::absolutePath()",
            " 290 -\n 291  \n 292 -\n 293  ",
            "    String absolutePath()\n    {\n        return absolutePath.isPresent() ? absolutePath.get() : \"\";\n    }",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308 +\n 309 +\n 310 +\n 311 +\n 312 +\n 313 +\n 314  ",
            "    /**\n     * Return the absolute path, if present, except for the last character (the descriptor separator), or\n     * the empty string if the record has no path. This method is only to be used internally for writing\n     * the record to file or computing the checksum.\n     *\n     * CASSANDRA-13294: the last character of the absolute path is the descriptor separator, it is removed\n     * from the absolute path for backward compatibility, to make sure that on upgrade from 3.0.x to 3.0.y\n     * or to 3.y or to 4.0, the checksum of existing txn files still matches (in case of non clean shutdown\n     * some txn files may be present). By removing the last character here, it means that\n     * it will never be written to txn files, but it is added after reading a txn file in LogFile.make().\n     */\n    private String absolutePath()\n    {\n        if (!absolutePath.isPresent())\n            return \"\";\n\n        String ret = absolutePath.get();\n        assert ret.charAt(ret.length() -1) == Component.separator : \"Invalid absolute path, should end with '-'\";\n        return ret.substring(0, ret.length() - 1);\n    }"
        ],
        [
            "LogRecord::make(Type,SSTable)",
            " 147  \n 148  \n 149 -\n 150  \n 151  ",
            "    public static LogRecord make(Type type, SSTable table)\n    {\n        String absoluteTablePath = FileUtils.getCanonicalPath(table.descriptor.baseFilename());\n        return make(type, getExistingFiles(absoluteTablePath), table.getAllFilePaths().size(), absoluteTablePath);\n    }",
            " 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153 +\n 154 +\n 155  \n 156  ",
            "    public static LogRecord make(Type type, SSTable table)\n    {\n        // CASSANDRA-13294: add the sstable component separator because for legacy (2.1) files\n        // there is no separator after the generation number, and this would cause files of sstables with\n        // a higher generation number that starts with the same number, to be incorrectly classified as files\n        // of this record sstable\n        String absoluteTablePath = FileUtils.getCanonicalPath(table.descriptor.baseFilename() + Component.separator);\n        return make(type, getExistingFiles(absoluteTablePath), table.getAllFilePaths().size(), absoluteTablePath);\n    }"
        ],
        [
            "LogTransactionTest::testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first()",
            " 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 -\n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            FileUtils.append(logFiles.get(1), sstableRecord);\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }",
            " 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 +\n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    @Test\n    public void testRemoveUnfinishedLeftovers_multipleFolders_partialNonFinalRecord_first() throws Throwable\n    {\n        testRemoveUnfinishedLeftovers_multipleFolders_errorConditions(txn -> {\n            List<File> logFiles = txn.logFiles();\n            Assert.assertEquals(2, logFiles.size());\n\n            // insert a partial sstable record and a full commit record\n            String sstableRecord = LogRecord.make(LogRecord.Type.ADD, Collections.emptyList(), 0, \"abc-\").raw;\n            int toChop = sstableRecord.length() / 2;\n            FileUtils.append(logFiles.get(0), sstableRecord.substring(0, sstableRecord.length() - toChop));\n            FileUtils.append(logFiles.get(1), sstableRecord);\n            String finalRecord = LogRecord.makeCommit(System.currentTimeMillis()).raw;\n            FileUtils.append(logFiles.get(0), finalRecord);\n            FileUtils.append(logFiles.get(1), finalRecord);\n\n        }, false);\n    }"
        ],
        [
            "LogRecord::make(String)",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "    public static LogRecord make(String line)\n    {\n        try\n        {\n            Matcher matcher = REGEX.matcher(line);\n            if (!matcher.matches())\n                return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line)\n                       .setError(String.format(\"Failed to parse [%s]\", line));\n\n            Type type = Type.fromPrefix(matcher.group(1));\n            return new LogRecord(type,\n                                 matcher.group(2),\n                                 Long.valueOf(matcher.group(3)),\n                                 Integer.valueOf(matcher.group(4)),\n                                 Long.valueOf(matcher.group(5)), line);\n        }\n        catch (Throwable t)\n        {\n            return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line).setError(t);\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "    public static LogRecord make(String line)\n    {\n        try\n        {\n            Matcher matcher = REGEX.matcher(line);\n            if (!matcher.matches())\n                return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line)\n                       .setError(String.format(\"Failed to parse [%s]\", line));\n\n            Type type = Type.fromPrefix(matcher.group(1));\n            return new LogRecord(type,\n                                 matcher.group(2) + Component.separator, // see comment on CASSANDRA-13294 below\n                                 Long.valueOf(matcher.group(3)),\n                                 Integer.valueOf(matcher.group(4)),\n                                 Long.valueOf(matcher.group(5)), line);\n        }\n        catch (Throwable t)\n        {\n            return new LogRecord(Type.UNKNOWN, null, 0, 0, 0, line).setError(t);\n        }\n    }"
        ]
    ],
    "6c3fa8e30de21aecce35032762470bfa0fb3cb5e": [
        [
            "StorageProxy::mutateMV(ByteBuffer,Collection,boolean,AtomicLong)",
            " 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663 -\n 664  \n 665  \n 666  \n 667 -\n 668 -\n 669 -\n 670 -\n 671 -\n 672 -\n 673  \n 674 -\n 675 -\n 676  \n 677 -\n 678 -\n 679  \n 680 -\n 681 -\n 682 -\n 683 -\n 684 -\n 685 -\n 686 -\n 687 -\n 688 -\n 689 -\n 690 -\n 691 -\n 692 -\n 693 -\n 694 -\n 695 -\n 696  \n 697 -\n 698 -\n 699  \n 700 -\n 701 -\n 702 -\n 703  \n 704 -\n 705 -\n 706 -\n 707 -\n 708 -\n 709  \n 710 -\n 711 -\n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  ",
            "    /**\n     * Use this method to have these Mutations applied\n     * across all replicas.\n     *\n     * @param mutations the mutations to be applied across the replicas\n     * @param writeCommitLog if commitlog should be written\n     * @param baseComplete time from epoch in ms that the local base mutation was(or will be) completed\n     */\n    public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations, boolean writeCommitLog, AtomicLong baseComplete)\n    throws UnavailableException, OverloadedException, WriteTimeoutException\n    {\n        Tracing.trace(\"Determining replicas for mutation\");\n        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n\n        long startTime = System.nanoTime();\n        List<WriteResponseHandlerWrapper> wrappers = new ArrayList<>(mutations.size());\n\n        try\n        {\n            Token baseToken = StorageService.instance.getTokenMetadata().partitioner.getToken(dataKey);\n\n            ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;\n\n            //Since the base -> view replication is 1:1 we only need to store the BL locally\n            final Collection<InetAddress> batchlogEndpoints = Collections.singleton(FBUtilities.getBroadcastAddress());\n            final UUID batchUUID = UUIDGen.getTimeUUID();\n            BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(),\n                                                                                                          () -> asyncRemoveFromBatchlog(batchlogEndpoints, batchUUID));\n\n            // add a handler for each mutation - includes checking availability, but doesn't initiate any writes, yet\n            for (Mutation mutation : mutations)\n            {\n                String keyspaceName = mutation.getKeyspaceName();\n                Token tk = mutation.key().getToken();\n                InetAddress pairedEndpoint = ViewUtils.getViewNaturalEndpoint(keyspaceName, baseToken, tk);\n                List<InetAddress> naturalEndpoints = Lists.newArrayList(pairedEndpoint);\n\n                WriteResponseHandlerWrapper wrapper = wrapViewBatchResponseHandler(mutation,\n                                                                                   consistencyLevel,\n                                                                                   consistencyLevel,\n                                                                                   naturalEndpoints,\n                                                                                   baseComplete,\n                                                                                   WriteType.BATCH,\n                                                                                   cleanup);\n\n                // When local node is the endpoint and there are no pending nodes we can\n                // Just apply the mutation locally.\n                if (pairedEndpoint.equals(FBUtilities.getBroadcastAddress()) && wrapper.handler.pendingEndpoints.isEmpty())\n                {\n                    mutation.apply(writeCommitLog);\n                    viewWriteMetrics.viewReplicasSuccess.inc();\n                }\n                else\n                    wrappers.add(wrapper);\n            }\n\n            if (!wrappers.isEmpty())\n            {\n                // Apply to local batchlog memtable in this thread\n                BatchlogManager.store(Batch.createLocal(batchUUID, FBUtilities.timestampMicros(), Lists.transform(wrappers, w -> w.mutation)),\n                                      writeCommitLog);\n\n                // now actually perform the writes and wait for them to complete\n                asyncWriteBatchedMutations(wrappers, localDataCenter, Stage.VIEW_MUTATION);\n            }\n        }\n        finally\n        {\n            viewWriteMetrics.addNano(System.nanoTime() - startTime);\n        }\n    }",
            " 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663 +\n 664  \n 665  \n 666  \n 667 +\n 668  \n 669  \n 670 +\n 671 +\n 672 +\n 673 +\n 674 +\n 675 +\n 676 +\n 677  \n 678 +\n 679 +\n 680 +\n 681 +\n 682 +\n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690  \n 691 +\n 692 +\n 693 +\n 694 +\n 695 +\n 696 +\n 697 +\n 698 +\n 699 +\n 700 +\n 701 +\n 702 +\n 703 +\n 704 +\n 705 +\n 706 +\n 707 +\n 708 +\n 709 +\n 710  \n 711  \n 712 +\n 713 +\n 714 +\n 715 +\n 716 +\n 717  \n 718 +\n 719 +\n 720 +\n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  ",
            "    /**\n     * Use this method to have these Mutations applied\n     * across all replicas.\n     *\n     * @param mutations the mutations to be applied across the replicas\n     * @param writeCommitLog if commitlog should be written\n     * @param baseComplete time from epoch in ms that the local base mutation was(or will be) completed\n     */\n    public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations, boolean writeCommitLog, AtomicLong baseComplete)\n    throws UnavailableException, OverloadedException, WriteTimeoutException\n    {\n        Tracing.trace(\"Determining replicas for mutation\");\n        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n\n        long startTime = System.nanoTime();\n\n\n        try\n        {\n            // if we haven't joined the ring, write everything to batchlog because paired replicas may be stale\n            final UUID batchUUID = UUIDGen.getTimeUUID();\n\n            if (!Gossiper.instance.isEnabled())\n            {\n                BatchlogManager.store(Batch.createLocal(batchUUID, FBUtilities.timestampMicros(),\n                                                        mutations),\n                                      writeCommitLog);\n            }\n            else\n            {\n                List<WriteResponseHandlerWrapper> wrappers = new ArrayList<>(mutations.size());\n                Token baseToken = StorageService.instance.getTokenMetadata().partitioner.getToken(dataKey);\n\n                ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;\n\n                //Since the base -> view replication is 1:1 we only need to store the BL locally\n                final Collection<InetAddress> batchlogEndpoints = Collections.singleton(FBUtilities.getBroadcastAddress());\n                BatchlogResponseHandler.BatchlogCleanup cleanup = new BatchlogResponseHandler.BatchlogCleanup(mutations.size(),\n                                                                                                              () -> asyncRemoveFromBatchlog(batchlogEndpoints, batchUUID));\n\n                // add a handler for each mutation - includes checking availability, but doesn't initiate any writes, yet\n                for (Mutation mutation : mutations)\n                {\n                    String keyspaceName = mutation.getKeyspaceName();\n                    Token tk = mutation.key().getToken();\n                    InetAddress pairedEndpoint = ViewUtils.getViewNaturalEndpoint(keyspaceName, baseToken, tk);\n                    List<InetAddress> naturalEndpoints = Lists.newArrayList(pairedEndpoint);\n\n                    WriteResponseHandlerWrapper wrapper = wrapViewBatchResponseHandler(mutation,\n                                                                                       consistencyLevel,\n                                                                                       consistencyLevel,\n                                                                                       naturalEndpoints,\n                                                                                       baseComplete,\n                                                                                       WriteType.BATCH,\n                                                                                       cleanup);\n\n                    // When local node is the endpoint and there are no pending nodes we can\n                    // Just apply the mutation locally.\n                    if (pairedEndpoint.equals(FBUtilities.getBroadcastAddress()) && wrapper.handler.pendingEndpoints.isEmpty() && StorageService.instance.isJoined())\n                        mutation.apply(writeCommitLog);\n                    else\n                        wrappers.add(wrapper);\n                }\n\n                if (!wrappers.isEmpty())\n                {\n                    // Apply to local batchlog memtable in this thread\n                    BatchlogManager.store(Batch.createLocal(batchUUID, FBUtilities.timestampMicros(), Lists.transform(wrappers, w -> w.mutation)),\n                                          writeCommitLog);\n\n                    // now actually perform the writes and wait for them to complete\n                    asyncWriteBatchedMutations(wrappers, localDataCenter, Stage.VIEW_MUTATION);\n                }\n            }\n        }\n        finally\n        {\n            viewWriteMetrics.addNano(System.nanoTime() - startTime);\n        }\n    }"
        ],
        [
            "StorageProxy::truncateBlocking(String,String)",
            "2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253 -\n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  ",
            "    /**\n     * Performs the truncate operatoin, which effectively deletes all data from\n     * the column family cfname\n     * @param keyspace\n     * @param cfname\n     * @throws UnavailableException If some of the hosts in the ring are down.\n     * @throws TimeoutException\n     * @throws IOException\n     */\n    public static void truncateBlocking(String keyspace, String cfname) throws UnavailableException, TimeoutException, IOException\n    {\n        logger.debug(\"Starting a blocking truncate operation on keyspace {}, CF {}\", keyspace, cfname);\n        if (isAnyStorageHostDown())\n        {\n            logger.info(\"Cannot perform truncate, some hosts are down\");\n            // Since the truncate operation is so aggressive and is typically only\n            // invoked by an admin, for simplicity we require that all nodes are up\n            // to perform the operation.\n            int liveMembers = Gossiper.instance.getLiveMembers().size();\n            throw new UnavailableException(ConsistencyLevel.ALL, liveMembers + Gossiper.instance.getUnreachableMembers().size(), liveMembers);\n        }\n\n        Set<InetAddress> allEndpoints = Gossiper.instance.getLiveTokenOwners();\n        \n        int blockFor = allEndpoints.size();\n        final TruncateResponseHandler responseHandler = new TruncateResponseHandler(blockFor);\n\n        // Send out the truncate calls and track the responses with the callbacks.\n        Tracing.trace(\"Enqueuing truncate messages to hosts {}\", allEndpoints);\n        final Truncation truncation = new Truncation(keyspace, cfname);\n        MessageOut<Truncation> message = truncation.createMessage();\n        for (InetAddress endpoint : allEndpoints)\n            MessagingService.instance().sendRR(message, endpoint, responseHandler);\n\n        // Wait for all\n        try\n        {\n            responseHandler.get();\n        }\n        catch (TimeoutException e)\n        {\n            Tracing.trace(\"Timed out\");\n            throw e;\n        }\n    }",
            "2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262 +\n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  ",
            "    /**\n     * Performs the truncate operatoin, which effectively deletes all data from\n     * the column family cfname\n     * @param keyspace\n     * @param cfname\n     * @throws UnavailableException If some of the hosts in the ring are down.\n     * @throws TimeoutException\n     * @throws IOException\n     */\n    public static void truncateBlocking(String keyspace, String cfname) throws UnavailableException, TimeoutException, IOException\n    {\n        logger.debug(\"Starting a blocking truncate operation on keyspace {}, CF {}\", keyspace, cfname);\n        if (isAnyStorageHostDown())\n        {\n            logger.info(\"Cannot perform truncate, some hosts are down\");\n            // Since the truncate operation is so aggressive and is typically only\n            // invoked by an admin, for simplicity we require that all nodes are up\n            // to perform the operation.\n            int liveMembers = Gossiper.instance.getLiveMembers().size();\n            throw new UnavailableException(ConsistencyLevel.ALL, liveMembers + Gossiper.instance.getUnreachableMembers().size(), liveMembers);\n        }\n\n        Set<InetAddress> allEndpoints = Gossiper.instance.getLiveTokenOwners();\n\n        int blockFor = allEndpoints.size();\n        final TruncateResponseHandler responseHandler = new TruncateResponseHandler(blockFor);\n\n        // Send out the truncate calls and track the responses with the callbacks.\n        Tracing.trace(\"Enqueuing truncate messages to hosts {}\", allEndpoints);\n        final Truncation truncation = new Truncation(keyspace, cfname);\n        MessageOut<Truncation> message = truncation.createMessage();\n        for (InetAddress endpoint : allEndpoints)\n            MessagingService.instance().sendRR(message, endpoint, responseHandler);\n\n        // Wait for all\n        try\n        {\n            responseHandler.get();\n        }\n        catch (TimeoutException e)\n        {\n            Tracing.trace(\"Timed out\");\n            throw e;\n        }\n    }"
        ]
    ],
    "67637d1bbaf638162b479ed33a75e4ab3fb06e49": [
        [
            "CompactionManager::performAnticompaction(ColumnFamilyStore,Collection,Refs,long)",
            " 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException, ExecutionException, IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      long repairedAt) throws InterruptedException, ExecutionException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.debug(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken(), sstable.partitioner);\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            cfs.getDataTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatuses);\n            cfs.getDataTracker().unmarkCompacting(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, sstables, repairedAt);\n        }\n        finally\n        {\n            validatedForRepair.release();\n            cfs.getDataTracker().unmarkCompacting(sstables);\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }",
            " 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476 +\n 477 +\n 478 +\n 479 +\n 480 +\n 481 +\n 482  \n 483 +\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505 +\n 506 +\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526 +\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException, ExecutionException, IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      long repairedAt) throws InterruptedException, ExecutionException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.debug(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        // we should only notify that repair status changed if it actually did:\n        Set<SSTableReader> mutatedRepairStatusToNotify = new HashSet<>();\n        Map<SSTableReader, Boolean> wasRepairedBefore = new HashMap<>();\n        for (SSTableReader sstable : sstables)\n            wasRepairedBefore.put(sstable, sstable.isRepaired());\n\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken(), sstable.partitioner);\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        if (!wasRepairedBefore.get(sstable))\n                            mutatedRepairStatusToNotify.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            cfs.getDataTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatusToNotify);\n            cfs.getDataTracker().unmarkCompacting(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, sstables, repairedAt);\n        }\n        finally\n        {\n            validatedForRepair.release();\n            cfs.getDataTracker().unmarkCompacting(sstables);\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }"
        ]
    ],
    "590d20b0a1e1b1fb71e97508bb6b475f8aac205a": [
        [
            "LeveledCompactionStrategyTest::waitForLeveling(ColumnFamilyStore)",
            " 196  \n 197  \n 198  \n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "    /**\n     * wait for leveled compaction to quiesce on the given columnfamily\n     */\n    private void waitForLeveling(ColumnFamilyStore cfs) throws InterruptedException\n    {\n        CompactionStrategyManager strategyManager = cfs.getCompactionStrategyManager();\n        while (true)\n        {\n            // since we run several compaction strategies we wait until L0 in all strategies is empty and\n            // atleast one L1+ is non-empty. In these tests we always run a single data directory with only unrepaired data\n            // so it should be good enough\n            boolean allL0Empty = true;\n            boolean anyL1NonEmpty = false;\n            for (List<AbstractCompactionStrategy> strategies : strategyManager.getStrategies())\n            {\n                for (AbstractCompactionStrategy strategy : strategies)\n                {\n                    if (!(strategy instanceof LeveledCompactionStrategy))\n                        return;\n                    // note that we check > 1 here, if there is too little data in L0, we don't compact it up to L1\n                    if (((LeveledCompactionStrategy)strategy).getLevelSize(0) > 1)\n                        allL0Empty = false;\n                    for (int i = 1; i < 5; i++)\n                        if (((LeveledCompactionStrategy)strategy).getLevelSize(i) > 0)\n                            anyL1NonEmpty = true;\n                }\n            }\n            if (allL0Empty && anyL1NonEmpty)\n                return;\n            Thread.sleep(100);\n        }\n    }",
            " 196  \n 197  \n 198  \n 199 +\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "    /**\n     * wait for leveled compaction to quiesce on the given columnfamily\n     */\n    public static void waitForLeveling(ColumnFamilyStore cfs) throws InterruptedException\n    {\n        CompactionStrategyManager strategyManager = cfs.getCompactionStrategyManager();\n        while (true)\n        {\n            // since we run several compaction strategies we wait until L0 in all strategies is empty and\n            // atleast one L1+ is non-empty. In these tests we always run a single data directory with only unrepaired data\n            // so it should be good enough\n            boolean allL0Empty = true;\n            boolean anyL1NonEmpty = false;\n            for (List<AbstractCompactionStrategy> strategies : strategyManager.getStrategies())\n            {\n                for (AbstractCompactionStrategy strategy : strategies)\n                {\n                    if (!(strategy instanceof LeveledCompactionStrategy))\n                        return;\n                    // note that we check > 1 here, if there is too little data in L0, we don't compact it up to L1\n                    if (((LeveledCompactionStrategy)strategy).getLevelSize(0) > 1)\n                        allL0Empty = false;\n                    for (int i = 1; i < 5; i++)\n                        if (((LeveledCompactionStrategy)strategy).getLevelSize(i) > 0)\n                            anyL1NonEmpty = true;\n                }\n            }\n            if (allL0Empty && anyL1NonEmpty)\n                return;\n            Thread.sleep(100);\n        }\n    }"
        ],
        [
            "LongLeveledCompactionStrategyTest::defineSchema()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        Map<String, String> leveledOptions = new HashMap<>();\n        leveledOptions.put(\"sstable_size_in_mb\", \"1\");\n        SchemaLoader.prepareServer();\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARDLVL)\n                                                .compaction(CompactionParams.lcs(leveledOptions)));\n    }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61  \n  62  ",
            "    @BeforeClass\n    public static void defineSchema() throws ConfigurationException\n    {\n        Map<String, String> leveledOptions = new HashMap<>();\n        leveledOptions.put(\"sstable_size_in_mb\", \"1\");\n        SchemaLoader.prepareServer();\n        SchemaLoader.createKeyspace(KEYSPACE1,\n                                    KeyspaceParams.simple(1),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARDLVL)\n                                                .compaction(CompactionParams.lcs(leveledOptions)),\n                                    SchemaLoader.standardCFMD(KEYSPACE1, CF_STANDARDLVL2)\n                                                .compaction(CompactionParams.lcs(leveledOptions)));\n    }"
        ],
        [
            "LongLeveledCompactionStrategyTest::testLeveledScanner()",
            " 145  \n 146  \n 147  \n 148 -\n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "    @Test\n    public void testLeveledScanner() throws Exception\n    {\n        testParallelLeveledCompaction();\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore store = keyspace.getColumnFamilyStore(CF_STANDARDLVL);\n        store.disableAutoCompaction();\n        CompactionStrategyManager mgr = store.getCompactionStrategyManager();\n        LeveledCompactionStrategy lcs = (LeveledCompactionStrategy) mgr.getStrategies().get(1).get(0);\n\n        ByteBuffer value = ByteBuffer.wrap(new byte[10 * 1024]); // 10 KB value\n\n        // Adds 10 partitions\n        for (int r = 0; r < 10; r++)\n        {\n            DecoratedKey key = Util.dk(String.valueOf(r));\n            UpdateBuilder builder = UpdateBuilder.create(store.metadata, key);\n            for (int c = 0; c < 10; c++)\n                builder.newRow(\"column\" + c).add(\"val\", value);\n\n            Mutation rm = new Mutation(builder.build());\n            rm.apply();\n        }\n\n        //Flush sstable\n        store.forceBlockingFlush();\n\n        Iterable<SSTableReader> allSSTables = store.getSSTables(SSTableSet.LIVE);\n        for (SSTableReader sstable : allSSTables)\n        {\n            if (sstable.getSSTableLevel() == 0)\n            {\n                System.out.println(\"Mutating L0-SSTABLE level to L1 to simulate a bug: \" + sstable.getFilename());\n                sstable.descriptor.getMetadataSerializer().mutateLevel(sstable.descriptor, 1);\n                sstable.reloadSSTableMetadata();\n            }\n        }\n\n        try (AbstractCompactionStrategy.ScannerList scannerList = lcs.getScanners(Lists.newArrayList(allSSTables)))\n        {\n            //Verify that leveled scanners will always iterate in ascending order (CASSANDRA-9935)\n            for (ISSTableScanner scanner : scannerList.scanners)\n            {\n                DecoratedKey lastKey = null;\n                while (scanner.hasNext())\n                {\n                    UnfilteredRowIterator row = scanner.next();\n                    if (lastKey != null)\n                    {\n                        assertTrue(\"row \" + row.partitionKey() + \" received out of order wrt \" + lastKey, row.partitionKey().compareTo(lastKey) >= 0);\n                    }\n                    lastKey = row.partitionKey();\n                }\n            }\n        }\n    }",
            " 148  \n 149  \n 150  \n 151  \n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158 +\n 159 +\n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172  \n 173  \n 174  \n 175  \n 176 +\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "    @Test\n    public void testLeveledScanner() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE1);\n        ColumnFamilyStore store = keyspace.getColumnFamilyStore(CF_STANDARDLVL2);\n        ByteBuffer value = ByteBuffer.wrap(new byte[100 * 1024]); // 100 KB value, make it easy to have multiple files\n\n        // Enough data to have a level 1 and 2\n        int rows = 128;\n        int columns = 10;\n\n        // Adds enough data to trigger multiple sstable per level\n        for (int r = 0; r < rows; r++)\n        {\n            DecoratedKey key = Util.dk(String.valueOf(r));\n            UpdateBuilder builder = UpdateBuilder.create(store.metadata, key);\n            for (int c = 0; c < columns; c++)\n                builder.newRow(\"column\" + c).add(\"val\", value);\n\n            Mutation rm = new Mutation(builder.build());\n            rm.apply();\n            store.forceBlockingFlush();\n        }\n        LeveledCompactionStrategyTest.waitForLeveling(store);\n        store.disableAutoCompaction();\n        CompactionStrategyManager mgr = store.getCompactionStrategyManager();\n        LeveledCompactionStrategy lcs = (LeveledCompactionStrategy) mgr.getStrategies().get(1).get(0);\n\n        value = ByteBuffer.wrap(new byte[10 * 1024]); // 10 KB value\n\n        // Adds 10 partitions\n        for (int r = 0; r < 10; r++)\n        {\n            DecoratedKey key = Util.dk(String.valueOf(r));\n            UpdateBuilder builder = UpdateBuilder.create(store.metadata, key);\n            for (int c = 0; c < 10; c++)\n                builder.newRow(\"column\" + c).add(\"val\", value);\n\n            Mutation rm = new Mutation(builder.build());\n            rm.apply();\n        }\n\n        //Flush sstable\n        store.forceBlockingFlush();\n\n        Iterable<SSTableReader> allSSTables = store.getSSTables(SSTableSet.LIVE);\n        for (SSTableReader sstable : allSSTables)\n        {\n            if (sstable.getSSTableLevel() == 0)\n            {\n                System.out.println(\"Mutating L0-SSTABLE level to L1 to simulate a bug: \" + sstable.getFilename());\n                sstable.descriptor.getMetadataSerializer().mutateLevel(sstable.descriptor, 1);\n                sstable.reloadSSTableMetadata();\n            }\n        }\n\n        try (AbstractCompactionStrategy.ScannerList scannerList = lcs.getScanners(Lists.newArrayList(allSSTables)))\n        {\n            //Verify that leveled scanners will always iterate in ascending order (CASSANDRA-9935)\n            for (ISSTableScanner scanner : scannerList.scanners)\n            {\n                DecoratedKey lastKey = null;\n                while (scanner.hasNext())\n                {\n                    UnfilteredRowIterator row = scanner.next();\n                    if (lastKey != null)\n                    {\n                        assertTrue(\"row \" + row.partitionKey() + \" received out of order wrt \" + lastKey, row.partitionKey().compareTo(lastKey) >= 0);\n                    }\n                    lastKey = row.partitionKey();\n                }\n            }\n        }\n    }"
        ]
    ],
    "1538c0921444d7969ebd07ca1abda9a7e40e4c73": [
        [
            "AnticompactionTask::run()",
            "  53  \n  54  \n  55 -\n  56 -\n  57 -\n  58  \n  59 -\n  60  \n  61 -\n  62  \n  63  \n  64  \n  65 -\n  66 -\n  67  \n  68  \n  69  \n  70  \n  71 -\n  72 -\n  73 -\n  74  \n  75  ",
            "    public void run()\n    {\n        AnticompactionRequest acr = new AnticompactionRequest(parentSession);\n        SemanticVersion peerVersion = SystemKeyspace.getReleaseVersion(neighbor);\n        if (peerVersion != null && peerVersion.compareTo(VERSION_CHECKER) > 0)\n        {\n            if (doAnticompaction)\n            {\n                MessagingService.instance().sendRR(acr.createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true);\n            }\n            else\n            {\n                // we need to clean up parent session\n                MessagingService.instance().sendRR(new CleanupMessage(parentSession).createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true);\n            }\n        }\n        else\n        {\n            MessagingService.instance().sendOneWay(acr.createMessage(), neighbor);\n            // immediately return after sending request\n            set(neighbor);\n        }\n    }",
            "  55  \n  56  \n  57 +\n  58  \n  59 +\n  60 +\n  61 +\n  62  \n  63 +\n  64 +\n  65 +\n  66 +\n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  ",
            "    public void run()\n    {\n        if (FailureDetector.instance.isAlive(neighbor))\n        {\n            AnticompactionRequest acr = new AnticompactionRequest(parentSession);\n            SemanticVersion peerVersion = SystemKeyspace.getReleaseVersion(neighbor);\n            if (peerVersion != null && peerVersion.compareTo(VERSION_CHECKER) > 0)\n            {\n                if (doAnticompaction)\n                {\n                    MessagingService.instance().sendRR(acr.createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true);\n                }\n                else\n                {\n                    // we need to clean up parent session\n                    MessagingService.instance().sendRR(new CleanupMessage(parentSession).createMessage(), neighbor, new AnticompactionCallback(this), TimeUnit.DAYS.toMillis(1), true);\n                }\n            }\n            else\n            {\n                MessagingService.instance().sendOneWay(acr.createMessage(), neighbor);\n                // immediately return after sending request\n                set(neighbor);\n            }\n        }\n        else\n        {\n            setException(new IOException(neighbor + \" is down\"));\n        }\n    }"
        ],
        [
            "ActiveRepairService::prepareForRepair(Set,Collection,List)",
            " 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 -\n 281  \n 282 -\n 283 -\n 284 -\n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  ",
            "    public synchronized UUID prepareForRepair(Set<InetAddress> endpoints, Collection<Range<Token>> ranges, List<ColumnFamilyStore> columnFamilyStores)\n    {\n        UUID parentRepairSession = UUIDGen.getTimeUUID();\n        registerParentRepairSession(parentRepairSession, columnFamilyStores, ranges);\n        final CountDownLatch prepareLatch = new CountDownLatch(endpoints.size());\n        final AtomicBoolean status = new AtomicBoolean(true);\n        final Set<String> failedNodes = Collections.synchronizedSet(new HashSet<String>());\n        IAsyncCallbackWithFailure callback = new IAsyncCallbackWithFailure()\n        {\n            public void response(MessageIn msg)\n            {\n                prepareLatch.countDown();\n            }\n\n            public boolean isLatencyForSnitch()\n            {\n                return false;\n            }\n\n            public void onFailure(InetAddress from)\n            {\n                status.set(false);\n                failedNodes.add(from.getHostAddress());\n                prepareLatch.countDown();\n            }\n        };\n\n        List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());\n        for (ColumnFamilyStore cfs : columnFamilyStores)\n            cfIds.add(cfs.metadata.cfId);\n\n        for(InetAddress neighbour : endpoints)\n        {\n            PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);\n            MessageOut<RepairMessage> msg = message.createMessage();\n            MessagingService.instance().sendRR(msg, neighbour, callback, TimeUnit.HOURS.toMillis(1), true);\n        }\n        try\n        {\n            prepareLatch.await(1, TimeUnit.HOURS);\n        }\n        catch (InterruptedException e)\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString(), e);\n        }\n\n        if (!status.get())\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get positive replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString());\n        }\n\n        return parentRepairSession;\n    }",
            " 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 +\n 281  \n 282 +\n 283 +\n 284 +\n 285 +\n 286 +\n 287 +\n 288 +\n 289 +\n 290 +\n 291 +\n 292 +\n 293 +\n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  ",
            "    public synchronized UUID prepareForRepair(Set<InetAddress> endpoints, Collection<Range<Token>> ranges, List<ColumnFamilyStore> columnFamilyStores)\n    {\n        UUID parentRepairSession = UUIDGen.getTimeUUID();\n        registerParentRepairSession(parentRepairSession, columnFamilyStores, ranges);\n        final CountDownLatch prepareLatch = new CountDownLatch(endpoints.size());\n        final AtomicBoolean status = new AtomicBoolean(true);\n        final Set<String> failedNodes = Collections.synchronizedSet(new HashSet<String>());\n        IAsyncCallbackWithFailure callback = new IAsyncCallbackWithFailure()\n        {\n            public void response(MessageIn msg)\n            {\n                prepareLatch.countDown();\n            }\n\n            public boolean isLatencyForSnitch()\n            {\n                return false;\n            }\n\n            public void onFailure(InetAddress from)\n            {\n                status.set(false);\n                failedNodes.add(from.getHostAddress());\n                prepareLatch.countDown();\n            }\n        };\n\n        List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());\n        for (ColumnFamilyStore cfs : columnFamilyStores)\n            cfIds.add(cfs.metadata.cfId);\n\n        for (InetAddress neighbour : endpoints)\n        {\n            if (FailureDetector.instance.isAlive(neighbour))\n            {\n                PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);\n                MessageOut<RepairMessage> msg = message.createMessage();\n                MessagingService.instance().sendRR(msg, neighbour, callback, TimeUnit.HOURS.toMillis(1), true);\n            }\n            else\n            {\n                status.set(false);\n                failedNodes.add(neighbour.getHostAddress());\n                prepareLatch.countDown();\n            }\n        }\n        try\n        {\n            prepareLatch.await(1, TimeUnit.HOURS);\n        }\n        catch (InterruptedException e)\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString(), e);\n        }\n\n        if (!status.get())\n        {\n            parentRepairSessions.remove(parentRepairSession);\n            throw new RuntimeException(\"Did not get positive replies from all endpoints. List of failed endpoint(s): \" + failedNodes.toString());\n        }\n\n        return parentRepairSession;\n    }"
        ]
    ],
    "60b23b1252e600189d11c614d971a4c2c3055edc": [
        [
            "AbstractQueryPager::Pager::onClose()",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "        @Override\n        public void onClose()\n        {\n            recordLast(lastKey, lastRow);\n\n            int counted = counter.counted();\n            remaining -= counted;\n            // If the clustering of the last row returned is a static one, it means that the partition was only\n            // containing data within the static columns. Therefore, there are not data remaining within the partition.\n            if (lastRow != null && lastRow.clustering() == Clustering.STATIC_CLUSTERING)\n            {\n                remainingInPartition = 0;\n            }\n            else\n            {\n                remainingInPartition -= counter.countedInCurrentPartition();\n            }\n            exhausted = counted < pageLimits.count();\n        }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "        @Override\n        public void onClose()\n        {\n            recordLast(lastKey, lastRow);\n\n            int counted = counter.counted();\n            remaining -= counted;\n            // If the clustering of the last row returned is a static one, it means that the partition was only\n            // containing data within the static columns. If the clustering of the last row returned is empty\n            // it means that there is only one row per partition. Therefore, in both cases there are no data remaining\n            // within the partition.\n            if (lastRow != null && (lastRow.clustering() == Clustering.STATIC_CLUSTERING\n                    || lastRow.clustering() == Clustering.EMPTY))\n            {\n                remainingInPartition = 0;\n            }\n            else\n            {\n                remainingInPartition -= counter.countedInCurrentPartition();\n            }\n            exhausted = counted < pageLimits.count();\n        }"
        ],
        [
            "PagingState::toString()",
            " 161  \n 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public String toString()\n    {\n        return String.format(\"PagingState(key=%s, cellname=%s, remaining=%d, remainingInPartition=%d\",\n                             ByteBufferUtil.bytesToHex(partitionKey),\n                             rowMark,\n                             remaining,\n                             remainingInPartition);\n    }",
            " 161  \n 162  \n 163  \n 164  \n 165 +\n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public String toString()\n    {\n        return String.format(\"PagingState(key=%s, cellname=%s, remaining=%d, remainingInPartition=%d\",\n                             partitionKey != null ? ByteBufferUtil.bytesToHex(partitionKey) : null,\n                             rowMark,\n                             remaining,\n                             remainingInPartition);\n    }"
        ]
    ],
    "dfbe3fabd266493e698c194ef90b4dfc7d62b030": [
        [
            "SSTableReader::validate()",
            "1176  \n1177  \n1178  \n1179  \n1180 -\n1181  \n1182  ",
            "    private void validate()\n    {\n        if (this.first.compareTo(this.last) > 0)\n        {\n            throw new IllegalStateException(String.format(\"SSTable first key %s > last key %s\", this.first, this.last));\n        }\n    }",
            "1191  \n1192  \n1193  \n1194  \n1195 +\n1196  \n1197  ",
            "    private void validate()\n    {\n        if (this.first.compareTo(this.last) > 0)\n        {\n            throw new CorruptSSTableException(new IllegalStateException(String.format(\"SSTable first key %s > last key %s\", this.first, this.last)), getFilename());\n        }\n    }"
        ],
        [
            "SSTableReader::openAll(Set,CFMetaData)",
            " 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  ",
            "    public static Collection<SSTableReader> openAll(Set<Map.Entry<Descriptor, Set<Component>>> entries,\n                                                    final CFMetaData metadata)\n    {\n        final Collection<SSTableReader> sstables = new LinkedBlockingQueue<>();\n\n        ExecutorService executor = DebuggableThreadPoolExecutor.createWithFixedPoolSize(\"SSTableBatchOpen\", FBUtilities.getAvailableProcessors());\n        for (final Map.Entry<Descriptor, Set<Component>> entry : entries)\n        {\n            Runnable runnable = new Runnable()\n            {\n                public void run()\n                {\n                    SSTableReader sstable;\n                    try\n                    {\n                        sstable = open(entry.getKey(), entry.getValue(), metadata);\n                    }\n                    catch (CorruptSSTableException ex)\n                    {\n                        FileUtils.handleCorruptSSTable(ex);\n                        logger.error(\"Corrupt sstable {}; skipping table\", entry, ex);\n                        return;\n                    }\n                    catch (FSError ex)\n                    {\n                        FileUtils.handleFSError(ex);\n                        logger.error(\"Cannot read sstable {}; file system error, skipping table\", entry, ex);\n                        return;\n                    }\n                    catch (IOException ex)\n                    {\n                        logger.error(\"Cannot read sstable {}; other IO error, skipping table\", entry, ex);\n                        return;\n                    }\n                    sstables.add(sstable);\n                }\n            };\n            executor.submit(runnable);\n        }\n\n        executor.shutdown();\n        try\n        {\n            executor.awaitTermination(7, TimeUnit.DAYS);\n        }\n        catch (InterruptedException e)\n        {\n            throw new AssertionError(e);\n        }\n\n        return sstables;\n\n    }",
            " 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551 +\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  ",
            "    public static Collection<SSTableReader> openAll(Set<Map.Entry<Descriptor, Set<Component>>> entries,\n                                                    final CFMetaData metadata)\n    {\n        final Collection<SSTableReader> sstables = new LinkedBlockingQueue<>();\n\n        ExecutorService executor = DebuggableThreadPoolExecutor.createWithFixedPoolSize(\"SSTableBatchOpen\", FBUtilities.getAvailableProcessors());\n        for (final Map.Entry<Descriptor, Set<Component>> entry : entries)\n        {\n            Runnable runnable = new Runnable()\n            {\n                public void run()\n                {\n                    SSTableReader sstable;\n                    try\n                    {\n                        sstable = open(entry.getKey(), entry.getValue(), metadata);\n                    }\n                    catch (CorruptSSTableException ex)\n                    {\n                        FileUtils.handleCorruptSSTable(ex);\n                        logger.error(\"Corrupt sstable {}; skipping table\", entry, ex);\n                        return;\n                    }\n                    catch (FSError ex)\n                    {\n                        FileUtils.handleFSError(ex);\n                        logger.error(\"Cannot read sstable {}; file system error, skipping table\", entry, ex);\n                        return;\n                    }\n                    catch (IOException ex)\n                    {\n                        FileUtils.handleCorruptSSTable(new CorruptSSTableException(ex, entry.getKey().filenameFor(Component.DATA)));\n                        logger.error(\"Cannot read sstable {}; other IO error, skipping table\", entry, ex);\n                        return;\n                    }\n                    sstables.add(sstable);\n                }\n            };\n            executor.submit(runnable);\n        }\n\n        executor.shutdown();\n        try\n        {\n            executor.awaitTermination(7, TimeUnit.DAYS);\n        }\n        catch (InterruptedException e)\n        {\n            throw new AssertionError(e);\n        }\n\n        return sstables;\n\n    }"
        ],
        [
            "ColumnFamilyStore::loadNewSSTables()",
            " 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694 -\n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721 -\n 722  \n 723 -\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  ",
            "    /**\n     * #{@inheritDoc}\n     */\n    public synchronized void loadNewSSTables()\n    {\n        logger.info(\"Loading new SSTables for {}/{}...\", keyspace.getName(), name);\n\n        Set<Descriptor> currentDescriptors = new HashSet<>();\n        for (SSTableReader sstable : getSSTables(SSTableSet.CANONICAL))\n            currentDescriptors.add(sstable.descriptor);\n        Set<SSTableReader> newSSTables = new HashSet<>();\n\n        Directories.SSTableLister lister = getDirectories().sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n        for (Map.Entry<Descriptor, Set<Component>> entry : lister.list().entrySet())\n        {\n            Descriptor descriptor = entry.getKey();\n\n            if (currentDescriptors.contains(descriptor))\n                continue; // old (initialized) SSTable found, skipping\n\n            if (!descriptor.isCompatible())\n                throw new RuntimeException(String.format(\"Can't open incompatible SSTable! Current version %s, found file: %s\",\n                        descriptor.getFormat().getLatestVersion(),\n                        descriptor));\n\n            // force foreign sstables to level 0\n            try\n            {\n                if (new File(descriptor.filenameFor(Component.STATS)).exists())\n                    descriptor.getMetadataSerializer().mutateLevel(descriptor, 0);\n            }\n            catch (IOException e)\n            {\n                SSTableReader.logOpenException(entry.getKey(), e);\n                continue;\n            }\n\n            // Increment the generation until we find a filename that doesn't exist. This is needed because the new\n            // SSTables that are being loaded might already use these generation numbers.\n            Descriptor newDescriptor;\n            do\n            {\n                newDescriptor = new Descriptor(descriptor.version,\n                                               descriptor.directory,\n                                               descriptor.ksname,\n                                               descriptor.cfname,\n                                               fileIndexGenerator.incrementAndGet(),\n                                               descriptor.formatType,\n                                               descriptor.digestComponent);\n            }\n            while (new File(newDescriptor.filenameFor(Component.DATA)).exists());\n\n            logger.info(\"Renaming new SSTable {} to {}\", descriptor, newDescriptor);\n            SSTableWriter.rename(descriptor, newDescriptor, entry.getValue());\n\n            SSTableReader reader;\n            try\n            {\n                reader = SSTableReader.open(newDescriptor, entry.getValue(), metadata);\n            }\n            catch (IOException e)\n            {\n                SSTableReader.logOpenException(entry.getKey(), e);\n                continue;\n            }\n            newSSTables.add(reader);\n        }\n\n        if (newSSTables.isEmpty())\n        {\n            logger.info(\"No new SSTables were found for {}/{}\", keyspace.getName(), name);\n            return;\n        }\n\n        logger.info(\"Loading new SSTables and building secondary indexes for {}/{}: {}\", keyspace.getName(), name, newSSTables);\n\n        try (Refs<SSTableReader> refs = Refs.ref(newSSTables))\n        {\n            data.addSSTables(newSSTables);\n            indexManager.buildAllIndexesBlocking(newSSTables);\n        }\n\n        logger.info(\"Done loading load new SSTables for {}/{}\", keyspace.getName(), name);\n    }",
            " 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696 +\n 697 +\n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724 +\n 725 +\n 726 +\n 727 +\n 728 +\n 729 +\n 730 +\n 731 +\n 732 +\n 733 +\n 734 +\n 735 +\n 736 +\n 737  \n 738 +\n 739 +\n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  ",
            "    /**\n     * #{@inheritDoc}\n     */\n    public synchronized void loadNewSSTables()\n    {\n        logger.info(\"Loading new SSTables for {}/{}...\", keyspace.getName(), name);\n\n        Set<Descriptor> currentDescriptors = new HashSet<>();\n        for (SSTableReader sstable : getSSTables(SSTableSet.CANONICAL))\n            currentDescriptors.add(sstable.descriptor);\n        Set<SSTableReader> newSSTables = new HashSet<>();\n\n        Directories.SSTableLister lister = getDirectories().sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n        for (Map.Entry<Descriptor, Set<Component>> entry : lister.list().entrySet())\n        {\n            Descriptor descriptor = entry.getKey();\n\n            if (currentDescriptors.contains(descriptor))\n                continue; // old (initialized) SSTable found, skipping\n\n            if (!descriptor.isCompatible())\n                throw new RuntimeException(String.format(\"Can't open incompatible SSTable! Current version %s, found file: %s\",\n                        descriptor.getFormat().getLatestVersion(),\n                        descriptor));\n\n            // force foreign sstables to level 0\n            try\n            {\n                if (new File(descriptor.filenameFor(Component.STATS)).exists())\n                    descriptor.getMetadataSerializer().mutateLevel(descriptor, 0);\n            }\n            catch (IOException e)\n            {\n                FileUtils.handleCorruptSSTable(new CorruptSSTableException(e, entry.getKey().filenameFor(Component.STATS)));\n                logger.error(\"Cannot read sstable {}; other IO error, skipping table\", entry, e);\n                continue;\n            }\n\n            // Increment the generation until we find a filename that doesn't exist. This is needed because the new\n            // SSTables that are being loaded might already use these generation numbers.\n            Descriptor newDescriptor;\n            do\n            {\n                newDescriptor = new Descriptor(descriptor.version,\n                                               descriptor.directory,\n                                               descriptor.ksname,\n                                               descriptor.cfname,\n                                               fileIndexGenerator.incrementAndGet(),\n                                               descriptor.formatType,\n                                               descriptor.digestComponent);\n            }\n            while (new File(newDescriptor.filenameFor(Component.DATA)).exists());\n\n            logger.info(\"Renaming new SSTable {} to {}\", descriptor, newDescriptor);\n            SSTableWriter.rename(descriptor, newDescriptor, entry.getValue());\n\n            SSTableReader reader;\n            try\n            {\n                reader = SSTableReader.open(newDescriptor, entry.getValue(), metadata);\n            }\n            catch (CorruptSSTableException ex)\n            {\n                FileUtils.handleCorruptSSTable(ex);\n                logger.error(\"Corrupt sstable {}; skipping table\", entry, ex);\n                continue;\n            }\n            catch (FSError ex)\n            {\n                FileUtils.handleFSError(ex);\n                logger.error(\"Cannot read sstable {}; file system error, skipping table\", entry, ex);\n                continue;\n            }\n            catch (IOException ex)\n            {\n                FileUtils.handleCorruptSSTable(new CorruptSSTableException(ex, entry.getKey().filenameFor(Component.DATA)));\n                logger.error(\"Cannot read sstable {}; other IO error, skipping table\", entry, ex);\n                continue;\n            }\n            newSSTables.add(reader);\n        }\n\n        if (newSSTables.isEmpty())\n        {\n            logger.info(\"No new SSTables were found for {}/{}\", keyspace.getName(), name);\n            return;\n        }\n\n        logger.info(\"Loading new SSTables and building secondary indexes for {}/{}: {}\", keyspace.getName(), name, newSSTables);\n\n        try (Refs<SSTableReader> refs = Refs.ref(newSSTables))\n        {\n            data.addSSTables(newSSTables);\n            indexManager.buildAllIndexesBlocking(newSSTables);\n        }\n\n        logger.info(\"Done loading load new SSTables for {}/{}\", keyspace.getName(), name);\n    }"
        ],
        [
            "ScrubTest::testScrubOutOfOrder()",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 -\n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  ",
            "    @Test\n    public void testScrubOutOfOrder() throws Exception\n    {\n        // This test assumes ByteOrderPartitioner to create out-of-order SSTable\n        IPartitioner oldPartitioner = DatabaseDescriptor.getPartitioner();\n        DatabaseDescriptor.setPartitionerUnsafe(new ByteOrderedPartitioner());\n\n        // Create out-of-order SSTable\n        File tempDir = File.createTempFile(\"ScrubTest.testScrubOutOfOrder\", \"\").getParentFile();\n        // create ks/cf directory\n        File tempDataDir = new File(tempDir, String.join(File.separator, KEYSPACE, CF3));\n        tempDataDir.mkdirs();\n        try\n        {\n            CompactionManager.instance.disableAutoCompaction();\n            Keyspace keyspace = Keyspace.open(KEYSPACE);\n            String columnFamily = CF3;\n            ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(columnFamily);\n            cfs.clearUnsafe();\n\n            List<String> keys = Arrays.asList(\"t\", \"a\", \"b\", \"z\", \"c\", \"y\", \"d\");\n            String filename = cfs.getSSTablePath(tempDataDir);\n            Descriptor desc = Descriptor.fromFilename(filename);\n\n            LifecycleTransaction txn = LifecycleTransaction.offline(OperationType.WRITE);\n            try (SSTableTxnWriter writer = new SSTableTxnWriter(txn, createTestWriter(desc, (long) keys.size(), cfs.metadata, txn)))\n            {\n\n                for (String k : keys)\n                {\n                    PartitionUpdate update = UpdateBuilder.create(cfs.metadata, Util.dk(k))\n                                                          .newRow(\"someName\").add(\"val\", \"someValue\")\n                                                          .build();\n\n                    writer.append(update.unfilteredIterator());\n                }\n                writer.finish(false);\n            }\n\n            try\n            {\n                SSTableReader.open(desc, cfs.metadata);\n                fail(\"SSTR validation should have caught the out-of-order rows\");\n            }\n            catch (IllegalStateException ise)\n            { /* this is expected */ }\n\n            // open without validation for scrubbing\n            Set<Component> components = new HashSet<>();\n            if (new File(desc.filenameFor(Component.COMPRESSION_INFO)).exists())\n                components.add(Component.COMPRESSION_INFO);\n            components.add(Component.DATA);\n            components.add(Component.PRIMARY_INDEX);\n            components.add(Component.FILTER);\n            components.add(Component.STATS);\n            components.add(Component.SUMMARY);\n            components.add(Component.TOC);\n\n            SSTableReader sstable = SSTableReader.openNoValidation(desc, components, cfs);\n            if (sstable.last.compareTo(sstable.first) < 0)\n                sstable.last = sstable.first;\n\n            try (LifecycleTransaction scrubTxn = LifecycleTransaction.offline(OperationType.SCRUB, sstable);\n                 Scrubber scrubber = new Scrubber(cfs, scrubTxn, false, true))\n            {\n                scrubber.scrub();\n            }\n            LifecycleTransaction.waitForDeletions();\n            cfs.loadNewSSTables();\n            assertOrderedAll(cfs, 7);\n        }\n        finally\n        {\n            FileUtils.deleteRecursive(tempDataDir);\n            // reset partitioner\n            DatabaseDescriptor.setPartitionerUnsafe(oldPartitioner);\n        }\n    }",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 +\n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  ",
            "    @Test\n    public void testScrubOutOfOrder() throws Exception\n    {\n        // This test assumes ByteOrderPartitioner to create out-of-order SSTable\n        IPartitioner oldPartitioner = DatabaseDescriptor.getPartitioner();\n        DatabaseDescriptor.setPartitionerUnsafe(new ByteOrderedPartitioner());\n\n        // Create out-of-order SSTable\n        File tempDir = File.createTempFile(\"ScrubTest.testScrubOutOfOrder\", \"\").getParentFile();\n        // create ks/cf directory\n        File tempDataDir = new File(tempDir, String.join(File.separator, KEYSPACE, CF3));\n        tempDataDir.mkdirs();\n        try\n        {\n            CompactionManager.instance.disableAutoCompaction();\n            Keyspace keyspace = Keyspace.open(KEYSPACE);\n            String columnFamily = CF3;\n            ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(columnFamily);\n            cfs.clearUnsafe();\n\n            List<String> keys = Arrays.asList(\"t\", \"a\", \"b\", \"z\", \"c\", \"y\", \"d\");\n            String filename = cfs.getSSTablePath(tempDataDir);\n            Descriptor desc = Descriptor.fromFilename(filename);\n\n            LifecycleTransaction txn = LifecycleTransaction.offline(OperationType.WRITE);\n            try (SSTableTxnWriter writer = new SSTableTxnWriter(txn, createTestWriter(desc, (long) keys.size(), cfs.metadata, txn)))\n            {\n\n                for (String k : keys)\n                {\n                    PartitionUpdate update = UpdateBuilder.create(cfs.metadata, Util.dk(k))\n                                                          .newRow(\"someName\").add(\"val\", \"someValue\")\n                                                          .build();\n\n                    writer.append(update.unfilteredIterator());\n                }\n                writer.finish(false);\n            }\n\n            try\n            {\n                SSTableReader.open(desc, cfs.metadata);\n                fail(\"SSTR validation should have caught the out-of-order rows\");\n            }\n            catch (CorruptSSTableException ise)\n            { /* this is expected */ }\n\n            // open without validation for scrubbing\n            Set<Component> components = new HashSet<>();\n            if (new File(desc.filenameFor(Component.COMPRESSION_INFO)).exists())\n                components.add(Component.COMPRESSION_INFO);\n            components.add(Component.DATA);\n            components.add(Component.PRIMARY_INDEX);\n            components.add(Component.FILTER);\n            components.add(Component.STATS);\n            components.add(Component.SUMMARY);\n            components.add(Component.TOC);\n\n            SSTableReader sstable = SSTableReader.openNoValidation(desc, components, cfs);\n            if (sstable.last.compareTo(sstable.first) < 0)\n                sstable.last = sstable.first;\n\n            try (LifecycleTransaction scrubTxn = LifecycleTransaction.offline(OperationType.SCRUB, sstable);\n                 Scrubber scrubber = new Scrubber(cfs, scrubTxn, false, true))\n            {\n                scrubber.scrub();\n            }\n            LifecycleTransaction.waitForDeletions();\n            cfs.loadNewSSTables();\n            assertOrderedAll(cfs, 7);\n        }\n        finally\n        {\n            FileUtils.deleteRecursive(tempDataDir);\n            // reset partitioner\n            DatabaseDescriptor.setPartitionerUnsafe(oldPartitioner);\n        }\n    }"
        ],
        [
            "SSTableReader::open(Descriptor,Set,CFMetaData,boolean,boolean)",
            " 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  ",
            "    public static SSTableReader open(Descriptor descriptor,\n                                      Set<Component> components,\n                                      CFMetaData metadata,\n                                      boolean validate,\n                                      boolean trackHotness) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        // For the 3.0+ sstable format, the (misnomed) stats component hold the serialization header which we need to deserialize the sstable content\n        assert !descriptor.version.storeRows() || components.contains(Component.STATS) : \"Stats component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n        Map<MetadataType, MetadataComponent> sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n        assert !descriptor.version.storeRows() || header != null;\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.debug(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header == null ? null : header.toHeader(metadata));\n\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata);\n            logger.trace(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(trackHotness);\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.trace(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw t;\n        }\n    }",
            " 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450 +\n 451 +\n 452 +\n 453 +\n 454 +\n 455 +\n 456 +\n 457 +\n 458 +\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500 +\n 501 +\n 502 +\n 503 +\n 504 +\n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "    public static SSTableReader open(Descriptor descriptor,\n                                      Set<Component> components,\n                                      CFMetaData metadata,\n                                      boolean validate,\n                                      boolean trackHotness) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        // For the 3.0+ sstable format, the (misnomed) stats component hold the serialization header which we need to deserialize the sstable content\n        assert !descriptor.version.storeRows() || components.contains(Component.STATS) : \"Stats component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n\n        Map<MetadataType, MetadataComponent> sstableMetadata;\n        try\n        {\n            sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        }\n        catch (IOException e)\n        {\n            throw new CorruptSSTableException(e, descriptor.filenameFor(Component.STATS));\n        }\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n        assert !descriptor.version.storeRows() || header != null;\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.debug(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header == null ? null : header.toHeader(metadata));\n\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata);\n            logger.trace(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(trackHotness);\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.trace(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (IOException e)\n        {\n            sstable.selfRef().release();\n            throw new CorruptSSTableException(e, sstable.getFilename());\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw t;\n        }\n    }"
        ]
    ],
    "4d8fc5b050efaef3da818605c31e62b508425972": [
        [
            "RepairSession::syncComplete(RepairJobDesc,NodePair,boolean,List)",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  ",
            "    /**\n     * Notify this session that sync completed/failed with given {@code NodePair}.\n     *\n     * @param desc synced repair job\n     * @param nodes nodes that completed sync\n     * @param success true if sync succeeded\n     */\n    public void syncComplete(RepairJobDesc desc, NodePair nodes, boolean success, List<SessionSummary> summaries)\n    {\n        CompletableRemoteSyncTask task = syncingTasks.get(Pair.create(desc, nodes));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        logger.debug(\"{} Repair completed between {} and {} on {}\", previewKind.logPrefix(getId()), nodes.endpoint1, nodes.endpoint2, desc.columnFamily);\n        task.syncComplete(success, summaries);\n    }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242 +\n 243  \n 244  ",
            "    /**\n     * Notify this session that sync completed/failed with given {@code NodePair}.\n     *\n     * @param desc synced repair job\n     * @param nodes nodes that completed sync\n     * @param success true if sync succeeded\n     */\n    public void syncComplete(RepairJobDesc desc, NodePair nodes, boolean success, List<SessionSummary> summaries)\n    {\n        CompletableRemoteSyncTask task = syncingTasks.get(Pair.create(desc, nodes));\n        if (task == null)\n        {\n            assert terminated;\n            return;\n        }\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"{} Repair completed between {} and {} on {}\", previewKind.logPrefix(getId()), nodes.endpoint1, nodes.endpoint2, desc.columnFamily);\n        task.syncComplete(success, summaries);\n    }"
        ],
        [
            "TokenSerializer::deserialize(IPartitioner,DataInput)",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException\n    {\n        Collection<Token> tokens = new ArrayList<Token>();\n        while (true)\n        {\n            int size = in.readInt();\n            if (size < 1)\n                break;\n            logger.trace(\"Reading token of {}\", FBUtilities.prettyPrintMemory(size));\n            byte[] bintoken = new byte[size];\n            in.readFully(bintoken);\n            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));\n        }\n        return tokens;\n    }",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "    public static Collection<Token> deserialize(IPartitioner partitioner, DataInput in) throws IOException\n    {\n        Collection<Token> tokens = new ArrayList<Token>();\n        while (true)\n        {\n            int size = in.readInt();\n            if (size < 1)\n                break;\n            if (logger.isTraceEnabled())\n                logger.trace(\"Reading token of {}\", FBUtilities.prettyPrintMemory(size));\n            byte[] bintoken = new byte[size];\n            in.readFully(bintoken);\n            tokens.add(partitioner.getTokenFactory().fromByteArray(ByteBuffer.wrap(bintoken)));\n        }\n        return tokens;\n    }"
        ],
        [
            "CompactionTask::runMayThrow()",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258 -\n 259 -\n 260 -\n 261 -\n 262 -\n 263 -\n 264 -\n 265 -\n 266 -\n 267 -\n 268 -\n 269 -\n 270 -\n 271 -\n 272 -\n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert transaction != null;\n\n        if (transaction.originals().isEmpty())\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        CompactionStrategyManager strategy = cfs.getCompactionStrategyManager();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        try (CompactionController controller = getCompactionController(transaction.originals()))\n        {\n\n            final Set<SSTableReader> fullyExpiredSSTables = controller.getFullyExpiredSSTables();\n\n            // select SSTables to compact based on available disk space.\n            buildCompactionCandidatesForAvailableDiskSpace(fullyExpiredSSTables);\n\n            // sanity check: all sstables must belong to the same cfs\n            assert !Iterables.any(transaction.originals(), new Predicate<SSTableReader>()\n            {\n                @Override\n                public boolean apply(SSTableReader sstable)\n                {\n                    return !sstable.descriptor.cfname.equals(cfs.name);\n                }\n            });\n\n            UUID taskId = transaction.opId();\n\n            // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n            // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n            // all the sstables (that existed when we started)\n            StringBuilder ssTableLoggerMsg = new StringBuilder(\"[\");\n            for (SSTableReader sstr : transaction.originals())\n            {\n                ssTableLoggerMsg.append(String.format(\"%s:level=%d, \", sstr.getFilename(), sstr.getSSTableLevel()));\n            }\n            ssTableLoggerMsg.append(\"]\");\n\n            logger.debug(\"Compacting ({}) {}\", taskId, ssTableLoggerMsg);\n\n            RateLimiter limiter = CompactionManager.instance.getRateLimiter();\n            long start = System.nanoTime();\n            long startTime = System.currentTimeMillis();\n            long totalKeysWritten = 0;\n            long estimatedKeys = 0;\n            long inputSizeBytes;\n\n            Set<SSTableReader> actuallyCompact = Sets.difference(transaction.originals(), fullyExpiredSSTables);\n            Collection<SSTableReader> newSStables;\n\n            long[] mergedRowCounts;\n            long totalSourceCQLRows;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            int nowInSec = FBUtilities.nowInSeconds();\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact);\n                 CompactionIterator ci = new CompactionIterator(compactionType, scanners.scanners, controller, nowInSec, taskId))\n            {\n                long lastCheckObsoletion = start;\n                inputSizeBytes = scanners.getTotalCompressedSize();\n                double compressionRatio = scanners.getCompressionRatio();\n                if (compressionRatio == MetadataCollector.NO_COMPRESSION_RATIO)\n                    compressionRatio = 1.0;\n\n                long lastBytesScanned = 0;\n\n                if (!controller.cfs.getCompactionStrategyManager().isActive())\n                    throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                if (collector != null)\n                    collector.beginCompaction(ci);\n\n                try (CompactionAwareWriter writer = getCompactionAwareWriter(cfs, getDirectories(), transaction, actuallyCompact))\n                {\n                    estimatedKeys = writer.estimatedKeys();\n                    while (ci.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                        if (writer.append(ci.next()))\n                            totalKeysWritten++;\n\n\n                        long bytesScanned = scanners.getTotalBytesScanned();\n\n                        //Rate limit the scanners, and account for compression\n                        CompactionManager.compactionRateLimiterAcquire(limiter, bytesScanned, lastBytesScanned, compressionRatio);\n\n                        lastBytesScanned = bytesScanned;\n\n                        if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                        {\n                            controller.maybeRefreshOverlaps();\n                            lastCheckObsoletion = System.nanoTime();\n                        }\n                    }\n\n                    // point of no return\n                    newSStables = writer.finish();\n                }\n                finally\n                {\n                    if (collector != null)\n                        collector.finishCompaction(ci);\n\n                    mergedRowCounts = ci.getMergedRowCounts();\n\n                    totalSourceCQLRows = ci.getTotalSourceCQLRows();\n                }\n            }\n\n            if (transaction.isOffline())\n            {\n                Refs.release(Refs.selfRefs(newSStables));\n            }\n            else\n            {\n                // log a bunch of statistics about the result and save to system table compaction_history\n\n                long durationInNano = System.nanoTime() - start;\n                long dTime = TimeUnit.NANOSECONDS.toMillis(durationInNano);\n                long startsize = inputSizeBytes;\n                long endsize = SSTableReader.getTotalBytes(newSStables);\n                double ratio = (double) endsize / (double) startsize;\n\n                StringBuilder newSSTableNames = new StringBuilder();\n                for (SSTableReader reader : newSStables)\n                    newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n                long totalSourceRows = 0;\n                for (int i = 0; i < mergedRowCounts.length; i++)\n                    totalSourceRows += mergedRowCounts[i] * (i + 1);\n\n                String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getTableName(), mergedRowCounts, startsize, endsize);\n                logger.debug(String.format(\"Compacted (%s) %d sstables to [%s] to level=%d.  %s to %s (~%d%% of original) in %,dms.  Read Throughput = %s, Write Throughput = %s, Row Throughput = ~%,d/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                           taskId,\n                                           transaction.originals().size(),\n                                           newSSTableNames.toString(),\n                                           getLevel(),\n                                           FBUtilities.prettyPrintMemory(startsize),\n                                           FBUtilities.prettyPrintMemory(endsize),\n                                           (int) (ratio * 100),\n                                           dTime,\n                                           FBUtilities.prettyPrintMemoryPerSecond(startsize, durationInNano),\n                                           FBUtilities.prettyPrintMemoryPerSecond(endsize, durationInNano),\n                                           (int) totalSourceCQLRows / (TimeUnit.NANOSECONDS.toSeconds(durationInNano) + 1),\n                                           totalSourceRows,\n                                           totalKeysWritten,\n                                           mergeSummary));\n                logger.trace(\"CF Total Bytes Compacted: {}\", FBUtilities.prettyPrintMemory(CompactionTask.addToTotalBytesCompacted(endsize)));\n                logger.trace(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedKeys, ((double)(totalKeysWritten - estimatedKeys)/totalKeysWritten));\n                cfs.getCompactionStrategyManager().compactionLogger.compaction(startTime, transaction.originals(), System.currentTimeMillis(), newSStables);\n\n                // update the metrics\n                cfs.metric.compactionBytesWritten.inc(endsize);\n            }\n        }\n    }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273 +\n 274 +\n 275 +\n 276 +\n 277 +\n 278 +\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert transaction != null;\n\n        if (transaction.originals().isEmpty())\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        CompactionStrategyManager strategy = cfs.getCompactionStrategyManager();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        try (CompactionController controller = getCompactionController(transaction.originals()))\n        {\n\n            final Set<SSTableReader> fullyExpiredSSTables = controller.getFullyExpiredSSTables();\n\n            // select SSTables to compact based on available disk space.\n            buildCompactionCandidatesForAvailableDiskSpace(fullyExpiredSSTables);\n\n            // sanity check: all sstables must belong to the same cfs\n            assert !Iterables.any(transaction.originals(), new Predicate<SSTableReader>()\n            {\n                @Override\n                public boolean apply(SSTableReader sstable)\n                {\n                    return !sstable.descriptor.cfname.equals(cfs.name);\n                }\n            });\n\n            UUID taskId = transaction.opId();\n\n            // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n            // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n            // all the sstables (that existed when we started)\n            StringBuilder ssTableLoggerMsg = new StringBuilder(\"[\");\n            for (SSTableReader sstr : transaction.originals())\n            {\n                ssTableLoggerMsg.append(String.format(\"%s:level=%d, \", sstr.getFilename(), sstr.getSSTableLevel()));\n            }\n            ssTableLoggerMsg.append(\"]\");\n\n            logger.debug(\"Compacting ({}) {}\", taskId, ssTableLoggerMsg);\n\n            RateLimiter limiter = CompactionManager.instance.getRateLimiter();\n            long start = System.nanoTime();\n            long startTime = System.currentTimeMillis();\n            long totalKeysWritten = 0;\n            long estimatedKeys = 0;\n            long inputSizeBytes;\n\n            Set<SSTableReader> actuallyCompact = Sets.difference(transaction.originals(), fullyExpiredSSTables);\n            Collection<SSTableReader> newSStables;\n\n            long[] mergedRowCounts;\n            long totalSourceCQLRows;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            int nowInSec = FBUtilities.nowInSeconds();\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact);\n                 CompactionIterator ci = new CompactionIterator(compactionType, scanners.scanners, controller, nowInSec, taskId))\n            {\n                long lastCheckObsoletion = start;\n                inputSizeBytes = scanners.getTotalCompressedSize();\n                double compressionRatio = scanners.getCompressionRatio();\n                if (compressionRatio == MetadataCollector.NO_COMPRESSION_RATIO)\n                    compressionRatio = 1.0;\n\n                long lastBytesScanned = 0;\n\n                if (!controller.cfs.getCompactionStrategyManager().isActive())\n                    throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                if (collector != null)\n                    collector.beginCompaction(ci);\n\n                try (CompactionAwareWriter writer = getCompactionAwareWriter(cfs, getDirectories(), transaction, actuallyCompact))\n                {\n                    estimatedKeys = writer.estimatedKeys();\n                    while (ci.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                        if (writer.append(ci.next()))\n                            totalKeysWritten++;\n\n\n                        long bytesScanned = scanners.getTotalBytesScanned();\n\n                        //Rate limit the scanners, and account for compression\n                        CompactionManager.compactionRateLimiterAcquire(limiter, bytesScanned, lastBytesScanned, compressionRatio);\n\n                        lastBytesScanned = bytesScanned;\n\n                        if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                        {\n                            controller.maybeRefreshOverlaps();\n                            lastCheckObsoletion = System.nanoTime();\n                        }\n                    }\n\n                    // point of no return\n                    newSStables = writer.finish();\n                }\n                finally\n                {\n                    if (collector != null)\n                        collector.finishCompaction(ci);\n\n                    mergedRowCounts = ci.getMergedRowCounts();\n\n                    totalSourceCQLRows = ci.getTotalSourceCQLRows();\n                }\n            }\n\n            if (transaction.isOffline())\n            {\n                Refs.release(Refs.selfRefs(newSStables));\n            }\n            else\n            {\n                // log a bunch of statistics about the result and save to system table compaction_history\n\n                long durationInNano = System.nanoTime() - start;\n                long dTime = TimeUnit.NANOSECONDS.toMillis(durationInNano);\n                long startsize = inputSizeBytes;\n                long endsize = SSTableReader.getTotalBytes(newSStables);\n                double ratio = (double) endsize / (double) startsize;\n\n                StringBuilder newSSTableNames = new StringBuilder();\n                for (SSTableReader reader : newSStables)\n                    newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n                long totalSourceRows = 0;\n                for (int i = 0; i < mergedRowCounts.length; i++)\n                    totalSourceRows += mergedRowCounts[i] * (i + 1);\n\n                String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getTableName(), mergedRowCounts, startsize, endsize);\n\n                if (logger.isDebugEnabled())\n                    logger.debug(String.format(\"Compacted (%s) %d sstables to [%s] to level=%d.  %s to %s (~%d%% of original) in %,dms.  Read Throughput = %s, Write Throughput = %s, Row Throughput = ~%,d/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                               taskId,\n                                               transaction.originals().size(),\n                                               newSSTableNames.toString(),\n                                               getLevel(),\n                                               FBUtilities.prettyPrintMemory(startsize),\n                                               FBUtilities.prettyPrintMemory(endsize),\n                                               (int) (ratio * 100),\n                                               dTime,\n                                               FBUtilities.prettyPrintMemoryPerSecond(startsize, durationInNano),\n                                               FBUtilities.prettyPrintMemoryPerSecond(endsize, durationInNano),\n                                               (int) totalSourceCQLRows / (TimeUnit.NANOSECONDS.toSeconds(durationInNano) + 1),\n                                               totalSourceRows,\n                                               totalKeysWritten,\n                                               mergeSummary));\n                if (logger.isTraceEnabled())\n                {\n                    logger.trace(\"CF Total Bytes Compacted: {}\", FBUtilities.prettyPrintMemory(CompactionTask.addToTotalBytesCompacted(endsize)));\n                    logger.trace(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedKeys, ((double)(totalKeysWritten - estimatedKeys)/totalKeysWritten));\n                }\n                cfs.getCompactionStrategyManager().compactionLogger.compaction(startTime, transaction.originals(), System.currentTimeMillis(), newSStables);\n\n                // update the metrics\n                cfs.metric.compactionBytesWritten.inc(endsize);\n            }\n        }\n    }"
        ],
        [
            "TruncateVerbHandler::doVerb(MessageIn,int)",
            "  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53 -\n  54  \n  55  ",
            "    public void doVerb(MessageIn<Truncation> message, int id)\n    {\n        Truncation t = message.payload;\n        Tracing.trace(\"Applying truncation of {}.{}\", t.keyspace, t.columnFamily);\n        try\n        {\n            ColumnFamilyStore cfs = Keyspace.open(t.keyspace).getColumnFamilyStore(t.columnFamily);\n            cfs.truncateBlocking();\n        }\n        catch (Exception e)\n        {\n            logger.error(\"Error in truncation\", e);\n            respondError(t, message);\n\n            if (FSError.findNested(e) != null)\n                throw FSError.findNested(e);\n        }\n        Tracing.trace(\"Enqueuing response to truncate operation to {}\", message.from);\n\n        TruncateResponse response = new TruncateResponse(t.keyspace, t.columnFamily, true);\n        logger.trace(\"{} applied.  Enqueuing response to {}@{} \", new Object[]{ t, id, message.from });\n        MessagingService.instance().sendReply(response.createMessage(), id, message.from);\n    }",
            "  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53 +\n  54  \n  55  ",
            "    public void doVerb(MessageIn<Truncation> message, int id)\n    {\n        Truncation t = message.payload;\n        Tracing.trace(\"Applying truncation of {}.{}\", t.keyspace, t.columnFamily);\n        try\n        {\n            ColumnFamilyStore cfs = Keyspace.open(t.keyspace).getColumnFamilyStore(t.columnFamily);\n            cfs.truncateBlocking();\n        }\n        catch (Exception e)\n        {\n            logger.error(\"Error in truncation\", e);\n            respondError(t, message);\n\n            if (FSError.findNested(e) != null)\n                throw FSError.findNested(e);\n        }\n        Tracing.trace(\"Enqueuing response to truncate operation to {}\", message.from);\n\n        TruncateResponse response = new TruncateResponse(t.keyspace, t.columnFamily, true);\n        logger.trace(\"{} applied.  Enqueuing response to {}@{} \", t, id, message.from );\n        MessagingService.instance().sendReply(response.createMessage(), id, message.from);\n    }"
        ],
        [
            "ColumnFamilyStore::createEphemeralSnapshotMarkerFile(String)",
            "1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005 -\n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  ",
            "    private void createEphemeralSnapshotMarkerFile(final String snapshot)\n    {\n        final File ephemeralSnapshotMarker = getDirectories().getNewEphemeralSnapshotMarkerFile(snapshot);\n\n        try\n        {\n            if (!ephemeralSnapshotMarker.getParentFile().exists())\n                ephemeralSnapshotMarker.getParentFile().mkdirs();\n\n            Files.createFile(ephemeralSnapshotMarker.toPath());\n            logger.trace(\"Created ephemeral snapshot marker file on {}.\", ephemeralSnapshotMarker.getAbsolutePath());\n        }\n        catch (IOException e)\n        {\n            logger.warn(String.format(\"Could not create marker file %s for ephemeral snapshot %s. \" +\n                                      \"In case there is a failure in the operation that created \" +\n                                      \"this snapshot, you may need to clean it manually afterwards.\",\n                                      ephemeralSnapshotMarker.getAbsolutePath(), snapshot), e);\n        }\n    }",
            "1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005 +\n2006 +\n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  ",
            "    private void createEphemeralSnapshotMarkerFile(final String snapshot)\n    {\n        final File ephemeralSnapshotMarker = getDirectories().getNewEphemeralSnapshotMarkerFile(snapshot);\n\n        try\n        {\n            if (!ephemeralSnapshotMarker.getParentFile().exists())\n                ephemeralSnapshotMarker.getParentFile().mkdirs();\n\n            Files.createFile(ephemeralSnapshotMarker.toPath());\n            if (logger.isTraceEnabled())\n                logger.trace(\"Created ephemeral snapshot marker file on {}.\", ephemeralSnapshotMarker.getAbsolutePath());\n        }\n        catch (IOException e)\n        {\n            logger.warn(String.format(\"Could not create marker file %s for ephemeral snapshot %s. \" +\n                                      \"In case there is a failure in the operation that created \" +\n                                      \"this snapshot, you may need to clean it manually afterwards.\",\n                                      ephemeralSnapshotMarker.getAbsolutePath(), snapshot), e);\n        }\n    }"
        ],
        [
            "NettyStreamingMessageSender::KeepAliveTask::run()",
            " 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451 -\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  ",
            "        public void run()\n        {\n            // if the channel has been closed, cancel the scheduled task and return\n            if (!channel.isOpen() || closed)\n            {\n                future.cancel(false);\n                return;\n            }\n\n            // if the channel is currently processing streaming, skip this execution. As this task executes\n            // on the event loop, even if there is a race with a FileStreamTask which changes the channel attribute\n            // after we check it, the FileStreamTask cannot send out any bytes as this KeepAliveTask is executing\n            // on the event loop (and FileStreamTask publishes it's buffer to the channel, consumed after we're done here).\n            if (channel.attr(TRANSFERRING_FILE_ATTR).get())\n                return;\n\n            try\n            {\n                logger.trace(\"{} Sending keep-alive to {}.\", createLogTag(session, channel), session.peer);\n                sendControlMessage(channel, new KeepAliveMessage(), this::keepAliveListener);\n            }\n            catch (IOException ioe)\n            {\n                future.cancel(false);\n            }\n        }",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456 +\n 457 +\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "        public void run()\n        {\n            // if the channel has been closed, cancel the scheduled task and return\n            if (!channel.isOpen() || closed)\n            {\n                future.cancel(false);\n                return;\n            }\n\n            // if the channel is currently processing streaming, skip this execution. As this task executes\n            // on the event loop, even if there is a race with a FileStreamTask which changes the channel attribute\n            // after we check it, the FileStreamTask cannot send out any bytes as this KeepAliveTask is executing\n            // on the event loop (and FileStreamTask publishes it's buffer to the channel, consumed after we're done here).\n            if (channel.attr(TRANSFERRING_FILE_ATTR).get())\n                return;\n\n            try\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"{} Sending keep-alive to {}.\", createLogTag(session, channel), session.peer);\n                sendControlMessage(channel, new KeepAliveMessage(), this::keepAliveListener);\n            }\n            catch (IOException ioe)\n            {\n                future.cancel(false);\n            }\n        }"
        ],
        [
            "Keyspace::applyInternal(Mutation,boolean,boolean,boolean,boolean,CompletableFuture)",
            " 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 -\n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "    /**\n     * This method appends a row to the global CommitLog, then updates memtables and indexes.\n     *\n     * @param mutation       the row to write.  Must not be modified after calling apply, since commitlog append\n     *                       may happen concurrently, depending on the CL Executor type.\n     * @param makeDurable    if true, don't return unless write has been made durable\n     * @param updateIndexes  false to disable index updates (used by CollationController \"defragmenting\")\n     * @param isDroppable    true if this should throw WriteTimeoutException if it does not acquire lock within write_request_timeout_in_ms\n     * @param isDeferrable   true if caller is not waiting for future to complete, so that future may be deferred\n     */\n    private CompletableFuture<?> applyInternal(final Mutation mutation,\n                                               final boolean makeDurable,\n                                               boolean updateIndexes,\n                                               boolean isDroppable,\n                                               boolean isDeferrable,\n                                               CompletableFuture<?> future)\n    {\n        if (TEST_FAIL_WRITES && metadata.name.equals(TEST_FAIL_WRITES_KS))\n            throw new RuntimeException(\"Testing write failures\");\n\n        Lock[] locks = null;\n\n        boolean requiresViewUpdate = updateIndexes && viewManager.updatesAffectView(Collections.singleton(mutation), false);\n\n        if (requiresViewUpdate)\n        {\n            mutation.viewLockAcquireStart.compareAndSet(0L, System.currentTimeMillis());\n\n            // the order of lock acquisition doesn't matter (from a deadlock perspective) because we only use tryLock()\n            Collection<TableId> tableIds = mutation.getTableIds();\n            Iterator<TableId> idIterator = tableIds.iterator();\n\n            locks = new Lock[tableIds.size()];\n            for (int i = 0; i < tableIds.size(); i++)\n            {\n                TableId tableId = idIterator.next();\n                int lockKey = Objects.hash(mutation.key().getKey(), tableId);\n                while (true)\n                {\n                    Lock lock = null;\n\n                    if (TEST_FAIL_MV_LOCKS_COUNT == 0)\n                        lock = ViewManager.acquireLockFor(lockKey);\n                    else\n                        TEST_FAIL_MV_LOCKS_COUNT--;\n\n                    if (lock == null)\n                    {\n                        //throw WTE only if request is droppable\n                        if (isDroppable && (System.currentTimeMillis() - mutation.createdAt) > DatabaseDescriptor.getWriteRpcTimeout())\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            logger.trace(\"Could not acquire lock for {} and table {}\", ByteBufferUtil.bytesToHex(mutation.key().getKey()), columnFamilyStores.get(tableId).name);\n                            Tracing.trace(\"Could not acquire MV lock\");\n                            if (future != null)\n                            {\n                                future.completeExceptionally(new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1));\n                                return future;\n                            }\n                            else\n                                throw new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1);\n                        }\n                        else if (isDeferrable)\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            // This view update can't happen right now. so rather than keep this thread busy\n                            // we will re-apply ourself to the queue and try again later\n                            final CompletableFuture<?> mark = future;\n                            StageManager.getStage(Stage.MUTATION).execute(() ->\n                                                                          applyInternal(mutation, makeDurable, true, isDroppable, true, mark)\n                            );\n                            return future;\n                        }\n                        else\n                        {\n                            // Retry lock on same thread, if mutation is not deferrable.\n                            // Mutation is not deferrable, if applied from MutationStage and caller is waiting for future to finish\n                            // If blocking caller defers future, this may lead to deadlock situation with all MutationStage workers\n                            // being blocked by waiting for futures which will never be processed as all workers are blocked\n                            try\n                            {\n                                // Wait a little bit before retrying to lock\n                                Thread.sleep(10);\n                            }\n                            catch (InterruptedException e)\n                            {\n                                // Just continue\n                            }\n                            continue;\n                        }\n                    }\n                    else\n                    {\n                        locks[i] = lock;\n                    }\n                    break;\n                }\n            }\n\n            long acquireTime = System.currentTimeMillis() - mutation.viewLockAcquireStart.get();\n            // Metrics are only collected for droppable write operations\n            // Bulk non-droppable operations (e.g. commitlog replay, hint delivery) are not measured\n            if (isDroppable)\n            {\n                for(TableId tableId : tableIds)\n                    columnFamilyStores.get(tableId).metric.viewLockAcquireTime.update(acquireTime, TimeUnit.MILLISECONDS);\n            }\n        }\n        int nowInSec = FBUtilities.nowInSeconds();\n        try (WriteContext ctx = getWriteHandler().beginWrite(mutation, makeDurable))\n        {\n            for (PartitionUpdate upd : mutation.getPartitionUpdates())\n            {\n                ColumnFamilyStore cfs = columnFamilyStores.get(upd.metadata().id);\n                if (cfs == null)\n                {\n                    logger.error(\"Attempting to mutate non-existant table {} ({}.{})\", upd.metadata().id, upd.metadata().keyspace, upd.metadata().name);\n                    continue;\n                }\n                AtomicLong baseComplete = new AtomicLong(Long.MAX_VALUE);\n\n                if (requiresViewUpdate)\n                {\n                    try\n                    {\n                        Tracing.trace(\"Creating materialized view mutations from base table replica\");\n                        viewManager.forTable(upd.metadata().id).pushViewReplicaUpdates(upd, makeDurable, baseComplete);\n                    }\n                    catch (Throwable t)\n                    {\n                        JVMStabilityInspector.inspectThrowable(t);\n                        logger.error(String.format(\"Unknown exception caught while attempting to update MaterializedView! %s\",\n                                                   upd.metadata().toString()), t);\n                        throw t;\n                    }\n                }\n\n                UpdateTransaction indexTransaction = updateIndexes\n                                                     ? cfs.indexManager.newUpdateTransaction(upd, ctx, nowInSec)\n                                                     : UpdateTransaction.NO_OP;\n                cfs.getWriteHandler().write(upd, ctx, indexTransaction);\n\n                if (requiresViewUpdate)\n                    baseComplete.set(System.currentTimeMillis());\n            }\n\n            if (future != null) {\n                future.complete(null);\n            }\n            return future;\n        }\n        finally\n        {\n            if (locks != null)\n            {\n                for (Lock lock : locks)\n                    if (lock != null)\n                        lock.unlock();\n            }\n        }\n    }",
            " 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 +\n 558 +\n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  ",
            "    /**\n     * This method appends a row to the global CommitLog, then updates memtables and indexes.\n     *\n     * @param mutation       the row to write.  Must not be modified after calling apply, since commitlog append\n     *                       may happen concurrently, depending on the CL Executor type.\n     * @param makeDurable    if true, don't return unless write has been made durable\n     * @param updateIndexes  false to disable index updates (used by CollationController \"defragmenting\")\n     * @param isDroppable    true if this should throw WriteTimeoutException if it does not acquire lock within write_request_timeout_in_ms\n     * @param isDeferrable   true if caller is not waiting for future to complete, so that future may be deferred\n     */\n    private CompletableFuture<?> applyInternal(final Mutation mutation,\n                                               final boolean makeDurable,\n                                               boolean updateIndexes,\n                                               boolean isDroppable,\n                                               boolean isDeferrable,\n                                               CompletableFuture<?> future)\n    {\n        if (TEST_FAIL_WRITES && metadata.name.equals(TEST_FAIL_WRITES_KS))\n            throw new RuntimeException(\"Testing write failures\");\n\n        Lock[] locks = null;\n\n        boolean requiresViewUpdate = updateIndexes && viewManager.updatesAffectView(Collections.singleton(mutation), false);\n\n        if (requiresViewUpdate)\n        {\n            mutation.viewLockAcquireStart.compareAndSet(0L, System.currentTimeMillis());\n\n            // the order of lock acquisition doesn't matter (from a deadlock perspective) because we only use tryLock()\n            Collection<TableId> tableIds = mutation.getTableIds();\n            Iterator<TableId> idIterator = tableIds.iterator();\n\n            locks = new Lock[tableIds.size()];\n            for (int i = 0; i < tableIds.size(); i++)\n            {\n                TableId tableId = idIterator.next();\n                int lockKey = Objects.hash(mutation.key().getKey(), tableId);\n                while (true)\n                {\n                    Lock lock = null;\n\n                    if (TEST_FAIL_MV_LOCKS_COUNT == 0)\n                        lock = ViewManager.acquireLockFor(lockKey);\n                    else\n                        TEST_FAIL_MV_LOCKS_COUNT--;\n\n                    if (lock == null)\n                    {\n                        //throw WTE only if request is droppable\n                        if (isDroppable && (System.currentTimeMillis() - mutation.createdAt) > DatabaseDescriptor.getWriteRpcTimeout())\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            if (logger.isTraceEnabled())\n                                logger.trace(\"Could not acquire lock for {} and table {}\", ByteBufferUtil.bytesToHex(mutation.key().getKey()), columnFamilyStores.get(tableId).name);\n                            Tracing.trace(\"Could not acquire MV lock\");\n                            if (future != null)\n                            {\n                                future.completeExceptionally(new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1));\n                                return future;\n                            }\n                            else\n                                throw new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1);\n                        }\n                        else if (isDeferrable)\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            // This view update can't happen right now. so rather than keep this thread busy\n                            // we will re-apply ourself to the queue and try again later\n                            final CompletableFuture<?> mark = future;\n                            StageManager.getStage(Stage.MUTATION).execute(() ->\n                                                                          applyInternal(mutation, makeDurable, true, isDroppable, true, mark)\n                            );\n                            return future;\n                        }\n                        else\n                        {\n                            // Retry lock on same thread, if mutation is not deferrable.\n                            // Mutation is not deferrable, if applied from MutationStage and caller is waiting for future to finish\n                            // If blocking caller defers future, this may lead to deadlock situation with all MutationStage workers\n                            // being blocked by waiting for futures which will never be processed as all workers are blocked\n                            try\n                            {\n                                // Wait a little bit before retrying to lock\n                                Thread.sleep(10);\n                            }\n                            catch (InterruptedException e)\n                            {\n                                // Just continue\n                            }\n                            continue;\n                        }\n                    }\n                    else\n                    {\n                        locks[i] = lock;\n                    }\n                    break;\n                }\n            }\n\n            long acquireTime = System.currentTimeMillis() - mutation.viewLockAcquireStart.get();\n            // Metrics are only collected for droppable write operations\n            // Bulk non-droppable operations (e.g. commitlog replay, hint delivery) are not measured\n            if (isDroppable)\n            {\n                for(TableId tableId : tableIds)\n                    columnFamilyStores.get(tableId).metric.viewLockAcquireTime.update(acquireTime, TimeUnit.MILLISECONDS);\n            }\n        }\n        int nowInSec = FBUtilities.nowInSeconds();\n        try (WriteContext ctx = getWriteHandler().beginWrite(mutation, makeDurable))\n        {\n            for (PartitionUpdate upd : mutation.getPartitionUpdates())\n            {\n                ColumnFamilyStore cfs = columnFamilyStores.get(upd.metadata().id);\n                if (cfs == null)\n                {\n                    logger.error(\"Attempting to mutate non-existant table {} ({}.{})\", upd.metadata().id, upd.metadata().keyspace, upd.metadata().name);\n                    continue;\n                }\n                AtomicLong baseComplete = new AtomicLong(Long.MAX_VALUE);\n\n                if (requiresViewUpdate)\n                {\n                    try\n                    {\n                        Tracing.trace(\"Creating materialized view mutations from base table replica\");\n                        viewManager.forTable(upd.metadata().id).pushViewReplicaUpdates(upd, makeDurable, baseComplete);\n                    }\n                    catch (Throwable t)\n                    {\n                        JVMStabilityInspector.inspectThrowable(t);\n                        logger.error(String.format(\"Unknown exception caught while attempting to update MaterializedView! %s\",\n                                                   upd.metadata().toString()), t);\n                        throw t;\n                    }\n                }\n\n                UpdateTransaction indexTransaction = updateIndexes\n                                                     ? cfs.indexManager.newUpdateTransaction(upd, ctx, nowInSec)\n                                                     : UpdateTransaction.NO_OP;\n                cfs.getWriteHandler().write(upd, ctx, indexTransaction);\n\n                if (requiresViewUpdate)\n                    baseComplete.set(System.currentTimeMillis());\n            }\n\n            if (future != null) {\n                future.complete(null);\n            }\n            return future;\n        }\n        finally\n        {\n            if (locks != null)\n            {\n                for (Lock lock : locks)\n                    if (lock != null)\n                        lock.unlock();\n            }\n        }\n    }"
        ],
        [
            "IndexSummaryRedistribution::adjustSamplingLevels(List,Map,double,long)",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181 -\n 182 -\n 183 -\n 184 -\n 185 -\n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "    private List<SSTableReader> adjustSamplingLevels(List<SSTableReader> sstables,\n                                                     Map<TableId, LifecycleTransaction> transactions,\n                                                     double totalReadsPerSec, long memoryPoolCapacity) throws IOException\n    {\n        List<ResampleEntry> toDownsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> toUpsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> forceResample = new ArrayList<>();\n        List<ResampleEntry> forceUpsample = new ArrayList<>();\n        List<SSTableReader> newSSTables = new ArrayList<>(sstables.size());\n\n        // Going from the coldest to the hottest sstables, try to give each sstable an amount of space proportional\n        // to the number of total reads/sec it handles.\n        remainingSpace = memoryPoolCapacity;\n        for (SSTableReader sstable : sstables)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            int minIndexInterval = sstable.metadata().params.minIndexInterval;\n            int maxIndexInterval = sstable.metadata().params.maxIndexInterval;\n\n            double readsPerSec = sstable.getReadMeter() == null ? 0.0 : sstable.getReadMeter().fifteenMinuteRate();\n            long idealSpace = Math.round(remainingSpace * (readsPerSec / totalReadsPerSec));\n\n            // figure out how many entries our idealSpace would buy us, and pick a new sampling level based on that\n            int currentNumEntries = sstable.getIndexSummarySize();\n            double avgEntrySize = sstable.getIndexSummaryOffHeapSize() / (double) currentNumEntries;\n            long targetNumEntries = Math.max(1, Math.round(idealSpace / avgEntrySize));\n            int currentSamplingLevel = sstable.getIndexSummarySamplingLevel();\n            int maxSummarySize = sstable.getMaxIndexSummarySize();\n\n            // if the min_index_interval changed, calculate what our current sampling level would be under the new min\n            if (sstable.getMinIndexInterval() != minIndexInterval)\n            {\n                int effectiveSamplingLevel = (int) Math.round(currentSamplingLevel * (minIndexInterval / (double) sstable.getMinIndexInterval()));\n                maxSummarySize = (int) Math.round(maxSummarySize * (sstable.getMinIndexInterval() / (double) minIndexInterval));\n                logger.trace(\"min_index_interval changed from {} to {}, so the current sampling level for {} is effectively now {} (was {})\",\n                             sstable.getMinIndexInterval(), minIndexInterval, sstable, effectiveSamplingLevel, currentSamplingLevel);\n                currentSamplingLevel = effectiveSamplingLevel;\n            }\n\n            int newSamplingLevel = IndexSummaryBuilder.calculateSamplingLevel(currentSamplingLevel, currentNumEntries, targetNumEntries,\n                    minIndexInterval, maxIndexInterval);\n            int numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, maxSummarySize);\n            double effectiveIndexInterval = sstable.getEffectiveIndexInterval();\n\n            logger.trace(\"{} has {} reads/sec; ideal space for index summary: {} ({} entries); considering moving \" +\n                    \"from level {} ({} entries, {}) \" +\n                    \"to level {} ({} entries, {})\",\n                    sstable.getFilename(), readsPerSec, FBUtilities.prettyPrintMemory(idealSpace), targetNumEntries,\n                    currentSamplingLevel, currentNumEntries, FBUtilities.prettyPrintMemory((long) (currentNumEntries * avgEntrySize)),\n                    newSamplingLevel, numEntriesAtNewSamplingLevel, FBUtilities.prettyPrintMemory((long) (numEntriesAtNewSamplingLevel * avgEntrySize)));\n\n            if (effectiveIndexInterval < minIndexInterval)\n            {\n                // The min_index_interval was changed; re-sample to match it.\n                logger.trace(\"Forcing resample of {} because the current index interval ({}) is below min_index_interval ({})\",\n                        sstable, effectiveIndexInterval, minIndexInterval);\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceResample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else if (effectiveIndexInterval > maxIndexInterval)\n            {\n                // The max_index_interval was lowered; force an upsample to the effective minimum sampling level\n                logger.trace(\"Forcing upsample of {} because the current index interval ({}) is above max_index_interval ({})\",\n                        sstable, effectiveIndexInterval, maxIndexInterval);\n                newSamplingLevel = Math.max(1, (BASE_SAMPLING_LEVEL * minIndexInterval) / maxIndexInterval);\n                numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, sstable.getMaxIndexSummarySize());\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries >= currentNumEntries * UPSAMPLE_THRESHOLD && newSamplingLevel > currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries < currentNumEntries * DOWNSAMPLE_THESHOLD && newSamplingLevel < currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toDownsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else\n            {\n                // keep the same sampling level\n                logger.trace(\"SSTable {} is within thresholds of ideal sampling\", sstable);\n                remainingSpace -= sstable.getIndexSummaryOffHeapSize();\n                newSSTables.add(sstable);\n                transactions.get(sstable.metadata().id).cancel(sstable);\n            }\n            totalReadsPerSec -= readsPerSec;\n        }\n\n        if (remainingSpace > 0)\n        {\n            Pair<List<SSTableReader>, List<ResampleEntry>> result = distributeRemainingSpace(toDownsample, remainingSpace);\n            toDownsample = result.right;\n            newSSTables.addAll(result.left);\n            for (SSTableReader sstable : result.left)\n                transactions.get(sstable.metadata().id).cancel(sstable);\n        }\n\n        // downsample first, then upsample\n        toDownsample.addAll(forceResample);\n        toDownsample.addAll(toUpsample);\n        toDownsample.addAll(forceUpsample);\n        for (ResampleEntry entry : toDownsample)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            SSTableReader sstable = entry.sstable;\n            logger.trace(\"Re-sampling index summary for {} from {}/{} to {}/{} of the original number of entries\",\n                         sstable, sstable.getIndexSummarySamplingLevel(), Downsampling.BASE_SAMPLING_LEVEL,\n                         entry.newSamplingLevel, Downsampling.BASE_SAMPLING_LEVEL);\n            ColumnFamilyStore cfs = Keyspace.open(sstable.metadata().keyspace).getColumnFamilyStore(sstable.metadata().id);\n            SSTableReader replacement = sstable.cloneWithNewSummarySamplingLevel(cfs, entry.newSamplingLevel);\n            newSSTables.add(replacement);\n            transactions.get(sstable.metadata().id).update(replacement, true);\n        }\n\n        return newSSTables;\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 +\n 182 +\n 183 +\n 184 +\n 185 +\n 186 +\n 187 +\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "    private List<SSTableReader> adjustSamplingLevels(List<SSTableReader> sstables,\n                                                     Map<TableId, LifecycleTransaction> transactions,\n                                                     double totalReadsPerSec, long memoryPoolCapacity) throws IOException\n    {\n        List<ResampleEntry> toDownsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> toUpsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> forceResample = new ArrayList<>();\n        List<ResampleEntry> forceUpsample = new ArrayList<>();\n        List<SSTableReader> newSSTables = new ArrayList<>(sstables.size());\n\n        // Going from the coldest to the hottest sstables, try to give each sstable an amount of space proportional\n        // to the number of total reads/sec it handles.\n        remainingSpace = memoryPoolCapacity;\n        for (SSTableReader sstable : sstables)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            int minIndexInterval = sstable.metadata().params.minIndexInterval;\n            int maxIndexInterval = sstable.metadata().params.maxIndexInterval;\n\n            double readsPerSec = sstable.getReadMeter() == null ? 0.0 : sstable.getReadMeter().fifteenMinuteRate();\n            long idealSpace = Math.round(remainingSpace * (readsPerSec / totalReadsPerSec));\n\n            // figure out how many entries our idealSpace would buy us, and pick a new sampling level based on that\n            int currentNumEntries = sstable.getIndexSummarySize();\n            double avgEntrySize = sstable.getIndexSummaryOffHeapSize() / (double) currentNumEntries;\n            long targetNumEntries = Math.max(1, Math.round(idealSpace / avgEntrySize));\n            int currentSamplingLevel = sstable.getIndexSummarySamplingLevel();\n            int maxSummarySize = sstable.getMaxIndexSummarySize();\n\n            // if the min_index_interval changed, calculate what our current sampling level would be under the new min\n            if (sstable.getMinIndexInterval() != minIndexInterval)\n            {\n                int effectiveSamplingLevel = (int) Math.round(currentSamplingLevel * (minIndexInterval / (double) sstable.getMinIndexInterval()));\n                maxSummarySize = (int) Math.round(maxSummarySize * (sstable.getMinIndexInterval() / (double) minIndexInterval));\n                logger.trace(\"min_index_interval changed from {} to {}, so the current sampling level for {} is effectively now {} (was {})\",\n                             sstable.getMinIndexInterval(), minIndexInterval, sstable, effectiveSamplingLevel, currentSamplingLevel);\n                currentSamplingLevel = effectiveSamplingLevel;\n            }\n\n            int newSamplingLevel = IndexSummaryBuilder.calculateSamplingLevel(currentSamplingLevel, currentNumEntries, targetNumEntries,\n                    minIndexInterval, maxIndexInterval);\n            int numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, maxSummarySize);\n            double effectiveIndexInterval = sstable.getEffectiveIndexInterval();\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"{} has {} reads/sec; ideal space for index summary: {} ({} entries); considering moving \" +\n                             \"from level {} ({} entries, {}) \" +\n                             \"to level {} ({} entries, {})\",\n                             sstable.getFilename(), readsPerSec, FBUtilities.prettyPrintMemory(idealSpace), targetNumEntries,\n                             currentSamplingLevel, currentNumEntries, FBUtilities.prettyPrintMemory((long) (currentNumEntries * avgEntrySize)),\n                             newSamplingLevel, numEntriesAtNewSamplingLevel, FBUtilities.prettyPrintMemory((long) (numEntriesAtNewSamplingLevel * avgEntrySize)));\n\n            if (effectiveIndexInterval < minIndexInterval)\n            {\n                // The min_index_interval was changed; re-sample to match it.\n                logger.trace(\"Forcing resample of {} because the current index interval ({}) is below min_index_interval ({})\",\n                        sstable, effectiveIndexInterval, minIndexInterval);\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceResample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else if (effectiveIndexInterval > maxIndexInterval)\n            {\n                // The max_index_interval was lowered; force an upsample to the effective minimum sampling level\n                logger.trace(\"Forcing upsample of {} because the current index interval ({}) is above max_index_interval ({})\",\n                        sstable, effectiveIndexInterval, maxIndexInterval);\n                newSamplingLevel = Math.max(1, (BASE_SAMPLING_LEVEL * minIndexInterval) / maxIndexInterval);\n                numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, sstable.getMaxIndexSummarySize());\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries >= currentNumEntries * UPSAMPLE_THRESHOLD && newSamplingLevel > currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries < currentNumEntries * DOWNSAMPLE_THESHOLD && newSamplingLevel < currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toDownsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else\n            {\n                // keep the same sampling level\n                logger.trace(\"SSTable {} is within thresholds of ideal sampling\", sstable);\n                remainingSpace -= sstable.getIndexSummaryOffHeapSize();\n                newSSTables.add(sstable);\n                transactions.get(sstable.metadata().id).cancel(sstable);\n            }\n            totalReadsPerSec -= readsPerSec;\n        }\n\n        if (remainingSpace > 0)\n        {\n            Pair<List<SSTableReader>, List<ResampleEntry>> result = distributeRemainingSpace(toDownsample, remainingSpace);\n            toDownsample = result.right;\n            newSSTables.addAll(result.left);\n            for (SSTableReader sstable : result.left)\n                transactions.get(sstable.metadata().id).cancel(sstable);\n        }\n\n        // downsample first, then upsample\n        toDownsample.addAll(forceResample);\n        toDownsample.addAll(toUpsample);\n        toDownsample.addAll(forceUpsample);\n        for (ResampleEntry entry : toDownsample)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            SSTableReader sstable = entry.sstable;\n            logger.trace(\"Re-sampling index summary for {} from {}/{} to {}/{} of the original number of entries\",\n                         sstable, sstable.getIndexSummarySamplingLevel(), Downsampling.BASE_SAMPLING_LEVEL,\n                         entry.newSamplingLevel, Downsampling.BASE_SAMPLING_LEVEL);\n            ColumnFamilyStore cfs = Keyspace.open(sstable.metadata().keyspace).getColumnFamilyStore(sstable.metadata().id);\n            SSTableReader replacement = sstable.cloneWithNewSummarySamplingLevel(cfs, entry.newSamplingLevel);\n            newSSTables.add(replacement);\n            transactions.get(sstable.metadata().id).update(replacement, true);\n        }\n\n        return newSSTables;\n    }"
        ],
        [
            "NettyStreamingMessageSender::close()",
            " 494  \n 495  \n 496  \n 497  \n 498 -\n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  ",
            "    @Override\n    public void close()\n    {\n        closed = true;\n        logger.debug(\"{} Closing stream connection channels on {}\", createLogTag(session, null), connectionId);\n        for (ScheduledFuture<?> future : channelKeepAlives)\n            future.cancel(false);\n        channelKeepAlives.clear();\n\n        List<Future<Void>> futures = new ArrayList<>(threadToChannelMap.size());\n        for (Channel channel : threadToChannelMap.values())\n            futures.add(channel.close());\n        FBUtilities.waitOnFutures(futures, 10 * 1000);\n        threadToChannelMap.clear();\n        fileTransferExecutor.shutdownNow();\n\n        if (controlMessageChannel != null)\n            controlMessageChannel.close();\n    }",
            " 501  \n 502  \n 503  \n 504  \n 505 +\n 506 +\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  ",
            "    @Override\n    public void close()\n    {\n        closed = true;\n        if (logger.isDebugEnabled())\n            logger.debug(\"{} Closing stream connection channels on {}\", createLogTag(session, null), connectionId);\n        for (ScheduledFuture<?> future : channelKeepAlives)\n            future.cancel(false);\n        channelKeepAlives.clear();\n\n        List<Future<Void>> futures = new ArrayList<>(threadToChannelMap.size());\n        for (Channel channel : threadToChannelMap.values())\n            futures.add(channel.close());\n        FBUtilities.waitOnFutures(futures, 10 * 1000);\n        threadToChannelMap.clear();\n        fileTransferExecutor.shutdownNow();\n\n        if (controlMessageChannel != null)\n            controlMessageChannel.close();\n    }"
        ],
        [
            "CompactionManager::performAnticompaction(ColumnFamilyStore,Collection,Refs,LifecycleTransaction,long,UUID,UUID)",
            " 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700 -\n 701 -\n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getActiveRepairedSSTableRefs(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @param parentRepairSession parent repair session ID\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt,\n                                      UUID pendingRepair,\n                                      UUID parentRepairSession) throws InterruptedException, IOException\n    {\n        try\n        {\n            ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(parentRepairSession);\n            Preconditions.checkArgument(!prs.isPreview(), \"Cannot anticompact for previews\");\n\n            logger.info(\"{} Starting anticompaction for {}.{} on {}/{} sstables\", PreviewKind.NONE.logPrefix(parentRepairSession), cfs.keyspace.getName(), cfs.getTableName(), validatedForRepair.size(), cfs.getLiveSSTables().size());\n            logger.trace(\"{} Starting anticompaction for ranges {}\", PreviewKind.NONE.logPrefix(parentRepairSession), ranges);\n            Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n\n            Iterator<SSTableReader> sstableIterator = sstables.iterator();\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            Set<SSTableReader> fullyContainedSSTables = findSSTablesToAnticompact(sstableIterator, normalizedRanges, parentRepairSession);\n\n            cfs.metric.bytesMutatedAnticompaction.inc(SSTableReader.getTotalBytes(fullyContainedSSTables));\n            cfs.getCompactionStrategyManager().mutateRepaired(fullyContainedSSTables, repairedAt, pendingRepair);\n            txn.cancel(fullyContainedSSTables);\n            validatedForRepair.release(fullyContainedSSTables);\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt, pendingRepair);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"{} Completed anticompaction successfully\", PreviewKind.NONE.logPrefix(parentRepairSession));\n    }",
            " 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700 +\n 701 +\n 702 +\n 703 +\n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getActiveRepairedSSTableRefs(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @param parentRepairSession parent repair session ID\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt,\n                                      UUID pendingRepair,\n                                      UUID parentRepairSession) throws InterruptedException, IOException\n    {\n        try\n        {\n            ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(parentRepairSession);\n            Preconditions.checkArgument(!prs.isPreview(), \"Cannot anticompact for previews\");\n\n            if (logger.isInfoEnabled())\n                logger.info(\"{} Starting anticompaction for {}.{} on {}/{} sstables\", PreviewKind.NONE.logPrefix(parentRepairSession), cfs.keyspace.getName(), cfs.getTableName(), validatedForRepair.size(), cfs.getLiveSSTables().size());\n            if (logger.isTraceEnabled())\n                logger.trace(\"{} Starting anticompaction for ranges {}\", PreviewKind.NONE.logPrefix(parentRepairSession), ranges);\n            Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n\n            Iterator<SSTableReader> sstableIterator = sstables.iterator();\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            Set<SSTableReader> fullyContainedSSTables = findSSTablesToAnticompact(sstableIterator, normalizedRanges, parentRepairSession);\n\n            cfs.metric.bytesMutatedAnticompaction.inc(SSTableReader.getTotalBytes(fullyContainedSSTables));\n            cfs.getCompactionStrategyManager().mutateRepaired(fullyContainedSSTables, repairedAt, pendingRepair);\n            txn.cancel(fullyContainedSSTables);\n            validatedForRepair.release(fullyContainedSSTables);\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt, pendingRepair);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"{} Completed anticompaction successfully\", PreviewKind.NONE.logPrefix(parentRepairSession));\n    }"
        ],
        [
            "StreamingInboundHandler::StreamDeserializingTask::run()",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 -\n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "        @Override\n        public void run()\n        {\n            try\n            {\n                while (true)\n                {\n                    // do a check of available bytes and possibly sleep some amount of time (then continue).\n                    // this way we can break out of run() sanely or we end up blocking indefintely in StreamMessage.deserialize()\n                    while (buffers.available() == 0)\n                    {\n                        if (closed)\n                            return;\n\n                        Uninterruptibles.sleepUninterruptibly(400, TimeUnit.MILLISECONDS);\n                    }\n\n                    StreamMessage message = StreamMessage.deserialize(buffers, protocolVersion, null);\n\n                    // keep-alives don't necessarily need to be tied to a session (they could be arrive before or after\n                    // wrt session lifecycle, due to races), just log that we received the message and carry on\n                    if (message instanceof KeepAliveMessage)\n                    {\n                        logger.debug(\"{} Received {}\", createLogTag(session, channel), message);\n                        continue;\n                    }\n\n                    if (session == null)\n                        session = deriveSession(message);\n                    logger.debug(\"{} Received {}\", createLogTag(session, channel), message);\n                    session.messageReceived(message);\n                }\n            }\n            catch (EOFException eof)\n            {\n                // ignore\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                if (session != null)\n                {\n                    session.onError(t);\n                }\n                else if (t instanceof StreamReceiveException)\n                {\n                    ((StreamReceiveException)t).session.onError(t);\n                }\n                else\n                {\n                    logger.error(\"{} stream operation from {} failed\", createLogTag(session, channel), remoteAddress, t);\n                }\n            }\n            finally\n            {\n                channel.close();\n                closed = true;\n\n                if (buffers != null)\n                    buffers.close();\n            }\n        }",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 +\n 184 +\n 185  \n 186  \n 187  \n 188  \n 189  \n 190 +\n 191 +\n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "        @Override\n        public void run()\n        {\n            try\n            {\n                while (true)\n                {\n                    // do a check of available bytes and possibly sleep some amount of time (then continue).\n                    // this way we can break out of run() sanely or we end up blocking indefintely in StreamMessage.deserialize()\n                    while (buffers.available() == 0)\n                    {\n                        if (closed)\n                            return;\n\n                        Uninterruptibles.sleepUninterruptibly(400, TimeUnit.MILLISECONDS);\n                    }\n\n                    StreamMessage message = StreamMessage.deserialize(buffers, protocolVersion, null);\n\n                    // keep-alives don't necessarily need to be tied to a session (they could be arrive before or after\n                    // wrt session lifecycle, due to races), just log that we received the message and carry on\n                    if (message instanceof KeepAliveMessage)\n                    {\n                        if (logger.isDebugEnabled())\n                            logger.debug(\"{} Received {}\", createLogTag(session, channel), message);\n                        continue;\n                    }\n\n                    if (session == null)\n                        session = deriveSession(message);\n\n                    if (logger.isDebugEnabled())\n                        logger.debug(\"{} Received {}\", createLogTag(session, channel), message);\n                    session.messageReceived(message);\n                }\n            }\n            catch (EOFException eof)\n            {\n                // ignore\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                if (session != null)\n                {\n                    session.onError(t);\n                }\n                else if (t instanceof StreamReceiveException)\n                {\n                    ((StreamReceiveException)t).session.onError(t);\n                }\n                else\n                {\n                    logger.error(\"{} stream operation from {} failed\", createLogTag(session, channel), remoteAddress, t);\n                }\n            }\n            finally\n            {\n                channel.close();\n                closed = true;\n\n                if (buffers != null)\n                    buffers.close();\n            }\n        }"
        ],
        [
            "Memtable::FlushRunnable::writeSortedContents()",
            " 454  \n 455  \n 456 -\n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486 -\n 487 -\n 488 -\n 489  \n 490  \n 491  \n 492  \n 493  \n 494  ",
            "        private void writeSortedContents()\n        {\n            logger.debug(\"Writing {}, flushed range = ({}, {}]\", Memtable.this.toString(), from, to);\n\n            boolean trackContention = logger.isTraceEnabled();\n            int heavilyContendedRowCount = 0;\n            // (we can't clear out the map as-we-go to free up memory,\n            //  since the memtable is being used for queries in the \"pending flush\" category)\n            for (AtomicBTreePartition partition : toFlush.values())\n            {\n                // Each batchlog partition is a separate entry in the log. And for an entry, we only do 2\n                // operations: 1) we insert the entry and 2) we delete it. Further, BL data is strictly local,\n                // we don't need to preserve tombstones for repair. So if both operation are in this\n                // memtable (which will almost always be the case if there is no ongoing failure), we can\n                // just skip the entry (CASSANDRA-4667).\n                if (isBatchLogTable && !partition.partitionLevelDeletion().isLive() && partition.hasRows())\n                    continue;\n\n                if (trackContention && partition.usePessimisticLocking())\n                    heavilyContendedRowCount++;\n\n                if (!partition.isEmpty())\n                {\n                    try (UnfilteredRowIterator iter = partition.unfilteredIterator())\n                    {\n                        writer.append(iter);\n                    }\n                }\n            }\n\n            long bytesFlushed = writer.getFilePointer();\n            logger.debug(\"Completed flushing {} ({}) for commitlog position {}\",\n                                                                              writer.getFilename(),\n                                                                              FBUtilities.prettyPrintMemory(bytesFlushed),\n                                                                              commitLogUpperBound);\n            // Update the metrics\n            cfs.metric.bytesFlushed.inc(bytesFlushed);\n\n            if (heavilyContendedRowCount > 0)\n                logger.trace(\"High update contention in {}/{} partitions of {} \", heavilyContendedRowCount, toFlush.size(), Memtable.this);\n        }",
            " 454  \n 455  \n 456 +\n 457 +\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487 +\n 488 +\n 489 +\n 490 +\n 491  \n 492  \n 493  \n 494  \n 495  \n 496  ",
            "        private void writeSortedContents()\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"Writing {}, flushed range = ({}, {}]\", Memtable.this.toString(), from, to);\n\n            boolean trackContention = logger.isTraceEnabled();\n            int heavilyContendedRowCount = 0;\n            // (we can't clear out the map as-we-go to free up memory,\n            //  since the memtable is being used for queries in the \"pending flush\" category)\n            for (AtomicBTreePartition partition : toFlush.values())\n            {\n                // Each batchlog partition is a separate entry in the log. And for an entry, we only do 2\n                // operations: 1) we insert the entry and 2) we delete it. Further, BL data is strictly local,\n                // we don't need to preserve tombstones for repair. So if both operation are in this\n                // memtable (which will almost always be the case if there is no ongoing failure), we can\n                // just skip the entry (CASSANDRA-4667).\n                if (isBatchLogTable && !partition.partitionLevelDeletion().isLive() && partition.hasRows())\n                    continue;\n\n                if (trackContention && partition.usePessimisticLocking())\n                    heavilyContendedRowCount++;\n\n                if (!partition.isEmpty())\n                {\n                    try (UnfilteredRowIterator iter = partition.unfilteredIterator())\n                    {\n                        writer.append(iter);\n                    }\n                }\n            }\n\n            long bytesFlushed = writer.getFilePointer();\n            if (logger.isDebugEnabled())\n                logger.debug(\"Completed flushing {} ({}) for commitlog position {}\",\n                             writer.getFilename(),\n                             FBUtilities.prettyPrintMemory(bytesFlushed),\n                             commitLogUpperBound);\n            // Update the metrics\n            cfs.metric.bytesFlushed.inc(bytesFlushed);\n\n            if (heavilyContendedRowCount > 0)\n                logger.trace(\"High update contention in {}/{} partitions of {} \", heavilyContendedRowCount, toFlush.size(), Memtable.this);\n        }"
        ],
        [
            "JMXServerUtils::logJmxSslConfig(SslRMIServerSocketFactory)",
            " 249  \n 250  \n 251 -\n 252 -\n 253 -\n 254 -\n 255 -\n 256 -\n 257 -\n 258 -\n 259  ",
            "    private static void logJmxSslConfig(SslRMIServerSocketFactory serverFactory)\n    {\n        logger.debug(\"JMX SSL configuration. { protocols: [{}], cipher_suites: [{}], require_client_auth: {} }\",\n                     serverFactory.getEnabledProtocols() == null\n                     ? \"'JVM defaults'\"\n                     : Arrays.stream(serverFactory.getEnabledProtocols()).collect(Collectors.joining(\"','\", \"'\", \"'\")),\n                     serverFactory.getEnabledCipherSuites() == null\n                     ? \"'JVM defaults'\"\n                     : Arrays.stream(serverFactory.getEnabledCipherSuites()).collect(Collectors.joining(\"','\", \"'\", \"'\")),\n                     serverFactory.getNeedClientAuth());\n    }",
            " 249  \n 250  \n 251 +\n 252 +\n 253 +\n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260  ",
            "    private static void logJmxSslConfig(SslRMIServerSocketFactory serverFactory)\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"JMX SSL configuration. { protocols: [{}], cipher_suites: [{}], require_client_auth: {} }\",\n                         serverFactory.getEnabledProtocols() == null\n                         ? \"'JVM defaults'\"\n                         : Arrays.stream(serverFactory.getEnabledProtocols()).collect(Collectors.joining(\"','\", \"'\", \"'\")),\n                         serverFactory.getEnabledCipherSuites() == null\n                         ? \"'JVM defaults'\"\n                         : Arrays.stream(serverFactory.getEnabledCipherSuites()).collect(Collectors.joining(\"','\", \"'\", \"'\")),\n                         serverFactory.getNeedClientAuth());\n    }"
        ],
        [
            "NettyStreamingMessageSender::FileStreamTask::acquirePermit(int)",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 -\n 362 -\n 363  \n 364  ",
            "        boolean acquirePermit(int logInterval)\n        {\n            long logIntervalNanos = TimeUnit.MINUTES.toNanos(logInterval);\n            long timeOfLastLogging = System.nanoTime();\n            while (true)\n            {\n                if (closed)\n                    return false;\n                try\n                {\n                    if (fileTransferSemaphore.tryAcquire(1, TimeUnit.SECONDS))\n                        return true;\n\n                    // log a helpful message to operators in case they are wondering why a given session might not be making progress.\n                    long now = System.nanoTime();\n                    if (now - timeOfLastLogging > logIntervalNanos)\n                    {\n                        timeOfLastLogging = now;\n                        OutgoingStreamMessage ofm = (OutgoingStreamMessage)msg;\n                        logger.info(\"{} waiting to acquire a permit to begin streaming {}. This message logs every {} minutes\",\n                                    createLogTag(session, null), ofm.getName(), logInterval);\n                    }\n                }",
            " 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365 +\n 366 +\n 367 +\n 368  \n 369  ",
            "        boolean acquirePermit(int logInterval)\n        {\n            long logIntervalNanos = TimeUnit.MINUTES.toNanos(logInterval);\n            long timeOfLastLogging = System.nanoTime();\n            while (true)\n            {\n                if (closed)\n                    return false;\n                try\n                {\n                    if (fileTransferSemaphore.tryAcquire(1, TimeUnit.SECONDS))\n                        return true;\n\n                    // log a helpful message to operators in case they are wondering why a given session might not be making progress.\n                    long now = System.nanoTime();\n                    if (now - timeOfLastLogging > logIntervalNanos)\n                    {\n                        timeOfLastLogging = now;\n                        OutgoingStreamMessage ofm = (OutgoingStreamMessage)msg;\n\n                        if (logger.isInfoEnabled())\n                            logger.info(\"{} waiting to acquire a permit to begin streaming {}. This message logs every {} minutes\",\n                                        createLogTag(session, null), ofm.getName(), logInterval);\n                    }\n                }"
        ],
        [
            "IndexSummaryRedistribution::redistributeSummaries()",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  ",
            "    public List<SSTableReader> redistributeSummaries() throws IOException\n    {\n        logger.info(\"Redistributing index summaries\");\n        List<SSTableReader> redistribute = new ArrayList<>();\n        for (LifecycleTransaction txn : transactions.values())\n        {\n            redistribute.addAll(txn.originals());\n        }\n\n        long total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, redistribute))\n            total += sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB\",\n                     redistribute.size(), memoryPoolBytes / 1024L / 1024L, total / 1024.0 / 1024.0);\n\n        final Map<SSTableReader, Double> readRates = new HashMap<>(redistribute.size());\n        double totalReadsPerSec = 0.0;\n        for (SSTableReader sstable : redistribute)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            if (sstable.getReadMeter() != null)\n            {\n                Double readRate = sstable.getReadMeter().fifteenMinuteRate();\n                totalReadsPerSec += readRate;\n                readRates.put(sstable, readRate);\n            }\n        }\n        logger.trace(\"Total reads/sec across all sstables in index summary resize process: {}\", totalReadsPerSec);\n\n        // copy and sort by read rates (ascending)\n        List<SSTableReader> sstablesByHotness = new ArrayList<>(redistribute);\n        Collections.sort(sstablesByHotness, new ReadRateComparator(readRates));\n\n        long remainingBytes = memoryPoolBytes;\n        for (SSTableReader sstable : compacting)\n            remainingBytes -= sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Index summaries for compacting SSTables are using {} MB of space\",\n                     (memoryPoolBytes - remainingBytes) / 1024.0 / 1024.0);\n        List<SSTableReader> newSSTables;\n        try (Refs<SSTableReader> refs = Refs.ref(sstablesByHotness))\n        {\n            newSSTables = adjustSamplingLevels(sstablesByHotness, transactions, totalReadsPerSec, remainingBytes);\n\n            for (LifecycleTransaction txn : transactions.values())\n                txn.finish();\n        }\n        total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, newSSTables))\n            total += sstable.getIndexSummaryOffHeapSize();\n        logger.trace(\"Completed resizing of index summaries; current approximate memory used: {}\",\n                     FBUtilities.prettyPrintMemory(total));\n\n        return newSSTables;\n    }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  ",
            "    public List<SSTableReader> redistributeSummaries() throws IOException\n    {\n        logger.info(\"Redistributing index summaries\");\n        List<SSTableReader> redistribute = new ArrayList<>();\n        for (LifecycleTransaction txn : transactions.values())\n        {\n            redistribute.addAll(txn.originals());\n        }\n\n        long total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, redistribute))\n            total += sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB\",\n                     redistribute.size(), memoryPoolBytes / 1024L / 1024L, total / 1024.0 / 1024.0);\n\n        final Map<SSTableReader, Double> readRates = new HashMap<>(redistribute.size());\n        double totalReadsPerSec = 0.0;\n        for (SSTableReader sstable : redistribute)\n        {\n            if (isStopRequested())\n                throw new CompactionInterruptedException(getCompactionInfo());\n\n            if (sstable.getReadMeter() != null)\n            {\n                Double readRate = sstable.getReadMeter().fifteenMinuteRate();\n                totalReadsPerSec += readRate;\n                readRates.put(sstable, readRate);\n            }\n        }\n        logger.trace(\"Total reads/sec across all sstables in index summary resize process: {}\", totalReadsPerSec);\n\n        // copy and sort by read rates (ascending)\n        List<SSTableReader> sstablesByHotness = new ArrayList<>(redistribute);\n        Collections.sort(sstablesByHotness, new ReadRateComparator(readRates));\n\n        long remainingBytes = memoryPoolBytes;\n        for (SSTableReader sstable : compacting)\n            remainingBytes -= sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Index summaries for compacting SSTables are using {} MB of space\",\n                     (memoryPoolBytes - remainingBytes) / 1024.0 / 1024.0);\n        List<SSTableReader> newSSTables;\n        try (Refs<SSTableReader> refs = Refs.ref(sstablesByHotness))\n        {\n            newSSTables = adjustSamplingLevels(sstablesByHotness, transactions, totalReadsPerSec, remainingBytes);\n\n            for (LifecycleTransaction txn : transactions.values())\n                txn.finish();\n        }\n        total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, newSSTables))\n            total += sstable.getIndexSummaryOffHeapSize();\n        if (logger.isTraceEnabled())\n            logger.trace(\"Completed resizing of index summaries; current approximate memory used: {}\",\n                     FBUtilities.prettyPrintMemory(total));\n\n        return newSSTables;\n    }"
        ],
        [
            "SSTableReader::openForBatch(Descriptor,Set,TableMetadataRef)",
            " 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431 -\n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  ",
            "    /**\n     * Open SSTable reader to be used in batch mode(such as sstableloader).\n     *\n     * @param descriptor\n     * @param components\n     * @param metadata\n     * @return opened SSTableReader\n     * @throws IOException\n     */\n    public static SSTableReader openForBatch(Descriptor descriptor, Set<Component> components, TableMetadataRef metadata)\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n        Map<MetadataType, MetadataComponent> sstableMetadata;\n        try\n        {\n             sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        }\n        catch (IOException e)\n        {\n            throw new CorruptSSTableException(e, descriptor.filenameFor(Component.STATS));\n        }\n\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.get().partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(\"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                         descriptor, validationMetadata.partitioner, partitionerName);\n            System.exit(1);\n        }\n\n        long fileLength = new File(descriptor.filenameFor(Component.DATA)).length();\n        logger.debug(\"Opening {} ({})\", descriptor, FBUtilities.prettyPrintMemory(fileLength));\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header.toHeader(metadata.get()));\n\n        try(FileHandle.Builder ibuilder = new FileHandle.Builder(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX))\n                                                     .mmapped(DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.mmap)\n                                                     .withChunkCache(ChunkCache.instance);\n            FileHandle.Builder dbuilder = new FileHandle.Builder(sstable.descriptor.filenameFor(Component.DATA)).compressed(sstable.compression)\n                                                     .mmapped(DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.mmap)\n                                                     .withChunkCache(ChunkCache.instance))\n        {\n            if (!sstable.loadSummary())\n            {\n                try\n                {\n                    sstable.buildSummary(false, false, Downsampling.BASE_SAMPLING_LEVEL);\n                }\n                catch(IOException e)\n                {\n                    throw new CorruptSSTableException(e, sstable.getFilename());\n                }\n            }\n            long indexFileLength = new File(descriptor.filenameFor(Component.PRIMARY_INDEX)).length();\n            int dataBufferSize = sstable.optimizationStrategy.bufferSize(statsMetadata.estimatedPartitionSize.percentile(DatabaseDescriptor.getDiskOptimizationEstimatePercentile()));\n            int indexBufferSize = sstable.optimizationStrategy.bufferSize(indexFileLength / sstable.indexSummary.size());\n            sstable.ifile = ibuilder.bufferSize(indexBufferSize).complete();\n            sstable.dfile = dbuilder.bufferSize(dataBufferSize).complete();\n            sstable.bf = FilterFactory.AlwaysPresent;\n            sstable.setup(false);\n            return sstable;\n        }\n    }",
            " 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431 +\n 432 +\n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "    /**\n     * Open SSTable reader to be used in batch mode(such as sstableloader).\n     *\n     * @param descriptor\n     * @param components\n     * @param metadata\n     * @return opened SSTableReader\n     * @throws IOException\n     */\n    public static SSTableReader openForBatch(Descriptor descriptor, Set<Component> components, TableMetadataRef metadata)\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n        Map<MetadataType, MetadataComponent> sstableMetadata;\n        try\n        {\n             sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        }\n        catch (IOException e)\n        {\n            throw new CorruptSSTableException(e, descriptor.filenameFor(Component.STATS));\n        }\n\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.get().partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(\"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                         descriptor, validationMetadata.partitioner, partitionerName);\n            System.exit(1);\n        }\n\n        long fileLength = new File(descriptor.filenameFor(Component.DATA)).length();\n        if (logger.isDebugEnabled())\n            logger.debug(\"Opening {} ({})\", descriptor, FBUtilities.prettyPrintMemory(fileLength));\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header.toHeader(metadata.get()));\n\n        try(FileHandle.Builder ibuilder = new FileHandle.Builder(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX))\n                                                     .mmapped(DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.mmap)\n                                                     .withChunkCache(ChunkCache.instance);\n            FileHandle.Builder dbuilder = new FileHandle.Builder(sstable.descriptor.filenameFor(Component.DATA)).compressed(sstable.compression)\n                                                     .mmapped(DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.mmap)\n                                                     .withChunkCache(ChunkCache.instance))\n        {\n            if (!sstable.loadSummary())\n            {\n                try\n                {\n                    sstable.buildSummary(false, false, Downsampling.BASE_SAMPLING_LEVEL);\n                }\n                catch(IOException e)\n                {\n                    throw new CorruptSSTableException(e, sstable.getFilename());\n                }\n            }\n            long indexFileLength = new File(descriptor.filenameFor(Component.PRIMARY_INDEX)).length();\n            int dataBufferSize = sstable.optimizationStrategy.bufferSize(statsMetadata.estimatedPartitionSize.percentile(DatabaseDescriptor.getDiskOptimizationEstimatePercentile()));\n            int indexBufferSize = sstable.optimizationStrategy.bufferSize(indexFileLength / sstable.indexSummary.size());\n            sstable.ifile = ibuilder.bufferSize(indexBufferSize).complete();\n            sstable.dfile = dbuilder.bufferSize(dataBufferSize).complete();\n            sstable.bf = FilterFactory.AlwaysPresent;\n            sstable.setup(false);\n            return sstable;\n        }\n    }"
        ],
        [
            "SSTableReader::open(Descriptor,Set,TableMetadataRef,boolean,boolean)",
            " 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521 -\n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  ",
            "    /**\n     * Open an SSTable for reading\n     * @param descriptor SSTable to open\n     * @param components Components included with this SSTable\n     * @param metadata for this SSTables CF\n     * @param validate Check SSTable for corruption (limited)\n     * @param isOffline Whether we are opening this SSTable \"offline\", for example from an external tool or not for inclusion in queries (validations)\n     *                  This stops regenerating BF + Summaries and also disables tracking of hotness for the SSTable.\n     * @return {@link SSTableReader}\n     * @throws IOException\n     */\n    public static SSTableReader open(Descriptor descriptor,\n                                     Set<Component> components,\n                                     TableMetadataRef metadata,\n                                     boolean validate,\n                                     boolean isOffline)\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        // For the 3.0+ sstable format, the (misnomed) stats component hold the serialization header which we need to deserialize the sstable content\n        assert components.contains(Component.STATS) : \"Stats component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n\n        Map<MetadataType, MetadataComponent> sstableMetadata;\n        try\n        {\n            sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        }\n        catch (Throwable t)\n        {\n            throw new CorruptSSTableException(t, descriptor.filenameFor(Component.STATS));\n        }\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n        assert header != null;\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.get().partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(\"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                         descriptor, validationMetadata.partitioner, partitionerName);\n            System.exit(1);\n        }\n\n        long fileLength = new File(descriptor.filenameFor(Component.DATA)).length();\n        logger.debug(\"Opening {} ({})\", descriptor, FBUtilities.prettyPrintMemory(fileLength));\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header.toHeader(metadata.get()));\n\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata, isOffline);\n            logger.trace(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(!isOffline); // Don't track hotness if we're offline.\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.trace(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw new CorruptSSTableException(t, sstable.getFilename());\n        }\n    }",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 +\n 523 +\n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  ",
            "    /**\n     * Open an SSTable for reading\n     * @param descriptor SSTable to open\n     * @param components Components included with this SSTable\n     * @param metadata for this SSTables CF\n     * @param validate Check SSTable for corruption (limited)\n     * @param isOffline Whether we are opening this SSTable \"offline\", for example from an external tool or not for inclusion in queries (validations)\n     *                  This stops regenerating BF + Summaries and also disables tracking of hotness for the SSTable.\n     * @return {@link SSTableReader}\n     * @throws IOException\n     */\n    public static SSTableReader open(Descriptor descriptor,\n                                     Set<Component> components,\n                                     TableMetadataRef metadata,\n                                     boolean validate,\n                                     boolean isOffline)\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        // For the 3.0+ sstable format, the (misnomed) stats component hold the serialization header which we need to deserialize the sstable content\n        assert components.contains(Component.STATS) : \"Stats component is missing for sstable \" + descriptor;\n\n        EnumSet<MetadataType> types = EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS, MetadataType.HEADER);\n\n        Map<MetadataType, MetadataComponent> sstableMetadata;\n        try\n        {\n            sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor, types);\n        }\n        catch (Throwable t)\n        {\n            throw new CorruptSSTableException(t, descriptor.filenameFor(Component.STATS));\n        }\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n        SerializationHeader.Component header = (SerializationHeader.Component) sstableMetadata.get(MetadataType.HEADER);\n        assert header != null;\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = metadata.get().partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(\"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                         descriptor, validationMetadata.partitioner, partitionerName);\n            System.exit(1);\n        }\n\n        long fileLength = new File(descriptor.filenameFor(Component.DATA)).length();\n        if (logger.isDebugEnabled())\n            logger.debug(\"Opening {} ({})\", descriptor, FBUtilities.prettyPrintMemory(fileLength));\n        SSTableReader sstable = internalOpen(descriptor,\n                                             components,\n                                             metadata,\n                                             System.currentTimeMillis(),\n                                             statsMetadata,\n                                             OpenReason.NORMAL,\n                                             header.toHeader(metadata.get()));\n\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata, isOffline);\n            logger.trace(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(!isOffline); // Don't track hotness if we're offline.\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.trace(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw new CorruptSSTableException(t, sstable.getFilename());\n        }\n    }"
        ],
        [
            "InboundHandshakeHandler::handleMessagingStartResponse(ChannelHandlerContext,ByteBuf)",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251  \n 252  \n 253  \n 254  ",
            "    /**\n     * Handles the third (and last) message in the internode messaging handshake protocol. Grabs the protocol version and\n     * IP addr the peer wants to use.\n     */\n    @VisibleForTesting\n    State handleMessagingStartResponse(ChannelHandlerContext ctx, ByteBuf in) throws IOException\n    {\n        ThirdHandshakeMessage msg = ThirdHandshakeMessage.maybeDecode(in);\n        if (msg == null)\n            return State.AWAIT_MESSAGING_START_RESPONSE;\n\n        logger.trace(\"received third handshake message from peer {}, message = {}\", ctx.channel().remoteAddress(), msg);\n        if (handshakeTimeout != null)\n        {\n            handshakeTimeout.cancel(false);\n            handshakeTimeout = null;\n        }\n\n        int maxVersion = msg.messagingVersion;\n        if (maxVersion > MessagingService.current_version)\n        {\n            logger.error(\"peer wants to use a messaging version higher ({}) than what this node supports ({})\", maxVersion, MessagingService.current_version);\n            ctx.close();\n            return State.HANDSHAKE_FAIL;\n        }\n\n        // record the (true) version of the endpoint\n        InetAddressAndPort from = msg.address;\n        MessagingService.instance().setVersion(from, maxVersion);\n        logger.trace(\"Set version for {} to {} (will use {})\", from, maxVersion, MessagingService.instance().getVersion(from));\n\n        setupMessagingPipeline(ctx.pipeline(), from, compressed, version);\n        return State.HANDSHAKE_COMPLETE;\n    }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 +\n 251 +\n 252  \n 253  \n 254  \n 255  ",
            "    /**\n     * Handles the third (and last) message in the internode messaging handshake protocol. Grabs the protocol version and\n     * IP addr the peer wants to use.\n     */\n    @VisibleForTesting\n    State handleMessagingStartResponse(ChannelHandlerContext ctx, ByteBuf in) throws IOException\n    {\n        ThirdHandshakeMessage msg = ThirdHandshakeMessage.maybeDecode(in);\n        if (msg == null)\n            return State.AWAIT_MESSAGING_START_RESPONSE;\n\n        logger.trace(\"received third handshake message from peer {}, message = {}\", ctx.channel().remoteAddress(), msg);\n        if (handshakeTimeout != null)\n        {\n            handshakeTimeout.cancel(false);\n            handshakeTimeout = null;\n        }\n\n        int maxVersion = msg.messagingVersion;\n        if (maxVersion > MessagingService.current_version)\n        {\n            logger.error(\"peer wants to use a messaging version higher ({}) than what this node supports ({})\", maxVersion, MessagingService.current_version);\n            ctx.close();\n            return State.HANDSHAKE_FAIL;\n        }\n\n        // record the (true) version of the endpoint\n        InetAddressAndPort from = msg.address;\n        MessagingService.instance().setVersion(from, maxVersion);\n        if (logger.isTraceEnabled())\n            logger.trace(\"Set version for {} to {} (will use {})\", from, maxVersion, MessagingService.instance().getVersion(from));\n\n        setupMessagingPipeline(ctx.pipeline(), from, compressed, version);\n        return State.HANDSHAKE_COMPLETE;\n    }"
        ],
        [
            "NettyStreamingMessageSender::scheduleKeepAliveTask(Channel)",
            " 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "    private void scheduleKeepAliveTask(Channel channel)\n    {\n        int keepAlivePeriod = DatabaseDescriptor.getStreamingKeepAlivePeriod();\n        logger.debug(\"{} Scheduling keep-alive task with {}s period.\", createLogTag(session, channel), keepAlivePeriod);\n\n        KeepAliveTask task = new KeepAliveTask(channel, session);\n        ScheduledFuture<?> scheduledFuture = channel.eventLoop().scheduleAtFixedRate(task, 0, keepAlivePeriod, TimeUnit.SECONDS);\n        channelKeepAlives.add(scheduledFuture);\n        task.future = scheduledFuture;\n    }",
            " 170  \n 171  \n 172  \n 173 +\n 174 +\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "    private void scheduleKeepAliveTask(Channel channel)\n    {\n        int keepAlivePeriod = DatabaseDescriptor.getStreamingKeepAlivePeriod();\n        if (logger.isDebugEnabled())\n            logger.debug(\"{} Scheduling keep-alive task with {}s period.\", createLogTag(session, channel), keepAlivePeriod);\n\n        KeepAliveTask task = new KeepAliveTask(channel, session);\n        ScheduledFuture<?> scheduledFuture = channel.eventLoop().scheduleAtFixedRate(task, 0, keepAlivePeriod, TimeUnit.SECONDS);\n        channelKeepAlives.add(scheduledFuture);\n        task.future = scheduledFuture;\n    }"
        ],
        [
            "NettyStreamingMessageSender::sendControlMessage(Channel,StreamMessage,GenericFutureListener)",
            " 233  \n 234  \n 235 -\n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "    private void sendControlMessage(Channel channel, StreamMessage message, GenericFutureListener listener) throws IOException\n    {\n        logger.debug(\"{} Sending {}\", createLogTag(session, channel), message);\n\n        // we anticipate that the control messages are rather small, so allocating a ByteBuf shouldn't  blow out of memory.\n        long messageSize = StreamMessage.serializedSize(message, protocolVersion);\n        if (messageSize > 1 << 30)\n        {\n            throw new IllegalStateException(String.format(\"%s something is seriously wrong with the calculated stream control message's size: %d bytes, type is %s\",\n                                                          createLogTag(session, channel), messageSize, message.type));\n        }\n\n        // as control messages are (expected to be) small, we can simply allocate a ByteBuf here, wrap it, and send via the channel\n        ByteBuf buf = channel.alloc().directBuffer((int) messageSize, (int) messageSize);\n        ByteBuffer nioBuf = buf.nioBuffer(0, (int) messageSize);\n        @SuppressWarnings(\"resource\")\n        DataOutputBufferFixed out = new DataOutputBufferFixed(nioBuf);\n        StreamMessage.serialize(message, out, protocolVersion, session);\n        assert nioBuf.position() == nioBuf.limit();\n        buf.writerIndex(nioBuf.position());\n\n        ChannelFuture channelFuture = channel.writeAndFlush(buf);\n        channelFuture.addListener(future -> listener.operationComplete(future));\n    }",
            " 235  \n 236  \n 237 +\n 238 +\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "    private void sendControlMessage(Channel channel, StreamMessage message, GenericFutureListener listener) throws IOException\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"{} Sending {}\", createLogTag(session, channel), message);\n\n        // we anticipate that the control messages are rather small, so allocating a ByteBuf shouldn't  blow out of memory.\n        long messageSize = StreamMessage.serializedSize(message, protocolVersion);\n        if (messageSize > 1 << 30)\n        {\n            throw new IllegalStateException(String.format(\"%s something is seriously wrong with the calculated stream control message's size: %d bytes, type is %s\",\n                                                          createLogTag(session, channel), messageSize, message.type));\n        }\n\n        // as control messages are (expected to be) small, we can simply allocate a ByteBuf here, wrap it, and send via the channel\n        ByteBuf buf = channel.alloc().directBuffer((int) messageSize, (int) messageSize);\n        ByteBuffer nioBuf = buf.nioBuffer(0, (int) messageSize);\n        @SuppressWarnings(\"resource\")\n        DataOutputBufferFixed out = new DataOutputBufferFixed(nioBuf);\n        StreamMessage.serialize(message, out, protocolVersion, session);\n        assert nioBuf.position() == nioBuf.limit();\n        buf.writerIndex(nioBuf.position());\n\n        ChannelFuture channelFuture = channel.writeAndFlush(buf);\n        channelFuture.addListener(future -> listener.operationComplete(future));\n    }"
        ],
        [
            "MetadataSerializer::mutateLevel(Descriptor,int)",
            " 222  \n 223  \n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "    public void mutateLevel(Descriptor descriptor, int newLevel) throws IOException\n    {\n        logger.trace(\"Mutating {} to level {}\", descriptor.filenameFor(Component.STATS), newLevel);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateLevel(newLevel));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }",
            " 222  \n 223  \n 224 +\n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "    public void mutateLevel(Descriptor descriptor, int newLevel) throws IOException\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Mutating {} to level {}\", descriptor.filenameFor(Component.STATS), newLevel);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateLevel(newLevel));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }"
        ],
        [
            "SizeEstimatesRecorder::run()",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81 -\n  82 -\n  83 -\n  84  \n  85  \n  86  ",
            "    public void run()\n    {\n        TokenMetadata metadata = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap();\n        if (!metadata.isMember(FBUtilities.getBroadcastAddressAndPort()))\n        {\n            logger.debug(\"Node is not part of the ring; not recording size estimates\");\n            return;\n        }\n\n        logger.trace(\"Recording size estimates\");\n\n        for (Keyspace keyspace : Keyspace.nonLocalStrategy())\n        {\n            Collection<Range<Token>> localRanges = StorageService.instance.getPrimaryRangesForEndpoint(keyspace.getName(),\n                    FBUtilities.getBroadcastAddressAndPort());\n            for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n            {\n                long start = System.nanoTime();\n                recordSizeEstimates(table, localRanges);\n                long passed = System.nanoTime() - start;\n                logger.trace(\"Spent {} milliseconds on estimating {}.{} size\",\n                             TimeUnit.NANOSECONDS.toMillis(passed),\n                             table.metadata.keyspace,\n                             table.metadata.name);\n            }\n        }\n    }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85  \n  86  \n  87  ",
            "    public void run()\n    {\n        TokenMetadata metadata = StorageService.instance.getTokenMetadata().cloneOnlyTokenMap();\n        if (!metadata.isMember(FBUtilities.getBroadcastAddressAndPort()))\n        {\n            logger.debug(\"Node is not part of the ring; not recording size estimates\");\n            return;\n        }\n\n        logger.trace(\"Recording size estimates\");\n\n        for (Keyspace keyspace : Keyspace.nonLocalStrategy())\n        {\n            Collection<Range<Token>> localRanges = StorageService.instance.getPrimaryRangesForEndpoint(keyspace.getName(),\n                    FBUtilities.getBroadcastAddressAndPort());\n            for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n            {\n                long start = System.nanoTime();\n                recordSizeEstimates(table, localRanges);\n                long passed = System.nanoTime() - start;\n                if (logger.isTraceEnabled())\n                    logger.trace(\"Spent {} milliseconds on estimating {}.{} size\",\n                                 TimeUnit.NANOSECONDS.toMillis(passed),\n                                 table.metadata.keyspace,\n                                 table.metadata.name);\n            }\n        }\n    }"
        ],
        [
            "NettyStreamingMessageSender::createLogTag(StreamSession,Channel)",
            " 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  ",
            "     static String createLogTag(StreamSession session, Channel channel)\n    {\n        StringBuilder sb = new StringBuilder(64);\n        sb.append(\"[Stream\");\n\n        if (session != null)\n                sb.append(\" #\").append(session.planId());\n\n        if (channel != null)\n                sb.append(\" channel: \").append(channel.id());\n\n        sb.append(']');\n        return sb.toString();",
            " 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  ",
            "    static String createLogTag(StreamSession session, Channel channel)\n    {\n        StringBuilder sb = new StringBuilder(64);\n        sb.append(\"[Stream\");\n\n        if (session != null)\n                sb.append(\" #\").append(session.planId());\n\n        if (channel != null)\n                sb.append(\" channel: \").append(channel.id());\n\n        sb.append(']');\n        return sb.toString();\n    }"
        ],
        [
            "assureSufficientLiveNodes(Keyspace,Iterable)",
            " 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 -\n 297  \n 298  \n 299  \n 300  \n 301  ",
            "    public void assureSufficientLiveNodes(Keyspace keyspace, Iterable<InetAddressAndPort> liveEndpoints) throws UnavailableException\n    {\n        int blockFor = blockFor(keyspace);\n        switch (this)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n                if (countLocalEndpoints(liveEndpoints) == 0)\n                    throw new UnavailableException(this, 1, 0);\n                break;\n            case LOCAL_QUORUM:\n                int localLive = countLocalEndpoints(liveEndpoints);\n                if (localLive < blockFor)\n                {\n                    if (logger.isTraceEnabled())\n                    {\n                        StringBuilder builder = new StringBuilder(\"Local replicas [\");\n                        for (InetAddressAndPort endpoint : liveEndpoints)\n                        {\n                            if (isLocal(endpoint))\n                                builder.append(endpoint).append(\",\");\n                        }\n                        builder.append(\"] are insufficient to satisfy LOCAL_QUORUM requirement of \").append(blockFor).append(\" live nodes in '\").append(DatabaseDescriptor.getLocalDataCenter()).append(\"'\");\n                        logger.trace(builder.toString());\n                    }\n                    throw new UnavailableException(this, blockFor, localLive);\n                }\n                break;\n            case EACH_QUORUM:\n                if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)\n                {\n                    for (Map.Entry<String, Integer> entry : countPerDCEndpoints(keyspace, liveEndpoints).entrySet())\n                    {\n                        int dcBlockFor = localQuorumFor(keyspace, entry.getKey());\n                        int dcLive = entry.getValue();\n                        if (dcLive < dcBlockFor)\n                            throw new UnavailableException(this, entry.getKey(), dcBlockFor, dcLive);\n                    }\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = Iterables.size(liveEndpoints);\n                if (live < blockFor)\n                {\n                    logger.trace(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(liveEndpoints), blockFor);\n                    throw new UnavailableException(this, blockFor, live);\n                }\n                break;\n        }\n    }",
            " 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 +\n 297 +\n 298  \n 299  \n 300  \n 301  \n 302  ",
            "    public void assureSufficientLiveNodes(Keyspace keyspace, Iterable<InetAddressAndPort> liveEndpoints) throws UnavailableException\n    {\n        int blockFor = blockFor(keyspace);\n        switch (this)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n                if (countLocalEndpoints(liveEndpoints) == 0)\n                    throw new UnavailableException(this, 1, 0);\n                break;\n            case LOCAL_QUORUM:\n                int localLive = countLocalEndpoints(liveEndpoints);\n                if (localLive < blockFor)\n                {\n                    if (logger.isTraceEnabled())\n                    {\n                        StringBuilder builder = new StringBuilder(\"Local replicas [\");\n                        for (InetAddressAndPort endpoint : liveEndpoints)\n                        {\n                            if (isLocal(endpoint))\n                                builder.append(endpoint).append(\",\");\n                        }\n                        builder.append(\"] are insufficient to satisfy LOCAL_QUORUM requirement of \").append(blockFor).append(\" live nodes in '\").append(DatabaseDescriptor.getLocalDataCenter()).append(\"'\");\n                        logger.trace(builder.toString());\n                    }\n                    throw new UnavailableException(this, blockFor, localLive);\n                }\n                break;\n            case EACH_QUORUM:\n                if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)\n                {\n                    for (Map.Entry<String, Integer> entry : countPerDCEndpoints(keyspace, liveEndpoints).entrySet())\n                    {\n                        int dcBlockFor = localQuorumFor(keyspace, entry.getKey());\n                        int dcLive = entry.getValue();\n                        if (dcLive < dcBlockFor)\n                            throw new UnavailableException(this, entry.getKey(), dcBlockFor, dcLive);\n                    }\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = Iterables.size(liveEndpoints);\n                if (live < blockFor)\n                {\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(liveEndpoints), blockFor);\n                    throw new UnavailableException(this, blockFor, live);\n                }\n                break;\n        }\n    }"
        ],
        [
            "MetadataSerializer::mutateRepaired(Descriptor,long,UUID)",
            " 232  \n 233  \n 234 -\n 235 -\n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "    public void mutateRepaired(Descriptor descriptor, long newRepairedAt, UUID newPendingRepair) throws IOException\n    {\n        logger.trace(\"Mutating {} to repairedAt time {} and pendingRepair {}\",\n                     descriptor.filenameFor(Component.STATS), newRepairedAt, newPendingRepair);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate time & id\n        currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt).mutatePendingRepair(newPendingRepair));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }",
            " 233  \n 234  \n 235 +\n 236 +\n 237 +\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    public void mutateRepaired(Descriptor descriptor, long newRepairedAt, UUID newPendingRepair) throws IOException\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Mutating {} to repairedAt time {} and pendingRepair {}\",\n                         descriptor.filenameFor(Component.STATS), newRepairedAt, newPendingRepair);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate time & id\n        currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt).mutatePendingRepair(newPendingRepair));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }"
        ],
        [
            "NettyStreamingMessageSender::sendMessage(StreamMessage)",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "    @Override\n    public void sendMessage(StreamMessage message)\n    {\n        if (closed)\n            throw new RuntimeException(\"stream has been closed, cannot send \" + message);\n\n        if (message instanceof OutgoingStreamMessage)\n        {\n            if (isPreview)\n                throw new RuntimeException(\"Cannot send stream data messages for preview streaming sessions\");\n            logger.debug(\"{} Sending {}\", createLogTag(session, null), message);\n            fileTransferExecutor.submit(new FileStreamTask((OutgoingStreamMessage)message));\n            return;\n        }\n\n        try\n        {\n            setupControlMessageChannel();\n            sendControlMessage(controlMessageChannel, message, future -> onControlMessageComplete(future, message));\n        }\n        catch (Exception e)\n        {\n            close();\n            session.onError(e);\n        }\n    }",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  ",
            "    @Override\n    public void sendMessage(StreamMessage message)\n    {\n        if (closed)\n            throw new RuntimeException(\"stream has been closed, cannot send \" + message);\n\n        if (message instanceof OutgoingStreamMessage)\n        {\n            if (isPreview)\n                throw new RuntimeException(\"Cannot send stream data messages for preview streaming sessions\");\n            if (logger.isDebugEnabled())\n                logger.debug(\"{} Sending {}\", createLogTag(session, null), message);\n            fileTransferExecutor.submit(new FileStreamTask((OutgoingStreamMessage)message));\n            return;\n        }\n\n        try\n        {\n            setupControlMessageChannel();\n            sendControlMessage(controlMessageChannel, message, future -> onControlMessageComplete(future, message));\n        }\n        catch (Exception e)\n        {\n            close();\n            session.onError(e);\n        }\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::LimitedLocalNodeFirstLocalBalancingPolicy(String)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 -\n  75  ",
            "    public LimitedLocalNodeFirstLocalBalancingPolicy(String[] replicas)\n    {\n        for (String replica : replicas)\n        {\n            try\n            {\n                InetAddress[] addresses = InetAddress.getAllByName(replica);\n                Collections.addAll(replicaAddresses, addresses);\n            }\n            catch (UnknownHostException e)\n            {\n                logger.warn(\"Invalid replica host name: {}, skipping it\", replica);\n            }\n        }\n        logger.trace(\"Created instance with the following replicas: {}\", Arrays.asList(replicas));\n    }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75 +\n  76  ",
            "    public LimitedLocalNodeFirstLocalBalancingPolicy(String[] replicas)\n    {\n        for (String replica : replicas)\n        {\n            try\n            {\n                InetAddress[] addresses = InetAddress.getAllByName(replica);\n                Collections.addAll(replicaAddresses, addresses);\n            }\n            catch (UnknownHostException e)\n            {\n                logger.warn(\"Invalid replica host name: {}, skipping it\", replica);\n            }\n        }\n        if (logger.isTraceEnabled())\n            logger.trace(\"Created instance with the following replicas: {}\", Arrays.asList(replicas));\n    }"
        ],
        [
            "StreamCoordinator::connectNext()",
            " 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  ",
            "    private void connectNext()\n    {\n        if (sessionsToConnect == null)\n            return;\n\n        if (sessionsToConnect.hasNext())\n        {\n            StreamSession next = sessionsToConnect.next();\n            logger.debug(\"Connecting next session {} with {}.\", next.planId(), next.peer.toString());\n            startSession(next);\n        }\n        else\n            logger.debug(\"Finished connecting all sessions\");\n    }",
            " 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  ",
            "    private void connectNext()\n    {\n        if (sessionsToConnect == null)\n            return;\n\n        if (sessionsToConnect.hasNext())\n        {\n            StreamSession next = sessionsToConnect.next();\n            if (logger.isDebugEnabled())\n                logger.debug(\"Connecting next session {} with {}.\", next.planId(), next.peer.toString());\n            startSession(next);\n        }\n        else\n            logger.debug(\"Finished connecting all sessions\");\n    }"
        ],
        [
            "NettyStreamingMessageSender::KeepAliveTask::keepAliveListener(Future)",
            " 460  \n 461  \n 462  \n 463  \n 464  \n 465 -\n 466 -\n 467  ",
            "        private void keepAliveListener(Future<? super Void> future)\n        {\n            if (future.isSuccess() || future.isCancelled())\n                return;\n\n            logger.debug(\"{} Could not send keep-alive message (perhaps stream session is finished?).\",\n                         createLogTag(session, channel), future.cause());\n        }",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471 +\n 472 +\n 473 +\n 474  ",
            "        private void keepAliveListener(Future<? super Void> future)\n        {\n            if (future.isSuccess() || future.isCancelled())\n                return;\n\n            if (logger.isDebugEnabled())\n                logger.debug(\"{} Could not send keep-alive message (perhaps stream session is finished?).\",\n                             createLogTag(session, channel), future.cause());\n        }"
        ]
    ],
    "1bd2c94252b52d99b0c49257d2365f7c85935dcd": [
        [
            "ViewUtils::getViewNaturalEndpoint(String,Token,Token)",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64 -\n  65 -\n  66  \n  67  \n  68 -\n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81 -\n  82 -\n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  ",
            "    /**\n     * Calculate the natural endpoint for the view.\n     *\n     * The view natural endpoint is the endpint which has the same cardinality as this node in the replication factor.\n     * The cardinality is the number at which this node would store a piece of data, given the change in replication\n     * factor.\n     *\n     * For example, if we have the following ring:\n     *   A, T1 -> B, T2 -> C, T3 -> A\n     *\n     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would\n     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality\n     * for T3 is 2.\n     *\n     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:\n     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)\n     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)\n     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)\n     *\n     * @throws RuntimeException if this method is called using a base token which does not belong to this replica\n     */\n    public static InetAddress getViewNaturalEndpoint(String keyspaceName, Token baseToken, Token viewToken)\n    {\n        AbstractReplicationStrategy replicationStrategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        List<InetAddress> localBaseEndpoints = new ArrayList<>();\n        List<InetAddress> localViewEndpoints = new ArrayList<>();\n        for (InetAddress baseEndpoint : replicationStrategy.getNaturalEndpoints(baseToken))\n        {\n            if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(baseEndpoint).equals(localDataCenter))\n                localBaseEndpoints.add(baseEndpoint);\n        }\n\n        for (InetAddress viewEndpoint : replicationStrategy.getNaturalEndpoints(viewToken))\n        {\n            // If we are a base endpoint which is also a view replica, we use ourselves as our view replica\n            if (viewEndpoint.equals(FBUtilities.getBroadcastAddress()))\n                return viewEndpoint;\n\n            // We have to remove any endpoint which is shared between the base and the view, as it will select itself\n            // and throw off the counts otherwise.\n            if (localBaseEndpoints.contains(viewEndpoint))\n                localBaseEndpoints.remove(viewEndpoint);\n            else if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(viewEndpoint).equals(localDataCenter))\n                localViewEndpoints.add(viewEndpoint);\n        }\n\n        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.\n        // Since the same replication strategy is used, the same placement should be used and we should get the same\n        // number of replicas for all of the tokens in the ring.\n        assert localBaseEndpoints.size() == localViewEndpoints.size() : \"Replication strategy should have the same number of endpoints for the base and the view\";\n        int baseIdx = localBaseEndpoints.indexOf(FBUtilities.getBroadcastAddress());\n\n        if (baseIdx < 0)\n        {\n\n            if (StorageService.instance.getTokenMetadata().pendingEndpointsFor(viewToken, keyspaceName).size() > 0)\n            {\n                //Since there are pending endpoints we are going to write to the batchlog regardless.\n                //So we can pretend we are the views endpoint.\n\n                return FBUtilities.getBroadcastAddress();\n            }\n\n            throw new RuntimeException(\"Trying to get the view natural endpoint on a non-data replica\");\n        }\n\n\n        return localViewEndpoints.get(baseIdx);\n    }",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67 +\n  68  \n  69  \n  70 +\n  71 +\n  72 +\n  73 +\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85 +\n  86 +\n  87 +\n  88 +\n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95 +\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 +\n 113  ",
            "    /**\n     * Calculate the natural endpoint for the view.\n     *\n     * The view natural endpoint is the endpoint which has the same cardinality as this node in the replication factor.\n     * The cardinality is the number at which this node would store a piece of data, given the change in replication\n     * factor. If the keyspace's replication strategy is a NetworkTopologyStrategy, we filter the ring to contain only\n     * nodes in the local datacenter when calculating cardinality.\n     *\n     * For example, if we have the following ring:\n     *   A, T1 -> B, T2 -> C, T3 -> A\n     *\n     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would\n     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality\n     * for T3 is 2.\n     *\n     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:\n     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)\n     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)\n     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)\n     *\n     * @throws RuntimeException if this method is called using a base token which does not belong to this replica\n     */\n    public static InetAddress getViewNaturalEndpoint(String keyspaceName, Token baseToken, Token viewToken)\n    {\n        AbstractReplicationStrategy replicationStrategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        List<InetAddress> baseEndpoints = new ArrayList<>();\n        List<InetAddress> viewEndpoints = new ArrayList<>();\n        for (InetAddress baseEndpoint : replicationStrategy.getNaturalEndpoints(baseToken))\n        {\n            // An endpoint is local if we're not using Net\n            if (!(replicationStrategy instanceof NetworkTopologyStrategy) ||\n                DatabaseDescriptor.getEndpointSnitch().getDatacenter(baseEndpoint).equals(localDataCenter))\n                baseEndpoints.add(baseEndpoint);\n        }\n\n        for (InetAddress viewEndpoint : replicationStrategy.getNaturalEndpoints(viewToken))\n        {\n            // If we are a base endpoint which is also a view replica, we use ourselves as our view replica\n            if (viewEndpoint.equals(FBUtilities.getBroadcastAddress()))\n                return viewEndpoint;\n\n            // We have to remove any endpoint which is shared between the base and the view, as it will select itself\n            // and throw off the counts otherwise.\n            if (baseEndpoints.contains(viewEndpoint))\n                baseEndpoints.remove(viewEndpoint);\n            else if (!(replicationStrategy instanceof NetworkTopologyStrategy) ||\n                     DatabaseDescriptor.getEndpointSnitch().getDatacenter(viewEndpoint).equals(localDataCenter))\n                viewEndpoints.add(viewEndpoint);\n        }\n\n        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.\n        // Since the same replication strategy is used, the same placement should be used and we should get the same\n        // number of replicas for all of the tokens in the ring.\n        assert baseEndpoints.size() == viewEndpoints.size() : \"Replication strategy should have the same number of endpoints for the base and the view\";\n        int baseIdx = baseEndpoints.indexOf(FBUtilities.getBroadcastAddress());\n\n        if (baseIdx < 0)\n        {\n\n            if (StorageService.instance.getTokenMetadata().pendingEndpointsFor(viewToken, keyspaceName).size() > 0)\n            {\n                //Since there are pending endpoints we are going to write to the batchlog regardless.\n                //So we can pretend we are the views endpoint.\n\n                return FBUtilities.getBroadcastAddress();\n            }\n\n            throw new RuntimeException(\"Trying to get the view natural endpoint on a non-data replica\");\n        }\n\n\n        return viewEndpoints.get(baseIdx);\n    }"
        ]
    ],
    "c7557bdecda6ffe76bb5ee62be4c2d6c53804681": [
        [
            "MaterializedViewUtils::getViewNaturalEndpoint(String,Token,Token)",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "    /**\n     * Calculate the natural endpoint for the view.\n     *\n     * The view natural endpoint is the endpint which has the same cardinality as this node in the replication factor.\n     * The cardinality is the number at which this node would store a piece of data, given the change in replication\n     * factor.\n     *\n     * For example, if we have the following ring:\n     *   A, T1 -> B, T2 -> C, T3 -> A\n     *\n     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would\n     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality\n     * for T3 is 2.\n     *\n     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:\n     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)\n     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)\n     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)\n     *\n     * @throws RuntimeException if this method is called using a base token which does not belong to this replica\n     */\n    public static InetAddress getViewNaturalEndpoint(String keyspaceName, Token baseToken, Token viewToken)\n    {\n        AbstractReplicationStrategy replicationStrategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        List<InetAddress> localBaseEndpoints = new ArrayList<>();\n        List<InetAddress> localViewEndpoints = new ArrayList<>();\n        for (InetAddress baseEndpoint : replicationStrategy.getNaturalEndpoints(baseToken))\n        {\n            if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(baseEndpoint).equals(localDataCenter))\n                localBaseEndpoints.add(baseEndpoint);\n        }\n\n        for (InetAddress viewEndpoint : replicationStrategy.getNaturalEndpoints(viewToken))\n        {\n            // If we are a base endpoint which is also a view replica, we use ourselves as our view replica\n            if (viewEndpoint.equals(FBUtilities.getBroadcastAddress()))\n                return viewEndpoint;\n\n            // We have to remove any endpoint which is shared between the base and the view, as it will select itself\n            // and throw off the counts otherwise.\n            if (localBaseEndpoints.contains(viewEndpoint))\n                localBaseEndpoints.remove(viewEndpoint);\n            else if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(viewEndpoint).equals(localDataCenter))\n                localViewEndpoints.add(viewEndpoint);\n        }\n\n        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.\n        // Since the same replication strategy is used, the same placement should be used and we should get the same\n        // number of replicas for all of the tokens in the ring.\n        assert localBaseEndpoints.size() == localViewEndpoints.size() : \"Replication strategy should have the same number of endpoints for the base and the view\";\n        int baseIdx = localBaseEndpoints.indexOf(FBUtilities.getBroadcastAddress());\n        if (baseIdx < 0)\n            throw new RuntimeException(\"Trying to get the view natural endpoint on a non-data replica\");\n\n        return localViewEndpoints.get(baseIdx);\n    }",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99 +\n 100 +\n 101 +\n 102 +\n 103  \n 104 +\n 105 +\n 106  \n 107  \n 108  ",
            "    /**\n     * Calculate the natural endpoint for the view.\n     *\n     * The view natural endpoint is the endpint which has the same cardinality as this node in the replication factor.\n     * The cardinality is the number at which this node would store a piece of data, given the change in replication\n     * factor.\n     *\n     * For example, if we have the following ring:\n     *   A, T1 -> B, T2 -> C, T3 -> A\n     *\n     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would\n     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality\n     * for T3 is 2.\n     *\n     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:\n     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)\n     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)\n     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)\n     *\n     * @throws RuntimeException if this method is called using a base token which does not belong to this replica\n     */\n    public static InetAddress getViewNaturalEndpoint(String keyspaceName, Token baseToken, Token viewToken)\n    {\n        AbstractReplicationStrategy replicationStrategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        List<InetAddress> localBaseEndpoints = new ArrayList<>();\n        List<InetAddress> localViewEndpoints = new ArrayList<>();\n        for (InetAddress baseEndpoint : replicationStrategy.getNaturalEndpoints(baseToken))\n        {\n            if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(baseEndpoint).equals(localDataCenter))\n                localBaseEndpoints.add(baseEndpoint);\n        }\n\n        for (InetAddress viewEndpoint : replicationStrategy.getNaturalEndpoints(viewToken))\n        {\n            // If we are a base endpoint which is also a view replica, we use ourselves as our view replica\n            if (viewEndpoint.equals(FBUtilities.getBroadcastAddress()))\n                return viewEndpoint;\n\n            // We have to remove any endpoint which is shared between the base and the view, as it will select itself\n            // and throw off the counts otherwise.\n            if (localBaseEndpoints.contains(viewEndpoint))\n                localBaseEndpoints.remove(viewEndpoint);\n            else if (DatabaseDescriptor.getEndpointSnitch().getDatacenter(viewEndpoint).equals(localDataCenter))\n                localViewEndpoints.add(viewEndpoint);\n        }\n\n        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.\n        // Since the same replication strategy is used, the same placement should be used and we should get the same\n        // number of replicas for all of the tokens in the ring.\n        assert localBaseEndpoints.size() == localViewEndpoints.size() : \"Replication strategy should have the same number of endpoints for the base and the view\";\n        int baseIdx = localBaseEndpoints.indexOf(FBUtilities.getBroadcastAddress());\n\n        if (baseIdx < 0)\n        {\n\n            if (StorageService.instance.getTokenMetadata().pendingEndpointsFor(viewToken, keyspaceName).size() > 0)\n            {\n                //Since there are pending endpoints we are going to store hints this in the batchlog regardless.\n                //So we can pretend we are the views endpoint.\n\n                return FBUtilities.getBroadcastAddress();\n            }\n\n            throw new RuntimeException(\"Trying to get the view natural endpoint on a non-data replica\");\n        }\n\n\n        return localViewEndpoints.get(baseIdx);\n    }"
        ]
    ],
    "4c6411f083b9448114a0ba349fc02e60299f6541": [
        [
            "OutboundTcpConnection::expireMessages()",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  ",
            "    private void expireMessages()\n    {\n        Iterator<QueuedMessage> iter = backlog.iterator();\n        while (iter.hasNext())\n        {\n            QueuedMessage qm = iter.next();\n            if (qm.timestampNanos >= System.nanoTime() - qm.message.getTimeout())\n                return;\n            iter.remove();\n            dropped.incrementAndGet();\n        }\n    }",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521 +\n 522 +\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "    private void expireMessages()\n    {\n        Iterator<QueuedMessage> iter = backlog.iterator();\n        while (iter.hasNext())\n        {\n            QueuedMessage qm = iter.next();\n            if (!qm.droppable)\n                continue;\n            if (qm.timestampNanos >= System.nanoTime() - qm.message.getTimeout())\n                return;\n            iter.remove();\n            dropped.incrementAndGet();\n        }\n    }"
        ]
    ]
}