{
    "d7e2745eac5e4d56e8af928e98626194406ca9f2": [
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::getReduceSinkOp(List,List,List,List,ArrayList,ArrayList,int,Operator,AcidUtils)",
            " 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  ",
            "    public ReduceSinkOperator getReduceSinkOp(List<Integer> partitionPositions,\n        List<Integer> sortPositions, List<Integer> sortOrder, List<Integer> sortNullOrder,\n        ArrayList<ExprNodeDesc> allCols, ArrayList<ExprNodeDesc> bucketColumns, int numBuckets,\n        Operator<? extends OperatorDesc> parent, AcidUtils.Operation writeType) throws SemanticException {\n\n      // Order of KEY columns\n      // 1) Partition columns\n      // 2) Bucket number column\n      // 3) Sort columns\n      Set<Integer> keyColsPosInVal = Sets.newLinkedHashSet();\n      ArrayList<ExprNodeDesc> keyCols = Lists.newArrayList();\n      List<Integer> newSortOrder = Lists.newArrayList();\n      List<Integer> newSortNullOrder = Lists.newArrayList();\n      int numPartAndBuck = partitionPositions.size();\n\n      keyColsPosInVal.addAll(partitionPositions);\n      if (!bucketColumns.isEmpty()) {\n        keyColsPosInVal.add(-1);\n        numPartAndBuck += 1;\n      }\n      keyColsPosInVal.addAll(sortPositions);\n\n      // by default partition and bucket columns are sorted in ascending order\n      Integer order = 1;\n      if (sortOrder != null && !sortOrder.isEmpty()) {\n        if (sortOrder.get(0).intValue() == 0) {\n          order = 0;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortOrder.add(order);\n      }\n      newSortOrder.addAll(sortOrder);\n\n      String orderStr = \"\";\n      for (Integer i : newSortOrder) {\n        if(i.intValue() == 1) {\n          orderStr += \"+\";\n        } else {\n          orderStr += \"-\";\n        }\n      }\n\n      // if partition and bucket columns are sorted in ascending order, by default\n      // nulls come first; otherwise nulls come last\n      Integer nullOrder = order == 1 ? 0 : 1;\n      if (sortNullOrder != null && !sortNullOrder.isEmpty()) {\n        if (sortNullOrder.get(0).intValue() == 0) {\n          nullOrder = 0;\n        } else {\n          nullOrder = 1;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortNullOrder.add(nullOrder);\n      }\n      newSortNullOrder.addAll(sortNullOrder);\n\n      String nullOrderStr = \"\";\n      for (Integer i : newSortNullOrder) {\n        if(i.intValue() == 0) {\n          nullOrderStr += \"a\";\n        } else {\n          nullOrderStr += \"z\";\n        }\n      }\n\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      ArrayList<ExprNodeDesc> partCols = Lists.newArrayList();\n\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      for (Integer idx : keyColsPosInVal) {\n        if (idx < 0) {\n          ExprNodeConstantDesc bucketNumCol = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, BUCKET_NUMBER_COL_NAME);\n          keyCols.add(bucketNumCol);\n          colExprMap.put(Utilities.ReduceField.KEY + \".'\" +BUCKET_NUMBER_COL_NAME+\"'\", bucketNumCol);\n        } else {\n          keyCols.add(allCols.get(idx).clone());\n        }\n      }\n\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (!keyColsPosInVal.contains(i)) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      for (Integer idx : partitionPositions) {\n        partCols.add(allCols.get(idx).clone());\n      }\n\n      // in the absence of SORTED BY clause, the sorted dynamic partition insert\n      // should honor the ordering of records provided by ORDER BY in SELECT statement\n      ReduceSinkOperator parentRSOp = OperatorUtils.findSingleOperatorUpstream(parent,\n          ReduceSinkOperator.class);\n      if (parentRSOp != null && parseCtx.getQueryProperties().hasOuterOrderBy()) {\n        String parentRSOpOrder = parentRSOp.getConf().getOrder();\n        String parentRSOpNullOrder = parentRSOp.getConf().getNullOrder();\n        if (parentRSOpOrder != null && !parentRSOpOrder.isEmpty() && sortPositions.isEmpty()) {\n          keyCols.addAll(parentRSOp.getConf().getKeyCols());\n          orderStr += parentRSOpOrder;\n          nullOrderStr += parentRSOpNullOrder;\n        }\n      }\n\n      // map _col0 to KEY._col0, etc\n      Map<String, String> nameMapping = new HashMap<>();\n      ArrayList<String> keyColNames = Lists.newArrayList();\n      for (ExprNodeDesc keyCol : keyCols) {\n        String keyColName = keyCol.getExprString();\n        keyColNames.add(keyColName);\n        colExprMap.put(Utilities.ReduceField.KEY + \".\" +keyColName, keyCol);\n        nameMapping.put(keyColName, Utilities.ReduceField.KEY + \".\" + keyColName);\n      }\n      ArrayList<String> valColNames = Lists.newArrayList();\n      for (ExprNodeDesc valCol : valCols) {\n        String colName = valCol.getExprString();\n        valColNames.add(colName);\n        colExprMap.put(Utilities.ReduceField.VALUE + \".\" + colName, valCol);\n        nameMapping.put(colName, Utilities.ReduceField.VALUE + \".\" + colName);\n      }\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable, writeType);\n      rsConf.setBucketCols(bucketColumns);\n      rsConf.setNumBuckets(numBuckets);\n\n      ArrayList<ColumnInfo> signature = new ArrayList<>();\n      for (int index = 0; index < parent.getSchema().getSignature().size(); index++) {\n        ColumnInfo colInfo = new ColumnInfo(parent.getSchema().getSignature().get(index));\n        colInfo.setInternalName(nameMapping.get(colInfo.getInternalName()));\n        signature.add(colInfo);\n      }\n      ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }",
            " 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  ",
            "    public ReduceSinkOperator getReduceSinkOp(List<Integer> partitionPositions,\n        List<Integer> sortPositions, List<Integer> sortOrder, List<Integer> sortNullOrder,\n        ArrayList<ExprNodeDesc> allCols, ArrayList<ExprNodeDesc> bucketColumns, int numBuckets,\n        Operator<? extends OperatorDesc> parent, AcidUtils.Operation writeType) throws SemanticException {\n\n      // Order of KEY columns\n      // 1) Partition columns\n      // 2) Bucket number column\n      // 3) Sort columns\n      Set<Integer> keyColsPosInVal = Sets.newLinkedHashSet();\n      ArrayList<ExprNodeDesc> keyCols = Lists.newArrayList();\n      List<Integer> newSortOrder = Lists.newArrayList();\n      List<Integer> newSortNullOrder = Lists.newArrayList();\n      int numPartAndBuck = partitionPositions.size();\n\n      keyColsPosInVal.addAll(partitionPositions);\n      if (!bucketColumns.isEmpty() || writeType == Operation.DELETE || writeType == Operation.UPDATE) {\n        keyColsPosInVal.add(-1);\n        numPartAndBuck += 1;\n      }\n      keyColsPosInVal.addAll(sortPositions);\n\n      // by default partition and bucket columns are sorted in ascending order\n      Integer order = 1;\n      if (sortOrder != null && !sortOrder.isEmpty()) {\n        if (sortOrder.get(0).intValue() == 0) {\n          order = 0;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortOrder.add(order);\n      }\n      newSortOrder.addAll(sortOrder);\n\n      String orderStr = \"\";\n      for (Integer i : newSortOrder) {\n        if(i.intValue() == 1) {\n          orderStr += \"+\";\n        } else {\n          orderStr += \"-\";\n        }\n      }\n\n      // if partition and bucket columns are sorted in ascending order, by default\n      // nulls come first; otherwise nulls come last\n      Integer nullOrder = order == 1 ? 0 : 1;\n      if (sortNullOrder != null && !sortNullOrder.isEmpty()) {\n        if (sortNullOrder.get(0).intValue() == 0) {\n          nullOrder = 0;\n        } else {\n          nullOrder = 1;\n        }\n      }\n      for (int i = 0; i < numPartAndBuck; i++) {\n        newSortNullOrder.add(nullOrder);\n      }\n      newSortNullOrder.addAll(sortNullOrder);\n\n      String nullOrderStr = \"\";\n      for (Integer i : newSortNullOrder) {\n        if(i.intValue() == 0) {\n          nullOrderStr += \"a\";\n        } else {\n          nullOrderStr += \"z\";\n        }\n      }\n\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      ArrayList<ExprNodeDesc> partCols = Lists.newArrayList();\n\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      for (Integer idx : keyColsPosInVal) {\n        if (idx < 0) {\n          ExprNodeConstantDesc bucketNumCol = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, BUCKET_NUMBER_COL_NAME);\n          keyCols.add(bucketNumCol);\n          colExprMap.put(Utilities.ReduceField.KEY + \".'\" +BUCKET_NUMBER_COL_NAME+\"'\", bucketNumCol);\n        } else {\n          keyCols.add(allCols.get(idx).clone());\n        }\n      }\n\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (!keyColsPosInVal.contains(i)) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      for (Integer idx : partitionPositions) {\n        partCols.add(allCols.get(idx).clone());\n      }\n\n      // in the absence of SORTED BY clause, the sorted dynamic partition insert\n      // should honor the ordering of records provided by ORDER BY in SELECT statement\n      ReduceSinkOperator parentRSOp = OperatorUtils.findSingleOperatorUpstream(parent,\n          ReduceSinkOperator.class);\n      if (parentRSOp != null && parseCtx.getQueryProperties().hasOuterOrderBy()) {\n        String parentRSOpOrder = parentRSOp.getConf().getOrder();\n        String parentRSOpNullOrder = parentRSOp.getConf().getNullOrder();\n        if (parentRSOpOrder != null && !parentRSOpOrder.isEmpty() && sortPositions.isEmpty()) {\n          keyCols.addAll(parentRSOp.getConf().getKeyCols());\n          orderStr += parentRSOpOrder;\n          nullOrderStr += parentRSOpNullOrder;\n        }\n      }\n\n      // map _col0 to KEY._col0, etc\n      Map<String, String> nameMapping = new HashMap<>();\n      ArrayList<String> keyColNames = Lists.newArrayList();\n      for (ExprNodeDesc keyCol : keyCols) {\n        String keyColName = keyCol.getExprString();\n        keyColNames.add(keyColName);\n        colExprMap.put(Utilities.ReduceField.KEY + \".\" +keyColName, keyCol);\n        nameMapping.put(keyColName, Utilities.ReduceField.KEY + \".\" + keyColName);\n      }\n      ArrayList<String> valColNames = Lists.newArrayList();\n      for (ExprNodeDesc valCol : valCols) {\n        String colName = valCol.getExprString();\n        valColNames.add(colName);\n        colExprMap.put(Utilities.ReduceField.VALUE + \".\" + colName, valCol);\n        nameMapping.put(colName, Utilities.ReduceField.VALUE + \".\" + colName);\n      }\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable, writeType);\n      rsConf.setBucketCols(bucketColumns);\n      rsConf.setNumBuckets(numBuckets);\n\n      ArrayList<ColumnInfo> signature = new ArrayList<>();\n      for (int index = 0; index < parent.getSchema().getSignature().size(); index++) {\n        ColumnInfo colInfo = new ColumnInfo(parent.getSchema().getSignature().get(index));\n        colInfo.setInternalName(nameMapping.get(colInfo.getInternalName()));\n        signature.add(colInfo);\n      }\n      ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }"
        ]
    ],
    "ed82cfa914769cfabfc7460b7b5abbdae71e562a": [
        [
            "FileSinkOperator::process(Object,int)",
            " 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 -\n 770 -\n 771  \n 772  \n 773  \n 774 -\n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781 -\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n    runTimeNumRows++;\n    /* Create list bucketing sub-directory only if stored-as-directories is on. */\n    String lbDirName = null;\n    lbDirName = (lbCtx == null) ? null : generateListBucketingDirName(row);\n\n    if (!bDynParts && !filesCreated) {\n      if (lbDirName != null) {\n        FSPaths fsp2 = lookupListBucketingPaths(lbDirName);\n      } else {\n        createBucketFiles(fsp);\n      }\n    }\n\n    try {\n      updateProgress();\n\n      // if DP is enabled, get the final output writers and prepare the real output row\n      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT\n          : \"input object inspector is not struct\";\n\n      if (bDynParts) {\n\n        // we need to read bucket number which is the last column in value (after partition columns)\n        if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {\n          numDynParts += 1;\n        }\n\n        // copy the DP column values from the input row to dpVals\n        dpVals.clear();\n        dpWritables.clear();\n        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol,numDynParts,\n            (StructObjectInspector) inputObjInspectors[0],ObjectInspectorCopyOption.WRITABLE);\n\n        // get a set of RecordWriter based on the DP column values\n        // pass the null value along to the escaping process to determine what the dir should be\n        for (Object o : dpWritables) {\n          if (o == null || o.toString().length() == 0) {\n            dpVals.add(dpCtx.getDefaultPartitionName());\n          } else {\n            dpVals.add(o.toString());\n          }\n        }\n\n        String invalidPartitionVal;\n        if((invalidPartitionVal = HiveStringUtils.getPartitionValWithInvalidCharacter(dpVals, dpCtx.getWhiteListPattern()))!=null) {\n          throw new HiveFatalException(\"Partition value '\" + invalidPartitionVal +\n              \"' contains a character not matched by whitelist pattern '\" +\n              dpCtx.getWhiteListPattern().toString() + \"'.  \" + \"(configure with \" +\n              HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname + \")\");\n        }\n        fpaths = getDynOutPaths(dpVals, lbDirName);\n\n        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row\n        recordValue = serializer.serialize(row, subSetOI);\n      } else {\n        if (lbDirName != null) {\n          fpaths = lookupListBucketingPaths(lbDirName);\n        } else {\n          fpaths = fsp;\n        }\n        recordValue = serializer.serialize(row, inputObjInspectors[0]);\n        // if serializer is ThriftJDBCBinarySerDe, then recordValue is null if the buffer is not full (the size of buffer\n        // is kept track of in the SerDe)\n        if (recordValue == null) {\n          return;\n        }\n      }\n\n      rowOutWriters = fpaths.outWriters;\n      // check if all record writers implement statistics. if atleast one RW\n      // doesn't implement stats interface we will fallback to conventional way\n      // of gathering stats\n      isCollectRWStats = areAllTrue(statsFromRecordWriter);\n      if (conf.isGatherStats() && !isCollectRWStats) {\n        SerDeStats stats = serializer.getSerDeStats();\n        if (stats != null) {\n          fpaths.stat.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());\n        }\n        fpaths.stat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n      }\n\n      if ((++numRows == cntr) && isLogInfoEnabled) {\n        cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;\n        if (cntr < 0 || numRows < 0) {\n          cntr = 0;\n          numRows = 1;\n        }\n        LOG.info(toString() + \": records written - \" + numRows);\n      }\n\n      // This should always be 0 for the final result file\n      int writerOffset = findWriterOffset(row);\n      // This if/else chain looks ugly in the inner loop, but given that it will be 100% the same\n      // for a given operator branch prediction should work quite nicely on it.\n      // RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we\n      // pass the row rather than recordValue.\n      if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID) {\n        rowOutWriters[writerOffset].write(recordValue);\n      } else if (conf.getWriteType() == AcidUtils.Operation.INSERT) {\n        fpaths.updaters[writerOffset].insert(conf.getTransactionId(), row);\n      } else {\n        // TODO I suspect we could skip much of the stuff above this in the function in the case\n        // of update and delete.  But I don't understand all of the side effects of the above\n        // code and don't want to skip over it yet.\n\n        // Find the bucket id, and switch buckets if need to\n        ObjectInspector rowInspector = bDynParts ? subSetOI : outputObjInspector;\n        Object recId = ((StructObjectInspector)rowInspector).getStructFieldData(row, recIdField);\n        int bucketNum =\n            bucketInspector.get(recIdInspector.getStructFieldData(recId, bucketField));\n        if (fpaths.acidLastBucket != bucketNum) {\n          fpaths.acidLastBucket = bucketNum;\n          // Switch files\n          fpaths.updaters[++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(\n              jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[fpaths.acidFileOffset],\n              rowInspector, reporter, 0);\n          if (isDebugEnabled) {\n            LOG.debug(\"Created updater for bucket number \" + bucketNum + \" using file \" +\n                fpaths.outPaths[fpaths.acidFileOffset]);\n          }\n        }\n\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE) {\n          fpaths.updaters[fpaths.acidFileOffset].update(conf.getTransactionId(), row);\n        } else if (conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          fpaths.updaters[fpaths.acidFileOffset].delete(conf.getTransactionId(), row);\n        } else {\n          throw new HiveException(\"Unknown write type \" + conf.getWriteType().toString());\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(e);\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n  }",
            " 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 +\n 770 +\n 771  \n 772  \n 773  \n 774 +\n 775  \n 776  \n 777  \n 778  \n 779 +\n 780  \n 781 +\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n    runTimeNumRows++;\n    /* Create list bucketing sub-directory only if stored-as-directories is on. */\n    String lbDirName = null;\n    lbDirName = (lbCtx == null) ? null : generateListBucketingDirName(row);\n\n    if (!bDynParts && !filesCreated) {\n      if (lbDirName != null) {\n        FSPaths fsp2 = lookupListBucketingPaths(lbDirName);\n      } else {\n        createBucketFiles(fsp);\n      }\n    }\n\n    try {\n      updateProgress();\n\n      // if DP is enabled, get the final output writers and prepare the real output row\n      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT\n          : \"input object inspector is not struct\";\n\n      if (bDynParts) {\n\n        // we need to read bucket number which is the last column in value (after partition columns)\n        if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {\n          numDynParts += 1;\n        }\n\n        // copy the DP column values from the input row to dpVals\n        dpVals.clear();\n        dpWritables.clear();\n        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol,numDynParts,\n            (StructObjectInspector) inputObjInspectors[0],ObjectInspectorCopyOption.WRITABLE);\n\n        // get a set of RecordWriter based on the DP column values\n        // pass the null value along to the escaping process to determine what the dir should be\n        for (Object o : dpWritables) {\n          if (o == null || o.toString().length() == 0) {\n            dpVals.add(dpCtx.getDefaultPartitionName());\n          } else {\n            dpVals.add(o.toString());\n          }\n        }\n\n        String invalidPartitionVal;\n        if((invalidPartitionVal = HiveStringUtils.getPartitionValWithInvalidCharacter(dpVals, dpCtx.getWhiteListPattern()))!=null) {\n          throw new HiveFatalException(\"Partition value '\" + invalidPartitionVal +\n              \"' contains a character not matched by whitelist pattern '\" +\n              dpCtx.getWhiteListPattern().toString() + \"'.  \" + \"(configure with \" +\n              HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN.varname + \")\");\n        }\n        fpaths = getDynOutPaths(dpVals, lbDirName);\n\n        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row\n        recordValue = serializer.serialize(row, subSetOI);\n      } else {\n        if (lbDirName != null) {\n          fpaths = lookupListBucketingPaths(lbDirName);\n        } else {\n          fpaths = fsp;\n        }\n        recordValue = serializer.serialize(row, inputObjInspectors[0]);\n        // if serializer is ThriftJDBCBinarySerDe, then recordValue is null if the buffer is not full (the size of buffer\n        // is kept track of in the SerDe)\n        if (recordValue == null) {\n          return;\n        }\n      }\n\n      rowOutWriters = fpaths.outWriters;\n      // check if all record writers implement statistics. if atleast one RW\n      // doesn't implement stats interface we will fallback to conventional way\n      // of gathering stats\n      isCollectRWStats = areAllTrue(statsFromRecordWriter);\n      if (conf.isGatherStats() && !isCollectRWStats) {\n        SerDeStats stats = serializer.getSerDeStats();\n        if (stats != null) {\n          fpaths.stat.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());\n        }\n        fpaths.stat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n      }\n\n      if ((++numRows == cntr) && isLogInfoEnabled) {\n        cntr = logEveryNRows == 0 ? cntr * 10 : numRows + logEveryNRows;\n        if (cntr < 0 || numRows < 0) {\n          cntr = 0;\n          numRows = 1;\n        }\n        LOG.info(toString() + \": records written - \" + numRows);\n      }\n\n      // This should always be 0 for the final result file\n      int writerOffset = findWriterOffset(row);\n      // This if/else chain looks ugly in the inner loop, but given that it will be 100% the same\n      // for a given operator branch prediction should work quite nicely on it.\n      // RecordUpdateer expects to get the actual row, not a serialized version of it.  Thus we\n      // pass the row rather than recordValue.\n      if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID) {\n        rowOutWriters[writerOffset].write(recordValue);\n      } else if (conf.getWriteType() == AcidUtils.Operation.INSERT) {\n        fpaths.updaters[writerOffset].insert(conf.getTransactionId(), row);\n      } else {\n        // TODO I suspect we could skip much of the stuff above this in the function in the case\n        // of update and delete.  But I don't understand all of the side effects of the above\n        // code and don't want to skip over it yet.\n\n        // Find the bucket id, and switch buckets if need to\n        ObjectInspector rowInspector = bDynParts ? subSetOI : outputObjInspector;\n        Object recId = ((StructObjectInspector)rowInspector).getStructFieldData(row, recIdField);\n        int bucketNum =\n            bucketInspector.get(recIdInspector.getStructFieldData(recId, bucketField));\n        if (fpaths.acidLastBucket != bucketNum) {\n          fpaths.acidLastBucket = bucketNum;\n          // Switch files\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 : ++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(\n              jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset],\n              rowInspector, reporter, 0);\n          if (isDebugEnabled) {\n            LOG.debug(\"Created updater for bucket number \" + bucketNum + \" using file \" +\n                fpaths.outPaths[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset]);\n          }\n        }\n\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE) {\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset].update(conf.getTransactionId(), row);\n        } else if (conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          fpaths.updaters[conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED) ? 0 :fpaths.acidFileOffset].delete(conf.getTransactionId(), row);\n        } else {\n          throw new HiveException(\"Unknown write type \" + conf.getWriteType().toString());\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(e);\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "ReduceSinkOperator::initializeOp(Configuration)",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    try {\n\n      numRows = 0;\n      cntr = 1;\n      logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);\n\n      statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);\n\n      List<ExprNodeDesc> keys = conf.getKeyCols();\n\n      if (isLogDebugEnabled) {\n        LOG.debug(\"keys size is \" + keys.size());\n        for (ExprNodeDesc k : keys) {\n          LOG.debug(\"Key exprNodeDesc \" + k.getExprString());\n        }\n      }\n\n      keyEval = new ExprNodeEvaluator[keys.size()];\n      int i = 0;\n      for (ExprNodeDesc e : keys) {\n        keyEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      numDistributionKeys = conf.getNumDistributionKeys();\n      distinctColIndices = conf.getDistinctColumnIndices();\n      numDistinctExprs = distinctColIndices.size();\n\n      valueEval = new ExprNodeEvaluator[conf.getValueCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getValueCols()) {\n        valueEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      partitionEval = new ExprNodeEvaluator[conf.getPartitionCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getPartitionCols()) {\n        int index = ExprNodeDescUtils.indexOf(e, keys);\n        partitionEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e): keyEval[index];\n      }\n\n      if (conf.getBucketCols() != null && !conf.getBucketCols().isEmpty()) {\n        bucketEval = new ExprNodeEvaluator[conf.getBucketCols().size()];\n\n        i = 0;\n        for (ExprNodeDesc e : conf.getBucketCols()) {\n          int index = ExprNodeDescUtils.indexOf(e, keys);\n          bucketEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e) : keyEval[index];\n        }\n\n        buckColIdxInKey = conf.getPartitionCols().size();\n      }\n\n      tag = conf.getTag();\n      tagByte[0] = (byte) tag;\n      skipTag = conf.getSkipTag();\n      if (isLogInfoEnabled) {\n        LOG.info(\"Using tag = \" + tag);\n      }\n\n      TableDesc keyTableDesc = conf.getKeySerializeInfo();\n      keySerializer = (Serializer) keyTableDesc.getDeserializerClass()\n          .newInstance();\n      keySerializer.initialize(null, keyTableDesc.getProperties());\n      keyIsText = keySerializer.getSerializedClass().equals(Text.class);\n\n      TableDesc valueTableDesc = conf.getValueSerializeInfo();\n      valueSerializer = (Serializer) valueTableDesc.getDeserializerClass()\n          .newInstance();\n      valueSerializer.initialize(null, valueTableDesc.getProperties());\n\n      int limit = conf.getTopN();\n      float memUsage = conf.getTopNMemoryUsage();\n\n      if (limit >= 0 && memUsage > 0) {\n        reducerHash = conf.isPTFReduceSink() ? new PTFTopNHash() : new TopNHash();\n        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);\n      }\n\n      useUniformHash = conf.getReducerTraits().contains(UNIFORM);\n\n      firstRow = true;\n    } catch (Exception e) {\n      String msg = \"Error initializing ReduceSinkOperator: \" + e.getMessage();\n      LOG.error(msg, e);\n      throw new RuntimeException(e);\n    }\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190 +\n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    try {\n\n      numRows = 0;\n      cntr = 1;\n      logEveryNRows = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVE_LOG_N_RECORDS);\n\n      statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);\n\n      List<ExprNodeDesc> keys = conf.getKeyCols();\n\n      if (isLogDebugEnabled) {\n        LOG.debug(\"keys size is \" + keys.size());\n        for (ExprNodeDesc k : keys) {\n          LOG.debug(\"Key exprNodeDesc \" + k.getExprString());\n        }\n      }\n\n      keyEval = new ExprNodeEvaluator[keys.size()];\n      int i = 0;\n      for (ExprNodeDesc e : keys) {\n        if (e instanceof ExprNodeConstantDesc && (\"_bucket_number\").equals(((ExprNodeConstantDesc)e).getValue())) {\n          buckColIdxInKeyForAcid = i;\n        }\n        keyEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      numDistributionKeys = conf.getNumDistributionKeys();\n      distinctColIndices = conf.getDistinctColumnIndices();\n      numDistinctExprs = distinctColIndices.size();\n\n      valueEval = new ExprNodeEvaluator[conf.getValueCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getValueCols()) {\n        valueEval[i++] = ExprNodeEvaluatorFactory.get(e);\n      }\n\n      partitionEval = new ExprNodeEvaluator[conf.getPartitionCols().size()];\n      i = 0;\n      for (ExprNodeDesc e : conf.getPartitionCols()) {\n        int index = ExprNodeDescUtils.indexOf(e, keys);\n        partitionEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e): keyEval[index];\n      }\n\n      if (conf.getBucketCols() != null && !conf.getBucketCols().isEmpty()) {\n        bucketEval = new ExprNodeEvaluator[conf.getBucketCols().size()];\n\n        i = 0;\n        for (ExprNodeDesc e : conf.getBucketCols()) {\n          int index = ExprNodeDescUtils.indexOf(e, keys);\n          bucketEval[i++] = index < 0 ? ExprNodeEvaluatorFactory.get(e) : keyEval[index];\n        }\n\n        buckColIdxInKey = conf.getPartitionCols().size();\n      }\n\n      tag = conf.getTag();\n      tagByte[0] = (byte) tag;\n      skipTag = conf.getSkipTag();\n      if (isLogInfoEnabled) {\n        LOG.info(\"Using tag = \" + tag);\n      }\n\n      TableDesc keyTableDesc = conf.getKeySerializeInfo();\n      keySerializer = (Serializer) keyTableDesc.getDeserializerClass()\n          .newInstance();\n      keySerializer.initialize(null, keyTableDesc.getProperties());\n      keyIsText = keySerializer.getSerializedClass().equals(Text.class);\n\n      TableDesc valueTableDesc = conf.getValueSerializeInfo();\n      valueSerializer = (Serializer) valueTableDesc.getDeserializerClass()\n          .newInstance();\n      valueSerializer.initialize(null, valueTableDesc.getProperties());\n\n      int limit = conf.getTopN();\n      float memUsage = conf.getTopNMemoryUsage();\n\n      if (limit >= 0 && memUsage > 0) {\n        reducerHash = conf.isPTFReduceSink() ? new PTFTopNHash() : new TopNHash();\n        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);\n      }\n\n      useUniformHash = conf.getReducerTraits().contains(UNIFORM);\n\n      firstRow = true;\n    } catch (Exception e) {\n      String msg = \"Error initializing ReduceSinkOperator: \" + e.getMessage();\n      LOG.error(msg, e);\n      throw new RuntimeException(e);\n    }\n  }"
        ],
        [
            "ReduceSinkOperator::process(Object,int)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Exptected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369 +\n 370 +\n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Exptected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        if (buckColIdxInKeyForAcid != -1) {\n          cachedKeys[0][buckColIdxInKeyForAcid] = new Text(String.valueOf(bucketNumber));\n        }\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271 -\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271 +\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty() || fsOp.getConf().getWriteType() == Operation.DELETE || fsOp.getConf().getWriteType() == Operation.UPDATE) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0 || fsOp.getConf().getWriteType() == Operation.DELETE || fsOp.getConf().getWriteType() == Operation.UPDATE) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }"
        ]
    ],
    "04b303b6957a6b6f580ee10a4215b21071d87999": [
        [
            "Hive::loadPartition(Path,Table,Map,boolean,boolean,boolean,boolean,boolean,boolean)",
            "1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }",
            "1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571 +\n1572 +\n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584 +\n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  ",
            "  /**\n   * Load a directory into a Hive Table Partition - Alters existing content of\n   * the partition with the contents of loadPath. - If the partition does not\n   * exist - one is created - files in loadPath are moved into Hive. But the\n   * directory itself is not removed.\n   *\n   * @param loadPath\n   *          Directory containing files to load into Table\n   * @param  tbl\n   *          name of table to be loaded.\n   * @param partSpec\n   *          defines which partition needs to be loaded\n   * @param replace\n   *          if true - replace files in the partition, otherwise add files to\n   *          the partition\n   * @param inheritTableSpecs if true, on [re]creating the partition, take the\n   *          location/inputformat/outputformat/serde details from table spec\n   * @param isSrcLocal\n   *          If the source directory is LOCAL\n   * @param isAcid true if this is an ACID operation\n   */\n  public Partition loadPartition(Path loadPath, Table tbl,\n      Map<String, String> partSpec, boolean replace,\n      boolean inheritTableSpecs, boolean isSkewedStoreAsSubdir,\n      boolean isSrcLocal, boolean isAcid, boolean hasFollowingStatsTask) throws HiveException {\n\n    Path tblDataLocationPath =  tbl.getDataLocation();\n    try {\n      Partition oldPart = getPartition(tbl, partSpec, false);\n      /**\n       * Move files before creating the partition since down stream processes\n       * check for existence of partition in metadata before accessing the data.\n       * If partition is created before data is moved, downstream waiting\n       * processes might move forward with partial data\n       */\n\n      Path oldPartPath = (oldPart != null) ? oldPart.getDataLocation() : null;\n      Path newPartPath = null;\n\n      if (inheritTableSpecs) {\n        Path partPath = new Path(tbl.getDataLocation(),\n            Warehouse.makePartPath(partSpec));\n        newPartPath = new Path(tblDataLocationPath.toUri().getScheme(), tblDataLocationPath.toUri().getAuthority(),\n            partPath.toUri().getPath());\n\n        if(oldPart != null) {\n          /*\n           * If we are moving the partition across filesystem boundaries\n           * inherit from the table properties. Otherwise (same filesystem) use the\n           * original partition location.\n           *\n           * See: HIVE-1707 and HIVE-2117 for background\n           */\n          FileSystem oldPartPathFS = oldPartPath.getFileSystem(getConf());\n          FileSystem loadPathFS = loadPath.getFileSystem(getConf());\n          if (FileUtils.equalsFileSystem(oldPartPathFS,loadPathFS)) {\n            newPartPath = oldPartPath;\n          }\n        }\n      } else {\n        newPartPath = oldPartPath;\n      }\n      List<Path> newFiles = null;\n      PerfLogger perfLogger = SessionState.getPerfLogger();\n      perfLogger.PerfLogBegin(\"MoveTask\", \"FileMoves\");\n      if (replace || (oldPart == null && !isAcid)) {\n        replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),\n            isSrcLocal);\n      } else {\n        if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPart != null) {\n          newFiles = Collections.synchronizedList(new ArrayList<Path>());\n        }\n\n        FileSystem fs = tbl.getDataLocation().getFileSystem(conf);\n        Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);\n      }\n      perfLogger.PerfLogEnd(\"MoveTask\", \"FileMoves\");\n      Partition newTPart = oldPart != null ? oldPart : new Partition(tbl, partSpec, newPartPath);\n      alterPartitionSpecInMemory(tbl, partSpec, newTPart.getTPartition(), inheritTableSpecs, newPartPath.toString());\n      validatePartition(newTPart);\n      if ((null != newFiles) || replace) {\n        fireInsertEvent(tbl, partSpec, newFiles);\n      } else {\n        LOG.debug(\"No new files were created, and is not a replace. Skipping generating INSERT event.\");\n      }\n\n      //column stats will be inaccurate\n      StatsSetupConst.clearColumnStatsState(newTPart.getParameters());\n\n      // recreate the partition if it existed before\n      if (isSkewedStoreAsSubdir) {\n        org.apache.hadoop.hive.metastore.api.Partition newCreatedTpart = newTPart.getTPartition();\n        SkewedInfo skewedInfo = newCreatedTpart.getSd().getSkewedInfo();\n        /* Construct list bucketing location mappings from sub-directory name. */\n        Map<List<String>, String> skewedColValueLocationMaps = constructListBucketingLocationMap(\n            newPartPath, skewedInfo);\n        /* Add list bucketing location mappings. */\n        skewedInfo.setSkewedColValueLocationMaps(skewedColValueLocationMaps);\n        newCreatedTpart.getSd().setSkewedInfo(skewedInfo);\n      }\n      if (!this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsSetupConst.setBasicStatsState(newTPart.getParameters(), StatsSetupConst.FALSE);\n      }\n      if (oldPart == null) {\n        newTPart.getTPartition().setParameters(new HashMap<String,String>());\n        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),\n              StatsSetupConst.TRUE);\n        }\n        MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());\n        try {\n          LOG.debug(\"Adding new partition \" + newTPart.getSpec());\n          getSychronizedMSC().add_partition(newTPart.getTPartition());\n        } catch (AlreadyExistsException aee) {\n          // With multiple users concurrently issuing insert statements on the same partition has\n          // a side effect that some queries may not see a partition at the time when they're issued,\n          // but will realize the partition is actually there when it is trying to add such partition\n          // to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).\n          // For example, imagine such a table is created:\n          //  create table T (name char(50)) partitioned by (ds string);\n          // and the following two queries are launched at the same time, from different sessions:\n          //  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'\n          //  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException\n          // In that case, we want to retry with alterPartition.\n          LOG.debug(\"Caught AlreadyExistsException, trying to alter partition instead\");\n          setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n        }\n      } else {\n        setStatsPropAndAlterPartition(hasFollowingStatsTask, tbl, newTPart);\n      }\n      return newTPart;\n    } catch (IOException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (MetaException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (InvalidOperationException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    } catch (TException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "Utilities::mvFileToFinalPath(Path,Configuration,boolean,Logger,DynamicPartitionCtx,FileSinkDesc,Reporter)",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }",
            "1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405 +\n1406 +\n1407  \n1408  \n1409 +\n1410  \n1411  \n1412 +\n1413  \n1414 +\n1415  \n1416  \n1417  \n1418  \n1419 +\n1420  \n1421 +\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  ",
            "  public static void mvFileToFinalPath(Path specPath, Configuration hconf,\n      boolean success, Logger log, DynamicPartitionCtx dpCtx, FileSinkDesc conf,\n      Reporter reporter) throws IOException,\n      HiveException {\n\n    FileSystem fs = specPath.getFileSystem(hconf);\n    Path tmpPath = Utilities.toTempPath(specPath);\n    Path taskTmpPath = Utilities.toTaskTempPath(specPath);\n    if (success) {\n      FileStatus[] statuses = HiveStatsUtils.getFileStatusRecurse(\n          tmpPath, ((dpCtx == null) ? 1 : dpCtx.getNumDPCols()), fs);\n      if(statuses != null && statuses.length > 0) {\n        PerfLogger perfLogger = SessionState.getPerfLogger();\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // remove any tmp file or double-committed output files\n        List<Path> emptyBuckets = Utilities.removeTempOrDuplicateFiles(fs, statuses, dpCtx, conf, hconf);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RemoveTempOrDuplicateFiles\");\n        // create empty buckets if necessary\n        if (emptyBuckets.size() > 0) {\n          perfLogger.PerfLogBegin(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n          createEmptyBuckets(hconf, emptyBuckets, conf, reporter);\n          perfLogger.PerfLogEnd(\"FileSinkOperator\", \"CreateEmptyBuckets\");\n        }\n\n        // move to the file destination\n        log.info(\"Moving tmp dir: \" + tmpPath + \" to: \" + specPath);\n        perfLogger.PerfLogBegin(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n        Utilities.renameOrMoveFiles(fs, tmpPath, specPath);\n        perfLogger.PerfLogEnd(\"FileSinkOperator\", \"RenameOrMoveFiles\");\n      }\n    } else {\n      fs.delete(tmpPath, true);\n    }\n    fs.delete(taskTmpPath, true);\n  }"
        ],
        [
            "MoveTask::execute(DriverContext)",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            console.printInfo(\"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }",
            " 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449 +\n 450 +\n 451 +\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n      }\n\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        int i = 0;\n        while (i <lmfd.getSourceDirs().size()) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          FileSystem fs = destPath.getFileSystem(conf);\n          if (!fs.exists(destPath.getParent())) {\n            fs.mkdirs(destPath.getParent());\n          }\n          moveFile(srcPath, destPath, isDfsDir);\n          i++;\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        if (work.getCheckFileFormat()) {\n          // Get all files from the src directory\n          FileStatus[] dirs;\n          ArrayList<FileStatus> files;\n          FileSystem srcFs; // source filesystem\n          try {\n            srcFs = tbd.getSourcePath().getFileSystem(conf);\n            dirs = srcFs.globStatus(tbd.getSourcePath());\n            files = new ArrayList<FileStatus>();\n            for (int i = 0; (dirs != null && i < dirs.length); i++) {\n              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));\n              // We only check one file, so exit the loop when we have at least\n              // one.\n              if (files.size() > 0) {\n                break;\n              }\n            }\n          } catch (IOException e) {\n            throw new HiveException(\n                \"addFiles: filesystem error in check phase\", e);\n          }\n\n          // handle file format check for table level\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            boolean flag = true;\n            // work.checkFileFormat is set to true only for Load Task, so assumption here is\n            // dynamic partition context is null\n            if (tbd.getDPCtx() == null) {\n              if (tbd.getPartitionSpec() == null || tbd.getPartitionSpec().isEmpty()) {\n                // Check if the file format of the file matches that of the table.\n                flag = HiveFileFormatUtils.checkInputFormat(\n                    srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n              } else {\n                // Check if the file format of the file matches that of the partition\n                Partition oldPart = db.getPartition(table, tbd.getPartitionSpec(), false);\n                if (oldPart == null) {\n                  // this means we have just created a table and are specifying partition in the\n                  // load statement (without pre-creating the partition), in which case lets use\n                  // table input format class. inheritTableSpecs defaults to true so when a new\n                  // partition is created later it will automatically inherit input format\n                  // from table object\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);\n                } else {\n                  flag = HiveFileFormatUtils.checkInputFormat(\n                      srcFs, conf, oldPart.getInputFormatClass(), files);\n                }\n              }\n              if (!flag) {\n                throw new HiveException(\n                    \"Wrong file format. Please check the file's format.\");\n              }\n            } else {\n              LOG.warn(\"Skipping file format check as dpCtx is not null\");\n            }\n          }\n        }\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getReplace(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd),\n              work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n              hasFollowingStatsTask());\n          if (work.getOutputs() != null) {\n            work.getOutputs().add(new WriteEntity(table,\n                (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                WriteEntity.WriteType.INSERT)));\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          List<BucketCol> bucketCols = null;\n          List<SortCol> sortCols = null;\n          int numBuckets = -1;\n          Task task = this;\n          String path = tbd.getSourcePath().toUri().toString();\n          // Find the first ancestor of this MoveTask which is some form of map reduce task\n          // (Either standard, local, or a merge)\n          while (task.getParentTasks() != null && task.getParentTasks().size() == 1) {\n            task = (Task)task.getParentTasks().get(0);\n            // If it was a merge task or a local map reduce task, nothing can be inferred\n            if (task instanceof MergeFileTask || task instanceof MapredLocalTask) {\n              break;\n            }\n\n            // If it's a standard map reduce task, check what, if anything, it inferred about\n            // the directory this move task is moving\n            if (task instanceof MapRedTask) {\n              MapredWork work = (MapredWork)task.getWork();\n              MapWork mapWork = work.getMapWork();\n              bucketCols = mapWork.getBucketedColsByDirectory().get(path);\n              sortCols = mapWork.getSortedColsByDirectory().get(path);\n              if (work.getReduceWork() != null) {\n                numBuckets = work.getReduceWork().getNumReduceTasks();\n              }\n\n              if (bucketCols != null || sortCols != null) {\n                // This must be a final map reduce task (the task containing the file sink\n                // operator that writes the final output)\n                assert work.isFinalMapRed();\n              }\n              break;\n            }\n\n            // If it's a move task, get the path the files were moved from, this is what any\n            // preceding map reduce task inferred information about, and moving does not invalidate\n            // those assumptions\n            // This can happen when a conditional merge is added before the final MoveTask, but the\n            // condition for merging is not met, see GenMRFileSink1.\n            if (task instanceof MoveTask) {\n              if (((MoveTask)task).getWork().getLoadFileWork() != null) {\n                path = ((MoveTask)task).getWork().getLoadFileWork().getSourcePath().toUri().toString();\n              }\n            }\n          }\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n\n            List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n            // publish DP columns to its subscribers\n            if (dps != null && dps.size() > 0) {\n              pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);\n            }\n            console.printInfo(System.getProperty(\"line.separator\"));\n            long startTime = System.currentTimeMillis();\n            // load the list of DP partitions and return the list of partition specs\n            // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n            // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n            // After that check the number of DPs created to not exceed the limit and\n            // iterate over it and call loadPartition() here.\n            // The reason we don't do inside HIVE-1361 is the latter is large and we\n            // want to isolate any potential issue it may introduce.\n            Map<Map<String, String>, Partition> dp =\n              db.loadDynamicPartitions(\n                tbd.getSourcePath(),\n                tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(),\n                tbd.getReplace(),\n                dpCtx.getNumDPCols(),\n                isSkewedStoredAsDirs(tbd),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,\n                SessionState.get().getTxnMgr().getCurrentTxnId(), hasFollowingStatsTask(),\n                work.getLoadTableWork().getWriteType());\n\n            String loadTime = \"\\t Time taken to load dynamic partitions: \"  +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\";\n            console.printInfo(loadTime);\n            LOG.info(loadTime);\n\n            if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n              throw new HiveException(\"This query creates no partitions.\" +\n                  \" To turn off this error, set hive.error.on.empty.partition=false.\");\n            }\n\n            startTime = System.currentTimeMillis();\n            // for each partition spec, get the partition\n            // and put it to WriteEntity for post-exec hook\n            for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n              Partition partn = entry.getValue();\n\n              if (bucketCols != null || sortCols != null) {\n                updatePartitionBucketSortColumns(\n                    db, table, partn, bucketCols, numBuckets, sortCols);\n              }\n\n              WriteEntity enty = new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE :\n                      WriteEntity.WriteType.INSERT));\n              if (work.getOutputs() != null) {\n                work.getOutputs().add(enty);\n              }\n              // Need to update the queryPlan's output as well so that post-exec hook get executed.\n              // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n              // constructed at compile time and the queryPlan already contains that.\n              // For DP, WriteEntity creation is deferred at this stage so we need to update\n              // queryPlan here.\n              if (queryPlan.getOutputs() == null) {\n                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n              }\n              queryPlan.getOutputs().add(enty);\n\n              // update columnar lineage for each partition\n              dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n              // Don't set lineage on delete as we don't have all the columns\n              if (SessionState.get() != null &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n                  work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n                SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc,\n                    table.getCols());\n              }\n              LOG.info(\"\\tLoading partition \" + entry.getKey());\n            }\n            console.printInfo(\"\\t Time taken for adding to write entity : \" +\n                (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n            dc = null; // reset data container to prevent it being added again.\n          } else { // static partitions\n            List<String> partVals = MetaStoreUtils.getPvals(table.getPartCols(),\n                tbd.getPartitionSpec());\n            db.validatePartitionNameCharacters(partVals);\n            db.loadPartition(tbd.getSourcePath(), tbd.getTable().getTableName(),\n                tbd.getPartitionSpec(), tbd.getReplace(),\n                tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),\n                work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID, hasFollowingStatsTask());\n            Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);\n\n            if (bucketCols != null || sortCols != null) {\n              updatePartitionBucketSortColumns(db, table, partn, bucketCols,\n                  numBuckets, sortCols);\n            }\n\n            dc = new DataContainer(table.getTTable(), partn.getTPartition());\n            // add this partition to post-execution hook\n            if (work.getOutputs() != null) {\n              work.getOutputs().add(new WriteEntity(partn,\n                  (tbd.getReplace() ? WriteEntity.WriteType.INSERT_OVERWRITE\n                      : WriteEntity.WriteType.INSERT)));\n            }\n         }\n        }\n        if (SessionState.get() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<FieldSchema>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          SessionState.get().getLineageState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }"
        ]
    ],
    "dcfb1ed2788a4c497bc251ab777c2d04652fa20c": [
        [
            "HiveDruidQueryBasedInputFormat::splitSelectQuery(Configuration,String,String,Path)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }"
        ],
        [
            "HiveDruidQueryBasedInputFormat::createSplitsIntervals(List,int)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 +\n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  private static List<List<Interval>> createSplitsIntervals(List<Interval> intervals, int numSplits) {\n    final long totalTime = DruidDateTimeUtils.extractTotalTime(intervals);\n    long startTime = intervals.get(0).getStartMillis();\n    long endTime = startTime;\n    long currTime = 0;\n    List<List<Interval>> newIntervals = new ArrayList<>();\n    for (int i = 0, posIntervals = 0; i < numSplits; i++) {\n      final long rangeSize = Math.round( (double) (totalTime * (i + 1)) / numSplits) -\n              Math.round( (double) (totalTime * i) / numSplits);\n      // Create the new interval(s)\n      List<Interval> currentIntervals = new ArrayList<>();\n      while (posIntervals < intervals.size()) {\n        final Interval interval = intervals.get(posIntervals);\n        final long expectedRange = rangeSize - currTime;\n        if (interval.getEndMillis() - startTime >= expectedRange) {\n          endTime = startTime + expectedRange;\n          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n          startTime = endTime;\n          currTime = 0;\n          break;\n        }\n        endTime = interval.getEndMillis();\n        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n        currTime += (endTime - startTime);\n        startTime = intervals.get(++posIntervals).getStartMillis();\n      }\n      newIntervals.add(currentIntervals);\n    }\n    assert endTime == intervals.get(intervals.size()-1).getEndMillis();\n    return newIntervals;\n  }"
        ]
    ],
    "9de529acf4268900f52b5df70e12c74fb4966bba": [
        [
            "ContainerRunnerImpl::ContainerRunnerImpl(Configuration,int,int,boolean,String,AtomicReference,AtomicReference,long,LlapDaemonExecutorMetrics,AMReporter,ClassLoader,DaemonId,UgiFactory)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133 -\n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  public ContainerRunnerImpl(Configuration conf, int numExecutors, int waitQueueSize,\n      boolean enablePreemption, String[] localDirsBase, AtomicReference<Integer> localShufflePort,\n      AtomicReference<InetSocketAddress> localAddress,\n      long totalMemoryAvailableBytes, LlapDaemonExecutorMetrics metrics,\n      AMReporter amReporter, ClassLoader classLoader, DaemonId daemonId, UgiFactory fsUgiFactory) {\n    super(\"ContainerRunnerImpl\");\n    Preconditions.checkState(numExecutors > 0,\n        \"Invalid number of executors: \" + numExecutors + \". Must be > 0\");\n    this.localAddress = localAddress;\n    this.localShufflePort = localShufflePort;\n    this.amReporter = amReporter;\n    this.signer = UserGroupInformation.isSecurityEnabled()\n        ? new LlapSignerImpl(conf, daemonId.getClusterString()) : null;\n    this.fsUgiFactory = fsUgiFactory;\n\n    this.clusterId = daemonId.getClusterString();\n    this.queryTracker = new QueryTracker(conf, localDirsBase, clusterId);\n    addIfService(queryTracker);\n    String waitQueueSchedulerClassName = HiveConf.getVar(\n        conf, ConfVars.LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME);\n    this.executorService = new TaskExecutorService(numExecutors, waitQueueSize,\n        waitQueueSchedulerClassName, enablePreemption, classLoader, metrics);\n\n    addIfService(executorService);\n\n    // 80% of memory considered for accounted buffers. Rest for objects.\n    // TODO Tune this based on the available size.\n    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes * 0.8 / (float) numExecutors);\n    this.metrics = metrics;\n\n    confParams = new TaskRunnerCallable.ConfParams(\n        conf.getInt(TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS_DEFAULT),\n        conf.getLong(\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS_DEFAULT),\n        conf.getInt(TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT,\n            TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT_DEFAULT)\n    );\n    tezHadoopShim = new HadoopShimsLoader(conf).getHadoopShim();\n\n    LOG.info(\"ContainerRunnerImpl config: \" +\n            \"memoryPerExecutorDerviced=\" + memoryPerExecutor",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "  public ContainerRunnerImpl(Configuration conf, int numExecutors, int waitQueueSize,\n      boolean enablePreemption, String[] localDirsBase, AtomicReference<Integer> localShufflePort,\n      AtomicReference<InetSocketAddress> localAddress,\n      long totalMemoryAvailableBytes, LlapDaemonExecutorMetrics metrics,\n      AMReporter amReporter, ClassLoader classLoader, DaemonId daemonId, UgiFactory fsUgiFactory) {\n    super(\"ContainerRunnerImpl\");\n    Preconditions.checkState(numExecutors > 0,\n        \"Invalid number of executors: \" + numExecutors + \". Must be > 0\");\n    this.localAddress = localAddress;\n    this.localShufflePort = localShufflePort;\n    this.amReporter = amReporter;\n    this.signer = UserGroupInformation.isSecurityEnabled()\n        ? new LlapSignerImpl(conf, daemonId.getClusterString()) : null;\n    this.fsUgiFactory = fsUgiFactory;\n\n    this.clusterId = daemonId.getClusterString();\n    this.queryTracker = new QueryTracker(conf, localDirsBase, clusterId);\n    addIfService(queryTracker);\n    String waitQueueSchedulerClassName = HiveConf.getVar(\n        conf, ConfVars.LLAP_DAEMON_WAIT_QUEUE_COMPARATOR_CLASS_NAME);\n    this.executorService = new TaskExecutorService(numExecutors, waitQueueSize,\n        waitQueueSchedulerClassName, enablePreemption, classLoader, metrics);\n\n    addIfService(executorService);\n\n    // Distribute the available memory between the tasks.\n    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes / (float) numExecutors);\n    this.metrics = metrics;\n\n    confParams = new TaskRunnerCallable.ConfParams(\n        conf.getInt(TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS_DEFAULT),\n        conf.getLong(\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS,\n            TezConfiguration.TEZ_TASK_AM_HEARTBEAT_COUNTER_INTERVAL_MS_DEFAULT),\n        conf.getInt(TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT,\n            TezConfiguration.TEZ_TASK_MAX_EVENTS_PER_HEARTBEAT_DEFAULT)\n    );\n    tezHadoopShim = new HadoopShimsLoader(conf).getHadoopShim();\n\n    LOG.info(\"ContainerRunnerImpl config: \" +\n            \"memoryPerExecutorDerviced=\" + memoryPerExecutor"
        ]
    ],
    "597ca1bdcd7d662be9cafb5238b6ae402a2972f1": [
        [
            "LlapDaemonExecutorMetrics::LlapDaemonExecutorMetrics(String,JvmMetrics,String,int,int)",
            " 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sessionId,\n      int numExecutors, final int[] intervals) {\n    this.name = displayName;\n    this.jvmMetrics = jm;\n    this.sessionId = sessionId;\n    this.registry = new MetricsRegistry(\"LlapDaemonExecutorRegistry\");\n    this.registry.tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME).tag(SessionId, sessionId);\n    this.numExecutors = numExecutors;\n    this.threadMXBean = ManagementFactory.getThreadMXBean();\n    this.executorThreadCpuTime = new MutableGaugeLong[numExecutors];\n    this.executorThreadUserTime = new MutableGaugeLong[numExecutors];\n    this.cpuMetricsInfoMap = new ConcurrentHashMap<>();\n    this.userMetricsInfoMap = new ConcurrentHashMap<>();\n\n    final int len = intervals == null ? 0 : intervals.length;\n    this.percentileTimeToKill = new MutableQuantiles[len];\n    this.percentileTimeLost = new MutableQuantiles[len];\n    for (int i=0; i<len; i++) {\n      int interval = intervals[i];\n      percentileTimeToKill[i] = registry.newQuantiles(\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeToKill.name() + \"_\" + interval + \"s\",\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeToKill.description(),\n          \"ops\", \"latency\", interval);\n      percentileTimeLost[i] = registry.newQuantiles(\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeLost.name() + \"_\" + interval + \"s\",\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeLost.description(),\n          \"ops\", \"latency\", interval);\n    }\n\n    for (int i = 0; i < numExecutors; i++) {\n      MetricsInfo mic = new LlapDaemonCustomMetricsInfo(ExecutorThreadCPUTime.name() + \"_\" + i,\n          ExecutorThreadCPUTime.description());\n      MetricsInfo miu = new LlapDaemonCustomMetricsInfo(ExecutorThreadUserTime.name() + \"_\" + i,\n          ExecutorThreadUserTime.description());\n      this.cpuMetricsInfoMap.put(i, mic);\n      this.userMetricsInfoMap.put(i, miu);\n      this.executorThreadCpuTime[i] = registry.newGauge(mic, 0L);\n      this.executorThreadUserTime[i] = registry.newGauge(miu, 0L);\n    }\n  }",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168 +\n 169  \n 170  ",
            "  private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sessionId,\n      int numExecutors, final int[] intervals) {\n    this.name = displayName;\n    this.jvmMetrics = jm;\n    this.sessionId = sessionId;\n    this.registry = new MetricsRegistry(\"LlapDaemonExecutorRegistry\");\n    this.registry.tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME).tag(SessionId, sessionId);\n    this.numExecutors = numExecutors;\n    this.threadMXBean = ManagementFactory.getThreadMXBean();\n    this.executorThreadCpuTime = new MutableGaugeLong[numExecutors];\n    this.executorThreadUserTime = new MutableGaugeLong[numExecutors];\n    this.cpuMetricsInfoMap = new ConcurrentHashMap<>();\n    this.userMetricsInfoMap = new ConcurrentHashMap<>();\n\n    final int len = intervals == null ? 0 : intervals.length;\n    this.percentileTimeToKill = new MutableQuantiles[len];\n    this.percentileTimeLost = new MutableQuantiles[len];\n    for (int i=0; i<len; i++) {\n      int interval = intervals[i];\n      percentileTimeToKill[i] = registry.newQuantiles(\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeToKill.name() + \"_\" + interval + \"s\",\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeToKill.description(),\n          \"ops\", \"latency\", interval);\n      percentileTimeLost[i] = registry.newQuantiles(\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeLost.name() + \"_\" + interval + \"s\",\n          LlapDaemonExecutorInfo.ExecutorMaxPreemptionTimeLost.description(),\n          \"ops\", \"latency\", interval);\n    }\n\n    this.executorNames = Maps.newHashMap();\n    for (int i = 0; i < numExecutors; i++) {\n      MetricsInfo mic = new LlapDaemonCustomMetricsInfo(ExecutorThreadCPUTime.name() + \"_\" + i,\n          ExecutorThreadCPUTime.description());\n      MetricsInfo miu = new LlapDaemonCustomMetricsInfo(ExecutorThreadUserTime.name() + \"_\" + i,\n          ExecutorThreadUserTime.description());\n      this.cpuMetricsInfoMap.put(i, mic);\n      this.userMetricsInfoMap.put(i, miu);\n      this.executorThreadCpuTime[i] = registry.newGauge(mic, 0L);\n      this.executorThreadUserTime[i] = registry.newGauge(miu, 0L);\n      this.executorNames.put(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + i, i);\n    }\n  }"
        ],
        [
            "LlapDaemonExecutorMetrics::updateThreadMetrics(MetricsRecordBuilder)",
            " 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 -\n 310 -\n 311 -\n 312 -\n 313 -\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  ",
            "  private void updateThreadMetrics(MetricsRecordBuilder rb) {\n    if (threadMXBean.isThreadCpuTimeSupported() && threadMXBean.isThreadCpuTimeEnabled()) {\n      final long[] ids = threadMXBean.getAllThreadIds();\n      final ThreadInfo[] infos = threadMXBean.getThreadInfo(ids);\n      for (int i = 0; i < ids.length; i++) {\n        ThreadInfo threadInfo = infos[i];\n        String threadName = threadInfo.getThreadName();\n        long threadId = ids[i];\n        for (int j = 0; j < numExecutors; j++) {\n          if (threadName.equals(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + j)) {\n            executorThreadCpuTime[j].set(threadMXBean.getThreadCpuTime(threadId));\n            executorThreadUserTime[j].set(threadMXBean.getThreadUserTime(threadId));\n          }\n        }\n      }\n\n      for (int i=0; i<numExecutors; i++) {\n        rb.addGauge(cpuMetricsInfoMap.get(i), executorThreadCpuTime[i].value());\n        rb.addGauge(userMetricsInfoMap.get(i), executorThreadUserTime[i].value());\n      }\n    }\n  }",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313 +\n 314 +\n 315  \n 316  \n 317 +\n 318 +\n 319 +\n 320 +\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "  private void updateThreadMetrics(MetricsRecordBuilder rb) {\n    if (threadMXBean.isThreadCpuTimeSupported() && threadMXBean.isThreadCpuTimeEnabled()) {\n      final long[] ids = threadMXBean.getAllThreadIds();\n      final ThreadInfo[] infos = threadMXBean.getThreadInfo(ids);\n      for (int i = 0; i < ids.length; i++) {\n        ThreadInfo threadInfo = infos[i];\n        if (threadInfo == null) {\n          continue;\n        }\n        String threadName = threadInfo.getThreadName();\n        long threadId = ids[i];\n        Integer id = executorNames.get(threadName);\n        if (id != null) {\n          executorThreadCpuTime[id].set(threadMXBean.getThreadCpuTime(threadId));\n          executorThreadUserTime[id].set(threadMXBean.getThreadUserTime(threadId));\n        }\n      }\n\n      for (int i=0; i<numExecutors; i++) {\n        rb.addGauge(cpuMetricsInfoMap.get(i), executorThreadCpuTime[i].value());\n        rb.addGauge(userMetricsInfoMap.get(i), executorThreadUserTime[i].value());\n      }\n    }\n  }"
        ]
    ],
    "36ff484f79b8d2f6771ef42ef6789678b7dcd295": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = JobClient.runJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }"
        ]
    ],
    "53f03358377f3dde21f58e6c841142c6db8a9c32": [
        [
            "TezSessionPoolManager::getSession(HiveConf,boolean)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.set(\"tez.queue.name\", null);\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 +\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.unset(\"tez.queue.name\");\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }"
        ]
    ],
    "03216f50137a226c67a25971319dbde96912bc6a": [
        [
            "TezSessionPoolManager::getNewSessionState(HiveConf,String,boolean)",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 -\n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "  /**\n   * @param conf HiveConf that is used to initialize the session\n   * @param queueName could be null. Set in the tez session.\n   * @param doOpen\n   * @return\n   * @throws Exception\n   */\n  private TezSessionState getNewSessionState(HiveConf conf,\n      String queueName, boolean doOpen) throws Exception {\n    TezSessionPoolSession retTezSessionState = createAndInitSession(queueName, false);\n    if (queueName != null) {\n      conf.set(\"tez.queue.name\", queueName);\n    }\n    if (doOpen) {\n      retTezSessionState.open(conf);\n      LOG.info(\"Started a new session for queue: \" + queueName +\n          \" session id: \" + retTezSessionState.getSessionId());\n    }\n    return retTezSessionState;\n  }",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 +\n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "  /**\n   * @param conf HiveConf that is used to initialize the session\n   * @param queueName could be null. Set in the tez session.\n   * @param doOpen\n   * @return\n   * @throws Exception\n   */\n  private TezSessionState getNewSessionState(HiveConf conf,\n      String queueName, boolean doOpen) throws Exception {\n    TezSessionPoolSession retTezSessionState = createAndInitSession(queueName, false);\n    if (queueName != null) {\n      conf.set(TezConfiguration.TEZ_QUEUE_NAME, queueName);\n    }\n    if (doOpen) {\n      retTezSessionState.open(conf);\n      LOG.info(\"Started a new session for queue: \" + queueName +\n          \" session id: \" + retTezSessionState.getSessionId());\n    }\n    return retTezSessionState;\n  }"
        ],
        [
            "TezSessionPoolManager::getSession(HiveConf,boolean)",
            " 306  \n 307  \n 308 -\n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(\"tez.queue.name\");\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying tez.queue.name is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.unset(\"tez.queue.name\");\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }",
            " 306  \n 307  \n 308 +\n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  \n 317 +\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  ",
            "  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n      throws Exception {\n    String queueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);\n    boolean hasQueue = (queueName != null) && !queueName.isEmpty();\n    if (hasQueue) {\n      switch (customQueueAllowed) {\n      case FALSE: throw new HiveException(\"Specifying \" + TezConfiguration.TEZ_QUEUE_NAME + \" is not allowed\");\n      case IGNORE: {\n        LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n        queueName = null;\n        hasQueue = false;\n        conf.unset(TezConfiguration.TEZ_QUEUE_NAME);\n      }\n      default: // All good.\n      }\n    }\n    for (ConfVars var : restrictedHiveConf) {\n      String userValue = HiveConf.getVarWithoutType(conf, var),\n          serverValue = HiveConf.getVarWithoutType(initConf, var);\n      // Note: with some trickery, we could add logic for each type in ConfVars; for now the\n      // potential spurious mismatches (e.g. 0 and 0.0 for float) should be easy to work around.\n      validateRestrictedConfigValues(var.varname, userValue, serverValue);\n    }\n    for (String var : restrictedNonHiveConf) {\n      String userValue = conf.get(var), serverValue = initConf.get(var);\n      validateRestrictedConfigValues(var, userValue, serverValue);\n    }\n\n    // TODO Session re-use completely disabled for doAs=true. Always launches a new session.\n    boolean nonDefaultUser = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n\n    /*\n     * if the user has specified a queue name themselves, we create a new session.\n     * also a new session is created if the user tries to submit to a queue using\n     * their own credentials. We expect that with the new security model, things will\n     * run as user hive in most cases.\n     */\n    if (nonDefaultUser || !hasInitialSessions || hasQueue) {\n      LOG.info(\"QueueName: {} nonDefaultUser: {} defaultQueuePool: {} hasInitialSessions: {}\",\n              queueName, nonDefaultUser, defaultQueuePool, hasInitialSessions);\n      return getNewSessionState(conf, queueName, doOpen);\n    }\n\n    LOG.info(\"Choosing a session from the defaultQueuePool\");\n    while (true) {\n      TezSessionPoolSession result = defaultQueuePool.take();\n      if (result.tryUse()) return result;\n      LOG.info(\"Couldn't use a session [\" + result + \"]; attempting another one\");\n    }\n  }"
        ]
    ],
    "14f6f164d1b1a40e1474a17399869dea2bd2348d": [
        [
            "TestHiveMetaStoreChecker::testSingleThreadedCheckMetastore()",
            " 335  \n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 336  \n 337  \n 338  \n 339 +\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "  public void testSingleThreadedCheckMetastore()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testSingleThreadedDeeplyNestedTables()",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360 -\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }",
            " 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 +\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Tests single threaded implementation for deeply nested partitioned tables\n   *\n   * @throws HiveException\n   * @throws AlreadyExistsException\n   * @throws IOException\n   */\n  public void testSingleThreadedDeeplyNestedTables()\n      throws HiveException, AlreadyExistsException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs\n    // no other easy way to set it deterministically for this test case\n    checker = Mockito.spy(checker);\n    Mockito.when(checker.getMinPoolSize()).thenReturn(2);\n    int poolSize = checker.getMinPoolSize();\n    // create a deeply nested table which has more partition keys than the pool size\n    Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    CheckResult result = new CheckResult();\n    checker.checkMetastore(dbName, tableName, null, result);\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n    assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n    assertEquals(Collections.<String> emptySet(), result.getPartitionsNotOnFs());\n    assertEquals(10, result.getPartitionsNotInMs().size());\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::testErrorForMissingPartitionsSingleThreaded()",
            " 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }",
            " 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "  public void testErrorForMissingPartitionsSingleThreaded()\n      throws AlreadyExistsException, HiveException, IOException {\n    // set num of threads to 0 so that single-threaded checkMetastore is called\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);\n    Table testTable = createPartitionedTestTable(dbName, tableName, 2, 0);\n    // add 10 partitions on the filesystem\n    createPartitionsDirectoriesOnFS(testTable, 10);\n    // create a fake directory to throw exception\n    StringBuilder sb = new StringBuilder(testTable.getDataLocation().toString());\n    sb.append(Path.SEPARATOR);\n    sb.append(\"dummyPart=error\");\n    createDirectory(sb.toString());\n    // check result now\n    CheckResult result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n    createFile(sb.toString(), \"dummyFile\");\n    result = new CheckResult();\n    try {\n      checker.checkMetastore(dbName, tableName, null, result);\n    } catch (Exception e) {\n      assertTrue(\"Expected exception HiveException got \" + e.getClass(),\n          e instanceof HiveException);\n    }\n  }"
        ],
        [
            "TestHiveMetaStoreChecker::setUp()",
            "  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.HIVE_MOVE_FILES_THREAD_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  ",
            "  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    hive = Hive.get();\n    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 15);\n    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n    checker = new HiveMetaStoreChecker(hive);\n\n    partCols = new ArrayList<FieldSchema>();\n    partCols.add(new FieldSchema(partDateName, serdeConstants.STRING_TYPE_NAME, \"\"));\n    partCols.add(new FieldSchema(partCityName, serdeConstants.STRING_TYPE_NAME, \"\"));\n\n    parts = new ArrayList<Map<String, String>>();\n    Map<String, String> part1 = new HashMap<String, String>();\n    part1.put(partDateName, \"2008-01-01\");\n    part1.put(partCityName, \"london\");\n    parts.add(part1);\n    Map<String, String> part2 = new HashMap<String, String>();\n    part2.put(partDateName, \"2008-01-02\");\n    part2.put(partCityName, \"stockholm\");\n    parts.add(part2);\n\n    //cleanup just in case something is left over from previous run\n    dropDbTable();\n  }"
        ]
    ],
    "a4fd2ea2313efb9850715a03bd826975c60e0ede": [
        [
            "TezCompiler::SemiJoinRemovalIfNoStatsProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      boolean removeSemiJoin = false;\n      for (AggregationDesc agg : aggregationDescs) {\n        if (agg.getGenericUDAFName() != \"bloom_filter\") {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          removeSemiJoin = true;\n          break;\n        }\n      }\n\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          removeSemiJoin = true;\n        }\n      }\n\n      if (removeSemiJoin) {\n        // The stats are not annotated, remove the semijoin operator\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n      }\n      return null;\n    }",
            " 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809 +\n 810 +\n 811 +\n 812 +\n 813 +\n 814 +\n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825 +\n 826 +\n 827 +\n 828 +\n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      boolean removeSemiJoin = false;\n      for (AggregationDesc agg : aggregationDescs) {\n        if (agg.getGenericUDAFName() != \"bloom_filter\") {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          removeSemiJoin = true;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                + \"Removing semijoin \"\n                + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n          break;\n        }\n      }\n\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          removeSemiJoin = true;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n        }\n      }\n\n      if (removeSemiJoin) {\n        // The stats are not annotated, remove the semijoin operator\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n      }\n      return null;\n    }"
        ],
        [
            "TezCompiler::removeSemijoinsParallelToMapJoin(OptimizeTezProcContext)",
            " 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN)) {\n      // Not needed without semi-join reduction\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }",
            " 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961 +\n 962 +\n 963 +\n 964 +\n 965  \n 966  \n 967  \n 968  \n 969  \n 970  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN)) {\n      // Not needed without semi-join reduction\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Semijoin optimization with parallel edge to map join. Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semijoins.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }"
        ],
        [
            "ConvertJoinMapJoin::removeCycleCreatingSemiJoinOps(MapJoinOperator,Operator,ParseContext)",
            " 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    Map<ReduceSinkOperator, TableScanOperator> semiJoinMap =\n            new HashMap<ReduceSinkOperator, TableScanOperator>();\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0) {\n        op = op.getChildOperators().get(0);\n      }\n\n      // If not ReduceSink Op, skip\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      ReduceSinkOperator rs = (ReduceSinkOperator) op;\n      TableScanOperator ts = parseContext.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // skip, no semijoin branch\n        continue;\n      }\n\n      // Found a semijoin branch.\n      for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n        if (!(parent instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                TableScanOperator.class);\n        for (TableScanOperator parentTS : tsOps) {\n          // If the parent is same as the ts, then we have a cycle.\n          if (ts == parentTS) {\n            semiJoinMap.put(rs, ts);\n            break;\n          }\n        }\n      }\n    }\n    if (semiJoinMap.size() > 0) {\n      for (ReduceSinkOperator rs : semiJoinMap.keySet()) {\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(parseContext, rs,\n                semiJoinMap.get(rs));\n      }\n    }\n  }",
            " 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859  \n 860  \n 861  \n 862  \n 863  \n 864  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    Map<ReduceSinkOperator, TableScanOperator> semiJoinMap =\n            new HashMap<ReduceSinkOperator, TableScanOperator>();\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0) {\n        op = op.getChildOperators().get(0);\n      }\n\n      // If not ReduceSink Op, skip\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      ReduceSinkOperator rs = (ReduceSinkOperator) op;\n      TableScanOperator ts = parseContext.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // skip, no semijoin branch\n        continue;\n      }\n\n      // Found a semijoin branch.\n      for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n        if (!(parent instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                TableScanOperator.class);\n        for (TableScanOperator parentTS : tsOps) {\n          // If the parent is same as the ts, then we have a cycle.\n          if (ts == parentTS) {\n            semiJoinMap.put(rs, ts);\n            break;\n          }\n        }\n      }\n    }\n    if (semiJoinMap.size() > 0) {\n      for (ReduceSinkOperator rs : semiJoinMap.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Found semijoin optimization from the big table side of a map join, which will cause a task cycle. \"\n              + \"Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semiJoinMap.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(parseContext, rs,\n                semiJoinMap.get(rs));\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeCycleOperator(Set,OptimizeTezProcContext)",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n        TableScanOperator ts = context.parseContext.getRsOpToTsOpMap().get(o);\n        if (ts == null) {\n          continue;\n        }\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                victimTS.getStatistics().getDataSize()) {\n            victimRS = (ReduceSinkOperator) o;\n            victimTS = ts;\n          }\n        }\n      }\n\n    // Always set the min/max optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    }\n    return;\n  }",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239 +\n 240 +\n 241 +\n 242  \n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248  \n 249  \n 250  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n        TableScanOperator ts = context.parseContext.getRsOpToTsOpMap().get(o);\n        if (ts == null) {\n          continue;\n        }\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                victimTS.getStatistics().getDataSize()) {\n            victimRS = (ReduceSinkOperator) o;\n            victimTS = ts;\n          }\n        }\n      }\n\n    // Always set the min/max optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cycle found. Removing semijoin \"\n            + OperatorUtils.getOpNamePretty(victimRS) + \" - \" + OperatorUtils.getOpNamePretty(victimTS));\n      }\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    } else {\n      // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n          + \". Needed to break cyclic dependency\");\n    }\n    return;\n  }"
        ],
        [
            "TezCompiler::removeSemiJoinCyclesDueToMapsideJoins(OptimizeTezProcContext)",
            " 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n        if (ts == null) {\n          continue;\n        }\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }",
            " 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766 +\n 767 +\n 768 +\n 769 +\n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        TableScanOperator ts = pCtx.getRsOpToTsOpMap().get(rs);\n        if (ts == null) {\n          continue;\n        }\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin cycle due to mapjoin. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeSemijoinOptimizationFromSMBJoins(OptimizeTezProcContext)",
            " 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  ",
            "  private static void removeSemijoinOptimizationFromSMBJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", TableScanOperator.getOperatorName() + \"%\" +\n                    \".*\" + TezDummyStoreOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SMBJoinOpProc());\n\n    SMBJoinOpProcContext ctx = new SMBJoinOpProcContext();\n    // The dispatcher finds SMB and if there is semijoin optimization before it, removes it.\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // Iterate over the map and remove semijoin optimizations if needed.\n    for (CommonMergeJoinOperator joinOp : ctx.JoinOpToTsOpMap.keySet()) {\n      List<TableScanOperator> tsOps = new ArrayList<TableScanOperator>();\n      // Get one top level TS Op directly from the stack\n      tsOps.add(ctx.JoinOpToTsOpMap.get(joinOp));\n\n      // Get the other one by examining Join Op\n      List<Operator<?>> parents = joinOp.getParentOperators();\n      for (Operator<?> parent : parents) {\n        if (parent instanceof TezDummyStoreOperator) {\n          // already accounted for\n          continue;\n        }\n\n        assert parent instanceof SelectOperator;\n        while(parent != null) {\n          if (parent instanceof TableScanOperator) {\n            tsOps.add((TableScanOperator) parent);\n            break;\n          }\n          parent = parent.getParentOperators().get(0);\n        }\n      }\n\n      // Now the relevant TableScanOperators are known, find if there exists\n      // a semijoin filter on any of them, if so, remove it.\n      ParseContext pctx = procCtx.parseContext;\n      for (TableScanOperator ts : tsOps) {\n        for (ReduceSinkOperator rs : pctx.getRsOpToTsOpMap().keySet()) {\n          if (ts == pctx.getRsOpToTsOpMap().get(rs)) {\n            // match!\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pctx, rs, ts);\n          }\n        }\n      }\n    }\n  }",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663 +\n 664 +\n 665 +\n 666 +\n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "  private static void removeSemijoinOptimizationFromSMBJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsOpToTsOpMap().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", TableScanOperator.getOperatorName() + \"%\" +\n                    \".*\" + TezDummyStoreOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SMBJoinOpProc());\n\n    SMBJoinOpProcContext ctx = new SMBJoinOpProcContext();\n    // The dispatcher finds SMB and if there is semijoin optimization before it, removes it.\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // Iterate over the map and remove semijoin optimizations if needed.\n    for (CommonMergeJoinOperator joinOp : ctx.JoinOpToTsOpMap.keySet()) {\n      List<TableScanOperator> tsOps = new ArrayList<TableScanOperator>();\n      // Get one top level TS Op directly from the stack\n      tsOps.add(ctx.JoinOpToTsOpMap.get(joinOp));\n\n      // Get the other one by examining Join Op\n      List<Operator<?>> parents = joinOp.getParentOperators();\n      for (Operator<?> parent : parents) {\n        if (parent instanceof TezDummyStoreOperator) {\n          // already accounted for\n          continue;\n        }\n\n        assert parent instanceof SelectOperator;\n        while(parent != null) {\n          if (parent instanceof TableScanOperator) {\n            tsOps.add((TableScanOperator) parent);\n            break;\n          }\n          parent = parent.getParentOperators().get(0);\n        }\n      }\n\n      // Now the relevant TableScanOperators are known, find if there exists\n      // a semijoin filter on any of them, if so, remove it.\n      ParseContext pctx = procCtx.parseContext;\n      for (TableScanOperator ts : tsOps) {\n        for (ReduceSinkOperator rs : pctx.getRsOpToTsOpMap().keySet()) {\n          if (ts == pctx.getRsOpToTsOpMap().get(rs)) {\n            // match!\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin optimization found going to SMB join. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pctx, rs, ts);\n          }\n        }\n      }\n    }\n  }"
        ]
    ],
    "ce037d14645599a5357fab4ca63167566b7037cb": [
        [
            "CompactorMR::launchCompactionJob(JobConf,Path,CompactionType,StringableList,List,int,int,HiveConf,TxnStore,long)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 +\n 317 +\n 318 +\n 319  ",
            "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = new JobClient(job).submitJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n    txnHandler.setHadoopJobId(rj.getID().toString(), id);\n    rj.waitForCompletion();\n    if (!rj.isSuccessful()) {\n      throw new IOException(\"Job failed!\");\n    }\n  }"
        ]
    ],
    "bbf0629a5a2e43531c4fd5e17d727497e89d267d": [
        [
            "CachedStore::setConf(Configuration)",
            " 196  \n 197  \n 198  \n 199  \n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    try {\n      rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n          rawStoreClassName)).newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }",
            " 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "  @Override\n  public void setConf(Configuration conf) {\n    String rawStoreClassName = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n        ObjectStore.class.getName());\n    if (rawStore == null) {\n      try {\n        rawStore = ((Class<? extends RawStore>) MetaStoreUtils.getClass(\n            rawStoreClassName)).newInstance();\n      } catch (Exception e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }\n    rawStore.setConf(conf);\n    Configuration oldConf = this.conf;\n    this.conf = conf;\n    if (expressionProxy != null && conf != oldConf) {\n      LOG.warn(\"Unexpected setConf when we were already configured\");\n    }\n    if (expressionProxy == null || conf != oldConf) {\n      expressionProxy = PartFilterExprUtil.createExpressionProxy(conf);\n    }\n    if (firstTime) {\n      try {\n        LOG.info(\"Prewarming CachedStore\");\n        prewarm();\n        LOG.info(\"CachedStore initialized\");\n        // Start the cache update master-worker threads\n        startCacheUpdateService();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n      firstTime = false;\n    }\n  }"
        ],
        [
            "CachedStore::CacheUpdateMasterWork::run()",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      try {\n        RawStore rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      }\n    }",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364 +\n 365 +\n 366 +\n 367 +\n 368 +\n 369 +\n 370  \n 371  ",
            "    @Override\n    public void run() {\n      // Prevents throwing exceptions in our raw store calls since we're not using RawStoreProxy\n      Deadline.registerIfNot(1000000);\n      LOG.debug(\"CachedStore: updating cached objects\");\n      String rawStoreClassName =\n          HiveConf.getVar(cachedStore.conf, HiveConf.ConfVars.METASTORE_CACHED_RAW_STORE_IMPL,\n              ObjectStore.class.getName());\n      RawStore rawStore = null;\n      try {\n        rawStore =\n            ((Class<? extends RawStore>) MetaStoreUtils.getClass(rawStoreClassName)).newInstance();\n        rawStore.setConf(cachedStore.conf);\n        List<String> dbNames = rawStore.getAllDatabases();\n        if (dbNames != null) {\n          // Update the database in cache\n          updateDatabases(rawStore, dbNames);\n          for (String dbName : dbNames) {\n            // Update the tables in cache\n            updateTables(rawStore, dbName);\n            List<String> tblNames = cachedStore.getAllTables(dbName);\n            for (String tblName : tblNames) {\n              // Update the partitions for a table in cache\n              updateTablePartitions(rawStore, dbName, tblName);\n              // Update the table column stats for a table in cache\n              updateTableColStats(rawStore, dbName, tblName);\n              // Update the partitions column stats for a table in cache\n              updateTablePartitionColStats(rawStore, dbName, tblName);\n            }\n          }\n        }\n      } catch (MetaException e) {\n        LOG.error(\"Updating CachedStore: error getting database names\", e);\n      } catch (InstantiationException | IllegalAccessException e) {\n        throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n      } finally {\n        try {\n          if (rawStore != null) {\n            rawStore.shutdown();\n          }\n        } catch (Exception e) {\n          LOG.error(\"Error shutting down RawStore\", e);\n        }\n      }\n    }"
        ],
        [
            "SessionState::unCacheDataNucleusClassLoaders()",
            "1682  \n1683  \n1684  \n1685  \n1686  \n1687 -\n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }",
            "1683  \n1684  \n1685  \n1686  \n1687  \n1688 +\n1689 +\n1690 +\n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  ",
            "  private void unCacheDataNucleusClassLoaders() {\n    try {\n      Hive threadLocalHive = Hive.get(sessionConf);\n      if ((threadLocalHive != null) && (threadLocalHive.getMSC() != null)\n          && (threadLocalHive.getMSC().isLocalMetaStore())) {\n        if (sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(ObjectStore.class.getName())\n            || sessionConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL).equals(CachedStore.class.getName()) &&\n            sessionConf.getVar(ConfVars.METASTORE_CACHED_RAW_STORE_IMPL).equals(ObjectStore.class.getName())) {\n          ObjectStore.unCacheDataNucleusClassLoaders();\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Failed to remove classloaders from DataNucleus \", e);\n    }\n  }"
        ]
    ],
    "4a14cfc01b6c06817899290b6d2c5f0849cb44dd": [
        [
            "HCatLoader::getStatistics(String,Job)",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 -\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setmBytes(getSizeInBytes(inputJobInfo) / 1024 / 1024);\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  /**\n   * Get statistics about the data to be loaded. Only input data size is implemented at this time.\n   */\n  @Override\n  public ResourceStatistics getStatistics(String location, Job job) throws IOException {\n    try {\n      ResourceStatistics stats = new ResourceStatistics();\n      InputJobInfo inputJobInfo = (InputJobInfo) HCatUtil.deserialize(\n        job.getConfiguration().get(HCatConstants.HCAT_KEY_JOB_INFO));\n      stats.setSizeInBytes(getSizeInBytes(inputJobInfo));\n      return stats;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ],
    "37be57b647932eab98e9ce77c44f10a0c58f1a6a": [
        [
            "DruidQueryBasedInputFormat::getInputSplits(Configuration)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null) {\n        throw new IOException(\"Druid data source cannot be empty\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "  @SuppressWarnings(\"deprecation\")\n  private HiveDruidSplit[] getInputSplits(Configuration conf) throws IOException {\n    String address = HiveConf.getVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n    );\n    if (StringUtils.isEmpty(address)) {\n      throw new IOException(\"Druid broker address not specified in configuration\");\n    }\n    String druidQuery = StringEscapeUtils.unescapeJava(conf.get(Constants.DRUID_QUERY_JSON));\n    String druidQueryType;\n    if (StringUtils.isEmpty(druidQuery)) {\n      // Empty, maybe because CBO did not run; we fall back to\n      // full Select query\n      if (LOG.isWarnEnabled()) {\n        LOG.warn(\"Druid query is empty; creating Select query\");\n      }\n      String dataSource = conf.get(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null || dataSource.isEmpty()) {\n        throw new IOException(\"Druid data source cannot be empty or null\");\n      }\n      druidQuery = createSelectStarQuery(dataSource);\n      druidQueryType = Query.SELECT;\n    } else {\n      druidQueryType = conf.get(Constants.DRUID_QUERY_TYPE);\n      if (druidQueryType == null) {\n        throw new IOException(\"Druid query type not recognized\");\n      }\n    }\n\n    // hive depends on FileSplits\n    Job job = new Job(conf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] paths = FileInputFormat.getInputPaths(jobContext);\n\n    // We need to deserialize and serialize query so intervals are written in the JSON\n    // Druid query with user timezone, as this is default Hive time semantics.\n    // Then, create splits with the Druid queries.\n    switch (druidQueryType) {\n      case Query.TIMESERIES:\n      case Query.TOPN:\n      case Query.GROUP_BY:\n        return new HiveDruidSplit[] { new HiveDruidSplit(deserializeSerialize(druidQuery),\n                paths[0], new String[] {address}) };\n      case Query.SELECT:\n        SelectQuery selectQuery = DruidStorageHandlerUtils.JSON_MAPPER.readValue(\n                druidQuery, SelectQuery.class);\n        boolean distributed = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_DISTRIBUTE);\n        if (distributed) {\n          return distributeSelectQuery(conf, address, selectQuery, paths[0]);\n        } else {\n          return splitSelectQuery(conf, address, selectQuery, paths[0]);\n        }\n      default:\n        throw new IOException(\"Druid query type not recognized\");\n    }\n  }"
        ],
        [
            "DruidQueryBasedInputFormat::createSelectStarQuery(String)",
            " 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList();\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }",
            " 164  \n 165  \n 166  \n 167  \n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "  private static String createSelectStarQuery(String dataSource) throws IOException {\n    // Create Select query\n    SelectQueryBuilder builder = new Druids.SelectQueryBuilder();\n    builder.dataSource(dataSource);\n    final List<Interval> intervals = Arrays.asList(DruidStorageHandlerUtils.DEFAULT_INTERVAL);\n    builder.intervals(intervals);\n    builder.pagingSpec(PagingSpec.newSpec(1));\n    Map<String, Object> context = new HashMap<>();\n    context.put(Constants.DRUID_QUERY_FETCH, false);\n    builder.context(context);\n    return DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(builder.build());\n  }"
        ]
    ],
    "fb07a1157b0b99cf51a3965e79b2c46e1845f5ee": [
        [
            "TestSQLStdHiveAccessControllerHS2::getSettableParams()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }"
        ]
    ],
    "4bba139d3719c11a015919c1560ac473651f93c5": [
        [
            "TestSQLStdHiveAccessControllerHS2::getSettableParams()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  ",
            "  /**\n   * @return list of parameters that should be possible to set\n   */\n  private List<String> getSettableParams() throws SecurityException, NoSuchFieldException,\n      IllegalArgumentException, IllegalAccessException {\n    // get all the variable names being converted to regex in HiveConf, using reflection\n    Field varNameField = HiveConf.class.getDeclaredField(\"sqlStdAuthSafeVarNames\");\n    varNameField.setAccessible(true);\n    List<String> confVarList = Arrays.asList((String[]) varNameField.get(null));\n\n    // create list with variables that match some of the regexes\n    List<String> confVarRegexList = Arrays.asList(\"hive.convert.join.bucket.mapjoin.tez\",\n        \"hive.optimize.index.filter.compact.maxsize\", \"hive.tez.dummy\", \"tez.task.dummy\",\n        \"hive.exec.dynamic.partition\", \"hive.exec.dynamic.partition.mode\",\n        \"hive.exec.max.dynamic.partitions\", \"hive.exec.max.dynamic.partitions.pernode\",\n        \"oozie.HadoopAccessorService.created\", \"tez.queue.name\",\"hive.druid.select.distribute\",\n        \"distcp.options.px\");\n\n    // combine two lists\n    List<String> varList = new ArrayList<String>();\n    varList.addAll(confVarList);\n    varList.addAll(confVarRegexList);\n    return varList;\n\n  }"
        ]
    ],
    "2139ef601b91d2982acd25ed1450ab3bda0dbc49": [
        [
            "Utilities::moveSpecifiedFiles(FileSystem,Path,Path,Set)",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      }\n    }\n  }",
            "1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184 +\n1185 +\n1186 +\n1187 +\n1188 +\n1189  \n1190  \n1191  ",
            "  /**\n   * Moves files from src to dst if it is within the specified set of paths\n   * @param fs\n   * @param src\n   * @param dst\n   * @param filesToMove\n   * @throws IOException\n   * @throws HiveException\n   */\n  private static void moveSpecifiedFiles(FileSystem fs, Path src, Path dst, Set<Path> filesToMove)\n      throws IOException, HiveException {\n    if (!fs.exists(dst)) {\n      fs.mkdirs(dst);\n    }\n\n    FileStatus[] files = fs.listStatus(src);\n    for (FileStatus file : files) {\n      if (filesToMove.contains(file.getPath())) {\n        Utilities.moveFile(fs, file, dst);\n      } else if (file.isDir()) {\n        // Traverse directory contents.\n        // Directory nesting for dst needs to match src.\n        Path nestedDstPath = new Path(dst, file.getPath().getName());\n        Utilities.moveSpecifiedFiles(fs, file.getPath(), nestedDstPath, filesToMove);\n      }\n    }\n  }"
        ]
    ],
    "8d9b84294d83c4afff5475523190bca0b0414a73": [
        [
            "ObjectStore::lockForUpdate()",
            "8577  \n8578 -\n8579  \n8580  \n8581  \n8582  \n8583  \n8584  \n8585  \n8586  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select \\\"NEXT_EVENT_ID\\\" from \\\"NOTIFICATION_SEQUENCE\\\"\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(conf, () -> {\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n    }).run();\n  }",
            "8577  \n8578 +\n8579  \n8580  \n8581  \n8582  \n8583  \n8584  \n8585  \n8586  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select \\\"NEXT_EVENT_ID\\\" from NOTIFICATION_SEQUENCE\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(conf, () -> {\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n    }).run();\n  }"
        ]
    ],
    "bc8307eacd291368b9822c1820a047febdb76952": [
        [
            "DruidSerDe::deserialize(Writable)",
            " 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }",
            " 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 +\n 470 +\n 471 +\n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484 +\n 485 +\n 486 +\n 487 +\n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  ",
            "  @Override\n  public Object deserialize(Writable writable) throws SerDeException {\n    DruidWritable input = (DruidWritable) writable;\n    List<Object> output = Lists.newArrayListWithExpectedSize(columns.length);\n    for (int i = 0; i < columns.length; i++) {\n      final Object value = input.getValue().get(columns[i]);\n      if (value == null) {\n        output.add(null);\n        continue;\n      }\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          output.add(new TimestampWritable(new Timestamp((Long) value)));\n          break;\n        case BYTE:\n          output.add(new ByteWritable(((Number) value).byteValue()));\n          break;\n        case SHORT:\n          output.add(new ShortWritable(((Number) value).shortValue()));\n          break;\n        case INT:\n          output.add(new IntWritable(((Number) value).intValue()));\n          break;\n        case LONG:\n          output.add(new LongWritable(((Number) value).longValue()));\n          break;\n        case FLOAT:\n          output.add(new FloatWritable(((Number) value).floatValue()));\n          break;\n        case DOUBLE:\n          output.add(new DoubleWritable(((Number) value).doubleValue()));\n          break;\n        case DECIMAL:\n          output.add(new HiveDecimalWritable(HiveDecimal.create(((Number) value).doubleValue())));\n          break;\n        case STRING:\n          output.add(new Text(value.toString()));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n    }\n    return output;\n  }"
        ]
    ],
    "2433fed55bf3417c02551ccee6bb76b282905a13": [
        [
            "SemanticAnalyzer::setupStats(TableScanDesc,QBParseInfo,Table,String,RowResolver)",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 -\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getExtTmpPathRelTo(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }",
            "10247  \n10248  \n10249  \n10250  \n10251  \n10252  \n10253  \n10254  \n10255 +\n10256  \n10257  \n10258  \n10259  \n10260  \n10261  \n10262  \n10263  \n10264  \n10265  \n10266  \n10267  \n10268  \n10269  \n10270  \n10271  \n10272  \n10273  \n10274  \n10275  \n10276  \n10277  \n10278  \n10279  \n10280  \n10281  \n10282  \n10283  \n10284  \n10285  \n10286  \n10287  \n10288  \n10289  \n10290  \n10291  \n10292  \n10293  \n10294  \n10295  \n10296  \n10297  \n10298  \n10299  \n10300  \n10301  \n10302  \n10303  \n10304  \n10305  \n10306  \n10307  \n10308  \n10309  \n10310  ",
            "  private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String alias,\n      RowResolver rwsch)\n      throws SemanticException {\n\n    if (!qbp.isAnalyzeCommand()) {\n      tsDesc.setGatherStats(false);\n    } else {\n      if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n        String statsTmpLoc = ctx.getTempDirForPath(tab.getPath()).toString();\n        LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n        tsDesc.setTmpStatsDir(statsTmpLoc);\n      }\n      tsDesc.setGatherStats(true);\n      tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n\n      // append additional virtual columns for storing statistics\n      Iterator<VirtualColumn> vcs = VirtualColumn.getStatsRegistry(conf).iterator();\n      List<VirtualColumn> vcList = new ArrayList<VirtualColumn>();\n      while (vcs.hasNext()) {\n        VirtualColumn vc = vcs.next();\n        rwsch.put(alias, vc.getName(), new ColumnInfo(vc.getName(),\n            vc.getTypeInfo(), alias, true, vc.getIsHidden()));\n        vcList.add(vc);\n      }\n      tsDesc.addVirtualCols(vcList);\n\n      String tblName = tab.getTableName();\n      TableSpec tblSpec = qbp.getTableSpec(alias);\n      Map<String, String> partSpec = tblSpec.getPartSpec();\n\n      if (partSpec != null) {\n        List<String> cols = new ArrayList<String>();\n        cols.addAll(partSpec.keySet());\n        tsDesc.setPartColumns(cols);\n      }\n\n      // Theoretically the key prefix could be any unique string shared\n      // between TableScanOperator (when publishing) and StatsTask (when aggregating).\n      // Here we use\n      // db_name.table_name + partitionSec\n      // as the prefix for easy of read during explain and debugging.\n      // Currently, partition spec can only be static partition.\n      String k = MetaStoreUtils.encodeTableName(tblName) + Path.SEPARATOR;\n      tsDesc.setStatsAggPrefix(tab.getDbName()+\".\"+k);\n\n      // set up WriteEntity for replication\n      outputs.add(new WriteEntity(tab, WriteEntity.WriteType.DDL_SHARED));\n\n      // add WriteEntity for each matching partition\n      if (tab.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(ErrorMsg.NEED_PARTITION_SPECIFICATION.getMsg());\n        }\n        List<Partition> partitions = qbp.getTableSpec().partitions;\n        if (partitions != null) {\n          for (Partition partn : partitions) {\n            // inputs.add(new ReadEntity(partn)); // is this needed at all?\n\t      LOG.info(\"XXX: adding part: \"+partn);\n            outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));\n          }\n        }\n      }\n    }\n  }"
        ]
    ],
    "e0203eb489d2ebc7b08c4c8fa150ad54ff659b07": [
        [
            "DynamicPartitionPruningOptimization::generateSemiJoinOperatorPlan(DynamicListContext,ParseContext,TableScanOperator,String)",
            " 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on parition column.\n      String internalColName = null;\n      ExprNodeDesc exprNodeDesc = key;\n      // Find the ExprNodeColumnDesc\n      while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n              (exprNodeDesc.getChildren() != null)) {\n        exprNodeDesc = exprNodeDesc.getChildren().get(0);\n      }\n\n      if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n        internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      } else {\n        // No column found!\n        // Bail out\n        return false;\n      }\n    }\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    new RowSchema(parentOfRS.getSchema()), parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }",
            " 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 +\n 589 +\n 590 +\n 591 +\n 592 +\n 593 +\n 594 +\n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607 +\n 608 +\n 609 +\n 610 +\n 611 +\n 612 +\n 613 +\n 614 +\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on parition column.\n      String internalColName = null;\n      ExprNodeDesc exprNodeDesc = key;\n      // Find the ExprNodeColumnDesc\n      while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n              (exprNodeDesc.getChildren() != null)) {\n        exprNodeDesc = exprNodeDesc.getChildren().get(0);\n      }\n\n      if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n        internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      } else {\n        // No column found!\n        // Bail out\n        return false;\n      }\n    }\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    new RowSchema(parentOfRS.getSchema()), parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(groupByOpFinal.getOperatorId());\n      rsOp.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(ts.getOperatorId());\n      rsOpFinal.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }"
        ]
    ],
    "48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c": [
        [
            "ExplainTask::outputPlan(Object,PrintStream,boolean,boolean,int,String)",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 -\n 801 -\n 802 -\n 803 -\n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n                  OUTPUT_OPERATORS,\n                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n                      .toArray()));\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }",
            " 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  ",
            "  private JSONObject outputPlan(Object work, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent, String appendToHeader) throws Exception {\n    // Check if work has an explain annotation\n    Annotation note = AnnotationUtils.getAnnotation(work.getClass(), Explain.class);\n\n    String keyJSONObject = null;\n\n    if (note instanceof Explain) {\n      Explain xpl_note = (Explain) note;\n      boolean invokeFlag = false;\n      if (this.work != null && this.work.isUserLevelExplain()) {\n        invokeFlag = Level.USER.in(xpl_note.explainLevels());\n      } else {\n        if (extended) {\n          invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n        } else {\n          invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n        }\n      }\n      if (invokeFlag) {\n        Vectorization vectorization = xpl_note.vectorization();\n        if (this.work != null && this.work.isVectorization()) {\n\n          // The EXPLAIN VECTORIZATION option was specified.\n          final boolean desireOnly = this.work.isVectorizationOnly();\n          final VectorizationDetailLevel desiredVecDetailLevel =\n              this.work.isVectorizationDetailLevel();\n\n          switch (vectorization) {\n          case NON_VECTORIZED:\n            // Display all non-vectorized leaf objects unless ONLY.\n            if (desireOnly) {\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            if (vectorization.rank < desiredVecDetailLevel.rank) {\n              // This detail not desired.\n              invokeFlag = false;\n            }\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            if (desireOnly) {\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // Suppress headers and all objects below.\n                invokeFlag = false;\n              }\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        } else  {\n          // Do not display vectorization objects.\n          switch (vectorization) {\n          case SUMMARY:\n          case OPERATOR:\n          case EXPRESSION:\n          case DETAIL:\n            invokeFlag = false;\n            break;\n          case NON_VECTORIZED:\n            // No action.\n            break;\n          case SUMMARY_PATH:\n          case OPERATOR_PATH:\n            // Always include headers since they contain non-vectorized objects, too.\n            break;\n          default:\n            throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n          }\n        }\n      }\n      if (invokeFlag) {\n        keyJSONObject = xpl_note.displayName();\n        if (out != null) {\n          out.print(indentString(indent));\n          if (appendToHeader != null && !appendToHeader.isEmpty()) {\n            out.println(xpl_note.displayName() + appendToHeader);\n          } else {\n            out.println(xpl_note.displayName());\n          }\n        }\n      }\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n    // If this is an operator then we need to call the plan generation on the\n    // conf and then the children\n    if (work instanceof Operator) {\n      Operator<? extends OperatorDesc> operator =\n        (Operator<? extends OperatorDesc>) work;\n      if (operator.getConf() != null) {\n        String appender = isLogical ? \" (\" + operator.getOperatorId() + \")\" : \"\";\n        JSONObject jsonOut = outputPlan(operator.getConf(), out, extended,\n            jsonOutput, jsonOutput ? 0 : indent, appender);\n        if (this.work != null && (this.work.isUserLevelExplain() || this.work.isFormatted())) {\n          if (jsonOut != null && jsonOut.length() > 0) {\n            ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\"OperatorId:\",\n                operator.getOperatorId());\n            if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                && operator instanceof ReduceSinkOperator) {\n              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n              if (outputOperators != null) {\n                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n                    Arrays.toString(outputOperators.toArray()));\n              }\n            }\n          }\n        }\n        if (jsonOutput) {\n            json = jsonOut;\n        }\n      }\n\n      if (!visitedOps.contains(operator) || !isLogical) {\n        visitedOps.add(operator);\n        if (operator.getChildOperators() != null) {\n          int cindent = jsonOutput ? 0 : indent + 2;\n          for (Operator<? extends OperatorDesc> op : operator.getChildOperators()) {\n            JSONObject jsonOut = outputPlan(op, out, extended, jsonOutput, cindent);\n            if (jsonOutput) {\n              ((JSONObject)json.get(JSONObject.getNames(json)[0])).accumulate(\"children\", jsonOut);\n            }\n          }\n        }\n      }\n\n      if (jsonOutput) {\n        return json;\n      }\n      return null;\n    }\n\n    // We look at all methods that generate values for explain\n    Method[] methods = work.getClass().getMethods();\n    Arrays.sort(methods, new MethodComparator());\n\n    for (Method m : methods) {\n      int prop_indents = jsonOutput ? 0 : indent + 2;\n      note = AnnotationUtils.getAnnotation(m, Explain.class);\n\n      if (note instanceof Explain) {\n        Explain xpl_note = (Explain) note;\n        boolean invokeFlag = false;\n        if (this.work != null && this.work.isUserLevelExplain()) {\n          invokeFlag = Level.USER.in(xpl_note.explainLevels());\n        } else {\n          if (extended) {\n            invokeFlag = Level.EXTENDED.in(xpl_note.explainLevels());\n          } else {\n            invokeFlag = Level.DEFAULT.in(xpl_note.explainLevels());\n          }\n        }\n        if (invokeFlag) {\n          Vectorization vectorization = xpl_note.vectorization();\n          if (this.work != null && this.work.isVectorization()) {\n\n            // The EXPLAIN VECTORIZATION option was specified.\n            final boolean desireOnly = this.work.isVectorizationOnly();\n            final VectorizationDetailLevel desiredVecDetailLevel =\n                this.work.isVectorizationDetailLevel();\n\n            switch (vectorization) {\n            case NON_VECTORIZED:\n              // Display all non-vectorized leaf objects unless ONLY.\n              if (desireOnly) {\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              if (vectorization.rank < desiredVecDetailLevel.rank) {\n                // This detail not desired.\n                invokeFlag = false;\n              }\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              if (desireOnly) {\n                if (vectorization.rank < desiredVecDetailLevel.rank) {\n                  // Suppress headers and all objects below.\n                  invokeFlag = false;\n                }\n              }\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          } else  {\n            // Do not display vectorization objects.\n            switch (vectorization) {\n            case SUMMARY:\n            case OPERATOR:\n            case EXPRESSION:\n            case DETAIL:\n              invokeFlag = false;\n              break;\n            case NON_VECTORIZED:\n              // No action.\n              break;\n            case SUMMARY_PATH:\n            case OPERATOR_PATH:\n              // Always include headers since they contain non-vectorized objects, too.\n              break;\n            default:\n              throw new RuntimeException(\"Unknown EXPLAIN vectorization \" + vectorization);\n            }\n          }\n        }\n        if (invokeFlag) {\n\n          Object val = null;\n          try {\n            val = m.invoke(work);\n          }\n          catch (InvocationTargetException ex) {\n            // Ignore the exception, this may be caused by external jars\n            val = null;\n          }\n\n          if (val == null) {\n            continue;\n          }\n\n          String header = null;\n          boolean skipHeader = xpl_note.skipHeader();\n          boolean emptyHeader = false;\n\n          if (!xpl_note.displayName().equals(\"\")) {\n            header = indentString(prop_indents) + xpl_note.displayName() + \":\";\n          }\n          else {\n            emptyHeader = true;\n            prop_indents = indent;\n            header = indentString(prop_indents);\n          }\n\n          // Try the output as a primitive object\n          if (isPrintable(val)) {\n            if (out != null && shouldPrint(xpl_note, val)) {\n              if (!skipHeader) {\n                out.print(header);\n                out.print(\" \");\n              }\n              out.println(val);\n            }\n            if (jsonOutput && shouldPrint(xpl_note, val)) {\n              json.put(header, val.toString());\n            }\n            continue;\n          }\n\n          int ind = 0;\n          if (!jsonOutput) {\n            if (!skipHeader) {\n              ind = prop_indents + 2;\n            } else {\n              ind = indent;\n            }\n          }\n\n          // Try this as a map\n          if (val instanceof Map) {\n            // Go through the map and print out the stuff\n            Map<?, ?> mp = (Map<?, ?>) val;\n\n            if (out != null && !skipHeader && mp != null && !mp.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONObject jsonOut = outputMap(mp, !skipHeader && !emptyHeader, out, extended, jsonOutput, ind);\n            if (jsonOutput && !mp.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n            continue;\n          }\n\n          // Try this as a list\n          if (val instanceof List || val instanceof Set) {\n            List l = val instanceof List ? (List)val : new ArrayList((Set)val);\n            if (out != null && !skipHeader && l != null && !l.isEmpty()) {\n              out.print(header);\n            }\n\n            JSONArray jsonOut = outputList(l, out, !skipHeader && !emptyHeader, extended, jsonOutput, ind);\n\n            if (jsonOutput && !l.isEmpty()) {\n              json.put(header, jsonOut);\n            }\n\n            continue;\n          }\n\n          // Finally check if it is serializable\n          try {\n            if (!skipHeader && out != null) {\n              out.println(header);\n            }\n            JSONObject jsonOut = outputPlan(val, out, extended, jsonOutput, ind);\n            if (jsonOutput && jsonOut != null && jsonOut.length() != 0) {\n              if (!skipHeader) {\n                json.put(header, jsonOut);\n              } else {\n                for(String k: JSONObject.getNames(jsonOut)) {\n                  json.put(k, jsonOut.get(k));\n                }\n              }\n            }\n            continue;\n          }\n          catch (ClassCastException ce) {\n            // Ignore\n          }\n        }\n      }\n    }\n\n    if (jsonOutput) {\n      if (keyJSONObject != null) {\n        JSONObject ret = new JSONObject(new LinkedHashMap<>());\n        ret.put(keyJSONObject, json);\n        return ret;\n      }\n\n      return json;\n    }\n    return null;\n  }"
        ]
    ],
    "28a2efd0c9fde800b9220bddad93c8afafb911bf": [
        [
            "Driver::compileInternal(String,boolean)",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318 -\n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345 -\n1346  \n1347  \n1348  \n1349  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }",
            "1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316 +\n1317 +\n1318  \n1319  \n1320 +\n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  ",
            "  private int compileInternal(String command, boolean deferClose) {\n    int ret;\n\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      metrics.incrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    final ReentrantLock compileLock = tryAcquireCompileLock(isParallelEnabled,\n      command);\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.WAIT_COMPILE);\n    if (metrics != null) {\n      metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);\n    }\n\n    if (compileLock == null) {\n      return ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCode();\n    }\n\n    try {\n      ret = compile(command, true, deferClose);\n    } finally {\n      compileLock.unlock();\n    }\n\n    if (ret != 0) {\n      try {\n        releaseLocksAndCommitOrRollback(false, null);\n      } catch (LockException e) {\n        LOG.warn(\"Exception in releasing locks. \"\n            + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    //Save compile-time PerfLogging for WebUI.\n    //Execution-time Perf logs are done by either another thread's PerfLogger\n    //or a reset PerfLogger.\n    queryDisplay.setPerfLogStarts(QueryDisplay.Phase.COMPILATION, perfLogger.getStartTimes());\n    queryDisplay.setPerfLogEnds(QueryDisplay.Phase.COMPILATION, perfLogger.getEndTimes());\n    return ret;\n  }"
        ]
    ],
    "d6db6ffff630bf377b157e9e971d48e1a6d10209": [
        [
            "ReplicationSemanticAnalyzer::analyzeReplDump(ASTNode)",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "  private void analyzeReplDump(ASTNode ast) throws SemanticException {\n    LOG.debug(\"ReplicationSemanticAnalyzer.analyzeReplDump: \" + String.valueOf(dbNameOrPattern)\n        + \".\" + String.valueOf(tblNameOrPattern) + \" from \" + String.valueOf(eventFrom) + \" to \"\n        + String.valueOf(eventTo) + \" maxEventLimit \" + String.valueOf(maxEventLimit));\n    String replRoot = conf.getVar(HiveConf.ConfVars.REPLDIR);\n    Path dumpRoot = new Path(replRoot, getNextDumpDir());\n    DumpMetaData dmd = new DumpMetaData(dumpRoot, conf);\n    Path cmRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLCMDIR));\n    Long lastReplId;\n    try {\n      if (eventFrom == null){\n        // bootstrap case\n        Long bootDumpBeginReplId = db.getMSC().getCurrentNotificationEventId().getEventId();\n        for (String dbName : matchesDb(dbNameOrPattern)) {\n          LOG.debug(\"ReplicationSemanticAnalyzer: analyzeReplDump dumping db: \" + dbName);\n          Path dbRoot = dumpDbMetadata(dbName, dumpRoot);\n          dumpFunctionMetadata(dbName, dumpRoot);\n          for (String tblName : matchesTbl(dbName, tblNameOrPattern)) {\n            LOG.debug(\"ReplicationSemanticAnalyzer: analyzeReplDump dumping table: \" + tblName\n                + \" to db root \" + dbRoot.toUri());\n            dumpTbl(ast, dbName, tblName, dbRoot);\n          }\n        }\n        Long bootDumpEndReplId = db.getMSC().getCurrentNotificationEventId().getEventId();\n        LOG.info(\"Bootstrap object dump phase took from {} to {}\", bootDumpBeginReplId, bootDumpEndReplId);\n\n        // Now that bootstrap has dumped all objects related, we have to account for the changes\n        // that occurred while bootstrap was happening - i.e. we have to look through all events\n        // during the bootstrap period and consolidate them with our dump.\n\n        IMetaStoreClient.NotificationFilter evFilter =\n            new DatabaseAndTableFilter(dbNameOrPattern, tblNameOrPattern);\n        EventUtils.MSClientNotificationFetcher evFetcher =\n            new EventUtils.MSClientNotificationFetcher(db.getMSC());\n        EventUtils.NotificationEventIterator evIter = new EventUtils.NotificationEventIterator(\n            evFetcher, bootDumpBeginReplId,\n            Ints.checkedCast(bootDumpEndReplId - bootDumpBeginReplId) + 1,\n            evFilter );\n\n        // Now we consolidate all the events that happenned during the objdump into the objdump\n        while (evIter.hasNext()){\n          NotificationEvent ev = evIter.next();\n          Path evRoot = new Path(dumpRoot, String.valueOf(ev.getEventId()));\n          // FIXME : implement consolidateEvent(..) similar to dumpEvent(ev,evRoot)\n        }\n        LOG.info(\n            \"Consolidation done, preparing to return {},{}->{}\",\n            dumpRoot.toUri(), bootDumpBeginReplId, bootDumpEndReplId);\n        dmd.setDump(DumpType.BOOTSTRAP, bootDumpBeginReplId, bootDumpEndReplId, cmRoot);\n        dmd.write();\n\n        // Set the correct last repl id to return to the user\n        lastReplId = bootDumpEndReplId;\n      } else {\n        // get list of events matching dbPattern & tblPattern\n        // go through each event, and dump out each event to a event-level dump dir inside dumproot\n        if (eventTo == null){\n          eventTo = db.getMSC().getCurrentNotificationEventId().getEventId();\n          LOG.debug(\"eventTo not specified, using current event id : {}\", eventTo);\n        } else if (eventTo < eventFrom) {\n          throw new Exception(\"Invalid event ID input received in TO clause\");\n        }\n\n        Integer maxRange = Ints.checkedCast(eventTo - eventFrom + 1);\n        if ((maxEventLimit == null) || (maxEventLimit > maxRange)){\n          maxEventLimit = maxRange;\n        }\n\n        // TODO : instead of simply restricting by message format, we should eventually\n        // move to a jdbc-driver-stype registering of message format, and picking message\n        // factory per event to decode. For now, however, since all messages have the\n        // same factory, restricting by message format is effectively a guard against\n        // older leftover data that would cause us problems.\n\n        IMetaStoreClient.NotificationFilter evFilter = new AndFilter(\n            new DatabaseAndTableFilter(dbNameOrPattern, tblNameOrPattern),\n            new EventBoundaryFilter(eventFrom, eventTo),\n            new MessageFormatFilter(MessageFactory.getInstance().getMessageFormat()));\n\n        EventUtils.MSClientNotificationFetcher evFetcher\n            = new EventUtils.MSClientNotificationFetcher(db.getMSC());\n\n        EventUtils.NotificationEventIterator evIter = new EventUtils.NotificationEventIterator(\n            evFetcher, eventFrom, maxEventLimit, evFilter);\n\n        lastReplId = eventTo;\n        while (evIter.hasNext()){\n          NotificationEvent ev = evIter.next();\n          lastReplId = ev.getEventId();\n          Path evRoot = new Path(dumpRoot, String.valueOf(lastReplId));\n          dumpEvent(ev, evRoot, cmRoot);\n        }\n\n        LOG.info(\"Done dumping events, preparing to return {},{}\", dumpRoot.toUri(), lastReplId);\n        Utils.writeOutput(\n            Arrays.asList(\n                \"incremental\",\n                String.valueOf(eventFrom),\n                String.valueOf(lastReplId)\n            ),\n            dmd.getDumpFilePath(), conf);\n        dmd.setDump(DumpType.INCREMENTAL, eventFrom, lastReplId, cmRoot);\n        dmd.write();\n      }\n      prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);\n      setFetchTask(createFetchTask(dumpSchema));\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      LOG.warn(\"Error during analyzeReplDump\", e);\n      throw new SemanticException(e);\n    }\n  }",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202 +\n 203 +\n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269 +\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 +\n 278 +\n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  ",
            "  private void analyzeReplDump(ASTNode ast) throws SemanticException {\n    LOG.debug(\"ReplicationSemanticAnalyzer.analyzeReplDump: \" + String.valueOf(dbNameOrPattern)\n        + \".\" + String.valueOf(tblNameOrPattern) + \" from \" + String.valueOf(eventFrom) + \" to \"\n        + String.valueOf(eventTo) + \" maxEventLimit \" + String.valueOf(maxEventLimit));\n    String replRoot = conf.getVar(HiveConf.ConfVars.REPLDIR);\n    Path dumpRoot = new Path(replRoot, getNextDumpDir());\n    DumpMetaData dmd = new DumpMetaData(dumpRoot, conf);\n    Path cmRoot = new Path(conf.getVar(HiveConf.ConfVars.REPLCMDIR));\n    Long lastReplId;\n    try {\n      if (eventFrom == null){\n        // bootstrap case\n        Long bootDumpBeginReplId = db.getMSC().getCurrentNotificationEventId().getEventId();\n        for (String dbName : matchesDb(dbNameOrPattern)) {\n          REPL_STATE_LOG.info(\"Repl Dump: Started analyzing Repl Dump for DB: {}, Dump Type: BOOTSTRAP\", dbName);\n          LOG.debug(\"ReplicationSemanticAnalyzer: analyzeReplDump dumping db: \" + dbName);\n          Path dbRoot = dumpDbMetadata(dbName, dumpRoot);\n          dumpFunctionMetadata(dbName, dumpRoot);\n          for (String tblName : matchesTbl(dbName, tblNameOrPattern)) {\n            LOG.debug(\"ReplicationSemanticAnalyzer: analyzeReplDump dumping table: \" + tblName\n                + \" to db root \" + dbRoot.toUri());\n            dumpTbl(ast, dbName, tblName, dbRoot);\n          }\n          REPL_STATE_LOG.info(\"Repl Dump: Completed analyzing Repl Dump for DB: {} and created {} COPY tasks to dump \" +\n                              \"metadata and data\",\n                              dbName, rootTasks.size());\n        }\n        Long bootDumpEndReplId = db.getMSC().getCurrentNotificationEventId().getEventId();\n        LOG.info(\"Bootstrap object dump phase took from {} to {}\", bootDumpBeginReplId, bootDumpEndReplId);\n\n        // Now that bootstrap has dumped all objects related, we have to account for the changes\n        // that occurred while bootstrap was happening - i.e. we have to look through all events\n        // during the bootstrap period and consolidate them with our dump.\n\n        IMetaStoreClient.NotificationFilter evFilter =\n            new DatabaseAndTableFilter(dbNameOrPattern, tblNameOrPattern);\n        EventUtils.MSClientNotificationFetcher evFetcher =\n            new EventUtils.MSClientNotificationFetcher(db.getMSC());\n        EventUtils.NotificationEventIterator evIter = new EventUtils.NotificationEventIterator(\n            evFetcher, bootDumpBeginReplId,\n            Ints.checkedCast(bootDumpEndReplId - bootDumpBeginReplId) + 1,\n            evFilter );\n\n        // Now we consolidate all the events that happenned during the objdump into the objdump\n        while (evIter.hasNext()){\n          NotificationEvent ev = evIter.next();\n          Path evRoot = new Path(dumpRoot, String.valueOf(ev.getEventId()));\n          // FIXME : implement consolidateEvent(..) similar to dumpEvent(ev,evRoot)\n        }\n        LOG.info(\n            \"Consolidation done, preparing to return {},{}->{}\",\n            dumpRoot.toUri(), bootDumpBeginReplId, bootDumpEndReplId);\n        dmd.setDump(DumpType.BOOTSTRAP, bootDumpBeginReplId, bootDumpEndReplId, cmRoot);\n        dmd.write();\n\n        // Set the correct last repl id to return to the user\n        lastReplId = bootDumpEndReplId;\n      } else {\n        // get list of events matching dbPattern & tblPattern\n        // go through each event, and dump out each event to a event-level dump dir inside dumproot\n        if (eventTo == null){\n          eventTo = db.getMSC().getCurrentNotificationEventId().getEventId();\n          LOG.debug(\"eventTo not specified, using current event id : {}\", eventTo);\n        } else if (eventTo < eventFrom) {\n          throw new Exception(\"Invalid event ID input received in TO clause\");\n        }\n\n        Integer maxRange = Ints.checkedCast(eventTo - eventFrom + 1);\n        if ((maxEventLimit == null) || (maxEventLimit > maxRange)){\n          maxEventLimit = maxRange;\n        }\n\n        // TODO : instead of simply restricting by message format, we should eventually\n        // move to a jdbc-driver-stype registering of message format, and picking message\n        // factory per event to decode. For now, however, since all messages have the\n        // same factory, restricting by message format is effectively a guard against\n        // older leftover data that would cause us problems.\n\n        IMetaStoreClient.NotificationFilter evFilter = new AndFilter(\n            new DatabaseAndTableFilter(dbNameOrPattern, tblNameOrPattern),\n            new EventBoundaryFilter(eventFrom, eventTo),\n            new MessageFormatFilter(MessageFactory.getInstance().getMessageFormat()));\n\n        EventUtils.MSClientNotificationFetcher evFetcher\n            = new EventUtils.MSClientNotificationFetcher(db.getMSC());\n\n        EventUtils.NotificationEventIterator evIter = new EventUtils.NotificationEventIterator(\n            evFetcher, eventFrom, maxEventLimit, evFilter);\n\n        lastReplId = eventTo;\n        REPL_STATE_LOG.info(\"Repl Dump: Started Repl Dump for DB: {}, Dump Type: INCREMENTAL\",\n                            (null != dbNameOrPattern && !dbNameOrPattern.isEmpty()) ? dbNameOrPattern : \"?\");\n        while (evIter.hasNext()){\n          NotificationEvent ev = evIter.next();\n          lastReplId = ev.getEventId();\n          Path evRoot = new Path(dumpRoot, String.valueOf(lastReplId));\n          dumpEvent(ev, evRoot, cmRoot);\n        }\n\n        REPL_STATE_LOG.info(\"Repl Dump: Completed Repl Dump for DB: {}\",\n                            (null != dbNameOrPattern && !dbNameOrPattern.isEmpty()) ? dbNameOrPattern : \"?\");\n\n        LOG.info(\"Done dumping events, preparing to return {},{}\", dumpRoot.toUri(), lastReplId);\n        Utils.writeOutput(\n            Arrays.asList(\n                \"incremental\",\n                String.valueOf(eventFrom),\n                String.valueOf(lastReplId)\n            ),\n            dmd.getDumpFilePath(), conf);\n        dmd.setDump(DumpType.INCREMENTAL, eventFrom, lastReplId, cmRoot);\n        dmd.write();\n      }\n      prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);\n      setFetchTask(createFetchTask(dumpSchema));\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      LOG.warn(\"Error during analyzeReplDump\", e);\n      throw new SemanticException(e);\n    }\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::dumpTbl(ASTNode,String,String,Path)",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  ",
            "  /**\n   *\n   * @param ast\n   * @param dbName\n   * @param tblName\n   * @param dbRoot\n   * @return tbl dumped path\n   * @throws SemanticException\n   */\n  private Path dumpTbl(ASTNode ast, String dbName, String tblName, Path dbRoot) throws SemanticException {\n    Path tableRoot = new Path(dbRoot, tblName);\n    try {\n      URI toURI = EximUtil.getValidatedURI(conf, tableRoot.toUri().toString());\n      TableSpec ts = new TableSpec(db, conf, dbName + \".\" + tblName, null);\n      ExportSemanticAnalyzer.prepareExport(ast, toURI, ts, getNewReplicationSpec(), db, conf, ctx,\n          rootTasks, inputs, outputs, LOG);\n    } catch (HiveException e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n    return tableRoot;\n  }",
            " 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 +\n 408  \n 409  \n 410 +\n 411 +\n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  /**\n   *\n   * @param ast\n   * @param dbName\n   * @param tblName\n   * @param dbRoot\n   * @return tbl dumped path\n   * @throws SemanticException\n   */\n  private Path dumpTbl(ASTNode ast, String dbName, String tblName, Path dbRoot) throws SemanticException {\n    Path tableRoot = new Path(dbRoot, tblName);\n    try {\n      URI toURI = EximUtil.getValidatedURI(conf, tableRoot.toUri().toString());\n      TableSpec ts = new TableSpec(db, conf, dbName + \".\" + tblName, null);\n\n      ExportSemanticAnalyzer.prepareExport(ast, toURI, ts, getNewReplicationSpec(), db, conf, ctx,\n          rootTasks, inputs, outputs, LOG);\n      REPL_STATE_LOG.info(\"Repl Dump: Analyzed dump for table/view: {}.{} and created copy tasks to dump metadata \" +\n                          \"and data to path {}\", dbName, tblName, toURI.toString());\n    } catch (HiveException e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n    return tableRoot;\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::dumpFunctionMetadata(String,Path)",
            " 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  ",
            "  private void dumpFunctionMetadata(String dbName, Path dumpRoot) throws SemanticException {\n    Path functionsRoot = new Path(new Path(dumpRoot, dbName), FUNCTIONS_ROOT_DIR_NAME);\n    try {\n      // TODO : This should ideally return the Function Objects and not Strings(function names) that should be done by the caller, Look at this separately.\n      List<String> functionNames = db.getFunctions(dbName, \"*\");\n      for (String functionName : functionNames) {\n        HiveWrapper.Tuple<Function> tuple;\n        try {\n          tuple = new HiveWrapper(db, dbName).function(functionName);\n        } catch (HiveException e) {\n          //This can happen as we are querying the getFunctions before we are getting the actual function\n          //in between there can be a drop function by a user in which case our call will fail.\n          LOG.info(\"Function \" + functionName + \" could not be found, we are ignoring it as it can be a valid state \", e);\n          continue;\n        }\n        if (tuple.object.getResourceUris().isEmpty()) {\n          SESSION_STATE_LOG.warn(\n              \"Not replicating function: \" + functionName + \" as it seems to have been created \"\n                  + \"without USING clause\");\n          continue;\n        }\n\n        Path functionMetadataRoot =\n            new Path(new Path(functionsRoot, functionName), FUNCTION_METADATA_DIR_NAME);\n        try (JsonWriter jsonWriter = new JsonWriter(functionMetadataRoot.getFileSystem(conf),\n            functionMetadataRoot)) {\n          new FunctionSerializer(tuple.object).writeTo(jsonWriter, tuple.replicationSpec);\n        }\n      }\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 +\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386 +\n 387  \n 388  \n 389  \n 390  \n 391  ",
            "  private void dumpFunctionMetadata(String dbName, Path dumpRoot) throws SemanticException {\n    Path functionsRoot = new Path(new Path(dumpRoot, dbName), FUNCTIONS_ROOT_DIR_NAME);\n    try {\n      // TODO : This should ideally return the Function Objects and not Strings(function names) that should be done by the caller, Look at this separately.\n      List<String> functionNames = db.getFunctions(dbName, \"*\");\n      for (String functionName : functionNames) {\n        HiveWrapper.Tuple<Function> tuple;\n        try {\n          tuple = new HiveWrapper(db, dbName).function(functionName);\n        } catch (HiveException e) {\n          //This can happen as we are querying the getFunctions before we are getting the actual function\n          //in between there can be a drop function by a user in which case our call will fail.\n          LOG.info(\"Function \" + functionName + \" could not be found, we are ignoring it as it can be a valid state \", e);\n          continue;\n        }\n        if (tuple.object.getResourceUris().isEmpty()) {\n          REPL_STATE_LOG.warn(\n              \"Not replicating function: \" + functionName + \" as it seems to have been created \"\n                  + \"without USING clause\");\n          continue;\n        }\n\n        Path functionMetadataRoot =\n            new Path(new Path(functionsRoot, functionName), FUNCTION_METADATA_DIR_NAME);\n        try (JsonWriter jsonWriter = new JsonWriter(functionMetadataRoot.getFileSystem(conf),\n            functionMetadataRoot)) {\n          new FunctionSerializer(tuple.object).writeTo(jsonWriter, tuple.replicationSpec);\n        }\n        REPL_STATE_LOG.info(\"Repl Dump: Dumped metadata for function: {}\", functionName);\n      }\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::dumpDbMetadata(String,Path)",
            " 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  ",
            "  /**\n   *\n   * @param dbName\n   * @param dumpRoot\n   * @return db dumped path\n   * @throws SemanticException\n   */\n  private Path dumpDbMetadata(String dbName, Path dumpRoot) throws SemanticException {\n    Path dbRoot = new Path(dumpRoot, dbName);\n    try {\n      // TODO : instantiating FS objects are generally costly. Refactor\n      FileSystem fs = dbRoot.getFileSystem(conf);\n      Path dumpPath = new Path(dbRoot, EximUtil.METADATA_NAME);\n      HiveWrapper.Tuple<Database> database = new HiveWrapper(db, dbName).database();\n      EximUtil.createDbExportDump(fs, dumpPath, database.object, database.replicationSpec);\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n    return dbRoot;\n  }",
            " 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 +\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "  /**\n   *\n   * @param dbName\n   * @param dumpRoot\n   * @return db dumped path\n   * @throws SemanticException\n   */\n  private Path dumpDbMetadata(String dbName, Path dumpRoot) throws SemanticException {\n    Path dbRoot = new Path(dumpRoot, dbName);\n    try {\n      // TODO : instantiating FS objects are generally costly. Refactor\n      FileSystem fs = dbRoot.getFileSystem(conf);\n      Path dumpPath = new Path(dbRoot, EximUtil.METADATA_NAME);\n      HiveWrapper.Tuple<Database> database = new HiveWrapper(db, dbName).database();\n      EximUtil.createDbExportDump(fs, dumpPath, database.object, database.replicationSpec);\n      REPL_STATE_LOG.info(\"Repl Dump: Dumped DB metadata\");\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n    return dbRoot;\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::analyzeReplLoad(ASTNode)",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  ",
            "  private void analyzeReplLoad(ASTNode ast) throws SemanticException {\n    LOG.debug(\"ReplSemanticAnalyzer.analyzeReplLoad: \" + String.valueOf(dbNameOrPattern) + \".\"\n        + String.valueOf(tblNameOrPattern) + \" from \" + String.valueOf(path));\n\n    // for analyze repl load, we walk through the dir structure available in the path,\n    // looking at each db, and then each table, and then setting up the appropriate\n    // import job in its place.\n\n    try {\n\n      Path loadPath = new Path(path);\n      final FileSystem fs = loadPath.getFileSystem(conf);\n\n      if (!fs.exists(loadPath)) {\n        // supposed dump path does not exist.\n        throw new FileNotFoundException(loadPath.toUri().toString());\n      }\n\n      // Now, the dumped path can be one of three things:\n      // a) It can be a db dump, in which case we expect a set of dirs, each with a\n      // db name, and with a _metadata file in each, and table dirs inside that.\n      // b) It can be a table dump dir, in which case we expect a _metadata dump of\n      // a table in question in the dir, and individual ptn dir hierarchy.\n      // c) A dump can be an incremental dump, which means we have several subdirs\n      // each of which have the evid as the dir name, and each of which correspond\n      // to a event-level dump. Currently, only CREATE_TABLE and ADD_PARTITION are\n      // handled, so all of these dumps will be at a table/ptn level.\n\n      // For incremental repl, we will have individual events which can\n      // be other things like roles and fns as well.\n      // At this point, all dump dirs should contain a _dumpmetadata file that\n      // tells us what is inside that dumpdir.\n\n      DumpMetaData dmd = new DumpMetaData(loadPath, conf);\n\n      boolean evDump = false;\n      if (dmd.isIncrementalDump()){\n        LOG.debug(\"{} contains an incremental dump\", loadPath);\n        evDump = true;\n      } else {\n        LOG.debug(\"{} contains an bootstrap dump\", loadPath);\n      }\n\n      if ((!evDump) && (tblNameOrPattern != null) && !(tblNameOrPattern.isEmpty())) {\n        // not an event dump, and table name pattern specified, this has to be a tbl-level dump\n        rootTasks.addAll(analyzeTableLoad(dbNameOrPattern, tblNameOrPattern, path, null, null, null));\n        return;\n      }\n\n      FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(fs, loadPath);\n      if (srcs == null || (srcs.length == 0)) {\n        LOG.warn(\"Nothing to load at {}\", loadPath.toUri().toString());\n        return;\n      }\n\n      FileStatus[] dirsInLoadPath = fs.listStatus(loadPath, EximUtil.getDirectoryFilter(fs));\n\n      if ((dirsInLoadPath == null) || (dirsInLoadPath.length == 0)) {\n        throw new IllegalArgumentException(\"No data to load in path \" + loadPath.toUri().toString());\n      }\n\n      if (!evDump){\n        // not an event dump, not a table dump - thus, a db dump\n        if ((dbNameOrPattern != null) && (dirsInLoadPath.length > 1)) {\n          LOG.debug(\"Found multiple dirs when we expected 1:\");\n          for (FileStatus d : dirsInLoadPath) {\n            LOG.debug(\"> \" + d.getPath().toUri().toString());\n          }\n          throw new IllegalArgumentException(\n              \"Multiple dirs in \"\n                  + loadPath.toUri().toString()\n                  + \" does not correspond to REPL LOAD expecting to load to a singular destination point.\");\n        }\n\n        for (FileStatus dir : dirsInLoadPath) {\n          analyzeDatabaseLoad(dbNameOrPattern, fs, dir);\n        }\n      } else {\n        // event dump, each subdir is an individual event dump.\n        Arrays.sort(dirsInLoadPath); // we need to guarantee that the directory listing we got is in order of evid.\n\n        Task<? extends Serializable> evTaskRoot = TaskFactory.get(new DependencyCollectionWork(), conf);\n        Task<? extends Serializable> taskChainTail = evTaskRoot;\n\n        int evstage = 0;\n        Long lastEvid = null;\n        Map<String,Long> dbsUpdated = new ReplicationSpec.ReplStateMap<String,Long>();\n        Map<String,Long> tablesUpdated = new ReplicationSpec.ReplStateMap<String,Long>();\n\n        for (FileStatus dir : dirsInLoadPath){\n          LOG.debug(\"Loading event from {} to {}.{}\", dir.getPath().toUri(), dbNameOrPattern, tblNameOrPattern);\n          // event loads will behave similar to table loads, with one crucial difference\n          // precursor order is strict, and each event must be processed after the previous one.\n          // The way we handle this strict order is as follows:\n          // First, we start with a taskChainTail which is a dummy noop task (a DependecyCollectionTask)\n          // at the head of our event chain. For each event we process, we tell analyzeTableLoad to\n          // create tasks that use the taskChainTail as a dependency. Then, we collect all those tasks\n          // and introduce a new barrier task(also a DependencyCollectionTask) which depends on all\n          // these tasks. Then, this barrier task becomes our new taskChainTail. Thus, we get a set of\n          // tasks as follows:\n          //\n          //                 --->ev1.task1--                          --->ev2.task1--\n          //                /               \\                        /               \\\n          //  evTaskRoot-->*---->ev1.task2---*--> ev1.barrierTask-->*---->ev2.task2---*->evTaskChainTail\n          //                \\               /\n          //                 --->ev1.task3--\n          //\n          // Once this entire chain is generated, we add evTaskRoot to rootTasks, so as to execute the\n          // entire chain\n\n          String locn = dir.getPath().toUri().toString();\n          DumpMetaData eventDmd = new DumpMetaData(new Path(locn), conf);\n          List<Task<? extends Serializable>> evTasks = analyzeEventLoad(\n              dbNameOrPattern, tblNameOrPattern, locn, taskChainTail,\n              dbsUpdated, tablesUpdated, eventDmd);\n          LOG.debug(\"evstage#{} got {} tasks\", evstage, evTasks!=null ? evTasks.size() : 0);\n          if ((evTasks != null) && (!evTasks.isEmpty())){\n            Task<? extends Serializable> barrierTask = TaskFactory.get(new DependencyCollectionWork(), conf);\n            for (Task<? extends Serializable> t : evTasks){\n              t.addDependentTask(barrierTask);\n              LOG.debug(\"Added {}:{} as a precursor of barrier task {}:{}\",\n                  t.getClass(), t.getId(), barrierTask.getClass(), barrierTask.getId());\n            }\n            LOG.debug(\"Updated taskChainTail from {}{} to {}{}\",\n                taskChainTail.getClass(), taskChainTail.getId(), barrierTask.getClass(), barrierTask.getId());\n            taskChainTail = barrierTask;\n            evstage++;\n            lastEvid = dmd.getEventTo();\n          }\n        }\n\n        // Now, we need to update repl.last.id for the various parent objects that were updated.\n        // This update logic will work differently based on what \"level\" REPL LOAD was run on.\n        //  a) If this was a REPL LOAD at a table level, i.e. both dbNameOrPattern and\n        //     tblNameOrPattern were specified, then the table is the only thing we should\n        //     update the repl.last.id for.\n        //  b) If this was a db-level REPL LOAD, then we should update the db, as well as any\n        //     tables affected by partition level operations. (any table level ops will\n        //     automatically be updated as the table gets updated. Note - renames will need\n        //     careful handling.\n        //  c) If this was a wh-level REPL LOAD, then we should update every db for which there\n        //     were events occurring, as well as tables for which there were ptn-level ops\n        //     happened. Again, renames must be taken care of.\n        //\n        // So, what we're going to do is have each event load update dbsUpdated and tablesUpdated\n        // accordingly, but ignore updates to tablesUpdated & dbsUpdated in the case of a\n        // table-level REPL LOAD, using only the table itself. In the case of a db-level REPL\n        // LOAD, we ignore dbsUpdated, but inject our own, and do not ignore tblsUpdated.\n        // And for wh-level, we do no special processing, and use all of dbsUpdated and\n        // tblsUpdated as-is.\n\n        // Additional Note - although this var says \"dbNameOrPattern\", on REPL LOAD side,\n        // we do not support a pattern It can be null or empty, in which case\n        // we re-use the existing name from the dump, or it can be specified,\n        // in which case we honour it. However, having this be a pattern is an error.\n        // Ditto for tblNameOrPattern.\n\n\n        if (evstage > 0){\n          if ((tblNameOrPattern != null) && (!tblNameOrPattern.isEmpty())){\n            // if tblNameOrPattern is specified, then dbNameOrPattern will be too, and\n            // thus, this is a table-level REPL LOAD - only table needs updating.\n            // If any of the individual events logged any other dbs as having changed,\n            // null them out.\n            dbsUpdated.clear();\n            tablesUpdated.clear();\n            tablesUpdated.put(dbNameOrPattern + \".\" + tblNameOrPattern, lastEvid);\n          } else  if ((dbNameOrPattern != null) && (!dbNameOrPattern.isEmpty())){\n            // if dbNameOrPattern is specified and tblNameOrPattern isn't, this is a\n            // db-level update, and thus, the database needs updating. In addition.\n            dbsUpdated.clear();\n            dbsUpdated.put(dbNameOrPattern, lastEvid);\n          }\n        }\n\n        for (String tableName : tablesUpdated.keySet()){\n          // weird - AlterTableDesc requires a HashMap to update props instead of a Map.\n          HashMap<String,String> mapProp = new HashMap<String,String>();\n          mapProp.put(\n              ReplicationSpec.KEY.CURR_STATE_ID.toString(),\n              tablesUpdated.get(tableName).toString());\n          AlterTableDesc alterTblDesc =  new AlterTableDesc(\n              AlterTableDesc.AlterTableTypes.ADDPROPS, null, false);\n          alterTblDesc.setProps(mapProp);\n          alterTblDesc.setOldName(tableName);\n          Task<? extends Serializable> updateReplIdTask = TaskFactory.get(\n              new DDLWork(inputs, outputs, alterTblDesc), conf);\n          taskChainTail.addDependentTask(updateReplIdTask);\n          taskChainTail = updateReplIdTask;\n        }\n        for (String dbName : dbsUpdated.keySet()){\n          Map<String,String> mapProp = new HashMap<String,String>();\n          mapProp.put(\n              ReplicationSpec.KEY.CURR_STATE_ID.toString(),\n              dbsUpdated.get(dbName).toString());\n          AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc(dbName, mapProp);\n          Task<? extends Serializable> updateReplIdTask = TaskFactory.get(\n              new DDLWork(inputs, outputs, alterDbDesc), conf);\n          taskChainTail.addDependentTask(updateReplIdTask);\n          taskChainTail = updateReplIdTask;\n        }\n        rootTasks.add(evTaskRoot);\n      }\n\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n\n  }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558 +\n 559  \n 560  \n 561  \n 562  \n 563 +\n 564 +\n 565 +\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592 +\n 593 +\n 594 +\n 595 +\n 596 +\n 597 +\n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685 +\n 686 +\n 687 +\n 688 +\n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  ",
            "  private void analyzeReplLoad(ASTNode ast) throws SemanticException {\n    LOG.debug(\"ReplSemanticAnalyzer.analyzeReplLoad: \" + String.valueOf(dbNameOrPattern) + \".\"\n        + String.valueOf(tblNameOrPattern) + \" from \" + String.valueOf(path));\n\n    // for analyze repl load, we walk through the dir structure available in the path,\n    // looking at each db, and then each table, and then setting up the appropriate\n    // import job in its place.\n\n    try {\n\n      Path loadPath = new Path(path);\n      final FileSystem fs = loadPath.getFileSystem(conf);\n\n      if (!fs.exists(loadPath)) {\n        // supposed dump path does not exist.\n        throw new FileNotFoundException(loadPath.toUri().toString());\n      }\n\n      // Now, the dumped path can be one of three things:\n      // a) It can be a db dump, in which case we expect a set of dirs, each with a\n      // db name, and with a _metadata file in each, and table dirs inside that.\n      // b) It can be a table dump dir, in which case we expect a _metadata dump of\n      // a table in question in the dir, and individual ptn dir hierarchy.\n      // c) A dump can be an incremental dump, which means we have several subdirs\n      // each of which have the evid as the dir name, and each of which correspond\n      // to a event-level dump. Currently, only CREATE_TABLE and ADD_PARTITION are\n      // handled, so all of these dumps will be at a table/ptn level.\n\n      // For incremental repl, we will have individual events which can\n      // be other things like roles and fns as well.\n      // At this point, all dump dirs should contain a _dumpmetadata file that\n      // tells us what is inside that dumpdir.\n\n      DumpMetaData dmd = new DumpMetaData(loadPath, conf);\n\n      boolean evDump = false;\n      if (dmd.isIncrementalDump()){\n        LOG.debug(\"{} contains an incremental dump\", loadPath);\n        evDump = true;\n      } else {\n        LOG.debug(\"{} contains an bootstrap dump\", loadPath);\n      }\n\n      if ((!evDump) && (tblNameOrPattern != null) && !(tblNameOrPattern.isEmpty())) {\n        // not an event dump, and table name pattern specified, this has to be a tbl-level dump\n        rootTasks.addAll(analyzeTableLoad(dbNameOrPattern, tblNameOrPattern, path, null, null, null));\n        return;\n      }\n\n      FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(fs, loadPath);\n      if (srcs == null || (srcs.length == 0)) {\n        LOG.warn(\"Nothing to load at {}\", loadPath.toUri().toString());\n        return;\n      }\n\n      FileStatus[] dirsInLoadPath = fs.listStatus(loadPath, EximUtil.getDirectoryFilter(fs));\n\n      if ((dirsInLoadPath == null) || (dirsInLoadPath.length == 0)) {\n        throw new IllegalArgumentException(\"No data to load in path \" + loadPath.toUri().toString());\n      }\n\n      if (!evDump){\n        // not an event dump, not a table dump - thus, a db dump\n        if ((dbNameOrPattern != null) && (dirsInLoadPath.length > 1)) {\n          LOG.debug(\"Found multiple dirs when we expected 1:\");\n          for (FileStatus d : dirsInLoadPath) {\n            LOG.debug(\"> \" + d.getPath().toUri().toString());\n          }\n          throw new IllegalArgumentException(\n              \"Multiple dirs in \"\n                  + loadPath.toUri().toString()\n                  + \" does not correspond to REPL LOAD expecting to load to a singular destination point.\");\n        }\n\n        for (FileStatus dir : dirsInLoadPath) {\n          analyzeDatabaseLoad(dbNameOrPattern, fs, dir);\n        }\n      } else {\n        // event dump, each subdir is an individual event dump.\n        Arrays.sort(dirsInLoadPath); // we need to guarantee that the directory listing we got is in order of evid.\n\n        Task<? extends Serializable> evTaskRoot = TaskFactory.get(new DependencyCollectionWork(), conf);\n        Task<? extends Serializable> taskChainTail = evTaskRoot;\n\n        int evstage = 0;\n        int evIter = 0;\n        Long lastEvid = null;\n        Map<String,Long> dbsUpdated = new ReplicationSpec.ReplStateMap<String,Long>();\n        Map<String,Long> tablesUpdated = new ReplicationSpec.ReplStateMap<String,Long>();\n\n        REPL_STATE_LOG.info(\"Repl Load: Started analyzing Repl load for DB: {} from path {}, Dump Type: INCREMENTAL\",\n                (null != dbNameOrPattern && !dbNameOrPattern.isEmpty()) ? dbNameOrPattern : \"?\",\n                loadPath.toUri().toString());\n        for (FileStatus dir : dirsInLoadPath){\n          LOG.debug(\"Loading event from {} to {}.{}\", dir.getPath().toUri(), dbNameOrPattern, tblNameOrPattern);\n          // event loads will behave similar to table loads, with one crucial difference\n          // precursor order is strict, and each event must be processed after the previous one.\n          // The way we handle this strict order is as follows:\n          // First, we start with a taskChainTail which is a dummy noop task (a DependecyCollectionTask)\n          // at the head of our event chain. For each event we process, we tell analyzeTableLoad to\n          // create tasks that use the taskChainTail as a dependency. Then, we collect all those tasks\n          // and introduce a new barrier task(also a DependencyCollectionTask) which depends on all\n          // these tasks. Then, this barrier task becomes our new taskChainTail. Thus, we get a set of\n          // tasks as follows:\n          //\n          //                 --->ev1.task1--                          --->ev2.task1--\n          //                /               \\                        /               \\\n          //  evTaskRoot-->*---->ev1.task2---*--> ev1.barrierTask-->*---->ev2.task2---*->evTaskChainTail\n          //                \\               /\n          //                 --->ev1.task3--\n          //\n          // Once this entire chain is generated, we add evTaskRoot to rootTasks, so as to execute the\n          // entire chain\n\n          String locn = dir.getPath().toUri().toString();\n          DumpMetaData eventDmd = new DumpMetaData(new Path(locn), conf);\n          List<Task<? extends Serializable>> evTasks = analyzeEventLoad(\n              dbNameOrPattern, tblNameOrPattern, locn, taskChainTail,\n              dbsUpdated, tablesUpdated, eventDmd);\n          evIter++;\n          REPL_STATE_LOG.info(\"Repl Load: Analyzed load for event {}/{} \" +\n                              \"with ID: {}, Type: {}, Path: {}\",\n                              evIter, dirsInLoadPath.length,\n                              dir.getPath().getName(), eventDmd.getDumpType().toString(), locn);\n\n          LOG.debug(\"evstage#{} got {} tasks\", evstage, evTasks!=null ? evTasks.size() : 0);\n          if ((evTasks != null) && (!evTasks.isEmpty())){\n            Task<? extends Serializable> barrierTask = TaskFactory.get(new DependencyCollectionWork(), conf);\n            for (Task<? extends Serializable> t : evTasks){\n              t.addDependentTask(barrierTask);\n              LOG.debug(\"Added {}:{} as a precursor of barrier task {}:{}\",\n                  t.getClass(), t.getId(), barrierTask.getClass(), barrierTask.getId());\n            }\n            LOG.debug(\"Updated taskChainTail from {}{} to {}{}\",\n                taskChainTail.getClass(), taskChainTail.getId(), barrierTask.getClass(), barrierTask.getId());\n            taskChainTail = barrierTask;\n            evstage++;\n            lastEvid = dmd.getEventTo();\n          }\n        }\n\n        // Now, we need to update repl.last.id for the various parent objects that were updated.\n        // This update logic will work differently based on what \"level\" REPL LOAD was run on.\n        //  a) If this was a REPL LOAD at a table level, i.e. both dbNameOrPattern and\n        //     tblNameOrPattern were specified, then the table is the only thing we should\n        //     update the repl.last.id for.\n        //  b) If this was a db-level REPL LOAD, then we should update the db, as well as any\n        //     tables affected by partition level operations. (any table level ops will\n        //     automatically be updated as the table gets updated. Note - renames will need\n        //     careful handling.\n        //  c) If this was a wh-level REPL LOAD, then we should update every db for which there\n        //     were events occurring, as well as tables for which there were ptn-level ops\n        //     happened. Again, renames must be taken care of.\n        //\n        // So, what we're going to do is have each event load update dbsUpdated and tablesUpdated\n        // accordingly, but ignore updates to tablesUpdated & dbsUpdated in the case of a\n        // table-level REPL LOAD, using only the table itself. In the case of a db-level REPL\n        // LOAD, we ignore dbsUpdated, but inject our own, and do not ignore tblsUpdated.\n        // And for wh-level, we do no special processing, and use all of dbsUpdated and\n        // tblsUpdated as-is.\n\n        // Additional Note - although this var says \"dbNameOrPattern\", on REPL LOAD side,\n        // we do not support a pattern It can be null or empty, in which case\n        // we re-use the existing name from the dump, or it can be specified,\n        // in which case we honour it. However, having this be a pattern is an error.\n        // Ditto for tblNameOrPattern.\n\n\n        if (evstage > 0){\n          if ((tblNameOrPattern != null) && (!tblNameOrPattern.isEmpty())){\n            // if tblNameOrPattern is specified, then dbNameOrPattern will be too, and\n            // thus, this is a table-level REPL LOAD - only table needs updating.\n            // If any of the individual events logged any other dbs as having changed,\n            // null them out.\n            dbsUpdated.clear();\n            tablesUpdated.clear();\n            tablesUpdated.put(dbNameOrPattern + \".\" + tblNameOrPattern, lastEvid);\n          } else  if ((dbNameOrPattern != null) && (!dbNameOrPattern.isEmpty())){\n            // if dbNameOrPattern is specified and tblNameOrPattern isn't, this is a\n            // db-level update, and thus, the database needs updating. In addition.\n            dbsUpdated.clear();\n            dbsUpdated.put(dbNameOrPattern, lastEvid);\n          }\n        }\n\n        for (String tableName : tablesUpdated.keySet()){\n          // weird - AlterTableDesc requires a HashMap to update props instead of a Map.\n          HashMap<String,String> mapProp = new HashMap<String,String>();\n          mapProp.put(\n              ReplicationSpec.KEY.CURR_STATE_ID.toString(),\n              tablesUpdated.get(tableName).toString());\n          AlterTableDesc alterTblDesc =  new AlterTableDesc(\n              AlterTableDesc.AlterTableTypes.ADDPROPS, null, false);\n          alterTblDesc.setProps(mapProp);\n          alterTblDesc.setOldName(tableName);\n          Task<? extends Serializable> updateReplIdTask = TaskFactory.get(\n              new DDLWork(inputs, outputs, alterTblDesc), conf);\n          taskChainTail.addDependentTask(updateReplIdTask);\n          taskChainTail = updateReplIdTask;\n        }\n        for (String dbName : dbsUpdated.keySet()){\n          Map<String,String> mapProp = new HashMap<String,String>();\n          mapProp.put(\n              ReplicationSpec.KEY.CURR_STATE_ID.toString(),\n              dbsUpdated.get(dbName).toString());\n          AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc(dbName, mapProp);\n          Task<? extends Serializable> updateReplIdTask = TaskFactory.get(\n              new DDLWork(inputs, outputs, alterDbDesc), conf);\n          taskChainTail.addDependentTask(updateReplIdTask);\n          taskChainTail = updateReplIdTask;\n        }\n        rootTasks.add(evTaskRoot);\n        REPL_STATE_LOG.info(\"Repl Load: Completed analyzing Repl load for DB: {} from path {} and created import \" +\n                            \"(DDL/COPY/MOVE) tasks\",\n                            (null != dbNameOrPattern && !dbNameOrPattern.isEmpty()) ? dbNameOrPattern : \"?\",\n                            loadPath.toUri().toString());\n      }\n\n    } catch (Exception e) {\n      // TODO : simple wrap & rethrow for now, clean up with error codes\n      throw new SemanticException(e);\n    }\n\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::dumpEvent(NotificationEvent,Path,Path)",
            " 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "  private void dumpEvent(NotificationEvent ev, Path evRoot, Path cmRoot) throws Exception {\n    EventHandler.Context context = new EventHandler.Context(\n        evRoot,\n        cmRoot,\n        db,\n        conf,\n        getNewEventOnlyReplicationSpec(ev.getEventId())\n    );\n    EventHandlerFactory.handlerFor(ev).handle(context);\n  }",
            " 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 +\n 310 +\n 311  ",
            "  private void dumpEvent(NotificationEvent ev, Path evRoot, Path cmRoot) throws Exception {\n    EventHandler.Context context = new EventHandler.Context(\n        evRoot,\n        cmRoot,\n        db,\n        conf,\n        getNewEventOnlyReplicationSpec(ev.getEventId())\n    );\n    EventHandlerFactory.handlerFor(ev).handle(context);\n    REPL_STATE_LOG.info(\"Repl Dump: Dumped event with ID: {}, Type: {} and dumped metadata and data to path {}\",\n                        String.valueOf(ev.getEventId()), ev.getEventType(), evRoot.toUri().toString());\n  }"
        ],
        [
            "ReplicationSemanticAnalyzer::analyzeDatabaseLoad(String,FileSystem,FileStatus)",
            " 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  ",
            "  private void analyzeDatabaseLoad(String dbName, FileSystem fs, FileStatus dir)\n      throws SemanticException {\n    try {\n      // Path being passed to us is a db dump location. We go ahead and load as needed.\n      // dbName might be null or empty, in which case we keep the original db name for the new\n      // database creation\n\n      // Two steps here - first, we read the _metadata file here, and create a CreateDatabaseDesc\n      // associated with that\n      // Then, we iterate over all subdirs, and create table imports for each.\n\n      MetaData rv = new MetaData();\n      try {\n        rv = EximUtil.readMetaData(fs, new Path(dir.getPath(), EximUtil.METADATA_NAME));\n      } catch (IOException e) {\n        throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);\n      }\n\n      Database dbObj = rv.getDatabase();\n\n      if (dbObj == null) {\n        throw new IllegalArgumentException(\n            \"_metadata file read did not contain a db object - invalid dump.\");\n      }\n\n      if ((dbName == null) || (dbName.isEmpty())) {\n        // We use dbName specified as long as it is not null/empty. If so, then we use the original\n        // name\n        // recorded in the thrift object.\n        dbName = dbObj.getName();\n      }\n\n      Task<? extends Serializable> dbRootTask = null;\n      if (existEmptyDb(dbName)) {\n        AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc(dbName, dbObj.getParameters());\n        dbRootTask = TaskFactory.get(new DDLWork(inputs, outputs, alterDbDesc), conf);\n      } else {\n        CreateDatabaseDesc createDbDesc = new CreateDatabaseDesc();\n        createDbDesc.setName(dbName);\n        createDbDesc.setComment(dbObj.getDescription());\n        createDbDesc.setDatabaseProperties(dbObj.getParameters());\n        // note that we do not set location - for repl load, we want that auto-created.\n\n        createDbDesc.setIfNotExists(false);\n        // If it exists, we want this to be an error condition. Repl Load is not intended to replace a\n        // db.\n        // TODO: we might revisit this in create-drop-recreate cases, needs some thinking on.\n        dbRootTask = TaskFactory.get(new DDLWork(inputs, outputs, createDbDesc), conf);\n      }\n\n      rootTasks.add(dbRootTask);\n      FileStatus[] dirsInDbPath = fs.listStatus(dir.getPath(), EximUtil.getDirectoryFilter(fs));\n\n      for (FileStatus tableDir : Collections2.filter(Arrays.asList(dirsInDbPath), new TableDirPredicate())) {\n        analyzeTableLoad(\n            dbName, null, tableDir.getPath().toUri().toString(), dbRootTask, null, null);\n      }\n\n      //Function load\n      Path functionMetaDataRoot = new Path(dir.getPath(), FUNCTIONS_ROOT_DIR_NAME);\n      if (fs.exists(functionMetaDataRoot)) {\n        List<FileStatus> functionDirectories =\n            Arrays.asList(fs.listStatus(functionMetaDataRoot, EximUtil.getDirectoryFilter(fs)));\n        for (FileStatus functionDir : functionDirectories) {\n          analyzeFunctionLoad(dbName, functionDir, dbRootTask);\n        }\n      }\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }",
            " 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774 +\n 775 +\n 776 +\n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801 +\n 802 +\n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812 +\n 813 +\n 814  \n 815  \n 816 +\n 817 +\n 818 +\n 819  \n 820  \n 821  \n 822  ",
            "  private void analyzeDatabaseLoad(String dbName, FileSystem fs, FileStatus dir)\n      throws SemanticException {\n    try {\n      // Path being passed to us is a db dump location. We go ahead and load as needed.\n      // dbName might be null or empty, in which case we keep the original db name for the new\n      // database creation\n\n      // Two steps here - first, we read the _metadata file here, and create a CreateDatabaseDesc\n      // associated with that\n      // Then, we iterate over all subdirs, and create table imports for each.\n\n      MetaData rv = new MetaData();\n      try {\n        rv = EximUtil.readMetaData(fs, new Path(dir.getPath(), EximUtil.METADATA_NAME));\n      } catch (IOException e) {\n        throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);\n      }\n\n      Database dbObj = rv.getDatabase();\n\n      if (dbObj == null) {\n        throw new IllegalArgumentException(\n            \"_metadata file read did not contain a db object - invalid dump.\");\n      }\n\n      if ((dbName == null) || (dbName.isEmpty())) {\n        // We use dbName specified as long as it is not null/empty. If so, then we use the original\n        // name\n        // recorded in the thrift object.\n        dbName = dbObj.getName();\n      }\n\n      REPL_STATE_LOG.info(\"Repl Load: Started analyzing Repl Load for DB: {} from Dump Dir: {}, Dump Type: BOOTSTRAP\",\n                          dbName, dir.getPath().toUri().toString());\n\n      Task<? extends Serializable> dbRootTask = null;\n      if (existEmptyDb(dbName)) {\n        AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc(dbName, dbObj.getParameters());\n        dbRootTask = TaskFactory.get(new DDLWork(inputs, outputs, alterDbDesc), conf);\n      } else {\n        CreateDatabaseDesc createDbDesc = new CreateDatabaseDesc();\n        createDbDesc.setName(dbName);\n        createDbDesc.setComment(dbObj.getDescription());\n        createDbDesc.setDatabaseProperties(dbObj.getParameters());\n        // note that we do not set location - for repl load, we want that auto-created.\n\n        createDbDesc.setIfNotExists(false);\n        // If it exists, we want this to be an error condition. Repl Load is not intended to replace a\n        // db.\n        // TODO: we might revisit this in create-drop-recreate cases, needs some thinking on.\n        dbRootTask = TaskFactory.get(new DDLWork(inputs, outputs, createDbDesc), conf);\n      }\n\n      rootTasks.add(dbRootTask);\n      FileStatus[] dirsInDbPath = fs.listStatus(dir.getPath(), EximUtil.getDirectoryFilter(fs));\n\n      for (FileStatus tableDir : Collections2.filter(Arrays.asList(dirsInDbPath), new TableDirPredicate())) {\n        analyzeTableLoad(\n            dbName, null, tableDir.getPath().toUri().toString(), dbRootTask, null, null);\n        REPL_STATE_LOG.info(\"Repl Load: Analyzed table/view/partition load from path {}\",\n                            tableDir.getPath().toUri().toString());\n      }\n\n      //Function load\n      Path functionMetaDataRoot = new Path(dir.getPath(), FUNCTIONS_ROOT_DIR_NAME);\n      if (fs.exists(functionMetaDataRoot)) {\n        List<FileStatus> functionDirectories =\n            Arrays.asList(fs.listStatus(functionMetaDataRoot, EximUtil.getDirectoryFilter(fs)));\n        for (FileStatus functionDir : functionDirectories) {\n          analyzeFunctionLoad(dbName, functionDir, dbRootTask);\n          REPL_STATE_LOG.info(\"Repl Load: Analyzed function load from path {}\",\n                              functionDir.getPath().toUri().toString());\n        }\n      }\n\n      REPL_STATE_LOG.info(\"Repl Load: Completed analyzing Repl Load for DB: {} and created import (DDL/COPY/MOVE) tasks\",\n              dbName);\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }"
        ]
    ],
    "302360f51c2f6d93db244b6e67fc05517a654b2b": [
        [
            "ObjectStore::shutdown()",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n    }\n  }",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 +\n 566  \n 567  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n      pm = null;\n    }\n  }"
        ],
        [
            "Hive::close()",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }",
            " 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418 +\n 419 +\n 420 +\n 421  \n 422  \n 423  \n 424  ",
            "  /**\n   * closes the connection to metastore for the calling thread\n   */\n  private void close() {\n    LOG.debug(\"Closing current thread's connection to Hive Metastore.\");\n    if (metaStoreClient != null) {\n      metaStoreClient.close();\n      metaStoreClient = null;\n    }\n    if (syncMetaStoreClient != null) {\n      syncMetaStoreClient.close();\n    }\n    if (owner != null) {\n      owner = null;\n    }\n  }"
        ],
        [
            "Hive::loadDynamicPartitions(Path,String,Map,boolean,int,boolean,boolean,long,boolean,AcidUtils)",
            "1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }",
            "1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980 +\n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007 +\n2008 +\n2009 +\n2010 +\n2011 +\n2012 +\n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034 +\n2035 +\n2036 +\n2037 +\n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  ",
            "  /**\n   * Given a source directory name of the load path, load all dynamically generated partitions\n   * into the specified table and return a list of strings that represent the dynamic partition\n   * paths.\n   * @param loadPath\n   * @param tableName\n   * @param partSpec\n   * @param replace\n   * @param numDP number of dynamic partitions\n   * @param listBucketingEnabled\n   * @param isAcid true if this is an ACID operation\n   * @param txnId txnId, can be 0 unless isAcid == true\n   * @return partition map details (PartitionSpec and Partition)\n   * @throws HiveException\n   */\n  public Map<Map<String, String>, Partition> loadDynamicPartitions(final Path loadPath,\n      final String tableName, final Map<String, String> partSpec, final boolean replace,\n      final int numDP, final boolean listBucketingEnabled, final boolean isAcid, final long txnId,\n      final boolean hasFollowingStatsTask, final AcidUtils.Operation operation)\n      throws HiveException {\n\n    final Map<Map<String, String>, Partition> partitionsMap =\n        Collections.synchronizedMap(new LinkedHashMap<Map<String, String>, Partition>());\n\n    int poolSize = conf.getInt(ConfVars.HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT.varname, 1);\n    final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n            new ThreadFactoryBuilder()\n                .setDaemon(true)\n                .setNameFormat(\"load-dynamic-partitions-%d\")\n                .build());\n\n    // Get all valid partition paths and existing partitions for them (if any)\n    final Table tbl = getTable(tableName);\n    final Set<Path> validPartitions = getValidPartitionsInPath(numDP, loadPath);\n\n    final int partsToLoad = validPartitions.size();\n    final AtomicInteger partitionsLoaded = new AtomicInteger(0);\n\n    final boolean inPlaceEligible = conf.getLong(\"fs.trash.interval\", 0) <= 0\n        && InPlaceUpdate.canRenderInPlace(conf) && !SessionState.getConsole().getIsSilent();\n    final PrintStream ps = (inPlaceEligible) ? SessionState.getConsole().getInfoStream() : null;\n    final SessionState parentSession = SessionState.get();\n\n    final List<Future<Void>> futures = Lists.newLinkedList();\n    try {\n      // for each dynamically created DP directory, construct a full partition spec\n      // and load the partition based on that\n      final Map<Long, RawStore> rawStoreMap = new HashMap<Long, RawStore>();\n      for(final Path partPath : validPartitions) {\n        // generate a full partition specification\n        final LinkedHashMap<String, String> fullPartSpec = Maps.newLinkedHashMap(partSpec);\n        Warehouse.makeSpecFromName(fullPartSpec, partPath);\n        futures.add(pool.submit(new Callable<Void>() {\n          @Override\n          public Void call() throws Exception {\n            try {\n              // move file would require session details (needCopy() invokes SessionState.get)\n              SessionState.setCurrentSessionState(parentSession);\n              LOG.info(\"New loading path = \" + partPath + \" with partSpec \" + fullPartSpec);\n\n              // load the partition\n              Partition newPartition = loadPartition(partPath, tbl, fullPartSpec,\n                  replace, true, listBucketingEnabled,\n                  false, isAcid, hasFollowingStatsTask);\n              partitionsMap.put(fullPartSpec, newPartition);\n\n              if (inPlaceEligible) {\n                synchronized (ps) {\n                  InPlaceUpdate.rePositionCursor(ps);\n                  partitionsLoaded.incrementAndGet();\n                  InPlaceUpdate.reprintLine(ps, \"Loaded : \" + partitionsLoaded.get() + \"/\"\n                      + partsToLoad + \" partitions.\");\n                }\n              }\n              // Add embedded rawstore, so we can cleanup later to avoid memory leak\n              if (getMSC().isLocalMetaStore()) {\n                if (!rawStoreMap.containsKey(Thread.currentThread().getId())) {\n                  rawStoreMap.put(Thread.currentThread().getId(), HiveMetaStore.HMSHandler.getRawStore());\n                }\n              }\n              return null;\n            } catch (Exception t) {\n              LOG.error(\"Exception when loading partition with parameters \"\n                  + \" partPath=\" + partPath + \", \"\n                  + \" table=\" + tbl.getTableName() + \", \"\n                  + \" partSpec=\" + fullPartSpec + \", \"\n                  + \" replace=\" + replace + \", \"\n                  + \" listBucketingEnabled=\" + listBucketingEnabled + \", \"\n                  + \" isAcid=\" + isAcid + \", \"\n                  + \" hasFollowingStatsTask=\" + hasFollowingStatsTask, t);\n              throw t;\n            }\n          }\n        }));\n      }\n      pool.shutdown();\n      LOG.debug(\"Number of partitions to be added is \" + futures.size());\n\n      for (Future future : futures) {\n        future.get();\n      }\n\n      for (RawStore rs : rawStoreMap.values()) {\n        rs.shutdown();\n      }\n    } catch (InterruptedException | ExecutionException e) {\n      LOG.debug(\"Cancelling \" + futures.size() + \" dynamic loading tasks\");\n      //cancel other futures\n      for (Future future : futures) {\n        future.cancel(true);\n      }\n      throw new HiveException(\"Exception when loading \"\n          + partsToLoad + \" in table \" + tbl.getTableName()\n          + \" with loadPath=\" + loadPath, e);\n    }\n\n    try {\n      if (isAcid) {\n        List<String> partNames = new ArrayList<>(partitionsMap.size());\n        for (Partition p : partitionsMap.values()) {\n          partNames.add(p.getName());\n        }\n        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n          partNames, AcidUtils.toDataOperationType(operation));\n      }\n      LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");\n      return partitionsMap;\n    } catch (TException te) {\n      throw new HiveException(\"Exception updating metastore for acid table \"\n          + tableName + \" with partitions \" + partitionsMap.values(), te);\n    }\n  }"
        ]
    ],
    "eca6b89458be22abae2529ae549894cb1ba75a6d": [
        [
            "LlapServiceDriver::run(String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.NDC.class, };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 +\n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.NDC.class,\n              io.netty.util.NetUtil.class };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }"
        ]
    ],
    "790976907070a2a737186543d81c71eb542a5337": [
        [
            "DynamicValueRegistryTez::getValue(String)",
            "  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  ",
            "  @Override\n  public Object getValue(String key) {\n    if (!values.containsKey(key)) {\n      throw new IllegalStateException(\"Value does not exist in registry: \" + key);\n    }\n    return values.get(key);\n  }",
            "  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  ",
            "  @Override\n  public Object getValue(String key) {\n    if (!values.containsKey(key)) {\n      throw new NoDynamicValuesException(\"Value does not exist in registry: \" + key);\n    }\n    return values.get(key);\n  }"
        ]
    ],
    "5e3d85c409b14afb6eb94bad01348d013a536503": [
        [
            "OrcEncodedDataReader::getStripeFooterFromCacheOrDisk(StripeInformation,OrcBatchKey)",
            " 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756 -\n 757 -\n 758  \n 759 -\n 760 -\n 761 -\n 762 -\n 763  \n 764  \n 765  ",
            "  private OrcProto.StripeFooter getStripeFooterFromCacheOrDisk(\n      StripeInformation si, OrcBatchKey stripeKey) throws IOException {\n    boolean hasCache = fileKey != null && metadataCache != null;\n    if (hasCache) {\n      LlapBufferOrBuffers footerBuffers = metadataCache.getStripeTail(stripeKey);\n      if (footerBuffers != null) {\n        try {\n          counters.incrCounter(LlapIOCounters.METADATA_CACHE_HIT);\n          ensureCodecFromFileMetadata();\n          MemoryBuffer footerBuffer = footerBuffers.getSingleBuffer();\n          if (footerBuffer != null) {\n            ByteBuffer bb = footerBuffer.getByteBufferDup();\n            return buildStripeFooter(Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)),\n                bb.remaining(), codec, fileMetadata.getCompressionBufferSize());\n          } else {\n            MemoryBuffer[] footerBufferArray = footerBuffers.getMultipleBuffers();\n            int pos = 0;\n            List<DiskRange> bcs = new ArrayList<>(footerBufferArray.length);\n            for (MemoryBuffer buf : footerBufferArray) {\n              ByteBuffer bb = buf.getByteBufferDup();\n              bcs.add(new BufferChunk(bb, pos));\n              pos += bb.remaining();\n            }\n            return buildStripeFooter(bcs, pos, codec, fileMetadata.getCompressionBufferSize());\n          }\n        } finally {\n          metadataCache.decRefBuffer(footerBuffers);\n        }\n      }\n      counters.incrCounter(LlapIOCounters.METADATA_CACHE_MISS);\n    }\n    long offset = si.getOffset() + si.getIndexLength() + si.getDataLength();\n    long startTime = counters.startTimeCounter();\n    ensureRawDataReader(true);\n    // TODO: add this to metadatareader in ORC - SI => metadata buffer, not just metadata.\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Reading [\" + offset + \", \"\n          + (offset + si.getFooterLength()) + \") based on \" + si);\n    }\n    DiskRangeList footerRange = rawDataReader.readFileData(\n        new DiskRangeList(offset, offset + si.getFooterLength()), 0, false);\n    // LOG.error(\"Got \" + RecordReaderUtils.stringifyDiskRanges(footerRange));\n    counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTime);\n    assert footerRange.next == null; // Can only happens w/zcr for a single input buffer.\n    if (hasCache) {\n      LlapBufferOrBuffers cacheBuf = metadataCache.putStripeTail(\n          stripeKey, footerRange.getData().duplicate());\n      metadataCache.decRefBuffer(cacheBuf); // We don't use this one.\n    }\n    ByteBuffer bb = footerRange.getData().duplicate();\n\n    CompressionKind kind = orcReader.getCompressionKind();\n    boolean isPool = useCodecPool;\n    CompressionCodec codec = isPool ? OrcCodecPool.getCodec(kind) : WriterImpl.createCodec(kind);\n    try {\n      return buildStripeFooter(Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)),\n          bb.remaining(), codec, orcReader.getCompressionSize());\n    } finally {\n      if (isPool) {\n        OrcCodecPool.returnCodec(kind, codec);\n      } else {\n        codec.close();\n      }\n    }\n  }",
            " 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755 +\n 756  \n 757 +\n 758 +\n 759 +\n 760 +\n 761  \n 762 +\n 763 +\n 764 +\n 765 +\n 766 +\n 767 +\n 768 +\n 769 +\n 770  \n 771  \n 772  ",
            "  private OrcProto.StripeFooter getStripeFooterFromCacheOrDisk(\n      StripeInformation si, OrcBatchKey stripeKey) throws IOException {\n    boolean hasCache = fileKey != null && metadataCache != null;\n    if (hasCache) {\n      LlapBufferOrBuffers footerBuffers = metadataCache.getStripeTail(stripeKey);\n      if (footerBuffers != null) {\n        try {\n          counters.incrCounter(LlapIOCounters.METADATA_CACHE_HIT);\n          ensureCodecFromFileMetadata();\n          MemoryBuffer footerBuffer = footerBuffers.getSingleBuffer();\n          if (footerBuffer != null) {\n            ByteBuffer bb = footerBuffer.getByteBufferDup();\n            return buildStripeFooter(Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)),\n                bb.remaining(), codec, fileMetadata.getCompressionBufferSize());\n          } else {\n            MemoryBuffer[] footerBufferArray = footerBuffers.getMultipleBuffers();\n            int pos = 0;\n            List<DiskRange> bcs = new ArrayList<>(footerBufferArray.length);\n            for (MemoryBuffer buf : footerBufferArray) {\n              ByteBuffer bb = buf.getByteBufferDup();\n              bcs.add(new BufferChunk(bb, pos));\n              pos += bb.remaining();\n            }\n            return buildStripeFooter(bcs, pos, codec, fileMetadata.getCompressionBufferSize());\n          }\n        } finally {\n          metadataCache.decRefBuffer(footerBuffers);\n        }\n      }\n      counters.incrCounter(LlapIOCounters.METADATA_CACHE_MISS);\n    }\n    long offset = si.getOffset() + si.getIndexLength() + si.getDataLength();\n    long startTime = counters.startTimeCounter();\n    ensureRawDataReader(true);\n    // TODO: add this to metadatareader in ORC - SI => metadata buffer, not just metadata.\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Reading [\" + offset + \", \"\n          + (offset + si.getFooterLength()) + \") based on \" + si);\n    }\n    DiskRangeList footerRange = rawDataReader.readFileData(\n        new DiskRangeList(offset, offset + si.getFooterLength()), 0, false);\n    // LOG.error(\"Got \" + RecordReaderUtils.stringifyDiskRanges(footerRange));\n    counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTime);\n    assert footerRange.next == null; // Can only happens w/zcr for a single input buffer.\n    if (hasCache) {\n      LlapBufferOrBuffers cacheBuf = metadataCache.putStripeTail(\n          stripeKey, footerRange.getData().duplicate());\n      metadataCache.decRefBuffer(cacheBuf); // We don't use this one.\n    }\n    ByteBuffer bb = footerRange.getData().duplicate();\n\n    CompressionKind kind = orcReader.getCompressionKind();\n    boolean isPool = useCodecPool;\n    CompressionCodec codec = isPool ? OrcCodecPool.getCodec(kind) : WriterImpl.createCodec(kind);\n    boolean isCodecError = true;\n    try {\n      OrcProto.StripeFooter result = buildStripeFooter(Lists.<DiskRange>newArrayList(\n          new BufferChunk(bb, 0)), bb.remaining(), codec, orcReader.getCompressionSize());\n      isCodecError = false;\n      return result;\n    } finally {\n      try {\n        if (isPool && !isCodecError) {\n          OrcCodecPool.returnCodec(kind, codec);\n        } else {\n          codec.close();\n        }\n      } catch (Exception ex) {\n        LOG.error(\"Ignoring codec cleanup error\", ex);\n      }\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::close()",
            " 678  \n 679  \n 680 -\n 681 -\n 682 -\n 683 -\n 684  \n 685 -\n 686  ",
            "  @Override\n  public void close() throws IOException {\n    if (isCodecFromPool) {\n      OrcCodecPool.returnCodec(compressionKind, codec);\n    } else {\n      codec.close();\n    }\n    dataReader.close();\n  }",
            " 679  \n 680  \n 681 +\n 682 +\n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690 +\n 691  \n 692  ",
            "  @Override\n  public void close() throws IOException {\n    try {\n      if (isCodecFromPool && !isCodecFailure) {\n        OrcCodecPool.returnCodec(compressionKind, codec);\n      } else {\n        codec.close();\n      }\n    } catch (Exception ex) {\n      LOG.error(\"Ignoring error from codec\", ex);\n    } finally {\n      dataReader.close();\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedStream(long,DiskRangeList,long,long,ColumnStreamData,long,long,IdentityHashMap)",
            " 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873 -\n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n        cacheWrapper.getDataBufferFactory());\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        decompressChunk(chunk.originalData, codec, dest);\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      // After we set originalData to null, we incref the buffer and the cleanup would decref it.\n      // Note that this assumes the failure during incref means incref didn't occur.\n      try {\n        cacheWrapper.reuseBuffer(chunk.getBuffer());\n      } finally {\n        chunk.originalData = null;\n      }\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }",
            " 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879 +\n 880 +\n 881 +\n 882 +\n 883 +\n 884 +\n 885 +\n 886 +\n 887 +\n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n        cacheWrapper.getDataBufferFactory());\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        boolean isOk = false;\n        try {\n          decompressChunk(chunk.originalData, codec, dest);\n          isOk = true;\n        } finally {\n          if (!isOk) {\n            isCodecFailure = true;\n          }\n        }\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      // After we set originalData to null, we incref the buffer and the cleanup would decref it.\n      // Note that this assumes the failure during incref means incref didn't occur.\n      try {\n        cacheWrapper.reuseBuffer(chunk.getBuffer());\n      } finally {\n        chunk.originalData = null;\n      }\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }"
        ]
    ],
    "8973d2c66394ed25b1baa20df3920870ae9b053c": [
        [
            "DruidSerDe::initialize(Configuration,Properties)",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177 -\n 178 -\n 179 -\n 180 -\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. Either it is a CTAS, or we need to create a Druid\n      // Segment Metadata query that retrieves all columns present in\n      // the data source (dimensions and metrics).\n      if (!org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMNS))\n              && !org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES))) {\n        columnNames.addAll(Utilities.getColumnNames(properties));\n        if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          throw new SerDeException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n                  \"') not specified in create table; list of columns is : \" +\n                  properties.getProperty(serdeConstants.LIST_COLUMNS));\n        }\n        columnTypes.addAll(Lists.transform(Utilities.getColumnTypes(properties),\n                new Function<String, PrimitiveTypeInfo>() {\n                  @Override\n                  public PrimitiveTypeInfo apply(String type) {\n                    return TypeInfoFactory.getPrimitiveTypeInfo(type);\n                  }\n                }\n        ));\n        inspectors.addAll(Lists.transform(columnTypes,\n                new Function<PrimitiveTypeInfo, ObjectInspector>() {\n                  @Override\n                  public ObjectInspector apply(PrimitiveTypeInfo type) {\n                    return PrimitiveObjectInspectorFactory\n                            .getPrimitiveWritableObjectInspector(type);\n                  }\n                }\n        ));\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      } else {\n        String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n        if (dataSource == null) {\n          throw new SerDeException(\"Druid data source not specified; use \" +\n                  Constants.DRUID_DATA_SOURCE + \" in table properties\");\n        }\n        SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n        builder.dataSource(dataSource);\n        builder.merge(true);\n        builder.analysisTypes();\n        SegmentMetadataQuery query = builder.build();\n\n        // Execute query in Druid\n        String address = HiveConf.getVar(configuration,\n                HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n        );\n        if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n          throw new SerDeException(\"Druid broker address not specified in configuration\");\n        }\n\n        numConnection = HiveConf\n              .getIntVar(configuration, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n        readTimeout = new Period(\n              HiveConf.getVar(configuration, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n        // Infer schema\n        SegmentAnalysis schemaInfo;\n        try {\n          schemaInfo = submitMetadataRequest(address, query);\n        } catch (IOException e) {\n          throw new SerDeException(e);\n        }\n        for (Entry<String, ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n          if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n            // Special handling for timestamp column\n            columnNames.add(columnInfo.getKey()); // field name\n            PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n            columnTypes.add(type);\n            inspectors\n                    .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n            continue;\n          }\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                  columnInfo.getValue().getType()); // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n        }\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      }\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n\n        switch (query.getType()) {\n          case Query.TIMESERIES:\n            inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n            break;\n          case Query.TOPN:\n            inferSchema((TopNQuery) query, columnNames, columnTypes);\n            break;\n          case Query.SELECT:\n            String address = HiveConf.getVar(configuration,\n                    HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n            if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n              throw new SerDeException(\"Druid broker address not specified in configuration\");\n            }\n            inferSchema((SelectQuery) query, columnNames, columnTypes, address);\n            break;\n          case Query.GROUP_BY:\n            inferSchema((GroupByQuery) query, columnNames, columnTypes);\n            break;\n          default:\n            throw new SerDeException(\"Not supported Druid query\");\n        }\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors\n                .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }",
            " 110  \n 111  \n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Init connection properties\n    numConnection = HiveConf\n          .getIntVar(configuration, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    readTimeout = new Period(\n          HiveConf.getVar(configuration, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. Either it is a CTAS, or we need to create a Druid\n      // Segment Metadata query that retrieves all columns present in\n      // the data source (dimensions and metrics).\n      if (!org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMNS))\n              && !org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES))) {\n        columnNames.addAll(Utilities.getColumnNames(properties));\n        if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          throw new SerDeException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n                  \"') not specified in create table; list of columns is : \" +\n                  properties.getProperty(serdeConstants.LIST_COLUMNS));\n        }\n        columnTypes.addAll(Lists.transform(Utilities.getColumnTypes(properties),\n                new Function<String, PrimitiveTypeInfo>() {\n                  @Override\n                  public PrimitiveTypeInfo apply(String type) {\n                    return TypeInfoFactory.getPrimitiveTypeInfo(type);\n                  }\n                }\n        ));\n        inspectors.addAll(Lists.transform(columnTypes,\n                new Function<PrimitiveTypeInfo, ObjectInspector>() {\n                  @Override\n                  public ObjectInspector apply(PrimitiveTypeInfo type) {\n                    return PrimitiveObjectInspectorFactory\n                            .getPrimitiveWritableObjectInspector(type);\n                  }\n                }\n        ));\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      } else {\n        String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n        if (dataSource == null) {\n          throw new SerDeException(\"Druid data source not specified; use \" +\n                  Constants.DRUID_DATA_SOURCE + \" in table properties\");\n        }\n        SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n        builder.dataSource(dataSource);\n        builder.merge(true);\n        builder.analysisTypes();\n        SegmentMetadataQuery query = builder.build();\n\n        // Execute query in Druid\n        String address = HiveConf.getVar(configuration,\n                HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n        );\n        if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n          throw new SerDeException(\"Druid broker address not specified in configuration\");\n        }\n\n        // Infer schema\n        SegmentAnalysis schemaInfo;\n        try {\n          schemaInfo = submitMetadataRequest(address, query);\n        } catch (IOException e) {\n          throw new SerDeException(e);\n        }\n        for (Entry<String, ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n          if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n            // Special handling for timestamp column\n            columnNames.add(columnInfo.getKey()); // field name\n            PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n            columnTypes.add(type);\n            inspectors\n                    .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n            continue;\n          }\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                  columnInfo.getValue().getType()); // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n        }\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      }\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n\n        switch (query.getType()) {\n          case Query.TIMESERIES:\n            inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n            break;\n          case Query.TOPN:\n            inferSchema((TopNQuery) query, columnNames, columnTypes);\n            break;\n          case Query.SELECT:\n            String address = HiveConf.getVar(configuration,\n                    HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n            if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n              throw new SerDeException(\"Druid broker address not specified in configuration\");\n            }\n            inferSchema((SelectQuery) query, columnNames, columnTypes, address);\n            break;\n          case Query.GROUP_BY:\n            inferSchema((GroupByQuery) query, columnNames, columnTypes);\n            break;\n          default:\n            throw new SerDeException(\"Not supported Druid query\");\n        }\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors\n                .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }"
        ]
    ],
    "dd0bc33d125817e20bd4854172b3c8326c834ea0": [
        [
            "TestRestrictedList::startServices()",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @BeforeClass\n  public static void startServices() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n\n    oldHiveSiteURL = HiveConf.getHiveSiteLocation();\n    oldHiveMetastoreSiteURL = HiveConf.getMetastoreSiteLocation();\n    String confDir = \"../../data/conf/rlist/\";\n    HiveConf.setHiveSiteLocation(\n        new URL(\"file://\" + new File(confDir).toURI().getPath() + \"/hive-site.xml\"));\n    System.out.println(\"Setting hive-site: \" + HiveConf.getHiveSiteLocation());\n    HiveConf.setHivemetastoreSiteUrl(\n        new URL(\"file://\" + new File(confDir).toURI().getPath() + \"/hivemetastore-site.xml\"));\n    System.out.println(\"Setting hive-site: \" + HiveConf.getHiveSiteLocation());\n\n    hiveConf = new HiveConf();\n    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS, 1);\n    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS, 1);\n    hiveConf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);\n\n    miniHS2 = new MiniHS2.Builder().withMiniMR().withRemoteMetastore().withConf(hiveConf).build();\n    miniHS2.start(new HashMap<String, String>());\n\n    // Add the parameter here if it cannot change at runtime\n    addToExpectedRestrictedMap(\"hive.conf.restricted.list\");\n    addToExpectedRestrictedMap(\"hive.security.authenticator.manager\");\n    addToExpectedRestrictedMap(\"hive.security.authorization.manager\");\n    addToExpectedRestrictedMap(\"hive.security.metastore.authorization.manager\");\n    addToExpectedRestrictedMap(\"hive.security.metastore.authenticator.manager\");\n    addToExpectedRestrictedMap(\"hive.users.in.admin.role\");\n    addToExpectedRestrictedMap(\"hive.server2.xsrf.filter.enabled\");\n    addToExpectedRestrictedMap(\"hive.security.authorization.enabled\");\n    addToExpectedRestrictedMap(\"hive.distcp.privileged.doAs\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.baseDN\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.url\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.Domain\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupDNPattern\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupFilter\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userDNPattern\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userFilter\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupMembershipKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userMembershipKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupClassKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.customLDAPQuery\");\n    addToExpectedRestrictedMap(\"hive.spark.client.channel.log.level\");\n    addToExpectedRestrictedMap(\"hive.spark.client.secret.bits\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.address\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.port\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.sasl.mechanisms\");\n    addToExpectedRestrictedMap(\"bonecp.test\");\n    addToExpectedRestrictedMap(\"hive.druid.broker.address.default\");\n    addToExpectedRestrictedMap(\"hive.druid.coordinator.address.default\");\n    addToExpectedRestrictedMap(\"hikari.test\");\n    addToExpectedRestrictedMap(\"hadoop.bin.path\");\n    addToExpectedRestrictedMap(\"yarn.bin.path\");\n    addToExpectedRestrictedMap(\"hive.spark.client.connect.timeout\");\n    addToExpectedRestrictedMap(\"hive.spark.client.server.connect.timeout\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.max.size\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.threads\");\n    addToExpectedRestrictedMap(\"_hive.local.session.path\");\n    addToExpectedRestrictedMap(\"_hive.tmp_table_space\");\n    addToExpectedRestrictedMap(\"_hive.hdfs.session.path\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.address\");\n  }",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  ",
            "  @BeforeClass\n  public static void startServices() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n\n    oldHiveSiteURL = HiveConf.getHiveSiteLocation();\n    oldHiveMetastoreSiteURL = HiveConf.getMetastoreSiteLocation();\n    String confDir = \"../../data/conf/rlist/\";\n    HiveConf.setHiveSiteLocation(\n        new URL(\"file://\" + new File(confDir).toURI().getPath() + \"/hive-site.xml\"));\n    System.out.println(\"Setting hive-site: \" + HiveConf.getHiveSiteLocation());\n    HiveConf.setHivemetastoreSiteUrl(\n        new URL(\"file://\" + new File(confDir).toURI().getPath() + \"/hivemetastore-site.xml\"));\n    System.out.println(\"Setting hive-site: \" + HiveConf.getHiveSiteLocation());\n\n    hiveConf = new HiveConf();\n    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MIN_WORKER_THREADS, 1);\n    hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_MAX_WORKER_THREADS, 1);\n    hiveConf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);\n\n    miniHS2 = new MiniHS2.Builder().withMiniMR().withRemoteMetastore().withConf(hiveConf).build();\n    miniHS2.start(new HashMap<String, String>());\n\n    // Add the parameter here if it cannot change at runtime\n    addToExpectedRestrictedMap(\"hive.conf.restricted.list\");\n    addToExpectedRestrictedMap(\"hive.security.authenticator.manager\");\n    addToExpectedRestrictedMap(\"hive.security.authorization.manager\");\n    addToExpectedRestrictedMap(\"hive.security.metastore.authorization.manager\");\n    addToExpectedRestrictedMap(\"hive.security.metastore.authenticator.manager\");\n    addToExpectedRestrictedMap(\"hive.users.in.admin.role\");\n    addToExpectedRestrictedMap(\"hive.server2.xsrf.filter.enabled\");\n    addToExpectedRestrictedMap(\"hive.security.authorization.enabled\");\n    addToExpectedRestrictedMap(\"hive.distcp.privileged.doAs\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.baseDN\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.url\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.Domain\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupDNPattern\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupFilter\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userDNPattern\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userFilter\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupMembershipKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.userMembershipKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.groupClassKey\");\n    addToExpectedRestrictedMap(\"hive.server2.authentication.ldap.customLDAPQuery\");\n    addToExpectedRestrictedMap(\"hive.spark.client.channel.log.level\");\n    addToExpectedRestrictedMap(\"hive.spark.client.secret.bits\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.address\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.port\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.sasl.mechanisms\");\n    addToExpectedRestrictedMap(\"bonecp.test\");\n    addToExpectedRestrictedMap(\"hive.druid.broker.address.default\");\n    addToExpectedRestrictedMap(\"hive.druid.coordinator.address.default\");\n    addToExpectedRestrictedMap(\"hikari.test\");\n    addToExpectedRestrictedMap(\"hadoop.bin.path\");\n    addToExpectedRestrictedMap(\"yarn.bin.path\");\n    addToExpectedRestrictedMap(\"hive.spark.client.connect.timeout\");\n    addToExpectedRestrictedMap(\"hive.spark.client.server.connect.timeout\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.max.size\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.threads\");\n    addToExpectedRestrictedMap(\"_hive.local.session.path\");\n    addToExpectedRestrictedMap(\"_hive.tmp_table_space\");\n    addToExpectedRestrictedMap(\"_hive.hdfs.session.path\");\n    addToExpectedRestrictedMap(\"hive.spark.client.rpc.server.address\");\n    addToExpectedRestrictedMap(\"spark.home\");\n  }"
        ]
    ],
    "5db30cd5aeb926d0ebae0a3c2447feb76056abe1": [
        [
            "preAlterTable(Table,EnvironmentContext)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  /**\n   * Called before a table is altered in the metastore\n   * during ALTER TABLE.\n   *\n   * @param table new table definition\n   */\n  public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // By default allow only ADDPROPS and DROPPROPS.\n    // alterOpType is null in case of stats update.\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ],
        [
            "DruidStorageHandler::preAlterTable(Table,EnvironmentContext)",
            " 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }",
            " 689  \n 690  \n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  ",
            "  @Override\n  public void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n    // alterOpType is null in case of stats update\n    if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)) {\n      throw new MetaException(\n          \"ALTER TABLE can not be used for \" + alterOpType + \" to a non-native table \");\n    }\n  }"
        ]
    ],
    "dc0938c42f6c9a42adb3fcbb391fb759a3bb0072": [
        [
            "Utils::getSplitLocationProvider(Configuration,Logger)",
            "  34  \n  35  \n  36  \n  37 -\n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  ",
            "  public static SplitLocationProvider getSplitLocationProvider(Configuration conf, Logger LOG) throws\n      IOException {\n    boolean useCustomLocations =\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS);\n    SplitLocationProvider splitLocationProvider;\n    LOG.info(\"SplitGenerator using llap affinitized locations: \" + useCustomLocations);\n    if (useCustomLocations) {\n      LlapRegistryService serviceRegistry = LlapRegistryService.getClient(conf);\n      LOG.info(\"Using LLAP instance \" + serviceRegistry.getApplicationId());\n\n      Collection<ServiceInstance> serviceInstances =\n          serviceRegistry.getInstances().getAllInstancesOrdered(true);\n      ArrayList<String> locations = new ArrayList<>(serviceInstances.size());\n      for (ServiceInstance serviceInstance : serviceInstances) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + serviceInstance.getWorkerIdentity() + \" with hostname=\" +\n              serviceInstance.getHost() + \" to list for split locations\");\n        }\n        locations.add(serviceInstance.getHost());\n      }\n      splitLocationProvider = new HostAffinitySplitLocationProvider(locations);\n    } else {\n      splitLocationProvider = new SplitLocationProvider() {\n        @Override\n        public String[] getLocations(InputSplit split) throws IOException {\n          if (split == null) {\n            return null;\n          }\n          String[] locations = split.getLocations();\n          if (locations != null && locations.length == 1) {\n            if (\"localhost\".equals(locations[0])) {\n              return ArrayUtils.EMPTY_STRING_ARRAY;\n            }\n          }\n          return locations;\n        }\n      };\n    }\n    return splitLocationProvider;\n  }",
            "  35  \n  36  \n  37  \n  38 +\n  39 +\n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  ",
            "  public static SplitLocationProvider getSplitLocationProvider(Configuration conf, Logger LOG) throws\n      IOException {\n    boolean useCustomLocations =\n        HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE).equals(\"llap\")\n        && HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS);\n    SplitLocationProvider splitLocationProvider;\n    LOG.info(\"SplitGenerator using llap affinitized locations: \" + useCustomLocations);\n    if (useCustomLocations) {\n      LlapRegistryService serviceRegistry = LlapRegistryService.getClient(conf);\n      LOG.info(\"Using LLAP instance \" + serviceRegistry.getApplicationId());\n\n      Collection<ServiceInstance> serviceInstances =\n          serviceRegistry.getInstances().getAllInstancesOrdered(true);\n      Preconditions.checkArgument(!serviceInstances.isEmpty(),\n          \"No running LLAP daemons! Please check LLAP service status and zookeeper configuration\");\n      ArrayList<String> locations = new ArrayList<>(serviceInstances.size());\n      for (ServiceInstance serviceInstance : serviceInstances) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + serviceInstance.getWorkerIdentity() + \" with hostname=\" +\n              serviceInstance.getHost() + \" to list for split locations\");\n        }\n        locations.add(serviceInstance.getHost());\n      }\n      splitLocationProvider = new HostAffinitySplitLocationProvider(locations);\n    } else {\n      splitLocationProvider = new SplitLocationProvider() {\n        @Override\n        public String[] getLocations(InputSplit split) throws IOException {\n          if (split == null) {\n            return null;\n          }\n          String[] locations = split.getLocations();\n          if (locations != null && locations.length == 1) {\n            if (\"localhost\".equals(locations[0])) {\n              return ArrayUtils.EMPTY_STRING_ARRAY;\n            }\n          }\n          return locations;\n        }\n      };\n    }\n    return splitLocationProvider;\n  }"
        ]
    ],
    "e46e473effbcd84951d9023ae973b1c135e7b0b9": [
        [
            "ObjectStore::lockForUpdate()",
            "8527  \n8528 -\n8529  \n8530  \n8531  \n8532  \n8533  \n8534  \n8535  \n8536  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select NEXT_EVENT_ID from NOTIFICATION_SEQUENCE\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(hiveConf, () -> {\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n    }).run();\n  }",
            "8527  \n8528 +\n8529  \n8530  \n8531  \n8532  \n8533  \n8534  \n8535  \n8536  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select \\\"NEXT_EVENT_ID\\\" from \\\"NOTIFICATION_SEQUENCE\\\"\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(hiveConf, () -> {\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n    }).run();\n  }"
        ]
    ],
    "41afa4e2185b45b0bb6816baaea330d9ed40d127": [
        [
            "TestKryoMessageCodec::testEmbeddedChannel()",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  ",
            "  @Test\n  public void testEmbeddedChannel() throws Exception {\n    EmbeddedChannel c = new EmbeddedChannel(\n      new LoggingHandler(getClass()),\n      new KryoMessageCodec(0));\n    c.writeAndFlush(MESSAGE);\n    assertEquals(1, c.outboundMessages().size());\n    assertFalse(MESSAGE.getClass().equals(c.outboundMessages().peek().getClass()));\n    c.writeInbound(c.readOutbound());\n    assertEquals(1, c.inboundMessages().size());\n    assertEquals(MESSAGE, c.readInbound());\n    c.close();\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77  \n  78  \n  79  \n  80  ",
            "  @Test\n  public void testEmbeddedChannel() throws Exception {\n    EmbeddedChannel c = new EmbeddedChannel(\n      new LoggingHandler(getClass()),\n      new KryoMessageCodec(0));\n    c.writeAndFlush(MESSAGE);\n    assertEquals(1, c.outboundMessages().size());\n    assertFalse(MESSAGE.getClass().equals(c.outboundMessages().peek().getClass()));\n    Object readOutboundResult = c.readOutbound();\n    c.writeInbound(readOutboundResult);\n    assertEquals(1, c.inboundMessages().size());\n    assertEquals(MESSAGE, c.readInbound());\n    c.close();\n  }"
        ],
        [
            "LocalHiveSparkClient::LocalHiveSparkClient(SparkConf,HiveConf)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  ",
            "  private LocalHiveSparkClient(SparkConf sparkConf, HiveConf hiveConf)\n      throws FileNotFoundException, MalformedURLException {\n    String regJar = null;\n    // the registrator jar should already be in CP when not in test mode\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_IN_TEST)) {\n      String kryoReg = sparkConf.get(\"spark.kryo.registrator\", \"\");\n      if (SparkClientUtilities.HIVE_KRYO_REG_NAME.equals(kryoReg)) {\n        regJar = SparkClientUtilities.findKryoRegistratorJar(hiveConf);\n        SparkClientUtilities.addJarToContextLoader(new File(regJar));\n      }\n    }\n    sc = new JavaSparkContext(sparkConf);\n    if (regJar != null) {\n      sc.addJar(regJar);\n    }\n    jobMetricsListener = new JobMetricsListener();\n    sc.sc().listenerBus().addListener(jobMetricsListener);\n  }",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  ",
            "  private LocalHiveSparkClient(SparkConf sparkConf, HiveConf hiveConf)\n      throws FileNotFoundException, MalformedURLException {\n    String regJar = null;\n    // the registrator jar should already be in CP when not in test mode\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_IN_TEST)) {\n      String kryoReg = sparkConf.get(\"spark.kryo.registrator\", \"\");\n      if (SparkClientUtilities.HIVE_KRYO_REG_NAME.equals(kryoReg)) {\n        regJar = SparkClientUtilities.findKryoRegistratorJar(hiveConf);\n        SparkClientUtilities.addJarToContextLoader(new File(regJar));\n      }\n    }\n    sc = new JavaSparkContext(sparkConf);\n    if (regJar != null) {\n      sc.addJar(regJar);\n    }\n    jobMetricsListener = new JobMetricsListener();\n    sc.sc().addSparkListener(jobMetricsListener);\n  }"
        ],
        [
            "TestRpc::transfer(Rpc,Rpc)",
            " 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326 -\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  ",
            "  private void transfer(Rpc serverRpc, Rpc clientRpc) {\n    EmbeddedChannel client = (EmbeddedChannel) clientRpc.getChannel();\n    EmbeddedChannel server = (EmbeddedChannel) serverRpc.getChannel();\n\n    server.runPendingTasks();\n    client.runPendingTasks();\n\n    int count = 0;\n    while (!client.outboundMessages().isEmpty()) {\n      server.writeInbound(client.readOutbound());\n      count++;\n    }\n    server.flush();\n    LOG.debug(\"Transferred {} outbound client messages.\", count);\n\n    count = 0;\n    while (!server.outboundMessages().isEmpty()) {\n      client.writeInbound(server.readOutbound());\n      count++;\n    }\n    client.flush();\n    LOG.debug(\"Transferred {} outbound server messages.\", count);\n  }",
            " 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326 +\n 327 +\n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  ",
            "  private void transfer(Rpc serverRpc, Rpc clientRpc) {\n    EmbeddedChannel client = (EmbeddedChannel) clientRpc.getChannel();\n    EmbeddedChannel server = (EmbeddedChannel) serverRpc.getChannel();\n\n    server.runPendingTasks();\n    client.runPendingTasks();\n\n    int count = 0;\n    while (!client.outboundMessages().isEmpty()) {\n      Object readOutboundResult = client.readOutbound();\n      server.writeInbound(readOutboundResult);\n      count++;\n    }\n    server.flush();\n    LOG.debug(\"Transferred {} outbound client messages.\", count);\n\n    count = 0;\n    while (!server.outboundMessages().isEmpty()) {\n      Object readOutboundResult = server.readOutbound();\n      client.writeInbound(readOutboundResult);\n      count++;\n    }\n    client.flush();\n    LOG.debug(\"Transferred {} outbound server messages.\", count);\n  }"
        ]
    ],
    "4a041428b14cd195c5d89c93e07ef518652fdcd3": [
        [
            "DruidOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 -\n 194 -\n 195 -\n 196 -\n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);\n      AggregatorFactory af;\n      switch (f.getPrimitiveCategory()) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n        case DECIMAL:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case TIMESTAMP:\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=\n                  PrimitiveGrouping.STRING_GROUP) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107 +\n 108 +\n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            QueryGranularity.fromString(\n                    tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY) == null\n                            ? \"NONE\"\n                            : tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY)),\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);\n      AggregatorFactory af;\n      switch (f.getPrimitiveCategory()) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n        case DECIMAL:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case TIMESTAMP:\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=\n                  PrimitiveGrouping.STRING_GROUP) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    Integer maxRowInMemory = HiveConf.getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_ROW_IN_MEMORY);\n\n    RealtimeTuningConfig realtimeTuningConfig = new RealtimeTuningConfig(maxRowInMemory,\n            null,\n            null,\n            new File(basePersistDirectory),\n            new CustomVersioningPolicy(version),\n            null,\n            null,\n            null,\n            null,\n            true,\n            0,\n            0,\n            true,\n            null\n    );\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)"
        ]
    ],
    "f4d017b801ea256bf076160a31dba88e61c80422": [
        [
            "EncodedReaderImpl::readEncodedColumns(int,StripeInformation,OrcProto,List,List,boolean,boolean,Consumer)",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 -\n 420 -\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 -\n 434 -\n 435 -\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[][] colRgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    boolean[] includedRgs = null; // Will always be the same for all cols at the moment.\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      includedRgs = colRgs[ctx.includedIx];\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (includedRgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (includedRgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead.get());\n    if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n      LOG.info(\"Resulting disk ranges to read (file \" + fileKey + \"): \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n    BooleanRef isAllInCache = new BooleanRef();\n    if (hasFileId) {\n      cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);\n      if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after cache (found everything \" + isAllInCache.value + \"; file \"\n            + fileKey + \", base offset \" + stripeOffset  + \"): \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n    }\n\n    // TODO: the memory release could be optimized - we could release original buffers after we\n    //       are fully done with each original buffer from disk. For now release all at the end;\n    //       it doesn't increase the total amount of memory we hold, just the duration a bit.\n    //       This is much simpler - we can just remember original ranges after reading them, and\n    //       release them at the end. In a few cases where it's easy to determine that a buffer\n    //       can be freed in advance, we remove it from the map.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = null;\n    if (!isAllInCache.value) {\n      if (!isDataReaderOpen) {\n        this.dataReader.open();\n        isDataReaderOpen = true;\n      }\n      dataReader.readFileData(toRead.next, stripeOffset, cacheWrapper.getAllocator().isDirectAlloc());\n      toRelease = new IdentityHashMap<>();\n      DiskRangeList drl = toRead.next;\n      while (drl != null) {\n        if (drl instanceof BufferChunk) {\n          toRelease.put(drl.getData(), true);\n        }\n        drl = drl.next;\n      }\n    }\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    if (codec == null) {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease = null;\n      }\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n    }\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n    for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n      boolean isLastRg = rgIx == rgCount - 1;\n      // Create the batch we will use to return data for this RG.\n      OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n      ecb.init(fileKey, stripeIx, rgIx, included.length);\n      boolean isRGSelected = true;\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        if (isTracingEnabled) {\n          LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n        }\n        // TODO: simplify this now that high-level cache has been removed. Same RGs for all cols.\n        if (colRgs[ctx.includedIx] != null && !colRgs[ctx.includedIx][rgIx]) {\n          // RG x col filtered.\n          isRGSelected = false;\n          if (isTracingEnabled) {\n            LOG.trace(\"colIxMod: {} rgIx: {} colRgs[{}]: {} colRgs[{}][{}]: {}\", ctx.includedIx, rgIx, ctx.includedIx,\n              Arrays.toString(colRgs[ctx.includedIx]), ctx.includedIx, rgIx, colRgs[ctx.includedIx][rgIx]);\n          }\n           continue;\n        }\n        OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n            nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n        ecb.initOrcColumn(ctx.colIx);\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          ColumnStreamData cb = null;\n          try {\n            if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n              // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n              if (isTracingEnabled) {\n                LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n              }\n              if (sctx.stripeLevelStream == null) {\n                sctx.stripeLevelStream = POOLS.csdPool.take();\n                // We will be using this for each RG while also sending RGs to processing.\n                // To avoid buffers being unlocked, run refcount one ahead; we will not increase\n                // it when building the last RG, so each RG processing will decref once, and the\n                // last one will unlock the buffers.\n                sctx.stripeLevelStream.incRef();\n                // For stripe-level streams we don't need the extra refcount on the block.\n                // See class comment about refcounts.\n                long unlockUntilCOffset = sctx.offset + sctx.length;\n                DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                    sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                    unlockUntilCOffset, sctx.offset, toRelease);\n                if (lastCached != null) {\n                  iter = lastCached;\n                }\n              }\n              if (!isLastRg) {\n                sctx.stripeLevelStream.incRef();\n              }\n              cb = sctx.stripeLevelStream;\n            } else {\n              // This stream can be separated by RG using index. Let's do that.\n              // Offset to where this RG begins.\n              long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n              // Offset relative to the beginning of the stream of where this RG ends.\n              long nextCOffsetRel = isLastRg ? sctx.length\n                  : nextIndex.getPositions(sctx.streamIndexOffset);\n              // Offset before which this RG is guaranteed to end. Can only be estimated.\n              // We estimate the same way for compressed and uncompressed for now.\n              long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                  isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n              // As we read, we can unlock initial refcounts for the buffers that end before\n              // the data that we need for this RG.\n              long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n              cb = createRgColumnStreamData(\n                  rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);\n              boolean isStartOfStream = sctx.bufferIter == null;\n              DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                  (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                  unlockUntilCOffset, sctx.offset, toRelease);\n              if (lastCached != null) {\n                sctx.bufferIter = iter = lastCached;\n              }\n            }\n            ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n          } catch (Exception ex) {\n            DiskRangeList drl = toRead == null ? null : toRead.next;\n            LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n            throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n          }\n        }\n      }\n      if (isRGSelected) {\n        consumer.consumeData(ecb);\n      }\n    }\n\n    if (isTracingEnabled) {\n      LOG.trace(\"Disk ranges after preparing all the data \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n\n    // Release the unreleased buffers. See class comment about refcounts.\n    releaseInitialRefcounts(toRead.next);\n    // Release buffers as we are done with all the streams... also see toRelease comment.\n    if (toRelease != null) {\n      releaseBuffers(toRelease.keySet(), true);\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420 +\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 +\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482 +\n 483 +\n 484 +\n 485 +\n 486 +\n 487 +\n 488 +\n 489 +\n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[][] colRgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      colCtxs[i] = new ColumnReadContext(i, encodings.get(i), indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    boolean[] includedRgs = null; // Will always be the same for all cols at the moment.\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      includedRgs = colRgs[ctx.includedIx];\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (includedRgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (includedRgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    DiskRangeList.MutateHelper toRead = new DiskRangeList.MutateHelper(listToRead.get());\n    if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n      LOG.info(\"Resulting disk ranges to read (file \" + fileKey + \"): \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n    BooleanRef isAllInCache = new BooleanRef();\n    if (hasFileId) {\n      cacheWrapper.getFileData(fileKey, toRead.next, stripeOffset, CC_FACTORY, isAllInCache);\n      if (/*isTracingEnabled && */LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after cache (found everything \" + isAllInCache.value + \"; file \"\n            + fileKey + \", base offset \" + stripeOffset  + \"): \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n    }\n\n    // TODO: the memory release could be optimized - we could release original buffers after we\n    //       are fully done with each original buffer from disk. For now release all at the end;\n    //       it doesn't increase the total amount of memory we hold, just the duration a bit.\n    //       This is much simpler - we can just remember original ranges after reading them, and\n    //       release them at the end. In a few cases where it's easy to determine that a buffer\n    //       can be freed in advance, we remove it from the map.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = null;\n    if (!isAllInCache.value) {\n      if (!isDataReaderOpen) {\n        this.dataReader.open();\n        isDataReaderOpen = true;\n      }\n      dataReader.readFileData(toRead.next, stripeOffset, cacheWrapper.getAllocator().isDirectAlloc());\n      toRelease = new IdentityHashMap<>();\n      DiskRangeList drl = toRead.next;\n      while (drl != null) {\n        if (drl instanceof BufferChunk) {\n          toRelease.put(drl.getData(), true);\n        }\n        drl = drl.next;\n      }\n    }\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    if (codec == null) {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease = null;\n      }\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n    }\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n    for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n      boolean isLastRg = rgIx == rgCount - 1;\n      // Create the batch we will use to return data for this RG.\n      OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n      ecb.init(fileKey, stripeIx, rgIx, included.length);\n      boolean isRGSelected = true;\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        if (isTracingEnabled) {\n          LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n        }\n        // TODO: simplify this now that high-level cache has been removed. Same RGs for all cols.\n        if (colRgs[ctx.includedIx] != null && !colRgs[ctx.includedIx][rgIx]) {\n          // RG x col filtered.\n          isRGSelected = false;\n          if (isTracingEnabled) {\n            LOG.trace(\"colIxMod: {} rgIx: {} colRgs[{}]: {} colRgs[{}][{}]: {}\", ctx.includedIx, rgIx, ctx.includedIx,\n              Arrays.toString(colRgs[ctx.includedIx]), ctx.includedIx, rgIx, colRgs[ctx.includedIx][rgIx]);\n          }\n           continue;\n        }\n        OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n            nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n        ecb.initOrcColumn(ctx.colIx);\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          ColumnStreamData cb = null;\n          try {\n            if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n              // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n              if (isTracingEnabled) {\n                LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n              }\n              if (sctx.stripeLevelStream == null) {\n                sctx.stripeLevelStream = POOLS.csdPool.take();\n                // We will be using this for each RG while also sending RGs to processing.\n                // To avoid buffers being unlocked, run refcount one ahead; so each RG \n                 // processing will decref once, and the\n                // last one will unlock the buffers.\n                sctx.stripeLevelStream.incRef();\n                // For stripe-level streams we don't need the extra refcount on the block.\n                // See class comment about refcounts.\n                long unlockUntilCOffset = sctx.offset + sctx.length;\n                DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                    sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                    unlockUntilCOffset, sctx.offset, toRelease);\n                if (lastCached != null) {\n                  iter = lastCached;\n                }\n              }\n              sctx.stripeLevelStream.incRef();\n              cb = sctx.stripeLevelStream;\n            } else {\n              // This stream can be separated by RG using index. Let's do that.\n              // Offset to where this RG begins.\n              long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n              // Offset relative to the beginning of the stream of where this RG ends.\n              long nextCOffsetRel = isLastRg ? sctx.length\n                  : nextIndex.getPositions(sctx.streamIndexOffset);\n              // Offset before which this RG is guaranteed to end. Can only be estimated.\n              // We estimate the same way for compressed and uncompressed for now.\n              long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                  isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n              // As we read, we can unlock initial refcounts for the buffers that end before\n              // the data that we need for this RG.\n              long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n              cb = createRgColumnStreamData(\n                  rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);\n              boolean isStartOfStream = sctx.bufferIter == null;\n              DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                  (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                  unlockUntilCOffset, sctx.offset, toRelease);\n              if (lastCached != null) {\n                sctx.bufferIter = iter = lastCached;\n              }\n            }\n            ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n          } catch (Exception ex) {\n            DiskRangeList drl = toRead == null ? null : toRead.next;\n            LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n            throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n          }\n        }\n      }\n      if (isRGSelected) {\n        consumer.consumeData(ecb);\n      }\n    }\n\n    if (isTracingEnabled) {\n      LOG.trace(\"Disk ranges after preparing all the data \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n\n    // Release the unreleased buffers. See class comment about refcounts.\n    for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n      ColumnReadContext ctx = colCtxs[colIx];\n      if (ctx == null) continue; // This column is not included.\n      for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n        StreamContext sctx = ctx.streams[streamIx];\n        if (sctx == null || sctx.stripeLevelStream == null) continue;\n        if (0 != sctx.stripeLevelStream.decRef()) continue;\n        for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Unlocking {} at the end of processing\", buf);\n          }\n          cacheWrapper.releaseBuffer(buf);\n        }\n      }\n    }\n\n    releaseInitialRefcounts(toRead.next);\n    // Release buffers as we are done with all the streams... also see toRelease comment.\n    if (toRelease != null) {\n      releaseBuffers(toRelease.keySet(), true);\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }"
        ]
    ],
    "f56abb4054cbc4ba8c8511596117f7823d60dbe6": [
        [
            "DruidQueryBasedInputFormat::distributeSelectQuery(Configuration,String,SelectQuery,Path)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), intervals);\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), URLEncoder.encode(intervals, \"UTF-8\"));\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }"
        ]
    ],
    "7845089c14a43b6bd2d79264c1c48acd4a9ca81d": [
        [
            "TezSessionState::close(boolean)",
            " 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646 -\n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  ",
            "  /**\n   * Close a tez session. Will cleanup any tez/am related resources. After closing a session no\n   * further DAGs can be executed against it. Only called by session management classes; some\n   * sessions should not simply be closed by users - e.g. pool sessions need to be restarted.\n   *\n   * @param keepDagFilesDir\n   *          whether or not to remove the scratch dir at the same time.\n   * @throws Exception\n   */\n  void close(boolean keepDagFilesDir) throws Exception {\n    console = null;\n    appJarLr = null;\n\n    try {\n      if (session != null) {\n        LOG.info(\"Closing Tez Session\");\n        closeClient(session);\n        session = null;\n      } else if (sessionFuture != null) {\n        sessionFuture.cancel(true);\n        TezClient asyncSession = null;\n        try {\n          asyncSession = sessionFuture.get(); // In case it was done and noone looked at it.\n        } catch (ExecutionException | CancellationException e) {\n          // ignore\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          // ignore\n        }\n        sessionFuture = null;\n        if (asyncSession != null) {\n          LOG.info(\"Closing Tez Session\");\n          closeClient(asyncSession);\n        }\n      }\n    } finally {\n      try {\n        cleanupScratchDir();\n      } finally {\n        if (!keepDagFilesDir) {\n          cleanupDagResources();\n        }\n      }\n    }\n  }",
            " 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646 +\n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  ",
            "  /**\n   * Close a tez session. Will cleanup any tez/am related resources. After closing a session no\n   * further DAGs can be executed against it. Only called by session management classes; some\n   * sessions should not simply be closed by users - e.g. pool sessions need to be restarted.\n   *\n   * @param keepDagFilesDir\n   *          whether or not to remove the scratch dir at the same time.\n   * @throws Exception\n   */\n  void close(boolean keepDagFilesDir) throws Exception {\n    console = null;\n    appJarLr = null;\n\n    try {\n      if (getSession() != null) {\n        LOG.info(\"Closing Tez Session\");\n        closeClient(session);\n        session = null;\n      } else if (sessionFuture != null) {\n        sessionFuture.cancel(true);\n        TezClient asyncSession = null;\n        try {\n          asyncSession = sessionFuture.get(); // In case it was done and noone looked at it.\n        } catch (ExecutionException | CancellationException e) {\n          // ignore\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          // ignore\n        }\n        sessionFuture = null;\n        if (asyncSession != null) {\n          LOG.info(\"Closing Tez Session\");\n          closeClient(asyncSession);\n        }\n      }\n    } finally {\n      try {\n        cleanupScratchDir();\n      } finally {\n        if (!keepDagFilesDir) {\n          cleanupDagResources();\n        }\n      }\n    }\n  }"
        ]
    ],
    "a94f382a0f55e72a4ab0bbeabe7bf7bbac5b384d": [
        [
            "ObjectStore::getNextNotification(NotificationEventRequest)",
            "9087  \n9088  \n9089  \n9090  \n9091  \n9092  \n9093  \n9094  \n9095  \n9096  \n9097  \n9098  \n9099  \n9100  \n9101  \n9102  \n9103  \n9104  \n9105  \n9106 -\n9107 -\n9108 -\n9109  \n9110  \n9111  \n9112  \n9113  \n9114  \n9115  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n\n    NotificationEventResponse result = new NotificationEventResponse();\n    result.setEvents(new ArrayList<>());\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return result;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      int numEvents = 0;\n      while (i.hasNext() && numEvents++ < maxEvents) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }",
            "9087  \n9088  \n9089  \n9090  \n9091  \n9092  \n9093  \n9094  \n9095  \n9096  \n9097 +\n9098  \n9099  \n9100  \n9101 +\n9102  \n9103  \n9104  \n9105  \n9106  \n9107  \n9108 +\n9109  \n9110  \n9111  \n9112  \n9113  \n9114  \n9115  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n\n    NotificationEventResponse result = new NotificationEventResponse();\n    result.setEvents(new ArrayList<>());\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      query.setRange(0, maxEvents);\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return result;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      while (i.hasNext()) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ]
    ],
    "dfaf90f2b3a69477ea6b38144cf4ed55de9c4d95": [
        [
            "TestTxnCommands::testInsertOverwrite()",
            "  77  \n  78  \n  79  \n  80  \n  81 -\n  82  ",
            "  @Test//todo: what is this for?\n  public void testInsertOverwrite() throws Exception {\n    runStatementOnDriver(\"insert overwrite table \" + Table.NONACIDORCTBL + \" select a,b from \" + Table.NONACIDORCTBL2);\n    runStatementOnDriver(\"create table \" + Table.NONACIDORCTBL2 + \"3(a int, b int) clustered by (a) into \" + BUCKET_COUNT + \" buckets stored as orc TBLPROPERTIES ('transactional'='false')\");\n\n  }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97  ",
            "  /**\n   * tests that a failing Insert Overwrite (which creates a new base_x) is properly marked as\n   * aborted.\n   */\n  @Test\n  public void testInsertOverwrite() throws Exception {\n    runStatementOnDriver(\"insert overwrite table \" + Table.NONACIDORCTBL + \" select a,b from \" + Table.NONACIDORCTBL2);\n    runStatementOnDriver(\"create table \" + Table.NONACIDORCTBL2 + \"3(a int, b int) clustered by (a) into \" + BUCKET_COUNT + \" buckets stored as orc TBLPROPERTIES ('transactional'='false')\");\n    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \" values(1,2)\");\n    List<String> rs = runStatementOnDriver(\"select a from \" + Table.ACIDTBL + \" where b = 2\");\n    Assert.assertEquals(1, rs.size());\n    Assert.assertEquals(\"1\", rs.get(0));\n    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \" values(3,2)\");\n    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \" values(5,6)\");\n    rs = runStatementOnDriver(\"select a from \" + Table.ACIDTBL + \" order by a\");\n    Assert.assertEquals(2, rs.size());\n    Assert.assertEquals(\"1\", rs.get(0));\n    Assert.assertEquals(\"5\", rs.get(1));\n  }"
        ]
    ],
    "780b0127fd22ec95a6b225a493872bcec364ef76": [
        [
            "HiveMaterializedViewsRegistry::init()",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(HiveConf.ConfVars.METASTORE_FILTER_HOOK.varname,\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  /**\n   * Initialize the registry for the given database. It will extract the materialized views\n   * that are enabled for rewriting from the metastore for the current user, parse them,\n   * and register them in this cache.\n   *\n   * The loading process runs on the background; the method returns in the moment that the\n   * runnable task is created, thus the views will still not be loaded in the cache when\n   * it returns.\n   */\n  public void init() {\n    try {\n      // Create a new conf object to bypass metastore authorization, as we need to\n      // retrieve all materialized views from all databases\n      HiveConf conf = new HiveConf();\n      conf.set(MetastoreConf.ConfVars.FILTER_HOOK.getVarname(),\n          DefaultMetaStoreFilterHookImpl.class.getName());\n      init(Hive.get(conf));\n    } catch (HiveException e) {\n      LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n    }\n  }"
        ],
        [
            "HiveMaterializedViewsRegistry::Loader::run()",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    @Override\n    public void run() {\n      try {\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }",
            " 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "    @Override\n    public void run() {\n      try {\n        SessionState.start(db.getConf());\n        for (String dbName : db.getAllDatabases()) {\n          for (Table mv : db.getAllMaterializedViewObjects(dbName)) {\n            addMaterializedView(db.getConf(), mv, OpType.LOAD);\n          }\n        }\n        initialized.set(true);\n        LOG.info(\"Materialized views registry has been initialized\");\n      } catch (HiveException e) {\n        LOG.error(\"Problem connecting to the metastore when initializing the view registry\", e);\n      }\n    }"
        ],
        [
            "HiveMaterializedViewsRegistry::parseQuery(HiveConf,String)",
            " 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 -\n 417  \n 418  \n 419  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(e.getMessage());\n      return null;\n    }\n  }",
            " 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420  \n 421  \n 422  ",
            "  private static RelNode parseQuery(HiveConf conf, String viewQuery) {\n    try {\n      final ASTNode node = ParseUtils.parse(viewQuery);\n      final QueryState qs =\n          new QueryState.Builder().withHiveConf(conf).build();\n      CalcitePlanner analyzer = new CalcitePlanner(qs);\n      Context ctx = new Context(conf);\n      ctx.setIsLoadingMaterializedView(true);\n      analyzer.initCtx(ctx);\n      analyzer.init(false);\n      return analyzer.genLogicalPlan(node);\n    } catch (Exception e) {\n      // We could not parse the view\n      LOG.error(\"Error parsing original query for materialized view\", e);\n      return null;\n    }\n  }"
        ]
    ],
    "5ec298fd42f730faebdaf61e711abb439edd78e6": [
        [
            "HiveSchemaTool::validateSchemaTables(Connection)",
            " 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 -\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729 -\n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763 -\n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778 -\n 779 -\n 780  \n 781  \n 782  \n 783  \n 784 -\n 785  \n 786  \n 787  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = getMetaStoreSchemaVersion(hmsConn);\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      LOG.error(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.info(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, null, \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = baseDir  + \"/\" + dbType + \"/hive-schema-\" + version + \".\" + dbType + \".sql\";\n\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.info(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Schema table validation failed!!!\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    int schemaSize = schemaTables.size();\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      System.out.println(\"Found \" + schemaSize + \" tables in schema definition, \" +\n          schemaTables.size() + \" tables [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Schema table validation failed!!!\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation. \" + schemaSize + \" tables matched\");\n      return true;\n    }\n  }",
            " 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739 +\n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746 +\n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 +\n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795 +\n 796  \n 797  \n 798  \n 799  \n 800 +\n 801  \n 802  \n 803  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = getMetaStoreSchemaVersion(hmsConn);\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      LOG.debug(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.debug(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, null, \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = baseDir  + \"/\" + dbType + \"/hive-schema-\" + version + \".\" + dbType + \".sql\";\n\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.debug(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Schema table validation failed!!!\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    int schemaSize = schemaTables.size();\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      System.out.println(\"Table(s) [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Schema table validation failed!!!\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation.\");\n      return true;\n    }\n  }"
        ],
        [
            "HiveSchemaTool::doValidate()",
            " 595  \n 596 -\n 597  \n 598  \n 599 -\n 600  \n 601 -\n 602  \n 603 -\n 604  \n 605 -\n 606  \n 607 -\n 608  \n 609 -\n 610  \n 611 -\n 612  \n 613 -\n 614  \n 615 -\n 616  \n 617 -\n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629 -\n 630  ",
            "  public void doValidate() throws HiveMetaException {\n    System.out.println(\"Starting metastore validation\");\n    Connection conn = getConnectionToMetastore(false);\n    try {\n      if (validateSchemaVersions(conn))\n        System.out.println(\"[SUCCESS]\\n\");\n      else\n        System.out.println(\"[FAIL]\\n\");\n      if (validateSequences(conn))\n        System.out.println(\"[SUCCESS]\\n\");\n      else\n        System.out.println(\"[FAIL]\\n\");\n      if (validateSchemaTables(conn))\n        System.out.println(\"[SUCCESS]\\n\");\n      else\n        System.out.println(\"[FAIL]\\n\");\n      if (validateLocations(conn, this.validationServers))\n        System.out.println(\"[SUCCESS]\\n\");\n      else\n        System.out.println(\"[FAIL]\\n\");\n      if (validateColumnNullValues(conn))\n        System.out.println(\"[SUCCESS]\\n\");\n      else\n        System.out.println(\"[FAIL]\\n\");\n    } finally {\n      if (conn != null) {\n        try {\n          conn.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close metastore connection\", e);\n        }\n      }\n    }\n\n    System.out.println(\"Done with metastore validation\");\n  }",
            " 595  \n 596 +\n 597  \n 598 +\n 599  \n 600 +\n 601  \n 602 +\n 603 +\n 604  \n 605 +\n 606 +\n 607  \n 608 +\n 609 +\n 610  \n 611 +\n 612 +\n 613  \n 614 +\n 615 +\n 616  \n 617 +\n 618 +\n 619  \n 620 +\n 621 +\n 622  \n 623 +\n 624 +\n 625  \n 626 +\n 627 +\n 628  \n 629 +\n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640 +\n 641 +\n 642 +\n 643 +\n 644 +\n 645 +\n 646 +\n 647  ",
            "  public void doValidate() throws HiveMetaException {\n    System.out.println(\"Starting metastore validation\\n\");\n    Connection conn = getConnectionToMetastore(false);\n    boolean success = true;\n    try {\n      if (validateSchemaVersions(conn)) {\n        System.out.println(\"[SUCCESS]\\n\");\n      } else {\n        success = false;\n        System.out.println(\"[FAIL]\\n\");\n      }\n      if (validateSequences(conn)) {\n        System.out.println(\"[SUCCESS]\\n\");\n      } else {\n        success = false;\n        System.out.println(\"[FAIL]\\n\");\n      }\n      if (validateSchemaTables(conn)) {\n        System.out.println(\"[SUCCESS]\\n\");\n      } else {\n        success = false;\n        System.out.println(\"[FAIL]\\n\");\n      }\n      if (validateLocations(conn, this.validationServers)) {\n        System.out.println(\"[SUCCESS]\\n\");\n      } else {\n        success = false;\n        System.out.println(\"[FAIL]\\n\");\n      }\n      if (validateColumnNullValues(conn)) {\n        System.out.println(\"[SUCCESS]\\n\");\n      } else {\n        success = false;\n        System.out.println(\"[FAIL]\\n\");\n      }\n    } finally {\n      if (conn != null) {\n        try {\n          conn.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close metastore connection\", e);\n        }\n      }\n    }\n\n    System.out.print(\"Done with metastore validation: \");\n    if (!success) {\n      System.out.println(\"[FAIL]\");\n      System.exit(1);\n    } else {\n      System.out.println(\"[SUCCESS]\");\n    }\n  }"
        ]
    ],
    "5616cbe40587c7cd218b15645c8563e6b4c6662e": [
        [
            "LlapServiceDriver::run(String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 -\n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.config.Log4j1ConfigurationFactory.class,\n              io.netty.util.NetUtil.class, // netty4\n              org.jboss.netty.util.NetUtil.class //netty3\n              };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 +\n 404 +\n 405 +\n 406 +\n 407 +\n 408 +\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  ",
            "  private int run(String[] args) throws Exception {\n    LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();\n    final LlapOptions options = optionsProcessor.processOptions(args);\n\n    final Properties propsDirectOptions = new Properties();\n\n    if (options == null) {\n      // help\n      return 1;\n    }\n\n    // Working directory.\n    Path tmpDir = new Path(options.getDirectory());\n\n    if (conf == null) {\n      throw new Exception(\"Cannot load any configuration to run command\");\n    }\n\n    final long t0 = System.nanoTime();\n\n    final FileSystem fs = FileSystem.get(conf);\n    final FileSystem lfs = FileSystem.getLocal(conf).getRawFileSystem();\n\n    int threadCount = Math.max(1, Runtime.getRuntime().availableProcessors() / 2);\n    final ExecutorService executor = Executors.newFixedThreadPool(threadCount,\n            new ThreadFactoryBuilder().setNameFormat(\"llap-pkg-%d\").build());\n    final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);\n\n    int rc = 0;\n    try {\n\n      // needed so that the file is actually loaded into configuration.\n      for (String f : NEEDED_CONFIGS) {\n        conf.addResource(f);\n        if (conf.getResource(f) == null) {\n          throw new Exception(\"Unable to find required config file: \" + f);\n        }\n      }\n      for (String f : OPTIONAL_CONFIGS) {\n        conf.addResource(f);\n      }\n\n      conf.reloadConfiguration();\n\n      populateConfWithLlapProperties(conf, options.getConfig());\n\n      if (options.getName() != null) {\n        // update service registry configs - caveat: this has nothing to do with the actual settings\n        // as read by the AM\n        // if needed, use --hiveconf llap.daemon.service.hosts=@llap0 to dynamically switch between\n        // instances\n        conf.set(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, \"@\" + options.getName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname,\n            \"@\" + options.getName());\n      }\n\n      if (options.getLogger() != null) {\n        HiveConf.setVar(conf, ConfVars.LLAP_DAEMON_LOGGER, options.getLogger());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_LOGGER.varname, options.getLogger());\n      }\n      boolean isDirect = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT);\n\n      if (options.getSize() != -1) {\n        if (options.getCache() != -1) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED) == false) {\n            // direct heap allocations need to be safer\n            Preconditions.checkArgument(options.getCache() < options.getSize(), \"Cache size (\"\n                + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller\"\n                + \" than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n          } else if (options.getCache() < options.getSize()) {\n            LOG.warn(\"Note that this might need YARN physical memory monitoring to be turned off \"\n                + \"(yarn.nodemanager.pmem-check-enabled=false)\");\n          }\n        }\n        if (options.getXmx() != -1) {\n          Preconditions.checkArgument(options.getXmx() < options.getSize(), \"Working memory (Xmx=\"\n              + LlapUtil.humanReadableByteCount(options.getXmx()) + \") has to be\"\n              + \" smaller than the container sizing (\" + LlapUtil.humanReadableByteCount(options.getSize())\n              + \")\");\n        }\n        if (isDirect && !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MAPPED)) {\n          // direct and not memory mapped\n          Preconditions.checkArgument(options.getXmx() + options.getCache() <= options.getSize(),\n            \"Working memory (Xmx=\" + LlapUtil.humanReadableByteCount(options.getXmx()) + \") + cache size (\"\n              + LlapUtil.humanReadableByteCount(options.getCache()) + \") has to be smaller than the container sizing (\"\n              + LlapUtil.humanReadableByteCount(options.getSize()) + \")\");\n        }\n      }\n\n\n      if (options.getExecutors() != -1) {\n        conf.setLong(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname, options.getExecutors());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,\n            String.valueOf(options.getExecutors()));\n        // TODO: vcpu settings - possibly when DRFA works right\n      }\n\n      if (options.getIoThreads() != -1) {\n        conf.setLong(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname, options.getIoThreads());\n        propsDirectOptions.setProperty(ConfVars.LLAP_IO_THREADPOOL_SIZE.varname,\n            String.valueOf(options.getIoThreads()));\n      }\n\n      long cache = -1, xmx = -1;\n      if (options.getCache() != -1) {\n        cache = options.getCache();\n        conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, Long.toString(cache));\n        propsDirectOptions.setProperty(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,\n            Long.toString(cache));\n      }\n\n      if (options.getXmx() != -1) {\n        // Needs more explanation here\n        // Xmx is not the max heap value in JDK8. You need to subtract 50% of the survivor fraction\n        // from this, to get actual usable memory before it goes into GC\n        xmx = options.getXmx();\n        long xmxMb = (xmx / (1024L * 1024L));\n        conf.setLong(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname, xmxMb);\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,\n            String.valueOf(xmxMb));\n      }\n\n      long size = options.getSize();\n      if (size == -1) {\n        long heapSize = xmx;\n        if (!isDirect) {\n          heapSize += cache;\n        }\n        size = Math.min((long)(heapSize * 1.2), heapSize + 1024L*1024*1024);\n        if (isDirect) {\n          size += cache;\n        }\n      }\n      long containerSize = size / (1024 * 1024);\n      final long minAlloc = conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1);\n      Preconditions.checkArgument(containerSize >= minAlloc, \"Container size (\"\n          + LlapUtil.humanReadableByteCount(options.getSize()) + \") should be greater\"\n          + \" than minimum allocation(\" + LlapUtil.humanReadableByteCount(minAlloc * 1024L * 1024L) + \")\");\n      conf.setLong(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);\n      propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,\n          String.valueOf(containerSize));\n\n      LOG.info(\"Memory settings: container memory: {} executor memory: {} cache memory: {}\",\n        LlapUtil.humanReadableByteCount(options.getSize()),\n        LlapUtil.humanReadableByteCount(options.getXmx()),\n        LlapUtil.humanReadableByteCount(options.getCache()));\n\n      if (options.getLlapQueueName() != null && !options.getLlapQueueName().isEmpty()) {\n        conf.set(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname, options.getLlapQueueName());\n        propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,\n            options.getLlapQueueName());\n      }\n\n      final URL logger = conf.getResource(LlapConstants.LOG4j2_PROPERTIES_FILE);\n\n      if (null == logger) {\n        throw new Exception(\"Unable to find required config file: llap-daemon-log4j2.properties\");\n      }\n\n      Path home = new Path(System.getenv(\"HIVE_HOME\"));\n      Path scriptParent = new Path(new Path(home, \"scripts\"), \"llap\");\n      Path scripts = new Path(scriptParent, \"bin\");\n\n      if (!lfs.exists(home)) {\n        throw new Exception(\"Unable to find HIVE_HOME:\" + home);\n      } else if (!lfs.exists(scripts)) {\n        LOG.warn(\"Unable to find llap scripts:\" + scripts);\n      }\n\n      final Path libDir = new Path(tmpDir, \"lib\");\n      final Path tezDir = new Path(libDir, \"tez\");\n      final Path udfDir = new Path(libDir, \"udfs\");\n      final Path confPath = new Path(tmpDir, \"conf\");\n      if (!lfs.mkdirs(confPath)) {\n        LOG.warn(\"mkdirs for \" + confPath + \" returned false\");\n      }\n      if (!lfs.mkdirs(tezDir)) {\n        LOG.warn(\"mkdirs for \" + tezDir + \" returned false\");\n      }\n      if (!lfs.mkdirs(udfDir)) {\n        LOG.warn(\"mkdirs for \" + udfDir + \" returned false\");\n      }\n\n      NamedCallable<Void> downloadTez = new NamedCallable<Void>(\"downloadTez\") {\n        @Override\n        public Void call() throws Exception {\n          synchronized (fs) {\n            String tezLibs = conf.get(TezConfiguration.TEZ_LIB_URIS);\n            if (tezLibs == null) {\n              LOG.warn(\"Missing tez.lib.uris in tez-site.xml\");\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying tez libs from \" + tezLibs);\n            }\n            lfs.mkdirs(tezDir);\n            fs.copyToLocalFile(new Path(tezLibs), new Path(libDir, \"tez.tar.gz\"));\n            CompressionUtils.unTar(new Path(libDir, \"tez.tar.gz\").toString(), tezDir.toString(),\n                true);\n            lfs.delete(new Path(libDir, \"tez.tar.gz\"), false);\n          }\n          return null;\n        }\n      };\n\n      NamedCallable<Void> copyLocalJars = new NamedCallable<Void>(\"copyLocalJars\") {\n        @Override\n        public Void call() throws Exception {\n          Class<?>[] dependencies = new Class<?>[] { LlapDaemonProtocolProtos.class, // llap-common\n              LlapTezUtils.class, // llap-tez\n              LlapInputFormat.class, // llap-server\n              HiveInputFormat.class, // hive-exec\n              SslContextFactory.class, // hive-common (https deps)\n              Rule.class, // Jetty rewrite class\n              RegistryUtils.ServiceRecordMarshal.class, // ZK registry\n              // log4j2\n              com.lmax.disruptor.RingBuffer.class, // disruptor\n              org.apache.logging.log4j.Logger.class, // log4j-api\n              org.apache.logging.log4j.core.Appender.class, // log4j-core\n              org.apache.logging.slf4j.Log4jLogger.class, // log4j-slf4j\n              // log4j-1.2-API needed for NDC\n              org.apache.log4j.config.Log4j1ConfigurationFactory.class,\n              io.netty.util.NetUtil.class, // netty4\n              org.jboss.netty.util.NetUtil.class, //netty3\n              org.apache.arrow.vector.types.pojo.ArrowType.class, //arrow-vector\n              org.apache.arrow.memory.BaseAllocator.class, //arrow-memory\n              org.apache.arrow.flatbuf.Schema.class, //arrow-format\n              com.google.flatbuffers.Table.class, //flatbuffers\n              com.carrotsearch.hppc.ByteArrayDeque.class //hppc\n              };\n\n          for (Class<?> c : dependencies) {\n            Path jarPath = new Path(Utilities.jarFinderGetJar(c));\n            lfs.copyFromLocalFile(jarPath, libDir);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Copying \" + jarPath + \" to \" + libDir);\n            }\n          }\n          return null;\n        }\n      };\n\n      // copy default aux classes (json/hbase)\n\n      NamedCallable<Void> copyAuxJars = new NamedCallable<Void>(\"copyAuxJars\") {\n        @Override\n        public Void call() throws Exception {\n          for (String className : DEFAULT_AUX_CLASSES) {\n            localizeJarForClass(lfs, libDir, className, false);\n          }\n          Collection<String> codecs = conf.getStringCollection(\"io.compression.codecs\");\n          if (codecs != null) {\n            for (String codecClassName : codecs) {\n              localizeJarForClass(lfs, libDir, codecClassName, false);\n            }\n          }\n\n          if (options.getIsHBase()) {\n            try {\n              localizeJarForClass(lfs, libDir, HBASE_SERDE_CLASS, true);\n              Job fakeJob = new Job(new JobConf()); // HBase API is convoluted.\n              TableMapReduceUtil.addDependencyJars(fakeJob);\n              Collection<String> hbaseJars =\n                  fakeJob.getConfiguration().getStringCollection(\"tmpjars\");\n              for (String jarPath : hbaseJars) {\n                if (!jarPath.isEmpty()) {\n                  lfs.copyFromLocalFile(new Path(jarPath), libDir);\n                }\n              }\n            } catch (Throwable t) {\n              String err =\n                  \"Failed to add HBase jars. Use --auxhbase=false to avoid localizing them\";\n              LOG.error(err);\n              System.err.println(err);\n              throw new RuntimeException(t);\n            }\n          }\n\n          HashSet<String> auxJars = new HashSet<>();\n          // There are many ways to have AUX jars in Hive... sigh\n          if (options.getIsHiveAux()) {\n            // Note: we don't add ADDED jars, RELOADABLE jars, etc. That is by design; there are too many ways\n            // to add jars in Hive, some of which are session/etc. specific. Env + conf + arg should be enough.\n            addAuxJarsToSet(auxJars, conf.getAuxJars(), \",\");\n            addAuxJarsToSet(auxJars, System.getenv(\"HIVE_AUX_JARS_PATH\"), \":\");\n            LOG.info(\"Adding the following aux jars from the environment and configs: \" + auxJars);\n          }\n\n          addAuxJarsToSet(auxJars, options.getAuxJars(), \",\");\n          for (String jarPath : auxJars) {\n            lfs.copyFromLocalFile(new Path(jarPath), libDir);\n          }\n          return null;\n        }\n\n        private void addAuxJarsToSet(HashSet<String> auxJarSet, String auxJars, String delimiter) {\n          if (auxJars != null && !auxJars.isEmpty()) {\n            // TODO: transitive dependencies warning?\n            String[] jarPaths = auxJars.split(delimiter);\n            for (String jarPath : jarPaths) {\n              if (!jarPath.isEmpty()) {\n                auxJarSet.add(jarPath);\n              }\n            }\n          }\n        }\n      };\n\n      NamedCallable<Void> copyUdfJars = new NamedCallable<Void>(\"copyUdfJars\") {\n        @Override\n        public Void call() throws Exception {\n          // UDFs\n          final Set<String> allowedUdfs;\n\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOW_PERMANENT_FNS)) {\n            synchronized (fs) {\n              allowedUdfs = downloadPermanentFunctions(conf, udfDir);\n            }\n          } else {\n            allowedUdfs = Collections.emptySet();\n          }\n\n          PrintWriter udfStream =\n              new PrintWriter(lfs.create(new Path(confPath,\n                  StaticPermanentFunctionChecker.PERMANENT_FUNCTIONS_LIST)));\n          for (String udfClass : allowedUdfs) {\n            udfStream.println(udfClass);\n          }\n\n          udfStream.close();\n          return null;\n        }\n      };\n\n      String java_home;\n      if (options.getJavaPath() == null || options.getJavaPath().isEmpty()) {\n        java_home = System.getenv(\"JAVA_HOME\");\n        String jre_home = System.getProperty(\"java.home\");\n        if (java_home == null) {\n          java_home = jre_home;\n        } else if (!java_home.equals(jre_home)) {\n          LOG.warn(\"Java versions might not match : JAVA_HOME=[{}],process jre=[{}]\", java_home,\n              jre_home);\n        }\n      } else {\n        java_home = options.getJavaPath();\n      }\n      if (java_home == null || java_home.isEmpty()) {\n        throw new RuntimeException(\n            \"Could not determine JAVA_HOME from command line parameters, environment or system properties\");\n      }\n      LOG.info(\"Using [{}] for JAVA_HOME\", java_home);\n\n      NamedCallable<Void> copyConfigs = new NamedCallable<Void>(\"copyConfigs\") {\n        @Override\n        public Void call() throws Exception {\n          // Copy over the mandatory configs for the package.\n          for (String f : NEEDED_CONFIGS) {\n            copyConfig(lfs, confPath, f);\n          }\n          for (String f : OPTIONAL_CONFIGS) {\n            try {\n              copyConfig(lfs, confPath, f);\n            } catch (Throwable t) {\n              LOG.info(\"Error getting an optional config \" + f + \"; ignoring: \" + t.getMessage());\n            }\n          }\n          createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());\n          setUpLogAndMetricConfigs(lfs, logger, confPath);\n          return null;\n        }\n      };\n\n      @SuppressWarnings(\"unchecked\")\n      final NamedCallable<Void>[] asyncWork =\n          new NamedCallable[] {\n          downloadTez,\n          copyUdfJars,\n          copyLocalJars,\n          copyAuxJars,\n          copyConfigs };\n      @SuppressWarnings(\"unchecked\")\n      final Future<Void>[] asyncResults = new Future[asyncWork.length];\n      for (int i = 0; i < asyncWork.length; i++) {\n        asyncResults[i] = asyncRunner.submit(asyncWork[i]);\n      }\n\n      // TODO: need to move from Python to Java for the rest of the script.\n      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);\n      writeConfigJson(tmpDir, lfs, configs);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Config generation took \" + (System.nanoTime() - t0) + \" ns\");\n      }\n      for (int i = 0; i < asyncWork.length; i++) {\n        final long t1 = System.nanoTime();\n        asyncResults[i].get();\n        final long t2 = System.nanoTime();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(asyncWork[i].getName() + \" waited for \" + (t2 - t1) + \" ns\");\n        }\n      }\n      if (options.isStarting()) {\n        String version = System.getenv(\"HIVE_VERSION\");\n        if (version == null || version.isEmpty()) {\n          version = DateTime.now().toString(\"ddMMMyyyy\");\n        }\n\n        String outputDir = options.getOutput();\n        Path packageDir = null;\n        if (outputDir == null) {\n          outputDir = OUTPUT_DIR_PREFIX + version;\n          packageDir = new Path(Paths.get(\".\").toAbsolutePath().toString(),\n              OUTPUT_DIR_PREFIX + version);\n        } else {\n          packageDir = new Path(outputDir);\n        }\n        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);\n        if (rc == 0) {\n          LlapSliderUtils.startCluster(conf, options.getName(), \"llap-\" + version + \".zip\",\n              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));\n        }\n      } else {\n        rc = 0;\n      }\n    } finally {\n      executor.shutdown();\n      lfs.close();\n      fs.close();\n    }\n\n    if (rc == 0) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Exiting successfully\");\n      }\n    } else {\n      LOG.info(\"Exiting with rc = \" + rc);\n    }\n    return rc;\n  }"
        ]
    ],
    "607077a906fa5157538ac74dd4be38c3ece33aa0": [
        [
            "MetaStoreUtils::getSchemaWithoutCols(org,org,Map,String,String,List)",
            "1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119 -\n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135 -\n1136 -\n1137 -\n1138 -\n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  ",
            "  public static Properties getSchemaWithoutCols(org.apache.hadoop.hive.metastore.api.StorageDescriptor sd,\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor tblsd,\n      Map<String, String> parameters, String databaseName, String tableName,\n      List<FieldSchema> partitionKeys) {\n    Properties schema = new Properties();\n    String inputFormat = sd.getInputFormat();\n    if (inputFormat == null || inputFormat.length() == 0) {\n      inputFormat = org.apache.hadoop.mapred.SequenceFileInputFormat.class\n        .getName();\n    }\n    schema.setProperty(\n      org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT,\n      inputFormat);\n    String outputFormat = sd.getOutputFormat();\n    if (outputFormat == null || outputFormat.length() == 0) {\n      outputFormat = org.apache.hadoop.mapred.SequenceFileOutputFormat.class\n        .getName();\n    }\n    schema.setProperty(\n      org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT,\n      outputFormat);\n\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,\n        databaseName + \".\" + tableName);\n\n    if (sd.getLocation() != null) {\n      schema.setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_LOCATION,\n          sd.getLocation());\n    }\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_COUNT, Integer\n            .toString(sd.getNumBuckets()));\n    if (sd.getBucketCols() != null && sd.getBucketCols().size() > 0) {\n      schema.setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_FIELD_NAME, sd\n              .getBucketCols().get(0));\n    }\n    if (sd.getSerdeInfo() != null) {\n      for (Map.Entry<String,String> param : sd.getSerdeInfo().getParameters().entrySet()) {\n        schema.put(param.getKey(), (param.getValue() != null) ? param.getValue() : \"\");\n      }\n\n      if (sd.getSerdeInfo().getSerializationLib() != null) {\n        schema.setProperty(\n            org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB, sd\n                .getSerdeInfo().getSerializationLib());\n      }\n    }\n\n    if (sd.getCols() != null) {\n      schema.setProperty(\n          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_DDL,\n          getDDLFromFieldSchema(tableName, sd.getCols()));\n    }\n\n    String partString = \"\";\n    String partStringSep = \"\";\n    String partTypesString = \"\";\n    String partTypesStringSep = \"\";\n    for (FieldSchema partKey : partitionKeys) {\n      partString = partString.concat(partStringSep);\n      partString = partString.concat(partKey.getName());\n      partTypesString = partTypesString.concat(partTypesStringSep);\n      partTypesString = partTypesString.concat(partKey.getType());\n      if (partStringSep.length() == 0) {\n        partStringSep = \"/\";\n        partTypesStringSep = \":\";\n      }\n    }\n    if (partString.length() > 0) {\n      schema\n          .setProperty(\n              org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS,\n              partString);\n      schema\n      .setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMN_TYPES,\n          partTypesString);\n    }\n\n    if (parameters != null) {\n      for (Entry<String, String> e : parameters.entrySet()) {\n        // add non-null parameters to the schema\n        if ( e.getValue() != null) {\n          schema.setProperty(e.getKey(), e.getValue());\n        }\n      }\n    }\n\n    return schema;\n  }",
            "1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104 +\n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120 +\n1121 +\n1122 +\n1123 +\n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  ",
            "  public static Properties getSchemaWithoutCols(org.apache.hadoop.hive.metastore.api.StorageDescriptor sd,\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor tblsd,\n      Map<String, String> parameters, String databaseName, String tableName,\n      List<FieldSchema> partitionKeys) {\n    Properties schema = new Properties();\n    String inputFormat = sd.getInputFormat();\n    if (inputFormat == null || inputFormat.length() == 0) {\n      inputFormat = org.apache.hadoop.mapred.SequenceFileInputFormat.class\n        .getName();\n    }\n    schema.setProperty(\n      org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT,\n      inputFormat);\n    String outputFormat = sd.getOutputFormat();\n    if (outputFormat == null || outputFormat.length() == 0) {\n      outputFormat = org.apache.hadoop.mapred.SequenceFileOutputFormat.class\n        .getName();\n    }\n    schema.setProperty(\n      org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT,\n      outputFormat);\n\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_NAME,\n        databaseName + \".\" + tableName);\n\n    if (sd.getLocation() != null) {\n      schema.setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_LOCATION,\n          sd.getLocation());\n    }\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_COUNT, Integer\n            .toString(sd.getNumBuckets()));\n    if (sd.getBucketCols() != null && sd.getBucketCols().size() > 0) {\n      schema.setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_FIELD_NAME, sd\n              .getBucketCols().get(0));\n    }\n    if (sd.getSerdeInfo() != null) {\n      for (Map.Entry<String,String> param : sd.getSerdeInfo().getParameters().entrySet()) {\n        schema.put(param.getKey(), (param.getValue() != null) ? param.getValue() : StringUtils.EMPTY);\n      }\n\n      if (sd.getSerdeInfo().getSerializationLib() != null) {\n        schema.setProperty(\n            org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB, sd\n                .getSerdeInfo().getSerializationLib());\n      }\n    }\n\n    if (sd.getCols() != null) {\n      schema.setProperty(\n          org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_DDL,\n          getDDLFromFieldSchema(tableName, sd.getCols()));\n    }\n\n    String partString = StringUtils.EMPTY;\n    String partStringSep = StringUtils.EMPTY;\n    String partTypesString = StringUtils.EMPTY;\n    String partTypesStringSep = StringUtils.EMPTY;\n    for (FieldSchema partKey : partitionKeys) {\n      partString = partString.concat(partStringSep);\n      partString = partString.concat(partKey.getName());\n      partTypesString = partTypesString.concat(partTypesStringSep);\n      partTypesString = partTypesString.concat(partKey.getType());\n      if (partStringSep.length() == 0) {\n        partStringSep = \"/\";\n        partTypesStringSep = \":\";\n      }\n    }\n    if (partString.length() > 0) {\n      schema\n          .setProperty(\n              org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS,\n              partString);\n      schema\n      .setProperty(\n          org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMN_TYPES,\n          partTypesString);\n    }\n\n    if (parameters != null) {\n      for (Entry<String, String> e : parameters.entrySet()) {\n        // add non-null parameters to the schema\n        if ( e.getValue() != null) {\n          schema.setProperty(e.getKey(), e.getValue());\n        }\n      }\n    }\n\n    return schema;\n  }"
        ],
        [
            "MetaStoreUtils::addToClassPath(ClassLoader,String)",
            "1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882 -\n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  ",
            "  /**\n   * Add new elements to the classpath.\n   *\n   * @param newPaths\n   *          Array of classpath elements\n   */\n  public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {\n    URLClassLoader loader = (URLClassLoader) cloader;\n    List<URL> curPath = Arrays.asList(loader.getURLs());\n    ArrayList<URL> newPath = new ArrayList<URL>();\n\n    // get a list with the current classpath components\n    for (URL onePath : curPath) {\n      newPath.add(onePath);\n    }\n    curPath = newPath;\n\n    for (String onestr : newPaths) {\n      URL oneurl = urlFromPathString(onestr);\n      if (oneurl != null && !curPath.contains(oneurl)) {\n        curPath.add(oneurl);\n      }\n    }\n\n    return new URLClassLoader(curPath.toArray(new URL[0]), loader);\n  }",
            "1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850 +\n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  ",
            "  /**\n   * Add new elements to the classpath.\n   *\n   * @param newPaths\n   *          Array of classpath elements\n   */\n  public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {\n    URLClassLoader loader = (URLClassLoader) cloader;\n    List<URL> curPath = Arrays.asList(loader.getURLs());\n    ArrayList<URL> newPath = new ArrayList<URL>(curPath.size());\n\n    // get a list with the current classpath components\n    for (URL onePath : curPath) {\n      newPath.add(onePath);\n    }\n    curPath = newPath;\n\n    for (String onestr : newPaths) {\n      URL oneurl = urlFromPathString(onestr);\n      if (oneurl != null && !curPath.contains(oneurl)) {\n        curPath.add(oneurl);\n      }\n    }\n\n    return new URLClassLoader(curPath.toArray(new URL[0]), loader);\n  }"
        ],
        [
            "MetaStoreUtils::compareFieldColumns(List,List)",
            "1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743 -\n1744 -\n1745 -\n1746  \n1747  \n1748 -\n1749 -\n1750 -\n1751 -\n1752 -\n1753 -\n1754 -\n1755 -\n1756 -\n1757 -\n1758 -\n1759 -\n1760  \n1761  \n1762  \n1763  \n1764  ",
            "  /**\n   * @param schema1: The first schema to be compared\n   * @param schema2: The second schema to be compared\n   * @return true if the two schemas are the same else false\n   *         for comparing a field we ignore the comment it has\n   */\n  public static boolean compareFieldColumns(List<FieldSchema> schema1, List<FieldSchema> schema2) {\n    if (schema1.size() != schema2.size()) {\n      return false;\n    }\n    for (int i = 0; i < schema1.size(); i++) {\n      FieldSchema f1 = schema1.get(i);\n      FieldSchema f2 = schema2.get(i);\n      // The default equals provided by thrift compares the comments too for\n      // equality, thus we need to compare the relevant fields here.\n      if (f1.getName() == null) {\n        if (f2.getName() != null) {\n          return false;\n        }\n      } else if (!f1.getName().equals(f2.getName())) {\n        return false;\n      }\n      if (f1.getType() == null) {\n        if (f2.getType() != null) {\n          return false;\n        }\n      } else if (!f1.getType().equals(f2.getType())) {\n        return false;\n      }\n    }\n    return true;\n  }",
            "1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723 +\n1724 +\n1725 +\n1726 +\n1727 +\n1728  \n1729  \n1730 +\n1731 +\n1732  \n1733  \n1734  \n1735  \n1736  ",
            "  /**\n   * @param schema1: The first schema to be compared\n   * @param schema2: The second schema to be compared\n   * @return true if the two schemas are the same else false\n   *         for comparing a field we ignore the comment it has\n   */\n  public static boolean compareFieldColumns(List<FieldSchema> schema1, List<FieldSchema> schema2) {\n    if (schema1.size() != schema2.size()) {\n      return false;\n    }\n    Iterator<FieldSchema> its1 = schema1.iterator();\n    Iterator<FieldSchema> its2 = schema2.iterator();\n    while (its1.hasNext()) {\n      FieldSchema f1 = its1.next();\n      FieldSchema f2 = its2.next();\n      // The default equals provided by thrift compares the comments too for\n      // equality, thus we need to compare the relevant fields here.\n      if (!StringUtils.equals(f1.getName(), f2.getName()) ||\n          !StringUtils.equals(f1.getType(), f2.getType())) {\n        return false;\n      }\n    }\n    return true;\n  }"
        ],
        [
            "MetaStoreUtils::getColumnNames(List)",
            "1972  \n1973 -\n1974  \n1975  \n1976  \n1977  \n1978  ",
            "  public static List<String> getColumnNames(List<FieldSchema> schema) {\n    List<String> cols = new ArrayList<>();\n    for (FieldSchema fs : schema) {\n      cols.add(fs.getName());\n    }\n    return cols;\n  }",
            "1940  \n1941 +\n1942  \n1943  \n1944  \n1945  \n1946  ",
            "  public static List<String> getColumnNames(List<FieldSchema> schema) {\n    List<String> cols = new ArrayList<>(schema.size());\n    for (FieldSchema fs : schema) {\n      cols.add(fs.getName());\n    }\n    return cols;\n  }"
        ],
        [
            "MetaStoreUtils::validateSkewedColNamesSubsetCol(List,List)",
            " 739  \n 740  \n 741 -\n 742  \n 743  \n 744 -\n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  ",
            "  public static String validateSkewedColNamesSubsetCol(List<String> skewedColNames,\n      List<FieldSchema> cols) {\n    if (null == skewedColNames) {\n      return null;\n    }\n    List<String> colNames = new ArrayList<String>();\n    for (FieldSchema fieldSchema : cols) {\n      colNames.add(fieldSchema.getName());\n    }\n    // make a copy\n    List<String> copySkewedColNames = new ArrayList<String>(skewedColNames);\n    // remove valid columns\n    copySkewedColNames.removeAll(colNames);\n    if (copySkewedColNames.isEmpty()) {\n      return null;\n    }\n    return copySkewedColNames.toString();\n  }",
            " 724  \n 725  \n 726 +\n 727  \n 728  \n 729 +\n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  ",
            "  public static String validateSkewedColNamesSubsetCol(List<String> skewedColNames,\n      List<FieldSchema> cols) {\n    if (CollectionUtils.isEmpty(skewedColNames)) {\n      return null;\n    }\n    List<String> colNames = new ArrayList<String>(cols.size());\n    for (FieldSchema fieldSchema : cols) {\n      colNames.add(fieldSchema.getName());\n    }\n    // make a copy\n    List<String> copySkewedColNames = new ArrayList<String>(skewedColNames);\n    // remove valid columns\n    copySkewedColNames.removeAll(colNames);\n    if (copySkewedColNames.isEmpty()) {\n      return null;\n    }\n    return copySkewedColNames.toString();\n  }"
        ],
        [
            "MetaStoreUtils::getPartSchemaFromTableSchema(org,org,Map,String,String,List,Properties)",
            " 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024 -\n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  ",
            "  /**\n   * Get partition level schema from table level schema.\n   * This function will use the same column names, column types and partition keys for\n   * each partition Properties. Their values are copied from the table Properties. This\n   * is mainly to save CPU and memory. CPU is saved because the first time the\n   * StorageDescriptor column names are accessed, JDO needs to execute a SQL query to\n   * retrieve the data. If we know the data will be the same as the table level schema\n   * and they are immutable, we should just reuse the table level schema objects.\n   *\n   * @param sd The Partition level Storage Descriptor.\n   * @param tblsd The Table level Storage Descriptor.\n   * @param parameters partition level parameters\n   * @param databaseName DB name\n   * @param tableName table name\n   * @param partitionKeys partition columns\n   * @param tblSchema The table level schema from which this partition should be copied.\n   * @return the properties\n   */\n  public static Properties getPartSchemaFromTableSchema(\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor sd,\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor tblsd,\n      Map<String, String> parameters, String databaseName, String tableName,\n      List<FieldSchema> partitionKeys,\n      Properties tblSchema) {\n\n    // Inherent most properties from table level schema and overwrite some properties\n    // in the following code.\n    // This is mainly for saving CPU and memory to reuse the column names, types and\n    // partition columns in the table level schema.\n    Properties schema = (Properties) tblSchema.clone();\n\n    // InputFormat\n    String inputFormat = sd.getInputFormat();\n    if (inputFormat == null || inputFormat.length() == 0) {\n      String tblInput =\n        schema.getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT);\n      if (tblInput == null) {\n        inputFormat = org.apache.hadoop.mapred.SequenceFileInputFormat.class.getName();\n      } else {\n        inputFormat = tblInput;\n      }\n    }\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT,\n        inputFormat);\n\n    // OutputFormat\n    String outputFormat = sd.getOutputFormat();\n    if (outputFormat == null || outputFormat.length() == 0) {\n      String tblOutput =\n        schema.getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT);\n      if (tblOutput == null) {\n        outputFormat = org.apache.hadoop.mapred.SequenceFileOutputFormat.class.getName();\n      } else {\n        outputFormat = tblOutput;\n      }\n    }\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT,\n        outputFormat);\n\n    // Location\n    if (sd.getLocation() != null) {\n      schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_LOCATION,\n          sd.getLocation());\n    }\n\n    // Bucket count\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_COUNT,\n        Integer.toString(sd.getNumBuckets()));\n\n    if (sd.getBucketCols() != null && sd.getBucketCols().size() > 0) {\n      schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_FIELD_NAME,\n          sd.getBucketCols().get(0));\n    }\n\n    // SerdeInfo\n    if (sd.getSerdeInfo() != null) {\n\n      // We should not update the following 3 values if SerDeInfo contains these.\n      // This is to keep backward compatible with getSchema(), where these 3 keys\n      // are updated after SerDeInfo properties got copied.\n      String cols = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n      String colTypes = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n      String parts = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS;\n\n      for (Map.Entry<String,String> param : sd.getSerdeInfo().getParameters().entrySet()) {\n        String key = param.getKey();\n        if (schema.get(key) != null &&\n            (key.equals(cols) || key.equals(colTypes) || key.equals(parts))) {\n          continue;\n        }\n        schema.put(key, (param.getValue() != null) ? param.getValue() : \"\");\n      }\n\n      if (sd.getSerdeInfo().getSerializationLib() != null) {\n        schema.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB,\n            sd.getSerdeInfo().getSerializationLib());\n      }\n    }\n\n    // skipping columns since partition level field schemas are the same as table level's\n    // skipping partition keys since it is the same as table level partition keys\n\n    if (parameters != null) {\n      for (Entry<String, String> e : parameters.entrySet()) {\n        schema.setProperty(e.getKey(), e.getValue());\n      }\n    }\n\n    return schema;\n  }",
            " 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009 +\n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  ",
            "  /**\n   * Get partition level schema from table level schema.\n   * This function will use the same column names, column types and partition keys for\n   * each partition Properties. Their values are copied from the table Properties. This\n   * is mainly to save CPU and memory. CPU is saved because the first time the\n   * StorageDescriptor column names are accessed, JDO needs to execute a SQL query to\n   * retrieve the data. If we know the data will be the same as the table level schema\n   * and they are immutable, we should just reuse the table level schema objects.\n   *\n   * @param sd The Partition level Storage Descriptor.\n   * @param tblsd The Table level Storage Descriptor.\n   * @param parameters partition level parameters\n   * @param databaseName DB name\n   * @param tableName table name\n   * @param partitionKeys partition columns\n   * @param tblSchema The table level schema from which this partition should be copied.\n   * @return the properties\n   */\n  public static Properties getPartSchemaFromTableSchema(\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor sd,\n      org.apache.hadoop.hive.metastore.api.StorageDescriptor tblsd,\n      Map<String, String> parameters, String databaseName, String tableName,\n      List<FieldSchema> partitionKeys,\n      Properties tblSchema) {\n\n    // Inherent most properties from table level schema and overwrite some properties\n    // in the following code.\n    // This is mainly for saving CPU and memory to reuse the column names, types and\n    // partition columns in the table level schema.\n    Properties schema = (Properties) tblSchema.clone();\n\n    // InputFormat\n    String inputFormat = sd.getInputFormat();\n    if (inputFormat == null || inputFormat.length() == 0) {\n      String tblInput =\n        schema.getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT);\n      if (tblInput == null) {\n        inputFormat = org.apache.hadoop.mapred.SequenceFileInputFormat.class.getName();\n      } else {\n        inputFormat = tblInput;\n      }\n    }\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_INPUT_FORMAT,\n        inputFormat);\n\n    // OutputFormat\n    String outputFormat = sd.getOutputFormat();\n    if (outputFormat == null || outputFormat.length() == 0) {\n      String tblOutput =\n        schema.getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT);\n      if (tblOutput == null) {\n        outputFormat = org.apache.hadoop.mapred.SequenceFileOutputFormat.class.getName();\n      } else {\n        outputFormat = tblOutput;\n      }\n    }\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.FILE_OUTPUT_FORMAT,\n        outputFormat);\n\n    // Location\n    if (sd.getLocation() != null) {\n      schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_LOCATION,\n          sd.getLocation());\n    }\n\n    // Bucket count\n    schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_COUNT,\n        Integer.toString(sd.getNumBuckets()));\n\n    if (sd.getBucketCols() != null && sd.getBucketCols().size() > 0) {\n      schema.setProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.BUCKET_FIELD_NAME,\n          sd.getBucketCols().get(0));\n    }\n\n    // SerdeInfo\n    if (sd.getSerdeInfo() != null) {\n\n      // We should not update the following 3 values if SerDeInfo contains these.\n      // This is to keep backward compatible with getSchema(), where these 3 keys\n      // are updated after SerDeInfo properties got copied.\n      String cols = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n      String colTypes = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n      String parts = org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS;\n\n      for (Map.Entry<String,String> param : sd.getSerdeInfo().getParameters().entrySet()) {\n        String key = param.getKey();\n        if (schema.get(key) != null &&\n            (key.equals(cols) || key.equals(colTypes) || key.equals(parts))) {\n          continue;\n        }\n        schema.put(key, (param.getValue() != null) ? param.getValue() : StringUtils.EMPTY);\n      }\n\n      if (sd.getSerdeInfo().getSerializationLib() != null) {\n        schema.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_LIB,\n            sd.getSerdeInfo().getSerializationLib());\n      }\n    }\n\n    // skipping columns since partition level field schemas are the same as table level's\n    // skipping partition keys since it is the same as table level partition keys\n\n    if (parameters != null) {\n      for (Entry<String, String> e : parameters.entrySet()) {\n        schema.setProperty(e.getKey(), e.getValue());\n      }\n    }\n\n    return schema;\n  }"
        ],
        [
            "MetaStoreUtils::getArchivingLevel(Partition)",
            "1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800 -\n1801 -\n1802  \n1803  ",
            "  public static int getArchivingLevel(Partition part) throws MetaException {\n    if (!isArchived(part)) {\n      throw new MetaException(\"Getting level of unarchived partition\");\n    }\n\n    String lv = part.getParameters().get(ARCHIVING_LEVEL);\n    if (lv != null) {\n      return Integer.parseInt(lv);\n    } else {  // partitions archived before introducing multiple archiving\n      return part.getValues().size();\n    }\n  }",
            "1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773 +\n1774 +\n1775  ",
            "  public static int getArchivingLevel(Partition part) throws MetaException {\n    if (!isArchived(part)) {\n      throw new MetaException(\"Getting level of unarchived partition\");\n    }\n\n    String lv = part.getParameters().get(ARCHIVING_LEVEL);\n    if (lv != null) {\n      return Integer.parseInt(lv);\n    }\n     // partitions archived before introducing multiple archiving\n    return part.getValues().size();\n  }"
        ],
        [
            "MetaStoreUtils::encodeTableName(String)",
            "1900  \n1901  \n1902  \n1903  \n1904  \n1905 -\n1906  \n1907  \n1908 -\n1909  \n1910 -\n1911  \n1912  \n1913 -\n1914  ",
            "  public static String encodeTableName(String name) {\n    // The encoding method is simple, e.g., replace\n    // all the special characters with the corresponding number in ASCII.\n    // Note that unicode is not supported in table names. And we have explicit\n    // checks for it.\n    String ret = \"\";\n    for (char ch : name.toCharArray()) {\n      if (Character.isLetterOrDigit(ch) || ch == '_') {\n        ret += ch;\n      } else {\n        ret += \"-\" + (int) ch + \"-\";\n      }\n    }\n    return ret;\n  }",
            "1868  \n1869  \n1870  \n1871  \n1872  \n1873 +\n1874  \n1875  \n1876 +\n1877  \n1878 +\n1879  \n1880  \n1881 +\n1882  ",
            "  public static String encodeTableName(String name) {\n    // The encoding method is simple, e.g., replace\n    // all the special characters with the corresponding number in ASCII.\n    // Note that unicode is not supported in table names. And we have explicit\n    // checks for it.\n    StringBuilder sb = new StringBuilder();\n    for (char ch : name.toCharArray()) {\n      if (Character.isLetterOrDigit(ch) || ch == '_') {\n        sb.append(ch);\n      } else {\n        sb.append('-').append((int) ch).append('-');\n      }\n    }\n    return sb.toString();\n  }"
        ],
        [
            "MetaStoreUtils::mergeColStats(ColumnStatistics,ColumnStatistics)",
            "1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926 -\n1927 -\n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  ",
            "  public static void mergeColStats(ColumnStatistics csNew, ColumnStatistics csOld)\n      throws InvalidObjectException {\n    List<ColumnStatisticsObj> list = new ArrayList<>();\n    if (csNew.getStatsObj().size() != csOld.getStatsObjSize()) {\n      // Some of the columns' stats are missing\n      // This implies partition schema has changed. We will merge columns\n      // present in both, overwrite stats for columns absent in metastore and\n      // leave alone columns stats missing from stats task. This last case may\n      // leave stats in stale state. This will be addressed later.\n      LOG.debug(\"New ColumnStats size is \" + csNew.getStatsObj().size()\n          + \". But old ColumnStats size is \" + csOld.getStatsObjSize());\n    }\n    // In this case, we have to find out which columns can be merged.\n    Map<String, ColumnStatisticsObj> map = new HashMap<>();\n    // We build a hash map from colName to object for old ColumnStats.\n    for (ColumnStatisticsObj obj : csOld.getStatsObj()) {\n      map.put(obj.getColName(), obj);\n    }\n    for (int index = 0; index < csNew.getStatsObj().size(); index++) {\n      ColumnStatisticsObj statsObjNew = csNew.getStatsObj().get(index);\n      ColumnStatisticsObj statsObjOld = map.get(statsObjNew.getColName());\n      if (statsObjOld != null) {\n        // If statsObjOld is found, we can merge.\n        ColumnStatsMerger merger = ColumnStatsMergerFactory.getColumnStatsMerger(statsObjNew,\n            statsObjOld);\n        merger.merge(statsObjNew, statsObjOld);\n      }\n      list.add(statsObjNew);\n    }\n    csNew.setStatsObj(list);\n  }",
            "1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894 +\n1895 +\n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  ",
            "  public static void mergeColStats(ColumnStatistics csNew, ColumnStatistics csOld)\n      throws InvalidObjectException {\n    List<ColumnStatisticsObj> list = new ArrayList<>();\n    if (csNew.getStatsObj().size() != csOld.getStatsObjSize()) {\n      // Some of the columns' stats are missing\n      // This implies partition schema has changed. We will merge columns\n      // present in both, overwrite stats for columns absent in metastore and\n      // leave alone columns stats missing from stats task. This last case may\n      // leave stats in stale state. This will be addressed later.\n      LOG.debug(\"New ColumnStats size is {}, but old ColumnStats size is {}\", \n          csNew.getStatsObj().size(), csOld.getStatsObjSize());\n    }\n    // In this case, we have to find out which columns can be merged.\n    Map<String, ColumnStatisticsObj> map = new HashMap<>();\n    // We build a hash map from colName to object for old ColumnStats.\n    for (ColumnStatisticsObj obj : csOld.getStatsObj()) {\n      map.put(obj.getColName(), obj);\n    }\n    for (int index = 0; index < csNew.getStatsObj().size(); index++) {\n      ColumnStatisticsObj statsObjNew = csNew.getStatsObj().get(index);\n      ColumnStatisticsObj statsObjOld = map.get(statsObjNew.getColName());\n      if (statsObjOld != null) {\n        // If statsObjOld is found, we can merge.\n        ColumnStatsMerger merger = ColumnStatsMergerFactory.getColumnStatsMerger(statsObjNew,\n            statsObjOld);\n        merger.merge(statsObjNew, statsObjOld);\n      }\n      list.add(statsObjNew);\n    }\n    csNew.setStatsObj(list);\n  }"
        ],
        [
            "MetaStoreUtils::getMetaStoreListeners(Class,HiveConf,String)",
            "1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646 -\n1647  \n1648 -\n1649 -\n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  ",
            "  /**\n   * create listener instances as per the configuration.\n   *\n   * @param clazz\n   * @param conf\n   * @param listenerImplList\n   * @return\n   * @throws MetaException\n   */\n  static <T> List<T> getMetaStoreListeners(Class<T> clazz,\n      HiveConf conf, String listenerImplList) throws MetaException {\n\n    List<T> listeners = new ArrayList<T>();\n    listenerImplList = listenerImplList.trim();\n    if (listenerImplList.equals(\"\")) {\n      return listeners;\n    }\n\n    String[] listenerImpls = listenerImplList.split(\",\");\n    for (String listenerImpl : listenerImpls) {\n      try {\n        T listener = (T) Class.forName(\n            listenerImpl.trim(), true, JavaUtils.getClassLoader()).getConstructor(\n                Configuration.class).newInstance(conf);\n        listeners.add(listener);\n      } catch (InvocationTargetException ie) {\n        throw new MetaException(\"Failed to instantiate listener named: \"+\n            listenerImpl + \", reason: \" + ie.getCause());\n      } catch (Exception e) {\n        throw new MetaException(\"Failed to instantiate listener named: \"+\n            listenerImpl + \", reason: \" + e);\n      }\n    }\n\n    return listeners;\n  }",
            "1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628 +\n1629 +\n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  ",
            "  /**\n   * create listener instances as per the configuration.\n   *\n   * @param clazz\n   * @param conf\n   * @param listenerImplList\n   * @return\n   * @throws MetaException\n   */\n  static <T> List<T> getMetaStoreListeners(Class<T> clazz,\n      HiveConf conf, String listenerImplList) throws MetaException {\n    List<T> listeners = new ArrayList<T>();\n\n    if (StringUtils.isBlank(listenerImplList)) {\n      return listeners;\n    }\n\n    String[] listenerImpls = listenerImplList.split(\",\");\n    for (String listenerImpl : listenerImpls) {\n      try {\n        T listener = (T) Class.forName(\n            listenerImpl.trim(), true, JavaUtils.getClassLoader()).getConstructor(\n                Configuration.class).newInstance(conf);\n        listeners.add(listener);\n      } catch (InvocationTargetException ie) {\n        throw new MetaException(\"Failed to instantiate listener named: \"+\n            listenerImpl + \", reason: \" + ie.getCause());\n      } catch (Exception e) {\n        throw new MetaException(\"Failed to instantiate listener named: \"+\n            listenerImpl + \", reason: \" + e);\n      }\n    }\n\n    return listeners;\n  }"
        ],
        [
            "MetaStoreUtils::validateName(String,Configuration)",
            " 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593 -\n 594 -\n 595 -\n 596 -\n 597  ",
            "  /**\n   * validateName\n   *\n   * Checks the name conforms to our standars which are: \"[a-zA-z_0-9]+\". checks\n   * this is just characters and numbers and _\n   *\n   * @param name\n   *          the name to validate\n   * @param conf\n   *          hive configuration\n   * @return true or false depending on conformance\n   *              if it doesn't match the pattern.\n   */\n  static public boolean validateName(String name, Configuration conf) {\n    Pattern tpat = null;\n    String allowedCharacters = \"\\\\w_\";\n    if (conf != null\n        && HiveConf.getBoolVar(conf,\n            HiveConf.ConfVars.HIVE_SUPPORT_SPECICAL_CHARACTERS_IN_TABLE_NAMES)) {\n      for (Character c : specialCharactersInTableNames) {\n        allowedCharacters += c;\n      }\n    }\n    tpat = Pattern.compile(\"[\" + allowedCharacters + \"]+\");\n    Matcher m = tpat.matcher(name);\n    if (m.matches()) {\n      return true;\n    }\n    return false;\n  }",
            " 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592 +\n 593  ",
            "  /**\n   * validateName\n   *\n   * Checks the name conforms to our standars which are: \"[a-zA-z_0-9]+\". checks\n   * this is just characters and numbers and _\n   *\n   * @param name\n   *          the name to validate\n   * @param conf\n   *          hive configuration\n   * @return true or false depending on conformance\n   *              if it doesn't match the pattern.\n   */\n  static public boolean validateName(String name, Configuration conf) {\n    Pattern tpat = null;\n    String allowedCharacters = \"\\\\w_\";\n    if (conf != null\n        && HiveConf.getBoolVar(conf,\n            HiveConf.ConfVars.HIVE_SUPPORT_SPECICAL_CHARACTERS_IN_TABLE_NAMES)) {\n      for (Character c : specialCharactersInTableNames) {\n        allowedCharacters += c;\n      }\n    }\n    tpat = Pattern.compile(\"[\" + allowedCharacters + \"]+\");\n    Matcher m = tpat.matcher(name);\n    return m.matches();\n  }"
        ],
        [
            "MetaStoreUtils::getQualifiedName(String,String)",
            "1805  \n1806  \n1807  \n1808  \n1809  \n1810 -\n1811  ",
            "  public static String[] getQualifiedName(String defaultDbName, String tableName) {\n    String[] names = tableName.split(\"\\\\.\");\n    if (names.length == 1) {\n      return new String[] { defaultDbName, tableName};\n    }\n    return new String[] {names[0], names[1]};\n  }",
            "1777  \n1778  \n1779  \n1780  \n1781  \n1782 +\n1783  ",
            "  public static String[] getQualifiedName(String defaultDbName, String tableName) {\n    String[] names = tableName.split(\"\\\\.\");\n    if (names.length == 1) {\n      return new String[] { defaultDbName, tableName};\n    }\n    return names;\n  }"
        ],
        [
            "MetaStoreUtils::apply(java)",
            "1818  \n1819  \n1820 -\n1821 -\n1822 -\n1823 -\n1824 -\n1825  ",
            "    @Override\n    public java.lang.String apply(@Nullable java.lang.String string) {\n      if (string == null){\n        return \"\";\n      } else {\n        return string;\n      }\n    }",
            "1790  \n1791  \n1792 +\n1793  ",
            "    @Override\n    public java.lang.String apply(@Nullable java.lang.String string) {\n      return StringUtils.defaultString(string);\n    }"
        ],
        [
            "MetaStoreUtils::validateSkewedColNames(List)",
            " 727  \n 728 -\n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  ",
            "  public static String validateSkewedColNames(List<String> cols) {\n    if (null == cols) {\n      return null;\n    }\n    for (String col : cols) {\n      if (!validateColumnName(col)) {\n        return col;\n      }\n    }\n    return null;\n  }",
            " 712  \n 713 +\n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  ",
            "  public static String validateSkewedColNames(List<String> cols) {\n    if (CollectionUtils.isEmpty(cols)) {\n      return null;\n    }\n    for (String col : cols) {\n      if (!validateColumnName(col)) {\n        return col;\n      }\n    }\n    return null;\n  }"
        ],
        [
            "MetaStoreUtils::getAllThreadStacksAsString()",
            "1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328 -\n1329  \n1330  \n1331  \n1332  ",
            "  private static String getAllThreadStacksAsString() {\n    Map<Thread, StackTraceElement[]> threadStacks = Thread.getAllStackTraces();\n    StringBuilder sb = new StringBuilder();\n    for (Map.Entry<Thread, StackTraceElement[]> entry : threadStacks.entrySet()) {\n      Thread t = entry.getKey();\n      sb.append(System.lineSeparator());\n      sb.append(\"Name: \").append(t.getName()).append(\" State: \" + t.getState());\n      addStackString(entry.getValue(), sb);\n    }\n    return sb.toString();\n  }",
            "1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313 +\n1314  \n1315  \n1316  \n1317  ",
            "  private static String getAllThreadStacksAsString() {\n    Map<Thread, StackTraceElement[]> threadStacks = Thread.getAllStackTraces();\n    StringBuilder sb = new StringBuilder();\n    for (Map.Entry<Thread, StackTraceElement[]> entry : threadStacks.entrySet()) {\n      Thread t = entry.getKey();\n      sb.append(System.lineSeparator());\n      sb.append(\"Name: \").append(t.getName()).append(\" State: \").append(t.getState());\n      addStackString(entry.getValue(), sb);\n    }\n    return sb.toString();\n  }"
        ],
        [
            "MetaStoreUtils::urlFromPathString(String)",
            "1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862 -\n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  ",
            "  /**\n   * Create a URL from a string representing a path to a local file.\n   * The path string can be just a path, or can start with file:/, file:///\n   * @param onestr  path string\n   * @return\n   */\n  private static URL urlFromPathString(String onestr) {\n    URL oneurl = null;\n    try {\n      if (StringUtils.indexOf(onestr, \"file:/\") == 0) {\n        oneurl = new URL(onestr);\n      } else {\n        oneurl = new File(onestr).toURL();\n      }\n    } catch (Exception err) {\n      LOG.error(\"Bad URL \" + onestr + \", ignoring path\");\n    }\n    return oneurl;\n  }",
            "1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830 +\n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  ",
            "  /**\n   * Create a URL from a string representing a path to a local file.\n   * The path string can be just a path, or can start with file:/, file:///\n   * @param onestr  path string\n   * @return\n   */\n  private static URL urlFromPathString(String onestr) {\n    URL oneurl = null;\n    try {\n      if (onestr.startsWith(\"file:/\")) {\n        oneurl = new URL(onestr);\n      } else {\n        oneurl = new File(onestr).toURL();\n      }\n    } catch (Exception err) {\n      LOG.error(\"Bad URL \" + onestr + \", ignoring path\");\n    }\n    return oneurl;\n  }"
        ],
        [
            "MetaStoreUtils::addCols(Properties,List)",
            "1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061 -\n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  ",
            "  public static Properties addCols(Properties schema, List<FieldSchema> cols) {\n\n    StringBuilder colNameBuf = new StringBuilder();\n    StringBuilder colTypeBuf = new StringBuilder();\n    StringBuilder colComment = new StringBuilder();\n\n    boolean first = true;\n    String columnNameDelimiter = getColumnNameDelimiter(cols);\n    for (FieldSchema col : cols) {\n      if (!first) {\n        colNameBuf.append(columnNameDelimiter);\n        colTypeBuf.append(\":\");\n        colComment.append('\\0');\n      }\n      colNameBuf.append(col.getName());\n      colTypeBuf.append(col.getType());\n      colComment.append((null != col.getComment()) ? col.getComment() : \"\");\n      first = false;\n    }\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS,\n        colNameBuf.toString());\n    schema.setProperty(serdeConstants.COLUMN_NAME_DELIMITER, columnNameDelimiter);\n    String colTypes = colTypeBuf.toString();\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES,\n        colTypes);\n    schema.setProperty(\"columns.comments\", colComment.toString());\n\n    return schema;\n\n  }",
            "1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046 +\n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  ",
            "  public static Properties addCols(Properties schema, List<FieldSchema> cols) {\n\n    StringBuilder colNameBuf = new StringBuilder();\n    StringBuilder colTypeBuf = new StringBuilder();\n    StringBuilder colComment = new StringBuilder();\n\n    boolean first = true;\n    String columnNameDelimiter = getColumnNameDelimiter(cols);\n    for (FieldSchema col : cols) {\n      if (!first) {\n        colNameBuf.append(columnNameDelimiter);\n        colTypeBuf.append(\":\");\n        colComment.append('\\0');\n      }\n      colNameBuf.append(col.getName());\n      colTypeBuf.append(col.getType());\n      colComment.append((null != col.getComment()) ? col.getComment() : StringUtils.EMPTY);\n      first = false;\n    }\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS,\n        colNameBuf.toString());\n    schema.setProperty(serdeConstants.COLUMN_NAME_DELIMITER, columnNameDelimiter);\n    String colTypes = colTypeBuf.toString();\n    schema.setProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES,\n        colTypes);\n    schema.setProperty(\"columns.comments\", colComment.toString());\n\n    return schema;\n\n  }"
        ],
        [
            "MetaStoreUtils::isArchived(org)",
            "1510  \n1511  \n1512  \n1513 -\n1514 -\n1515 -\n1516 -\n1517 -\n1518  ",
            "  public static boolean isArchived(\n      org.apache.hadoop.hive.metastore.api.Partition part) {\n    Map<String, String> params = part.getParameters();\n    if (\"true\".equalsIgnoreCase(params.get(hive_metastoreConstants.IS_ARCHIVED))) {\n      return true;\n    } else {\n      return false;\n    }\n  }",
            "1495  \n1496  \n1497  \n1498 +\n1499  ",
            "  public static boolean isArchived(\n      org.apache.hadoop.hive.metastore.api.Partition part) {\n    Map<String, String> params = part.getParameters();\n    return \"TRUE\".equalsIgnoreCase(params.get(hive_metastoreConstants.IS_ARCHIVED));\n  }"
        ],
        [
            "MetaStoreUtils::areSameColumns(List,List)",
            " 639  \n 640 -\n 641 -\n 642 -\n 643 -\n 644 -\n 645 -\n 646 -\n 647 -\n 648 -\n 649 -\n 650 -\n 651 -\n 652  ",
            "  static boolean areSameColumns(List<FieldSchema> oldCols, List<FieldSchema> newCols) {\n    if (oldCols.size() != newCols.size()) {\n      return false;\n    } else {\n      for (int i = 0; i < oldCols.size(); i++) {\n        FieldSchema oldCol = oldCols.get(i);\n        FieldSchema newCol = newCols.get(i);\n        if(!oldCol.equals(newCol)) {\n          return false;\n        }\n      }\n    }\n    return true;\n  }",
            " 635  \n 636 +\n 637  ",
            "  static boolean areSameColumns(List<FieldSchema> oldCols, List<FieldSchema> newCols) {\n    return ListUtils.isEqualList(oldCols, newCols);\n  }"
        ],
        [
            "MetaStoreUtils::createColumnsetSchema(String,List,List,Configuration)",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "  public static Table createColumnsetSchema(String name, List<String> columns,\n      List<String> partCols, Configuration conf) throws MetaException {\n\n    if (columns == null) {\n      throw new MetaException(\"columns not specified for table \" + name);\n    }\n\n    Table tTable = new Table();\n    tTable.setTableName(name);\n    tTable.setSd(new StorageDescriptor());\n    StorageDescriptor sd = tTable.getSd();\n    sd.setSerdeInfo(new SerDeInfo());\n    SerDeInfo serdeInfo = sd.getSerdeInfo();\n    serdeInfo.setSerializationLib(LazySimpleSerDe.class.getName());\n    serdeInfo.setParameters(new HashMap<String, String>());\n    serdeInfo.getParameters().put(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT,\n        DEFAULT_SERIALIZATION_FORMAT);\n\n    List<FieldSchema> fields = new ArrayList<FieldSchema>();\n    sd.setCols(fields);\n    for (String col : columns) {\n      FieldSchema field = new FieldSchema(col,\n          org.apache.hadoop.hive.serde.serdeConstants.STRING_TYPE_NAME, \"'default'\");\n      fields.add(field);\n    }\n\n    tTable.setPartitionKeys(new ArrayList<FieldSchema>());\n    for (String partCol : partCols) {\n      FieldSchema part = new FieldSchema();\n      part.setName(partCol);\n      part.setType(org.apache.hadoop.hive.serde.serdeConstants.STRING_TYPE_NAME); // default\n                                                                             // partition\n                                                                             // key\n      tTable.getPartitionKeys().add(part);\n    }\n    sd.setNumBuckets(-1);\n    return tTable;\n  }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  public static Table createColumnsetSchema(String name, List<String> columns,\n      List<String> partCols, Configuration conf) throws MetaException {\n\n    if (columns == null) {\n      throw new MetaException(\"columns not specified for table \" + name);\n    }\n\n    Table tTable = new Table();\n    tTable.setTableName(name);\n    tTable.setSd(new StorageDescriptor());\n    StorageDescriptor sd = tTable.getSd();\n    sd.setSerdeInfo(new SerDeInfo());\n    SerDeInfo serdeInfo = sd.getSerdeInfo();\n    serdeInfo.setSerializationLib(LazySimpleSerDe.class.getName());\n    serdeInfo.setParameters(new HashMap<String, String>());\n    serdeInfo.getParameters().put(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_FORMAT,\n        DEFAULT_SERIALIZATION_FORMAT);\n\n    List<FieldSchema> fields = new ArrayList<FieldSchema>(columns.size());\n    sd.setCols(fields);\n    for (String col : columns) {\n      FieldSchema field = new FieldSchema(col,\n          org.apache.hadoop.hive.serde.serdeConstants.STRING_TYPE_NAME, \"'default'\");\n      fields.add(field);\n    }\n\n    tTable.setPartitionKeys(new ArrayList<FieldSchema>());\n    for (String partCol : partCols) {\n      FieldSchema part = new FieldSchema();\n      part.setName(partCol);\n      part.setType(org.apache.hadoop.hive.serde.serdeConstants.STRING_TYPE_NAME); // default\n                                                                             // partition\n                                                                             // key\n      tTable.getPartitionKeys().add(part);\n    }\n    sd.setNumBuckets(-1);\n    return tTable;\n  }"
        ],
        [
            "MetaStoreUtils::getPvals(List,Map)",
            " 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 -\n 558  \n 559 -\n 560 -\n 561 -\n 562 -\n 563  \n 564  \n 565  \n 566  ",
            "  /**\n   * Given a list of partition columns and a partial mapping from\n   * some partition columns to values the function returns the values\n   * for the column.\n   * @param partCols the list of table partition columns\n   * @param partSpec the partial mapping from partition column to values\n   * @return list of values of for given partition columns, any missing\n   *         values in partSpec is replaced by an empty string\n   */\n  public static List<String> getPvals(List<FieldSchema> partCols,\n      Map<String, String> partSpec) {\n    List<String> pvals = new ArrayList<String>();\n    for (FieldSchema field : partCols) {\n      String val = partSpec.get(field.getName());\n      if (val == null) {\n        val = \"\";\n      }\n      pvals.add(val);\n    }\n    return pvals;\n  }",
            " 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559 +\n 560  \n 561 +\n 562  \n 563  \n 564  \n 565  ",
            "  /**\n   * Given a list of partition columns and a partial mapping from\n   * some partition columns to values the function returns the values\n   * for the column.\n   * @param partCols the list of table partition columns\n   * @param partSpec the partial mapping from partition column to values\n   * @return list of values of for given partition columns, any missing\n   *         values in partSpec is replaced by an empty string\n   */\n  public static List<String> getPvals(List<FieldSchema> partCols,\n      Map<String, String> partSpec) {\n    List<String> pvals = new ArrayList<String>(partCols.size());\n    for (FieldSchema field : partCols) {\n      String val = StringUtils.defaultString(partSpec.get(field.getName()));\n      pvals.add(val);\n    }\n    return pvals;\n  }"
        ],
        [
            "MetaStoreUtils::getDDLFromFieldSchema(String,List)",
            " 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908 -\n 909  \n 910  ",
            "  /**\n   * Convert FieldSchemas to Thrift DDL.\n   */\n  public static String getDDLFromFieldSchema(String structName,\n      List<FieldSchema> fieldSchemas) {\n    StringBuilder ddl = new StringBuilder();\n    ddl.append(\"struct \");\n    ddl.append(structName);\n    ddl.append(\" { \");\n    boolean first = true;\n    for (FieldSchema col : fieldSchemas) {\n      if (first) {\n        first = false;\n      } else {\n        ddl.append(\", \");\n      }\n      ddl.append(typeToThriftType(col.getType()));\n      ddl.append(' ');\n      ddl.append(col.getName());\n    }\n    ddl.append(\"}\");\n\n    LOG.debug(\"DDL: \" + ddl);\n    return ddl.toString();\n  }",
            " 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893 +\n 894  \n 895  ",
            "  /**\n   * Convert FieldSchemas to Thrift DDL.\n   */\n  public static String getDDLFromFieldSchema(String structName,\n      List<FieldSchema> fieldSchemas) {\n    StringBuilder ddl = new StringBuilder();\n    ddl.append(\"struct \");\n    ddl.append(structName);\n    ddl.append(\" { \");\n    boolean first = true;\n    for (FieldSchema col : fieldSchemas) {\n      if (first) {\n        first = false;\n      } else {\n        ddl.append(\", \");\n      }\n      ddl.append(typeToThriftType(col.getType()));\n      ddl.append(' ');\n      ddl.append(col.getName());\n    }\n    ddl.append(\"}\");\n\n    LOG.trace(\"DDL: {}\", ddl);\n    return ddl.toString();\n  }"
        ]
    ],
    "e7cee300114b2f54b3912d5aa92043d3bdf295b6": [
        [
            "SessionState::start(SessionState,boolean,LogHelper)",
            " 604 -\n 605  \n 606  \n 607 -\n 608 -\n 609  \n 610 -\n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  ",
            "  synchronized private static void start(SessionState startSs, boolean isAsync, LogHelper console) {\n    setCurrentSessionState(startSs);\n\n    if (startSs.isStarted) {\n      return;\n    }\n    startSs.isStarted = true;\n\n    if (startSs.hiveHist == null){\n      if (startSs.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SESSION_HISTORY_ENABLED)) {\n        startSs.hiveHist = new HiveHistoryImpl(startSs);\n      } else {\n        // Hive history is disabled, create a no-op proxy\n        startSs.hiveHist = HiveHistoryProxyHandler.getNoOpHiveHistoryProxy();\n      }\n    }\n\n    // Get the following out of the way when you start the session these take a\n    // while and should be done when we start up.\n    try {\n      UserGroupInformation sessionUGI = Utils.getUGI();\n      FileSystem.get(startSs.sessionConf);\n\n      // Create scratch dirs for this session\n      startSs.createSessionDirs(sessionUGI.getShortUserName());\n\n      // Set temp file containing results to be sent to HiveClient\n      if (startSs.getTmpOutputFile() == null) {\n        try {\n          startSs.setTmpOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n\n      // Set temp file containing error output to be sent to client\n      if (startSs.getTmpErrOutputFile() == null) {\n        try {\n          startSs.setTmpErrOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // Catch-all due to some exec time dependencies on session state\n      // that would cause ClassNoFoundException otherwise\n      throw new RuntimeException(e);\n    }\n\n    String engine = HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);\n    if (!engine.equals(\"tez\") || startSs.isHiveServerQuery) {\n      return;\n    }\n\n    try {\n      if (startSs.tezSessionState == null) {\n        startSs.setTezSession(new TezSessionState(startSs.getSessionId(), startSs.sessionConf));\n      } else {\n        // Only TezTask sets this, and then removes when done, so we don't expect to see it.\n        LOG.warn(\"Tez session was already present in SessionState before start: \"\n            + startSs.tezSessionState);\n      }\n      if (startSs.tezSessionState.isOpen()) {\n        return;\n      }\n      if (startSs.tezSessionState.isOpening()) {\n        if (!isAsync) {\n          startSs.tezSessionState.endOpen();\n        }\n        return;\n      }\n      // Neither open nor opening.\n      if (!isAsync) {\n        startSs.tezSessionState.open();\n      } else {\n        startSs.tezSessionState.beginOpen(null, console);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }",
            " 605 +\n 606  \n 607  \n 608 +\n 609 +\n 610 +\n 611 +\n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  ",
            "  private static void start(SessionState startSs, boolean isAsync, LogHelper console) {\n    setCurrentSessionState(startSs);\n\n    synchronized(SessionState.class) {\n      if (!startSs.isStarted.compareAndSet(false, true)) {\n        return;\n      }\n    }\n\n    if (startSs.hiveHist == null){\n      if (startSs.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SESSION_HISTORY_ENABLED)) {\n        startSs.hiveHist = new HiveHistoryImpl(startSs);\n      } else {\n        // Hive history is disabled, create a no-op proxy\n        startSs.hiveHist = HiveHistoryProxyHandler.getNoOpHiveHistoryProxy();\n      }\n    }\n\n    // Get the following out of the way when you start the session these take a\n    // while and should be done when we start up.\n    try {\n      UserGroupInformation sessionUGI = Utils.getUGI();\n      FileSystem.get(startSs.sessionConf);\n\n      // Create scratch dirs for this session\n      startSs.createSessionDirs(sessionUGI.getShortUserName());\n\n      // Set temp file containing results to be sent to HiveClient\n      if (startSs.getTmpOutputFile() == null) {\n        try {\n          startSs.setTmpOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n\n      // Set temp file containing error output to be sent to client\n      if (startSs.getTmpErrOutputFile() == null) {\n        try {\n          startSs.setTmpErrOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // Catch-all due to some exec time dependencies on session state\n      // that would cause ClassNoFoundException otherwise\n      throw new RuntimeException(e);\n    }\n\n    String engine = HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);\n    if (!engine.equals(\"tez\") || startSs.isHiveServerQuery) {\n      return;\n    }\n\n    try {\n      if (startSs.tezSessionState == null) {\n        startSs.setTezSession(new TezSessionState(startSs.getSessionId(), startSs.sessionConf));\n      } else {\n        // Only TezTask sets this, and then removes when done, so we don't expect to see it.\n        LOG.warn(\"Tez session was already present in SessionState before start: \"\n            + startSs.tezSessionState);\n      }\n      if (startSs.tezSessionState.isOpen()) {\n        return;\n      }\n      if (startSs.tezSessionState.isOpening()) {\n        if (!isAsync) {\n          startSs.tezSessionState.endOpen();\n        }\n        return;\n      }\n      // Neither open nor opening.\n      if (!isAsync) {\n        startSs.tezSessionState.open();\n      } else {\n        startSs.tezSessionState.beginOpen(null, console);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }"
        ]
    ],
    "cc8ac97bcadd20a645e113f3193fc6b2d9db087d": [
        [
            "ObjectInspectorFactory::getUnionStructObjectInspector(List)",
            " 348  \n 349  \n 350 -\n 351 -\n 352  \n 353  \n 354 -\n 355 -\n 356 -\n 357 -\n 358 -\n 359  \n 360  \n 361  ",
            "  public static UnionStructObjectInspector getUnionStructObjectInspector(\n      List<StructObjectInspector> structObjectInspectors) {\n    UnionStructObjectInspector result = cachedUnionStructObjectInspector\n        .get(structObjectInspectors);\n    if (result == null) {\n      result = new UnionStructObjectInspector(structObjectInspectors);\n      UnionStructObjectInspector prev =\n        cachedUnionStructObjectInspector.putIfAbsent(structObjectInspectors, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }",
            " 355  \n 356  \n 357 +\n 358  \n 359  \n 360 +\n 361  \n 362  \n 363  ",
            "  public static UnionStructObjectInspector getUnionStructObjectInspector(\n      List<StructObjectInspector> structObjectInspectors) {\n    UnionStructObjectInspector result = cachedUnionStructObjectInspector.getIfPresent(structObjectInspectors);\n    if (result == null) {\n      result = new UnionStructObjectInspector(structObjectInspectors);\n      cachedUnionStructObjectInspector.put(structObjectInspectors, result);\n    }\n    return result;\n  }"
        ]
    ],
    "e9e1f8f6e2f7b66585dc58ca561002b4bdee87fa": [
        [
            "HiveServer2::startPrivilegeSynchonizer(HiveConf)",
            " 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  ",
            "  public void startPrivilegeSynchonizer(HiveConf hiveConf) throws Exception {\n\n    PolicyProviderContainer policyContainer = new PolicyProviderContainer();\n    HiveAuthorizer authorizer = SessionState.get().getAuthorizerV2();\n    if (authorizer.getHivePolicyProvider() != null) {\n      policyContainer.addAuthorizer(authorizer);\n    }\n    if (hiveConf.get(MetastoreConf.ConfVars.PRE_EVENT_LISTENERS.getVarname()) != null &&\n        hiveConf.get(MetastoreConf.ConfVars.PRE_EVENT_LISTENERS.getVarname()).contains(\n        \"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\") &&\n        hiveConf.get(MetastoreConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.getVarname())!= null) {\n      List<HiveMetastoreAuthorizationProvider> providers = HiveUtils.getMetaStoreAuthorizeProviderManagers(\n          hiveConf, HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER, SessionState.get().getAuthenticator());\n      for (HiveMetastoreAuthorizationProvider provider : providers) {\n        if (provider.getHivePolicyProvider() != null) {\n          policyContainer.addAuthorizationProvider(provider);\n        }\n      }\n    }\n\n    if (policyContainer.size() > 0) {\n      zKClientForPrivSync = startZookeeperClient(hiveConf);\n      String rootNamespace = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_ZOOKEEPER_NAMESPACE);\n      String path = ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace\n          + ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + \"leader\";\n      LeaderLatch privilegeSynchonizerLatch = new LeaderLatch(zKClientForPrivSync, path);\n      privilegeSynchonizerLatch.start();\n      Thread privilegeSynchonizerThread = new Thread(\n          new PrivilegeSynchonizer(privilegeSynchonizerLatch, policyContainer, hiveConf), \"PrivilegeSynchonizer\");\n      privilegeSynchonizerThread.start();\n    } else {\n      LOG.warn(\n          \"No policy provider found, skip creating PrivilegeSynchonizer\");\n    }\n  }",
            " 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014 +\n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  ",
            "  public void startPrivilegeSynchonizer(HiveConf hiveConf) throws Exception {\n\n    PolicyProviderContainer policyContainer = new PolicyProviderContainer();\n    HiveAuthorizer authorizer = SessionState.get().getAuthorizerV2();\n    if (authorizer.getHivePolicyProvider() != null) {\n      policyContainer.addAuthorizer(authorizer);\n    }\n    if (hiveConf.get(MetastoreConf.ConfVars.PRE_EVENT_LISTENERS.getVarname()) != null &&\n        hiveConf.get(MetastoreConf.ConfVars.PRE_EVENT_LISTENERS.getVarname()).contains(\n        \"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener\") &&\n        hiveConf.get(MetastoreConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.getVarname())!= null) {\n      List<HiveMetastoreAuthorizationProvider> providers = HiveUtils.getMetaStoreAuthorizeProviderManagers(\n          hiveConf, HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER, SessionState.get().getAuthenticator());\n      for (HiveMetastoreAuthorizationProvider provider : providers) {\n        if (provider.getHivePolicyProvider() != null) {\n          policyContainer.addAuthorizationProvider(provider);\n        }\n      }\n    }\n\n    if (policyContainer.size() > 0) {\n      zKClientForPrivSync = startZookeeperClient(hiveConf);\n      String rootNamespace = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_ZOOKEEPER_NAMESPACE);\n      String path = ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace\n          + ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + \"leader\";\n      LeaderLatch privilegeSynchonizerLatch = new LeaderLatch(zKClientForPrivSync, path);\n      privilegeSynchonizerLatch.start();\n      LOG.info(\"Find \" + policyContainer.size() + \" policy to synchronize, start PrivilegeSynchonizer\");\n      Thread privilegeSynchonizerThread = new Thread(\n          new PrivilegeSynchonizer(privilegeSynchonizerLatch, policyContainer, hiveConf), \"PrivilegeSynchonizer\");\n      privilegeSynchonizerThread.start();\n    } else {\n      LOG.warn(\n          \"No policy provider found, skip creating PrivilegeSynchonizer\");\n    }\n  }"
        ],
        [
            "PrivilegeSynchonizer::run()",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204  \n 205 -\n 206 -\n 207  \n 208 -\n 209  \n 210  \n 211  ",
            "  @Override\n  public void run() {\n    while (true) {\n      long interval = HiveConf.getTimeVar(hiveConf, ConfVars.HIVE_PRIVILEGE_SYNCHRONIZER_INTERVAL, TimeUnit.SECONDS);\n      try {\n        for (HivePolicyProvider policyProvider : policyProviderContainer) {\n          String authorizer = policyProvider.getClass().getSimpleName();\n          if (!privilegeSynchonizerLatch.await(interval, TimeUnit.SECONDS)) {\n            continue;\n          }\n          LOG.info(\"Start synchonize privilege\");\n          for (String dbName : hiveClient.getAllDatabases()) {\n            HiveObjectRef dbToRefresh = getObjToRefresh(HiveObjectType.DATABASE, dbName, null);\n            PrivilegeBag grantDatabaseBag = new PrivilegeBag();\n            addGrantPrivilegesToBag(policyProvider, grantDatabaseBag, HiveObjectType.DATABASE,\n                dbName, null, null, authorizer);\n            hiveClient.refresh_privileges(dbToRefresh, authorizer, grantDatabaseBag);\n\n            for (String tblName : hiveClient.getAllTables(dbName)) {\n              HiveObjectRef tableToRefresh = getObjToRefresh(HiveObjectType.TABLE, dbName, tblName);\n              PrivilegeBag grantTableBag = new PrivilegeBag();\n              addGrantPrivilegesToBag(policyProvider, grantTableBag, HiveObjectType.TABLE,\n                  dbName, tblName, null, authorizer);\n              hiveClient.refresh_privileges(tableToRefresh, authorizer, grantTableBag);\n\n              HiveObjectRef tableOfColumnsToRefresh = getObjToRefresh(HiveObjectType.COLUMN, dbName, tblName);\n              PrivilegeBag grantColumnBag = new PrivilegeBag();\n              Table tbl = hiveClient.getTable(dbName, tblName);\n              for (FieldSchema fs : tbl.getPartitionKeys()) {\n                addGrantPrivilegesToBag(policyProvider, grantColumnBag, HiveObjectType.COLUMN,\n                    dbName, tblName, fs.getName(), authorizer);\n              }\n              for (FieldSchema fs : tbl.getSd().getCols()) {\n                addGrantPrivilegesToBag(policyProvider, grantColumnBag, HiveObjectType.COLUMN,\n                    dbName, tblName, fs.getName(), authorizer);\n              }\n              hiveClient.refresh_privileges(tableOfColumnsToRefresh, authorizer, grantColumnBag);\n            }\n          }\n          // Wait if no exception happens, otherwise, retry immediately\n        }\n        Thread.sleep(interval * 1000);\n        LOG.info(\"Success synchonize privilege\");\n\n      } catch (Exception e) {\n        LOG.error(\"Error initializing PrivilegeSynchonizer: \" + e.getMessage(), e);\n      }\n    }\n  }",
            " 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172 +\n 173  \n 174  \n 175 +\n 176  \n 177 +\n 178  \n 179  \n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186 +\n 187 +\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209 +\n 210  \n 211 +\n 212 +\n 213  \n 214  \n 215 +\n 216  \n 217  \n 218  ",
            "  @Override\n  public void run() {\n    while (true) {\n      long interval = HiveConf.getTimeVar(hiveConf, ConfVars.HIVE_PRIVILEGE_SYNCHRONIZER_INTERVAL, TimeUnit.SECONDS);\n      try {\n        for (HivePolicyProvider policyProvider : policyProviderContainer) {\n          LOG.info(\"Start synchronize privilege \" + policyProvider.getClass().getName());\n          String authorizer = policyProvider.getClass().getSimpleName();\n          if (!privilegeSynchonizerLatch.await(interval, TimeUnit.SECONDS)) {\n            LOG.info(\"Not selected as leader, skip\");\n            continue;\n          }\n          int numDb = 0, numTbl = 0;\n          for (String dbName : hiveClient.getAllDatabases()) {\n            numDb++;\n            HiveObjectRef dbToRefresh = getObjToRefresh(HiveObjectType.DATABASE, dbName, null);\n            PrivilegeBag grantDatabaseBag = new PrivilegeBag();\n            addGrantPrivilegesToBag(policyProvider, grantDatabaseBag, HiveObjectType.DATABASE,\n                dbName, null, null, authorizer);\n            hiveClient.refresh_privileges(dbToRefresh, authorizer, grantDatabaseBag);\n            LOG.debug(\"processing \" + dbName);\n\n            for (String tblName : hiveClient.getAllTables(dbName)) {\n              numTbl++;\n              LOG.debug(\"processing \" + dbName + \".\" + tblName);\n              HiveObjectRef tableToRefresh = getObjToRefresh(HiveObjectType.TABLE, dbName, tblName);\n              PrivilegeBag grantTableBag = new PrivilegeBag();\n              addGrantPrivilegesToBag(policyProvider, grantTableBag, HiveObjectType.TABLE,\n                  dbName, tblName, null, authorizer);\n              hiveClient.refresh_privileges(tableToRefresh, authorizer, grantTableBag);\n\n              HiveObjectRef tableOfColumnsToRefresh = getObjToRefresh(HiveObjectType.COLUMN, dbName, tblName);\n              PrivilegeBag grantColumnBag = new PrivilegeBag();\n              Table tbl = hiveClient.getTable(dbName, tblName);\n              for (FieldSchema fs : tbl.getPartitionKeys()) {\n                addGrantPrivilegesToBag(policyProvider, grantColumnBag, HiveObjectType.COLUMN,\n                    dbName, tblName, fs.getName(), authorizer);\n              }\n              for (FieldSchema fs : tbl.getSd().getCols()) {\n                addGrantPrivilegesToBag(policyProvider, grantColumnBag, HiveObjectType.COLUMN,\n                    dbName, tblName, fs.getName(), authorizer);\n              }\n              hiveClient.refresh_privileges(tableOfColumnsToRefresh, authorizer, grantColumnBag);\n            }\n          }\n          LOG.info(\"Success synchronize privilege \" + policyProvider.getClass().getName() + \":\" + numDb + \" databases, \"\n                  + numTbl + \" tables\");\n        }\n        // Wait if no exception happens, otherwise, retry immediately\n        LOG.info(\"Wait for \" + interval + \" seconds\");\n        Thread.sleep(interval * 1000);\n      } catch (Exception e) {\n        LOG.error(\"Error initializing PrivilegeSynchronizer: \" + e.getMessage(), e);\n      }\n    }\n  }"
        ]
    ],
    "b750a16be5a8e378ac11a88ace244e826d23cfe5": [
        [
            "HiveMetaStore::HMSHandler::append_partition_with_environment_context(String,String,List,EnvironmentContext)",
            "2809  \n2810  \n2811  \n2812  \n2813  \n2814  \n2815  \n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  \n2829  \n2830  \n2831  \n2832  \n2833  \n2834  \n2835  \n2836  \n2837  \n2838  \n2839  ",
            "    @Override\n    public Partition append_partition_with_environment_context(final String dbName,\n        final String tableName, final List<String> part_vals, final EnvironmentContext envContext)\n        throws InvalidObjectException, AlreadyExistsException, MetaException {\n      startPartitionFunction(\"append_partition\", dbName, tableName, part_vals);\n      if (LOG.isDebugEnabled()) {\n        for (String part : part_vals) {\n          LOG.debug(part);\n        }\n      }\n\n      Partition ret = null;\n      Exception ex = null;\n      try {\n        ret = append_partition_common(getMS(), dbName, tableName, part_vals, envContext);\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof InvalidObjectException) {\n          throw (InvalidObjectException) e;\n        } else if (e instanceof AlreadyExistsException) {\n          throw (AlreadyExistsException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"append_partition\", ret != null, ex, tableName);\n      }\n      return ret;\n    }",
            "2809  \n2810  \n2811  \n2812  \n2813 +\n2814 +\n2815 +\n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  \n2829  \n2830  \n2831  \n2832  \n2833  \n2834  \n2835  \n2836  \n2837  \n2838  \n2839  \n2840  \n2841  \n2842  ",
            "    @Override\n    public Partition append_partition_with_environment_context(final String dbName,\n        final String tableName, final List<String> part_vals, final EnvironmentContext envContext)\n        throws InvalidObjectException, AlreadyExistsException, MetaException {\n      if (part_vals == null) {\n        throw new MetaException(\"The partition values must not be null.\");\n      }\n      startPartitionFunction(\"append_partition\", dbName, tableName, part_vals);\n      if (LOG.isDebugEnabled()) {\n        for (String part : part_vals) {\n          LOG.debug(part);\n        }\n      }\n\n      Partition ret = null;\n      Exception ex = null;\n      try {\n        ret = append_partition_common(getMS(), dbName, tableName, part_vals, envContext);\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof InvalidObjectException) {\n          throw (InvalidObjectException) e;\n        } else if (e instanceof AlreadyExistsException) {\n          throw (AlreadyExistsException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"append_partition\", ret != null, ex, tableName);\n      }\n      return ret;\n    }"
        ],
        [
            "TestAppendPartitions::testAppendPartitionNullPartValues()",
            " 232  \n 233  \n 234  \n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242  ",
            "  @Test\n  public void testAppendPartitionNullPartValues() throws Exception {\n\n    try {\n      Table table = tableWithPartitions;\n      client.appendPartition(table.getDbName(), table.getTableName(), (List<String>) null);\n      Assert.fail(\"Exception should have been thrown.\");\n    } catch (TTransportException | NullPointerException e) {\n      // TODO: NPE should not be thrown\n    }\n  }",
            " 231  \n 232  \n 233  \n 234 +\n 235 +\n 236  ",
            "  @Test(expected = MetaException.class)\n  public void testAppendPartitionNullPartValues() throws Exception {\n\n    Table table = tableWithPartitions;\n    client.appendPartition(table.getDbName(), table.getTableName(), (List<String>) null);\n  }"
        ]
    ],
    "539165119e0f09b46b4422ac79677f5e95075c80": [
        [
            "CliConfigs::MiniLlapLocalCliConfig::MiniLlapLocalCliConfig()",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234 -\n 235 -\n 236 -\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"special_character_in_tabnames_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"sysdb.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"tez_smb_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"union_fast_stats.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"druid_timestamptz.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_joins.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_masking.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_test1.q\"); // Disabled in HIVE-20322\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"special_character_in_tabnames_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"sysdb.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"tez_smb_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"union_fast_stats.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\"); // Disabled in HIVE-19509\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "CliConfigs::MiniDruidCliConfig::MiniDruidCliConfig()",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179 +\n 180 +\n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n        excludeQuery(\"druid_timestamptz.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_joins.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_masking.q\"); // Disabled in HIVE-20322\n        excludeQuery(\"druidmini_test1.q\"); // Disabled in HIVE-20322\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ]
    ],
    "464a3f61a0c4a1c4e44a1ce427f604295534e969": [
        [
            "TestDruidStorageHandler::testInsertIntoAppendOneMorePartition()",
            " 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 -\n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584 -\n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  ",
            "  @Test\n  public void testInsertIntoAppendOneMorePartition() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v0\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(1, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575 +\n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585 +\n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "  @Test\n  public void testInsertIntoAppendOneMorePartition() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(1, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertOverwriteTable()",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 -\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 -\n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "  @Test\n  public void testCommitInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // This creates and publish new segment\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250), \"v1\", new LinearShardSpec(0));\n\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(1, dataSegmentList.size());\n    DataSegment persistedSegment = Iterables.getOnlyElement(dataSegmentList);\n    Assert.assertEquals(dataSegment, persistedSegment);\n    Assert.assertEquals(dataSegment.getVersion(), persistedSegment.getVersion());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  ",
            "  @Test\n  public void testCommitInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // This creates and publish new segment\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(1, dataSegmentList.size());\n    DataSegment persistedSegment = Iterables.getOnlyElement(dataSegmentList);\n    Assert.assertEquals(dataSegment, persistedSegment);\n    Assert.assertEquals(dataSegment.getVersion(), persistedSegment.getVersion());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::createSegment(String)",
            "  90  \n  91 -\n  92  ",
            "  private DataSegment createSegment(String location) throws IOException {\n    return createSegment(location, new Interval(100, 170), \"v1\", new LinearShardSpec(0));\n  }",
            "  91  \n  92 +\n  93  ",
            "  private DataSegment createSegment(String location) throws IOException {\n    return createSegment(location, new Interval(100, 170, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n  }"
        ],
        [
            "TestDruidRecordWriter::verifyRows(List,List)",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  ",
            "  private void verifyRows(List<ImmutableMap<String, Object>> expectedRows,\n          List<InputRow> actualRows\n  ) {\n    System.out.println(\"actualRows = \" + actualRows);\n    Assert.assertEquals(expectedRows.size(), actualRows.size());\n\n    for (int i = 0; i < expectedRows.size(); i++) {\n      Map<String, Object> expected = expectedRows.get(i);\n      InputRow actual = actualRows.get(i);\n\n      Assert.assertEquals(ImmutableList.of(\"host\"), actual.getDimensions());\n\n      Assert.assertEquals(expected.get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN),\n              actual.getTimestamp().getMillis()\n      );\n      Assert.assertEquals(expected.get(\"host\"), actual.getDimension(\"host\"));\n      Assert.assertEquals(expected.get(\"visited_sum\"), actual.getLongMetric(\"visited_sum\"));\n      Assert.assertEquals(\n              (Double) expected.get(\"unique_hosts\"),\n              (Double) HyperUniquesAggregatorFactory\n                      .estimateCardinality(actual.getRaw(\"unique_hosts\")),\n              0.001",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 +\n 234  ",
            "  private void verifyRows(List<ImmutableMap<String, Object>> expectedRows,\n          List<InputRow> actualRows\n  ) {\n    System.out.println(\"actualRows = \" + actualRows);\n    Assert.assertEquals(expectedRows.size(), actualRows.size());\n\n    for (int i = 0; i < expectedRows.size(); i++) {\n      Map<String, Object> expected = expectedRows.get(i);\n      InputRow actual = actualRows.get(i);\n\n      Assert.assertEquals(ImmutableList.of(\"host\"), actual.getDimensions());\n\n      Assert.assertEquals(expected.get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN),\n              actual.getTimestamp().getMillis()\n      );\n      Assert.assertEquals(expected.get(\"host\"), actual.getDimension(\"host\"));\n      Assert.assertEquals(expected.get(\"visited_sum\"), actual.getLongMetric(\"visited_sum\"));\n      Assert.assertEquals(\n              (Double) expected.get(\"unique_hosts\"),\n              (Double) HyperUniquesAggregatorFactory\n                      .estimateCardinality(actual.getRaw(\"unique_hosts\"), false),\n              0.001"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWhenDestinationSegmentFileExist()",
            " 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624 -\n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 -\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644 -\n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  ",
            "  @Test\n  public void testCommitInsertIntoWhenDestinationSegmentFileExist()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old.zip\").toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, \"index.zip\").toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    // Create segment file at the destination location with LinearShardSpec(2)\n    DataSegment segment = createSegment(new Path(taskDirPath, \"index_conflict.zip\").toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(1));\n    Path segmentPath = new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher.makeIndexPathName(segment, DruidStorageHandlerUtils.INDEX_ZIP));\n    FileUtils.writeStringToFile(new File(segmentPath.toUri()), \"dummy\");\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    // insert into should skip and increment partition number to 3\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625 +\n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637 +\n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645 +\n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "  @Test\n  public void testCommitInsertIntoWhenDestinationSegmentFileExist()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, \"index.zip\").toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    // Create segment file at the destination location with LinearShardSpec(2)\n    DataSegment segment = createSegment(new Path(taskDirPath, \"index_conflict.zip\").toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(1));\n    Path segmentPath = new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher.makeIndexPathName(segment, DruidStorageHandlerUtils.INDEX_ZIP));\n    FileUtils.writeStringToFile(new File(segmentPath.toUri()), \"dummy\");\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    // insert into should skip and increment partition number to 3\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWithNonExtendableSegment()",
            " 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726 -\n 727  \n 728 -\n 729  \n 730 -\n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744 -\n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithNonExtendableSegment() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150), \"v0\", new NoneShardSpec()),\n                    createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                            new Interval(200, 250), \"v0\", new LinearShardSpec(0)),\n                    createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                            new Interval(250, 300), \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending to non extendable shard spec\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n\n  }",
            " 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727 +\n 728  \n 729 +\n 730  \n 731 +\n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745 +\n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithNonExtendableSegment() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new NoneShardSpec()),\n                    createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                            new Interval(200, 250, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)),\n                    createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                            new Interval(250, 300, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending to non extendable shard spec\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitMultiInsertOverwriteTable()",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 -\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428 -\n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 -\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  ",
            "  @Test\n  public void testCommitMultiInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    druidStorageHandler.preCreateTable(tableMock);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    // Check that there is one datasource with the published segment\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    // Sequence is the following:\n    // 1) INSERT with no segments -> Original segment still present in the datasource\n    // 2) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 3) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 4) INSERT with no segments -> Datasource is empty\n    // 5) INSERT with one segment -> Datasource has one segment\n    // 6) INSERT OVERWRITE with one segment -> Datasource has one segment\n    // 7) INSERT with one segment -> Datasource has two segments\n    // 8) INSERT OVERWRITE with no segments -> Datasource is empty\n\n    // We start:\n    // #1\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #2\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #3\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #4\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #5\n    DataSegment dataSegment1 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath1 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment1,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment1, descriptorPath1);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #6\n    DataSegment dataSegment2 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(200, 250), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath2 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment2,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment2, descriptorPath2);\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #7\n    DataSegment dataSegment3 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 200), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath3 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment3,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment3, descriptorPath3);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(2, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #8\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n  }",
            " 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 +\n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429 +\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "  @Test\n  public void testCommitMultiInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    druidStorageHandler.preCreateTable(tableMock);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    // Check that there is one datasource with the published segment\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    // Sequence is the following:\n    // 1) INSERT with no segments -> Original segment still present in the datasource\n    // 2) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 3) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 4) INSERT with no segments -> Datasource is empty\n    // 5) INSERT with one segment -> Datasource has one segment\n    // 6) INSERT OVERWRITE with one segment -> Datasource has one segment\n    // 7) INSERT with one segment -> Datasource has two segments\n    // 8) INSERT OVERWRITE with no segments -> Datasource is empty\n\n    // We start:\n    // #1\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #2\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #3\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #4\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #5\n    DataSegment dataSegment1 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath1 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment1,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment1, descriptorPath1);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #6\n    DataSegment dataSegment2 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(200, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath2 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment2,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment2, descriptorPath2);\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #7\n    DataSegment dataSegment3 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 200, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath3 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment3,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment3, descriptorPath3);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(2, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #8\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWithConflictingIntervalSegment()",
            " 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685 -\n 686  \n 687  \n 688 -\n 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706 -\n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithConflictingIntervalSegment()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays.asList(\n            createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                    new Interval(150, 200),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                    new Interval(200, 300),\n                    \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending segment with conflicting interval\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 300), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n  }",
            " 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686 +\n 687  \n 688  \n 689 +\n 690  \n 691  \n 692 +\n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707 +\n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithConflictingIntervalSegment()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays.asList(\n            createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                    new Interval(150, 200, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                    new Interval(200, 300, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending segment with conflicting interval\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 300, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoTable()",
            " 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 -\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530 -\n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  ",
            "  @Test\n  public void testCommitInsertIntoTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 +\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531 +\n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  @Test\n  public void testCommitInsertIntoTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ]
    ],
    "5ba634aa665416743d2f63cbb12f601fe408fe9a": [
        [
            "TezSessionState::ensureLocalResources(Configuration,String)",
            " 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598 -\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610 -\n 611 -\n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  ",
            "  /** This is called in openInternal and in TezTask.updateSession to localize conf resources. */\n  public void ensureLocalResources(Configuration conf, String[] newFilesNotFromConf)\n          throws IOException, LoginException, URISyntaxException, TezException {\n    if (resources == null) {\n      throw new AssertionError(\"Ensure called on an unitialized (or closed) session \" + sessionId);\n    }\n    String dir = resources.dagResourcesDir.toString();\n    resources.localizedResources.clear();\n\n    // Always localize files from conf; duplicates are handled on FS level.\n    // TODO: we could do the same thing as below and only localize if missing.\n    //       That could be especially valuable given that this almost always the same set.\n    List<LocalResource> lrs = utils.localizeTempFilesFromConf(dir, conf);\n    if (lrs != null) {\n      resources.localizedResources.addAll(lrs);\n    }\n\n    // Localize the non-conf resources that are missing from the current list.\n    List<LocalResource> newResources = null;\n    if (newFilesNotFromConf != null && newFilesNotFromConf.length > 0) {\n      boolean hasResources = !resources.additionalFilesNotFromConf.isEmpty();\n      if (hasResources) {\n        for (String s : newFilesNotFromConf) {\n          hasResources = resources.additionalFilesNotFromConf.contains(s);\n          if (!hasResources) {\n            break;\n          }\n        }\n      }\n      if (!hasResources) {\n        String[] skipFilesFromConf = DagUtils.getTempFilesFromConf(conf);\n        newResources = utils.localizeTempFiles(dir, conf, newFilesNotFromConf, skipFilesFromConf);\n        if (newResources != null) {\n          resources.localizedResources.addAll(newResources);\n        }\n        for (String fullName : newFilesNotFromConf) {\n          resources.additionalFilesNotFromConf.add(fullName);\n        }\n      }\n    }\n\n    // Finally, add the files to the existing AM (if any). The old code seems to do this twice,\n    // first for all the new resources regardless of type; and then for all the session resources\n    // that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession\n    // and with resourceMap from submit).\n    // TODO: Do we really need all this nonsense?\n    if (session != null) {\n      if (newResources != null && !newResources.isEmpty()) {\n        session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n      }\n      if (!resources.localizedResources.isEmpty()) {\n        session.addAppMasterLocalFiles(\n            DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n      }\n    }\n  }",
            " 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598 +\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610 +\n 611 +\n 612  \n 613 +\n 614 +\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  /** This is called in openInternal and in TezTask.updateSession to localize conf resources. */\n  public void ensureLocalResources(Configuration conf, String[] newFilesNotFromConf)\n          throws IOException, LoginException, URISyntaxException, TezException {\n    if (resources == null) {\n      throw new AssertionError(\"Ensure called on an unitialized (or closed) session \" + sessionId);\n    }\n    String dir = resources.dagResourcesDir.toString();\n    resources.localizedResources.clear();\n\n    // Always localize files from conf; duplicates are handled on FS level.\n    // TODO: we could do the same thing as below and only localize if missing.\n    //       That could be especially valuable given that this almost always the same set.\n    List<LocalResource> lrs = utils.localizeTempFilesFromConf(dir, conf);\n    if (lrs != null) {\n      resources.localizedResources.addAll(lrs);\n    }\n\n    // Localize the non-conf resources that are missing from the current list.\n    List<LocalResource> newResources = null;\n    if (newFilesNotFromConf != null && newFilesNotFromConf.length > 0) {\n      boolean hasResources = !resources.additionalFilesNotFromConf.isEmpty();\n      if (hasResources) {\n        for (String s : newFilesNotFromConf) {\n          hasResources = resources.additionalFilesNotFromConf.keySet().contains(s);\n          if (!hasResources) {\n            break;\n          }\n        }\n      }\n      if (!hasResources) {\n        String[] skipFilesFromConf = DagUtils.getTempFilesFromConf(conf);\n        newResources = utils.localizeTempFiles(dir, conf, newFilesNotFromConf, skipFilesFromConf);\n        if (newResources != null) {\n          resources.localizedResources.addAll(newResources);\n        }\n        for (int i=0;i<newFilesNotFromConf.length;i++) {\n          resources.additionalFilesNotFromConf.put(newFilesNotFromConf[i], newResources.get(i));\n        }\n      } else {\n        resources.localizedResources.addAll(resources.additionalFilesNotFromConf.values());\n      }\n    }\n\n    // Finally, add the files to the existing AM (if any). The old code seems to do this twice,\n    // first for all the new resources regardless of type; and then for all the session resources\n    // that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession\n    // and with resourceMap from submit).\n    // TODO: Do we really need all this nonsense?\n    if (session != null) {\n      if (newResources != null && !newResources.isEmpty()) {\n        session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n      }\n      if (!resources.localizedResources.isEmpty()) {\n        session.addAppMasterLocalFiles(\n            DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n      }\n    }\n  }"
        ]
    ],
    "610748287846cbd26d0b7c8ccc414f8636fb6ba1": [
        [
            "TestJdbcWithMiniHS2::testHttpHeaderSize()",
            " 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 -\n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 -\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  ",
            "  /**\n   * Test for http header size\n   * @throws Exception\n   */\n  @Test\n  public void testHttpHeaderSize() throws Exception {\n    // Stop HiveServer2\n    stopMiniHS2();\n    HiveConf conf = new HiveConf();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 1024);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 1024);\n    startMiniHS2(conf, true);\n\n    // Username and password are added to the http request header.\n    // We will test the reconfiguration of the header size by changing the password length.\n    String userName = \"userName\";\n    String password = StringUtils.leftPad(\"*\", 100);\n    Connection conn = null;\n    // This should go fine, since header should be less than the configured header size\n    try {\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // This should fail with given HTTP response code 413 in error message, since header is more\n    // than the configured the header size\n    password = StringUtils.leftPad(\"*\", 2000);\n    Exception headerException = null;\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      headerException = e;\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n\n      assertTrue(\"Header exception should be thrown\", headerException != null);\n      assertTrue(\"Incorrect HTTP Response:\" + headerException.getMessage(),\n          headerException.getMessage().contains(\"HTTP Response code: 413\"));\n    }\n\n    // Stop HiveServer2 to increase header size\n    stopMiniHS2();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 3000);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 3000);\n    startMiniHS2(conf);\n\n    // This should now go fine, since we increased the configured header size\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // Restore original state\n    restoreMiniHS2AndConnections();\n  }",
            " 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979 +\n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 +\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  ",
            "  /**\n   * Test for http header size\n   * @throws Exception\n   */\n  @Test\n  public void testHttpHeaderSize() throws Exception {\n    // Stop HiveServer2\n    stopMiniHS2();\n    HiveConf conf = new HiveConf();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 1024);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 1024);\n    startMiniHS2(conf, true);\n\n    // Username and password are added to the http request header.\n    // We will test the reconfiguration of the header size by changing the password length.\n    String userName = \"userName\";\n    String password = StringUtils.leftPad(\"*\", 100);\n    Connection conn = null;\n    // This should go fine, since header should be less than the configured header size\n    try {\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // This should fail with given HTTP response code 431 in error message, since header is more\n    // than the configured the header size\n    password = StringUtils.leftPad(\"*\", 2000);\n    Exception headerException = null;\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      headerException = e;\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n\n      assertTrue(\"Header exception should be thrown\", headerException != null);\n      assertTrue(\"Incorrect HTTP Response:\" + headerException.getMessage(),\n          headerException.getMessage().contains(\"HTTP Response code: 431\"));\n    }\n\n    // Stop HiveServer2 to increase header size\n    stopMiniHS2();\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_REQUEST_HEADER_SIZE, 3000);\n    conf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_RESPONSE_HEADER_SIZE, 3000);\n    startMiniHS2(conf);\n\n    // This should now go fine, since we increased the configured header size\n    try {\n      conn = null;\n      conn = getConnection(miniHS2.getJdbcURL(testDbName), userName, password);\n    } catch (Exception e) {\n      fail(\"Not expecting exception: \" + e);\n    } finally {\n      if (conn != null) {\n        conn.close();\n      }\n    }\n\n    // Restore original state\n    restoreMiniHS2AndConnections();\n  }"
        ]
    ],
    "691e6544833f0e982d848eef181a80f6cf75a3b6": [
        [
            "Hive::dropTable(String,String,boolean,boolean,boolean)",
            "1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  ",
            "  /**\n   * Drops the table.\n   *\n   * @param dbName\n   * @param tableName\n   * @param deleteData\n   *          deletes the underlying data along with metadata\n   * @param ignoreUnknownTab\n   *          an exception is thrown if this is false and the table doesn't exist\n   * @param ifPurge\n   *          completely purge the table skipping trash while removing data from warehouse\n   * @throws HiveException\n   */\n  public void dropTable(String dbName, String tableName, boolean deleteData,\n      boolean ignoreUnknownTab, boolean ifPurge) throws HiveException {\n    try {\n      getMSC().dropTable(dbName, tableName, deleteData, ignoreUnknownTab, ifPurge);\n    } catch (NoSuchObjectException e) {\n      if (!ignoreUnknownTab) {\n        throw new HiveException(e);\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            "1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050 +\n1051 +\n1052 +\n1053 +\n1054 +\n1055 +\n1056 +\n1057  \n1058  \n1059  \n1060  ",
            "  /**\n   * Drops the table.\n   *\n   * @param dbName\n   * @param tableName\n   * @param deleteData\n   *          deletes the underlying data along with metadata\n   * @param ignoreUnknownTab\n   *          an exception is thrown if this is false and the table doesn't exist\n   * @param ifPurge\n   *          completely purge the table skipping trash while removing data from warehouse\n   * @throws HiveException\n   */\n  public void dropTable(String dbName, String tableName, boolean deleteData,\n      boolean ignoreUnknownTab, boolean ifPurge) throws HiveException {\n    try {\n      getMSC().dropTable(dbName, tableName, deleteData, ignoreUnknownTab, ifPurge);\n    } catch (NoSuchObjectException e) {\n      if (!ignoreUnknownTab) {\n        throw new HiveException(e);\n      }\n    } catch (MetaException e) {\n      int idx = ExceptionUtils.indexOfType(e, SQLIntegrityConstraintViolationException.class);\n      if (idx != -1 && ExceptionUtils.getThrowables(e)[idx].getMessage().contains(\"MV_TABLES_USED\")) {\n        throw new HiveException(\"Cannot drop table since it is used by at least one materialized view definition. \" +\n            \"Please drop any materialized view that uses the table before dropping it\", e);\n      }\n      throw new HiveException(e);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ]
    ],
    "766c3dc21e189afbecace308cd24cd1c5bde09b2": [
        [
            "SemanticAnalyzer::addDefaultProperties(Map,boolean,StorageFormat,String,List,boolean,boolean)",
            "12820  \n12821  \n12822  \n12823  \n12824  \n12825  \n12826  \n12827  \n12828  \n12829  \n12830  \n12831  \n12832  \n12833  \n12834  \n12835  \n12836  \n12837  \n12838  \n12839  \n12840  \n12841  \n12842  \n12843  \n12844  \n12845  \n12846  \n12847  \n12848  \n12849  \n12850 -\n12851  \n12852  \n12853  \n12854  \n12855  \n12856  \n12857  \n12858  \n12859  \n12860  \n12861  \n12862  \n12863  \n12864  \n12865  \n12866  \n12867  \n12868  \n12869  \n12870  \n12871  \n12872  \n12873  \n12874  \n12875  \n12876  \n12877  \n12878  \n12879  \n12880  \n12881  \n12882  \n12883  \n12884  \n12885  \n12886  \n12887  \n12888  \n12889  \n12890  \n12891  \n12892  ",
            "  /**\n   * Add default properties for table property. If a default parameter exists\n   * in the tblProp, the value in tblProp will be kept.\n   *\n   * @param tblProp\n   *          property map\n   * @return Modified table property map\n   */\n  private Map<String, String> addDefaultProperties(\n      Map<String, String> tblProp, boolean isExt, StorageFormat storageFormat,\n      String qualifiedTableName, List<Order> sortCols, boolean isMaterialization,\n      boolean isTemporaryTable) {\n    Map<String, String> retValue;\n    if (tblProp == null) {\n      retValue = new HashMap<String, String>();\n    } else {\n      retValue = tblProp;\n    }\n    String paraString = HiveConf.getVar(conf, ConfVars.NEWTABLEDEFAULTPARA);\n    if (paraString != null && !paraString.isEmpty()) {\n      for (String keyValuePair : paraString.split(\",\")) {\n        String[] keyValue = keyValuePair.split(\"=\", 2);\n        if (keyValue.length != 2) {\n          continue;\n        }\n        if (!retValue.containsKey(keyValue[0])) {\n          retValue.put(keyValue[0], keyValue[1]);\n        }\n      }\n    }\n    boolean makeInsertOnly = HiveConf.getBoolVar(conf, ConfVars.HIVE_CREATE_TABLES_AS_INSERT_ONLY);\n    boolean makeAcid = !isTemporaryTable &&\n        MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID) &&\n        HiveConf.getBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY) &&\n        DbTxnManager.class.getCanonicalName().equals(HiveConf.getVar(conf, ConfVars.HIVE_TXN_MANAGER));\n    if ((makeInsertOnly || makeAcid)\n        && !isExt  && !isMaterialization && StringUtils.isBlank(storageFormat.getStorageHandler())\n        //don't overwrite user choice if transactional attribute is explicitly set\n        && !retValue.containsKey(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)) {\n      if (makeInsertOnly) {\n        retValue.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, \"true\");\n        retValue.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n            TransactionalValidationListener.INSERTONLY_TRANSACTIONAL_PROPERTY);\n      }\n      if (makeAcid) {\n        /*for CTAS, TransactionalValidationListener.makeAcid() runs to late to make table Acid\n         so the initial write ends up running as non-acid...*/\n        try {\n          Class inputFormatClass = storageFormat.getInputFormat() == null ? null :\n              Class.forName(storageFormat.getInputFormat());\n          Class outputFormatClass = storageFormat.getOutputFormat() == null ? null :\n              Class.forName(storageFormat.getOutputFormat());\n          if (inputFormatClass == null || outputFormatClass == null ||\n              !AcidInputFormat.class.isAssignableFrom(inputFormatClass) ||\n              !AcidOutputFormat.class.isAssignableFrom(outputFormatClass)) {\n            return retValue;\n          }\n        } catch (ClassNotFoundException e) {\n          LOG.warn(\"Could not verify InputFormat=\" + storageFormat.getInputFormat() + \" or OutputFormat=\" +\n              storageFormat.getOutputFormat() + \"  for \" + qualifiedTableName);\n          return retValue;\n        }\n        if(sortCols != null && !sortCols.isEmpty()) {\n          return retValue;\n        }\n        retValue.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, \"true\");\n        retValue.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n            TransactionalValidationListener.DEFAULT_TRANSACTIONAL_PROPERTY);\n        LOG.info(\"Automatically chose to make \" + qualifiedTableName + \" acid.\");\n      }\n    }\n    return retValue;\n  }",
            "12820  \n12821  \n12822  \n12823  \n12824  \n12825  \n12826  \n12827  \n12828  \n12829  \n12830  \n12831  \n12832  \n12833  \n12834  \n12835  \n12836  \n12837  \n12838  \n12839  \n12840  \n12841  \n12842  \n12843  \n12844  \n12845  \n12846  \n12847  \n12848  \n12849  \n12850 +\n12851  \n12852  \n12853  \n12854  \n12855  \n12856  \n12857  \n12858  \n12859  \n12860  \n12861  \n12862  \n12863  \n12864  \n12865  \n12866  \n12867  \n12868  \n12869  \n12870  \n12871  \n12872  \n12873  \n12874  \n12875  \n12876  \n12877  \n12878  \n12879  \n12880  \n12881  \n12882  \n12883  \n12884  \n12885  \n12886  \n12887  \n12888  \n12889  \n12890  \n12891  \n12892  ",
            "  /**\n   * Add default properties for table property. If a default parameter exists\n   * in the tblProp, the value in tblProp will be kept.\n   *\n   * @param tblProp\n   *          property map\n   * @return Modified table property map\n   */\n  private Map<String, String> addDefaultProperties(\n      Map<String, String> tblProp, boolean isExt, StorageFormat storageFormat,\n      String qualifiedTableName, List<Order> sortCols, boolean isMaterialization,\n      boolean isTemporaryTable) {\n    Map<String, String> retValue;\n    if (tblProp == null) {\n      retValue = new HashMap<String, String>();\n    } else {\n      retValue = tblProp;\n    }\n    String paraString = HiveConf.getVar(conf, ConfVars.NEWTABLEDEFAULTPARA);\n    if (paraString != null && !paraString.isEmpty()) {\n      for (String keyValuePair : paraString.split(\",\")) {\n        String[] keyValue = keyValuePair.split(\"=\", 2);\n        if (keyValue.length != 2) {\n          continue;\n        }\n        if (!retValue.containsKey(keyValue[0])) {\n          retValue.put(keyValue[0], keyValue[1]);\n        }\n      }\n    }\n    boolean makeInsertOnly = !isTemporaryTable && HiveConf.getBoolVar(conf, ConfVars.HIVE_CREATE_TABLES_AS_INSERT_ONLY);\n    boolean makeAcid = !isTemporaryTable &&\n        MetastoreConf.getBoolVar(conf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID) &&\n        HiveConf.getBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY) &&\n        DbTxnManager.class.getCanonicalName().equals(HiveConf.getVar(conf, ConfVars.HIVE_TXN_MANAGER));\n    if ((makeInsertOnly || makeAcid)\n        && !isExt  && !isMaterialization && StringUtils.isBlank(storageFormat.getStorageHandler())\n        //don't overwrite user choice if transactional attribute is explicitly set\n        && !retValue.containsKey(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)) {\n      if (makeInsertOnly) {\n        retValue.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, \"true\");\n        retValue.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n            TransactionalValidationListener.INSERTONLY_TRANSACTIONAL_PROPERTY);\n      }\n      if (makeAcid) {\n        /*for CTAS, TransactionalValidationListener.makeAcid() runs to late to make table Acid\n         so the initial write ends up running as non-acid...*/\n        try {\n          Class inputFormatClass = storageFormat.getInputFormat() == null ? null :\n              Class.forName(storageFormat.getInputFormat());\n          Class outputFormatClass = storageFormat.getOutputFormat() == null ? null :\n              Class.forName(storageFormat.getOutputFormat());\n          if (inputFormatClass == null || outputFormatClass == null ||\n              !AcidInputFormat.class.isAssignableFrom(inputFormatClass) ||\n              !AcidOutputFormat.class.isAssignableFrom(outputFormatClass)) {\n            return retValue;\n          }\n        } catch (ClassNotFoundException e) {\n          LOG.warn(\"Could not verify InputFormat=\" + storageFormat.getInputFormat() + \" or OutputFormat=\" +\n              storageFormat.getOutputFormat() + \"  for \" + qualifiedTableName);\n          return retValue;\n        }\n        if(sortCols != null && !sortCols.isEmpty()) {\n          return retValue;\n        }\n        retValue.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, \"true\");\n        retValue.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n            TransactionalValidationListener.DEFAULT_TRANSACTIONAL_PROPERTY);\n        LOG.info(\"Automatically chose to make \" + qualifiedTableName + \" acid.\");\n      }\n    }\n    return retValue;\n  }"
        ]
    ],
    "ae82715f1014c4ed514441311b61ed1891e2a12b": [
        [
            "HiveStatement::getQueryId()",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026 -\n1027 -\n1028 -\n1029  \n1030  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return null;\n    }\n  }",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  ",
            "  /**\n   * Returns the Query ID if it is running.\n   * This method is a public API for usage outside of Hive, although it is not part of the\n   * interface java.sql.Statement.\n   * @return Valid query ID if it is running else returns NULL.\n   * @throws SQLException If any internal failures.\n   */\n  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n  public String getQueryId() throws SQLException {\n    if (stmtHandle == null) {\n      // If query is not running or already closed.\n      return null;\n    }\n    try {\n      return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n    } catch (TException e) {\n      throw new SQLException(e);\n    }\n  }"
        ],
        [
            "ThriftCLIService::GetQueryId(TGetQueryIdReq)",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    }\n  }",
            " 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857 +\n 858 +\n 859 +\n 860  \n 861  ",
            "  @Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n    try {\n      return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n    } catch (HiveSQLException e) {\n      throw new TException(e);\n    } catch (Exception e) {\n      // If concurrently the query is closed before we fetch queryID.\n      return new TGetQueryIdResp((String)null);\n    }\n  }"
        ]
    ],
    "84b5ba7ac9f93c6a496386db91ae4cd5ab7a451d": [
        [
            "GenericUDTFGetSplits::getSplits(JobConf,int,TezWork,Schema,ApplicationId,boolean)",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 -\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = UserGroupInformation.getLoginUser().getShortUserName();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "  public InputSplit[] getSplits(JobConf job, int numSplits, TezWork work, Schema schema, ApplicationId applicationId,\n    final boolean generateSingleSplit)\n    throws IOException {\n\n    if(numSplits == 0) {\n      //Schema only\n      LlapInputSplit schemaSplit = new LlapInputSplit(\n          0, new byte[0], new byte[0], new byte[0],\n          new SplitLocationInfo[0], schema, \"\", new byte[0]);\n      return new InputSplit[] { schemaSplit };\n    }\n\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(job.getCredentials());\n\n    DagUtils utils = DagUtils.getInstance();\n    Context ctx = new Context(job);\n    MapWork mapWork = (MapWork) work.getAllWork().get(0);\n    // bunch of things get setup in the context based on conf but we need only the MR tmp directory\n    // for the following method.\n    JobConf wxConf = utils.initializeVertexConf(job, ctx, mapWork);\n    // TODO: should we also whitelist input formats here? from mapred.input.format.class\n    Path scratchDir = utils.createTezDir(ctx.getMRScratchDir(), job);\n    FileSystem fs = scratchDir.getFileSystem(job);\n    try {\n      LocalResource appJarLr = createJarLocalResource(utils.getExecJarPathLocal(ctx.getConf()), utils, job);\n\n      LlapCoordinator coordinator = LlapCoordinator.getInstance();\n      if (coordinator == null) {\n        throw new IOException(\"LLAP coordinator is not initialized; must be running in HS2 with \"\n            + ConfVars.LLAP_HS2_ENABLE_COORDINATOR.varname + \" enabled\");\n      }\n\n      // Update the queryId to use the generated applicationId. See comment below about\n      // why this is done.\n      HiveConf.setVar(wxConf, HiveConf.ConfVars.HIVEQUERYID, applicationId.toString());\n      Vertex wx = utils.createVertex(wxConf, mapWork, scratchDir, fs, ctx, false, work,\n          work.getVertexType(mapWork), DagUtils.createTezLrMap(appJarLr, null));\n      String vertexName = wx.getName();\n      dag.addVertex(wx);\n      utils.addCredentials(mapWork, dag);\n\n\n      // we have the dag now proceed to get the splits:\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS));\n      Preconditions.checkState(HiveConf.getBoolVar(wxConf,\n              ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS));\n\n      HiveSplitGenerator splitGenerator = new HiveSplitGenerator(wxConf, mapWork, generateSingleSplit);\n      List<Event> eventList = splitGenerator.initialize();\n      InputSplit[] result = new InputSplit[eventList.size() - 1];\n\n      InputConfigureVertexTasksEvent configureEvent\n        = (InputConfigureVertexTasksEvent) eventList.get(0);\n\n      List<TaskLocationHint> hints = configureEvent.getLocationHint().getTaskLocationHints();\n\n      Preconditions.checkState(hints.size() == eventList.size() - 1);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"NumEvents=\" + eventList.size() + \", NumSplits=\" + result.length);\n      }\n\n      // This assumes LLAP cluster owner is always the HS2 user.\n      String llapUser = RegistryUtils.currentUser();\n\n      String queryUser = null;\n      byte[] tokenBytes = null;\n      LlapSigner signer = null;\n      if (UserGroupInformation.isSecurityEnabled()) {\n        signer = coordinator.getLlapSigner(job);\n\n        // 1. Generate the token for query user (applies to all splits).\n        queryUser = SessionState.getUserFromAuthenticator();\n        if (queryUser == null) {\n          queryUser = UserGroupInformation.getCurrentUser().getUserName();\n          LOG.warn(\"Cannot determine the session user; using \" + queryUser + \" instead\");\n        }\n        LlapTokenLocalClient tokenClient = coordinator.getLocalTokenClient(job, llapUser);\n        // We put the query user, not LLAP user, into the message and token.\n        Token<LlapTokenIdentifier> token = tokenClient.createToken(\n            applicationId.toString(), queryUser, true);\n        LOG.info(\"Created the token for remote user: {}\", token);\n        bos.reset();\n        token.write(dos);\n        tokenBytes = bos.toByteArray();\n      } else {\n        queryUser = UserGroupInformation.getCurrentUser().getUserName();\n      }\n\n      // Generate umbilical token (applies to all splits)\n      Token<JobTokenIdentifier> umbilicalToken = JobTokenCreator.createJobToken(applicationId);\n\n      LOG.info(\"Number of splits: \" + (eventList.size() - 1));\n      SignedMessage signedSvs = null;\n      for (int i = 0; i < eventList.size() - 1; i++) {\n        TaskSpec taskSpec = new TaskSpecBuilder().constructTaskSpec(dag, vertexName,\n              eventList.size() - 1, applicationId, i);\n\n        // 2. Generate the vertex/submit information for all events.\n        if (i == 0) {\n          // The queryId could either be picked up from the current request being processed, or\n          // generated. The current request isn't exactly correct since the query is 'done' once we\n          // return the results. Generating a new one has the added benefit of working once this\n          // is moved out of a UDTF into a proper API.\n          // Setting this to the generated AppId which is unique.\n          // Despite the differences in TaskSpec, the vertex spec should be the same.\n          signedSvs = createSignedVertexSpec(signer, taskSpec, applicationId, queryUser,\n              applicationId.toString());\n        }\n\n        SubmitWorkInfo submitWorkInfo = new SubmitWorkInfo(applicationId,\n            System.currentTimeMillis(), taskSpec.getVertexParallelism(), signedSvs.message,\n            signedSvs.signature, umbilicalToken);\n        byte[] submitWorkBytes = SubmitWorkInfo.toBytes(submitWorkInfo);\n\n        // 3. Generate input event.\n        SignedMessage eventBytes = makeEventBytes(wx, vertexName, eventList.get(i + 1), signer);\n\n        // 4. Make location hints.\n        SplitLocationInfo[] locations = makeLocationHints(hints.get(i));\n\n        result[i] = new LlapInputSplit(i, submitWorkBytes, eventBytes.message,\n            eventBytes.signature, locations, schema, llapUser, tokenBytes);\n       }\n      return result;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ]
    ]
}