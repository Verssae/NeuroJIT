{
    "01880a470c31c6c27bb82d73727623fe253eef9f": [
        [
            "IndexSummaryManagerTest::createSSTables(String,String,int,int)",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 -\n 184 -\n 185 -\n 186 -\n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  ",
            "    private void createSSTables(String ksname, String cfname, int numSSTables, int numRows)\n    {\n        Keyspace keyspace = Keyspace.open(ksname);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfname);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        ArrayList<Future> futures = new ArrayList<>(numSSTables);\n        ByteBuffer value = ByteBuffer.wrap(new byte[100]);\n        for (int sstable = 0; sstable < numSSTables; sstable++)\n        {\n            for (int row = 0; row < numRows; row++)\n            {\n                DecoratedKey key = Util.dk(String.format(\"%3d\", row));\n                Mutation rm = new Mutation(ksname, key.getKey());\n                rm.add(cfname, Util.cellname(\"column\"), value, 0);\n                rm.applyUnsafe();\n            }\n            futures.add(cfs.forceFlush());\n        }\n        for (Future future : futures)\n        {\n            try\n            {\n                future.get();\n            } catch (InterruptedException e)\n            {\n                throw new RuntimeException(e);\n            }\n            catch (ExecutionException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }\n        assertEquals(numSSTables, cfs.getSSTables().size());\n        validateData(cfs, numRows);\n    }",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "    private void createSSTables(String ksname, String cfname, int numSSTables, int numRows)\n    {\n        Keyspace keyspace = Keyspace.open(ksname);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfname);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        ArrayList<Future> futures = new ArrayList<>(numSSTables);\n        ByteBuffer value = ByteBuffer.wrap(new byte[100]);\n        for (int sstable = 0; sstable < numSSTables; sstable++)\n        {\n            for (int row = 0; row < numRows; row++)\n            {\n                DecoratedKey key = Util.dk(String.format(\"%3d\", row));\n                Mutation rm = new Mutation(ksname, key.getKey());\n                rm.add(cfname, Util.cellname(\"column\"), value, 0);\n                rm.applyUnsafe();\n            }\n            futures.add(cfs.forceFlush());\n        }\n        for (Future future : futures)\n        {\n            try\n            {\n                future.get();\n            } catch (InterruptedException | ExecutionException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }\n        assertEquals(numSSTables, cfs.getSSTables().size());\n        validateData(cfs, numRows);\n    }"
        ],
        [
            "TypeParserTest::testParseError()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "    @Test\n    public void testParseError()\n    {\n        try\n        {\n            TypeParser.parse(\"y\");\n            fail(\"Should not pass\");\n        }\n        catch (ConfigurationException e) {}\n        catch (SyntaxException e) {}\n\n        try\n        {\n            TypeParser.parse(\"LongType(reversed@)\");\n            fail(\"Should not pass\");\n        }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    @Test\n    public void testParseError()\n    {\n        try\n        {\n            TypeParser.parse(\"y\");\n            fail(\"Should not pass\");\n        }\n        catch (ConfigurationException | SyntaxException e) {}\n\n        try\n        {\n            TypeParser.parse(\"LongType(reversed@)\");\n            fail(\"Should not pass\");\n        }"
        ],
        [
            "HadoopCompat::invoke(Method,Object,Object)",
            " 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252 -\n 253 -\n 254 -\n 255  \n 256  \n 257  ",
            "    /**\n     * Invokes a method and rethrows any exception as runtime excetpions.\n     */\n    private static Object invoke(Method method, Object obj, Object... args) {\n        try {\n            return method.invoke(obj, args);\n        } catch (IllegalAccessException e) {\n            throw new IllegalArgumentException(\"Can't invoke method \" + method.getName(), e);\n        } catch (InvocationTargetException e) {\n            throw new IllegalArgumentException(\"Can't invoke method \" + method.getName(), e);\n        }\n    }",
            " 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248 +\n 249  \n 250  \n 251  ",
            "    /**\n     * Invokes a method and rethrows any exception as runtime excetpions.\n     */\n    private static Object invoke(Method method, Object obj, Object... args) {\n        try {\n            return method.invoke(obj, args);\n        } catch (IllegalAccessException | InvocationTargetException e) {\n            throw new IllegalArgumentException(\"Can't invoke method \" + method.getName(), e);\n        }\n    }"
        ],
        [
            "RoundRobinScheduler::queue(Thread,String,long)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    public void queue(Thread t, String id, long timeoutMS) throws TimeoutException\n    {\n        WeightedQueue weightedQueue = getWeightedQueue(id);\n\n        try\n        {\n            queueSize.release();\n            try\n            {\n                weightedQueue.put(t, timeoutMS);\n                // the scheduler will release us when a slot is available\n            }\n            catch (TimeoutException e)\n            {\n                queueSize.acquireUninterruptibly();\n                throw e;\n            }\n            catch (InterruptedException e)\n            {\n                queueSize.acquireUninterruptibly();\n                throw e;\n            }\n        }\n        catch (InterruptedException e)\n        {\n            throw new RuntimeException(\"Interrupted while queueing requests\", e);\n        }\n    }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "    public void queue(Thread t, String id, long timeoutMS) throws TimeoutException\n    {\n        WeightedQueue weightedQueue = getWeightedQueue(id);\n\n        try\n        {\n            queueSize.release();\n            try\n            {\n                weightedQueue.put(t, timeoutMS);\n                // the scheduler will release us when a slot is available\n            }\n            catch (TimeoutException | InterruptedException e)\n            {\n                queueSize.acquireUninterruptibly();\n                throw e;\n            }\n        }\n        catch (InterruptedException e)\n        {\n            throw new RuntimeException(\"Interrupted while queueing requests\", e);\n        }\n    }"
        ],
        [
            "TypeParser::getAbstractType(String,TypeParser)",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 -\n 372 -\n 373 -\n 374 -\n 375 -\n 376 -\n 377 -\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "    private static AbstractType<?> getAbstractType(String compareWith, TypeParser parser) throws SyntaxException, ConfigurationException\n    {\n        String className = compareWith.contains(\".\") ? compareWith : \"org.apache.cassandra.db.marshal.\" + compareWith;\n        Class<? extends AbstractType<?>> typeClass = FBUtilities.<AbstractType<?>>classForName(className, \"abstract-type\");\n        try\n        {\n            Method method = typeClass.getDeclaredMethod(\"getInstance\", TypeParser.class);\n            return (AbstractType<?>) method.invoke(null, parser);\n        }\n        catch (NoSuchMethodException e)\n        {\n            // Trying to see if we have an instance field and apply the default parameter to it\n            AbstractType<?> type = getRawAbstractType(typeClass);\n            return AbstractType.parseDefaultParameters(type, parser);\n        }\n        catch (IllegalAccessException e)\n        {\n            // Trying to see if we have an instance field and apply the default parameter to it\n            AbstractType<?> type = getRawAbstractType(typeClass);\n            return AbstractType.parseDefaultParameters(type, parser);\n        }\n        catch (InvocationTargetException e)\n        {\n            ConfigurationException ex = new ConfigurationException(\"Invalid definition for comparator \" + typeClass.getName() + \".\");\n            ex.initCause(e.getTargetException());\n            throw ex;\n        }\n    }",
            " 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366 +\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "    private static AbstractType<?> getAbstractType(String compareWith, TypeParser parser) throws SyntaxException, ConfigurationException\n    {\n        String className = compareWith.contains(\".\") ? compareWith : \"org.apache.cassandra.db.marshal.\" + compareWith;\n        Class<? extends AbstractType<?>> typeClass = FBUtilities.<AbstractType<?>>classForName(className, \"abstract-type\");\n        try\n        {\n            Method method = typeClass.getDeclaredMethod(\"getInstance\", TypeParser.class);\n            return (AbstractType<?>) method.invoke(null, parser);\n        }\n        catch (NoSuchMethodException | IllegalAccessException e)\n        {\n            // Trying to see if we have an instance field and apply the default parameter to it\n            AbstractType<?> type = getRawAbstractType(typeClass);\n            return AbstractType.parseDefaultParameters(type, parser);\n        }\n        catch (InvocationTargetException e)\n        {\n            ConfigurationException ex = new ConfigurationException(\"Invalid definition for comparator \" + typeClass.getName() + \".\");\n            ex.initCause(e.getTargetException());\n            throw ex;\n        }\n    }"
        ],
        [
            "AbstractColumnFamilyInputFormat::getRangeMap(Configuration)",
            " 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321 -\n 322 -\n 323 -\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "    private List<TokenRange> getRangeMap(Configuration conf) throws IOException\n    {\n        Cassandra.Client client = ConfigHelper.getClientFromInputAddressList(conf);\n\n        List<TokenRange> map;\n        try\n        {\n            map = client.describe_local_ring(ConfigHelper.getInputKeyspace(conf));\n        }\n        catch (InvalidRequestException e)\n        {\n            throw new RuntimeException(e);\n        }\n        catch (TException e)\n        {\n            throw new RuntimeException(e);\n        }\n        return map;\n    }",
            " 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 +\n 325  \n 326  ",
            "    private List<TokenRange> getRangeMap(Configuration conf) throws IOException\n    {\n        Cassandra.Client client = ConfigHelper.getClientFromInputAddressList(conf);\n\n        List<TokenRange> map;\n        try\n        {\n            map = client.describe_local_ring(ConfigHelper.getInputKeyspace(conf));\n        }\n        catch (TException e)\n        {\n            throw new RuntimeException(e);\n        }\n        \n        return map;\n    }"
        ],
        [
            "AbstractCassandraStorage::getValidatorMap(CfDef)",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "    /** get the validators */\n    protected Map<ByteBuffer, AbstractType> getValidatorMap(CfDef cfDef) throws IOException\n    {\n        Map<ByteBuffer, AbstractType> validators = new HashMap<ByteBuffer, AbstractType>();\n        for (ColumnDef cd : cfDef.getColumn_metadata())\n        {\n            if (cd.getValidation_class() != null && !cd.getValidation_class().isEmpty())\n            {\n                AbstractType validator = null;\n                try\n                {\n                    validator = TypeParser.parse(cd.getValidation_class());\n                    if (validator instanceof CounterColumnType)\n                        validator = LongType.instance; \n                    validators.put(cd.name, validator);\n                }\n                catch (ConfigurationException e)\n                {\n                    throw new IOException(e);\n                }\n                catch (SyntaxException e)\n                {\n                    throw new IOException(e);\n                }\n            }\n        }\n        return validators;\n    }",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "    /** get the validators */\n    protected Map<ByteBuffer, AbstractType> getValidatorMap(CfDef cfDef) throws IOException\n    {\n        Map<ByteBuffer, AbstractType> validators = new HashMap<ByteBuffer, AbstractType>();\n        for (ColumnDef cd : cfDef.getColumn_metadata())\n        {\n            if (cd.getValidation_class() != null && !cd.getValidation_class().isEmpty())\n            {\n                AbstractType validator = null;\n                try\n                {\n                    validator = TypeParser.parse(cd.getValidation_class());\n                    if (validator instanceof CounterColumnType)\n                        validator = LongType.instance; \n                    validators.put(cd.name, validator);\n                }\n                catch (ConfigurationException | SyntaxException e)\n                {\n                    throw new IOException(e);\n                }\n            }\n        }\n        return validators;\n    }"
        ],
        [
            "CassandraMetricsRegistry::registerMBean(Metric,ObjectName)",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  \n 144 -\n 145 -\n 146 -\n 147  \n 148  ",
            "    private void registerMBean(Metric metric, ObjectName name)\n    {\n        AbstractBean mbean;\n\n        if (metric instanceof Gauge)\n        {\n            mbean = new JmxGauge((Gauge<?>) metric, name);\n        } else if (metric instanceof Counter)\n        {\n            mbean = new JmxCounter((Counter) metric, name);\n        } else if (metric instanceof Histogram)\n        {\n            mbean = new JmxHistogram((Histogram) metric, name);\n        } else if (metric instanceof Meter)\n        {\n            mbean = new JmxMeter((Meter) metric, name, TimeUnit.SECONDS);\n        } else if (metric instanceof Timer)\n        {\n            mbean = new JmxTimer((Timer) metric, name, TimeUnit.SECONDS, TimeUnit.MICROSECONDS);\n        } else\n        {\n            throw new IllegalArgumentException(\"Unknown metric type: \" + metric.getClass());\n        }\n\n        try\n        {\n            mBeanServer.registerMBean(mbean, name);\n        } catch (InstanceAlreadyExistsException e)\n        {\n            logger.debug(\"Metric bean already exists\", e);\n        } catch (MBeanRegistrationException e)\n        {\n            logger.debug(\"Unable to register metric bean\", e);\n        } catch (NotCompliantMBeanException e)\n        {\n            logger.warn(\"Unable to register metric bean\", e);\n        }\n    }",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  \n 145  ",
            "    private void registerMBean(Metric metric, ObjectName name)\n    {\n        AbstractBean mbean;\n\n        if (metric instanceof Gauge)\n        {\n            mbean = new JmxGauge((Gauge<?>) metric, name);\n        } else if (metric instanceof Counter)\n        {\n            mbean = new JmxCounter((Counter) metric, name);\n        } else if (metric instanceof Histogram)\n        {\n            mbean = new JmxHistogram((Histogram) metric, name);\n        } else if (metric instanceof Meter)\n        {\n            mbean = new JmxMeter((Meter) metric, name, TimeUnit.SECONDS);\n        } else if (metric instanceof Timer)\n        {\n            mbean = new JmxTimer((Timer) metric, name, TimeUnit.SECONDS, TimeUnit.MICROSECONDS);\n        } else\n        {\n            throw new IllegalArgumentException(\"Unknown metric type: \" + metric.getClass());\n        }\n\n        try\n        {\n            mBeanServer.registerMBean(mbean, name);\n        } catch (InstanceAlreadyExistsException e)\n        {\n            logger.debug(\"Metric bean already exists\", e);\n        } catch (MBeanRegistrationException | NotCompliantMBeanException e)\n        {\n            logger.debug(\"Unable to register metric bean\", e);\n        }\n    }"
        ],
        [
            "TypeParser::getAbstractType(String)",
            " 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 -\n 351 -\n 352 -\n 353 -\n 354 -\n 355 -\n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    private static AbstractType<?> getAbstractType(String compareWith) throws ConfigurationException\n    {\n        String className = compareWith.contains(\".\") ? compareWith : \"org.apache.cassandra.db.marshal.\" + compareWith;\n        Class<? extends AbstractType<?>> typeClass = FBUtilities.<AbstractType<?>>classForName(className, \"abstract-type\");\n        try\n        {\n            Field field = typeClass.getDeclaredField(\"instance\");\n            return (AbstractType<?>) field.get(null);\n        }\n        catch (NoSuchFieldException e)\n        {\n            // Trying with empty parser\n            return getRawAbstractType(typeClass, EMPTY_PARSER);\n        }\n        catch (IllegalAccessException e)\n        {\n            // Trying with empty parser\n            return getRawAbstractType(typeClass, EMPTY_PARSER);\n        }\n    }",
            " 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 +\n 351  \n 352  \n 353  \n 354  \n 355  ",
            "    private static AbstractType<?> getAbstractType(String compareWith) throws ConfigurationException\n    {\n        String className = compareWith.contains(\".\") ? compareWith : \"org.apache.cassandra.db.marshal.\" + compareWith;\n        Class<? extends AbstractType<?>> typeClass = FBUtilities.<AbstractType<?>>classForName(className, \"abstract-type\");\n        try\n        {\n            Field field = typeClass.getDeclaredField(\"instance\");\n            return (AbstractType<?>) field.get(null);\n        }\n        catch (NoSuchFieldException | IllegalAccessException e)\n        {\n            // Trying with empty parser\n            return getRawAbstractType(typeClass, EMPTY_PARSER);\n        }\n    }"
        ],
        [
            "NodeProbe::truncateHints()",
            " 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 -\n 855 -\n 856 -\n 857 -\n 858 -\n 859  \n 860  \n 861  \n 862  ",
            "    public void truncateHints()\n    {\n        try\n        {\n            hhProxy.truncateAllHints();\n        }\n        catch (ExecutionException e)\n        {\n            throw new RuntimeException(\"Error while executing truncate hints\", e);\n        }\n        catch (InterruptedException e)\n        {\n            throw new RuntimeException(\"Error while executing truncate hints\", e);\n        }\n    }",
            " 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 +\n 855  \n 856  \n 857  \n 858  ",
            "    public void truncateHints()\n    {\n        try\n        {\n            hhProxy.truncateAllHints();\n        }\n        catch (ExecutionException | InterruptedException e)\n        {\n            throw new RuntimeException(\"Error while executing truncate hints\", e);\n        }\n    }"
        ],
        [
            "SnappyCompressor::isAvailable()",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67 -\n  68 -\n  69 -\n  70 -\n  71 -\n  72 -\n  73 -\n  74 -\n  75  \n  76  \n  77  \n  78  ",
            "    public static boolean isAvailable()\n    {\n        try\n        {\n            create(Collections.<String, String>emptyMap());\n            return true;\n        }\n        catch (Exception e)\n        {\n            JVMStabilityInspector.inspectThrowable(e);\n            return false;\n        }\n        catch (NoClassDefFoundError e)\n        {\n            return false;\n        }\n        catch (SnappyError e)\n        {\n            return false;\n        }\n        catch (UnsatisfiedLinkError e)\n        {\n            return false;\n        }\n    }",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67  \n  68  \n  69  \n  70  ",
            "    public static boolean isAvailable()\n    {\n        try\n        {\n            create(Collections.<String, String>emptyMap());\n            return true;\n        }\n        catch (Exception e)\n        {\n            JVMStabilityInspector.inspectThrowable(e);\n            return false;\n        }\n        catch (NoClassDefFoundError | SnappyError | UnsatisfiedLinkError e)\n        {\n            return false;\n        }\n    }"
        ],
        [
            "IntervalTree::Serializer::deserialize(DataInput,int,Comparator)",
            " 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 -\n 353 -\n 354 -\n 355 -\n 356 -\n 357 -\n 358 -\n 359 -\n 360 -\n 361  \n 362  \n 363  \n 364  ",
            "        public IntervalTree<C, D, I> deserialize(DataInput in, int version, Comparator<C> comparator) throws IOException\n        {\n            try\n            {\n                int count = in.readInt();\n                List<I> intervals = new ArrayList<I>(count);\n                for (int i = 0; i < count; i++)\n                {\n                    C min = pointSerializer.deserialize(in);\n                    C max = pointSerializer.deserialize(in);\n                    D data = dataSerializer.deserialize(in);\n                    intervals.add(constructor.newInstance(min, max, data));\n                }\n                return new IntervalTree<C, D, I>(intervals);\n            }\n            catch (InstantiationException e)\n            {\n                throw new RuntimeException(e);\n            }\n            catch (InvocationTargetException e)\n            {\n                throw new RuntimeException(e);\n            }\n            catch (IllegalAccessException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }",
            " 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 +\n 353  \n 354  \n 355  \n 356  ",
            "        public IntervalTree<C, D, I> deserialize(DataInput in, int version, Comparator<C> comparator) throws IOException\n        {\n            try\n            {\n                int count = in.readInt();\n                List<I> intervals = new ArrayList<I>(count);\n                for (int i = 0; i < count; i++)\n                {\n                    C min = pointSerializer.deserialize(in);\n                    C max = pointSerializer.deserialize(in);\n                    D data = dataSerializer.deserialize(in);\n                    intervals.add(constructor.newInstance(min, max, data));\n                }\n                return new IntervalTree<C, D, I>(intervals);\n            }\n            catch (InstantiationException | InvocationTargetException | IllegalAccessException e)\n            {\n                throw new RuntimeException(e);\n            }\n        }"
        ],
        [
            "TypeParser::getRawAbstractType(Class,TypeParser)",
            " 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 -\n 416 -\n 417 -\n 418 -\n 419 -\n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  ",
            "    private static AbstractType<?> getRawAbstractType(Class<? extends AbstractType<?>> typeClass, TypeParser parser) throws ConfigurationException\n    {\n        try\n        {\n            Method method = typeClass.getDeclaredMethod(\"getInstance\", TypeParser.class);\n            return (AbstractType<?>) method.invoke(null, parser);\n        }\n        catch (NoSuchMethodException e)\n        {\n            throw new ConfigurationException(\"Invalid comparator class \" + typeClass.getName() + \": must define a public static instance field or a public static method getInstance(TypeParser).\");\n        }\n        catch (IllegalAccessException e)\n        {\n            throw new ConfigurationException(\"Invalid comparator class \" + typeClass.getName() + \": must define a public static instance field or a public static method getInstance(TypeParser).\");\n        }\n        catch (InvocationTargetException e)\n        {\n            ConfigurationException ex = new ConfigurationException(\"Invalid definition for comparator \" + typeClass.getName() + \".\");\n            ex.initCause(e.getTargetException());\n            throw ex;\n        }\n    }",
            " 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 +\n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  ",
            "    private static AbstractType<?> getRawAbstractType(Class<? extends AbstractType<?>> typeClass, TypeParser parser) throws ConfigurationException\n    {\n        try\n        {\n            Method method = typeClass.getDeclaredMethod(\"getInstance\", TypeParser.class);\n            return (AbstractType<?>) method.invoke(null, parser);\n        }\n        catch (NoSuchMethodException | IllegalAccessException e)\n        {\n            throw new ConfigurationException(\"Invalid comparator class \" + typeClass.getName() + \": must define a public static instance field or a public static method getInstance(TypeParser).\");\n        }\n        catch (InvocationTargetException e)\n        {\n            ConfigurationException ex = new ConfigurationException(\"Invalid definition for comparator \" + typeClass.getName() + \".\");\n            ex.initCause(e.getTargetException());\n            throw ex;\n        }\n    }"
        ],
        [
            "FBUtilities::classForName(String,String)",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 -\n 450 -\n 451 -\n 452 -\n 453 -\n 454  \n 455  \n 456  \n 457  ",
            "    /**\n     * @return The Class for the given name.\n     * @param classname Fully qualified classname.\n     * @param readable Descriptive noun for the role the class plays.\n     * @throws ConfigurationException If the class cannot be found.\n     */\n    public static <T> Class<T> classForName(String classname, String readable) throws ConfigurationException\n    {\n        try\n        {\n            return (Class<T>)Class.forName(classname);\n        }\n        catch (ClassNotFoundException e)\n        {\n            throw new ConfigurationException(String.format(\"Unable to find %s class '%s'\", readable, classname), e);\n        }\n        catch (NoClassDefFoundError e)\n        {\n            throw new ConfigurationException(String.format(\"Unable to find %s class '%s'\", readable, classname), e);\n        }\n    }",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  ",
            "    /**\n     * @return The Class for the given name.\n     * @param classname Fully qualified classname.\n     * @param readable Descriptive noun for the role the class plays.\n     * @throws ConfigurationException If the class cannot be found.\n     */\n    public static <T> Class<T> classForName(String classname, String readable) throws ConfigurationException\n    {\n        try\n        {\n            return (Class<T>)Class.forName(classname);\n        }\n        catch (ClassNotFoundException | NoClassDefFoundError e)\n        {\n            throw new ConfigurationException(String.format(\"Unable to find %s class '%s'\", readable, classname), e);\n        }\n    }"
        ],
        [
            "HadoopCompat::newGenericCounter(String,String,long)",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242  \n 243  \n 244  ",
            "    /**\n     * @return with Hadoop 2 : <code>new GenericCounter(args)</code>,<br>\n     *         with Hadoop 1 : <code>new Counter(args)</code>\n     */\n    public static Counter newGenericCounter(String name, String displayName, long value) {\n        try {\n            return (Counter)\n                    GENERIC_COUNTER_CONSTRUCTOR.newInstance(name, displayName, value);\n        } catch (InstantiationException e) {\n            throw new IllegalArgumentException(\"Can't instantiate Counter\", e);\n        } catch (IllegalAccessException e) {\n            throw new IllegalArgumentException(\"Can't instantiate Counter\", e);\n        } catch (InvocationTargetException e) {\n            throw new IllegalArgumentException(\"Can't instantiate Counter\", e);\n        }\n    }",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 +\n 238  \n 239  \n 240  ",
            "    /**\n     * @return with Hadoop 2 : <code>new GenericCounter(args)</code>,<br>\n     *         with Hadoop 1 : <code>new Counter(args)</code>\n     */\n    public static Counter newGenericCounter(String name, String displayName, long value) {\n        try {\n            return (Counter)\n                    GENERIC_COUNTER_CONSTRUCTOR.newInstance(name, displayName, value);\n        } catch (InstantiationException | IllegalAccessException | InvocationTargetException e) {\n            throw new IllegalArgumentException(\"Can't instantiate Counter\", e);\n        }\n    }"
        ],
        [
            "CompositeTypeTest::testEmptyParametersNotallowed()",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 -\n 218 -\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "    @Test\n    public void testEmptyParametersNotallowed()\n    {\n        try\n        {\n            TypeParser.parse(\"CompositeType\");\n            fail(\"Shouldn't work\");\n        }\n        catch (ConfigurationException e) {}\n        catch (SyntaxException e) {}\n\n        try\n        {\n            TypeParser.parse(\"CompositeType()\");\n            fail(\"Shouldn't work\");\n        }",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "    @Test\n    public void testEmptyParametersNotallowed()\n    {\n        try\n        {\n            TypeParser.parse(\"CompositeType\");\n            fail(\"Shouldn't work\");\n        }\n        catch (ConfigurationException | SyntaxException e) {}\n\n        try\n        {\n            TypeParser.parse(\"CompositeType()\");\n            fail(\"Shouldn't work\");\n        }"
        ]
    ],
    "774bd0bceeee56abd97a122c563852210cf54bbc": [
        [
            "SchemaLoader::prepareServer()",
            "  71  \n  72  \n  73  \n  74 -\n  75 -\n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "    public static void prepareServer()\n    {\n        // Cleanup first\n        cleanupAndLeaveDirs();\n\n        CommitLog.instance.allocator.enableReserveSegmentCreation();\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                logger.error(\"Fatal exception in thread \" + t, e);\n            }\n        });\n\n        Keyspace.setInitialized();\n    }",
            "  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "    public static void prepareServer()\n    {\n        // Cleanup first\n        try\n        {\n            cleanupAndLeaveDirs();\n        }\n        catch (IOException e)\n        {\n            logger.error(\"Failed to cleanup and recreate directories and files.\");\n            throw new RuntimeException(e);\n        }\n\n        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler()\n        {\n            public void uncaughtException(Thread t, Throwable e)\n            {\n                logger.error(\"Fatal exception in thread \" + t, e);\n            }\n        });\n\n        Keyspace.setInitialized();\n    }"
        ],
        [
            "OrderPreservingPartitionerTest::cleanStatesFromPreviousTest()",
            "  28  \n  29 -\n  30  \n  31  \n  32  \n  33  \n  34  ",
            "    @BeforeClass\n    public static void cleanStatesFromPreviousTest()\n    {\n        // Since OrderPreservingPartitioner#describeOwnership tries to read SSTables,\n        // we need to clear data dir to clear garbage from previous test before running tests.\n        SchemaLoader.cleanupAndLeaveDirs();\n    }",
            "  30  \n  31 +\n  32  \n  33  \n  34  \n  35  \n  36  ",
            "    @BeforeClass\n    public static void cleanStatesFromPreviousTest() throws IOException\n    {\n        // Since OrderPreservingPartitioner#describeOwnership tries to read SSTables,\n        // we need to clear data dir to clear garbage from previous test before running tests.\n        SchemaLoader.cleanupAndLeaveDirs();\n    }"
        ],
        [
            "CommitLog::recover()",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    /**\n     * Perform recovery on commit logs located in the directory specified by the config file.\n     *\n     * @return the number of mutations replayed\n     */\n    public int recover() throws IOException\n    {\n        assert !allocator.createReserveSegments;\n\n        // Allocator could be in the process of initial startup with 0 active and available segments. We need to wait for\n        // the allocation manager to finish allocation and add it to available segments so we don't get an invalid response\n        // on allocator.manages(...) below by grabbing a file off the filesystem before it's added to the CLQ.\n        allocator.allocatingFrom();\n\n        FilenameFilter unmanagedFilesFilter = new FilenameFilter()\n        {\n            public boolean accept(File dir, String name)\n            {\n                // we used to try to avoid instantiating commitlog (thus creating an empty segment ready for writes)\n                // until after recover was finished.  this turns out to be fragile; it is less error-prone to go\n                // ahead and allow writes before recover(), and just skip active segments when we do.\n                return CommitLogDescriptor.isValid(name) && !allocator.manages(name);\n            }\n        };\n\n        // submit all existing files in the commit log dir for archiving prior to recovery - CASSANDRA-6904\n        for (File file : new File(DatabaseDescriptor.getCommitLogLocation()).listFiles(unmanagedFilesFilter))\n        {\n            archiver.maybeArchive(file.getPath(), file.getName());\n            archiver.maybeWaitForArchiving(file.getName());\n        }\n\n        assert archiver.archivePending.isEmpty() : \"Not all commit log archive tasks were completed before restore\";\n        archiver.maybeRestoreArchive();\n\n        File[] files = new File(DatabaseDescriptor.getCommitLogLocation()).listFiles(unmanagedFilesFilter);\n        int replayed = 0;\n        if (files.length == 0)\n        {\n            logger.info(\"No commitlog files found; skipping replay\");\n        }\n        else\n        {\n            Arrays.sort(files, new CommitLogSegmentFileComparator());\n            logger.info(\"Replaying {}\", StringUtils.join(files, \", \"));\n            replayed = recover(files);\n            logger.info(\"Log replay complete, {} replayed mutations\", replayed);\n\n            for (File f : files)\n                allocator.recycleSegment(f);\n        }\n\n        allocator.enableReserveSegmentCreation();\n        return replayed;\n    }",
            " 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "    /**\n     * Perform recovery on commit logs located in the directory specified by the config file.\n     *\n     * @return the number of mutations replayed\n     */\n    public int recover() throws IOException\n    {\n        // If createReserveSegments is already flipped, the CLSM is running and recovery has already taken place.\n        if (allocator.createReserveSegments)\n            return 0;\n\n        // Allocator could be in the process of initial startup with 0 active and available segments. We need to wait for\n        // the allocation manager to finish allocation and add it to available segments so we don't get an invalid response\n        // on allocator.manages(...) below by grabbing a file off the filesystem before it's added to the CLQ.\n        allocator.allocatingFrom();\n\n        FilenameFilter unmanagedFilesFilter = new FilenameFilter()\n        {\n            public boolean accept(File dir, String name)\n            {\n                // we used to try to avoid instantiating commitlog (thus creating an empty segment ready for writes)\n                // until after recover was finished.  this turns out to be fragile; it is less error-prone to go\n                // ahead and allow writes before recover(), and just skip active segments when we do.\n                return CommitLogDescriptor.isValid(name) && !allocator.manages(name);\n            }\n        };\n\n        // submit all existing files in the commit log dir for archiving prior to recovery - CASSANDRA-6904\n        for (File file : new File(DatabaseDescriptor.getCommitLogLocation()).listFiles(unmanagedFilesFilter))\n        {\n            archiver.maybeArchive(file.getPath(), file.getName());\n            archiver.maybeWaitForArchiving(file.getName());\n        }\n\n        assert archiver.archivePending.isEmpty() : \"Not all commit log archive tasks were completed before restore\";\n        archiver.maybeRestoreArchive();\n\n        File[] files = new File(DatabaseDescriptor.getCommitLogLocation()).listFiles(unmanagedFilesFilter);\n        int replayed = 0;\n        if (files.length == 0)\n        {\n            logger.info(\"No commitlog files found; skipping replay\");\n        }\n        else\n        {\n            Arrays.sort(files, new CommitLogSegmentFileComparator());\n            logger.info(\"Replaying {}\", StringUtils.join(files, \", \"));\n            replayed = recover(files);\n            logger.info(\"Log replay complete, {} replayed mutations\", replayed);\n\n            for (File f : files)\n                allocator.recycleSegment(f);\n        }\n\n        allocator.enableReserveSegmentCreation();\n        return replayed;\n    }"
        ],
        [
            "SchemaLoader::cleanupAndLeaveDirs()",
            " 447 -\n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  ",
            "    public static void cleanupAndLeaveDirs()\n    {\n        // We need to stop and unmap all CLS instances prior to cleanup() or we'll get failures on Windows.\n        CommitLog.instance.stopUnsafe(true);\n        mkdirs();\n        cleanup();\n        mkdirs();\n        CommitLog.instance.startUnsafe();\n    }",
            " 454 +\n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  ",
            "    public static void cleanupAndLeaveDirs() throws IOException\n    {\n        // We need to stop and unmap all CLS instances prior to cleanup() or we'll get failures on Windows.\n        CommitLog.instance.stopUnsafe(true);\n        mkdirs();\n        cleanup();\n        mkdirs();\n        CommitLog.instance.startUnsafe();\n    }"
        ],
        [
            "CommitLogSegmentManager::enableReserveSegmentCreation()",
            " 412  \n 413  \n 414  \n 415  \n 416 -\n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * Throws a flag that enables the behavior of keeping at least one spare segment\n     * available at all times.\n     */\n    public void enableReserveSegmentCreation()\n    {\n        createReserveSegments = true;\n        wakeManager();\n    }",
            " 412  \n 413  \n 414  \n 415  \n 416 +\n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * Throws a flag that enables the behavior of keeping at least one spare segment\n     * available at all times.\n     */\n    void enableReserveSegmentCreation()\n    {\n        createReserveSegments = true;\n        wakeManager();\n    }"
        ],
        [
            "CommitLog::resetUnsafe(boolean)",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387 -\n 388 -\n 389  ",
            "    /**\n     * FOR TESTING PURPOSES. See CommitLogAllocator.\n     * @return the number of files recovered\n     */\n    public int resetUnsafe(boolean deleteSegments) throws IOException\n    {\n        stopUnsafe(deleteSegments);\n        startUnsafe();\n        return recover();\n    }",
            " 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 +\n 390  ",
            "    /**\n     * FOR TESTING PURPOSES. See CommitLogAllocator.\n     * @return the number of files recovered\n     */\n    public int resetUnsafe(boolean deleteSegments) throws IOException\n    {\n        stopUnsafe(deleteSegments);\n        return startUnsafe();\n    }"
        ],
        [
            "CommitLog::startUnsafe()",
            " 408  \n 409  \n 410  \n 411 -\n 412  \n 413  \n 414  \n 415  ",
            "    /**\n     * FOR TESTING PURPOSES.  See CommitLogAllocator\n     */\n    public void startUnsafe()\n    {\n        allocator.startUnsafe();\n        executor.startUnsafe();\n    }",
            " 409  \n 410  \n 411  \n 412 +\n 413  \n 414  \n 415  \n 416 +\n 417  ",
            "    /**\n     * FOR TESTING PURPOSES.  See CommitLogAllocator\n     */\n    public int startUnsafe() throws IOException\n    {\n        allocator.startUnsafe();\n        executor.startUnsafe();\n        return recover();\n    }"
        ],
        [
            "DatabaseDescriptorTest::testTransKsMigration()",
            "  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "    @Test\n    public void testTransKsMigration() throws ConfigurationException\n    {\n        SchemaLoader.cleanupAndLeaveDirs();\n        Schema.instance.loadFromDisk();\n        assertEquals(0, Schema.instance.getNonSystemKeyspaces().size());\n\n        Gossiper.instance.start((int)(System.currentTimeMillis() / 1000));\n        Keyspace.setInitialized();\n\n        try\n        {\n            // add a few.\n            MigrationManager.announceNewKeyspace(KSMetaData.testMetadata(\"ks0\", SimpleStrategy.class, KSMetaData.optsWithRF(3)));\n            MigrationManager.announceNewKeyspace(KSMetaData.testMetadata(\"ks1\", SimpleStrategy.class, KSMetaData.optsWithRF(3)));\n\n            assertNotNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNotNull(Schema.instance.getKSMetaData(\"ks1\"));\n\n            Schema.instance.clearKeyspaceDefinition(Schema.instance.getKSMetaData(\"ks0\"));\n            Schema.instance.clearKeyspaceDefinition(Schema.instance.getKSMetaData(\"ks1\"));\n\n            assertNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNull(Schema.instance.getKSMetaData(\"ks1\"));\n\n            Schema.instance.loadFromDisk();\n\n            assertNotNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNotNull(Schema.instance.getKSMetaData(\"ks1\"));\n        }\n        finally\n        {\n            Gossiper.instance.stop();\n        }\n    }",
            "  79  \n  80 +\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "    @Test\n    public void testTransKsMigration() throws ConfigurationException, IOException\n    {\n        SchemaLoader.cleanupAndLeaveDirs();\n        Schema.instance.loadFromDisk();\n        assertEquals(0, Schema.instance.getNonSystemKeyspaces().size());\n\n        Gossiper.instance.start((int)(System.currentTimeMillis() / 1000));\n        Keyspace.setInitialized();\n\n        try\n        {\n            // add a few.\n            MigrationManager.announceNewKeyspace(KSMetaData.testMetadata(\"ks0\", SimpleStrategy.class, KSMetaData.optsWithRF(3)));\n            MigrationManager.announceNewKeyspace(KSMetaData.testMetadata(\"ks1\", SimpleStrategy.class, KSMetaData.optsWithRF(3)));\n\n            assertNotNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNotNull(Schema.instance.getKSMetaData(\"ks1\"));\n\n            Schema.instance.clearKeyspaceDefinition(Schema.instance.getKSMetaData(\"ks0\"));\n            Schema.instance.clearKeyspaceDefinition(Schema.instance.getKSMetaData(\"ks1\"));\n\n            assertNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNull(Schema.instance.getKSMetaData(\"ks1\"));\n\n            Schema.instance.loadFromDisk();\n\n            assertNotNull(Schema.instance.getKSMetaData(\"ks0\"));\n            assertNotNull(Schema.instance.getKSMetaData(\"ks1\"));\n        }\n        finally\n        {\n            Gossiper.instance.stop();\n        }\n    }"
        ]
    ],
    "9b10928c159317160fb3049727679a48232b6041": [
        [
            "BlacklistingCompactionsTest::testBlacklisting(String)",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  ",
            "    public void testBlacklisting(String compactionStrategy) throws Exception\n    {\n        // this test does enough rows to force multiple block indexes to be used\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        final ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(\"Standard1\");\n\n        final int ROWS_PER_SSTABLE = 10;\n        final int SSTABLES = cfs.metadata.getIndexInterval() * 2 / ROWS_PER_SSTABLE;\n\n        cfs.setCompactionStrategyClass(compactionStrategy);\n\n        // disable compaction while flushing\n        cfs.disableAutoCompaction();\n        //test index corruption\n        //now create a few new SSTables\n        long maxTimestampExpected = Long.MIN_VALUE;\n        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();\n        for (int j = 0; j < SSTABLES; j++)\n        {\n            for (int i = 0; i < ROWS_PER_SSTABLE; i++)\n            {\n                DecoratedKey key = Util.dk(String.valueOf(i % 2));\n                RowMutation rm = new RowMutation(KEYSPACE, key.key);\n                long timestamp = j * ROWS_PER_SSTABLE + i;\n                rm.add(\"Standard1\", ByteBufferUtil.bytes(String.valueOf(i / 2)),\n                       ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                       timestamp);\n                maxTimestampExpected = Math.max(timestamp, maxTimestampExpected);\n                rm.apply();\n                inserted.add(key);\n            }\n            cfs.forceBlockingFlush();\n            CompactionsTest.assertMaxTimestamp(cfs, maxTimestampExpected);\n            assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(cfs).size());\n        }\n\n        Collection<SSTableReader> sstables = cfs.getSSTables();\n        int currentSSTable = 0;\n        int sstablesToCorrupt = 8;\n\n        // corrupt first 'sstablesToCorrupt' SSTables\n        for (SSTableReader sstable : sstables)\n        {\n            if(currentSSTable + 1 > sstablesToCorrupt)\n                break;\n\n            RandomAccessFile raf = null;\n\n            try\n            {\n                raf = new RandomAccessFile(sstable.getFilename(), \"rw\");\n                assertNotNull(raf);\n                raf.write(0xFFFFFF);\n            }\n            finally\n            {\n                FileUtils.closeQuietly(raf);\n            }\n\n            currentSSTable++;\n        }\n\n        int failures = 0;\n\n        // in case something will go wrong we don't want to loop forever using for (;;)\n        for (int i = 0; i < sstables.size(); i++)\n        {\n            try\n            {\n                cfs.forceMajorCompaction();\n            }\n            catch (Exception e)\n            {\n                // kind of a hack since we're not specifying just CorruptSSTableExceptions, or (what we actually expect)\n                // an ExecutionException wrapping a CSSTE.  This is probably Good Enough though, since if there are\n                // other errors in compaction presumably the other tests would bring that to light.\n                failures++;\n                continue;\n            }\n\n            assertEquals(sstablesToCorrupt + 1, cfs.getSSTables().size());\n            break;\n        }\n\n\n        cfs.truncateBlocking();\n        assertEquals(failures, sstablesToCorrupt);\n    }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  ",
            "    public void testBlacklisting(String compactionStrategy) throws Exception\n    {\n        // this test does enough rows to force multiple block indexes to be used\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        final ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(\"Standard1\");\n\n        final int ROWS_PER_SSTABLE = 10;\n        final int SSTABLES = cfs.metadata.getIndexInterval() * 2 / ROWS_PER_SSTABLE;\n\n        cfs.setCompactionStrategyClass(compactionStrategy);\n\n        // disable compaction while flushing\n        cfs.disableAutoCompaction();\n        //test index corruption\n        //now create a few new SSTables\n        long maxTimestampExpected = Long.MIN_VALUE;\n        Set<DecoratedKey> inserted = new HashSet<DecoratedKey>();\n        for (int j = 0; j < SSTABLES; j++)\n        {\n            for (int i = 0; i < ROWS_PER_SSTABLE; i++)\n            {\n                DecoratedKey key = Util.dk(String.valueOf(i % 2));\n                RowMutation rm = new RowMutation(KEYSPACE, key.key);\n                long timestamp = j * ROWS_PER_SSTABLE + i;\n                rm.add(\"Standard1\", ByteBufferUtil.bytes(String.valueOf(i / 2)),\n                       ByteBufferUtil.EMPTY_BYTE_BUFFER,\n                       timestamp);\n                maxTimestampExpected = Math.max(timestamp, maxTimestampExpected);\n                rm.apply();\n                inserted.add(key);\n            }\n            cfs.forceBlockingFlush();\n            CompactionsTest.assertMaxTimestamp(cfs, maxTimestampExpected);\n            assertEquals(inserted.toString(), inserted.size(), Util.getRangeSlice(cfs).size());\n        }\n\n        Collection<SSTableReader> sstables = cfs.getSSTables();\n        int currentSSTable = 0;\n        int sstablesToCorrupt = 8;\n\n        // corrupt first 'sstablesToCorrupt' SSTables\n        for (SSTableReader sstable : sstables)\n        {\n            if(currentSSTable + 1 > sstablesToCorrupt)\n                break;\n\n            RandomAccessFile raf = null;\n\n            try\n            {\n                raf = new RandomAccessFile(sstable.getFilename(), \"rw\");\n                assertNotNull(raf);\n                assertTrue(raf.length() > 20);\n                raf.seek(new Random().nextInt((int)(raf.length() - 20)));\n                // We want to write something large enough that the corruption cannot get undetected\n                // (even without compression)\n                byte[] corruption = new byte[20];\n                Arrays.fill(corruption, (byte)0xFF);\n                raf.write(corruption);\n\n            }\n            finally\n            {\n                FileUtils.closeQuietly(raf);\n            }\n\n            currentSSTable++;\n        }\n\n        int failures = 0;\n\n        // in case something will go wrong we don't want to loop forever using for (;;)\n        for (int i = 0; i < sstables.size(); i++)\n        {\n            try\n            {\n                cfs.forceMajorCompaction();\n            }\n            catch (Exception e)\n            {\n                // kind of a hack since we're not specifying just CorruptSSTableExceptions, or (what we actually expect)\n                // an ExecutionException wrapping a CSSTE.  This is probably Good Enough though, since if there are\n                // other errors in compaction presumably the other tests would bring that to light.\n                failures++;\n                continue;\n            }\n\n            assertEquals(sstablesToCorrupt + 1, cfs.getSSTables().size());\n            break;\n        }\n\n\n        cfs.truncateBlocking();\n        assertEquals(sstablesToCorrupt, failures);\n    }"
        ],
        [
            "SSTableIdentityIterator::SSTableIdentityIterator(CFMetaData,DataInput,String,DecoratedKey,long,boolean,SSTableReader,ColumnSerializer)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "    private SSTableIdentityIterator(CFMetaData metadata,\n                                    DataInput in,\n                                    String filename,\n                                    DecoratedKey key,\n                                    long dataSize,\n                                    boolean checkData,\n                                    SSTableReader sstable,\n                                    ColumnSerializer.Flag flag)\n    {\n        assert !checkData || (sstable != null);\n        this.in = in;\n        this.filename = filename;\n        this.key = key;\n        this.dataSize = dataSize;\n        this.expireBefore = (int)(System.currentTimeMillis() / 1000);\n        this.flag = flag;\n        this.validateColumns = checkData;\n        this.dataVersion = sstable == null ? Descriptor.Version.CURRENT : sstable.descriptor.version;\n\n        try\n        {\n            columnFamily = EmptyColumns.factory.create(metadata);\n            columnFamily.delete(DeletionTime.serializer.deserialize(in));\n            columnCount = dataVersion.hasRowSizeAndColumnCount ? in.readInt() : Integer.MAX_VALUE;\n            atomIterator = columnFamily.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        }\n        catch (IOException e)\n        {\n            if (sstable != null)\n                sstable.markSuspect();\n            throw new CorruptSSTableException(e, filename);\n        }\n    }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "    private SSTableIdentityIterator(CFMetaData metadata,\n                                    DataInput in,\n                                    String filename,\n                                    DecoratedKey key,\n                                    long dataSize,\n                                    boolean checkData,\n                                    SSTableReader sstable,\n                                    ColumnSerializer.Flag flag)\n    {\n        assert !checkData || (sstable != null);\n        this.in = in;\n        this.filename = filename;\n        this.key = key;\n        this.dataSize = dataSize;\n        this.expireBefore = (int)(System.currentTimeMillis() / 1000);\n        this.flag = flag;\n        this.validateColumns = checkData;\n        this.dataVersion = sstable == null ? Descriptor.Version.CURRENT : sstable.descriptor.version;\n        this.sstable = sstable;\n\n        try\n        {\n            columnFamily = EmptyColumns.factory.create(metadata);\n            columnFamily.delete(DeletionTime.serializer.deserialize(in));\n            columnCount = dataVersion.hasRowSizeAndColumnCount ? in.readInt() : Integer.MAX_VALUE;\n            atomIterator = columnFamily.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        }\n        catch (IOException e)\n        {\n            if (sstable != null)\n                sstable.markSuspect();\n            throw new CorruptSSTableException(e, filename);\n        }\n    }"
        ],
        [
            "SSTableIdentityIterator::getColumnFamilyWithColumns(ColumnFamily)",
            " 180  \n 181  \n 182  \n 183  \n 184 -\n 185 -\n 186 -\n 187  \n 188 -\n 189  \n 190 -\n 191  \n 192 -\n 193  \n 194 -\n 195  \n 196 -\n 197  \n 198  \n 199 -\n 200  ",
            "    public ColumnFamily getColumnFamilyWithColumns(ColumnFamily.Factory containerFactory)\n    {\n        ColumnFamily cf = columnFamily.cloneMeShallow(containerFactory, false);\n        // since we already read column count, just pass that value and continue deserialization\n        Iterator<OnDiskAtom> iter = cf.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n        while (iter.hasNext())\n            cf.addAtom(iter.next());\n\n        if (validateColumns)\n        {\n            try\n            {\n                cf.metadata().validateColumns(cf);\n            }\n            catch (MarshalException e)\n            {\n                throw new RuntimeException(\"Error validating row \" + key, e);\n            }\n        }\n        return cf;\n    }",
            " 190  \n 191  \n 192  \n 193  \n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214  \n 215 +\n 216 +\n 217  \n 218 +\n 219 +\n 220 +\n 221  \n 222 +\n 223  \n 224 +\n 225  \n 226  \n 227  ",
            "    public ColumnFamily getColumnFamilyWithColumns(ColumnFamily.Factory containerFactory)\n    {\n        ColumnFamily cf = columnFamily.cloneMeShallow(containerFactory, false);\n        // since we already read column count, just pass that value and continue deserialization\n        try\n        {\n            Iterator<OnDiskAtom> iter = cf.metadata().getOnDiskIterator(in, columnCount, flag, expireBefore, dataVersion);\n            while (iter.hasNext())\n                cf.addAtom(iter.next());\n\n            if (validateColumns)\n            {\n                try\n                {\n                    cf.metadata().validateColumns(cf);\n                }\n                catch (MarshalException e)\n                {\n                    throw new RuntimeException(\"Error validating row \" + key, e);\n                }\n            }\n            return cf;\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n            {\n                if (sstable != null)\n                    sstable.markSuspect();\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }"
        ],
        [
            "SSTableIdentityIterator::hasNext()",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "    public boolean hasNext()\n    {\n        try\n        {\n            return atomIterator.hasNext();\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            else\n                throw e;\n        }\n    }",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140 +\n 141 +\n 142  \n 143 +\n 144  \n 145 +\n 146  \n 147 +\n 148  \n 149  ",
            "    public boolean hasNext()\n    {\n        try\n        {\n            return atomIterator.hasNext();\n        }\n        catch (IOError e)\n        {\n            // catch here b/c atomIterator is an AbstractIterator; hasNext reads the value\n            if (e.getCause() instanceof IOException)\n            {\n                if (sstable != null)\n                    sstable.markSuspect();\n                throw new CorruptSSTableException((IOException)e.getCause(), filename);\n            }\n            else\n            {\n                throw e;\n            }\n        }\n    }"
        ]
    ],
    "3caf0e02970b8da90d154d81d769662057d7c772": [
        [
            "ColumnFamilyStore::selectAndReference(Function)",
            "1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  ",
            "    public RefViewFragment selectAndReference(Function<DataTracker.View, List<SSTableReader>> filter)\n    {\n        while (true)\n        {\n            ViewFragment view = select(filter);\n            Refs<SSTableReader> refs = Refs.tryRef(view.sstables);\n            if (refs != null)\n                return new RefViewFragment(view.sstables, view.memtables, refs);\n        }\n    }",
            "1852  \n1853  \n1854 +\n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861 +\n1862 +\n1863 +\n1864 +\n1865 +\n1866 +\n1867 +\n1868 +\n1869 +\n1870 +\n1871 +\n1872 +\n1873 +\n1874 +\n1875  \n1876  ",
            "    public RefViewFragment selectAndReference(Function<DataTracker.View, List<SSTableReader>> filter)\n    {\n        long failingSince = -1L;\n        while (true)\n        {\n            ViewFragment view = select(filter);\n            Refs<SSTableReader> refs = Refs.tryRef(view.sstables);\n            if (refs != null)\n                return new RefViewFragment(view.sstables, view.memtables, refs);\n            if (failingSince <= 0)\n            {\n                failingSince = System.nanoTime();\n            }\n            else if (TimeUnit.MILLISECONDS.toNanos(100) > System.nanoTime() - failingSince)\n            {\n                List<SSTableReader> released = new ArrayList<>();\n                for (SSTableReader reader : view.sstables)\n                    if (reader.selfRef().globalCount() == 0)\n                        released.add(reader);\n                logger.info(\"Spinning trying to capture released readers {}\", released);\n                logger.info(\"Spinning trying to capture all readers {}\", view.sstables);\n                failingSince = System.nanoTime();\n            }\n        }\n    }"
        ],
        [
            "CompactionTask::runMayThrow()",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert sstables != null;\n\n        if (sstables.size() == 0)\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        // note that we need to do a rough estimate early if we can fit the compaction on disk - this is pessimistic, but\n        // since we might remove sstables from the compaction in checkAvailableDiskSpace it needs to be done here\n        long earlySSTableEstimate = Math.max(1, cfs.getExpectedCompactedFileSize(sstables, compactionType) / strategy.getMaxSSTableBytes());\n        checkAvailableDiskSpace(earlySSTableEstimate);\n\n        // sanity check: all sstables must belong to the same cfs\n        assert !Iterables.any(sstables, new Predicate<SSTableReader>()\n        {\n            @Override\n            public boolean apply(SSTableReader sstable)\n            {\n                return !sstable.descriptor.cfname.equals(cfs.name);\n            }\n        });\n\n        UUID taskId = SystemKeyspace.startCompaction(cfs, sstables);\n\n        // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n        // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n        // all the sstables (that existed when we started)\n        logger.info(\"Compacting {}\", sstables);\n\n        long start = System.nanoTime();\n\n        long totalKeysWritten = 0;\n\n        try (CompactionController controller = getCompactionController(sstables);)\n        {\n            Set<SSTableReader> actuallyCompact = Sets.difference(sstables, controller.getFullyExpiredSSTables());\n\n            long estimatedTotalKeys = Math.max(cfs.metadata.getMinIndexInterval(), SSTableReader.getApproximateKeyCount(actuallyCompact));\n            long estimatedSSTables = Math.max(1, cfs.getExpectedCompactedFileSize(actuallyCompact, compactionType) / strategy.getMaxSSTableBytes());\n            long keysPerSSTable = (long) Math.ceil((double) estimatedTotalKeys / estimatedSSTables);\n            long expectedSSTableSize = Math.min(getExpectedWriteSize(), strategy.getMaxSSTableBytes());\n            logger.debug(\"Expected bloom filter size : {}\", keysPerSSTable);\n\n            List<SSTableReader> newSStables;\n            AbstractCompactionIterable ci;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            try (AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact))\n            {\n                ci = new CompactionIterable(compactionType, scanners.scanners, controller);\n                Iterator<AbstractCompactedRow> iter = ci.iterator();\n                // we can't preheat until the tracker has been set. This doesn't happen until we tell the cfs to\n                // replace the old entries.  Track entries to preheat here until then.\n                long minRepairedAt = getMinRepairedAt(actuallyCompact);\n                // we only need the age of the data that we're actually retaining\n                long maxAge = getMaxDataAge(actuallyCompact);\n                if (collector != null)\n                    collector.beginCompaction(ci);\n                long lastCheckObsoletion = start;\n                SSTableRewriter writer = new SSTableRewriter(cfs, sstables, maxAge, offline);\n                try\n                {\n                    if (!controller.cfs.getCompactionStrategy().isActive)\n                       throw new CompactionInterruptedException(ci.getCompactionInfo());\n                    if (!iter.hasNext())\n                    {\n                        // don't mark compacted in the finally block, since if there _is_ nondeleted data,\n                        // we need to sync it (via closeAndOpen) first, so there is no period during which\n                        // a crash could cause data loss.\n                        cfs.markObsolete(sstables, compactionType);\n                        return;\n                    }\n\n                    writer.switchWriter(createCompactionWriter(cfs.directories.getLocationForDisk(getWriteDirectory(expectedSSTableSize)), keysPerSSTable, minRepairedAt));\n                    while (iter.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                        AbstractCompactedRow row = iter.next();\n                        if (writer.append(row) != null)\n                        {\n                            totalKeysWritten++;\n                            if (newSSTableSegmentThresholdReached(writer.currentWriter()))\n                            {\n                                writer.switchWriter(createCompactionWriter(cfs.directories.getLocationForDisk(getWriteDirectory(expectedSSTableSize)), keysPerSSTable, minRepairedAt));\n                            }\n                        }\n\n                        if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                        {\n                            controller.maybeRefreshOverlaps();\n                            lastCheckObsoletion = System.nanoTime();\n                        }\n                    }\n\n                    // don't replace old sstables yet, as we need to mark the compaction finished in the system table\n                    newSStables = writer.finish();\n                }\n                catch (Throwable t)\n                {\n                    try\n                    {\n                        writer.abort();\n                    }\n                    catch (Throwable t2)\n                    {\n                        t.addSuppressed(t2);\n                    }\n                    throw t;\n                }\n                finally\n                {\n                    // point of no return -- the new sstables are live on disk; next we'll start deleting the old ones\n                    // (in replaceCompactedSSTables)\n                    if (taskId != null)\n                        SystemKeyspace.finishCompaction(taskId);\n\n                    if (collector != null)\n                        collector.finishCompaction(ci);\n                }\n            }\n\n            Collection<SSTableReader> oldSStables = this.sstables;\n            if (!offline)\n                cfs.getDataTracker().markCompactedSSTablesReplaced(oldSStables, newSStables, compactionType);\n\n            // log a bunch of statistics about the result and save to system table compaction_history\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = SSTableReader.getTotalBytes(oldSStables);\n            long endsize = SSTableReader.getTotalBytes(newSStables);\n            double ratio = (double) endsize / (double) startsize;\n\n            StringBuilder newSSTableNames = new StringBuilder();\n            for (SSTableReader reader : newSStables)\n                newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n\n            double mbps = dTime > 0 ? (double) endsize / (1024 * 1024) / ((double) dTime / 1000) : 0;\n            long totalSourceRows = 0;\n            long[] counts = ci.getMergedRowCounts();\n            StringBuilder mergeSummary = new StringBuilder(counts.length * 10);\n            Map<Integer, Long> mergedRows = new HashMap<>();\n            for (int i = 0; i < counts.length; i++)\n            {\n                long count = counts[i];\n                if (count == 0)\n                    continue;\n\n                int rows = i + 1;\n                totalSourceRows += rows * count;\n                mergeSummary.append(String.format(\"%d:%d, \", rows, count));\n                mergedRows.put(rows, count);\n            }\n\n            SystemKeyspace.updateCompactionHistory(cfs.keyspace.getName(), cfs.name, System.currentTimeMillis(), startsize, endsize, mergedRows);\n            logger.info(String.format(\"Compacted %d sstables to [%s].  %,d bytes to %,d (~%d%% of original) in %,dms = %fMB/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                      oldSStables.size(), newSSTableNames.toString(), startsize, endsize, (int) (ratio * 100), dTime, mbps, totalSourceRows, totalKeysWritten, mergeSummary.toString()));\n            logger.debug(String.format(\"CF Total Bytes Compacted: %,d\", CompactionTask.addToTotalBytesCompacted(endsize)));\n            logger.debug(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedTotalKeys, ((double)(totalKeysWritten - estimatedTotalKeys)/totalKeysWritten));\n        }\n    }",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 +\n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert sstables != null;\n\n        if (sstables.size() == 0)\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        // note that we need to do a rough estimate early if we can fit the compaction on disk - this is pessimistic, but\n        // since we might remove sstables from the compaction in checkAvailableDiskSpace it needs to be done here\n        long earlySSTableEstimate = Math.max(1, cfs.getExpectedCompactedFileSize(sstables, compactionType) / strategy.getMaxSSTableBytes());\n        checkAvailableDiskSpace(earlySSTableEstimate);\n\n        // sanity check: all sstables must belong to the same cfs\n        assert !Iterables.any(sstables, new Predicate<SSTableReader>()\n        {\n            @Override\n            public boolean apply(SSTableReader sstable)\n            {\n                return !sstable.descriptor.cfname.equals(cfs.name);\n            }\n        });\n\n        UUID taskId = SystemKeyspace.startCompaction(cfs, sstables);\n\n        // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n        // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n        // all the sstables (that existed when we started)\n        logger.info(\"Compacting {}\", sstables);\n\n        long start = System.nanoTime();\n\n        long totalKeysWritten = 0;\n\n        try (CompactionController controller = getCompactionController(sstables);)\n        {\n            Set<SSTableReader> actuallyCompact = Sets.difference(sstables, controller.getFullyExpiredSSTables());\n\n            long estimatedTotalKeys = Math.max(cfs.metadata.getMinIndexInterval(), SSTableReader.getApproximateKeyCount(actuallyCompact));\n            long estimatedSSTables = Math.max(1, cfs.getExpectedCompactedFileSize(actuallyCompact, compactionType) / strategy.getMaxSSTableBytes());\n            long keysPerSSTable = (long) Math.ceil((double) estimatedTotalKeys / estimatedSSTables);\n            long expectedSSTableSize = Math.min(getExpectedWriteSize(), strategy.getMaxSSTableBytes());\n            logger.debug(\"Expected bloom filter size : {}\", keysPerSSTable);\n\n            List<SSTableReader> newSStables;\n            AbstractCompactionIterable ci;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact))\n            {\n                ci = new CompactionIterable(compactionType, scanners.scanners, controller);\n                Iterator<AbstractCompactedRow> iter = ci.iterator();\n                // we can't preheat until the tracker has been set. This doesn't happen until we tell the cfs to\n                // replace the old entries.  Track entries to preheat here until then.\n                long minRepairedAt = getMinRepairedAt(actuallyCompact);\n                // we only need the age of the data that we're actually retaining\n                long maxAge = getMaxDataAge(actuallyCompact);\n                if (collector != null)\n                    collector.beginCompaction(ci);\n                long lastCheckObsoletion = start;\n                SSTableRewriter writer = new SSTableRewriter(cfs, sstables, maxAge, offline);\n                try\n                {\n                    if (!controller.cfs.getCompactionStrategy().isActive)\n                       throw new CompactionInterruptedException(ci.getCompactionInfo());\n                    if (!iter.hasNext())\n                    {\n                        // don't mark compacted in the finally block, since if there _is_ nondeleted data,\n                        // we need to sync it (via closeAndOpen) first, so there is no period during which\n                        // a crash could cause data loss.\n                        cfs.markObsolete(sstables, compactionType);\n                        return;\n                    }\n\n                    writer.switchWriter(createCompactionWriter(cfs.directories.getLocationForDisk(getWriteDirectory(expectedSSTableSize)), keysPerSSTable, minRepairedAt));\n                    while (iter.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                        AbstractCompactedRow row = iter.next();\n                        if (writer.append(row) != null)\n                        {\n                            totalKeysWritten++;\n                            if (newSSTableSegmentThresholdReached(writer.currentWriter()))\n                            {\n                                writer.switchWriter(createCompactionWriter(cfs.directories.getLocationForDisk(getWriteDirectory(expectedSSTableSize)), keysPerSSTable, minRepairedAt));\n                            }\n                        }\n\n                        if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                        {\n                            controller.maybeRefreshOverlaps();\n                            lastCheckObsoletion = System.nanoTime();\n                        }\n                    }\n\n                    // don't replace old sstables yet, as we need to mark the compaction finished in the system table\n                    newSStables = writer.finish();\n                }\n                catch (Throwable t)\n                {\n                    try\n                    {\n                        writer.abort();\n                    }\n                    catch (Throwable t2)\n                    {\n                        t.addSuppressed(t2);\n                    }\n                    throw t;\n                }\n                finally\n                {\n                    // point of no return -- the new sstables are live on disk; next we'll start deleting the old ones\n                    // (in replaceCompactedSSTables)\n                    if (taskId != null)\n                        SystemKeyspace.finishCompaction(taskId);\n\n                    if (collector != null)\n                        collector.finishCompaction(ci);\n                }\n            }\n\n            Collection<SSTableReader> oldSStables = this.sstables;\n            if (!offline)\n                cfs.getDataTracker().markCompactedSSTablesReplaced(oldSStables, newSStables, compactionType);\n\n            // log a bunch of statistics about the result and save to system table compaction_history\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = SSTableReader.getTotalBytes(oldSStables);\n            long endsize = SSTableReader.getTotalBytes(newSStables);\n            double ratio = (double) endsize / (double) startsize;\n\n            StringBuilder newSSTableNames = new StringBuilder();\n            for (SSTableReader reader : newSStables)\n                newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n\n            double mbps = dTime > 0 ? (double) endsize / (1024 * 1024) / ((double) dTime / 1000) : 0;\n            long totalSourceRows = 0;\n            long[] counts = ci.getMergedRowCounts();\n            StringBuilder mergeSummary = new StringBuilder(counts.length * 10);\n            Map<Integer, Long> mergedRows = new HashMap<>();\n            for (int i = 0; i < counts.length; i++)\n            {\n                long count = counts[i];\n                if (count == 0)\n                    continue;\n\n                int rows = i + 1;\n                totalSourceRows += rows * count;\n                mergeSummary.append(String.format(\"%d:%d, \", rows, count));\n                mergedRows.put(rows, count);\n            }\n\n            SystemKeyspace.updateCompactionHistory(cfs.keyspace.getName(), cfs.name, System.currentTimeMillis(), startsize, endsize, mergedRows);\n            logger.info(String.format(\"Compacted %d sstables to [%s].  %,d bytes to %,d (~%d%% of original) in %,dms = %fMB/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                      oldSStables.size(), newSSTableNames.toString(), startsize, endsize, (int) (ratio * 100), dTime, mbps, totalSourceRows, totalKeysWritten, mergeSummary.toString()));\n            logger.debug(String.format(\"CF Total Bytes Compacted: %,d\", CompactionTask.addToTotalBytesCompacted(endsize)));\n            logger.debug(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedTotalKeys, ((double)(totalKeysWritten - estimatedTotalKeys)/totalKeysWritten));\n        }\n    }"
        ]
    ],
    "8d7c608bac9440911f4803db04f306f0616f2a22": [
        [
            "LegacySchemaMigrator::deserializeKind(String)",
            " 468  \n 469  \n 470  \n 471 -\n 472  \n 473  \n 474  \n 475  ",
            "    private static ColumnDefinition.Kind deserializeKind(String kind)\n    {\n        if (\"clustering_key\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.CLUSTERING_COLUMN;\n        if (\"compact_value\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.REGULAR;\n        return Enum.valueOf(ColumnDefinition.Kind.class, kind.toUpperCase());\n    }",
            " 468  \n 469  \n 470  \n 471 +\n 472  \n 473  \n 474  \n 475  ",
            "    private static ColumnDefinition.Kind deserializeKind(String kind)\n    {\n        if (\"clustering_key\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.CLUSTERING;\n        if (\"compact_value\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.REGULAR;\n        return Enum.valueOf(ColumnDefinition.Kind.class, kind.toUpperCase());\n    }"
        ],
        [
            "ColumnDefinition::clusteringKeyDef(String,String,String,AbstractType,Integer)",
            " 102  \n 103  \n 104 -\n 105  ",
            "    public static ColumnDefinition clusteringKeyDef(String ksName, String cfName, String name, AbstractType<?> validator, Integer componentIndex)\n    {\n        return new ColumnDefinition(ksName, cfName, ColumnIdentifier.getInterned(name, true),  validator, null, null, null, componentIndex, Kind.CLUSTERING_COLUMN);\n    }",
            " 101  \n 102  \n 103 +\n 104  ",
            "    public static ColumnDefinition clusteringKeyDef(String ksName, String cfName, String name, AbstractType<?> validator, Integer componentIndex)\n    {\n        return new ColumnDefinition(ksName, cfName, ColumnIdentifier.getInterned(name, true),  validator, null, null, null, componentIndex, Kind.CLUSTERING);\n    }"
        ],
        [
            "ColumnDefinition::isClusteringColumn()",
            " 229  \n 230  \n 231 -\n 232  ",
            "    public boolean isClusteringColumn()\n    {\n        return kind == Kind.CLUSTERING_COLUMN;\n    }",
            " 228  \n 229  \n 230 +\n 231  ",
            "    public boolean isClusteringColumn()\n    {\n        return kind == Kind.CLUSTERING;\n    }"
        ],
        [
            "SelectStatement::processPartition(RowIterator,QueryOptions,Selection,int)",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 -\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    void processPartition(RowIterator partition, QueryOptions options, Selection.ResultSetBuilder result, int nowInSec)\n    throws InvalidRequestException\n    {\n        int protocolVersion = options.getProtocolVersion();\n\n        ByteBuffer[] keyComponents = getComponents(cfm, partition.partitionKey());\n\n        Row staticRow = partition.staticRow().takeAlias();\n        // If there is no rows, then provided the select was a full partition selection\n        // (i.e. not a 2ndary index search and there was no condition on clustering columns),\n        // we want to include static columns and we're done.\n        if (!partition.hasNext())\n        {\n            if (!staticRow.isEmpty() && (!restrictions.usesSecondaryIndexing() || cfm.isStaticCompactTable()) && !restrictions.hasClusteringColumnsRestriction())\n            {\n                result.newRow(protocolVersion);\n                for (ColumnDefinition def : selection.getColumns())\n                {\n                    switch (def.kind)\n                    {\n                        case PARTITION_KEY:\n                            result.add(keyComponents[def.position()]);\n                            break;\n                        case STATIC:\n                            addValue(result, def, staticRow, nowInSec, protocolVersion);\n                            break;\n                        default:\n                            result.add((ByteBuffer)null);\n                    }\n                }\n            }\n            return;\n        }\n\n        while (partition.hasNext())\n        {\n            Row row = partition.next();\n            result.newRow(protocolVersion);\n            // Respect selection order\n            for (ColumnDefinition def : selection.getColumns())\n            {\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                        result.add(keyComponents[def.position()]);\n                        break;\n                    case CLUSTERING_COLUMN:\n                        result.add(row.clustering().get(def.position()));\n                        break;\n                    case REGULAR:\n                        addValue(result, def, row, nowInSec, protocolVersion);\n                        break;\n                    case STATIC:\n                        addValue(result, def, staticRow, nowInSec, protocolVersion);\n                        break;\n                }\n            }\n        }\n    }",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 +\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  ",
            "    void processPartition(RowIterator partition, QueryOptions options, Selection.ResultSetBuilder result, int nowInSec)\n    throws InvalidRequestException\n    {\n        int protocolVersion = options.getProtocolVersion();\n\n        ByteBuffer[] keyComponents = getComponents(cfm, partition.partitionKey());\n\n        Row staticRow = partition.staticRow().takeAlias();\n        // If there is no rows, then provided the select was a full partition selection\n        // (i.e. not a 2ndary index search and there was no condition on clustering columns),\n        // we want to include static columns and we're done.\n        if (!partition.hasNext())\n        {\n            if (!staticRow.isEmpty() && (!restrictions.usesSecondaryIndexing() || cfm.isStaticCompactTable()) && !restrictions.hasClusteringColumnsRestriction())\n            {\n                result.newRow(protocolVersion);\n                for (ColumnDefinition def : selection.getColumns())\n                {\n                    switch (def.kind)\n                    {\n                        case PARTITION_KEY:\n                            result.add(keyComponents[def.position()]);\n                            break;\n                        case STATIC:\n                            addValue(result, def, staticRow, nowInSec, protocolVersion);\n                            break;\n                        default:\n                            result.add((ByteBuffer)null);\n                    }\n                }\n            }\n            return;\n        }\n\n        while (partition.hasNext())\n        {\n            Row row = partition.next();\n            result.newRow(protocolVersion);\n            // Respect selection order\n            for (ColumnDefinition def : selection.getColumns())\n            {\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                        result.add(keyComponents[def.position()]);\n                        break;\n                    case CLUSTERING:\n                        result.add(row.clustering().get(def.position()));\n                        break;\n                    case REGULAR:\n                        addValue(result, def, row, nowInSec, protocolVersion);\n                        break;\n                    case STATIC:\n                        addValue(result, def, staticRow, nowInSec, protocolVersion);\n                        break;\n                }\n            }\n        }\n    }"
        ],
        [
            "RowFilter::Expression::getValue(DecoratedKey,Row)",
            " 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  ",
            "        protected ByteBuffer getValue(DecoratedKey partitionKey, Row row)\n        {\n            switch (column.kind)\n            {\n                case PARTITION_KEY:\n                    return column.isOnAllComponents()\n                         ? partitionKey.getKey()\n                         : CompositeType.extractComponent(partitionKey.getKey(), column.position());\n                case CLUSTERING_COLUMN:\n                    return row.clustering().get(column.position());\n                default:\n                    Cell cell = row.getCell(column);\n                    return cell == null ? null : cell.value();\n            }\n        }",
            " 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 +\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  ",
            "        protected ByteBuffer getValue(DecoratedKey partitionKey, Row row)\n        {\n            switch (column.kind)\n            {\n                case PARTITION_KEY:\n                    return column.isOnAllComponents()\n                         ? partitionKey.getKey()\n                         : CompositeType.extractComponent(partitionKey.getKey(), column.position());\n                case CLUSTERING:\n                    return row.clustering().get(column.position());\n                default:\n                    Cell cell = row.getCell(column);\n                    return cell == null ? null : cell.value();\n            }\n        }"
        ],
        [
            "AbstractSimplePerColumnSecondaryIndex::validate(Clustering)",
            " 220  \n 221  \n 222 -\n 223  \n 224  ",
            "    public void validate(Clustering clustering) throws InvalidRequestException\n    {\n        if (columnDef.kind == ColumnDefinition.Kind.CLUSTERING_COLUMN)\n            validateIndexedValue(getIndexedValue(null, clustering, null, null));\n    }",
            " 220  \n 221  \n 222 +\n 223  \n 224  ",
            "    public void validate(Clustering clustering) throws InvalidRequestException\n    {\n        if (columnDef.kind == ColumnDefinition.Kind.CLUSTERING)\n            validateIndexedValue(getIndexedValue(null, clustering, null, null));\n    }"
        ],
        [
            "ModificationStatement::processWhereClause(List,VariableSpecifications)",
            " 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 -\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  ",
            "    public void processWhereClause(List<Relation> whereClause, VariableSpecifications names) throws InvalidRequestException\n    {\n        for (Relation relation : whereClause)\n        {\n            if (relation.isMultiColumn())\n            {\n                throw new InvalidRequestException(\n                        String.format(\"Multi-column relations cannot be used in WHERE clauses for UPDATE and DELETE statements: %s\", relation));\n            }\n            SingleColumnRelation rel = (SingleColumnRelation) relation;\n\n            if (rel.onToken())\n                throw new InvalidRequestException(String.format(\"The token function cannot be used in WHERE clauses for UPDATE and DELETE statements: %s\", relation));\n\n            ColumnIdentifier id = rel.getEntity().prepare(cfm);\n            ColumnDefinition def = cfm.getColumnDefinition(id);\n            if (def == null)\n                throw new InvalidRequestException(String.format(\"Unknown key identifier %s\", id));\n\n            switch (def.kind)\n            {\n                case PARTITION_KEY:\n                case CLUSTERING_COLUMN:\n                    Restriction restriction;\n\n                    if (rel.isEQ() || (def.isPartitionKey() && rel.isIN()))\n                    {\n                        restriction = rel.toRestriction(cfm, names);\n                    }\n                    else\n                    {\n                        throw new InvalidRequestException(String.format(\"Invalid operator %s for PRIMARY KEY part %s\", rel.operator(), def.name));\n                    }\n\n                    addKeyValues(def, restriction);\n                    break;\n                default:\n                    throw new InvalidRequestException(String.format(\"Non PRIMARY KEY %s found in where clause\", def.name));\n            }\n        }\n    }",
            " 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 +\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  ",
            "    public void processWhereClause(List<Relation> whereClause, VariableSpecifications names) throws InvalidRequestException\n    {\n        for (Relation relation : whereClause)\n        {\n            if (relation.isMultiColumn())\n            {\n                throw new InvalidRequestException(\n                        String.format(\"Multi-column relations cannot be used in WHERE clauses for UPDATE and DELETE statements: %s\", relation));\n            }\n            SingleColumnRelation rel = (SingleColumnRelation) relation;\n\n            if (rel.onToken())\n                throw new InvalidRequestException(String.format(\"The token function cannot be used in WHERE clauses for UPDATE and DELETE statements: %s\", relation));\n\n            ColumnIdentifier id = rel.getEntity().prepare(cfm);\n            ColumnDefinition def = cfm.getColumnDefinition(id);\n            if (def == null)\n                throw new InvalidRequestException(String.format(\"Unknown key identifier %s\", id));\n\n            switch (def.kind)\n            {\n                case PARTITION_KEY:\n                case CLUSTERING:\n                    Restriction restriction;\n\n                    if (rel.isEQ() || (def.isPartitionKey() && rel.isIN()))\n                    {\n                        restriction = rel.toRestriction(cfm, names);\n                    }\n                    else\n                    {\n                        throw new InvalidRequestException(String.format(\"Invalid operator %s for PRIMARY KEY part %s\", rel.operator(), def.name));\n                    }\n\n                    addKeyValues(def, restriction);\n                    break;\n                default:\n                    throw new InvalidRequestException(String.format(\"Non PRIMARY KEY %s found in where clause\", def.name));\n            }\n        }\n    }"
        ],
        [
            "ModificationStatement::addKeyValues(ColumnDefinition,Restriction)",
            " 330  \n 331  \n 332 -\n 333  \n 334  \n 335  \n 336  ",
            "    private void addKeyValues(ColumnDefinition def, Restriction values) throws InvalidRequestException\n    {\n        if (def.kind == ColumnDefinition.Kind.CLUSTERING_COLUMN)\n            hasNoClusteringColumns = false;\n        if (processedKeys.put(def.name, values) != null)\n            throw new InvalidRequestException(String.format(\"Multiple definitions found for PRIMARY KEY part %s\", def.name));\n    }",
            " 330  \n 331  \n 332 +\n 333  \n 334  \n 335  \n 336  ",
            "    private void addKeyValues(ColumnDefinition def, Restriction values) throws InvalidRequestException\n    {\n        if (def.kind == ColumnDefinition.Kind.CLUSTERING)\n            hasNoClusteringColumns = false;\n        if (processedKeys.put(def.name, values) != null)\n            throw new InvalidRequestException(String.format(\"Multiple definitions found for PRIMARY KEY part %s\", def.name));\n    }"
        ],
        [
            "ModificationStatement::Parsed::prepare(VariableSpecifications)",
            " 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902 -\n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  ",
            "        public ModificationStatement prepare(VariableSpecifications boundNames) throws InvalidRequestException\n        {\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace(), columnFamily());\n\n            Attributes preparedAttributes = attrs.prepare(keyspace(), columnFamily());\n            preparedAttributes.collectMarkerSpecification(boundNames);\n\n            ModificationStatement stmt = prepareInternal(metadata, boundNames, preparedAttributes);\n\n            if (ifNotExists || ifExists || !conditions.isEmpty())\n            {\n                if (stmt.isCounter())\n                    throw new InvalidRequestException(\"Conditional updates are not supported on counter tables\");\n\n                if (attrs.timestamp != null)\n                    throw new InvalidRequestException(\"Cannot provide custom timestamp for conditional updates\");\n\n                if (ifNotExists)\n                {\n                    // To have both 'IF NOT EXISTS' and some other conditions doesn't make sense.\n                    // So far this is enforced by the parser, but let's assert it for sanity if ever the parse changes.\n                    assert conditions.isEmpty();\n                    assert !ifExists;\n                    stmt.setIfNotExistCondition();\n                }\n                else if (ifExists)\n                {\n                    assert conditions.isEmpty();\n                    assert !ifNotExists;\n                    stmt.setIfExistCondition();\n                }\n                else\n                {\n                    for (Pair<ColumnIdentifier.Raw, ColumnCondition.Raw> entry : conditions)\n                    {\n                        ColumnIdentifier id = entry.left.prepare(metadata);\n                        ColumnDefinition def = metadata.getColumnDefinition(id);\n                        if (def == null)\n                            throw new InvalidRequestException(String.format(\"Unknown identifier %s\", id));\n\n                        ColumnCondition condition = entry.right.prepare(keyspace(), def);\n                        condition.collectMarkerSpecification(boundNames);\n\n                        switch (def.kind)\n                        {\n                            case PARTITION_KEY:\n                            case CLUSTERING_COLUMN:\n                                throw new InvalidRequestException(String.format(\"PRIMARY KEY column '%s' cannot have IF conditions\", id));\n                            default:\n                                stmt.addCondition(condition);\n                                break;\n                        }\n                    }\n                }\n\n                stmt.validateWhereClauseForConditions();\n            }\n\n            stmt.finishPreparation();\n            return stmt;\n        }",
            " 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902 +\n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  ",
            "        public ModificationStatement prepare(VariableSpecifications boundNames) throws InvalidRequestException\n        {\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace(), columnFamily());\n\n            Attributes preparedAttributes = attrs.prepare(keyspace(), columnFamily());\n            preparedAttributes.collectMarkerSpecification(boundNames);\n\n            ModificationStatement stmt = prepareInternal(metadata, boundNames, preparedAttributes);\n\n            if (ifNotExists || ifExists || !conditions.isEmpty())\n            {\n                if (stmt.isCounter())\n                    throw new InvalidRequestException(\"Conditional updates are not supported on counter tables\");\n\n                if (attrs.timestamp != null)\n                    throw new InvalidRequestException(\"Cannot provide custom timestamp for conditional updates\");\n\n                if (ifNotExists)\n                {\n                    // To have both 'IF NOT EXISTS' and some other conditions doesn't make sense.\n                    // So far this is enforced by the parser, but let's assert it for sanity if ever the parse changes.\n                    assert conditions.isEmpty();\n                    assert !ifExists;\n                    stmt.setIfNotExistCondition();\n                }\n                else if (ifExists)\n                {\n                    assert conditions.isEmpty();\n                    assert !ifNotExists;\n                    stmt.setIfExistCondition();\n                }\n                else\n                {\n                    for (Pair<ColumnIdentifier.Raw, ColumnCondition.Raw> entry : conditions)\n                    {\n                        ColumnIdentifier id = entry.left.prepare(metadata);\n                        ColumnDefinition def = metadata.getColumnDefinition(id);\n                        if (def == null)\n                            throw new InvalidRequestException(String.format(\"Unknown identifier %s\", id));\n\n                        ColumnCondition condition = entry.right.prepare(keyspace(), def);\n                        condition.collectMarkerSpecification(boundNames);\n\n                        switch (def.kind)\n                        {\n                            case PARTITION_KEY:\n                            case CLUSTERING:\n                                throw new InvalidRequestException(String.format(\"PRIMARY KEY column '%s' cannot have IF conditions\", id));\n                            default:\n                                stmt.addCondition(condition);\n                                break;\n                        }\n                    }\n                }\n\n                stmt.validateWhereClauseForConditions();\n            }\n\n            stmt.finishPreparation();\n            return stmt;\n        }"
        ],
        [
            "CFMetaData::addOrReplaceColumnDefinition(ColumnDefinition)",
            "1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162 -\n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  ",
            "    public CFMetaData addOrReplaceColumnDefinition(ColumnDefinition def)\n    {\n        // Adds the definition and rebuild what is necessary. We could call rebuild() but it's not too hard to\n        // only rebuild the necessary bits.\n        switch (def.kind)\n        {\n            case PARTITION_KEY:\n                partitionKeyColumns.set(def.position(), def);\n                List<AbstractType<?>> keyTypes = extractTypes(partitionKeyColumns);\n                keyValidator = keyTypes.size() == 1 ? keyTypes.get(0) : CompositeType.getInstance(keyTypes);\n                break;\n            case CLUSTERING_COLUMN:\n                clusteringColumns.set(def.position(), def);\n                comparator = new ClusteringComparator(extractTypes(clusteringColumns));\n                break;\n            case REGULAR:\n            case STATIC:\n                PartitionColumns.Builder builder = PartitionColumns.builder();\n                for (ColumnDefinition column : partitionColumns)\n                    if (!column.name.equals(def.name))\n                        builder.add(column);\n                builder.add(def);\n                partitionColumns = builder.build();\n                // If dense, we must have modified the compact value since that's the only one we can have.\n                if (isDense)\n                    this.compactValueColumn = def;\n                break;\n        }\n        this.columnMetadata.put(def.name.bytes, def);\n        return this;\n    }",
            "1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162 +\n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  ",
            "    public CFMetaData addOrReplaceColumnDefinition(ColumnDefinition def)\n    {\n        // Adds the definition and rebuild what is necessary. We could call rebuild() but it's not too hard to\n        // only rebuild the necessary bits.\n        switch (def.kind)\n        {\n            case PARTITION_KEY:\n                partitionKeyColumns.set(def.position(), def);\n                List<AbstractType<?>> keyTypes = extractTypes(partitionKeyColumns);\n                keyValidator = keyTypes.size() == 1 ? keyTypes.get(0) : CompositeType.getInstance(keyTypes);\n                break;\n            case CLUSTERING:\n                clusteringColumns.set(def.position(), def);\n                comparator = new ClusteringComparator(extractTypes(clusteringColumns));\n                break;\n            case REGULAR:\n            case STATIC:\n                PartitionColumns.Builder builder = PartitionColumns.builder();\n                for (ColumnDefinition column : partitionColumns)\n                    if (!column.name.equals(def.name))\n                        builder.add(column);\n                builder.add(def);\n                partitionColumns = builder.build();\n                // If dense, we must have modified the compact value since that's the only one we can have.\n                if (isDense)\n                    this.compactValueColumn = def;\n                break;\n        }\n        this.columnMetadata.put(def.name.bytes, def);\n        return this;\n    }"
        ],
        [
            "SchemaKeyspace::deserializeKind(String)",
            "1110  \n1111  \n1112  \n1113 -\n1114  \n1115  \n1116  \n1117  ",
            "    public static ColumnDefinition.Kind deserializeKind(String kind)\n    {\n        if (\"clustering_key\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.CLUSTERING_COLUMN;\n        if (\"compact_value\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.REGULAR;\n        return Enum.valueOf(ColumnDefinition.Kind.class, kind.toUpperCase());\n    }",
            "1110  \n1111  \n1112  \n1113 +\n1114  \n1115  \n1116  \n1117  ",
            "    public static ColumnDefinition.Kind deserializeKind(String kind)\n    {\n        if (\"clustering_key\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.CLUSTERING;\n        if (\"compact_value\".equalsIgnoreCase(kind))\n            return ColumnDefinition.Kind.REGULAR;\n        return Enum.valueOf(ColumnDefinition.Kind.class, kind.toUpperCase());\n    }"
        ],
        [
            "UpdateStatement::ParsedUpdate::prepareInternal(CFMetaData,VariableSpecifications,Attributes)",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "        protected ModificationStatement prepareInternal(CFMetaData cfm, VariableSpecifications boundNames, Attributes attrs) throws InvalidRequestException\n        {\n            UpdateStatement stmt = new UpdateStatement(ModificationStatement.StatementType.UPDATE, boundNames.size(), cfm, attrs);\n\n            for (Pair<ColumnIdentifier.Raw, Operation.RawUpdate> entry : updates)\n            {\n                ColumnDefinition def = cfm.getColumnDefinition(entry.left.prepare(cfm));\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Unknown identifier %s\", entry.left));\n\n                Operation operation = entry.right.prepare(keyspace(), def);\n                operation.collectMarkerSpecification(boundNames);\n\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                    case CLUSTERING_COLUMN:\n                        throw new InvalidRequestException(String.format(\"PRIMARY KEY part %s found in SET part\", entry.left));\n                    default:\n                        stmt.addOperation(operation);\n                        break;\n                }\n            }\n\n            stmt.processWhereClause(whereClause, boundNames);\n            return stmt;\n        }",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 +\n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "        protected ModificationStatement prepareInternal(CFMetaData cfm, VariableSpecifications boundNames, Attributes attrs) throws InvalidRequestException\n        {\n            UpdateStatement stmt = new UpdateStatement(ModificationStatement.StatementType.UPDATE, boundNames.size(), cfm, attrs);\n\n            for (Pair<ColumnIdentifier.Raw, Operation.RawUpdate> entry : updates)\n            {\n                ColumnDefinition def = cfm.getColumnDefinition(entry.left.prepare(cfm));\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Unknown identifier %s\", entry.left));\n\n                Operation operation = entry.right.prepare(keyspace(), def);\n                operation.collectMarkerSpecification(boundNames);\n\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                    case CLUSTERING:\n                        throw new InvalidRequestException(String.format(\"PRIMARY KEY part %s found in SET part\", entry.left));\n                    default:\n                        stmt.addOperation(operation);\n                        break;\n                }\n            }\n\n            stmt.processWhereClause(whereClause, boundNames);\n            return stmt;\n        }"
        ],
        [
            "ColumnDefinition::clusteringKeyDef(CFMetaData,ByteBuffer,AbstractType,Integer)",
            "  97  \n  98  \n  99 -\n 100  ",
            "    public static ColumnDefinition clusteringKeyDef(CFMetaData cfm, ByteBuffer name, AbstractType<?> validator, Integer componentIndex)\n    {\n        return new ColumnDefinition(cfm, name, validator, componentIndex, Kind.CLUSTERING_COLUMN);\n    }",
            "  96  \n  97  \n  98 +\n  99  ",
            "    public static ColumnDefinition clusteringKeyDef(CFMetaData cfm, ByteBuffer name, AbstractType<?> validator, Integer componentIndex)\n    {\n        return new ColumnDefinition(cfm, name, validator, componentIndex, Kind.CLUSTERING);\n    }"
        ],
        [
            "CompositesIndex::create(ColumnDefinition)",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "    public static CompositesIndex create(ColumnDefinition cfDef)\n    {\n        if (cfDef.type.isCollection() && cfDef.type.isMultiCell())\n        {\n            switch (((CollectionType)cfDef.type).kind)\n            {\n                case LIST:\n                    return new CompositesIndexOnCollectionValue();\n                case SET:\n                    return new CompositesIndexOnCollectionKey();\n                case MAP:\n                    if (cfDef.hasIndexOption(SecondaryIndex.INDEX_KEYS_OPTION_NAME))\n                        return new CompositesIndexOnCollectionKey();\n                    else if (cfDef.hasIndexOption(SecondaryIndex.INDEX_ENTRIES_OPTION_NAME))\n                        return new CompositesIndexOnCollectionKeyAndValue();\n                    else\n                        return new CompositesIndexOnCollectionValue();\n            }\n        }\n\n        switch (cfDef.kind)\n        {\n            case CLUSTERING_COLUMN:\n                return new CompositesIndexOnClusteringKey();\n            case REGULAR:\n                return new CompositesIndexOnRegular();\n            case PARTITION_KEY:\n                return new CompositesIndexOnPartitionKey();\n            //case COMPACT_VALUE:\n            //    return new CompositesIndexOnCompactValue();\n        }\n        throw new AssertionError();\n    }",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "    public static CompositesIndex create(ColumnDefinition cfDef)\n    {\n        if (cfDef.type.isCollection() && cfDef.type.isMultiCell())\n        {\n            switch (((CollectionType)cfDef.type).kind)\n            {\n                case LIST:\n                    return new CompositesIndexOnCollectionValue();\n                case SET:\n                    return new CompositesIndexOnCollectionKey();\n                case MAP:\n                    if (cfDef.hasIndexOption(SecondaryIndex.INDEX_KEYS_OPTION_NAME))\n                        return new CompositesIndexOnCollectionKey();\n                    else if (cfDef.hasIndexOption(SecondaryIndex.INDEX_ENTRIES_OPTION_NAME))\n                        return new CompositesIndexOnCollectionKeyAndValue();\n                    else\n                        return new CompositesIndexOnCollectionValue();\n            }\n        }\n\n        switch (cfDef.kind)\n        {\n            case CLUSTERING:\n                return new CompositesIndexOnClusteringKey();\n            case REGULAR:\n                return new CompositesIndexOnRegular();\n            case PARTITION_KEY:\n                return new CompositesIndexOnPartitionKey();\n            //case COMPACT_VALUE:\n            //    return new CompositesIndexOnCompactValue();\n        }\n        throw new AssertionError();\n    }"
        ],
        [
            "CFMetaData::create(String,String,UUID,boolean,boolean,boolean,boolean,List)",
            " 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313 -\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "    public static CFMetaData create(String ksName,\n                                    String name,\n                                    UUID cfId,\n                                    boolean isDense,\n                                    boolean isCompound,\n                                    boolean isSuper,\n                                    boolean isCounter,\n                                    List<ColumnDefinition> columns)\n    {\n        List<ColumnDefinition> partitions = new ArrayList<>();\n        List<ColumnDefinition> clusterings = new ArrayList<>();\n        PartitionColumns.Builder builder = PartitionColumns.builder();\n\n        for (ColumnDefinition column : columns)\n        {\n            switch (column.kind)\n            {\n                case PARTITION_KEY:\n                    partitions.add(column);\n                    break;\n                case CLUSTERING_COLUMN:\n                    clusterings.add(column);\n                    break;\n                default:\n                    builder.add(column);\n                    break;\n            }\n        }\n\n        Collections.sort(partitions);\n        Collections.sort(clusterings);\n\n        return new CFMetaData(ksName,\n                              name,\n                              cfId,\n                              isSuper,\n                              isCounter,\n                              isDense,\n                              isCompound,\n                              partitions,\n                              clusterings,\n                              builder.build());\n    }",
            " 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313 +\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "    public static CFMetaData create(String ksName,\n                                    String name,\n                                    UUID cfId,\n                                    boolean isDense,\n                                    boolean isCompound,\n                                    boolean isSuper,\n                                    boolean isCounter,\n                                    List<ColumnDefinition> columns)\n    {\n        List<ColumnDefinition> partitions = new ArrayList<>();\n        List<ColumnDefinition> clusterings = new ArrayList<>();\n        PartitionColumns.Builder builder = PartitionColumns.builder();\n\n        for (ColumnDefinition column : columns)\n        {\n            switch (column.kind)\n            {\n                case PARTITION_KEY:\n                    partitions.add(column);\n                    break;\n                case CLUSTERING:\n                    clusterings.add(column);\n                    break;\n                default:\n                    builder.add(column);\n                    break;\n            }\n        }\n\n        Collections.sort(partitions);\n        Collections.sort(clusterings);\n\n        return new CFMetaData(ksName,\n                              name,\n                              cfId,\n                              isSuper,\n                              isCounter,\n                              isDense,\n                              isCompound,\n                              partitions,\n                              clusterings,\n                              builder.build());\n    }"
        ],
        [
            "AlterTableStatement::announceMigration(boolean)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  ",
            "    public boolean announceMigration(boolean isLocalOnly) throws RequestValidationException\n    {\n        CFMetaData meta = validateColumnFamily(keyspace(), columnFamily());\n        CFMetaData cfm = meta.copy();\n\n        CQL3Type validator = this.validator == null ? null : this.validator.prepare(keyspace());\n        ColumnIdentifier columnName = null;\n        ColumnDefinition def = null;\n        if (rawColumnName != null)\n        {\n            columnName = rawColumnName.prepare(cfm);\n            def = cfm.getColumnDefinition(columnName);\n        }\n\n        switch (oType)\n        {\n            case ADD:\n                assert columnName != null;\n                if (cfm.isDense())\n                    throw new InvalidRequestException(\"Cannot add new column to a COMPACT STORAGE table\");\n\n                if (isStatic)\n                {\n                    if (!cfm.isCompound())\n                        throw new InvalidRequestException(\"Static columns are not allowed in COMPACT STORAGE tables\");\n                    if (cfm.clusteringColumns().isEmpty())\n                        throw new InvalidRequestException(\"Static columns are only useful (and thus allowed) if the table has at least one clustering column\");\n                }\n\n                if (def != null)\n                {\n                    switch (def.kind)\n                    {\n                        case PARTITION_KEY:\n                        case CLUSTERING_COLUMN:\n                            throw new InvalidRequestException(String.format(\"Invalid column name %s because it conflicts with a PRIMARY KEY part\", columnName));\n                        default:\n                            throw new InvalidRequestException(String.format(\"Invalid column name %s because it conflicts with an existing column\", columnName));\n                    }\n                }\n\n                // Cannot re-add a dropped counter column. See #7831.\n                if (meta.isCounter() && meta.getDroppedColumns().containsKey(columnName.bytes))\n                    throw new InvalidRequestException(String.format(\"Cannot re-add previously dropped counter column %s\", columnName));\n\n                AbstractType<?> type = validator.getType();\n                if (type.isCollection() && type.isMultiCell())\n                {\n                    if (!cfm.isCompound())\n                        throw new InvalidRequestException(\"Cannot use non-frozen collections in COMPACT STORAGE tables\");\n                    if (cfm.isSuper())\n                        throw new InvalidRequestException(\"Cannot use non-frozen collections with super column families\");\n\n                    // If there used to be a collection column with the same name (that has been dropped), we could still have\n                    // some data using the old type, and so we can't allow adding a collection with the same name unless\n                    // the types are compatible (see #6276).\n                    CFMetaData.DroppedColumn dropped = cfm.getDroppedColumns().get(columnName.bytes);\n                    // We could have type == null for old dropped columns, in which case we play it safe and refuse\n                    if (dropped != null && (dropped.type == null || (dropped.type instanceof CollectionType && !type.isCompatibleWith(dropped.type))))\n                        throw new InvalidRequestException(String.format(\"Cannot add a collection with the name %s \" +\n                                    \"because a collection with the same name and a different type%s has already been used in the past\",\n                                    columnName, dropped.type == null ? \"\" : \" (\" + dropped.type.asCQL3Type() + \")\"));\n                }\n\n                Integer componentIndex = cfm.isCompound() ? cfm.comparator.size() : null;\n                cfm.addColumnDefinition(isStatic\n                                        ? ColumnDefinition.staticDef(cfm, columnName.bytes, type, componentIndex)\n                                        : ColumnDefinition.regularDef(cfm, columnName.bytes, type, componentIndex));\n                break;\n\n            case ALTER:\n                assert columnName != null;\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Column %s was not found in table %s\", columnName, columnFamily()));\n\n                AbstractType<?> validatorType = validator.getType();\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                        if (validatorType instanceof CounterColumnType)\n                            throw new InvalidRequestException(String.format(\"counter type is not supported for PRIMARY KEY part %s\", columnName));\n\n                        AbstractType<?> currentType = cfm.getKeyValidatorAsClusteringComparator().subtype(def.position());\n                        if (!validatorType.isValueCompatibleWith(currentType))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are incompatible.\",\n                                                                           columnName,\n                                                                           currentType.asCQL3Type(),\n                                                                           validator));\n                        break;\n                    case CLUSTERING_COLUMN:\n                        AbstractType<?> oldType = cfm.comparator.subtype(def.position());\n                        // Note that CFMetaData.validateCompatibility already validate the change we're about to do. However, the error message it\n                        // sends is a bit cryptic for a CQL3 user, so validating here for a sake of returning a better error message\n                        // Do note that we need isCompatibleWith here, not just isValueCompatibleWith.\n                        if (!validatorType.isCompatibleWith(oldType))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are not order-compatible.\",\n                                                                           columnName,\n                                                                           oldType.asCQL3Type(),\n                                                                           validator));\n\n                        break;\n                    case REGULAR:\n                    case STATIC:\n                        // Thrift allows to change a column validator so CFMetaData.validateCompatibility will let it slide\n                        // if we change to an incompatible type (contrarily to the comparator case). But we don't want to\n                        // allow it for CQL3 (see #5882) so validating it explicitly here. We only care about value compatibility\n                        // though since we won't compare values (except when there is an index, but that is validated by\n                        // ColumnDefinition already).\n                        if (!validatorType.isValueCompatibleWith(def.type))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are incompatible.\",\n                                                                           columnName,\n                                                                           def.type.asCQL3Type(),\n                                                                           validator));\n                        break;\n                }\n                // In any case, we update the column definition\n                cfm.addOrReplaceColumnDefinition(def.withNewType(validatorType));\n                break;\n\n            case DROP:\n                assert columnName != null;\n                if (!cfm.isCQLTable())\n                    throw new InvalidRequestException(\"Cannot drop columns from a non-CQL3 table\");\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Column %s was not found in table %s\", columnName, columnFamily()));\n\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                    case CLUSTERING_COLUMN:\n                        throw new InvalidRequestException(String.format(\"Cannot drop PRIMARY KEY part %s\", columnName));\n                    case REGULAR:\n                    case STATIC:\n                        ColumnDefinition toDelete = null;\n                        for (ColumnDefinition columnDef : cfm.partitionColumns())\n                        {\n                            if (columnDef.name.equals(columnName))\n                            {\n                                toDelete = columnDef;\n                                break;\n                            }\n                        }\n                        assert toDelete != null;\n                        cfm.removeColumnDefinition(toDelete);\n                        cfm.recordColumnDrop(toDelete);\n                        break;\n                }\n                break;\n            case OPTS:\n                if (cfProps == null)\n                    throw new InvalidRequestException(\"ALTER TABLE WITH invoked, but no parameters found\");\n\n                cfProps.validate();\n\n                if (meta.isCounter() && cfProps.getDefaultTimeToLive() > 0)\n                    throw new InvalidRequestException(\"Cannot set default_time_to_live on a table with counters\");\n\n                cfProps.applyToCFMetadata(cfm);\n                break;\n            case RENAME:\n                for (Map.Entry<ColumnIdentifier.Raw, ColumnIdentifier.Raw> entry : renames.entrySet())\n                {\n                    ColumnIdentifier from = entry.getKey().prepare(cfm);\n                    ColumnIdentifier to = entry.getValue().prepare(cfm);\n                    cfm.renameColumn(from, to);\n                }\n                break;\n        }\n\n        MigrationManager.announceColumnFamilyUpdate(cfm, false, isLocalOnly);\n        return true;\n    }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  ",
            "    public boolean announceMigration(boolean isLocalOnly) throws RequestValidationException\n    {\n        CFMetaData meta = validateColumnFamily(keyspace(), columnFamily());\n        CFMetaData cfm = meta.copy();\n\n        CQL3Type validator = this.validator == null ? null : this.validator.prepare(keyspace());\n        ColumnIdentifier columnName = null;\n        ColumnDefinition def = null;\n        if (rawColumnName != null)\n        {\n            columnName = rawColumnName.prepare(cfm);\n            def = cfm.getColumnDefinition(columnName);\n        }\n\n        switch (oType)\n        {\n            case ADD:\n                assert columnName != null;\n                if (cfm.isDense())\n                    throw new InvalidRequestException(\"Cannot add new column to a COMPACT STORAGE table\");\n\n                if (isStatic)\n                {\n                    if (!cfm.isCompound())\n                        throw new InvalidRequestException(\"Static columns are not allowed in COMPACT STORAGE tables\");\n                    if (cfm.clusteringColumns().isEmpty())\n                        throw new InvalidRequestException(\"Static columns are only useful (and thus allowed) if the table has at least one clustering column\");\n                }\n\n                if (def != null)\n                {\n                    switch (def.kind)\n                    {\n                        case PARTITION_KEY:\n                        case CLUSTERING:\n                            throw new InvalidRequestException(String.format(\"Invalid column name %s because it conflicts with a PRIMARY KEY part\", columnName));\n                        default:\n                            throw new InvalidRequestException(String.format(\"Invalid column name %s because it conflicts with an existing column\", columnName));\n                    }\n                }\n\n                // Cannot re-add a dropped counter column. See #7831.\n                if (meta.isCounter() && meta.getDroppedColumns().containsKey(columnName.bytes))\n                    throw new InvalidRequestException(String.format(\"Cannot re-add previously dropped counter column %s\", columnName));\n\n                AbstractType<?> type = validator.getType();\n                if (type.isCollection() && type.isMultiCell())\n                {\n                    if (!cfm.isCompound())\n                        throw new InvalidRequestException(\"Cannot use non-frozen collections in COMPACT STORAGE tables\");\n                    if (cfm.isSuper())\n                        throw new InvalidRequestException(\"Cannot use non-frozen collections with super column families\");\n\n                    // If there used to be a collection column with the same name (that has been dropped), we could still have\n                    // some data using the old type, and so we can't allow adding a collection with the same name unless\n                    // the types are compatible (see #6276).\n                    CFMetaData.DroppedColumn dropped = cfm.getDroppedColumns().get(columnName.bytes);\n                    // We could have type == null for old dropped columns, in which case we play it safe and refuse\n                    if (dropped != null && (dropped.type == null || (dropped.type instanceof CollectionType && !type.isCompatibleWith(dropped.type))))\n                        throw new InvalidRequestException(String.format(\"Cannot add a collection with the name %s \" +\n                                    \"because a collection with the same name and a different type%s has already been used in the past\",\n                                    columnName, dropped.type == null ? \"\" : \" (\" + dropped.type.asCQL3Type() + \")\"));\n                }\n\n                Integer componentIndex = cfm.isCompound() ? cfm.comparator.size() : null;\n                cfm.addColumnDefinition(isStatic\n                                        ? ColumnDefinition.staticDef(cfm, columnName.bytes, type, componentIndex)\n                                        : ColumnDefinition.regularDef(cfm, columnName.bytes, type, componentIndex));\n                break;\n\n            case ALTER:\n                assert columnName != null;\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Column %s was not found in table %s\", columnName, columnFamily()));\n\n                AbstractType<?> validatorType = validator.getType();\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                        if (validatorType instanceof CounterColumnType)\n                            throw new InvalidRequestException(String.format(\"counter type is not supported for PRIMARY KEY part %s\", columnName));\n\n                        AbstractType<?> currentType = cfm.getKeyValidatorAsClusteringComparator().subtype(def.position());\n                        if (!validatorType.isValueCompatibleWith(currentType))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are incompatible.\",\n                                                                           columnName,\n                                                                           currentType.asCQL3Type(),\n                                                                           validator));\n                        break;\n                    case CLUSTERING:\n                        AbstractType<?> oldType = cfm.comparator.subtype(def.position());\n                        // Note that CFMetaData.validateCompatibility already validate the change we're about to do. However, the error message it\n                        // sends is a bit cryptic for a CQL3 user, so validating here for a sake of returning a better error message\n                        // Do note that we need isCompatibleWith here, not just isValueCompatibleWith.\n                        if (!validatorType.isCompatibleWith(oldType))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are not order-compatible.\",\n                                                                           columnName,\n                                                                           oldType.asCQL3Type(),\n                                                                           validator));\n\n                        break;\n                    case REGULAR:\n                    case STATIC:\n                        // Thrift allows to change a column validator so CFMetaData.validateCompatibility will let it slide\n                        // if we change to an incompatible type (contrarily to the comparator case). But we don't want to\n                        // allow it for CQL3 (see #5882) so validating it explicitly here. We only care about value compatibility\n                        // though since we won't compare values (except when there is an index, but that is validated by\n                        // ColumnDefinition already).\n                        if (!validatorType.isValueCompatibleWith(def.type))\n                            throw new ConfigurationException(String.format(\"Cannot change %s from type %s to type %s: types are incompatible.\",\n                                                                           columnName,\n                                                                           def.type.asCQL3Type(),\n                                                                           validator));\n                        break;\n                }\n                // In any case, we update the column definition\n                cfm.addOrReplaceColumnDefinition(def.withNewType(validatorType));\n                break;\n\n            case DROP:\n                assert columnName != null;\n                if (!cfm.isCQLTable())\n                    throw new InvalidRequestException(\"Cannot drop columns from a non-CQL3 table\");\n                if (def == null)\n                    throw new InvalidRequestException(String.format(\"Column %s was not found in table %s\", columnName, columnFamily()));\n\n                switch (def.kind)\n                {\n                    case PARTITION_KEY:\n                    case CLUSTERING:\n                        throw new InvalidRequestException(String.format(\"Cannot drop PRIMARY KEY part %s\", columnName));\n                    case REGULAR:\n                    case STATIC:\n                        ColumnDefinition toDelete = null;\n                        for (ColumnDefinition columnDef : cfm.partitionColumns())\n                        {\n                            if (columnDef.name.equals(columnName))\n                            {\n                                toDelete = columnDef;\n                                break;\n                            }\n                        }\n                        assert toDelete != null;\n                        cfm.removeColumnDefinition(toDelete);\n                        cfm.recordColumnDrop(toDelete);\n                        break;\n                }\n                break;\n            case OPTS:\n                if (cfProps == null)\n                    throw new InvalidRequestException(\"ALTER TABLE WITH invoked, but no parameters found\");\n\n                cfProps.validate();\n\n                if (meta.isCounter() && cfProps.getDefaultTimeToLive() > 0)\n                    throw new InvalidRequestException(\"Cannot set default_time_to_live on a table with counters\");\n\n                cfProps.applyToCFMetadata(cfm);\n                break;\n            case RENAME:\n                for (Map.Entry<ColumnIdentifier.Raw, ColumnIdentifier.Raw> entry : renames.entrySet())\n                {\n                    ColumnIdentifier from = entry.getKey().prepare(cfm);\n                    ColumnIdentifier to = entry.getValue().prepare(cfm);\n                    cfm.renameColumn(from, to);\n                }\n                break;\n        }\n\n        MigrationManager.announceColumnFamilyUpdate(cfm, false, isLocalOnly);\n        return true;\n    }"
        ],
        [
            "SchemaKeyspace::serializeKind(ColumnDefinition,boolean)",
            "1098  \n1099  \n1100 -\n1101 -\n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  ",
            "    private static String serializeKind(ColumnDefinition.Kind kind, boolean isDense)\n    {\n        // For backward compatibility, we special case CLUSTERING_COLUMN and the case where the table is dense.\n        if (kind == ColumnDefinition.Kind.CLUSTERING_COLUMN)\n            return \"clustering_key\";\n\n        if (kind == ColumnDefinition.Kind.REGULAR && isDense)\n            return \"compact_value\";\n\n        return kind.toString().toLowerCase();\n    }",
            "1098  \n1099  \n1100 +\n1101 +\n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  ",
            "    private static String serializeKind(ColumnDefinition.Kind kind, boolean isDense)\n    {\n        // For backward compatibility, we special case CLUSTERING and the case where the table is dense.\n        if (kind == ColumnDefinition.Kind.CLUSTERING)\n            return \"clustering_key\";\n\n        if (kind == ColumnDefinition.Kind.REGULAR && isDense)\n            return \"compact_value\";\n\n        return kind.toString().toLowerCase();\n    }"
        ],
        [
            "CFMetaData::Builder::build()",
            "1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485 -\n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  ",
            "        public CFMetaData build()\n        {\n            if (tableId == null)\n                tableId = UUIDGen.getTimeUUID();\n\n            List<ColumnDefinition> partitions = new ArrayList<>(partitionKeys.size());\n            List<ColumnDefinition> clusterings = new ArrayList<>(clusteringColumns.size());\n            PartitionColumns.Builder builder = PartitionColumns.builder();\n\n            for (int i = 0; i < partitionKeys.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = partitionKeys.get(i);\n                Integer componentIndex = partitionKeys.size() == 1 ? null : i;\n                partitions.add(new ColumnDefinition(keyspace, table, p.left, p.right, componentIndex, ColumnDefinition.Kind.PARTITION_KEY));\n            }\n\n            for (int i = 0; i < clusteringColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = clusteringColumns.get(i);\n                clusterings.add(new ColumnDefinition(keyspace, table, p.left, p.right, i, ColumnDefinition.Kind.CLUSTERING_COLUMN));\n            }\n\n            for (int i = 0; i < regularColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = regularColumns.get(i);\n                builder.add(new ColumnDefinition(keyspace, table, p.left, p.right, null, ColumnDefinition.Kind.REGULAR));\n            }\n\n            for (int i = 0; i < staticColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = staticColumns.get(i);\n                builder.add(new ColumnDefinition(keyspace, table, p.left, p.right, null, ColumnDefinition.Kind.STATIC));\n            }\n\n            return new CFMetaData(keyspace,\n                                  table,\n                                  tableId,\n                                  isSuper,\n                                  isCounter,\n                                  isDense,\n                                  isCompound,\n                                  partitions,\n                                  clusterings,\n                                  builder.build());\n        }",
            "1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485 +\n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  ",
            "        public CFMetaData build()\n        {\n            if (tableId == null)\n                tableId = UUIDGen.getTimeUUID();\n\n            List<ColumnDefinition> partitions = new ArrayList<>(partitionKeys.size());\n            List<ColumnDefinition> clusterings = new ArrayList<>(clusteringColumns.size());\n            PartitionColumns.Builder builder = PartitionColumns.builder();\n\n            for (int i = 0; i < partitionKeys.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = partitionKeys.get(i);\n                Integer componentIndex = partitionKeys.size() == 1 ? null : i;\n                partitions.add(new ColumnDefinition(keyspace, table, p.left, p.right, componentIndex, ColumnDefinition.Kind.PARTITION_KEY));\n            }\n\n            for (int i = 0; i < clusteringColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = clusteringColumns.get(i);\n                clusterings.add(new ColumnDefinition(keyspace, table, p.left, p.right, i, ColumnDefinition.Kind.CLUSTERING));\n            }\n\n            for (int i = 0; i < regularColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = regularColumns.get(i);\n                builder.add(new ColumnDefinition(keyspace, table, p.left, p.right, null, ColumnDefinition.Kind.REGULAR));\n            }\n\n            for (int i = 0; i < staticColumns.size(); i++)\n            {\n                Pair<ColumnIdentifier, AbstractType> p = staticColumns.get(i);\n                builder.add(new ColumnDefinition(keyspace, table, p.left, p.right, null, ColumnDefinition.Kind.STATIC));\n            }\n\n            return new CFMetaData(keyspace,\n                                  table,\n                                  tableId,\n                                  isSuper,\n                                  isCounter,\n                                  isDense,\n                                  isCompound,\n                                  partitions,\n                                  clusterings,\n                                  builder.build());\n        }"
        ],
        [
            "LegacySchemaMigratorTest::serializeKind(ColumnDefinition,boolean)",
            " 443  \n 444  \n 445 -\n 446 -\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    private static String serializeKind(ColumnDefinition.Kind kind, boolean isDense)\n    {\n        // For backward compatibility, we special case CLUSTERING_COLUMN and the case where the table is dense.\n        if (kind == ColumnDefinition.Kind.CLUSTERING_COLUMN)\n            return \"clustering_key\";\n\n        if (kind == ColumnDefinition.Kind.REGULAR && isDense)\n            return \"compact_value\";\n\n        return kind.toString().toLowerCase();\n    }",
            " 443  \n 444  \n 445 +\n 446 +\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    private static String serializeKind(ColumnDefinition.Kind kind, boolean isDense)\n    {\n        // For backward compatibility, we special case CLUSTERING and the case where the table is dense.\n        if (kind == ColumnDefinition.Kind.CLUSTERING)\n            return \"clustering_key\";\n\n        if (kind == ColumnDefinition.Kind.REGULAR && isDense)\n            return \"compact_value\";\n\n        return kind.toString().toLowerCase();\n    }"
        ],
        [
            "ColumnDefinition::isPrimaryKeyKind()",
            "  53  \n  54  \n  55 -\n  56  ",
            "        public boolean isPrimaryKeyKind()\n        {\n            return this == PARTITION_KEY || this == CLUSTERING_COLUMN;\n        }",
            "  52  \n  53  \n  54 +\n  55  ",
            "        public boolean isPrimaryKeyKind()\n        {\n            return this == PARTITION_KEY || this == CLUSTERING;\n        }"
        ]
    ],
    "649a106c39b6a166c988fb647eaa33341e7371c4": [
        [
            "SerializationHeader::timestampSerializedSize(long)",
            " 208  \n 209  \n 210 -\n 211  ",
            "    public long timestampSerializedSize(long timestamp)\n    {\n        return TypeSizes.sizeofVInt(timestamp - stats.minTimestamp);\n    }",
            " 208  \n 209  \n 210 +\n 211  ",
            "    public long timestampSerializedSize(long timestamp)\n    {\n        return TypeSizes.sizeofUnsignedVInt(timestamp - stats.minTimestamp);\n    }"
        ],
        [
            "SerializationHeader::readLocalDeletionTime(DataInputPlus)",
            " 191  \n 192  \n 193 -\n 194  ",
            "    public int readLocalDeletionTime(DataInputPlus in) throws IOException\n    {\n        return (int)in.readVInt() + stats.minLocalDeletionTime;\n    }",
            " 191  \n 192  \n 193 +\n 194  ",
            "    public int readLocalDeletionTime(DataInputPlus in) throws IOException\n    {\n        return (int)in.readUnsignedVInt() + stats.minLocalDeletionTime;\n    }"
        ],
        [
            "HintMessage::Serializer::serializedSize(HintMessage,int)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  \n  93  ",
            "        public long serializedSize(HintMessage message, int version)\n        {\n            long size = UUIDSerializer.serializer.serializedSize(message.hostId, version);\n\n            long hintSize = Hint.serializer.serializedSize(message.hint, version);\n            size += TypeSizes.sizeofVInt(hintSize);\n            size += hintSize;\n\n            return size;\n        }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  ",
            "        public long serializedSize(HintMessage message, int version)\n        {\n            long size = UUIDSerializer.serializer.serializedSize(message.hostId, version);\n\n            long hintSize = Hint.serializer.serializedSize(message.hint, version);\n            size += TypeSizes.sizeofUnsignedVInt(hintSize);\n            size += hintSize;\n\n            return size;\n        }"
        ],
        [
            "SerializationHeader::Serializer::deserialize(Version,DataInputPlus)",
            " 430  \n 431  \n 432  \n 433  \n 434  \n 435 -\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  ",
            "        public Component deserialize(Version version, DataInputPlus in) throws IOException\n        {\n            EncodingStats stats = EncodingStats.serializer.deserialize(in);\n\n            AbstractType<?> keyType = readType(in);\n            int size = (int)in.readVInt();\n            List<AbstractType<?>> clusteringTypes = new ArrayList<>(size);\n            for (int i = 0; i < size; i++)\n                clusteringTypes.add(readType(in));\n\n            Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();\n            Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();\n\n            readColumnsWithType(in, staticColumns);\n            readColumnsWithType(in, regularColumns);\n\n            return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);\n        }",
            " 430  \n 431  \n 432  \n 433  \n 434  \n 435 +\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  ",
            "        public Component deserialize(Version version, DataInputPlus in) throws IOException\n        {\n            EncodingStats stats = EncodingStats.serializer.deserialize(in);\n\n            AbstractType<?> keyType = readType(in);\n            int size = (int)in.readUnsignedVInt();\n            List<AbstractType<?>> clusteringTypes = new ArrayList<>(size);\n            for (int i = 0; i < size; i++)\n                clusteringTypes.add(readType(in));\n\n            Map<ByteBuffer, AbstractType<?>> staticColumns = new LinkedHashMap<>();\n            Map<ByteBuffer, AbstractType<?>> regularColumns = new LinkedHashMap<>();\n\n            readColumnsWithType(in, staticColumns);\n            readColumnsWithType(in, regularColumns);\n\n            return new Component(keyType, clusteringTypes, staticColumns, regularColumns, stats);\n        }"
        ],
        [
            "EncodedHintMessage::Serializer::serialize(EncodedHintMessage,DataOutputPlus,int)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  ",
            "        public void serialize(EncodedHintMessage message, DataOutputPlus out, int version) throws IOException\n        {\n            if (version != message.version)\n                throw new IllegalArgumentException(\"serialize() called with non-matching version \" + version);\n\n            UUIDSerializer.serializer.serialize(message.hostId, out, version);\n            out.writeVInt(message.hint.remaining());\n            out.write(message.hint);\n        }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81  \n  82  ",
            "        public void serialize(EncodedHintMessage message, DataOutputPlus out, int version) throws IOException\n        {\n            if (version != message.version)\n                throw new IllegalArgumentException(\"serialize() called with non-matching version \" + version);\n\n            UUIDSerializer.serializer.serialize(message.hostId, out, version);\n            out.writeUnsignedVInt(message.hint.remaining());\n            out.write(message.hint);\n        }"
        ],
        [
            "ColumnFilter::Serializer::deserialize(DataInputPlus,int,CFMetaData)",
            " 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396 -\n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  ",
            "        public ColumnFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            int header = in.readUnsignedByte();\n            boolean isFetchAll = (header & IS_FETCH_ALL_MASK) != 0;\n            boolean hasSelection = (header & HAS_SELECTION_MASK) != 0;\n            boolean hasSubSelections = (header & HAS_SUB_SELECTIONS_MASK) != 0;\n\n            PartitionColumns selection = null;\n            if (hasSelection)\n            {\n                Columns statics = Columns.serializer.deserialize(in, metadata);\n                Columns regulars = Columns.serializer.deserialize(in, metadata);\n                selection = new PartitionColumns(statics, regulars);\n            }\n\n            SortedSetMultimap<ColumnIdentifier, ColumnSubselection> subSelections = null;\n            if (hasSubSelections)\n            {\n                subSelections = TreeMultimap.create(Comparator.<ColumnIdentifier>naturalOrder(), Comparator.<ColumnSubselection>naturalOrder());\n                int size = (int)in.readVInt();\n                for (int i = 0; i < size; i++)\n                {\n                    ColumnSubselection subSel = ColumnSubselection.serializer.deserialize(in, version, metadata);\n                    subSelections.put(subSel.column().name, subSel);\n                }\n            }\n\n            return new ColumnFilter(isFetchAll, isFetchAll ? metadata : null, selection, subSelections);\n        }",
            " 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396 +\n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  ",
            "        public ColumnFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            int header = in.readUnsignedByte();\n            boolean isFetchAll = (header & IS_FETCH_ALL_MASK) != 0;\n            boolean hasSelection = (header & HAS_SELECTION_MASK) != 0;\n            boolean hasSubSelections = (header & HAS_SUB_SELECTIONS_MASK) != 0;\n\n            PartitionColumns selection = null;\n            if (hasSelection)\n            {\n                Columns statics = Columns.serializer.deserialize(in, metadata);\n                Columns regulars = Columns.serializer.deserialize(in, metadata);\n                selection = new PartitionColumns(statics, regulars);\n            }\n\n            SortedSetMultimap<ColumnIdentifier, ColumnSubselection> subSelections = null;\n            if (hasSubSelections)\n            {\n                subSelections = TreeMultimap.create(Comparator.<ColumnIdentifier>naturalOrder(), Comparator.<ColumnSubselection>naturalOrder());\n                int size = (int)in.readUnsignedVInt();\n                for (int i = 0; i < size; i++)\n                {\n                    ColumnSubselection subSel = ColumnSubselection.serializer.deserialize(in, version, metadata);\n                    subSelections.put(subSel.column().name, subSel);\n                }\n            }\n\n            return new ColumnFilter(isFetchAll, isFetchAll ? metadata : null, selection, subSelections);\n        }"
        ],
        [
            "HintMessage::Serializer::deserialize(DataInputPlus,int)",
            " 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "        public HintMessage deserialize(DataInputPlus in, int version) throws IOException\n        {\n            UUID hostId = UUIDSerializer.serializer.deserialize(in, version);\n\n            long hintSize = in.readVInt();\n            BytesReadTracker countingIn = new BytesReadTracker(in);\n            try\n            {\n                return new HintMessage(hostId, Hint.serializer.deserialize(countingIn, version));\n            }\n            catch (UnknownColumnFamilyException e)\n            {\n                in.skipBytes(Ints.checkedCast(hintSize - countingIn.getBytesRead()));\n                return new HintMessage(hostId, e.cfId);\n            }\n        }",
            " 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "        public HintMessage deserialize(DataInputPlus in, int version) throws IOException\n        {\n            UUID hostId = UUIDSerializer.serializer.deserialize(in, version);\n\n            long hintSize = in.readUnsignedVInt();\n            BytesReadTracker countingIn = new BytesReadTracker(in);\n            try\n            {\n                return new HintMessage(hostId, Hint.serializer.deserialize(countingIn, version));\n            }\n            catch (UnknownColumnFamilyException e)\n            {\n                in.skipBytes(Ints.checkedCast(hintSize - countingIn.getBytesRead()));\n                return new HintMessage(hostId, e.cfId);\n            }\n        }"
        ],
        [
            "HintMessage::Serializer::serialize(HintMessage,DataOutputPlus,int)",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  ",
            "        public void serialize(HintMessage message, DataOutputPlus out, int version) throws IOException\n        {\n            Objects.requireNonNull(message.hint); // we should never *send* a HintMessage with null hint\n\n            UUIDSerializer.serializer.serialize(message.hostId, out, version);\n\n            /*\n             * We are serializing the hint size so that the receiver of the message could gracefully handle\n             * deserialize failure when a table had been dropped, by simply skipping the unread bytes.\n             */\n            out.writeVInt(Hint.serializer.serializedSize(message.hint, version));\n\n            Hint.serializer.serialize(message.hint, out, version);\n        }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  ",
            "        public void serialize(HintMessage message, DataOutputPlus out, int version) throws IOException\n        {\n            Objects.requireNonNull(message.hint); // we should never *send* a HintMessage with null hint\n\n            UUIDSerializer.serializer.serialize(message.hostId, out, version);\n\n            /*\n             * We are serializing the hint size so that the receiver of the message could gracefully handle\n             * deserialize failure when a table had been dropped, by simply skipping the unread bytes.\n             */\n            out.writeUnsignedVInt(Hint.serializer.serializedSize(message.hint, version));\n\n            Hint.serializer.serialize(message.hint, out, version);\n        }"
        ],
        [
            "DataLimits::Serializer::deserialize(DataInputPlus,int)",
            " 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 -\n 674 -\n 675  \n 676  \n 677  \n 678  \n 679  \n 680 -\n 681  \n 682  \n 683  \n 684 -\n 685 -\n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "        public DataLimits deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Kind kind = Kind.values()[in.readUnsignedByte()];\n            switch (kind)\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    int rowLimit = (int)in.readVInt();\n                    int perPartitionLimit = (int)in.readVInt();\n                    boolean isDistinct = in.readBoolean();\n                    if (kind == Kind.CQL_LIMIT)\n                        return new CQLLimits(rowLimit, perPartitionLimit, isDistinct);\n\n                    ByteBuffer lastKey = ByteBufferUtil.readWithVIntLength(in);\n                    int lastRemaining = (int)in.readVInt();\n                    return new CQLPagingLimits(rowLimit, perPartitionLimit, isDistinct, lastKey, lastRemaining);\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    int partitionLimit = (int)in.readVInt();\n                    int cellPerPartitionLimit = (int)in.readVInt();\n                    return kind == Kind.THRIFT_LIMIT\n                         ? new ThriftLimits(partitionLimit, cellPerPartitionLimit)\n                         : new SuperColumnCountingLimits(partitionLimit, cellPerPartitionLimit);\n            }\n            throw new AssertionError();\n        }",
            " 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 +\n 674 +\n 675  \n 676  \n 677  \n 678  \n 679  \n 680 +\n 681  \n 682  \n 683  \n 684 +\n 685 +\n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "        public DataLimits deserialize(DataInputPlus in, int version) throws IOException\n        {\n            Kind kind = Kind.values()[in.readUnsignedByte()];\n            switch (kind)\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    int rowLimit = (int)in.readUnsignedVInt();\n                    int perPartitionLimit = (int)in.readUnsignedVInt();\n                    boolean isDistinct = in.readBoolean();\n                    if (kind == Kind.CQL_LIMIT)\n                        return new CQLLimits(rowLimit, perPartitionLimit, isDistinct);\n\n                    ByteBuffer lastKey = ByteBufferUtil.readWithVIntLength(in);\n                    int lastRemaining = (int)in.readUnsignedVInt();\n                    return new CQLPagingLimits(rowLimit, perPartitionLimit, isDistinct, lastKey, lastRemaining);\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    int partitionLimit = (int)in.readUnsignedVInt();\n                    int cellPerPartitionLimit = (int)in.readUnsignedVInt();\n                    return kind == Kind.THRIFT_LIMIT\n                         ? new ThriftLimits(partitionLimit, cellPerPartitionLimit)\n                         : new SuperColumnCountingLimits(partitionLimit, cellPerPartitionLimit);\n            }\n            throw new AssertionError();\n        }"
        ],
        [
            "SerializationHeader::writeTTL(int,DataOutputPlus)",
            " 175  \n 176  \n 177 -\n 178  ",
            "    public void writeTTL(int ttl, DataOutputPlus out) throws IOException\n    {\n        out.writeVInt(ttl - stats.minTTL);\n    }",
            " 175  \n 176  \n 177 +\n 178  ",
            "    public void writeTTL(int ttl, DataOutputPlus out) throws IOException\n    {\n        out.writeUnsignedVInt(ttl - stats.minTTL);\n    }"
        ],
        [
            "ColumnFilter::Serializer::serialize(ColumnFilter,DataOutputPlus,int)",
            " 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 -\n 372  \n 373  \n 374  \n 375  ",
            "        public void serialize(ColumnFilter selection, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeByte(makeHeaderByte(selection));\n\n            if (selection.selection != null)\n            {\n                Columns.serializer.serialize(selection.selection.statics, out);\n                Columns.serializer.serialize(selection.selection.regulars, out);\n            }\n\n            if (selection.subSelections != null)\n            {\n                out.writeVInt(selection.subSelections.size());\n                for (ColumnSubselection subSel : selection.subSelections.values())\n                    ColumnSubselection.serializer.serialize(subSel, out, version);\n            }\n        }",
            " 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 +\n 372  \n 373  \n 374  \n 375  ",
            "        public void serialize(ColumnFilter selection, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeByte(makeHeaderByte(selection));\n\n            if (selection.selection != null)\n            {\n                Columns.serializer.serialize(selection.selection.statics, out);\n                Columns.serializer.serialize(selection.selection.regulars, out);\n            }\n\n            if (selection.subSelections != null)\n            {\n                out.writeUnsignedVInt(selection.subSelections.size());\n                for (ColumnSubselection subSel : selection.subSelections.values())\n                    ColumnSubselection.serializer.serialize(subSel, out, version);\n            }\n        }"
        ],
        [
            "Mutation::MutationSerializer::serialize(Mutation,DataOutputPlus,int)",
            " 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  ",
            "        public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException\n        {\n            if (version < MessagingService.VERSION_20)\n                out.writeUTF(mutation.getKeyspaceName());\n\n            /* serialize the modifications in the mutation */\n            int size = mutation.modifications.size();\n\n            if (version < MessagingService.VERSION_30)\n            {\n                ByteBufferUtil.writeWithShortLength(mutation.key().getKey(), out);\n                out.writeInt(size);\n            }\n            else\n            {\n                out.writeVInt(size);\n            }\n\n            assert size > 0;\n            for (Map.Entry<UUID, PartitionUpdate> entry : mutation.modifications.entrySet())\n                PartitionUpdate.serializer.serialize(entry.getValue(), out, version);\n        }",
            " 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  ",
            "        public void serialize(Mutation mutation, DataOutputPlus out, int version) throws IOException\n        {\n            if (version < MessagingService.VERSION_20)\n                out.writeUTF(mutation.getKeyspaceName());\n\n            /* serialize the modifications in the mutation */\n            int size = mutation.modifications.size();\n\n            if (version < MessagingService.VERSION_30)\n            {\n                ByteBufferUtil.writeWithShortLength(mutation.key().getKey(), out);\n                out.writeInt(size);\n            }\n            else\n            {\n                out.writeUnsignedVInt(size);\n            }\n\n            assert size > 0;\n            for (Map.Entry<UUID, PartitionUpdate> entry : mutation.modifications.entrySet())\n                PartitionUpdate.serializer.serialize(entry.getValue(), out, version);\n        }"
        ],
        [
            "SerializationHeader::skipTTL(DataInputPlus)",
            " 239  \n 240  \n 241 -\n 242  ",
            "    public void skipTTL(DataInputPlus in) throws IOException\n    {\n        in.readVInt();\n    }",
            " 239  \n 240  \n 241 +\n 242  ",
            "    public void skipTTL(DataInputPlus in) throws IOException\n    {\n        in.readUnsignedVInt();\n    }"
        ],
        [
            "Batch::Serializer::serialize(Batch,DataOutputPlus,int)",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 -\n 109  \n 110  \n 111 -\n 112  \n 113  \n 114  ",
            "        public void serialize(Batch batch, DataOutputPlus out, int version) throws IOException\n        {\n            assert batch.encodedMutations.isEmpty() : \"attempted to serialize a 'remote' batch\";\n\n            UUIDSerializer.serializer.serialize(batch.id, out, version);\n            out.writeLong(batch.creationTime);\n\n            out.writeVInt(batch.decodedMutations.size());\n            for (Mutation mutation : batch.decodedMutations)\n            {\n                out.writeVInt(Mutation.serializer.serializedSize(mutation, version));\n                Mutation.serializer.serialize(mutation, out, version);\n            }\n        }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111 +\n 112  \n 113  \n 114  ",
            "        public void serialize(Batch batch, DataOutputPlus out, int version) throws IOException\n        {\n            assert batch.encodedMutations.isEmpty() : \"attempted to serialize a 'remote' batch\";\n\n            UUIDSerializer.serializer.serialize(batch.id, out, version);\n            out.writeLong(batch.creationTime);\n\n            out.writeUnsignedVInt(batch.decodedMutations.size());\n            for (Mutation mutation : batch.decodedMutations)\n            {\n                out.writeUnsignedVInt(Mutation.serializer.serializedSize(mutation, version));\n                Mutation.serializer.serialize(mutation, out, version);\n            }\n        }"
        ],
        [
            "Slices::Serializer::serialize(Slices,DataOutputPlus,int)",
            " 297  \n 298  \n 299  \n 300 -\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "        public void serialize(Slices slices, DataOutputPlus out, int version) throws IOException\n        {\n            int size = slices.size();\n            out.writeVInt(size);\n\n            if (size == 0)\n                return;\n\n            List<AbstractType<?>> types = slices == ALL\n                                        ? Collections.<AbstractType<?>>emptyList()\n                                        : ((ArrayBackedSlices)slices).comparator.subtypes();\n\n            for (Slice slice : slices)\n                Slice.serializer.serialize(slice, out, version, types);\n        }",
            " 297  \n 298  \n 299  \n 300 +\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "        public void serialize(Slices slices, DataOutputPlus out, int version) throws IOException\n        {\n            int size = slices.size();\n            out.writeUnsignedVInt(size);\n\n            if (size == 0)\n                return;\n\n            List<AbstractType<?>> types = slices == ALL\n                                        ? Collections.<AbstractType<?>>emptyList()\n                                        : ((ArrayBackedSlices)slices).comparator.subtypes();\n\n            for (Slice slice : slices)\n                Slice.serializer.serialize(slice, out, version, types);\n        }"
        ],
        [
            "Batch::Serializer::decodeMutations(DataInputPlus,int)",
            " 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  ",
            "        private static Collection<Mutation> decodeMutations(DataInputPlus in, int version) throws IOException\n        {\n            int count = (int) in.readVInt();\n\n            ArrayList<Mutation> mutations = new ArrayList<>(count);\n            for (int i = 0; i < count; i++)\n            {\n                in.readVInt(); // skip mutation size\n                mutations.add(Mutation.serializer.deserialize(in, version));\n            }\n\n            return mutations;\n        }",
            " 141  \n 142  \n 143 +\n 144  \n 145  \n 146  \n 147  \n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  ",
            "        private static Collection<Mutation> decodeMutations(DataInputPlus in, int version) throws IOException\n        {\n            int count = (int) in.readUnsignedVInt();\n\n            ArrayList<Mutation> mutations = new ArrayList<>(count);\n            for (int i = 0; i < count; i++)\n            {\n                in.readUnsignedVInt(); // skip mutation size\n                mutations.add(Mutation.serializer.deserialize(in, version));\n            }\n\n            return mutations;\n        }"
        ],
        [
            "RowFilter::Serializer::serialize(RowFilter,DataOutputPlus,int)",
            " 748  \n 749  \n 750  \n 751 -\n 752  \n 753  \n 754  ",
            "        public void serialize(RowFilter filter, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeBoolean(filter instanceof ThriftFilter);\n            out.writeVInt(filter.expressions.size());\n            for (Expression expr : filter.expressions)\n                Expression.serializer.serialize(expr, out, version);\n        }",
            " 748  \n 749  \n 750  \n 751 +\n 752  \n 753  \n 754  ",
            "        public void serialize(RowFilter filter, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeBoolean(filter instanceof ThriftFilter);\n            out.writeUnsignedVInt(filter.expressions.size());\n            for (Expression expr : filter.expressions)\n                Expression.serializer.serialize(expr, out, version);\n        }"
        ],
        [
            "SerializationHeader::ttlSerializedSize(int)",
            " 218  \n 219  \n 220 -\n 221  ",
            "    public long ttlSerializedSize(int ttl)\n    {\n        return TypeSizes.sizeofVInt(ttl - stats.minTTL);\n    }",
            " 218  \n 219  \n 220 +\n 221  ",
            "    public long ttlSerializedSize(int ttl)\n    {\n        return TypeSizes.sizeofUnsignedVInt(ttl - stats.minTTL);\n    }"
        ],
        [
            "Mutation::MutationSerializer::deserialize(DataInputPlus,int,SerializationHelper)",
            " 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 -\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "        public Mutation deserialize(DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException\n        {\n            if (version < MessagingService.VERSION_20)\n                in.readUTF(); // read pre-2.0 keyspace name\n\n            ByteBuffer key = null;\n            int size;\n            if (version < MessagingService.VERSION_30)\n            {\n                key = ByteBufferUtil.readWithShortLength(in);\n                size = in.readInt();\n            }\n            else\n            {\n                size = (int)in.readVInt();\n            }\n\n            assert size > 0;\n\n            PartitionUpdate update = PartitionUpdate.serializer.deserialize(in, version, flag, key);\n            if (size == 1)\n                return new Mutation(update);\n\n            Map<UUID, PartitionUpdate> modifications = new HashMap<>(size);\n            DecoratedKey dk = update.partitionKey();\n\n            modifications.put(update.metadata().cfId, update);\n            for (int i = 1; i < size; ++i)\n            {\n                update = PartitionUpdate.serializer.deserialize(in, version, flag, dk);\n                modifications.put(update.metadata().cfId, update);\n            }\n\n            return new Mutation(update.metadata().ksName, dk, modifications);\n        }",
            " 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "        public Mutation deserialize(DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException\n        {\n            if (version < MessagingService.VERSION_20)\n                in.readUTF(); // read pre-2.0 keyspace name\n\n            ByteBuffer key = null;\n            int size;\n            if (version < MessagingService.VERSION_30)\n            {\n                key = ByteBufferUtil.readWithShortLength(in);\n                size = in.readInt();\n            }\n            else\n            {\n                size = (int)in.readUnsignedVInt();\n            }\n\n            assert size > 0;\n\n            PartitionUpdate update = PartitionUpdate.serializer.deserialize(in, version, flag, key);\n            if (size == 1)\n                return new Mutation(update);\n\n            Map<UUID, PartitionUpdate> modifications = new HashMap<>(size);\n            DecoratedKey dk = update.partitionKey();\n\n            modifications.put(update.metadata().cfId, update);\n            for (int i = 1; i < size; ++i)\n            {\n                update = PartitionUpdate.serializer.deserialize(in, version, flag, dk);\n                modifications.put(update.metadata().cfId, update);\n            }\n\n            return new Mutation(update.metadata().ksName, dk, modifications);\n        }"
        ],
        [
            "UnfilteredRowIteratorSerializer::deserializeHeader(CFMetaData,ColumnFilter,DataInputPlus,int,SerializationHelper)",
            " 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  \n 196  ",
            "    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException\n    {\n        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));\n        int flags = in.readUnsignedByte();\n        boolean isReversed = (flags & IS_REVERSED) != 0;\n        if ((flags & IS_EMPTY) != 0)\n        {\n            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);\n            return new Header(sh, key, isReversed, true, null, null, 0);\n        }\n\n        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;\n        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;\n        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;\n\n        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);\n\n        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;\n\n        Row staticRow = Rows.EMPTY_STATIC_ROW;\n        if (hasStatic)\n            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));\n\n        int rowEstimate = hasRowEstimate ? (int)in.readVInt() : -1;\n        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);\n    }",
            " 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  \n 196  ",
            "    public Header deserializeHeader(CFMetaData metadata, ColumnFilter selection, DataInputPlus in, int version, SerializationHelper.Flag flag) throws IOException\n    {\n        DecoratedKey key = metadata.decorateKey(ByteBufferUtil.readWithVIntLength(in));\n        int flags = in.readUnsignedByte();\n        boolean isReversed = (flags & IS_REVERSED) != 0;\n        if ((flags & IS_EMPTY) != 0)\n        {\n            SerializationHeader sh = new SerializationHeader(metadata, PartitionColumns.NONE, EncodingStats.NO_STATS);\n            return new Header(sh, key, isReversed, true, null, null, 0);\n        }\n\n        boolean hasPartitionDeletion = (flags & HAS_PARTITION_DELETION) != 0;\n        boolean hasStatic = (flags & HAS_STATIC_ROW) != 0;\n        boolean hasRowEstimate = (flags & HAS_ROW_ESTIMATE) != 0;\n\n        SerializationHeader header = SerializationHeader.serializer.deserializeForMessaging(in, metadata, selection, hasStatic);\n\n        DeletionTime partitionDeletion = hasPartitionDeletion ? header.readDeletionTime(in) : DeletionTime.LIVE;\n\n        Row staticRow = Rows.EMPTY_STATIC_ROW;\n        if (hasStatic)\n            staticRow = UnfilteredSerializer.serializer.deserializeStaticRow(in, header, new SerializationHelper(metadata, version, flag));\n\n        int rowEstimate = hasRowEstimate ? (int)in.readUnsignedVInt() : -1;\n        return new Header(header, key, isReversed, false, partitionDeletion, staticRow, rowEstimate);\n    }"
        ],
        [
            "Mutation::MutationSerializer::serializedSize(Mutation,int)",
            " 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 -\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  ",
            "        public long serializedSize(Mutation mutation, int version)\n        {\n            int size = 0;\n\n            if (version < MessagingService.VERSION_20)\n                size += TypeSizes.sizeof(mutation.getKeyspaceName());\n\n            if (version < MessagingService.VERSION_30)\n            {\n                int keySize = mutation.key().getKey().remaining();\n                size += TypeSizes.sizeof((short) keySize) + keySize;\n                size += TypeSizes.sizeof(mutation.modifications.size());\n            }\n            else\n            {\n                size += TypeSizes.sizeofVInt(mutation.modifications.size());\n            }\n\n            for (Map.Entry<UUID, PartitionUpdate> entry : mutation.modifications.entrySet())\n                size += PartitionUpdate.serializer.serializedSize(entry.getValue(), version);\n\n            return size;\n        }",
            " 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 +\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  ",
            "        public long serializedSize(Mutation mutation, int version)\n        {\n            int size = 0;\n\n            if (version < MessagingService.VERSION_20)\n                size += TypeSizes.sizeof(mutation.getKeyspaceName());\n\n            if (version < MessagingService.VERSION_30)\n            {\n                int keySize = mutation.key().getKey().remaining();\n                size += TypeSizes.sizeof((short) keySize) + keySize;\n                size += TypeSizes.sizeof(mutation.modifications.size());\n            }\n            else\n            {\n                size += TypeSizes.sizeofUnsignedVInt(mutation.modifications.size());\n            }\n\n            for (Map.Entry<UUID, PartitionUpdate> entry : mutation.modifications.entrySet())\n                size += PartitionUpdate.serializer.serializedSize(entry.getValue(), version);\n\n            return size;\n        }"
        ],
        [
            "SerializationHeader::Serializer::writeColumnsWithTypes(Map,DataOutputPlus)",
            " 464  \n 465  \n 466 -\n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "        private void writeColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns, DataOutputPlus out) throws IOException\n        {\n            out.writeVInt(columns.size());\n            for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())\n            {\n                ByteBufferUtil.writeWithVIntLength(entry.getKey(), out);\n                writeType(entry.getValue(), out);\n            }\n        }",
            " 464  \n 465  \n 466 +\n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "        private void writeColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns, DataOutputPlus out) throws IOException\n        {\n            out.writeUnsignedVInt(columns.size());\n            for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())\n            {\n                ByteBufferUtil.writeWithVIntLength(entry.getKey(), out);\n                writeType(entry.getValue(), out);\n            }\n        }"
        ],
        [
            "UnfilteredRowIteratorSerializer::serialize(UnfilteredRowIterator,SerializationHeader,ColumnFilter,DataOutputPlus,int,int)",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  ",
            "    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException\n    {\n        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);\n\n        int flags = 0;\n        if (iterator.isReverseOrder())\n            flags |= IS_REVERSED;\n\n        if (iterator.isEmpty())\n        {\n            out.writeByte((byte)(flags | IS_EMPTY));\n            return;\n        }\n\n        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();\n        if (!partitionDeletion.isLive())\n            flags |= HAS_PARTITION_DELETION;\n        Row staticRow = iterator.staticRow();\n        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;\n        if (hasStatic)\n            flags |= HAS_STATIC_ROW;\n\n        if (rowEstimate >= 0)\n            flags |= HAS_ROW_ESTIMATE;\n\n        out.writeByte((byte)flags);\n\n        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);\n\n        if (!partitionDeletion.isLive())\n            header.writeDeletionTime(partitionDeletion, out);\n\n        if (hasStatic)\n            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);\n\n        if (rowEstimate >= 0)\n            out.writeVInt(rowEstimate);\n\n        while (iterator.hasNext())\n            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);\n        UnfilteredSerializer.serializer.writeEndOfPartition(out);\n    }",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  ",
            "    public void serialize(UnfilteredRowIterator iterator, SerializationHeader header, ColumnFilter selection, DataOutputPlus out, int version, int rowEstimate) throws IOException\n    {\n        ByteBufferUtil.writeWithVIntLength(iterator.partitionKey().getKey(), out);\n\n        int flags = 0;\n        if (iterator.isReverseOrder())\n            flags |= IS_REVERSED;\n\n        if (iterator.isEmpty())\n        {\n            out.writeByte((byte)(flags | IS_EMPTY));\n            return;\n        }\n\n        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();\n        if (!partitionDeletion.isLive())\n            flags |= HAS_PARTITION_DELETION;\n        Row staticRow = iterator.staticRow();\n        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;\n        if (hasStatic)\n            flags |= HAS_STATIC_ROW;\n\n        if (rowEstimate >= 0)\n            flags |= HAS_ROW_ESTIMATE;\n\n        out.writeByte((byte)flags);\n\n        SerializationHeader.serializer.serializeForMessaging(header, selection, out, hasStatic);\n\n        if (!partitionDeletion.isLive())\n            header.writeDeletionTime(partitionDeletion, out);\n\n        if (hasStatic)\n            UnfilteredSerializer.serializer.serialize(staticRow, header, out, version);\n\n        if (rowEstimate >= 0)\n            out.writeUnsignedVInt(rowEstimate);\n\n        while (iterator.hasNext())\n            UnfilteredSerializer.serializer.serialize(iterator.next(), header, out, version);\n        UnfilteredSerializer.serializer.writeEndOfPartition(out);\n    }"
        ],
        [
            "SerializationHeader::localDeletionTimeSerializedSize(int)",
            " 213  \n 214  \n 215 -\n 216  ",
            "    public long localDeletionTimeSerializedSize(int localDeletionTime)\n    {\n        return TypeSizes.sizeofVInt(localDeletionTime - stats.minLocalDeletionTime);\n    }",
            " 213  \n 214  \n 215 +\n 216  ",
            "    public long localDeletionTimeSerializedSize(int localDeletionTime)\n    {\n        return TypeSizes.sizeofUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);\n    }"
        ],
        [
            "Columns::Serializer::deserialize(DataInputPlus,CFMetaData)",
            " 416  \n 417  \n 418 -\n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  ",
            "        public Columns deserialize(DataInputPlus in, CFMetaData metadata) throws IOException\n        {\n            int length = (int)in.readVInt();\n            BTree.Builder<ColumnDefinition> builder = BTree.builder(Comparator.naturalOrder());\n            builder.auto(false);\n            for (int i = 0; i < length; i++)\n            {\n                ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);\n                ColumnDefinition column = metadata.getColumnDefinition(name);\n                if (column == null)\n                {\n                    // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't\n                    // fail deserialization because of that. So we grab a \"fake\" ColumnDefinition that ensure proper\n                    // deserialization. The column will be ignore later on anyway.\n                    column = metadata.getDroppedColumnDefinition(name);\n                    if (column == null)\n                        throw new RuntimeException(\"Unknown column \" + UTF8Type.instance.getString(name) + \" during deserialization\");\n                }\n                builder.add(column);\n            }\n            return new Columns(builder.build());\n        }",
            " 416  \n 417  \n 418 +\n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  ",
            "        public Columns deserialize(DataInputPlus in, CFMetaData metadata) throws IOException\n        {\n            int length = (int)in.readUnsignedVInt();\n            BTree.Builder<ColumnDefinition> builder = BTree.builder(Comparator.naturalOrder());\n            builder.auto(false);\n            for (int i = 0; i < length; i++)\n            {\n                ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);\n                ColumnDefinition column = metadata.getColumnDefinition(name);\n                if (column == null)\n                {\n                    // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't\n                    // fail deserialization because of that. So we grab a \"fake\" ColumnDefinition that ensure proper\n                    // deserialization. The column will be ignore later on anyway.\n                    column = metadata.getDroppedColumnDefinition(name);\n                    if (column == null)\n                        throw new RuntimeException(\"Unknown column \" + UTF8Type.instance.getString(name) + \" during deserialization\");\n                }\n                builder.add(column);\n            }\n            return new Columns(builder.build());\n        }"
        ],
        [
            "Hint::Serializer::serializedSize(Hint,int)",
            " 111  \n 112  \n 113  \n 114 -\n 115  \n 116  \n 117  ",
            "        public long serializedSize(Hint hint, int version)\n        {\n            long size = sizeof(hint.creationTime);\n            size += sizeofVInt(hint.gcgs);\n            size += Mutation.serializer.serializedSize(hint.mutation, version);\n            return size;\n        }",
            " 111  \n 112  \n 113  \n 114 +\n 115  \n 116  \n 117  ",
            "        public long serializedSize(Hint hint, int version)\n        {\n            long size = sizeof(hint.creationTime);\n            size += sizeofUnsignedVInt(hint.gcgs);\n            size += Mutation.serializer.serializedSize(hint.mutation, version);\n            return size;\n        }"
        ],
        [
            "ByteBufferUtil::writeWithVIntLength(ByteBuffer,DataOutputPlus)",
            " 294  \n 295  \n 296 -\n 297  \n 298  ",
            "    public static void writeWithVIntLength(ByteBuffer bytes, DataOutputPlus out) throws IOException\n    {\n        out.writeVInt(bytes.remaining());\n        out.write(bytes);\n    }",
            " 294  \n 295  \n 296 +\n 297  \n 298  ",
            "    public static void writeWithVIntLength(ByteBuffer bytes, DataOutputPlus out) throws IOException\n    {\n        out.writeUnsignedVInt(bytes.remaining());\n        out.write(bytes);\n    }"
        ],
        [
            "ReadCommand::Serializer::deserialize(DataInputPlus,int)",
            " 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589 -\n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  ",
            "        public ReadCommand deserialize(DataInputPlus in, int version) throws IOException\n        {\n            if (version < MessagingService.VERSION_30)\n                return legacyReadCommandSerializer.deserialize(in, version);\n\n            Kind kind = Kind.values()[in.readByte()];\n            int flags = in.readByte();\n            boolean isDigest = isDigest(flags);\n            boolean isForThrift = isForThrift(flags);\n            boolean hasIndex = hasIndex(flags);\n            int digestVersion = isDigest ? (int)in.readVInt() : 0;\n            CFMetaData metadata = CFMetaData.serializer.deserialize(in, version);\n            int nowInSec = in.readInt();\n            ColumnFilter columnFilter = ColumnFilter.serializer.deserialize(in, version, metadata);\n            RowFilter rowFilter = RowFilter.serializer.deserialize(in, version, metadata);\n            DataLimits limits = DataLimits.serializer.deserialize(in, version);\n            Optional<IndexMetadata> index = hasIndex\n                                            ? deserializeIndexMetadata(in, version, metadata)\n                                            : Optional.empty();\n\n            return kind.selectionDeserializer.deserialize(in, version, isDigest, digestVersion, isForThrift, metadata, nowInSec, columnFilter, rowFilter, limits, index);\n        }",
            " 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589 +\n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  ",
            "        public ReadCommand deserialize(DataInputPlus in, int version) throws IOException\n        {\n            if (version < MessagingService.VERSION_30)\n                return legacyReadCommandSerializer.deserialize(in, version);\n\n            Kind kind = Kind.values()[in.readByte()];\n            int flags = in.readByte();\n            boolean isDigest = isDigest(flags);\n            boolean isForThrift = isForThrift(flags);\n            boolean hasIndex = hasIndex(flags);\n            int digestVersion = isDigest ? (int)in.readUnsignedVInt() : 0;\n            CFMetaData metadata = CFMetaData.serializer.deserialize(in, version);\n            int nowInSec = in.readInt();\n            ColumnFilter columnFilter = ColumnFilter.serializer.deserialize(in, version, metadata);\n            RowFilter rowFilter = RowFilter.serializer.deserialize(in, version, metadata);\n            DataLimits limits = DataLimits.serializer.deserialize(in, version);\n            Optional<IndexMetadata> index = hasIndex\n                                            ? deserializeIndexMetadata(in, version, metadata)\n                                            : Optional.empty();\n\n            return kind.selectionDeserializer.deserialize(in, version, isDigest, digestVersion, isForThrift, metadata, nowInSec, columnFilter, rowFilter, limits, index);\n        }"
        ],
        [
            "ReadCommand::Serializer::serializedSize(ReadCommand,int)",
            " 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626 -\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  ",
            "        public long serializedSize(ReadCommand command, int version)\n        {\n            // for serialization, createLegacyMessage() should cause legacyReadCommandSerializer to be used directly\n            assert version >= MessagingService.VERSION_30;\n\n            return 2 // kind + flags\n                 + (command.isDigestQuery() ? TypeSizes.sizeofVInt(command.digestVersion()) : 0)\n                 + CFMetaData.serializer.serializedSize(command.metadata(), version)\n                 + TypeSizes.sizeof(command.nowInSec())\n                 + ColumnFilter.serializer.serializedSize(command.columnFilter(), version)\n                 + RowFilter.serializer.serializedSize(command.rowFilter(), version)\n                 + DataLimits.serializer.serializedSize(command.limits(), version)\n                 + command.selectionSerializedSize(version)\n                 + command.indexSerializedSize(version);\n        }",
            " 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626 +\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  ",
            "        public long serializedSize(ReadCommand command, int version)\n        {\n            // for serialization, createLegacyMessage() should cause legacyReadCommandSerializer to be used directly\n            assert version >= MessagingService.VERSION_30;\n\n            return 2 // kind + flags\n                 + (command.isDigestQuery() ? TypeSizes.sizeofUnsignedVInt(command.digestVersion()) : 0)\n                 + CFMetaData.serializer.serializedSize(command.metadata(), version)\n                 + TypeSizes.sizeof(command.nowInSec())\n                 + ColumnFilter.serializer.serializedSize(command.columnFilter(), version)\n                 + RowFilter.serializer.serializedSize(command.rowFilter(), version)\n                 + DataLimits.serializer.serializedSize(command.limits(), version)\n                 + command.selectionSerializedSize(version)\n                 + command.indexSerializedSize(version);\n        }"
        ],
        [
            "EncodingStats::Serializer::serialize(EncodingStats,DataOutputPlus)",
            " 231  \n 232  \n 233 -\n 234 -\n 235 -\n 236  ",
            "        public void serialize(EncodingStats stats, DataOutputPlus out) throws IOException\n        {\n            out.writeVInt(stats.minTimestamp - TIMESTAMP_EPOCH);\n            out.writeVInt(stats.minLocalDeletionTime - DELETION_TIME_EPOCH);\n            out.writeVInt(stats.minTTL - TTL_EPOCH);\n        }",
            " 231  \n 232  \n 233 +\n 234 +\n 235 +\n 236  ",
            "        public void serialize(EncodingStats stats, DataOutputPlus out) throws IOException\n        {\n            out.writeUnsignedVInt(stats.minTimestamp - TIMESTAMP_EPOCH);\n            out.writeUnsignedVInt(stats.minLocalDeletionTime - DELETION_TIME_EPOCH);\n            out.writeUnsignedVInt(stats.minTTL - TTL_EPOCH);\n        }"
        ],
        [
            "SerializationHeader::writeTimestamp(long,DataOutputPlus)",
            " 165  \n 166  \n 167 -\n 168  ",
            "    public void writeTimestamp(long timestamp, DataOutputPlus out) throws IOException\n    {\n        out.writeVInt(timestamp - stats.minTimestamp);\n    }",
            " 165  \n 166  \n 167 +\n 168  ",
            "    public void writeTimestamp(long timestamp, DataOutputPlus out) throws IOException\n    {\n        out.writeUnsignedVInt(timestamp - stats.minTimestamp);\n    }"
        ],
        [
            "DataLimits::Serializer::serialize(DataLimits,DataOutputPlus,int)",
            " 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647 -\n 648 -\n 649  \n 650  \n 651  \n 652  \n 653  \n 654 -\n 655  \n 656  \n 657  \n 658  \n 659  \n 660 -\n 661 -\n 662  \n 663  \n 664  ",
            "        public void serialize(DataLimits limits, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeByte(limits.kind().ordinal());\n            switch (limits.kind())\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    CQLLimits cqlLimits = (CQLLimits)limits;\n                    out.writeVInt(cqlLimits.rowLimit);\n                    out.writeVInt(cqlLimits.perPartitionLimit);\n                    out.writeBoolean(cqlLimits.isDistinct);\n                    if (limits.kind() == Kind.CQL_PAGING_LIMIT)\n                    {\n                        CQLPagingLimits pagingLimits = (CQLPagingLimits)cqlLimits;\n                        ByteBufferUtil.writeWithVIntLength(pagingLimits.lastReturnedKey, out);\n                        out.writeVInt(pagingLimits.lastReturnedKeyRemaining);\n                    }\n                    break;\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    ThriftLimits thriftLimits = (ThriftLimits)limits;\n                    out.writeVInt(thriftLimits.partitionLimit);\n                    out.writeVInt(thriftLimits.cellPerPartitionLimit);\n                    break;\n            }\n        }",
            " 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647 +\n 648 +\n 649  \n 650  \n 651  \n 652  \n 653  \n 654 +\n 655  \n 656  \n 657  \n 658  \n 659  \n 660 +\n 661 +\n 662  \n 663  \n 664  ",
            "        public void serialize(DataLimits limits, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeByte(limits.kind().ordinal());\n            switch (limits.kind())\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    CQLLimits cqlLimits = (CQLLimits)limits;\n                    out.writeUnsignedVInt(cqlLimits.rowLimit);\n                    out.writeUnsignedVInt(cqlLimits.perPartitionLimit);\n                    out.writeBoolean(cqlLimits.isDistinct);\n                    if (limits.kind() == Kind.CQL_PAGING_LIMIT)\n                    {\n                        CQLPagingLimits pagingLimits = (CQLPagingLimits)cqlLimits;\n                        ByteBufferUtil.writeWithVIntLength(pagingLimits.lastReturnedKey, out);\n                        out.writeUnsignedVInt(pagingLimits.lastReturnedKeyRemaining);\n                    }\n                    break;\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    ThriftLimits thriftLimits = (ThriftLimits)limits;\n                    out.writeUnsignedVInt(thriftLimits.partitionLimit);\n                    out.writeUnsignedVInt(thriftLimits.cellPerPartitionLimit);\n                    break;\n            }\n        }"
        ],
        [
            "EncodingStats::Serializer::serializedSize(EncodingStats)",
            " 238  \n 239  \n 240 -\n 241 -\n 242 -\n 243  ",
            "        public int serializedSize(EncodingStats stats)\n        {\n            return TypeSizes.sizeofVInt(stats.minTimestamp - TIMESTAMP_EPOCH)\n                   + TypeSizes.sizeofVInt(stats.minLocalDeletionTime - DELETION_TIME_EPOCH)\n                   + TypeSizes.sizeofVInt(stats.minTTL - TTL_EPOCH);\n        }",
            " 238  \n 239  \n 240 +\n 241 +\n 242 +\n 243  ",
            "        public int serializedSize(EncodingStats stats)\n        {\n            return TypeSizes.sizeofUnsignedVInt(stats.minTimestamp - TIMESTAMP_EPOCH)\n                   + TypeSizes.sizeofUnsignedVInt(stats.minLocalDeletionTime - DELETION_TIME_EPOCH)\n                   + TypeSizes.sizeofUnsignedVInt(stats.minTTL - TTL_EPOCH);\n        }"
        ],
        [
            "RowFilter::Serializer::deserialize(DataInputPlus,int,CFMetaData)",
            " 756  \n 757  \n 758  \n 759 -\n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  ",
            "        public RowFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            boolean forThrift = in.readBoolean();\n            int size = (int)in.readVInt();\n            List<Expression> expressions = new ArrayList<>(size);\n            for (int i = 0; i < size; i++)\n                expressions.add(Expression.serializer.deserialize(in, version, metadata));\n            return forThrift\n                 ? new ThriftFilter(expressions)\n                 : new CQLFilter(expressions);\n        }",
            " 756  \n 757  \n 758  \n 759 +\n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  ",
            "        public RowFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            boolean forThrift = in.readBoolean();\n            int size = (int)in.readUnsignedVInt();\n            List<Expression> expressions = new ArrayList<>(size);\n            for (int i = 0; i < size; i++)\n                expressions.add(Expression.serializer.deserialize(in, version, metadata));\n            return forThrift\n                 ? new ThriftFilter(expressions)\n                 : new CQLFilter(expressions);\n        }"
        ],
        [
            "ByteBufferUtil::readWithVIntLength(DataInputPlus)",
            " 333  \n 334  \n 335 -\n 336  \n 337  \n 338  \n 339  \n 340  ",
            "    public static ByteBuffer readWithVIntLength(DataInputPlus in) throws IOException\n    {\n        int length = (int)in.readVInt();\n        if (length < 0)\n            throw new IOException(\"Corrupt (negative) value length encountered\");\n\n        return ByteBufferUtil.read(in, length);\n    }",
            " 333  \n 334  \n 335 +\n 336  \n 337  \n 338  \n 339  \n 340  ",
            "    public static ByteBuffer readWithVIntLength(DataInputPlus in) throws IOException\n    {\n        int length = (int)in.readUnsignedVInt();\n        if (length < 0)\n            throw new IOException(\"Corrupt (negative) value length encountered\");\n\n        return ByteBufferUtil.read(in, length);\n    }"
        ],
        [
            "ClusteringIndexNamesFilter::NamesDeserializer::deserialize(DataInputPlus,int,CFMetaData,boolean)",
            " 259  \n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  ",
            "        public ClusteringIndexFilter deserialize(DataInputPlus in, int version, CFMetaData metadata, boolean reversed) throws IOException\n        {\n            ClusteringComparator comparator = metadata.comparator;\n            BTreeSet.Builder<Clustering> clusterings = BTreeSet.builder(comparator);\n            int size = (int)in.readVInt();\n            for (int i = 0; i < size; i++)\n                clusterings.add(Clustering.serializer.deserialize(in, version, comparator.subtypes()));\n\n            return new ClusteringIndexNamesFilter(clusterings.build(), reversed);\n        }",
            " 259  \n 260  \n 261  \n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  ",
            "        public ClusteringIndexFilter deserialize(DataInputPlus in, int version, CFMetaData metadata, boolean reversed) throws IOException\n        {\n            ClusteringComparator comparator = metadata.comparator;\n            BTreeSet.Builder<Clustering> clusterings = BTreeSet.builder(comparator);\n            int size = (int)in.readUnsignedVInt();\n            for (int i = 0; i < size; i++)\n                clusterings.add(Clustering.serializer.deserialize(in, version, comparator.subtypes()));\n\n            return new ClusteringIndexNamesFilter(clusterings.build(), reversed);\n        }"
        ],
        [
            "TypeSizes::sizeofWithVIntLength(ByteBuffer)",
            "  71  \n  72  \n  73 -\n  74  ",
            "    public static int sizeofWithVIntLength(ByteBuffer value)\n    {\n        return sizeofVInt(value.remaining()) + value.remaining();\n    }",
            "  71  \n  72  \n  73 +\n  74  ",
            "    public static int sizeofWithVIntLength(ByteBuffer value)\n    {\n        return sizeofUnsignedVInt(value.remaining()) + value.remaining();\n    }"
        ],
        [
            "SerializationHeader::Serializer::sizeofColumnsWithTypes(Map)",
            " 474  \n 475  \n 476 -\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  ",
            "        private long sizeofColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns)\n        {\n            long size = TypeSizes.sizeofVInt(columns.size());\n            for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())\n            {\n                size += ByteBufferUtil.serializedSizeWithVIntLength(entry.getKey());\n                size += sizeofType(entry.getValue());\n            }\n            return size;\n        }",
            " 474  \n 475  \n 476 +\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  ",
            "        private long sizeofColumnsWithTypes(Map<ByteBuffer, AbstractType<?>> columns)\n        {\n            long size = TypeSizes.sizeofUnsignedVInt(columns.size());\n            for (Map.Entry<ByteBuffer, AbstractType<?>> entry : columns.entrySet())\n            {\n                size += ByteBufferUtil.serializedSizeWithVIntLength(entry.getKey());\n                size += sizeofType(entry.getValue());\n            }\n            return size;\n        }"
        ],
        [
            "ClusteringIndexNamesFilter::serializeInternal(DataOutputPlus,int)",
            " 240  \n 241  \n 242  \n 243 -\n 244  \n 245  \n 246  ",
            "    protected void serializeInternal(DataOutputPlus out, int version) throws IOException\n    {\n        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();\n        out.writeVInt(clusterings.size());\n        for (Clustering clustering : clusterings)\n            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());\n    }",
            " 240  \n 241  \n 242  \n 243 +\n 244  \n 245  \n 246  ",
            "    protected void serializeInternal(DataOutputPlus out, int version) throws IOException\n    {\n        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();\n        out.writeUnsignedVInt(clusterings.size());\n        for (Clustering clustering : clusterings)\n            Clustering.serializer.serialize(clustering, out, version, comparator.subtypes());\n    }"
        ],
        [
            "SerializationHeader::readTimestamp(DataInputPlus)",
            " 186  \n 187  \n 188 -\n 189  ",
            "    public long readTimestamp(DataInputPlus in) throws IOException\n    {\n        return in.readVInt() + stats.minTimestamp;\n    }",
            " 186  \n 187  \n 188 +\n 189  ",
            "    public long readTimestamp(DataInputPlus in) throws IOException\n    {\n        return in.readUnsignedVInt() + stats.minTimestamp;\n    }"
        ],
        [
            "SerializationHeader::writeLocalDeletionTime(int,DataOutputPlus)",
            " 170  \n 171  \n 172 -\n 173  ",
            "    public void writeLocalDeletionTime(int localDeletionTime, DataOutputPlus out) throws IOException\n    {\n        out.writeVInt(localDeletionTime - stats.minLocalDeletionTime);\n    }",
            " 170  \n 171  \n 172 +\n 173  ",
            "    public void writeLocalDeletionTime(int localDeletionTime, DataOutputPlus out) throws IOException\n    {\n        out.writeUnsignedVInt(localDeletionTime - stats.minLocalDeletionTime);\n    }"
        ],
        [
            "ClusteringIndexNamesFilter::serializedSizeInternal(int)",
            " 248  \n 249  \n 250  \n 251 -\n 252  \n 253  \n 254  \n 255  ",
            "    protected long serializedSizeInternal(int version)\n    {\n        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();\n        long size = TypeSizes.sizeofVInt(clusterings.size());\n        for (Clustering clustering : clusterings)\n            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());\n        return size;\n    }",
            " 248  \n 249  \n 250  \n 251 +\n 252  \n 253  \n 254  \n 255  ",
            "    protected long serializedSizeInternal(int version)\n    {\n        ClusteringComparator comparator = (ClusteringComparator)clusterings.comparator();\n        long size = TypeSizes.sizeofUnsignedVInt(clusterings.size());\n        for (Clustering clustering : clusterings)\n            size += Clustering.serializer.serializedSize(clustering, version, comparator.subtypes());\n        return size;\n    }"
        ],
        [
            "Slices::Serializer::serializedSize(Slices,int)",
            " 313  \n 314  \n 315 -\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  ",
            "        public long serializedSize(Slices slices, int version)\n        {\n            long size = TypeSizes.sizeofVInt(slices.size());\n\n            if (slices.size() == 0)\n                return size;\n\n            List<AbstractType<?>> types = slices instanceof SelectAllSlices\n                                        ? Collections.<AbstractType<?>>emptyList()\n                                        : ((ArrayBackedSlices)slices).comparator.subtypes();\n\n            for (Slice slice : slices)\n                size += Slice.serializer.serializedSize(slice, version, types);\n\n            return size;\n        }",
            " 313  \n 314  \n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  ",
            "        public long serializedSize(Slices slices, int version)\n        {\n            long size = TypeSizes.sizeofUnsignedVInt(slices.size());\n\n            if (slices.size() == 0)\n                return size;\n\n            List<AbstractType<?>> types = slices instanceof SelectAllSlices\n                                        ? Collections.<AbstractType<?>>emptyList()\n                                        : ((ArrayBackedSlices)slices).comparator.subtypes();\n\n            for (Slice slice : slices)\n                size += Slice.serializer.serializedSize(slice, version, types);\n\n            return size;\n        }"
        ],
        [
            "ColumnFilter::Serializer::serializedSize(ColumnFilter,int)",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420 -\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  ",
            "        public long serializedSize(ColumnFilter selection, int version)\n        {\n            long size = 1; // header byte\n\n            if (selection.selection != null)\n            {\n                size += Columns.serializer.serializedSize(selection.selection.statics);\n                size += Columns.serializer.serializedSize(selection.selection.regulars);\n            }\n\n            if (selection.subSelections != null)\n            {\n\n                size += TypeSizes.sizeofVInt(selection.subSelections.size());\n                for (ColumnSubselection subSel : selection.subSelections.values())\n                    size += ColumnSubselection.serializer.serializedSize(subSel, version);\n            }\n\n            return size;\n        }",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420 +\n 421  \n 422  \n 423  \n 424  \n 425  \n 426  ",
            "        public long serializedSize(ColumnFilter selection, int version)\n        {\n            long size = 1; // header byte\n\n            if (selection.selection != null)\n            {\n                size += Columns.serializer.serializedSize(selection.selection.statics);\n                size += Columns.serializer.serializedSize(selection.selection.regulars);\n            }\n\n            if (selection.subSelections != null)\n            {\n\n                size += TypeSizes.sizeofUnsignedVInt(selection.subSelections.size());\n                for (ColumnSubselection subSel : selection.subSelections.values())\n                    size += ColumnSubselection.serializer.serializedSize(subSel, version);\n            }\n\n            return size;\n        }"
        ],
        [
            "SerializationHeader::Serializer::serializedSize(Version,Component)",
            " 450  \n 451  \n 452  \n 453  \n 454  \n 455 -\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  ",
            "        public int serializedSize(Version version, Component header)\n        {\n            int size = EncodingStats.serializer.serializedSize(header.stats);\n\n            size += sizeofType(header.keyType);\n            size += TypeSizes.sizeofVInt(header.clusteringTypes.size());\n            for (AbstractType<?> type : header.clusteringTypes)\n                size += sizeofType(type);\n\n            size += sizeofColumnsWithTypes(header.staticColumns);\n            size += sizeofColumnsWithTypes(header.regularColumns);\n            return size;\n        }",
            " 450  \n 451  \n 452  \n 453  \n 454  \n 455 +\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  ",
            "        public int serializedSize(Version version, Component header)\n        {\n            int size = EncodingStats.serializer.serializedSize(header.stats);\n\n            size += sizeofType(header.keyType);\n            size += TypeSizes.sizeofUnsignedVInt(header.clusteringTypes.size());\n            for (AbstractType<?> type : header.clusteringTypes)\n                size += sizeofType(type);\n\n            size += sizeofColumnsWithTypes(header.staticColumns);\n            size += sizeofColumnsWithTypes(header.regularColumns);\n            return size;\n        }"
        ],
        [
            "EncodingStats::Serializer::deserialize(DataInputPlus)",
            " 245  \n 246  \n 247 -\n 248 -\n 249 -\n 250  \n 251  ",
            "        public EncodingStats deserialize(DataInputPlus in) throws IOException\n        {\n            long minTimestamp = in.readVInt() + TIMESTAMP_EPOCH;\n            int minLocalDeletionTime = (int)in.readVInt() + DELETION_TIME_EPOCH;\n            int minTTL = (int)in.readVInt() + TTL_EPOCH;\n            return new EncodingStats(minTimestamp, minLocalDeletionTime, minTTL);\n        }",
            " 245  \n 246  \n 247 +\n 248 +\n 249 +\n 250  \n 251  ",
            "        public EncodingStats deserialize(DataInputPlus in) throws IOException\n        {\n            long minTimestamp = in.readUnsignedVInt() + TIMESTAMP_EPOCH;\n            int minLocalDeletionTime = (int)in.readUnsignedVInt() + DELETION_TIME_EPOCH;\n            int minTTL = (int)in.readUnsignedVInt() + TTL_EPOCH;\n            return new EncodingStats(minTimestamp, minLocalDeletionTime, minTTL);\n        }"
        ],
        [
            "ByteBufferUtil::serializedSizeWithVIntLength(ByteBuffer)",
            " 348  \n 349  \n 350  \n 351 -\n 352  ",
            "    public static int serializedSizeWithVIntLength(ByteBuffer buffer)\n    {\n        int size = buffer.remaining();\n        return TypeSizes.sizeofVInt(size) + size;\n    }",
            " 348  \n 349  \n 350  \n 351 +\n 352  ",
            "    public static int serializedSizeWithVIntLength(ByteBuffer buffer)\n    {\n        int size = buffer.remaining();\n        return TypeSizes.sizeofUnsignedVInt(size) + size;\n    }"
        ],
        [
            "SerializationHeader::readTTL(DataInputPlus)",
            " 196  \n 197  \n 198 -\n 199  ",
            "    public int readTTL(DataInputPlus in) throws IOException\n    {\n        return (int)in.readVInt() + stats.minTTL;\n    }",
            " 196  \n 197  \n 198 +\n 199  ",
            "    public int readTTL(DataInputPlus in) throws IOException\n    {\n        return (int)in.readUnsignedVInt() + stats.minTTL;\n    }"
        ],
        [
            "Hint::Serializer::serialize(Hint,DataOutputPlus,int)",
            " 119  \n 120  \n 121  \n 122 -\n 123  \n 124  ",
            "        public void serialize(Hint hint, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeLong(hint.creationTime);\n            out.writeVInt(hint.gcgs);\n            Mutation.serializer.serialize(hint.mutation, out, version);\n        }",
            " 119  \n 120  \n 121  \n 122 +\n 123  \n 124  ",
            "        public void serialize(Hint hint, DataOutputPlus out, int version) throws IOException\n        {\n            out.writeLong(hint.creationTime);\n            out.writeUnsignedVInt(hint.gcgs);\n            Mutation.serializer.serialize(hint.mutation, out, version);\n        }"
        ],
        [
            "Batch::Serializer::readEncodedMutations(DataInputPlus)",
            " 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "        private static Collection<ByteBuffer> readEncodedMutations(DataInputPlus in) throws IOException\n        {\n            int count = (int) in.readVInt();\n\n            ArrayList<ByteBuffer> mutations = new ArrayList<>(count);\n            for (int i = 0; i < count; i++)\n                mutations.add(ByteBufferUtil.readWithVIntLength(in));\n\n            return mutations;\n        }",
            " 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "        private static Collection<ByteBuffer> readEncodedMutations(DataInputPlus in) throws IOException\n        {\n            int count = (int) in.readUnsignedVInt();\n\n            ArrayList<ByteBuffer> mutations = new ArrayList<>(count);\n            for (int i = 0; i < count; i++)\n                mutations.add(ByteBufferUtil.readWithVIntLength(in));\n\n            return mutations;\n        }"
        ],
        [
            "EncodedHintMessage::Serializer::serializedSize(EncodedHintMessage,int)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  ",
            "        public long serializedSize(EncodedHintMessage message, int version)\n        {\n            if (version != message.version)\n                throw new IllegalArgumentException(\"serializedSize() called with non-matching version \" + version);\n\n            long size = UUIDSerializer.serializer.serializedSize(message.hostId, version);\n            size += TypeSizes.sizeofVInt(message.hint.remaining());\n            size += message.hint.remaining();\n            return size;\n        }",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  ",
            "        public long serializedSize(EncodedHintMessage message, int version)\n        {\n            if (version != message.version)\n                throw new IllegalArgumentException(\"serializedSize() called with non-matching version \" + version);\n\n            long size = UUIDSerializer.serializer.serializedSize(message.hostId, version);\n            size += TypeSizes.sizeofUnsignedVInt(message.hint.remaining());\n            size += message.hint.remaining();\n            return size;\n        }"
        ],
        [
            "SerializationHeader::skipLocalDeletionTime(DataInputPlus)",
            " 234  \n 235  \n 236 -\n 237  ",
            "    public void skipLocalDeletionTime(DataInputPlus in) throws IOException\n    {\n        in.readVInt();\n    }",
            " 234  \n 235  \n 236 +\n 237  ",
            "    public void skipLocalDeletionTime(DataInputPlus in) throws IOException\n    {\n        in.readUnsignedVInt();\n    }"
        ],
        [
            "RowFilter::Serializer::serializedSize(RowFilter,int)",
            " 768  \n 769  \n 770  \n 771 -\n 772  \n 773  \n 774  \n 775  ",
            "        public long serializedSize(RowFilter filter, int version)\n        {\n            long size = 1 // forThrift\n                      + TypeSizes.sizeofVInt(filter.expressions.size());\n            for (Expression expr : filter.expressions)\n                size += Expression.serializer.serializedSize(expr, version);\n            return size;\n        }",
            " 768  \n 769  \n 770  \n 771 +\n 772  \n 773  \n 774  \n 775  ",
            "        public long serializedSize(RowFilter filter, int version)\n        {\n            long size = 1 // forThrift\n                      + TypeSizes.sizeofUnsignedVInt(filter.expressions.size());\n            for (Expression expr : filter.expressions)\n                size += Expression.serializer.serializedSize(expr, version);\n            return size;\n        }"
        ],
        [
            "Columns::Serializer::serializedSize(Columns)",
            " 408  \n 409  \n 410 -\n 411  \n 412  \n 413  \n 414  ",
            "        public long serializedSize(Columns columns)\n        {\n            long size = TypeSizes.sizeofVInt(columns.size());\n            for (ColumnDefinition column : columns)\n                size += ByteBufferUtil.serializedSizeWithVIntLength(column.name.bytes);\n            return size;\n        }",
            " 408  \n 409  \n 410 +\n 411  \n 412  \n 413  \n 414  ",
            "        public long serializedSize(Columns columns)\n        {\n            long size = TypeSizes.sizeofUnsignedVInt(columns.size());\n            for (ColumnDefinition column : columns)\n                size += ByteBufferUtil.serializedSizeWithVIntLength(column.name.bytes);\n            return size;\n        }"
        ],
        [
            "SerializationHeader::Serializer::readColumnsWithType(DataInputPlus,Map)",
            " 485  \n 486  \n 487 -\n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "        private void readColumnsWithType(DataInputPlus in, Map<ByteBuffer, AbstractType<?>> typeMap) throws IOException\n        {\n            int length = (int)in.readVInt();\n            for (int i = 0; i < length; i++)\n            {\n                ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);\n                typeMap.put(name, readType(in));\n            }\n        }",
            " 485  \n 486  \n 487 +\n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "        private void readColumnsWithType(DataInputPlus in, Map<ByteBuffer, AbstractType<?>> typeMap) throws IOException\n        {\n            int length = (int)in.readUnsignedVInt();\n            for (int i = 0; i < length; i++)\n            {\n                ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);\n                typeMap.put(name, readType(in));\n            }\n        }"
        ],
        [
            "SerializationHeader::Serializer::serialize(Version,Component,DataOutputPlus)",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421 -\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "        public void serialize(Version version, Component header, DataOutputPlus out) throws IOException\n        {\n            EncodingStats.serializer.serialize(header.stats, out);\n\n            writeType(header.keyType, out);\n            out.writeVInt(header.clusteringTypes.size());\n            for (AbstractType<?> type : header.clusteringTypes)\n                writeType(type, out);\n\n            writeColumnsWithTypes(header.staticColumns, out);\n            writeColumnsWithTypes(header.regularColumns, out);\n        }",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421 +\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "        public void serialize(Version version, Component header, DataOutputPlus out) throws IOException\n        {\n            EncodingStats.serializer.serialize(header.stats, out);\n\n            writeType(header.keyType, out);\n            out.writeUnsignedVInt(header.clusteringTypes.size());\n            for (AbstractType<?> type : header.clusteringTypes)\n                writeType(type, out);\n\n            writeColumnsWithTypes(header.staticColumns, out);\n            writeColumnsWithTypes(header.regularColumns, out);\n        }"
        ],
        [
            "SerializationHeader::skipTimestamp(DataInputPlus)",
            " 229  \n 230  \n 231 -\n 232  ",
            "    public void skipTimestamp(DataInputPlus in) throws IOException\n    {\n        in.readVInt();\n    }",
            " 229  \n 230  \n 231 +\n 232  ",
            "    public void skipTimestamp(DataInputPlus in) throws IOException\n    {\n        in.readUnsignedVInt();\n    }"
        ],
        [
            "UnfilteredRowIteratorSerializer::serializedSize(UnfilteredRowIterator,ColumnFilter,int,int)",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)\n    {\n        SerializationHeader header = new SerializationHeader(iterator.metadata(),\n                                                             iterator.columns(),\n                                                             iterator.stats());\n\n        assert rowEstimate >= 0;\n\n        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())\n                  + 1; // flags\n\n        if (iterator.isEmpty())\n            return size;\n\n        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();\n        Row staticRow = iterator.staticRow();\n        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;\n\n        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);\n\n        if (!partitionDeletion.isLive())\n            size += header.deletionTimeSerializedSize(partitionDeletion);\n\n        if (hasStatic)\n            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);\n\n        if (rowEstimate >= 0)\n            size += TypeSizes.sizeofVInt(rowEstimate);\n\n        while (iterator.hasNext())\n            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);\n        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();\n\n        return size;\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    public long serializedSize(UnfilteredRowIterator iterator, ColumnFilter selection, int version, int rowEstimate)\n    {\n        SerializationHeader header = new SerializationHeader(iterator.metadata(),\n                                                             iterator.columns(),\n                                                             iterator.stats());\n\n        assert rowEstimate >= 0;\n\n        long size = ByteBufferUtil.serializedSizeWithVIntLength(iterator.partitionKey().getKey())\n                  + 1; // flags\n\n        if (iterator.isEmpty())\n            return size;\n\n        DeletionTime partitionDeletion = iterator.partitionLevelDeletion();\n        Row staticRow = iterator.staticRow();\n        boolean hasStatic = staticRow != Rows.EMPTY_STATIC_ROW;\n\n        size += SerializationHeader.serializer.serializedSizeForMessaging(header, selection, hasStatic);\n\n        if (!partitionDeletion.isLive())\n            size += header.deletionTimeSerializedSize(partitionDeletion);\n\n        if (hasStatic)\n            size += UnfilteredSerializer.serializer.serializedSize(staticRow, header, version);\n\n        if (rowEstimate >= 0)\n            size += TypeSizes.sizeofUnsignedVInt(rowEstimate);\n\n        while (iterator.hasNext())\n            size += UnfilteredSerializer.serializer.serializedSize(iterator.next(), header, version);\n        size += UnfilteredSerializer.serializer.serializedSizeEndOfPartition();\n\n        return size;\n    }"
        ],
        [
            "Hint::Serializer::deserialize(DataInputPlus,int)",
            " 126  \n 127  \n 128  \n 129 -\n 130  \n 131  ",
            "        public Hint deserialize(DataInputPlus in, int version) throws IOException\n        {\n            long creationTime = in.readLong();\n            int gcgs = (int) in.readVInt();\n            return new Hint(Mutation.serializer.deserialize(in, version), creationTime, gcgs);\n        }",
            " 126  \n 127  \n 128  \n 129 +\n 130  \n 131  ",
            "        public Hint deserialize(DataInputPlus in, int version) throws IOException\n        {\n            long creationTime = in.readLong();\n            int gcgs = (int) in.readUnsignedVInt();\n            return new Hint(Mutation.serializer.deserialize(in, version), creationTime, gcgs);\n        }"
        ],
        [
            "Batch::Serializer::serializedSize(Batch,int)",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  ",
            "        public long serializedSize(Batch batch, int version)\n        {\n            assert batch.encodedMutations.isEmpty() : \"attempted to serialize a 'remote' batch\";\n\n            long size = UUIDSerializer.serializer.serializedSize(batch.id, version);\n            size += sizeof(batch.creationTime);\n\n            size += sizeofVInt(batch.decodedMutations.size());\n            for (Mutation mutation : batch.decodedMutations)\n            {\n                int mutationSize = (int) Mutation.serializer.serializedSize(mutation, version);\n                size += sizeofVInt(mutationSize);\n                size += mutationSize;\n            }\n\n            return size;\n        }",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  ",
            "        public long serializedSize(Batch batch, int version)\n        {\n            assert batch.encodedMutations.isEmpty() : \"attempted to serialize a 'remote' batch\";\n\n            long size = UUIDSerializer.serializer.serializedSize(batch.id, version);\n            size += sizeof(batch.creationTime);\n\n            size += sizeofUnsignedVInt(batch.decodedMutations.size());\n            for (Mutation mutation : batch.decodedMutations)\n            {\n                int mutationSize = (int) Mutation.serializer.serializedSize(mutation, version);\n                size += sizeofUnsignedVInt(mutationSize);\n                size += mutationSize;\n            }\n\n            return size;\n        }"
        ],
        [
            "Columns::Serializer::serialize(Columns,DataOutputPlus)",
            " 401  \n 402  \n 403 -\n 404  \n 405  \n 406  ",
            "        public void serialize(Columns columns, DataOutputPlus out) throws IOException\n        {\n            out.writeVInt(columns.size());\n            for (ColumnDefinition column : columns)\n                ByteBufferUtil.writeWithVIntLength(column.name.bytes, out);\n        }",
            " 401  \n 402  \n 403 +\n 404  \n 405  \n 406  ",
            "        public void serialize(Columns columns, DataOutputPlus out) throws IOException\n        {\n            out.writeUnsignedVInt(columns.size());\n            for (ColumnDefinition column : columns)\n                ByteBufferUtil.writeWithVIntLength(column.name.bytes, out);\n        }"
        ],
        [
            "ByteBufferUtil::skipWithVIntLength(DataInputPlus)",
            " 354  \n 355  \n 356 -\n 357  \n 358  \n 359  \n 360  \n 361  ",
            "    public static void skipWithVIntLength(DataInputPlus in) throws IOException\n    {\n        int length = (int)in.readVInt();\n        if (length < 0)\n            throw new IOException(\"Corrupt (negative) value length encountered\");\n\n        FileUtils.skipBytesFully(in, length);\n    }",
            " 354  \n 355  \n 356 +\n 357  \n 358  \n 359  \n 360  \n 361  ",
            "    public static void skipWithVIntLength(DataInputPlus in) throws IOException\n    {\n        int length = (int)in.readUnsignedVInt();\n        if (length < 0)\n            throw new IOException(\"Corrupt (negative) value length encountered\");\n\n        FileUtils.skipBytesFully(in, length);\n    }"
        ],
        [
            "ReadCommand::Serializer::serialize(ReadCommand,DataOutputPlus,int)",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567 -\n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            // for serialization, createLegacyMessage() should cause legacyReadCommandSerializer to be used directly\n            assert version >= MessagingService.VERSION_30;\n\n            out.writeByte(command.kind.ordinal());\n            out.writeByte(digestFlag(command.isDigestQuery()) | thriftFlag(command.isForThrift()) | indexFlag(command.index.isPresent()));\n            if (command.isDigestQuery())\n                out.writeVInt(command.digestVersion());\n            CFMetaData.serializer.serialize(command.metadata(), out, version);\n            out.writeInt(command.nowInSec());\n            ColumnFilter.serializer.serialize(command.columnFilter(), out, version);\n            RowFilter.serializer.serialize(command.rowFilter(), out, version);\n            DataLimits.serializer.serialize(command.limits(), out, version);\n            if (command.index.isPresent())\n                IndexMetadata.serializer.serialize(command.index.get(), out, version);\n\n            command.serializeSelection(out, version);\n        }",
            " 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567 +\n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  ",
            "        public void serialize(ReadCommand command, DataOutputPlus out, int version) throws IOException\n        {\n            // for serialization, createLegacyMessage() should cause legacyReadCommandSerializer to be used directly\n            assert version >= MessagingService.VERSION_30;\n\n            out.writeByte(command.kind.ordinal());\n            out.writeByte(digestFlag(command.isDigestQuery()) | thriftFlag(command.isForThrift()) | indexFlag(command.index.isPresent()));\n            if (command.isDigestQuery())\n                out.writeUnsignedVInt(command.digestVersion());\n            CFMetaData.serializer.serialize(command.metadata(), out, version);\n            out.writeInt(command.nowInSec());\n            ColumnFilter.serializer.serialize(command.columnFilter(), out, version);\n            RowFilter.serializer.serialize(command.rowFilter(), out, version);\n            DataLimits.serializer.serialize(command.limits(), out, version);\n            if (command.index.isPresent())\n                IndexMetadata.serializer.serialize(command.index.get(), out, version);\n\n            command.serializeSelection(out, version);\n        }"
        ],
        [
            "Slices::Serializer::deserialize(DataInputPlus,int,CFMetaData)",
            " 330  \n 331  \n 332 -\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "        public Slices deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            int size = (int)in.readVInt();\n\n            if (size == 0)\n                return NONE;\n\n            Slice[] slices = new Slice[size];\n            for (int i = 0; i < size; i++)\n                slices[i] = Slice.serializer.deserialize(in, version, metadata.comparator.subtypes());\n\n            if (size == 1 && slices[0].start() == Slice.Bound.BOTTOM && slices[0].end() == Slice.Bound.TOP)\n                return ALL;\n\n            return new ArrayBackedSlices(metadata.comparator, slices);\n        }",
            " 330  \n 331  \n 332 +\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "        public Slices deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException\n        {\n            int size = (int)in.readUnsignedVInt();\n\n            if (size == 0)\n                return NONE;\n\n            Slice[] slices = new Slice[size];\n            for (int i = 0; i < size; i++)\n                slices[i] = Slice.serializer.deserialize(in, version, metadata.comparator.subtypes());\n\n            if (size == 1 && slices[0].start() == Slice.Bound.BOTTOM && slices[0].end() == Slice.Bound.TOP)\n                return ALL;\n\n            return new ArrayBackedSlices(metadata.comparator, slices);\n        }"
        ],
        [
            "DataLimits::Serializer::serializedSize(DataLimits,int)",
            " 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701 -\n 702 -\n 703  \n 704  \n 705  \n 706  \n 707  \n 708 -\n 709  \n 710  \n 711  \n 712  \n 713  \n 714 -\n 715 -\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  ",
            "        public long serializedSize(DataLimits limits, int version)\n        {\n            long size = TypeSizes.sizeof((byte)limits.kind().ordinal());\n            switch (limits.kind())\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    CQLLimits cqlLimits = (CQLLimits)limits;\n                    size += TypeSizes.sizeofVInt(cqlLimits.rowLimit);\n                    size += TypeSizes.sizeofVInt(cqlLimits.perPartitionLimit);\n                    size += TypeSizes.sizeof(cqlLimits.isDistinct);\n                    if (limits.kind() == Kind.CQL_PAGING_LIMIT)\n                    {\n                        CQLPagingLimits pagingLimits = (CQLPagingLimits)cqlLimits;\n                        size += ByteBufferUtil.serializedSizeWithVIntLength(pagingLimits.lastReturnedKey);\n                        size += TypeSizes.sizeofVInt(pagingLimits.lastReturnedKeyRemaining);\n                    }\n                    break;\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    ThriftLimits thriftLimits = (ThriftLimits)limits;\n                    size += TypeSizes.sizeofVInt(thriftLimits.partitionLimit);\n                    size += TypeSizes.sizeofVInt(thriftLimits.cellPerPartitionLimit);\n                    break;\n                default:\n                    throw new AssertionError();\n            }\n            return size;\n        }",
            " 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701 +\n 702 +\n 703  \n 704  \n 705  \n 706  \n 707  \n 708 +\n 709  \n 710  \n 711  \n 712  \n 713  \n 714 +\n 715 +\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  ",
            "        public long serializedSize(DataLimits limits, int version)\n        {\n            long size = TypeSizes.sizeof((byte)limits.kind().ordinal());\n            switch (limits.kind())\n            {\n                case CQL_LIMIT:\n                case CQL_PAGING_LIMIT:\n                    CQLLimits cqlLimits = (CQLLimits)limits;\n                    size += TypeSizes.sizeofUnsignedVInt(cqlLimits.rowLimit);\n                    size += TypeSizes.sizeofUnsignedVInt(cqlLimits.perPartitionLimit);\n                    size += TypeSizes.sizeof(cqlLimits.isDistinct);\n                    if (limits.kind() == Kind.CQL_PAGING_LIMIT)\n                    {\n                        CQLPagingLimits pagingLimits = (CQLPagingLimits)cqlLimits;\n                        size += ByteBufferUtil.serializedSizeWithVIntLength(pagingLimits.lastReturnedKey);\n                        size += TypeSizes.sizeofUnsignedVInt(pagingLimits.lastReturnedKeyRemaining);\n                    }\n                    break;\n                case THRIFT_LIMIT:\n                case SUPER_COLUMN_COUNTING_LIMIT:\n                    ThriftLimits thriftLimits = (ThriftLimits)limits;\n                    size += TypeSizes.sizeofUnsignedVInt(thriftLimits.partitionLimit);\n                    size += TypeSizes.sizeofUnsignedVInt(thriftLimits.cellPerPartitionLimit);\n                    break;\n                default:\n                    throw new AssertionError();\n            }\n            return size;\n        }"
        ]
    ],
    "4a849efeb7c7c1a54bc12094d2d6a9f3f008a2fa": [
        [
            "CompactionManager::submitBackground(ColumnFamilyStore)",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168 -\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "    /**\n     * Call this whenever a compaction might be needed on the given columnfamily.\n     * It's okay to over-call (within reason) if a call is unnecessary, it will\n     * turn into a no-op in the bucketing/candidate-scan phase.\n     */\n    public List<Future<?>> submitBackground(final ColumnFamilyStore cfs)\n    {\n        if (cfs.isAutoCompactionDisabled())\n        {\n            logger.debug(\"Autocompaction is disabled\");\n            return Collections.emptyList();\n        }\n\n        int count = compactingCF.count(cfs);\n        if (count > 0 && executor.getActiveCount() >= executor.getMaximumPoolSize())\n        {\n            logger.debug(\"Background compaction is still running for {}.{} ({} remaining). Skipping\",\n                         cfs.keyspace.getName(), cfs.name, count);\n            return Collections.emptyList();\n        }\n\n        logger.debug(\"Scheduling a background task check for {}.{} with {}\",\n                     cfs.keyspace.getName(),\n                     cfs.name,\n                     cfs.getCompactionStrategy().getName());\n        List<Future<?>> futures = new ArrayList<>();\n        // we must schedule it at least once, otherwise compaction will stop for a CF until next flush\n        if (executor.isShutdown())\n        {\n            logger.info(\"Executor has shut down, not submitting background task\");\n            return Collections.emptyList();\n        }\n        compactingCF.add(cfs);\n        futures.add(executor.submit(new BackgroundCompactionCandidate(cfs)));\n\n        return futures;\n    }",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 +\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "    /**\n     * Call this whenever a compaction might be needed on the given columnfamily.\n     * It's okay to over-call (within reason) if a call is unnecessary, it will\n     * turn into a no-op in the bucketing/candidate-scan phase.\n     */\n    public List<Future<?>> submitBackground(final ColumnFamilyStore cfs)\n    {\n        if (cfs.isAutoCompactionDisabled())\n        {\n            logger.trace(\"Autocompaction is disabled\");\n            return Collections.emptyList();\n        }\n\n        int count = compactingCF.count(cfs);\n        if (count > 0 && executor.getActiveCount() >= executor.getMaximumPoolSize())\n        {\n            logger.trace(\"Background compaction is still running for {}.{} ({} remaining). Skipping\",\n                         cfs.keyspace.getName(), cfs.name, count);\n            return Collections.emptyList();\n        }\n\n        logger.trace(\"Scheduling a background task check for {}.{} with {}\",\n                     cfs.keyspace.getName(),\n                     cfs.name,\n                     cfs.getCompactionStrategy().getName());\n        List<Future<?>> futures = new ArrayList<>();\n        // we must schedule it at least once, otherwise compaction will stop for a CF until next flush\n        if (executor.isShutdown())\n        {\n            logger.info(\"Executor has shut down, not submitting background task\");\n            return Collections.emptyList();\n        }\n        compactingCF.add(cfs);\n        futures.add(executor.submit(new BackgroundCompactionCandidate(cfs)));\n\n        return futures;\n    }"
        ],
        [
            "MigrationRequestVerbHandler::doVerb(MessageIn,int)",
            "  40  \n  41  \n  42 -\n  43  \n  44  \n  45  \n  46  \n  47  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        logger.debug(\"Received migration request from {}.\", message.from);\n        MessageOut<Collection<Mutation>> response = new MessageOut<>(MessagingService.Verb.INTERNAL_RESPONSE,\n                                                                     LegacySchemaTables.convertSchemaToMutations(),\n                                                                     MigrationManager.MigrationsSerializer.instance);\n        MessagingService.instance().sendReply(response, id, message.from);\n    }",
            "  40  \n  41  \n  42 +\n  43  \n  44  \n  45  \n  46  \n  47  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        logger.trace(\"Received migration request from {}.\", message.from);\n        MessageOut<Collection<Mutation>> response = new MessageOut<>(MessagingService.Verb.INTERNAL_RESPONSE,\n                                                                     LegacySchemaTables.convertSchemaToMutations(),\n                                                                     MigrationManager.MigrationsSerializer.instance);\n        MessagingService.instance().sendReply(response, id, message.from);\n    }"
        ],
        [
            "CassandraServer::cas(ByteBuffer,String,List,List,ConsistencyLevel,ConsistencyLevel)",
            " 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 -\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  ",
            "    public CASResult cas(ByteBuffer key,\n                         String column_family,\n                         List<Column> expected,\n                         List<Column> updates,\n                         ConsistencyLevel serial_consistency_level,\n                         ConsistencyLevel commit_consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            ImmutableMap.Builder<String,String> builder = ImmutableMap.builder();\n            builder.put(\"key\", ByteBufferUtil.bytesToHex(key));\n            builder.put(\"column_family\", column_family);\n            builder.put(\"old\", expected.toString());\n            builder.put(\"updates\", updates.toString());\n            builder.put(\"consistency_level\", commit_consistency_level.name());\n            builder.put(\"serial_consistency_level\", serial_consistency_level.name());\n            Map<String,String> traceParameters = builder.build();\n\n            Tracing.instance.begin(\"cas\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"cas\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.MODIFY);\n            // CAS updates can be used to simulate a get request, so should require Permission.SELECT.\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_family, false);\n            ThriftValidation.validateKey(metadata, key);\n            if (metadata.cfType == ColumnFamilyType.Super)\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"CAS does not support supercolumns\");\n\n            Iterable<ByteBuffer> names = Iterables.transform(updates, new Function<Column, ByteBuffer>()\n            {\n                public ByteBuffer apply(Column column)\n                {\n                    return column.name;\n                }\n            });\n            ThriftValidation.validateColumnNames(metadata, new ColumnParent(column_family), names);\n            for (Column column : updates)\n                ThriftValidation.validateColumnData(metadata, key, null, column);\n\n            CFMetaData cfm = Schema.instance.getCFMetaData(cState.getKeyspace(), column_family);\n            ColumnFamily cfUpdates = ArrayBackedSortedColumns.factory.create(cfm);\n            for (Column column : updates)\n                cfUpdates.addColumn(cfm.comparator.cellFromByteBuffer(column.name), column.value, column.timestamp);\n\n            ColumnFamily cfExpected;\n            if (expected.isEmpty())\n            {\n                cfExpected = null;\n            }\n            else\n            {\n                cfExpected = ArrayBackedSortedColumns.factory.create(cfm);\n                for (Column column : expected)\n                    cfExpected.addColumn(cfm.comparator.cellFromByteBuffer(column.name), column.value, column.timestamp);\n            }\n\n            schedule(DatabaseDescriptor.getWriteRpcTimeout());\n            ColumnFamily result = StorageProxy.cas(cState.getKeyspace(),\n                                                   column_family,\n                                                   key,\n                                                   new ThriftCASRequest(cfExpected, cfUpdates),\n                                                   ThriftConversion.fromThrift(serial_consistency_level),\n                                                   ThriftConversion.fromThrift(commit_consistency_level),\n                                                   cState);\n            return result == null\n                 ? new CASResult(true)\n                 : new CASResult(false).setCurrent_values(thriftifyColumnsAsColumns(result.getSortedColumns(), System.currentTimeMillis()));\n        }\n        catch (RequestTimeoutException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 +\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  ",
            "    public CASResult cas(ByteBuffer key,\n                         String column_family,\n                         List<Column> expected,\n                         List<Column> updates,\n                         ConsistencyLevel serial_consistency_level,\n                         ConsistencyLevel commit_consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            ImmutableMap.Builder<String,String> builder = ImmutableMap.builder();\n            builder.put(\"key\", ByteBufferUtil.bytesToHex(key));\n            builder.put(\"column_family\", column_family);\n            builder.put(\"old\", expected.toString());\n            builder.put(\"updates\", updates.toString());\n            builder.put(\"consistency_level\", commit_consistency_level.name());\n            builder.put(\"serial_consistency_level\", serial_consistency_level.name());\n            Map<String,String> traceParameters = builder.build();\n\n            Tracing.instance.begin(\"cas\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"cas\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.MODIFY);\n            // CAS updates can be used to simulate a get request, so should require Permission.SELECT.\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_family, false);\n            ThriftValidation.validateKey(metadata, key);\n            if (metadata.cfType == ColumnFamilyType.Super)\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"CAS does not support supercolumns\");\n\n            Iterable<ByteBuffer> names = Iterables.transform(updates, new Function<Column, ByteBuffer>()\n            {\n                public ByteBuffer apply(Column column)\n                {\n                    return column.name;\n                }\n            });\n            ThriftValidation.validateColumnNames(metadata, new ColumnParent(column_family), names);\n            for (Column column : updates)\n                ThriftValidation.validateColumnData(metadata, key, null, column);\n\n            CFMetaData cfm = Schema.instance.getCFMetaData(cState.getKeyspace(), column_family);\n            ColumnFamily cfUpdates = ArrayBackedSortedColumns.factory.create(cfm);\n            for (Column column : updates)\n                cfUpdates.addColumn(cfm.comparator.cellFromByteBuffer(column.name), column.value, column.timestamp);\n\n            ColumnFamily cfExpected;\n            if (expected.isEmpty())\n            {\n                cfExpected = null;\n            }\n            else\n            {\n                cfExpected = ArrayBackedSortedColumns.factory.create(cfm);\n                for (Column column : expected)\n                    cfExpected.addColumn(cfm.comparator.cellFromByteBuffer(column.name), column.value, column.timestamp);\n            }\n\n            schedule(DatabaseDescriptor.getWriteRpcTimeout());\n            ColumnFamily result = StorageProxy.cas(cState.getKeyspace(),\n                                                   column_family,\n                                                   key,\n                                                   new ThriftCASRequest(cfExpected, cfUpdates),\n                                                   ThriftConversion.fromThrift(serial_consistency_level),\n                                                   ThriftConversion.fromThrift(commit_consistency_level),\n                                                   cState);\n            return result == null\n                 ? new CASResult(true)\n                 : new CASResult(false).setCurrent_values(thriftifyColumnsAsColumns(result.getSortedColumns(), System.currentTimeMillis()));\n        }\n        catch (RequestTimeoutException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "LoadBroadcaster::startBroadcasting()",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    public void startBroadcasting()\n    {\n        // send the first broadcast \"right away\" (i.e., in 2 gossip heartbeats, when we should have someone to talk to);\n        // after that send every BROADCAST_INTERVAL.\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                if (logger.isDebugEnabled())\n                    logger.debug(\"Disseminating load info ...\");\n                Gossiper.instance.addLocalApplicationState(ApplicationState.LOAD,\n                                                           StorageService.instance.valueFactory.load(StorageMetrics.load.getCount()));\n            }\n        };\n        ScheduledExecutors.scheduledTasks.scheduleWithFixedDelay(runnable, 2 * Gossiper.intervalInMillis, BROADCAST_INTERVAL, TimeUnit.MILLISECONDS);\n    }",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    public void startBroadcasting()\n    {\n        // send the first broadcast \"right away\" (i.e., in 2 gossip heartbeats, when we should have someone to talk to);\n        // after that send every BROADCAST_INTERVAL.\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"Disseminating load info ...\");\n                Gossiper.instance.addLocalApplicationState(ApplicationState.LOAD,\n                                                           StorageService.instance.valueFactory.load(StorageMetrics.load.getCount()));\n            }\n        };\n        ScheduledExecutors.scheduledTasks.scheduleWithFixedDelay(runnable, 2 * Gossiper.intervalInMillis, BROADCAST_INTERVAL, TimeUnit.MILLISECONDS);\n    }"
        ],
        [
            "SerializingCache::deserialize(RefCountedMemory)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  ",
            "    private V deserialize(RefCountedMemory mem)\n    {\n        try\n        {\n            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"Cannot fetch in memory data, we will fallback to read from disk \", e);\n            return null;\n        }\n    }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96  \n  97  \n  98  ",
            "    private V deserialize(RefCountedMemory mem)\n    {\n        try\n        {\n            return serializer.deserialize(new EncodedDataInputStream(new MemoryInputStream(mem)));\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"Cannot fetch in memory data, we will fallback to read from disk \", e);\n            return null;\n        }\n    }"
        ],
        [
            "IncomingStreamingConnection::run()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  ",
            "    @Override\n    public void run()\n    {\n        try\n        {\n            // streaming connections are per-session and have a fixed version.  we can't do anything with a wrong-version stream connection, so drop it.\n            if (version != StreamMessage.CURRENT_VERSION)\n                throw new IOException(String.format(\"Received stream using protocol version %d (my version %d). Terminating connection\", version, MessagingService.current_version));\n\n            DataInput input = new DataInputStream(socket.getInputStream());\n            StreamInitMessage init = StreamInitMessage.serializer.deserialize(input, version);\n\n            // The initiator makes two connections, one for incoming and one for outgoing.\n            // The receiving side distinguish two connections by looking at StreamInitMessage#isForOutgoing.\n            // Note: we cannot use the same socket for incoming and outgoing streams because we want to\n            // parallelize said streams and the socket is blocking, so we might deadlock.\n            StreamResultFuture.initReceivingSide(init.sessionIndex, init.planId, init.description, init.from, socket, init.isForOutgoing, version, init.keepSSTableLevel, init.isIncremental);\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"IOException reading from socket; closing\", e);\n            close();\n        }\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74  \n  75  \n  76  ",
            "    @Override\n    public void run()\n    {\n        try\n        {\n            // streaming connections are per-session and have a fixed version.  we can't do anything with a wrong-version stream connection, so drop it.\n            if (version != StreamMessage.CURRENT_VERSION)\n                throw new IOException(String.format(\"Received stream using protocol version %d (my version %d). Terminating connection\", version, MessagingService.current_version));\n\n            DataInput input = new DataInputStream(socket.getInputStream());\n            StreamInitMessage init = StreamInitMessage.serializer.deserialize(input, version);\n\n            // The initiator makes two connections, one for incoming and one for outgoing.\n            // The receiving side distinguish two connections by looking at StreamInitMessage#isForOutgoing.\n            // Note: we cannot use the same socket for incoming and outgoing streams because we want to\n            // parallelize said streams and the socket is blocking, so we might deadlock.\n            StreamResultFuture.initReceivingSide(init.sessionIndex, init.planId, init.description, init.from, socket, init.isForOutgoing, version, init.keepSSTableLevel, init.isIncremental);\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"IOException reading from socket; closing\", e);\n            close();\n        }\n    }"
        ],
        [
            "LeveledManifest::getCandidatesFor(int)",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542 -\n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  ",
            "    /**\n     * @return highest-priority sstables to compact for the given level.\n     * If no compactions are possible (because of concurrent compactions or because some sstables are blacklisted\n     * for prior failure), will return an empty list.  Never returns null.\n     */\n    private Collection<SSTableReader> getCandidatesFor(int level)\n    {\n        assert !getLevel(level).isEmpty();\n        logger.debug(\"Choosing candidates for L{}\", level);\n\n        final Set<SSTableReader> compacting = cfs.getTracker().getCompacting();\n\n        if (level == 0)\n        {\n            Set<SSTableReader> compactingL0 = getCompacting(0);\n\n            RowPosition lastCompactingKey = null;\n            RowPosition firstCompactingKey = null;\n            for (SSTableReader candidate : compactingL0)\n            {\n                if (firstCompactingKey == null || candidate.first.compareTo(firstCompactingKey) < 0)\n                    firstCompactingKey = candidate.first;\n                if (lastCompactingKey == null || candidate.last.compareTo(lastCompactingKey) > 0)\n                    lastCompactingKey = candidate.last;\n            }\n\n            // L0 is the dumping ground for new sstables which thus may overlap each other.\n            //\n            // We treat L0 compactions specially:\n            // 1a. add sstables to the candidate set until we have at least maxSSTableSizeInMB\n            // 1b. prefer choosing older sstables as candidates, to newer ones\n            // 1c. any L0 sstables that overlap a candidate, will also become candidates\n            // 2. At most MAX_COMPACTING_L0 sstables from L0 will be compacted at once\n            // 3. If total candidate size is less than maxSSTableSizeInMB, we won't bother compacting with L1,\n            //    and the result of the compaction will stay in L0 instead of being promoted (see promote())\n            //\n            // Note that we ignore suspect-ness of L1 sstables here, since if an L1 sstable is suspect we're\n            // basically screwed, since we expect all or most L0 sstables to overlap with each L1 sstable.\n            // So if an L1 sstable is suspect we can't do much besides try anyway and hope for the best.\n            Set<SSTableReader> candidates = new HashSet<>();\n            Set<SSTableReader> remaining = new HashSet<>();\n            Iterables.addAll(remaining, Iterables.filter(getLevel(0), Predicates.not(suspectP)));\n            for (SSTableReader sstable : ageSortedSSTables(remaining))\n            {\n                if (candidates.contains(sstable))\n                    continue;\n\n                Sets.SetView<SSTableReader> overlappedL0 = Sets.union(Collections.singleton(sstable), overlapping(sstable, remaining));\n                if (!Sets.intersection(overlappedL0, compactingL0).isEmpty())\n                    continue;\n\n                for (SSTableReader newCandidate : overlappedL0)\n                {\n                    if (firstCompactingKey == null || lastCompactingKey == null || overlapping(firstCompactingKey.getToken(), lastCompactingKey.getToken(), Arrays.asList(newCandidate)).size() == 0)\n                        candidates.add(newCandidate);\n                    remaining.remove(newCandidate);\n                }\n\n                if (candidates.size() > MAX_COMPACTING_L0)\n                {\n                    // limit to only the MAX_COMPACTING_L0 oldest candidates\n                    candidates = new HashSet<>(ageSortedSSTables(candidates).subList(0, MAX_COMPACTING_L0));\n                    break;\n                }\n            }\n\n            // leave everything in L0 if we didn't end up with a full sstable's worth of data\n            if (SSTableReader.getTotalBytes(candidates) > maxSSTableSizeInBytes)\n            {\n                // add sstables from L1 that overlap candidates\n                // if the overlapping ones are already busy in a compaction, leave it out.\n                // TODO try to find a set of L0 sstables that only overlaps with non-busy L1 sstables\n                Set<SSTableReader> l1overlapping = overlapping(candidates, getLevel(1));\n                if (Sets.intersection(l1overlapping, compacting).size() > 0)\n                    return Collections.emptyList();\n                if (!overlapping(candidates, compactingL0).isEmpty())\n                    return Collections.emptyList();\n                candidates = Sets.union(candidates, l1overlapping);\n            }\n            if (candidates.size() < 2)\n                return Collections.emptyList();\n            else\n                return candidates;\n        }\n\n        // for non-L0 compactions, pick up where we left off last time\n        Collections.sort(getLevel(level), SSTableReader.sstableComparator);\n        int start = 0; // handles case where the prior compaction touched the very last range\n        for (int i = 0; i < getLevel(level).size(); i++)\n        {\n            SSTableReader sstable = getLevel(level).get(i);\n            if (sstable.first.compareTo(lastCompactedKeys[level]) > 0)\n            {\n                start = i;\n                break;\n            }\n        }\n\n        // look for a non-suspect keyspace to compact with, starting with where we left off last time,\n        // and wrapping back to the beginning of the generation if necessary\n        for (int i = 0; i < getLevel(level).size(); i++)\n        {\n            SSTableReader sstable = getLevel(level).get((start + i) % getLevel(level).size());\n            Set<SSTableReader> candidates = Sets.union(Collections.singleton(sstable), overlapping(sstable, getLevel(level + 1)));\n            if (Iterables.any(candidates, suspectP))\n                continue;\n            if (Sets.intersection(candidates, compacting).isEmpty())\n                return candidates;\n        }\n\n        // all the sstables were suspect or overlapped with something suspect\n        return Collections.emptyList();\n    }",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542 +\n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  ",
            "    /**\n     * @return highest-priority sstables to compact for the given level.\n     * If no compactions are possible (because of concurrent compactions or because some sstables are blacklisted\n     * for prior failure), will return an empty list.  Never returns null.\n     */\n    private Collection<SSTableReader> getCandidatesFor(int level)\n    {\n        assert !getLevel(level).isEmpty();\n        logger.trace(\"Choosing candidates for L{}\", level);\n\n        final Set<SSTableReader> compacting = cfs.getTracker().getCompacting();\n\n        if (level == 0)\n        {\n            Set<SSTableReader> compactingL0 = getCompacting(0);\n\n            RowPosition lastCompactingKey = null;\n            RowPosition firstCompactingKey = null;\n            for (SSTableReader candidate : compactingL0)\n            {\n                if (firstCompactingKey == null || candidate.first.compareTo(firstCompactingKey) < 0)\n                    firstCompactingKey = candidate.first;\n                if (lastCompactingKey == null || candidate.last.compareTo(lastCompactingKey) > 0)\n                    lastCompactingKey = candidate.last;\n            }\n\n            // L0 is the dumping ground for new sstables which thus may overlap each other.\n            //\n            // We treat L0 compactions specially:\n            // 1a. add sstables to the candidate set until we have at least maxSSTableSizeInMB\n            // 1b. prefer choosing older sstables as candidates, to newer ones\n            // 1c. any L0 sstables that overlap a candidate, will also become candidates\n            // 2. At most MAX_COMPACTING_L0 sstables from L0 will be compacted at once\n            // 3. If total candidate size is less than maxSSTableSizeInMB, we won't bother compacting with L1,\n            //    and the result of the compaction will stay in L0 instead of being promoted (see promote())\n            //\n            // Note that we ignore suspect-ness of L1 sstables here, since if an L1 sstable is suspect we're\n            // basically screwed, since we expect all or most L0 sstables to overlap with each L1 sstable.\n            // So if an L1 sstable is suspect we can't do much besides try anyway and hope for the best.\n            Set<SSTableReader> candidates = new HashSet<>();\n            Set<SSTableReader> remaining = new HashSet<>();\n            Iterables.addAll(remaining, Iterables.filter(getLevel(0), Predicates.not(suspectP)));\n            for (SSTableReader sstable : ageSortedSSTables(remaining))\n            {\n                if (candidates.contains(sstable))\n                    continue;\n\n                Sets.SetView<SSTableReader> overlappedL0 = Sets.union(Collections.singleton(sstable), overlapping(sstable, remaining));\n                if (!Sets.intersection(overlappedL0, compactingL0).isEmpty())\n                    continue;\n\n                for (SSTableReader newCandidate : overlappedL0)\n                {\n                    if (firstCompactingKey == null || lastCompactingKey == null || overlapping(firstCompactingKey.getToken(), lastCompactingKey.getToken(), Arrays.asList(newCandidate)).size() == 0)\n                        candidates.add(newCandidate);\n                    remaining.remove(newCandidate);\n                }\n\n                if (candidates.size() > MAX_COMPACTING_L0)\n                {\n                    // limit to only the MAX_COMPACTING_L0 oldest candidates\n                    candidates = new HashSet<>(ageSortedSSTables(candidates).subList(0, MAX_COMPACTING_L0));\n                    break;\n                }\n            }\n\n            // leave everything in L0 if we didn't end up with a full sstable's worth of data\n            if (SSTableReader.getTotalBytes(candidates) > maxSSTableSizeInBytes)\n            {\n                // add sstables from L1 that overlap candidates\n                // if the overlapping ones are already busy in a compaction, leave it out.\n                // TODO try to find a set of L0 sstables that only overlaps with non-busy L1 sstables\n                Set<SSTableReader> l1overlapping = overlapping(candidates, getLevel(1));\n                if (Sets.intersection(l1overlapping, compacting).size() > 0)\n                    return Collections.emptyList();\n                if (!overlapping(candidates, compactingL0).isEmpty())\n                    return Collections.emptyList();\n                candidates = Sets.union(candidates, l1overlapping);\n            }\n            if (candidates.size() < 2)\n                return Collections.emptyList();\n            else\n                return candidates;\n        }\n\n        // for non-L0 compactions, pick up where we left off last time\n        Collections.sort(getLevel(level), SSTableReader.sstableComparator);\n        int start = 0; // handles case where the prior compaction touched the very last range\n        for (int i = 0; i < getLevel(level).size(); i++)\n        {\n            SSTableReader sstable = getLevel(level).get(i);\n            if (sstable.first.compareTo(lastCompactedKeys[level]) > 0)\n            {\n                start = i;\n                break;\n            }\n        }\n\n        // look for a non-suspect keyspace to compact with, starting with where we left off last time,\n        // and wrapping back to the beginning of the generation if necessary\n        for (int i = 0; i < getLevel(level).size(); i++)\n        {\n            SSTableReader sstable = getLevel(level).get((start + i) % getLevel(level).size());\n            Set<SSTableReader> candidates = Sets.union(Collections.singleton(sstable), overlapping(sstable, getLevel(level + 1)));\n            if (Iterables.any(candidates, suspectP))\n                continue;\n            if (Sets.intersection(candidates, compacting).isEmpty())\n                return candidates;\n        }\n\n        // all the sstables were suspect or overlapped with something suspect\n        return Collections.emptyList();\n    }"
        ],
        [
            "MessagingService::convict(InetAddress)",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447  \n 448  ",
            "    /**\n     * called from gossiper when it notices a node is not responding.\n     */\n    public void convict(InetAddress ep)\n    {\n        logger.debug(\"Resetting pool for {}\", ep);\n        getConnectionPool(ep).reset();\n    }",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446 +\n 447  \n 448  ",
            "    /**\n     * called from gossiper when it notices a node is not responding.\n     */\n    public void convict(InetAddress ep)\n    {\n        logger.trace(\"Resetting pool for {}\", ep);\n        getConnectionPool(ep).reset();\n    }"
        ],
        [
            "CommitLog::discardCompletedSegments(UUID,ReplayPosition)",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319 -\n 320  \n 321  \n 322  \n 323  \n 324 -\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  ",
            "    /**\n     * Modifies the per-CF dirty cursors of any commit log segments for the column family according to the position\n     * given. Discards any commit log segments that are no longer used.\n     *\n     * @param cfId    the column family ID that was flushed\n     * @param context the replay position of the flush\n     */\n    public void discardCompletedSegments(final UUID cfId, final ReplayPosition context)\n    {\n        logger.debug(\"discard completed log segments for {}, table {}\", context, cfId);\n\n        // Go thru the active segment files, which are ordered oldest to newest, marking the\n        // flushed CF as clean, until we reach the segment file containing the ReplayPosition passed\n        // in the arguments. Any segments that become unused after they are marked clean will be\n        // recycled or discarded.\n        for (Iterator<CommitLogSegment> iter = allocator.getActiveSegments().iterator(); iter.hasNext();)\n        {\n            CommitLogSegment segment = iter.next();\n            segment.markClean(cfId, context);\n\n            if (segment.isUnused())\n            {\n                logger.debug(\"Commit log segment {} is unused\", segment);\n                allocator.recycleSegment(segment);\n            }\n            else\n            {\n                logger.debug(\"Not safe to delete{} commit log segment {}; dirty is {}\",\n                        (iter.hasNext() ? \"\" : \" active\"), segment, segment.dirtyString());\n            }\n\n            // Don't mark or try to delete any newer segments once we've reached the one containing the\n            // position of the flush.\n            if (segment.contains(context))\n                break;\n        }\n    }",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319 +\n 320  \n 321  \n 322  \n 323  \n 324 +\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  ",
            "    /**\n     * Modifies the per-CF dirty cursors of any commit log segments for the column family according to the position\n     * given. Discards any commit log segments that are no longer used.\n     *\n     * @param cfId    the column family ID that was flushed\n     * @param context the replay position of the flush\n     */\n    public void discardCompletedSegments(final UUID cfId, final ReplayPosition context)\n    {\n        logger.trace(\"discard completed log segments for {}, table {}\", context, cfId);\n\n        // Go thru the active segment files, which are ordered oldest to newest, marking the\n        // flushed CF as clean, until we reach the segment file containing the ReplayPosition passed\n        // in the arguments. Any segments that become unused after they are marked clean will be\n        // recycled or discarded.\n        for (Iterator<CommitLogSegment> iter = allocator.getActiveSegments().iterator(); iter.hasNext();)\n        {\n            CommitLogSegment segment = iter.next();\n            segment.markClean(cfId, context);\n\n            if (segment.isUnused())\n            {\n                logger.trace(\"Commit log segment {} is unused\", segment);\n                allocator.recycleSegment(segment);\n            }\n            else\n            {\n                logger.trace(\"Not safe to delete{} commit log segment {}; dirty is {}\",\n                        (iter.hasNext() ? \"\" : \" active\"), segment, segment.dirtyString());\n            }\n\n            // Don't mark or try to delete any newer segments once we've reached the one containing the\n            // position of the flush.\n            if (segment.contains(context))\n                break;\n        }\n    }"
        ],
        [
            "IndexSummaryManager::redistributeSummaries(List,Map,long)",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279 -\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313 -\n 314  \n 315  \n 316  \n 317  ",
            "    /**\n     * Attempts to fairly distribute a fixed pool of memory for index summaries across a set of SSTables based on\n     * their recent read rates.\n     * @param transactions containing the sstables we are to redistribute the memory pool across\n     * @param memoryPoolBytes a size (in bytes) that the total index summary space usage should stay close to or\n     *                        under, if possible\n     * @return a list of new SSTableReader instances\n     */\n    @VisibleForTesting\n    public static List<SSTableReader> redistributeSummaries(List<SSTableReader> compacting, Map<UUID, LifecycleTransaction> transactions, long memoryPoolBytes) throws IOException\n    {\n        logger.info(\"Redistributing index summaries\");\n        List<SSTableReader> oldFormatSSTables = new ArrayList<>();\n        List<SSTableReader> redistribute = new ArrayList<>();\n        for (LifecycleTransaction txn : transactions.values())\n        {\n            for (SSTableReader sstable : ImmutableList.copyOf(txn.originals()))\n            {\n                // We can't change the sampling level of sstables with the old format, because the serialization format\n                // doesn't include the sampling level.  Leave this one as it is.  (See CASSANDRA-8993 for details.)\n                logger.trace(\"SSTable {} cannot be re-sampled due to old sstable format\", sstable);\n                if (!sstable.descriptor.version.hasSamplingLevel())\n                {\n                    oldFormatSSTables.add(sstable);\n                    txn.cancel(sstable);\n                }\n            }\n            redistribute.addAll(txn.originals());\n        }\n\n        long total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, redistribute))\n            total += sstable.getIndexSummaryOffHeapSize();\n\n        logger.debug(\"Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB\",\n                     redistribute.size(), memoryPoolBytes / 1024L / 1024L, total / 1024.0 / 1024.0);\n\n        final Map<SSTableReader, Double> readRates = new HashMap<>(redistribute.size());\n        double totalReadsPerSec = 0.0;\n        for (SSTableReader sstable : redistribute)\n        {\n            if (sstable.getReadMeter() != null)\n            {\n                Double readRate = sstable.getReadMeter().fifteenMinuteRate();\n                totalReadsPerSec += readRate;\n                readRates.put(sstable, readRate);\n            }\n        }\n        logger.trace(\"Total reads/sec across all sstables in index summary resize process: {}\", totalReadsPerSec);\n\n        // copy and sort by read rates (ascending)\n        List<SSTableReader> sstablesByHotness = new ArrayList<>(redistribute);\n        Collections.sort(sstablesByHotness, new ReadRateComparator(readRates));\n\n        long remainingBytes = memoryPoolBytes;\n        for (SSTableReader sstable : Iterables.concat(compacting, oldFormatSSTables))\n            remainingBytes -= sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Index summaries for compacting SSTables are using {} MB of space\",\n                     (memoryPoolBytes - remainingBytes) / 1024.0 / 1024.0);\n        List<SSTableReader> newSSTables = adjustSamplingLevels(sstablesByHotness, transactions, totalReadsPerSec, remainingBytes);\n\n        for (LifecycleTransaction txn : transactions.values())\n            txn.finish();\n\n        total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, oldFormatSSTables, newSSTables))\n            total += sstable.getIndexSummaryOffHeapSize();\n        logger.debug(\"Completed resizing of index summaries; current approximate memory used: {} MB\",\n                     total / 1024.0 / 1024.0);\n\n        return newSSTables;\n    }",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313 +\n 314  \n 315  \n 316  \n 317  ",
            "    /**\n     * Attempts to fairly distribute a fixed pool of memory for index summaries across a set of SSTables based on\n     * their recent read rates.\n     * @param transactions containing the sstables we are to redistribute the memory pool across\n     * @param memoryPoolBytes a size (in bytes) that the total index summary space usage should stay close to or\n     *                        under, if possible\n     * @return a list of new SSTableReader instances\n     */\n    @VisibleForTesting\n    public static List<SSTableReader> redistributeSummaries(List<SSTableReader> compacting, Map<UUID, LifecycleTransaction> transactions, long memoryPoolBytes) throws IOException\n    {\n        logger.info(\"Redistributing index summaries\");\n        List<SSTableReader> oldFormatSSTables = new ArrayList<>();\n        List<SSTableReader> redistribute = new ArrayList<>();\n        for (LifecycleTransaction txn : transactions.values())\n        {\n            for (SSTableReader sstable : ImmutableList.copyOf(txn.originals()))\n            {\n                // We can't change the sampling level of sstables with the old format, because the serialization format\n                // doesn't include the sampling level.  Leave this one as it is.  (See CASSANDRA-8993 for details.)\n                logger.trace(\"SSTable {} cannot be re-sampled due to old sstable format\", sstable);\n                if (!sstable.descriptor.version.hasSamplingLevel())\n                {\n                    oldFormatSSTables.add(sstable);\n                    txn.cancel(sstable);\n                }\n            }\n            redistribute.addAll(txn.originals());\n        }\n\n        long total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, redistribute))\n            total += sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB\",\n                     redistribute.size(), memoryPoolBytes / 1024L / 1024L, total / 1024.0 / 1024.0);\n\n        final Map<SSTableReader, Double> readRates = new HashMap<>(redistribute.size());\n        double totalReadsPerSec = 0.0;\n        for (SSTableReader sstable : redistribute)\n        {\n            if (sstable.getReadMeter() != null)\n            {\n                Double readRate = sstable.getReadMeter().fifteenMinuteRate();\n                totalReadsPerSec += readRate;\n                readRates.put(sstable, readRate);\n            }\n        }\n        logger.trace(\"Total reads/sec across all sstables in index summary resize process: {}\", totalReadsPerSec);\n\n        // copy and sort by read rates (ascending)\n        List<SSTableReader> sstablesByHotness = new ArrayList<>(redistribute);\n        Collections.sort(sstablesByHotness, new ReadRateComparator(readRates));\n\n        long remainingBytes = memoryPoolBytes;\n        for (SSTableReader sstable : Iterables.concat(compacting, oldFormatSSTables))\n            remainingBytes -= sstable.getIndexSummaryOffHeapSize();\n\n        logger.trace(\"Index summaries for compacting SSTables are using {} MB of space\",\n                     (memoryPoolBytes - remainingBytes) / 1024.0 / 1024.0);\n        List<SSTableReader> newSSTables = adjustSamplingLevels(sstablesByHotness, transactions, totalReadsPerSec, remainingBytes);\n\n        for (LifecycleTransaction txn : transactions.values())\n            txn.finish();\n\n        total = 0;\n        for (SSTableReader sstable : Iterables.concat(compacting, oldFormatSSTables, newSSTables))\n            total += sstable.getIndexSummaryOffHeapSize();\n        logger.trace(\"Completed resizing of index summaries; current approximate memory used: {} MB\",\n                     total / 1024.0 / 1024.0);\n\n        return newSSTables;\n    }"
        ],
        [
            "CompactionManager::BackgroundCompactionCandidate::run()",
            " 210  \n 211  \n 212  \n 213  \n 214 -\n 215  \n 216  \n 217 -\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "        public void run()\n        {\n            try\n            {\n                logger.debug(\"Checking {}.{}\", cfs.keyspace.getName(), cfs.name);\n                if (!cfs.isValid())\n                {\n                    logger.debug(\"Aborting compaction for dropped CF\");\n                    return;\n                }\n\n                AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n                AbstractCompactionTask task = strategy.getNextBackgroundTask(getDefaultGcBefore(cfs));\n                if (task == null)\n                {\n                    logger.debug(\"No tasks available\");\n                    return;\n                }\n                task.execute(metrics);\n            }\n            finally\n            {\n                compactingCF.remove(cfs);\n            }\n            submitBackground(cfs);\n        }",
            " 210  \n 211  \n 212  \n 213  \n 214 +\n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "        public void run()\n        {\n            try\n            {\n                logger.trace(\"Checking {}.{}\", cfs.keyspace.getName(), cfs.name);\n                if (!cfs.isValid())\n                {\n                    logger.trace(\"Aborting compaction for dropped CF\");\n                    return;\n                }\n\n                AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n                AbstractCompactionTask task = strategy.getNextBackgroundTask(getDefaultGcBefore(cfs));\n                if (task == null)\n                {\n                    logger.trace(\"No tasks available\");\n                    return;\n                }\n                task.execute(metrics);\n            }\n            finally\n            {\n                compactingCF.remove(cfs);\n            }\n            submitBackground(cfs);\n        }"
        ],
        [
            "ColumnFamilyStore::removeUnfinishedCompactionLeftovers(CFMetaData,Map)",
            " 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652 -\n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685 -\n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702 -\n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  ",
            "    /**\n     * Replacing compacted sstables is atomic as far as observers of Tracker are concerned, but not on the\n     * filesystem: first the new sstables are renamed to \"live\" status (i.e., the tmp marker is removed), then\n     * their ancestors are removed.\n     *\n     * If an unclean shutdown happens at the right time, we can thus end up with both the new ones and their\n     * ancestors \"live\" in the system.  This is harmless for normal data, but for counters it can cause overcounts.\n     *\n     * To prevent this, we record sstables being compacted in the system keyspace.  If we find unfinished\n     * compactions, we remove the new ones (since those may be incomplete -- under LCS, we may create multiple\n     * sstables from any given ancestor).\n     */\n    public static void removeUnfinishedCompactionLeftovers(CFMetaData metadata, Map<Integer, UUID> unfinishedCompactions)\n    {\n        Directories directories = new Directories(metadata);\n        Set<Integer> allGenerations = new HashSet<>();\n        for (Descriptor desc : directories.sstableLister().list().keySet())\n            allGenerations.add(desc.generation);\n\n        // sanity-check unfinishedCompactions\n        Set<Integer> unfinishedGenerations = unfinishedCompactions.keySet();\n        if (!allGenerations.containsAll(unfinishedGenerations))\n        {\n            HashSet<Integer> missingGenerations = new HashSet<>(unfinishedGenerations);\n            missingGenerations.removeAll(allGenerations);\n            logger.debug(\"Unfinished compactions of {}.{} reference missing sstables of generations {}\",\n                         metadata.ksName, metadata.cfName, missingGenerations);\n        }\n\n        // remove new sstables from compactions that didn't complete, and compute\n        // set of ancestors that shouldn't exist anymore\n        Set<Integer> completedAncestors = new HashSet<>();\n        for (Map.Entry<Descriptor, Set<Component>> sstableFiles : directories.sstableLister().skipTemporary(true).list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n\n            Set<Integer> ancestors;\n            try\n            {\n                CompactionMetadata compactionMetadata = (CompactionMetadata) desc.getMetadataSerializer().deserialize(desc, MetadataType.COMPACTION);\n                ancestors = compactionMetadata.ancestors;\n            }\n            catch (IOException e)\n            {\n                throw new FSReadError(e, desc.filenameFor(Component.STATS));\n            }\n            catch (NullPointerException e)\n            {\n                throw new FSReadError(e, \"Failed to remove unfinished compaction leftovers (file: \" + desc.filenameFor(Component.STATS) + \").  See log for details.\");\n            }\n\n            if (!ancestors.isEmpty()\n                && unfinishedGenerations.containsAll(ancestors)\n                && allGenerations.containsAll(ancestors))\n            {\n                // any of the ancestors would work, so we'll just lookup the compaction task ID with the first one\n                UUID compactionTaskID = unfinishedCompactions.get(ancestors.iterator().next());\n                assert compactionTaskID != null;\n                logger.debug(\"Going to delete unfinished compaction product {}\", desc);\n                SSTable.delete(desc, sstableFiles.getValue());\n                SystemKeyspace.finishCompaction(compactionTaskID);\n            }\n            else\n            {\n                completedAncestors.addAll(ancestors);\n            }\n        }\n\n        // remove old sstables from compactions that did complete\n        for (Map.Entry<Descriptor, Set<Component>> sstableFiles : directories.sstableLister().list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            if (completedAncestors.contains(desc.generation))\n            {\n                // if any of the ancestors were participating in a compaction, finish that compaction\n                logger.debug(\"Going to delete leftover compaction ancestor {}\", desc);\n                SSTable.delete(desc, sstableFiles.getValue());\n                UUID compactionTaskID = unfinishedCompactions.get(desc.generation);\n                if (compactionTaskID != null)\n                    SystemKeyspace.finishCompaction(unfinishedCompactions.get(desc.generation));\n            }\n        }\n    }",
            " 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652 +\n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685 +\n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702 +\n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  ",
            "    /**\n     * Replacing compacted sstables is atomic as far as observers of Tracker are concerned, but not on the\n     * filesystem: first the new sstables are renamed to \"live\" status (i.e., the tmp marker is removed), then\n     * their ancestors are removed.\n     *\n     * If an unclean shutdown happens at the right time, we can thus end up with both the new ones and their\n     * ancestors \"live\" in the system.  This is harmless for normal data, but for counters it can cause overcounts.\n     *\n     * To prevent this, we record sstables being compacted in the system keyspace.  If we find unfinished\n     * compactions, we remove the new ones (since those may be incomplete -- under LCS, we may create multiple\n     * sstables from any given ancestor).\n     */\n    public static void removeUnfinishedCompactionLeftovers(CFMetaData metadata, Map<Integer, UUID> unfinishedCompactions)\n    {\n        Directories directories = new Directories(metadata);\n        Set<Integer> allGenerations = new HashSet<>();\n        for (Descriptor desc : directories.sstableLister().list().keySet())\n            allGenerations.add(desc.generation);\n\n        // sanity-check unfinishedCompactions\n        Set<Integer> unfinishedGenerations = unfinishedCompactions.keySet();\n        if (!allGenerations.containsAll(unfinishedGenerations))\n        {\n            HashSet<Integer> missingGenerations = new HashSet<>(unfinishedGenerations);\n            missingGenerations.removeAll(allGenerations);\n            logger.trace(\"Unfinished compactions of {}.{} reference missing sstables of generations {}\",\n                         metadata.ksName, metadata.cfName, missingGenerations);\n        }\n\n        // remove new sstables from compactions that didn't complete, and compute\n        // set of ancestors that shouldn't exist anymore\n        Set<Integer> completedAncestors = new HashSet<>();\n        for (Map.Entry<Descriptor, Set<Component>> sstableFiles : directories.sstableLister().skipTemporary(true).list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n\n            Set<Integer> ancestors;\n            try\n            {\n                CompactionMetadata compactionMetadata = (CompactionMetadata) desc.getMetadataSerializer().deserialize(desc, MetadataType.COMPACTION);\n                ancestors = compactionMetadata.ancestors;\n            }\n            catch (IOException e)\n            {\n                throw new FSReadError(e, desc.filenameFor(Component.STATS));\n            }\n            catch (NullPointerException e)\n            {\n                throw new FSReadError(e, \"Failed to remove unfinished compaction leftovers (file: \" + desc.filenameFor(Component.STATS) + \").  See log for details.\");\n            }\n\n            if (!ancestors.isEmpty()\n                && unfinishedGenerations.containsAll(ancestors)\n                && allGenerations.containsAll(ancestors))\n            {\n                // any of the ancestors would work, so we'll just lookup the compaction task ID with the first one\n                UUID compactionTaskID = unfinishedCompactions.get(ancestors.iterator().next());\n                assert compactionTaskID != null;\n                logger.trace(\"Going to delete unfinished compaction product {}\", desc);\n                SSTable.delete(desc, sstableFiles.getValue());\n                SystemKeyspace.finishCompaction(compactionTaskID);\n            }\n            else\n            {\n                completedAncestors.addAll(ancestors);\n            }\n        }\n\n        // remove old sstables from compactions that did complete\n        for (Map.Entry<Descriptor, Set<Component>> sstableFiles : directories.sstableLister().list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            if (completedAncestors.contains(desc.generation))\n            {\n                // if any of the ancestors were participating in a compaction, finish that compaction\n                logger.trace(\"Going to delete leftover compaction ancestor {}\", desc);\n                SSTable.delete(desc, sstableFiles.getValue());\n                UUID compactionTaskID = unfinishedCompactions.get(desc.generation);\n                if (compactionTaskID != null)\n                    SystemKeyspace.finishCompaction(unfinishedCompactions.get(desc.generation));\n            }\n        }\n    }"
        ],
        [
            "TokenMetadata::calculatePendingRanges(AbstractReplicationStrategy,String)",
            " 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740 -\n 741 -\n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805 -\n 806 -\n 807  \n 808  \n 809  \n 810  \n 811  \n 812  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            Multimap<Range<Token>, InetAddress> newPendingRanges = HashMultimap.create();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isDebugEnabled())\n                    logger.debug(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                newPendingRanges.putAll(range, Sets.difference(newEndpoints, currentEndpoints));\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                    newPendingRanges.put(range, endpoint);\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.put(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isDebugEnabled())\n                logger.debug(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }",
            " 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740 +\n 741 +\n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805 +\n 806 +\n 807  \n 808  \n 809  \n 810  \n 811  \n 812  ",
            "     /**\n     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:\n     *\n     * (1) When in doubt, it is better to write too much to a node than too little. That is, if\n     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning\n     * up unneeded data afterwards is better than missing writes during movement.\n     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional\n     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore\n     * we will first remove _all_ leaving tokens for the sake of calculation and then check what\n     * ranges would go where if all nodes are to leave. This way we get the biggest possible\n     * ranges with regard current leave operations, covering all subsets of possible final range\n     * values.\n     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing\n     * complex calculations to see if multiple bootstraps overlap, we simply base calculations\n     * on the same token ring used before (reflecting situation after all leave operations have\n     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and\n     * checked what their ranges would be. This will give us the biggest possible ranges the\n     * node could have. It might be that other bootstraps make our actual final ranges smaller,\n     * but it does not matter as we can clean up the data afterwards.\n     *\n     * NOTE: This is heavy and ineffective operation. This will be done only once when a node\n     * changes state in the cluster, so it should be manageable.\n     */\n    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)\n    {\n        lock.readLock().lock();\n        try\n        {\n            Multimap<Range<Token>, InetAddress> newPendingRanges = HashMultimap.create();\n\n            if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}\", keyspaceName);\n\n                pendingRanges.put(keyspaceName, newPendingRanges);\n                return;\n            }\n\n            Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();\n\n            // Copy of metadata reflecting the situation after all leave operations are finished.\n            TokenMetadata allLeftMetadata = cloneAfterAllLeft();\n\n            // get all ranges that will be affected by leaving nodes\n            Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();\n            for (InetAddress endpoint : leavingEndpoints)\n                affectedRanges.addAll(addressRanges.get(endpoint));\n\n            // for each of those ranges, find what new nodes will be responsible for the range when\n            // all leaving nodes are gone.\n            TokenMetadata metadata = cloneOnlyTokenMap(); // don't do this in the loop! #7758\n            for (Range<Token> range : affectedRanges)\n            {\n                Set<InetAddress> currentEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, metadata));\n                Set<InetAddress> newEndpoints = ImmutableSet.copyOf(strategy.calculateNaturalEndpoints(range.right, allLeftMetadata));\n                newPendingRanges.putAll(range, Sets.difference(newEndpoints, currentEndpoints));\n            }\n\n            // At this stage newPendingRanges has been updated according to leave operations. We can\n            // now continue the calculation by checking bootstrapping nodes.\n\n            // For each of the bootstrapping nodes, simply add and remove them one by one to\n            // allLeftMetadata and check in between what their ranges would be.\n            Multimap<InetAddress, Token> bootstrapAddresses = bootstrapTokens.inverse();\n            for (InetAddress endpoint : bootstrapAddresses.keySet())\n            {\n                Collection<Token> tokens = bootstrapAddresses.get(endpoint);\n\n                allLeftMetadata.updateNormalTokens(tokens, endpoint);\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                    newPendingRanges.put(range, endpoint);\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.\n            // We can now finish the calculation by checking moving nodes.\n\n            // For each of the moving nodes, we do the same thing we did for bootstrapping:\n            // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.\n            for (Pair<Token, InetAddress> moving : movingEndpoints)\n            {\n                InetAddress endpoint = moving.right; // address of the moving node\n\n                //  moving.left is a new token of the endpoint\n                allLeftMetadata.updateNormalToken(moving.left, endpoint);\n\n                for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))\n                {\n                    newPendingRanges.put(range, endpoint);\n                }\n\n                allLeftMetadata.removeEndpoint(endpoint);\n            }\n\n            pendingRanges.put(keyspaceName, newPendingRanges);\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Pending ranges:\\n{}\", (pendingRanges.isEmpty() ? \"<empty>\" : printPendingRanges()));\n        }\n        finally\n        {\n            lock.readLock().unlock();\n        }\n    }"
        ],
        [
            "HintedHandOffManager::doDeliverHintsToEndpoint(InetAddress)",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437 -\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450 -\n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "    private void doDeliverHintsToEndpoint(InetAddress endpoint)\n    {\n        // find the hints for the node using its token.\n        UUID hostId = Gossiper.instance.getHostId(endpoint);\n        logger.info(\"Started hinted handoff for host: {} with IP: {}\", hostId, endpoint);\n        final ByteBuffer hostIdBytes = ByteBuffer.wrap(UUIDGen.decompose(hostId));\n        DecoratedKey epkey =  StorageService.getPartitioner().decorateKey(hostIdBytes);\n\n        final AtomicInteger rowsReplayed = new AtomicInteger(0);\n        Composite startColumn = Composites.EMPTY;\n\n        int pageSize = calculatePageSize();\n        logger.debug(\"Using pageSize of {}\", pageSize);\n\n        // rate limit is in bytes per second. Uses Double.MAX_VALUE if disabled (set to 0 in cassandra.yaml).\n        // max rate is scaled by the number of nodes in the cluster (CASSANDRA-5272).\n        int throttleInKB = DatabaseDescriptor.getHintedHandoffThrottleInKB()\n                           / (StorageService.instance.getTokenMetadata().getAllEndpoints().size() - 1);\n        RateLimiter rateLimiter = RateLimiter.create(throttleInKB == 0 ? Double.MAX_VALUE : throttleInKB * 1024);\n\n        delivery:\n        while (true)\n        {\n            long now = System.currentTimeMillis();\n            QueryFilter filter = QueryFilter.getSliceFilter(epkey,\n                                                            SystemKeyspace.HINTS,\n                                                            startColumn,\n                                                            Composites.EMPTY,\n                                                            false,\n                                                            pageSize,\n                                                            now);\n\n            ColumnFamily hintsPage = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(filter), (int) (now / 1000));\n\n            if (pagingFinished(hintsPage, startColumn))\n            {\n                logger.info(\"Finished hinted handoff of {} rows to endpoint {}\", rowsReplayed, endpoint);\n                break;\n            }\n\n            // check if node is still alive and we should continue delivery process\n            if (!FailureDetector.instance.isAlive(endpoint))\n            {\n                logger.info(\"Endpoint {} died during hint delivery; aborting ({} delivered)\", endpoint, rowsReplayed);\n                break;\n            }\n\n            List<WriteResponseHandler<Mutation>> responseHandlers = Lists.newArrayList();\n            for (final Cell hint : hintsPage)\n            {\n                // check if hints delivery has been paused during the process\n                if (hintedHandOffPaused)\n                {\n                    logger.debug(\"Hints delivery process is paused, aborting\");\n                    break delivery;\n                }\n\n                // Skip tombstones:\n                // if we iterate quickly enough, it's possible that we could request a new page in the same millisecond\n                // in which the local deletion timestamp was generated on the last column in the old page, in which\n                // case the hint will have no columns (since it's deleted) but will still be included in the resultset\n                // since (even with gcgs=0) it's still a \"relevant\" tombstone.\n                if (!hint.isLive())\n                    continue;\n\n                startColumn = hint.name();\n\n                int version = Int32Type.instance.compose(hint.name().get(1));\n                DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(hint.value()));\n                Mutation mutation;\n                try\n                {\n                    mutation = Mutation.serializer.deserialize(in, version);\n                }\n                catch (UnknownColumnFamilyException e)\n                {\n                    logger.debug(\"Skipping delivery of hint for deleted table\", e);\n                    deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    continue;\n                }\n                catch (IOException e)\n                {\n                    throw new AssertionError(e);\n                }\n\n                for (UUID cfId : mutation.getColumnFamilyIds())\n                {\n                    if (hint.timestamp() <= SystemKeyspace.getTruncatedAt(cfId))\n                    {\n                        logger.debug(\"Skipping delivery of hint for truncated table {}\", cfId);\n                        mutation = mutation.without(cfId);\n                    }\n                }\n\n                if (mutation.isEmpty())\n                {\n                    deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    continue;\n                }\n\n                MessageOut<Mutation> message = mutation.createMessage();\n                rateLimiter.acquire(message.serializedSize(MessagingService.current_version));\n                Runnable callback = new Runnable()\n                {\n                    public void run()\n                    {\n                        rowsReplayed.incrementAndGet();\n                        deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    }\n                };\n                WriteResponseHandler<Mutation> responseHandler = new WriteResponseHandler<>(endpoint, WriteType.SIMPLE, callback);\n                MessagingService.instance().sendRR(message, endpoint, responseHandler, false);\n                responseHandlers.add(responseHandler);\n            }\n\n            for (WriteResponseHandler<Mutation> handler : responseHandlers)\n            {\n                try\n                {\n                    handler.get();\n                }\n                catch (WriteTimeoutException|WriteFailureException e)\n                {\n                    logger.info(\"Failed replaying hints to {}; aborting ({} delivered), error : {}\",\n                        endpoint, rowsReplayed, e.getMessage());\n                    break delivery;\n                }\n            }\n        }\n\n        // Flush all the tombstones to disk\n        hintStore.forceBlockingFlush();\n    }",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 +\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 +\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437 +\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450 +\n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "    private void doDeliverHintsToEndpoint(InetAddress endpoint)\n    {\n        // find the hints for the node using its token.\n        UUID hostId = Gossiper.instance.getHostId(endpoint);\n        logger.info(\"Started hinted handoff for host: {} with IP: {}\", hostId, endpoint);\n        final ByteBuffer hostIdBytes = ByteBuffer.wrap(UUIDGen.decompose(hostId));\n        DecoratedKey epkey =  StorageService.getPartitioner().decorateKey(hostIdBytes);\n\n        final AtomicInteger rowsReplayed = new AtomicInteger(0);\n        Composite startColumn = Composites.EMPTY;\n\n        int pageSize = calculatePageSize();\n        logger.trace(\"Using pageSize of {}\", pageSize);\n\n        // rate limit is in bytes per second. Uses Double.MAX_VALUE if disabled (set to 0 in cassandra.yaml).\n        // max rate is scaled by the number of nodes in the cluster (CASSANDRA-5272).\n        int throttleInKB = DatabaseDescriptor.getHintedHandoffThrottleInKB()\n                           / (StorageService.instance.getTokenMetadata().getAllEndpoints().size() - 1);\n        RateLimiter rateLimiter = RateLimiter.create(throttleInKB == 0 ? Double.MAX_VALUE : throttleInKB * 1024);\n\n        delivery:\n        while (true)\n        {\n            long now = System.currentTimeMillis();\n            QueryFilter filter = QueryFilter.getSliceFilter(epkey,\n                                                            SystemKeyspace.HINTS,\n                                                            startColumn,\n                                                            Composites.EMPTY,\n                                                            false,\n                                                            pageSize,\n                                                            now);\n\n            ColumnFamily hintsPage = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(filter), (int) (now / 1000));\n\n            if (pagingFinished(hintsPage, startColumn))\n            {\n                logger.info(\"Finished hinted handoff of {} rows to endpoint {}\", rowsReplayed, endpoint);\n                break;\n            }\n\n            // check if node is still alive and we should continue delivery process\n            if (!FailureDetector.instance.isAlive(endpoint))\n            {\n                logger.info(\"Endpoint {} died during hint delivery; aborting ({} delivered)\", endpoint, rowsReplayed);\n                break;\n            }\n\n            List<WriteResponseHandler<Mutation>> responseHandlers = Lists.newArrayList();\n            for (final Cell hint : hintsPage)\n            {\n                // check if hints delivery has been paused during the process\n                if (hintedHandOffPaused)\n                {\n                    logger.trace(\"Hints delivery process is paused, aborting\");\n                    break delivery;\n                }\n\n                // Skip tombstones:\n                // if we iterate quickly enough, it's possible that we could request a new page in the same millisecond\n                // in which the local deletion timestamp was generated on the last column in the old page, in which\n                // case the hint will have no columns (since it's deleted) but will still be included in the resultset\n                // since (even with gcgs=0) it's still a \"relevant\" tombstone.\n                if (!hint.isLive())\n                    continue;\n\n                startColumn = hint.name();\n\n                int version = Int32Type.instance.compose(hint.name().get(1));\n                DataInputStream in = new DataInputStream(ByteBufferUtil.inputStream(hint.value()));\n                Mutation mutation;\n                try\n                {\n                    mutation = Mutation.serializer.deserialize(in, version);\n                }\n                catch (UnknownColumnFamilyException e)\n                {\n                    logger.trace(\"Skipping delivery of hint for deleted table\", e);\n                    deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    continue;\n                }\n                catch (IOException e)\n                {\n                    throw new AssertionError(e);\n                }\n\n                for (UUID cfId : mutation.getColumnFamilyIds())\n                {\n                    if (hint.timestamp() <= SystemKeyspace.getTruncatedAt(cfId))\n                    {\n                        logger.trace(\"Skipping delivery of hint for truncated table {}\", cfId);\n                        mutation = mutation.without(cfId);\n                    }\n                }\n\n                if (mutation.isEmpty())\n                {\n                    deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    continue;\n                }\n\n                MessageOut<Mutation> message = mutation.createMessage();\n                rateLimiter.acquire(message.serializedSize(MessagingService.current_version));\n                Runnable callback = new Runnable()\n                {\n                    public void run()\n                    {\n                        rowsReplayed.incrementAndGet();\n                        deleteHint(hostIdBytes, hint.name(), hint.timestamp());\n                    }\n                };\n                WriteResponseHandler<Mutation> responseHandler = new WriteResponseHandler<>(endpoint, WriteType.SIMPLE, callback);\n                MessagingService.instance().sendRR(message, endpoint, responseHandler, false);\n                responseHandlers.add(responseHandler);\n            }\n\n            for (WriteResponseHandler<Mutation> handler : responseHandlers)\n            {\n                try\n                {\n                    handler.get();\n                }\n                catch (WriteTimeoutException|WriteFailureException e)\n                {\n                    logger.info(\"Failed replaying hints to {}; aborting ({} delivered), error : {}\",\n                        endpoint, rowsReplayed, e.getMessage());\n                    break delivery;\n                }\n            }\n        }\n\n        // Flush all the tombstones to disk\n        hintStore.forceBlockingFlush();\n    }"
        ],
        [
            "FileUtils::deleteRecursiveOnExit(File)",
            " 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396 -\n 397  \n 398  ",
            "    /**\n     * Schedules deletion of all file and subdirectories under \"dir\" on JVM shutdown.\n     * @param dir Directory to be deleted\n     */\n    public static void deleteRecursiveOnExit(File dir)\n    {\n        if (dir.isDirectory())\n        {\n            String[] children = dir.list();\n            for (String child : children)\n                deleteRecursiveOnExit(new File(dir, child));\n        }\n\n        logger.debug(\"Scheduling deferred deletion of file: \" + dir);\n        dir.deleteOnExit();\n    }",
            " 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396 +\n 397  \n 398  ",
            "    /**\n     * Schedules deletion of all file and subdirectories under \"dir\" on JVM shutdown.\n     * @param dir Directory to be deleted\n     */\n    public static void deleteRecursiveOnExit(File dir)\n    {\n        if (dir.isDirectory())\n        {\n            String[] children = dir.list();\n            for (String child : children)\n                deleteRecursiveOnExit(new File(dir, child));\n        }\n\n        logger.trace(\"Scheduling deferred deletion of file: \" + dir);\n        dir.deleteOnExit();\n    }"
        ],
        [
            "CassandraStorage::getColumnMeta(Cassandra,boolean,boolean)",
            " 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031 -\n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  ",
            "    /** get column meta data */\n    protected List<ColumnDef> getColumnMeta(Cassandra.Client client, boolean cassandraStorage, boolean includeCompactValueColumn)\n            throws org.apache.cassandra.thrift.InvalidRequestException,\n            UnavailableException,\n            TimedOutException,\n            SchemaDisagreementException,\n            TException,\n            CharacterCodingException,\n            org.apache.cassandra.exceptions.InvalidRequestException,\n            ConfigurationException,\n            NotFoundException\n    {\n        String query = String.format(\"SELECT column_name, validator, index_type, type \" +\n                        \"FROM %s.%s \" +\n                        \"WHERE keyspace_name = '%s' AND columnfamily_name = '%s'\",\n                SystemKeyspace.NAME,\n                LegacySchemaTables.COLUMNS,\n                keyspace,\n                column_family);\n\n        CqlResult result = client.execute_cql3_query(ByteBufferUtil.bytes(query), Compression.NONE, ConsistencyLevel.ONE);\n\n        List<CqlRow> rows = result.rows;\n        List<ColumnDef> columnDefs = new ArrayList<ColumnDef>();\n        if (rows == null || rows.isEmpty())\n        {\n            // if CassandraStorage, just return the empty list\n            if (cassandraStorage)\n                return columnDefs;\n\n            // otherwise for CqlNativeStorage, check metadata for classic thrift tables\n            CFMetaData cfm = getCFMetaData(keyspace, column_family, client);\n            for (ColumnDefinition def : cfm.regularAndStaticColumns())\n            {\n                ColumnDef cDef = new ColumnDef();\n                String columnName = def.name.toString();\n                String type = def.type.toString();\n                logger.debug(\"name: {}, type: {} \", columnName, type);\n                cDef.name = ByteBufferUtil.bytes(columnName);\n                cDef.validation_class = type;\n                columnDefs.add(cDef);\n            }\n            // we may not need to include the value column for compact tables as we\n            // could have already processed it as schema_columnfamilies.value_alias\n            if (columnDefs.size() == 0 && includeCompactValueColumn && cfm.compactValueColumn() != null)\n            {\n                ColumnDefinition def = cfm.compactValueColumn();\n                if (\"value\".equals(def.name.toString()))\n                {\n                    ColumnDef cDef = new ColumnDef();\n                    cDef.name = def.name.bytes;\n                    cDef.validation_class = def.type.toString();\n                    columnDefs.add(cDef);\n                }\n            }\n            return columnDefs;\n        }\n\n        Iterator<CqlRow> iterator = rows.iterator();\n        while (iterator.hasNext())\n        {\n            CqlRow row = iterator.next();\n            ColumnDef cDef = new ColumnDef();\n            String type = ByteBufferUtil.string(row.getColumns().get(3).value);\n            if (!type.equals(\"regular\"))\n                continue;\n            cDef.setName(ByteBufferUtil.clone(row.getColumns().get(0).value));\n            cDef.validation_class = ByteBufferUtil.string(row.getColumns().get(1).value);\n            ByteBuffer indexType = row.getColumns().get(2).value;\n            if (indexType != null)\n                cDef.index_type = getIndexType(ByteBufferUtil.string(indexType));\n            columnDefs.add(cDef);\n        }\n        return columnDefs;\n    }",
            " 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031 +\n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  ",
            "    /** get column meta data */\n    protected List<ColumnDef> getColumnMeta(Cassandra.Client client, boolean cassandraStorage, boolean includeCompactValueColumn)\n            throws org.apache.cassandra.thrift.InvalidRequestException,\n            UnavailableException,\n            TimedOutException,\n            SchemaDisagreementException,\n            TException,\n            CharacterCodingException,\n            org.apache.cassandra.exceptions.InvalidRequestException,\n            ConfigurationException,\n            NotFoundException\n    {\n        String query = String.format(\"SELECT column_name, validator, index_type, type \" +\n                        \"FROM %s.%s \" +\n                        \"WHERE keyspace_name = '%s' AND columnfamily_name = '%s'\",\n                SystemKeyspace.NAME,\n                LegacySchemaTables.COLUMNS,\n                keyspace,\n                column_family);\n\n        CqlResult result = client.execute_cql3_query(ByteBufferUtil.bytes(query), Compression.NONE, ConsistencyLevel.ONE);\n\n        List<CqlRow> rows = result.rows;\n        List<ColumnDef> columnDefs = new ArrayList<ColumnDef>();\n        if (rows == null || rows.isEmpty())\n        {\n            // if CassandraStorage, just return the empty list\n            if (cassandraStorage)\n                return columnDefs;\n\n            // otherwise for CqlNativeStorage, check metadata for classic thrift tables\n            CFMetaData cfm = getCFMetaData(keyspace, column_family, client);\n            for (ColumnDefinition def : cfm.regularAndStaticColumns())\n            {\n                ColumnDef cDef = new ColumnDef();\n                String columnName = def.name.toString();\n                String type = def.type.toString();\n                logger.trace(\"name: {}, type: {} \", columnName, type);\n                cDef.name = ByteBufferUtil.bytes(columnName);\n                cDef.validation_class = type;\n                columnDefs.add(cDef);\n            }\n            // we may not need to include the value column for compact tables as we\n            // could have already processed it as schema_columnfamilies.value_alias\n            if (columnDefs.size() == 0 && includeCompactValueColumn && cfm.compactValueColumn() != null)\n            {\n                ColumnDefinition def = cfm.compactValueColumn();\n                if (\"value\".equals(def.name.toString()))\n                {\n                    ColumnDef cDef = new ColumnDef();\n                    cDef.name = def.name.bytes;\n                    cDef.validation_class = def.type.toString();\n                    columnDefs.add(cDef);\n                }\n            }\n            return columnDefs;\n        }\n\n        Iterator<CqlRow> iterator = rows.iterator();\n        while (iterator.hasNext())\n        {\n            CqlRow row = iterator.next();\n            ColumnDef cDef = new ColumnDef();\n            String type = ByteBufferUtil.string(row.getColumns().get(3).value);\n            if (!type.equals(\"regular\"))\n                continue;\n            cDef.setName(ByteBufferUtil.clone(row.getColumns().get(0).value));\n            cDef.validation_class = ByteBufferUtil.string(row.getColumns().get(1).value);\n            ByteBuffer indexType = row.getColumns().get(2).value;\n            if (indexType != null)\n                cDef.index_type = getIndexType(ByteBufferUtil.string(indexType));\n            columnDefs.add(cDef);\n        }\n        return columnDefs;\n    }"
        ],
        [
            "NetworkTopologyStrategy::NetworkTopologyStrategy(String,TokenMetadata,IEndpointSnitch,Map)",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 -\n  75  ",
            "    public NetworkTopologyStrategy(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions) throws ConfigurationException\n    {\n        super(keyspaceName, tokenMetadata, snitch, configOptions);\n        this.snitch = snitch;\n\n        Map<String, Integer> newDatacenters = new HashMap<String, Integer>();\n        if (configOptions != null)\n        {\n            for (Entry<String, String> entry : configOptions.entrySet())\n            {\n                String dc = entry.getKey();\n                if (dc.equalsIgnoreCase(\"replication_factor\"))\n                    throw new ConfigurationException(\"replication_factor is an option for SimpleStrategy, not NetworkTopologyStrategy\");\n                Integer replicas = Integer.valueOf(entry.getValue());\n                newDatacenters.put(dc, replicas);\n            }\n        }\n\n        datacenters = Collections.unmodifiableMap(newDatacenters);\n        logger.debug(\"Configured datacenter replicas are {}\", FBUtilities.toString(datacenters));\n    }",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75  ",
            "    public NetworkTopologyStrategy(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions) throws ConfigurationException\n    {\n        super(keyspaceName, tokenMetadata, snitch, configOptions);\n        this.snitch = snitch;\n\n        Map<String, Integer> newDatacenters = new HashMap<String, Integer>();\n        if (configOptions != null)\n        {\n            for (Entry<String, String> entry : configOptions.entrySet())\n            {\n                String dc = entry.getKey();\n                if (dc.equalsIgnoreCase(\"replication_factor\"))\n                    throw new ConfigurationException(\"replication_factor is an option for SimpleStrategy, not NetworkTopologyStrategy\");\n                Integer replicas = Integer.valueOf(entry.getValue());\n                newDatacenters.put(dc, replicas);\n            }\n        }\n\n        datacenters = Collections.unmodifiableMap(newDatacenters);\n        logger.trace(\"Configured datacenter replicas are {}\", FBUtilities.toString(datacenters));\n    }"
        ],
        [
            "ColumnFamilyStore::createEphemeralSnapshotMarkerFile(String)",
            "2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376 -\n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  ",
            "    private void createEphemeralSnapshotMarkerFile(final String snapshot)\n    {\n        final File ephemeralSnapshotMarker = directories.getNewEphemeralSnapshotMarkerFile(snapshot);\n\n        try\n        {\n            if (!ephemeralSnapshotMarker.getParentFile().exists())\n                ephemeralSnapshotMarker.getParentFile().mkdirs();\n\n            Files.createFile(ephemeralSnapshotMarker.toPath());\n            logger.debug(\"Created ephemeral snapshot marker file on {}.\", ephemeralSnapshotMarker.getAbsolutePath());\n        }\n        catch (IOException e)\n        {\n            logger.warn(String.format(\"Could not create marker file %s for ephemeral snapshot %s. \" +\n                                      \"In case there is a failure in the operation that created \" +\n                                      \"this snapshot, you may need to clean it manually afterwards.\",\n                                      ephemeralSnapshotMarker.getAbsolutePath(), snapshot), e);\n        }\n    }",
            "2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376 +\n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  ",
            "    private void createEphemeralSnapshotMarkerFile(final String snapshot)\n    {\n        final File ephemeralSnapshotMarker = directories.getNewEphemeralSnapshotMarkerFile(snapshot);\n\n        try\n        {\n            if (!ephemeralSnapshotMarker.getParentFile().exists())\n                ephemeralSnapshotMarker.getParentFile().mkdirs();\n\n            Files.createFile(ephemeralSnapshotMarker.toPath());\n            logger.trace(\"Created ephemeral snapshot marker file on {}.\", ephemeralSnapshotMarker.getAbsolutePath());\n        }\n        catch (IOException e)\n        {\n            logger.warn(String.format(\"Could not create marker file %s for ephemeral snapshot %s. \" +\n                                      \"In case there is a failure in the operation that created \" +\n                                      \"this snapshot, you may need to clean it manually afterwards.\",\n                                      ephemeralSnapshotMarker.getAbsolutePath(), snapshot), e);\n        }\n    }"
        ],
        [
            "HintedHandOffManager::start()",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "    public void start()\n    {\n        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n        try\n        {\n            mbs.registerMBean(this, new ObjectName(MBEAN_NAME));\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n        logger.debug(\"Created HHOM instance, registered MBean.\");\n\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                scheduleAllDeliveries();\n                metrics.log();\n            }\n        };\n        executor.scheduleWithFixedDelay(runnable, 10, 10, TimeUnit.MINUTES);\n    }",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "    public void start()\n    {\n        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n        try\n        {\n            mbs.registerMBean(this, new ObjectName(MBEAN_NAME));\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n        logger.trace(\"Created HHOM instance, registered MBean.\");\n\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                scheduleAllDeliveries();\n                metrics.log();\n            }\n        };\n        executor.scheduleWithFixedDelay(runnable, 10, 10, TimeUnit.MINUTES);\n    }"
        ],
        [
            "RolesCache::initCache(LoadingCache)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    private LoadingCache<RoleResource, Set<RoleResource>> initCache(LoadingCache<RoleResource, Set<RoleResource>> existing)\n    {\n        if (DatabaseDescriptor.getAuthenticator() instanceof AllowAllAuthenticator)\n            return null;\n\n        if (DatabaseDescriptor.getRolesValidity() <= 0)\n            return null;\n\n        LoadingCache<RoleResource, Set<RoleResource>> newcache = CacheBuilder.newBuilder()\n                .refreshAfterWrite(DatabaseDescriptor.getRolesUpdateInterval(), TimeUnit.MILLISECONDS)\n                .expireAfterWrite(DatabaseDescriptor.getRolesValidity(), TimeUnit.MILLISECONDS)\n                .maximumSize(DatabaseDescriptor.getRolesCacheMaxEntries())\n                .build(new CacheLoader<RoleResource, Set<RoleResource>>()\n                {\n                    public Set<RoleResource> load(RoleResource primaryRole)\n                    {\n                        return roleManager.getRoles(primaryRole, true);\n                    }\n\n                    public ListenableFuture<Set<RoleResource>> reload(final RoleResource primaryRole,\n                                                                      final Set<RoleResource> oldValue)\n                    {\n                        ListenableFutureTask<Set<RoleResource>> task;\n                        task = ListenableFutureTask.create(new Callable<Set<RoleResource>>()\n                        {\n                            public Set<RoleResource> call() throws Exception\n                            {\n                                try\n                                {\n                                    return roleManager.getRoles(primaryRole, true);\n                                } catch (Exception e)\n                                {\n                                    logger.debug(\"Error performing async refresh of user roles\", e);\n                                    throw e;\n                                }\n                            }\n                        });\n                        cacheRefreshExecutor.execute(task);\n                        return task;\n                    }\n                });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    private LoadingCache<RoleResource, Set<RoleResource>> initCache(LoadingCache<RoleResource, Set<RoleResource>> existing)\n    {\n        if (DatabaseDescriptor.getAuthenticator() instanceof AllowAllAuthenticator)\n            return null;\n\n        if (DatabaseDescriptor.getRolesValidity() <= 0)\n            return null;\n\n        LoadingCache<RoleResource, Set<RoleResource>> newcache = CacheBuilder.newBuilder()\n                .refreshAfterWrite(DatabaseDescriptor.getRolesUpdateInterval(), TimeUnit.MILLISECONDS)\n                .expireAfterWrite(DatabaseDescriptor.getRolesValidity(), TimeUnit.MILLISECONDS)\n                .maximumSize(DatabaseDescriptor.getRolesCacheMaxEntries())\n                .build(new CacheLoader<RoleResource, Set<RoleResource>>()\n                {\n                    public Set<RoleResource> load(RoleResource primaryRole)\n                    {\n                        return roleManager.getRoles(primaryRole, true);\n                    }\n\n                    public ListenableFuture<Set<RoleResource>> reload(final RoleResource primaryRole,\n                                                                      final Set<RoleResource> oldValue)\n                    {\n                        ListenableFutureTask<Set<RoleResource>> task;\n                        task = ListenableFutureTask.create(new Callable<Set<RoleResource>>()\n                        {\n                            public Set<RoleResource> call() throws Exception\n                            {\n                                try\n                                {\n                                    return roleManager.getRoles(primaryRole, true);\n                                } catch (Exception e)\n                                {\n                                    logger.trace(\"Error performing async refresh of user roles\", e);\n                                    throw e;\n                                }\n                            }\n                        });\n                        cacheRefreshExecutor.execute(task);\n                        return task;\n                    }\n                });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }"
        ],
        [
            "MessageDeliveryTask::run()",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 -\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "    public void run()\n    {\n        MessagingService.Verb verb = message.verb;\n        if (MessagingService.DROPPABLE_VERBS.contains(verb)\n            && System.currentTimeMillis() > constructionTime + message.getTimeout())\n        {\n            MessagingService.instance().incrementDroppedMessages(verb, isCrossNodeTimestamp);\n            return;\n        }\n\n        IVerbHandler verbHandler = MessagingService.instance().getVerbHandler(verb);\n        if (verbHandler == null)\n        {\n            logger.debug(\"Unknown verb {}\", verb);\n            return;\n        }\n\n        try\n        {\n            verbHandler.doVerb(message, id);\n        }\n        catch (IOException ioe)\n        {\n            handleFailure(ioe);\n            throw new RuntimeException(ioe);\n        }\n        catch (TombstoneOverwhelmingException toe)\n        {\n            handleFailure(toe);\n            logger.error(toe.getMessage());\n        }\n        catch (Throwable t)\n        {\n            handleFailure(t);\n            throw t;\n        }\n\n        if (GOSSIP_VERBS.contains(message.verb))\n            Gossiper.instance.setLastProcessedMessageAt(constructionTime);\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 +\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "    public void run()\n    {\n        MessagingService.Verb verb = message.verb;\n        if (MessagingService.DROPPABLE_VERBS.contains(verb)\n            && System.currentTimeMillis() > constructionTime + message.getTimeout())\n        {\n            MessagingService.instance().incrementDroppedMessages(verb, isCrossNodeTimestamp);\n            return;\n        }\n\n        IVerbHandler verbHandler = MessagingService.instance().getVerbHandler(verb);\n        if (verbHandler == null)\n        {\n            logger.trace(\"Unknown verb {}\", verb);\n            return;\n        }\n\n        try\n        {\n            verbHandler.doVerb(message, id);\n        }\n        catch (IOException ioe)\n        {\n            handleFailure(ioe);\n            throw new RuntimeException(ioe);\n        }\n        catch (TombstoneOverwhelmingException toe)\n        {\n            handleFailure(toe);\n            logger.error(toe.getMessage());\n        }\n        catch (Throwable t)\n        {\n            handleFailure(t);\n            throw t;\n        }\n\n        if (GOSSIP_VERBS.contains(message.verb))\n            Gossiper.instance.setLastProcessedMessageAt(constructionTime);\n    }"
        ],
        [
            "MmappedSegmentedFile::Cleanup::tidy()",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "        public void tidy()\n        {\n            super.tidy();\n\n            if (!FileUtils.isCleanerAvailable())\n                return;\n\n        /*\n         * Try forcing the unmapping of segments using undocumented unsafe sun APIs.\n         * If this fails (non Sun JVM), we'll have to wait for the GC to finalize the mapping.\n         * If this works and a thread tries to access any segment, hell will unleash on earth.\n         */\n            try\n            {\n                for (Segment segment : segments)\n                {\n                    if (segment.right == null)\n                        continue;\n                    FileUtils.clean(segment.right);\n                }\n                logger.debug(\"All segments have been unmapped successfully\");\n            }\n            catch (Exception e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                // This is not supposed to happen\n                logger.error(\"Error while unmapping segments\", e);\n            }\n        }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "        public void tidy()\n        {\n            super.tidy();\n\n            if (!FileUtils.isCleanerAvailable())\n                return;\n\n        /*\n         * Try forcing the unmapping of segments using undocumented unsafe sun APIs.\n         * If this fails (non Sun JVM), we'll have to wait for the GC to finalize the mapping.\n         * If this works and a thread tries to access any segment, hell will unleash on earth.\n         */\n            try\n            {\n                for (Segment segment : segments)\n                {\n                    if (segment.right == null)\n                        continue;\n                    FileUtils.clean(segment.right);\n                }\n                logger.trace(\"All segments have been unmapped successfully\");\n            }\n            catch (Exception e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                // This is not supposed to happen\n                logger.error(\"Error while unmapping segments\", e);\n            }\n        }"
        ],
        [
            "LeveledManifest::getOverlappingStarvedSSTables(int,Collection)",
            " 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390 -\n 391  \n 392  \n 393 -\n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "    /**\n     * If we do something that makes many levels contain too little data (cleanup, change sstable size) we will \"never\"\n     * compact the high levels.\n     *\n     * This method finds if we have gone many compaction rounds without doing any high-level compaction, if so\n     * we start bringing in one sstable from the highest level until that level is either empty or is doing compaction.\n     *\n     * @param targetLevel the level the candidates will be compacted into\n     * @param candidates the original sstables to compact\n     * @return\n     */\n    private Collection<SSTableReader> getOverlappingStarvedSSTables(int targetLevel, Collection<SSTableReader> candidates)\n    {\n        Set<SSTableReader> withStarvedCandidate = new HashSet<>(candidates);\n\n        for (int i = generations.length - 1; i > 0; i--)\n            compactionCounter[i]++;\n        compactionCounter[targetLevel] = 0;\n        if (logger.isDebugEnabled())\n        {\n            for (int j = 0; j < compactionCounter.length; j++)\n                logger.debug(\"CompactionCounter: {}: {}\", j, compactionCounter[j]);\n        }\n\n        for (int i = generations.length - 1; i > 0; i--)\n        {\n            if (getLevelSize(i) > 0)\n            {\n                if (compactionCounter[i] > NO_COMPACTION_LIMIT)\n                {\n                    // we try to find an sstable that is fully contained within  the boundaries we are compacting;\n                    // say we are compacting 3 sstables: 0->30 in L1 and 0->12, 12->33 in L2\n                    // this means that we will not create overlap in L2 if we add an sstable\n                    // contained within 0 -> 33 to the compaction\n                    RowPosition max = null;\n                    RowPosition min = null;\n                    for (SSTableReader candidate : candidates)\n                    {\n                        if (min == null || candidate.first.compareTo(min) < 0)\n                            min = candidate.first;\n                        if (max == null || candidate.last.compareTo(max) > 0)\n                            max = candidate.last;\n                    }\n                    if (min == null || max == null || min.equals(max)) // single partition sstables - we cannot include a high level sstable.\n                        return candidates;\n                    Set<SSTableReader> compacting = cfs.getTracker().getCompacting();\n                    Range<RowPosition> boundaries = new Range<>(min, max);\n                    for (SSTableReader sstable : getLevel(i))\n                    {\n                        Range<RowPosition> r = new Range<RowPosition>(sstable.first, sstable.last);\n                        if (boundaries.contains(r) && !compacting.contains(sstable))\n                        {\n                            logger.info(\"Adding high-level (L{}) {} to candidates\", sstable.getSSTableLevel(), sstable);\n                            withStarvedCandidate.add(sstable);\n                            return withStarvedCandidate;\n                        }\n                    }\n                }\n                return candidates;\n            }\n        }\n\n        return candidates;\n    }",
            " 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390 +\n 391  \n 392  \n 393 +\n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "    /**\n     * If we do something that makes many levels contain too little data (cleanup, change sstable size) we will \"never\"\n     * compact the high levels.\n     *\n     * This method finds if we have gone many compaction rounds without doing any high-level compaction, if so\n     * we start bringing in one sstable from the highest level until that level is either empty or is doing compaction.\n     *\n     * @param targetLevel the level the candidates will be compacted into\n     * @param candidates the original sstables to compact\n     * @return\n     */\n    private Collection<SSTableReader> getOverlappingStarvedSSTables(int targetLevel, Collection<SSTableReader> candidates)\n    {\n        Set<SSTableReader> withStarvedCandidate = new HashSet<>(candidates);\n\n        for (int i = generations.length - 1; i > 0; i--)\n            compactionCounter[i]++;\n        compactionCounter[targetLevel] = 0;\n        if (logger.isTraceEnabled())\n        {\n            for (int j = 0; j < compactionCounter.length; j++)\n                logger.trace(\"CompactionCounter: {}: {}\", j, compactionCounter[j]);\n        }\n\n        for (int i = generations.length - 1; i > 0; i--)\n        {\n            if (getLevelSize(i) > 0)\n            {\n                if (compactionCounter[i] > NO_COMPACTION_LIMIT)\n                {\n                    // we try to find an sstable that is fully contained within  the boundaries we are compacting;\n                    // say we are compacting 3 sstables: 0->30 in L1 and 0->12, 12->33 in L2\n                    // this means that we will not create overlap in L2 if we add an sstable\n                    // contained within 0 -> 33 to the compaction\n                    RowPosition max = null;\n                    RowPosition min = null;\n                    for (SSTableReader candidate : candidates)\n                    {\n                        if (min == null || candidate.first.compareTo(min) < 0)\n                            min = candidate.first;\n                        if (max == null || candidate.last.compareTo(max) > 0)\n                            max = candidate.last;\n                    }\n                    if (min == null || max == null || min.equals(max)) // single partition sstables - we cannot include a high level sstable.\n                        return candidates;\n                    Set<SSTableReader> compacting = cfs.getTracker().getCompacting();\n                    Range<RowPosition> boundaries = new Range<>(min, max);\n                    for (SSTableReader sstable : getLevel(i))\n                    {\n                        Range<RowPosition> r = new Range<RowPosition>(sstable.first, sstable.last);\n                        if (boundaries.contains(r) && !compacting.contains(sstable))\n                        {\n                            logger.info(\"Adding high-level (L{}) {} to candidates\", sstable.getSSTableLevel(), sstable);\n                            withStarvedCandidate.add(sstable);\n                            return withStarvedCandidate;\n                        }\n                    }\n                }\n                return candidates;\n            }\n        }\n\n        return candidates;\n    }"
        ],
        [
            "Keyspace::Keyspace(String,boolean)",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 -\n 270  \n 271  \n 272  ",
            "    private Keyspace(String keyspaceName, boolean loadSSTables)\n    {\n        metadata = Schema.instance.getKSMetaData(keyspaceName);\n        assert metadata != null : \"Unknown keyspace \" + keyspaceName;\n        createReplicationStrategy(metadata);\n\n        this.metric = new KeyspaceMetrics(this);\n        for (CFMetaData cfm : new ArrayList<>(metadata.cfMetaData().values()))\n        {\n            logger.debug(\"Initializing {}.{}\", getName(), cfm.cfName);\n            initCf(cfm.cfId, cfm.cfName, loadSSTables);\n        }\n    }",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 +\n 270  \n 271  \n 272  ",
            "    private Keyspace(String keyspaceName, boolean loadSSTables)\n    {\n        metadata = Schema.instance.getKSMetaData(keyspaceName);\n        assert metadata != null : \"Unknown keyspace \" + keyspaceName;\n        createReplicationStrategy(metadata);\n\n        this.metric = new KeyspaceMetrics(this);\n        for (CFMetaData cfm : new ArrayList<>(metadata.cfMetaData().values()))\n        {\n            logger.trace(\"Initializing {}.{}\", getName(), cfm.cfName);\n            initCf(cfm.cfId, cfm.cfName, loadSSTables);\n        }\n    }"
        ],
        [
            "CassandraServer::system_add_keyspace(KsDef)",
            "1542  \n1543  \n1544  \n1545 -\n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  ",
            "    public String system_add_keyspace(KsDef ks_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"add_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(ks_def.name);\n            state().hasAllKeyspacesAccess(Permission.CREATE);\n            ThriftValidation.validateKeyspaceNotYetExisting(ks_def.name);\n\n            // generate a meaningful error if the user setup keyspace and/or column definition incorrectly\n            for (CfDef cf : ks_def.cf_defs)\n            {\n                if (!cf.getKeyspace().equals(ks_def.getName()))\n                {\n                    throw new InvalidRequestException(\"CfDef (\" + cf.getName() +\") had a keyspace definition that did not match KsDef\");\n                }\n            }\n\n            Collection<CFMetaData> cfDefs = new ArrayList<CFMetaData>(ks_def.cf_defs.size());\n            for (CfDef cf_def : ks_def.cf_defs)\n            {\n                cf_def.unsetId(); // explicitly ignore any id set by client (same as system_add_column_family)\n                CFMetaData cfm = ThriftConversion.fromThrift(cf_def);\n                cfm.addDefaultIndexNames();\n\n                if (!cfm.getTriggers().isEmpty())\n                    state().ensureIsSuper(\"Only superusers are allowed to add triggers.\");\n\n                cfDefs.add(cfm);\n            }\n            MigrationManager.announceNewKeyspace(ThriftConversion.fromThrift(ks_def, cfDefs.toArray(new CFMetaData[cfDefs.size()])));\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1542  \n1543  \n1544  \n1545 +\n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  ",
            "    public String system_add_keyspace(KsDef ks_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"add_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(ks_def.name);\n            state().hasAllKeyspacesAccess(Permission.CREATE);\n            ThriftValidation.validateKeyspaceNotYetExisting(ks_def.name);\n\n            // generate a meaningful error if the user setup keyspace and/or column definition incorrectly\n            for (CfDef cf : ks_def.cf_defs)\n            {\n                if (!cf.getKeyspace().equals(ks_def.getName()))\n                {\n                    throw new InvalidRequestException(\"CfDef (\" + cf.getName() +\") had a keyspace definition that did not match KsDef\");\n                }\n            }\n\n            Collection<CFMetaData> cfDefs = new ArrayList<CFMetaData>(ks_def.cf_defs.size());\n            for (CfDef cf_def : ks_def.cf_defs)\n            {\n                cf_def.unsetId(); // explicitly ignore any id set by client (same as system_add_column_family)\n                CFMetaData cfm = ThriftConversion.fromThrift(cf_def);\n                cfm.addDefaultIndexNames();\n\n                if (!cfm.getTriggers().isEmpty())\n                    state().ensureIsSuper(\"Only superusers are allowed to add triggers.\");\n\n                cfDefs.add(cfm);\n            }\n            MigrationManager.announceNewKeyspace(ThriftConversion.fromThrift(ks_def, cfDefs.toArray(new CFMetaData[cfDefs.size()])));\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "CassandraServer::remove(ByteBuffer,ColumnPath,long,ConsistencyLevel)",
            "1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042 -\n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  ",
            "    public void remove(ByteBuffer key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", column_path.toString(),\n                                                                  \"timestamp\", timestamp + \"\",\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"remove\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"remove\");\n        }\n\n        try\n        {\n            internal_remove(key, column_path, timestamp, consistency_level, false);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042 +\n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  ",
            "    public void remove(ByteBuffer key, ColumnPath column_path, long timestamp, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", column_path.toString(),\n                                                                  \"timestamp\", timestamp + \"\",\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"remove\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"remove\");\n        }\n\n        try\n        {\n            internal_remove(key, column_path, timestamp, consistency_level, false);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "BatchlogManager::replayAllFailedBatches()",
            " 164  \n 165  \n 166 -\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  ",
            "    private void replayAllFailedBatches() throws ExecutionException, InterruptedException\n    {\n        logger.debug(\"Started replayAllFailedBatches\");\n\n        // rate limit is in bytes per second. Uses Double.MAX_VALUE if disabled (set to 0 in cassandra.yaml).\n        // max rate is scaled by the number of nodes in the cluster (same as for HHOM - see CASSANDRA-5272).\n        int throttleInKB = DatabaseDescriptor.getBatchlogReplayThrottleInKB() / StorageService.instance.getTokenMetadata().getAllEndpoints().size();\n        RateLimiter rateLimiter = RateLimiter.create(throttleInKB == 0 ? Double.MAX_VALUE : throttleInKB * 1024);\n\n        UntypedResultSet page = executeInternal(String.format(\"SELECT id, data, written_at, version FROM %s.%s LIMIT %d\",\n                                                              SystemKeyspace.NAME,\n                                                              SystemKeyspace.BATCHLOG,\n                                                              PAGE_SIZE));\n\n        while (!page.isEmpty())\n        {\n            UUID id = processBatchlogPage(page, rateLimiter);\n\n            if (page.size() < PAGE_SIZE)\n                break; // we've exhausted the batchlog, next query would be empty.\n\n            page = executeInternal(String.format(\"SELECT id, data, written_at, version FROM %s.%s WHERE token(id) > token(?) LIMIT %d\",\n                                                 SystemKeyspace.NAME,\n                                                 SystemKeyspace.BATCHLOG,\n                                                 PAGE_SIZE),\n                                   id);\n        }\n\n        cleanup();\n\n        logger.debug(\"Finished replayAllFailedBatches\");\n    }",
            " 164  \n 165  \n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  ",
            "    private void replayAllFailedBatches() throws ExecutionException, InterruptedException\n    {\n        logger.trace(\"Started replayAllFailedBatches\");\n\n        // rate limit is in bytes per second. Uses Double.MAX_VALUE if disabled (set to 0 in cassandra.yaml).\n        // max rate is scaled by the number of nodes in the cluster (same as for HHOM - see CASSANDRA-5272).\n        int throttleInKB = DatabaseDescriptor.getBatchlogReplayThrottleInKB() / StorageService.instance.getTokenMetadata().getAllEndpoints().size();\n        RateLimiter rateLimiter = RateLimiter.create(throttleInKB == 0 ? Double.MAX_VALUE : throttleInKB * 1024);\n\n        UntypedResultSet page = executeInternal(String.format(\"SELECT id, data, written_at, version FROM %s.%s LIMIT %d\",\n                                                              SystemKeyspace.NAME,\n                                                              SystemKeyspace.BATCHLOG,\n                                                              PAGE_SIZE));\n\n        while (!page.isEmpty())\n        {\n            UUID id = processBatchlogPage(page, rateLimiter);\n\n            if (page.size() < PAGE_SIZE)\n                break; // we've exhausted the batchlog, next query would be empty.\n\n            page = executeInternal(String.format(\"SELECT id, data, written_at, version FROM %s.%s WHERE token(id) > token(?) LIMIT %d\",\n                                                 SystemKeyspace.NAME,\n                                                 SystemKeyspace.BATCHLOG,\n                                                 PAGE_SIZE),\n                                   id);\n        }\n\n        cleanup();\n\n        logger.trace(\"Finished replayAllFailedBatches\");\n    }"
        ],
        [
            "CompositesSearcher::getIndexedIterator(OpOrder,ExtendedFilter,IndexExpression,CompositesIndex)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243 -\n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "    private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final OpOrder.Group writeOp, final ExtendedFilter filter, final IndexExpression primary, final CompositesIndex index)\n    {\n        // Start with the most-restrictive indexed clause, then apply remaining clauses\n        // to each row matching that clause.\n        // TODO: allow merge join instead of just one index + loop\n        assert index != null;\n        assert index.getIndexCfs() != null;\n        final DecoratedKey indexKey = index.getIndexKeyFor(primary.value);\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"Most-selective indexed predicate is {}\", index.expressionString(primary));\n\n        /*\n         * XXX: If the range requested is a token range, we'll have to start at the beginning (and stop at the end) of\n         * the indexed row unfortunately (which will be inefficient), because we have not way to intuit the smallest\n         * possible key having a given token. A fix would be to actually store the token along the key in the\n         * indexed row.\n         */\n        final AbstractBounds<RowPosition> range = filter.dataRange.keyRange();\n        ByteBuffer startKey = range.left instanceof DecoratedKey ? ((DecoratedKey)range.left).getKey() : ByteBufferUtil.EMPTY_BYTE_BUFFER;\n        ByteBuffer endKey = range.right instanceof DecoratedKey ? ((DecoratedKey)range.right).getKey() : ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n        final CellNameType baseComparator = baseCfs.getComparator();\n        final CellNameType indexComparator = index.getIndexCfs().getComparator();\n\n        final Composite startPrefix = makePrefix(index, startKey, filter, true);\n        final Composite endPrefix = makePrefix(index, endKey, filter, false);\n\n        return new ColumnFamilyStore.AbstractScanIterator()\n        {\n            private Composite lastSeenPrefix = startPrefix;\n            private Deque<Cell> indexCells;\n            private int columnsRead = Integer.MAX_VALUE;\n            private int limit = filter.currentLimit();\n            private int columnsCount = 0;\n\n            // We have to fetch at least two rows to avoid breaking paging if the first row doesn't satisfy all clauses\n            private int indexCellsPerQuery = Math.max(2, Math.min(filter.maxColumns(), filter.maxRows()));\n\n            public boolean needsFiltering()\n            {\n                return false;\n            }\n\n            private Row makeReturn(DecoratedKey key, ColumnFamily data)\n            {\n                if (data == null)\n                    return endOfData();\n\n                assert key != null;\n                return new Row(key, data);\n            }\n\n            protected Row computeNext()\n            {\n                /*\n                 * Our internal index code is wired toward internal rows. So we need to accumulate all results for a given\n                 * row before returning from this method. Which unfortunately means that this method has to do what\n                 * CFS.filter does for KeysIndex.\n                 */\n                DecoratedKey currentKey = null;\n                ColumnFamily data = null;\n                Composite previousPrefix = null;\n\n                while (true)\n                {\n                    // Did we get more columns that needed to respect the user limit?\n                    // (but we still need to return what has been fetched already)\n                    if (columnsCount >= limit)\n                        return makeReturn(currentKey, data);\n\n                    if (indexCells == null || indexCells.isEmpty())\n                    {\n                        if (columnsRead < indexCellsPerQuery)\n                        {\n                            logger.trace(\"Read only {} (< {}) last page through, must be done\", columnsRead, indexCellsPerQuery);\n                            return makeReturn(currentKey, data);\n                        }\n\n                        if (logger.isTraceEnabled())\n                            logger.trace(\"Scanning index {} starting with {}\",\n                                         index.expressionString(primary), indexComparator.getString(startPrefix));\n\n                        QueryFilter indexFilter = QueryFilter.getSliceFilter(indexKey,\n                                                                             index.getIndexCfs().name,\n                                                                             lastSeenPrefix,\n                                                                             endPrefix,\n                                                                             false,\n                                                                             indexCellsPerQuery,\n                                                                             filter.timestamp);\n                        ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);\n                        if (indexRow == null || !indexRow.hasColumns())\n                            return makeReturn(currentKey, data);\n\n                        Collection<Cell> sortedCells = indexRow.getSortedColumns();\n                        columnsRead = sortedCells.size();\n                        indexCells = new ArrayDeque<>(sortedCells);\n                        Cell firstCell = sortedCells.iterator().next();\n\n                        // Paging is racy, so it is possible the first column of a page is not the last seen one.\n                        if (lastSeenPrefix != startPrefix && lastSeenPrefix.equals(firstCell.name()))\n                        {\n                            // skip the row we already saw w/ the last page of results\n                            indexCells.poll();\n                            logger.trace(\"Skipping {}\", indexComparator.getString(firstCell.name()));\n                        }\n                    }\n\n                    while (!indexCells.isEmpty() && columnsCount <= limit)\n                    {\n                        Cell cell = indexCells.poll();\n                        lastSeenPrefix = cell.name();\n                        if (!cell.isLive(filter.timestamp))\n                        {\n                            logger.trace(\"skipping {}\", cell.name());\n                            continue;\n                        }\n\n                        CompositesIndex.IndexedEntry entry = index.decodeEntry(indexKey, cell);\n                        DecoratedKey dk = baseCfs.partitioner.decorateKey(entry.indexedKey);\n\n                        // Are we done for this row?\n                        if (currentKey == null)\n                        {\n                            currentKey = dk;\n                        }\n                        else if (!currentKey.equals(dk))\n                        {\n                            DecoratedKey previousKey = currentKey;\n                            currentKey = dk;\n                            previousPrefix = null;\n\n                            // We're done with the previous row, return it if it had data, continue otherwise\n                            indexCells.addFirst(cell);\n                            if (data == null)\n                                continue;\n                            else\n                                return makeReturn(previousKey, data);\n                        }\n\n                        if (!range.contains(dk))\n                        {\n                            // Either we're not yet in the range cause the range is start excluding, or we're\n                            // past it.\n                            if (!range.right.isMinimum() && range.right.compareTo(dk) < 0)\n                            {\n                                logger.trace(\"Reached end of assigned scan range\");\n                                return endOfData();\n                            }\n                            else\n                            {\n                                logger.debug(\"Skipping entry {} before assigned scan range\", dk.getToken());\n                                continue;\n                            }\n                        }\n\n                        // Check if this entry cannot be a hit due to the original cell filter\n                        Composite start = entry.indexedEntryPrefix;\n                        if (!filter.columnFilter(dk.getKey()).maySelectPrefix(baseComparator, start))\n                            continue;\n\n                        // If we've record the previous prefix, it means we're dealing with an index on the collection value. In\n                        // that case, we can have multiple index prefix for the same CQL3 row. In that case, we want to only add\n                        // the CQL3 row once (because requesting the data multiple time would be inefficient but more importantly\n                        // because we shouldn't count the columns multiple times with the lastCounted() call at the end of this\n                        // method).\n                        if (previousPrefix != null && previousPrefix.equals(start))\n                            continue;\n                        else\n                            previousPrefix = null;\n\n                        if (logger.isTraceEnabled())\n                            logger.trace(\"Adding index hit to current row for {}\", indexComparator.getString(cell.name()));\n\n                        // We always query the whole CQL3 row. In the case where the original filter was a name filter this might be\n                        // slightly wasteful, but this probably doesn't matter in practice and it simplify things.\n                        ColumnSlice dataSlice = new ColumnSlice(start, entry.indexedEntryPrefix.end());\n                        // If the table has static columns, we must fetch them too as they may need to be returned too.\n                        // Note that this is potentially wasteful for 2 reasons:\n                        //  1) we will retrieve the static parts for each indexed row, even if we have more than one row in\n                        //     the same partition. If we were to group data queries to rows on the same slice, which would\n                        //     speed up things in general, we would also optimize here since we would fetch static columns only\n                        //     once for each group.\n                        //  2) at this point we don't know if the user asked for static columns or not, so we might be fetching\n                        //     them for nothing. We would however need to ship the list of \"CQL3 columns selected\" with getRangeSlice\n                        //     to be able to know that.\n                        // TODO: we should improve both point above\n                        ColumnSlice[] slices = baseCfs.metadata.hasStaticColumns()\n                                             ? new ColumnSlice[]{ baseCfs.metadata.comparator.staticPrefix().slice(), dataSlice }\n                                             : new ColumnSlice[]{ dataSlice };\n                        SliceQueryFilter dataFilter = new SliceQueryFilter(slices, false, Integer.MAX_VALUE, baseCfs.metadata.clusteringColumns().size());\n                        ColumnFamily newData = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, dataFilter, filter.timestamp));\n                        if (newData == null || index.isStale(entry, newData, filter.timestamp))\n                        {\n                            index.delete(entry, writeOp);\n                            continue;\n                        }\n\n                        assert newData != null : \"An entry with no data should have been considered stale\";\n\n                        // We know the entry is not stale and so the entry satisfy the primary clause. So whether\n                        // or not the data satisfies the other clauses, there will be no point to re-check the\n                        // same CQL3 row if we run into another collection value entry for this row.\n                        if (entry.indexedEntryCollectionKey != null)\n                            previousPrefix = start;\n\n                        if (!filter.isSatisfiedBy(dk, newData, entry.indexedEntryPrefix, entry.indexedEntryCollectionKey))\n                            continue;\n\n                        if (data == null)\n                            data = ArrayBackedSortedColumns.factory.create(baseCfs.metadata);\n                        data.addAll(newData);\n                        columnsCount += dataFilter.lastCounted();\n                    }\n                 }\n             }\n\n            public void close() throws IOException {}\n        };\n    }",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243 +\n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "    private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final OpOrder.Group writeOp, final ExtendedFilter filter, final IndexExpression primary, final CompositesIndex index)\n    {\n        // Start with the most-restrictive indexed clause, then apply remaining clauses\n        // to each row matching that clause.\n        // TODO: allow merge join instead of just one index + loop\n        assert index != null;\n        assert index.getIndexCfs() != null;\n        final DecoratedKey indexKey = index.getIndexKeyFor(primary.value);\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"Most-selective indexed predicate is {}\", index.expressionString(primary));\n\n        /*\n         * XXX: If the range requested is a token range, we'll have to start at the beginning (and stop at the end) of\n         * the indexed row unfortunately (which will be inefficient), because we have not way to intuit the smallest\n         * possible key having a given token. A fix would be to actually store the token along the key in the\n         * indexed row.\n         */\n        final AbstractBounds<RowPosition> range = filter.dataRange.keyRange();\n        ByteBuffer startKey = range.left instanceof DecoratedKey ? ((DecoratedKey)range.left).getKey() : ByteBufferUtil.EMPTY_BYTE_BUFFER;\n        ByteBuffer endKey = range.right instanceof DecoratedKey ? ((DecoratedKey)range.right).getKey() : ByteBufferUtil.EMPTY_BYTE_BUFFER;\n\n        final CellNameType baseComparator = baseCfs.getComparator();\n        final CellNameType indexComparator = index.getIndexCfs().getComparator();\n\n        final Composite startPrefix = makePrefix(index, startKey, filter, true);\n        final Composite endPrefix = makePrefix(index, endKey, filter, false);\n\n        return new ColumnFamilyStore.AbstractScanIterator()\n        {\n            private Composite lastSeenPrefix = startPrefix;\n            private Deque<Cell> indexCells;\n            private int columnsRead = Integer.MAX_VALUE;\n            private int limit = filter.currentLimit();\n            private int columnsCount = 0;\n\n            // We have to fetch at least two rows to avoid breaking paging if the first row doesn't satisfy all clauses\n            private int indexCellsPerQuery = Math.max(2, Math.min(filter.maxColumns(), filter.maxRows()));\n\n            public boolean needsFiltering()\n            {\n                return false;\n            }\n\n            private Row makeReturn(DecoratedKey key, ColumnFamily data)\n            {\n                if (data == null)\n                    return endOfData();\n\n                assert key != null;\n                return new Row(key, data);\n            }\n\n            protected Row computeNext()\n            {\n                /*\n                 * Our internal index code is wired toward internal rows. So we need to accumulate all results for a given\n                 * row before returning from this method. Which unfortunately means that this method has to do what\n                 * CFS.filter does for KeysIndex.\n                 */\n                DecoratedKey currentKey = null;\n                ColumnFamily data = null;\n                Composite previousPrefix = null;\n\n                while (true)\n                {\n                    // Did we get more columns that needed to respect the user limit?\n                    // (but we still need to return what has been fetched already)\n                    if (columnsCount >= limit)\n                        return makeReturn(currentKey, data);\n\n                    if (indexCells == null || indexCells.isEmpty())\n                    {\n                        if (columnsRead < indexCellsPerQuery)\n                        {\n                            logger.trace(\"Read only {} (< {}) last page through, must be done\", columnsRead, indexCellsPerQuery);\n                            return makeReturn(currentKey, data);\n                        }\n\n                        if (logger.isTraceEnabled())\n                            logger.trace(\"Scanning index {} starting with {}\",\n                                         index.expressionString(primary), indexComparator.getString(startPrefix));\n\n                        QueryFilter indexFilter = QueryFilter.getSliceFilter(indexKey,\n                                                                             index.getIndexCfs().name,\n                                                                             lastSeenPrefix,\n                                                                             endPrefix,\n                                                                             false,\n                                                                             indexCellsPerQuery,\n                                                                             filter.timestamp);\n                        ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);\n                        if (indexRow == null || !indexRow.hasColumns())\n                            return makeReturn(currentKey, data);\n\n                        Collection<Cell> sortedCells = indexRow.getSortedColumns();\n                        columnsRead = sortedCells.size();\n                        indexCells = new ArrayDeque<>(sortedCells);\n                        Cell firstCell = sortedCells.iterator().next();\n\n                        // Paging is racy, so it is possible the first column of a page is not the last seen one.\n                        if (lastSeenPrefix != startPrefix && lastSeenPrefix.equals(firstCell.name()))\n                        {\n                            // skip the row we already saw w/ the last page of results\n                            indexCells.poll();\n                            logger.trace(\"Skipping {}\", indexComparator.getString(firstCell.name()));\n                        }\n                    }\n\n                    while (!indexCells.isEmpty() && columnsCount <= limit)\n                    {\n                        Cell cell = indexCells.poll();\n                        lastSeenPrefix = cell.name();\n                        if (!cell.isLive(filter.timestamp))\n                        {\n                            logger.trace(\"skipping {}\", cell.name());\n                            continue;\n                        }\n\n                        CompositesIndex.IndexedEntry entry = index.decodeEntry(indexKey, cell);\n                        DecoratedKey dk = baseCfs.partitioner.decorateKey(entry.indexedKey);\n\n                        // Are we done for this row?\n                        if (currentKey == null)\n                        {\n                            currentKey = dk;\n                        }\n                        else if (!currentKey.equals(dk))\n                        {\n                            DecoratedKey previousKey = currentKey;\n                            currentKey = dk;\n                            previousPrefix = null;\n\n                            // We're done with the previous row, return it if it had data, continue otherwise\n                            indexCells.addFirst(cell);\n                            if (data == null)\n                                continue;\n                            else\n                                return makeReturn(previousKey, data);\n                        }\n\n                        if (!range.contains(dk))\n                        {\n                            // Either we're not yet in the range cause the range is start excluding, or we're\n                            // past it.\n                            if (!range.right.isMinimum() && range.right.compareTo(dk) < 0)\n                            {\n                                logger.trace(\"Reached end of assigned scan range\");\n                                return endOfData();\n                            }\n                            else\n                            {\n                                logger.trace(\"Skipping entry {} before assigned scan range\", dk.getToken());\n                                continue;\n                            }\n                        }\n\n                        // Check if this entry cannot be a hit due to the original cell filter\n                        Composite start = entry.indexedEntryPrefix;\n                        if (!filter.columnFilter(dk.getKey()).maySelectPrefix(baseComparator, start))\n                            continue;\n\n                        // If we've record the previous prefix, it means we're dealing with an index on the collection value. In\n                        // that case, we can have multiple index prefix for the same CQL3 row. In that case, we want to only add\n                        // the CQL3 row once (because requesting the data multiple time would be inefficient but more importantly\n                        // because we shouldn't count the columns multiple times with the lastCounted() call at the end of this\n                        // method).\n                        if (previousPrefix != null && previousPrefix.equals(start))\n                            continue;\n                        else\n                            previousPrefix = null;\n\n                        if (logger.isTraceEnabled())\n                            logger.trace(\"Adding index hit to current row for {}\", indexComparator.getString(cell.name()));\n\n                        // We always query the whole CQL3 row. In the case where the original filter was a name filter this might be\n                        // slightly wasteful, but this probably doesn't matter in practice and it simplify things.\n                        ColumnSlice dataSlice = new ColumnSlice(start, entry.indexedEntryPrefix.end());\n                        // If the table has static columns, we must fetch them too as they may need to be returned too.\n                        // Note that this is potentially wasteful for 2 reasons:\n                        //  1) we will retrieve the static parts for each indexed row, even if we have more than one row in\n                        //     the same partition. If we were to group data queries to rows on the same slice, which would\n                        //     speed up things in general, we would also optimize here since we would fetch static columns only\n                        //     once for each group.\n                        //  2) at this point we don't know if the user asked for static columns or not, so we might be fetching\n                        //     them for nothing. We would however need to ship the list of \"CQL3 columns selected\" with getRangeSlice\n                        //     to be able to know that.\n                        // TODO: we should improve both point above\n                        ColumnSlice[] slices = baseCfs.metadata.hasStaticColumns()\n                                             ? new ColumnSlice[]{ baseCfs.metadata.comparator.staticPrefix().slice(), dataSlice }\n                                             : new ColumnSlice[]{ dataSlice };\n                        SliceQueryFilter dataFilter = new SliceQueryFilter(slices, false, Integer.MAX_VALUE, baseCfs.metadata.clusteringColumns().size());\n                        ColumnFamily newData = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, dataFilter, filter.timestamp));\n                        if (newData == null || index.isStale(entry, newData, filter.timestamp))\n                        {\n                            index.delete(entry, writeOp);\n                            continue;\n                        }\n\n                        assert newData != null : \"An entry with no data should have been considered stale\";\n\n                        // We know the entry is not stale and so the entry satisfy the primary clause. So whether\n                        // or not the data satisfies the other clauses, there will be no point to re-check the\n                        // same CQL3 row if we run into another collection value entry for this row.\n                        if (entry.indexedEntryCollectionKey != null)\n                            previousPrefix = start;\n\n                        if (!filter.isSatisfiedBy(dk, newData, entry.indexedEntryPrefix, entry.indexedEntryCollectionKey))\n                            continue;\n\n                        if (data == null)\n                            data = ArrayBackedSortedColumns.factory.create(baseCfs.metadata);\n                        data.addAll(newData);\n                        columnsCount += dataFilter.lastCounted();\n                    }\n                 }\n             }\n\n            public void close() throws IOException {}\n        };\n    }"
        ],
        [
            "CompactionManager::antiCompactGroup(ColumnFamilyStore,Collection,LifecycleTransaction,long)",
            "1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246 -\n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  ",
            "    private int antiCompactGroup(ColumnFamilyStore cfs, Collection<Range<Token>> ranges,\n                             LifecycleTransaction anticompactionGroup, long repairedAt)\n    {\n        long groupMaxDataAge = -1;\n\n        // check that compaction hasn't stolen any sstables used in previous repair sessions\n        // if we need to skip the anticompaction, it will be carried out by the next repair\n        for (Iterator<SSTableReader> i = anticompactionGroup.originals().iterator(); i.hasNext();)\n        {\n            SSTableReader sstable = i.next();\n            if (!new File(sstable.getFilename()).exists())\n            {\n                logger.info(\"Skipping anticompaction for {}, required sstable was compacted and is no longer available.\", sstable);\n                i.remove();\n                continue;\n            }\n            if (groupMaxDataAge < sstable.maxDataAge)\n                groupMaxDataAge = sstable.maxDataAge;\n        }\n\n        if (anticompactionGroup.originals().size() == 0)\n        {\n            logger.info(\"No valid anticompactions for this group, All sstables were compacted and are no longer available\");\n            return 0;\n        }\n\n        logger.info(\"Anticompacting {}\", anticompactionGroup);\n        Set<SSTableReader> sstableAsSet = anticompactionGroup.originals();\n\n        File destination = cfs.directories.getWriteableLocationAsFile(cfs.getExpectedCompactedFileSize(sstableAsSet, OperationType.ANTICOMPACTION));\n        long repairedKeyCount = 0;\n        long unrepairedKeyCount = 0;\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n        try (SSTableRewriter repairedSSTableWriter = new SSTableRewriter(cfs, anticompactionGroup, groupMaxDataAge, false, false);\n             SSTableRewriter unRepairedSSTableWriter = new SSTableRewriter(cfs, anticompactionGroup, groupMaxDataAge, false, false);\n             AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(anticompactionGroup.originals());\n             CompactionController controller = new CompactionController(cfs, sstableAsSet, getDefaultGcBefore(cfs)))\n        {\n            int expectedBloomFilterSize = Math.max(cfs.metadata.getMinIndexInterval(), (int)(SSTableReader.getApproximateKeyCount(sstableAsSet)));\n\n            repairedSSTableWriter.switchWriter(CompactionManager.createWriterForAntiCompaction(cfs, destination, expectedBloomFilterSize, repairedAt, sstableAsSet));\n            unRepairedSSTableWriter.switchWriter(CompactionManager.createWriterForAntiCompaction(cfs, destination, expectedBloomFilterSize, ActiveRepairService.UNREPAIRED_SSTABLE, sstableAsSet));\n\n            CompactionIterable ci = new CompactionIterable(OperationType.ANTICOMPACTION, scanners.scanners, controller, DatabaseDescriptor.getSSTableFormat(), UUIDGen.getTimeUUID());\n            metrics.beginCompaction(ci);\n            try\n            {\n                @SuppressWarnings(\"resource\")\n                CloseableIterator<AbstractCompactedRow> iter = ci.iterator();\n                while (iter.hasNext())\n                {\n                    @SuppressWarnings(\"resource\")\n                    AbstractCompactedRow row = iter.next();\n                    // if current range from sstable is repaired, save it into the new repaired sstable\n                    if (Range.isInRanges(row.key.getToken(), ranges))\n                    {\n                        repairedSSTableWriter.append(row);\n                        repairedKeyCount++;\n                    }\n                    // otherwise save into the new 'non-repaired' table\n                    else\n                    {\n                        unRepairedSSTableWriter.append(row);\n                        unrepairedKeyCount++;\n                    }\n                }\n            }\n            finally\n            {\n                metrics.finishCompaction(ci);\n            }\n\n            List<SSTableReader> anticompactedSSTables = new ArrayList<>();\n            // since both writers are operating over the same Transaction, we cannot use the convenience Transactional.finish() method,\n            // as on the second finish() we would prepareToCommit() on a Transaction that has already been committed, which is forbidden by the API\n            // (since it indicates misuse). We call permitRedundantTransitions so that calls that transition to a state already occupied are permitted.\n            anticompactionGroup.permitRedundantTransitions();\n            repairedSSTableWriter.setRepairedAt(repairedAt).prepareToCommit();\n            unRepairedSSTableWriter.prepareToCommit();\n            anticompactedSSTables.addAll(repairedSSTableWriter.finished());\n            anticompactedSSTables.addAll(unRepairedSSTableWriter.finished());\n            repairedSSTableWriter.commit();\n            unRepairedSSTableWriter.commit();\n\n            logger.debug(\"Repaired {} keys out of {} for {}/{} in {}\", repairedKeyCount,\n                                                                       repairedKeyCount + unrepairedKeyCount,\n                                                                       cfs.keyspace.getName(),\n                                                                       cfs.getColumnFamilyName(),\n                                                                       anticompactionGroup);\n            return anticompactedSSTables.size();\n        }\n        catch (Throwable e)\n        {\n            JVMStabilityInspector.inspectThrowable(e);\n            logger.error(\"Error anticompacting \" + anticompactionGroup, e);\n        }\n        return 0;\n    }",
            "1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246 +\n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  ",
            "    private int antiCompactGroup(ColumnFamilyStore cfs, Collection<Range<Token>> ranges,\n                             LifecycleTransaction anticompactionGroup, long repairedAt)\n    {\n        long groupMaxDataAge = -1;\n\n        // check that compaction hasn't stolen any sstables used in previous repair sessions\n        // if we need to skip the anticompaction, it will be carried out by the next repair\n        for (Iterator<SSTableReader> i = anticompactionGroup.originals().iterator(); i.hasNext();)\n        {\n            SSTableReader sstable = i.next();\n            if (!new File(sstable.getFilename()).exists())\n            {\n                logger.info(\"Skipping anticompaction for {}, required sstable was compacted and is no longer available.\", sstable);\n                i.remove();\n                continue;\n            }\n            if (groupMaxDataAge < sstable.maxDataAge)\n                groupMaxDataAge = sstable.maxDataAge;\n        }\n\n        if (anticompactionGroup.originals().size() == 0)\n        {\n            logger.info(\"No valid anticompactions for this group, All sstables were compacted and are no longer available\");\n            return 0;\n        }\n\n        logger.info(\"Anticompacting {}\", anticompactionGroup);\n        Set<SSTableReader> sstableAsSet = anticompactionGroup.originals();\n\n        File destination = cfs.directories.getWriteableLocationAsFile(cfs.getExpectedCompactedFileSize(sstableAsSet, OperationType.ANTICOMPACTION));\n        long repairedKeyCount = 0;\n        long unrepairedKeyCount = 0;\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n        try (SSTableRewriter repairedSSTableWriter = new SSTableRewriter(cfs, anticompactionGroup, groupMaxDataAge, false, false);\n             SSTableRewriter unRepairedSSTableWriter = new SSTableRewriter(cfs, anticompactionGroup, groupMaxDataAge, false, false);\n             AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(anticompactionGroup.originals());\n             CompactionController controller = new CompactionController(cfs, sstableAsSet, getDefaultGcBefore(cfs)))\n        {\n            int expectedBloomFilterSize = Math.max(cfs.metadata.getMinIndexInterval(), (int)(SSTableReader.getApproximateKeyCount(sstableAsSet)));\n\n            repairedSSTableWriter.switchWriter(CompactionManager.createWriterForAntiCompaction(cfs, destination, expectedBloomFilterSize, repairedAt, sstableAsSet));\n            unRepairedSSTableWriter.switchWriter(CompactionManager.createWriterForAntiCompaction(cfs, destination, expectedBloomFilterSize, ActiveRepairService.UNREPAIRED_SSTABLE, sstableAsSet));\n\n            CompactionIterable ci = new CompactionIterable(OperationType.ANTICOMPACTION, scanners.scanners, controller, DatabaseDescriptor.getSSTableFormat(), UUIDGen.getTimeUUID());\n            metrics.beginCompaction(ci);\n            try\n            {\n                @SuppressWarnings(\"resource\")\n                CloseableIterator<AbstractCompactedRow> iter = ci.iterator();\n                while (iter.hasNext())\n                {\n                    @SuppressWarnings(\"resource\")\n                    AbstractCompactedRow row = iter.next();\n                    // if current range from sstable is repaired, save it into the new repaired sstable\n                    if (Range.isInRanges(row.key.getToken(), ranges))\n                    {\n                        repairedSSTableWriter.append(row);\n                        repairedKeyCount++;\n                    }\n                    // otherwise save into the new 'non-repaired' table\n                    else\n                    {\n                        unRepairedSSTableWriter.append(row);\n                        unrepairedKeyCount++;\n                    }\n                }\n            }\n            finally\n            {\n                metrics.finishCompaction(ci);\n            }\n\n            List<SSTableReader> anticompactedSSTables = new ArrayList<>();\n            // since both writers are operating over the same Transaction, we cannot use the convenience Transactional.finish() method,\n            // as on the second finish() we would prepareToCommit() on a Transaction that has already been committed, which is forbidden by the API\n            // (since it indicates misuse). We call permitRedundantTransitions so that calls that transition to a state already occupied are permitted.\n            anticompactionGroup.permitRedundantTransitions();\n            repairedSSTableWriter.setRepairedAt(repairedAt).prepareToCommit();\n            unRepairedSSTableWriter.prepareToCommit();\n            anticompactedSSTables.addAll(repairedSSTableWriter.finished());\n            anticompactedSSTables.addAll(unRepairedSSTableWriter.finished());\n            repairedSSTableWriter.commit();\n            unRepairedSSTableWriter.commit();\n\n            logger.trace(\"Repaired {} keys out of {} for {}/{} in {}\", repairedKeyCount,\n                                                                       repairedKeyCount + unrepairedKeyCount,\n                                                                       cfs.keyspace.getName(),\n                                                                       cfs.getColumnFamilyName(),\n                                                                       anticompactionGroup);\n            return anticompactedSSTables.size();\n        }\n        catch (Throwable e)\n        {\n            JVMStabilityInspector.inspectThrowable(e);\n            logger.error(\"Error anticompacting \" + anticompactionGroup, e);\n        }\n        return 0;\n    }"
        ],
        [
            "ColumnFamilyRecordReader::nextKeyValue()",
            " 176  \n 177  \n 178  \n 179  \n 180 -\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    public boolean nextKeyValue() throws IOException\n    {\n        if (!iter.hasNext())\n        {\n            logger.debug(\"Finished scanning {} rows (estimate was: {})\", iter.rowsRead(), totalRowCount);\n            return false;\n        }\n\n        currentRow = iter.next();\n        return true;\n    }",
            " 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    public boolean nextKeyValue() throws IOException\n    {\n        if (!iter.hasNext())\n        {\n            logger.trace(\"Finished scanning {} rows (estimate was: {})\", iter.rowsRead(), totalRowCount);\n            return false;\n        }\n\n        currentRow = iter.next();\n        return true;\n    }"
        ],
        [
            "CassandraRoleManager::convertLegacyData()",
            " 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 -\n 446  \n 447  \n 448  ",
            "    private void convertLegacyData() throws Exception\n    {\n        try\n        {\n            // read old data at QUORUM as it may contain the data for the default superuser\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"users\") != null)\n            {\n                logger.info(\"Converting legacy users\");\n                UntypedResultSet users = QueryProcessor.process(\"SELECT * FROM system_auth.users\",\n                                                                ConsistencyLevel.QUORUM);\n                for (UntypedResultSet.Row row : users)\n                {\n                    RoleOptions options = new RoleOptions();\n                    options.setOption(Option.SUPERUSER, row.getBoolean(\"super\"));\n                    options.setOption(Option.LOGIN, true);\n                    createRole(null, RoleResource.role(row.getString(\"name\")), options);\n                }\n                logger.info(\"Completed conversion of legacy users\");\n            }\n\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"credentials\") != null)\n            {\n                logger.info(\"Migrating legacy credentials data to new system table\");\n                UntypedResultSet credentials = QueryProcessor.process(\"SELECT * FROM system_auth.credentials\",\n                                                                      ConsistencyLevel.QUORUM);\n                for (UntypedResultSet.Row row : credentials)\n                {\n                    // Write the password directly into the table to avoid doubly encrypting it\n                    QueryProcessor.process(String.format(\"UPDATE %s.%s SET salted_hash = '%s' WHERE role = '%s'\",\n                                                         AuthKeyspace.NAME,\n                                                         AuthKeyspace.ROLES,\n                                                         row.getString(\"salted_hash\"),\n                                                         row.getString(\"username\")),\n                                           consistencyForRole(row.getString(\"username\")));\n                }\n                logger.info(\"Completed conversion of legacy credentials\");\n            }\n        }\n        catch (Exception e)\n        {\n            logger.info(\"Unable to complete conversion of legacy auth data (perhaps not enough nodes are upgraded yet). \" +\n                        \"Conversion should not be considered complete\");\n            logger.debug(\"Conversion error\", e);\n            throw e;\n        }\n    }",
            " 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  ",
            "    private void convertLegacyData() throws Exception\n    {\n        try\n        {\n            // read old data at QUORUM as it may contain the data for the default superuser\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"users\") != null)\n            {\n                logger.info(\"Converting legacy users\");\n                UntypedResultSet users = QueryProcessor.process(\"SELECT * FROM system_auth.users\",\n                                                                ConsistencyLevel.QUORUM);\n                for (UntypedResultSet.Row row : users)\n                {\n                    RoleOptions options = new RoleOptions();\n                    options.setOption(Option.SUPERUSER, row.getBoolean(\"super\"));\n                    options.setOption(Option.LOGIN, true);\n                    createRole(null, RoleResource.role(row.getString(\"name\")), options);\n                }\n                logger.info(\"Completed conversion of legacy users\");\n            }\n\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"credentials\") != null)\n            {\n                logger.info(\"Migrating legacy credentials data to new system table\");\n                UntypedResultSet credentials = QueryProcessor.process(\"SELECT * FROM system_auth.credentials\",\n                                                                      ConsistencyLevel.QUORUM);\n                for (UntypedResultSet.Row row : credentials)\n                {\n                    // Write the password directly into the table to avoid doubly encrypting it\n                    QueryProcessor.process(String.format(\"UPDATE %s.%s SET salted_hash = '%s' WHERE role = '%s'\",\n                                                         AuthKeyspace.NAME,\n                                                         AuthKeyspace.ROLES,\n                                                         row.getString(\"salted_hash\"),\n                                                         row.getString(\"username\")),\n                                           consistencyForRole(row.getString(\"username\")));\n                }\n                logger.info(\"Completed conversion of legacy credentials\");\n            }\n        }\n        catch (Exception e)\n        {\n            logger.info(\"Unable to complete conversion of legacy auth data (perhaps not enough nodes are upgraded yet). \" +\n                        \"Conversion should not be considered complete\");\n            logger.trace(\"Conversion error\", e);\n            throw e;\n        }\n    }"
        ],
        [
            "CassandraServer::execute_cql3_query(ByteBuffer,Compression,ConsistencyLevel)",
            "1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893 -\n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  ",
            "    public CqlResult execute_cql3_query(ByteBuffer query, Compression compression, ConsistencyLevel cLevel) throws TException\n    {\n        try\n        {\n            String queryString = uncompress(query, compression);\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"execute_cql3_query\",\n                                       ImmutableMap.of(\"query\", queryString,\n                                                       \"consistency_level\", cLevel.name()));\n            }\n            else\n            {\n                logger.debug(\"execute_cql3_query\");\n            }\n\n            ThriftClientState cState = state();\n            return ClientState.getCQLQueryHandler().process(queryString,\n                                                            cState.getQueryState(),\n                                                            QueryOptions.fromProtocolV2(ThriftConversion.fromThrift(cLevel), Collections.<ByteBuffer>emptyList()),\n                                                            null).toThriftResult();\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893 +\n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  ",
            "    public CqlResult execute_cql3_query(ByteBuffer query, Compression compression, ConsistencyLevel cLevel) throws TException\n    {\n        try\n        {\n            String queryString = uncompress(query, compression);\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"execute_cql3_query\",\n                                       ImmutableMap.of(\"query\", queryString,\n                                                       \"consistency_level\", cLevel.name()));\n            }\n            else\n            {\n                logger.trace(\"execute_cql3_query\");\n            }\n\n            ThriftClientState cState = state();\n            return ClientState.getCQLQueryHandler().process(queryString,\n                                                            cState.getQueryState(),\n                                                            QueryOptions.fromProtocolV2(ThriftConversion.fromThrift(cLevel), Collections.<ByteBuffer>emptyList()),\n                                                            null).toThriftResult();\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "LeveledCompactionStrategy::LeveledCompactionStrategy(ColumnFamilyStore,Map)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 -\n  75  ",
            "    public LeveledCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        super(cfs, options);\n        int configuredMaxSSTableSize = 160;\n        SizeTieredCompactionStrategyOptions localOptions = new SizeTieredCompactionStrategyOptions(options);\n        if (options != null)\n        {             \n            if (options.containsKey(SSTABLE_SIZE_OPTION))             \n            {                 \n                configuredMaxSSTableSize = Integer.parseInt(options.get(SSTABLE_SIZE_OPTION));                 \n                if (!Boolean.getBoolean(\"cassandra.tolerate_sstable_size\"))                 \n                {                     \n                    if (configuredMaxSSTableSize >= 1000)\n                        logger.warn(\"Max sstable size of {}MB is configured for {}.{}; having a unit of compaction this large is probably a bad idea\",\n                                configuredMaxSSTableSize, cfs.name, cfs.getColumnFamilyName());\n                    if (configuredMaxSSTableSize < 50)  \n                        logger.warn(\"Max sstable size of {}MB is configured for {}.{}.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB\",\n                                configuredMaxSSTableSize, cfs.name, cfs.getColumnFamilyName());\n                }\n            }\n        }\n        maxSSTableSizeInMB = configuredMaxSSTableSize;\n\n        manifest = new LeveledManifest(cfs, this.maxSSTableSizeInMB, localOptions);\n        logger.debug(\"Created {}\", manifest);\n    }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75  ",
            "    public LeveledCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        super(cfs, options);\n        int configuredMaxSSTableSize = 160;\n        SizeTieredCompactionStrategyOptions localOptions = new SizeTieredCompactionStrategyOptions(options);\n        if (options != null)\n        {             \n            if (options.containsKey(SSTABLE_SIZE_OPTION))             \n            {                 \n                configuredMaxSSTableSize = Integer.parseInt(options.get(SSTABLE_SIZE_OPTION));                 \n                if (!Boolean.getBoolean(\"cassandra.tolerate_sstable_size\"))                 \n                {                     \n                    if (configuredMaxSSTableSize >= 1000)\n                        logger.warn(\"Max sstable size of {}MB is configured for {}.{}; having a unit of compaction this large is probably a bad idea\",\n                                configuredMaxSSTableSize, cfs.name, cfs.getColumnFamilyName());\n                    if (configuredMaxSSTableSize < 50)  \n                        logger.warn(\"Max sstable size of {}MB is configured for {}.{}.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB\",\n                                configuredMaxSSTableSize, cfs.name, cfs.getColumnFamilyName());\n                }\n            }\n        }\n        maxSSTableSizeInMB = configuredMaxSSTableSize;\n\n        manifest = new LeveledManifest(cfs, this.maxSSTableSizeInMB, localOptions);\n        logger.trace(\"Created {}\", manifest);\n    }"
        ],
        [
            "CommitLogSegmentManager::stopUnsafe(boolean)",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 -\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 -\n 502  ",
            "    /**\n     * Stops CL, for testing purposes. DO NOT USE THIS OUTSIDE OF TESTS.\n     * Only call this after the AbstractCommitLogService is shut down.\n     */\n    public void stopUnsafe(boolean deleteSegments)\n    {\n        logger.debug(\"CLSM closing and clearing existing commit log segments...\");\n        createReserveSegments = false;\n\n        awaitManagementTasksCompletion();\n\n        shutdown();\n        try\n        {\n            awaitTermination();\n        }\n        catch (InterruptedException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        for (CommitLogSegment segment : activeSegments)\n            closeAndDeleteSegmentUnsafe(segment, deleteSegments);\n        activeSegments.clear();\n\n        for (CommitLogSegment segment : availableSegments)\n            closeAndDeleteSegmentUnsafe(segment, deleteSegments);\n        availableSegments.clear();\n\n        allocatingFrom = null;\n\n        segmentManagementTasks.clear();\n\n        size.set(0L);\n\n        logger.debug(\"CLSM done with closing and clearing existing commit log segments.\");\n    }",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 +\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 +\n 502  ",
            "    /**\n     * Stops CL, for testing purposes. DO NOT USE THIS OUTSIDE OF TESTS.\n     * Only call this after the AbstractCommitLogService is shut down.\n     */\n    public void stopUnsafe(boolean deleteSegments)\n    {\n        logger.trace(\"CLSM closing and clearing existing commit log segments...\");\n        createReserveSegments = false;\n\n        awaitManagementTasksCompletion();\n\n        shutdown();\n        try\n        {\n            awaitTermination();\n        }\n        catch (InterruptedException e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        for (CommitLogSegment segment : activeSegments)\n            closeAndDeleteSegmentUnsafe(segment, deleteSegments);\n        activeSegments.clear();\n\n        for (CommitLogSegment segment : availableSegments)\n            closeAndDeleteSegmentUnsafe(segment, deleteSegments);\n        availableSegments.clear();\n\n        allocatingFrom = null;\n\n        segmentManagementTasks.clear();\n\n        size.set(0L);\n\n        logger.trace(\"CLSM done with closing and clearing existing commit log segments.\");\n    }"
        ],
        [
            "CompactionTask::runMayThrow()",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226 -\n 227 -\n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert transaction != null;\n\n        if (transaction.originals().isEmpty())\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        // note that we need to do a rough estimate early if we can fit the compaction on disk - this is pessimistic, but\n        // since we might remove sstables from the compaction in checkAvailableDiskSpace it needs to be done here\n        long expectedWriteSize = cfs.getExpectedCompactedFileSize(transaction.originals(), compactionType);\n        long earlySSTableEstimate = Math.max(1, expectedWriteSize / strategy.getMaxSSTableBytes());\n        checkAvailableDiskSpace(earlySSTableEstimate, expectedWriteSize);\n\n        // sanity check: all sstables must belong to the same cfs\n        assert !Iterables.any(transaction.originals(), new Predicate<SSTableReader>()\n        {\n            @Override\n            public boolean apply(SSTableReader sstable)\n            {\n                return !sstable.descriptor.cfname.equals(cfs.name);\n            }\n        });\n\n        UUID taskId = SystemKeyspace.startCompaction(cfs, transaction.originals());\n\n        // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n        // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n        // all the sstables (that existed when we started)\n        StringBuilder ssTableLoggerMsg = new StringBuilder(\"[\");\n        for (SSTableReader sstr : transaction.originals())\n        {\n            ssTableLoggerMsg.append(String.format(\"%s:level=%d, \", sstr.getFilename(), sstr.getSSTableLevel()));\n        }\n        ssTableLoggerMsg.append(\"]\");\n        String taskIdLoggerMsg = taskId == null ? UUIDGen.getTimeUUID().toString() : taskId.toString();\n        logger.info(\"Compacting ({}) {}\", taskIdLoggerMsg, ssTableLoggerMsg);\n\n        long start = System.nanoTime();\n\n        long totalKeysWritten = 0;\n\n        long estimatedKeys = 0;\n        try (CompactionController controller = getCompactionController(transaction.originals()))\n        {\n            Set<SSTableReader> actuallyCompact = Sets.difference(transaction.originals(), controller.getFullyExpiredSSTables());\n\n            SSTableFormat.Type sstableFormat = getFormatType(transaction.originals());\n\n            List<SSTableReader> newSStables;\n            AbstractCompactionIterable ci;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact))\n            {\n                ci = new CompactionIterable(compactionType, scanners.scanners, controller, sstableFormat, taskId);\n                try (CloseableIterator<AbstractCompactedRow> iter = ci.iterator())\n                {\n                    if (collector != null)\n                        collector.beginCompaction(ci);\n                    long lastCheckObsoletion = start;\n\n                    if (!controller.cfs.getCompactionStrategy().isActive)\n                        throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                    try (CompactionAwareWriter writer = getCompactionAwareWriter(cfs, transaction, actuallyCompact))\n                    {\n                        estimatedKeys = writer.estimatedKeys();\n                        while (iter.hasNext())\n                        {\n                            if (ci.isStopRequested())\n                                throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                            try (AbstractCompactedRow row = iter.next())\n                            {\n                                if (writer.append(row))\n                                    totalKeysWritten++;\n\n                                if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                                {\n                                    controller.maybeRefreshOverlaps();\n                                    lastCheckObsoletion = System.nanoTime();\n                                }\n                            }\n                        }\n\n                        // don't replace old sstables yet, as we need to mark the compaction finished in the system table\n                        newSStables = writer.finish();\n                    }\n                    finally\n                    {\n                        // point of no return -- the new sstables are live on disk; next we'll start deleting the old ones\n                        // (in replaceCompactedSSTables)\n                        if (taskId != null)\n                            SystemKeyspace.finishCompaction(taskId);\n\n                        if (collector != null)\n                            collector.finishCompaction(ci);\n                    }\n                }\n            }\n\n            // log a bunch of statistics about the result and save to system table compaction_history\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = SSTableReader.getTotalBytes(transaction.originals());\n            long endsize = SSTableReader.getTotalBytes(newSStables);\n            double ratio = (double) endsize / (double) startsize;\n\n            StringBuilder newSSTableNames = new StringBuilder();\n            for (SSTableReader reader : newSStables)\n                newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n\n            double mbps = dTime > 0 ? (double) endsize / (1024 * 1024) / ((double) dTime / 1000) : 0;\n            long totalSourceRows = 0;\n            String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getColumnFamilyName(), ci, startsize, endsize);\n            logger.info(String.format(\"Compacted (%s) %d sstables to [%s] to level=%d.  %,d bytes to %,d (~%d%% of original) in %,dms = %fMB/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                      taskIdLoggerMsg, transaction.originals().size(), newSSTableNames.toString(), getLevel(), startsize, endsize, (int) (ratio * 100), dTime, mbps, totalSourceRows, totalKeysWritten, mergeSummary));\n            logger.debug(String.format(\"CF Total Bytes Compacted: %,d\", CompactionTask.addToTotalBytesCompacted(endsize)));\n            logger.debug(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedKeys, ((double)(totalKeysWritten - estimatedKeys)/totalKeysWritten));\n\n            if (offline)\n                Refs.release(Refs.selfRefs(newSStables));\n        }\n    }",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 +\n 225  \n 226 +\n 227 +\n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    /**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\n    protected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert transaction != null;\n\n        if (transaction.originals().isEmpty())\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        AbstractCompactionStrategy strategy = cfs.getCompactionStrategy();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        // note that we need to do a rough estimate early if we can fit the compaction on disk - this is pessimistic, but\n        // since we might remove sstables from the compaction in checkAvailableDiskSpace it needs to be done here\n        long expectedWriteSize = cfs.getExpectedCompactedFileSize(transaction.originals(), compactionType);\n        long earlySSTableEstimate = Math.max(1, expectedWriteSize / strategy.getMaxSSTableBytes());\n        checkAvailableDiskSpace(earlySSTableEstimate, expectedWriteSize);\n\n        // sanity check: all sstables must belong to the same cfs\n        assert !Iterables.any(transaction.originals(), new Predicate<SSTableReader>()\n        {\n            @Override\n            public boolean apply(SSTableReader sstable)\n            {\n                return !sstable.descriptor.cfname.equals(cfs.name);\n            }\n        });\n\n        UUID taskId = SystemKeyspace.startCompaction(cfs, transaction.originals());\n\n        // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n        // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n        // all the sstables (that existed when we started)\n        StringBuilder ssTableLoggerMsg = new StringBuilder(\"[\");\n        for (SSTableReader sstr : transaction.originals())\n        {\n            ssTableLoggerMsg.append(String.format(\"%s:level=%d, \", sstr.getFilename(), sstr.getSSTableLevel()));\n        }\n        ssTableLoggerMsg.append(\"]\");\n        String taskIdLoggerMsg = taskId == null ? UUIDGen.getTimeUUID().toString() : taskId.toString();\n        logger.debug(\"Compacting ({}) {}\", taskIdLoggerMsg, ssTableLoggerMsg);\n\n        long start = System.nanoTime();\n\n        long totalKeysWritten = 0;\n\n        long estimatedKeys = 0;\n        try (CompactionController controller = getCompactionController(transaction.originals()))\n        {\n            Set<SSTableReader> actuallyCompact = Sets.difference(transaction.originals(), controller.getFullyExpiredSSTables());\n\n            SSTableFormat.Type sstableFormat = getFormatType(transaction.originals());\n\n            List<SSTableReader> newSStables;\n            AbstractCompactionIterable ci;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact))\n            {\n                ci = new CompactionIterable(compactionType, scanners.scanners, controller, sstableFormat, taskId);\n                try (CloseableIterator<AbstractCompactedRow> iter = ci.iterator())\n                {\n                    if (collector != null)\n                        collector.beginCompaction(ci);\n                    long lastCheckObsoletion = start;\n\n                    if (!controller.cfs.getCompactionStrategy().isActive)\n                        throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                    try (CompactionAwareWriter writer = getCompactionAwareWriter(cfs, transaction, actuallyCompact))\n                    {\n                        estimatedKeys = writer.estimatedKeys();\n                        while (iter.hasNext())\n                        {\n                            if (ci.isStopRequested())\n                                throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                            try (AbstractCompactedRow row = iter.next())\n                            {\n                                if (writer.append(row))\n                                    totalKeysWritten++;\n\n                                if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                                {\n                                    controller.maybeRefreshOverlaps();\n                                    lastCheckObsoletion = System.nanoTime();\n                                }\n                            }\n                        }\n\n                        // don't replace old sstables yet, as we need to mark the compaction finished in the system table\n                        newSStables = writer.finish();\n                    }\n                    finally\n                    {\n                        // point of no return -- the new sstables are live on disk; next we'll start deleting the old ones\n                        // (in replaceCompactedSSTables)\n                        if (taskId != null)\n                            SystemKeyspace.finishCompaction(taskId);\n\n                        if (collector != null)\n                            collector.finishCompaction(ci);\n                    }\n                }\n            }\n\n            // log a bunch of statistics about the result and save to system table compaction_history\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = SSTableReader.getTotalBytes(transaction.originals());\n            long endsize = SSTableReader.getTotalBytes(newSStables);\n            double ratio = (double) endsize / (double) startsize;\n\n            StringBuilder newSSTableNames = new StringBuilder();\n            for (SSTableReader reader : newSStables)\n                newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n\n            double mbps = dTime > 0 ? (double) endsize / (1024 * 1024) / ((double) dTime / 1000) : 0;\n            long totalSourceRows = 0;\n            String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getColumnFamilyName(), ci, startsize, endsize);\n            logger.debug(String.format(\"Compacted (%s) %d sstables to [%s] to level=%d.  %,d bytes to %,d (~%d%% of original) in %,dms = %fMB/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                      taskIdLoggerMsg, transaction.originals().size(), newSSTableNames.toString(), getLevel(), startsize, endsize, (int) (ratio * 100), dTime, mbps, totalSourceRows, totalKeysWritten, mergeSummary));\n            logger.trace(String.format(\"CF Total Bytes Compacted: %,d\", CompactionTask.addToTotalBytesCompacted(endsize)));\n            logger.trace(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedKeys, ((double)(totalKeysWritten - estimatedKeys)/totalKeysWritten));\n\n            if (offline)\n                Refs.release(Refs.selfRefs(newSStables));\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::FlushLargestColumnFamily::run()",
            "1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211 -\n1212  \n1213  \n1214  \n1215  \n1216  ",
            "        public void run()\n        {\n            float largestRatio = 0f;\n            Memtable largest = null;\n            float liveOnHeap = 0, liveOffHeap = 0;\n            for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n            {\n                // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n                // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n                // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n                Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n                // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n                // both on- and off-heap, and select the largest of the two ratios to weight this CF\n                float onHeap = 0f, offHeap = 0f;\n                onHeap += current.getAllocator().onHeap().ownershipRatio();\n                offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n                for (SecondaryIndex index : cfs.indexManager.getIndexes())\n                {\n                    if (index.getIndexCfs() != null)\n                    {\n                        MemtableAllocator allocator = index.getIndexCfs().getTracker().getView().getCurrentMemtable().getAllocator();\n                        onHeap += allocator.onHeap().ownershipRatio();\n                        offHeap += allocator.offHeap().ownershipRatio();\n                    }\n                }\n\n                float ratio = Math.max(onHeap, offHeap);\n                if (ratio > largestRatio)\n                {\n                    largest = current;\n                    largestRatio = ratio;\n                }\n\n                liveOnHeap += onHeap;\n                liveOffHeap += offHeap;\n            }\n\n            if (largest != null)\n            {\n                float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n                float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n                float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n                float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n                float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n                float thisOffHeap = largest.getAllocator().onHeap().ownershipRatio();\n                logger.info(\"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}\",\n                            largest.cfs, ratio(usedOnHeap, usedOffHeap), ratio(liveOnHeap, liveOffHeap),\n                            ratio(flushingOnHeap, flushingOffHeap), ratio(thisOnHeap, thisOffHeap));\n                largest.cfs.switchMemtableIfCurrent(largest);\n            }\n        }",
            "1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211 +\n1212  \n1213  \n1214  \n1215  \n1216  ",
            "        public void run()\n        {\n            float largestRatio = 0f;\n            Memtable largest = null;\n            float liveOnHeap = 0, liveOffHeap = 0;\n            for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n            {\n                // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n                // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n                // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n                Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n                // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n                // both on- and off-heap, and select the largest of the two ratios to weight this CF\n                float onHeap = 0f, offHeap = 0f;\n                onHeap += current.getAllocator().onHeap().ownershipRatio();\n                offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n                for (SecondaryIndex index : cfs.indexManager.getIndexes())\n                {\n                    if (index.getIndexCfs() != null)\n                    {\n                        MemtableAllocator allocator = index.getIndexCfs().getTracker().getView().getCurrentMemtable().getAllocator();\n                        onHeap += allocator.onHeap().ownershipRatio();\n                        offHeap += allocator.offHeap().ownershipRatio();\n                    }\n                }\n\n                float ratio = Math.max(onHeap, offHeap);\n                if (ratio > largestRatio)\n                {\n                    largest = current;\n                    largestRatio = ratio;\n                }\n\n                liveOnHeap += onHeap;\n                liveOffHeap += offHeap;\n            }\n\n            if (largest != null)\n            {\n                float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n                float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n                float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n                float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n                float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n                float thisOffHeap = largest.getAllocator().onHeap().ownershipRatio();\n                logger.debug(\"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}\",\n                            largest.cfs, ratio(usedOnHeap, usedOffHeap), ratio(liveOnHeap, liveOffHeap),\n                            ratio(flushingOnHeap, flushingOffHeap), ratio(thisOnHeap, thisOffHeap));\n                largest.cfs.switchMemtableIfCurrent(largest);\n            }\n        }"
        ],
        [
            "CompactionManager::submitCacheWrite(AutoSavingCache)",
            "1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298 -\n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  ",
            "    public Future<?> submitCacheWrite(final AutoSavingCache.Writer writer)\n    {\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                if (!AutoSavingCache.flushInProgress.add(writer.cacheType()))\n                {\n                    logger.debug(\"Cache flushing was already in progress: skipping {}\", writer.getCompactionInfo());\n                    return;\n                }\n                try\n                {\n                    metrics.beginCompaction(writer);\n                    try\n                    {\n                        writer.saveCache();\n                    }\n                    finally\n                    {\n                        metrics.finishCompaction(writer);\n                    }\n                }\n                finally\n                {\n                    AutoSavingCache.flushInProgress.remove(writer.cacheType());\n                }\n            }\n        };\n        if (executor.isShutdown())\n        {\n            logger.info(\"Executor has shut down, not submitting background task\");\n            Futures.immediateCancelledFuture();\n        }\n        return executor.submit(runnable);\n    }",
            "1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298 +\n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  ",
            "    public Future<?> submitCacheWrite(final AutoSavingCache.Writer writer)\n    {\n        Runnable runnable = new Runnable()\n        {\n            public void run()\n            {\n                if (!AutoSavingCache.flushInProgress.add(writer.cacheType()))\n                {\n                    logger.trace(\"Cache flushing was already in progress: skipping {}\", writer.getCompactionInfo());\n                    return;\n                }\n                try\n                {\n                    metrics.beginCompaction(writer);\n                    try\n                    {\n                        writer.saveCache();\n                    }\n                    finally\n                    {\n                        metrics.finishCompaction(writer);\n                    }\n                }\n                finally\n                {\n                    AutoSavingCache.flushInProgress.remove(writer.cacheType());\n                }\n            }\n        };\n        if (executor.isShutdown())\n        {\n            logger.info(\"Executor has shut down, not submitting background task\");\n            Futures.immediateCancelledFuture();\n        }\n        return executor.submit(runnable);\n    }"
        ],
        [
            "ColumnFamilyStore::runWithCompactionsDisabled(Callable,boolean)",
            "2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692 -\n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712 -\n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  ",
            "    public <V> V runWithCompactionsDisabled(Callable<V> callable, boolean interruptValidation)\n    {\n        // synchronize so that concurrent invocations don't re-enable compactions partway through unexpectedly,\n        // and so we only run one major compaction at a time\n        synchronized (this)\n        {\n            logger.debug(\"Cancelling in-progress compactions for {}\", metadata.cfName);\n\n            Iterable<ColumnFamilyStore> selfWithIndexes = concatWithIndexes();\n            for (ColumnFamilyStore cfs : selfWithIndexes)\n                cfs.getCompactionStrategy().pause();\n            try\n            {\n                // interrupt in-progress compactions\n                CompactionManager.instance.interruptCompactionForCFs(selfWithIndexes, interruptValidation);\n                CompactionManager.instance.waitForCessation(selfWithIndexes);\n\n                // doublecheck that we finished, instead of timing out\n                for (ColumnFamilyStore cfs : selfWithIndexes)\n                {\n                    if (!cfs.getTracker().getCompacting().isEmpty())\n                    {\n                        logger.warn(\"Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.\", metadata.cfName);\n                        return null;\n                    }\n                }\n                logger.debug(\"Compactions successfully cancelled\");\n\n                // run our task\n                try\n                {\n                    return callable.call();\n                }\n                catch (Exception e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n            finally\n            {\n                for (ColumnFamilyStore cfs : selfWithIndexes)\n                    cfs.getCompactionStrategy().resume();\n            }\n        }\n    }",
            "2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692 +\n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712 +\n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  ",
            "    public <V> V runWithCompactionsDisabled(Callable<V> callable, boolean interruptValidation)\n    {\n        // synchronize so that concurrent invocations don't re-enable compactions partway through unexpectedly,\n        // and so we only run one major compaction at a time\n        synchronized (this)\n        {\n            logger.trace(\"Cancelling in-progress compactions for {}\", metadata.cfName);\n\n            Iterable<ColumnFamilyStore> selfWithIndexes = concatWithIndexes();\n            for (ColumnFamilyStore cfs : selfWithIndexes)\n                cfs.getCompactionStrategy().pause();\n            try\n            {\n                // interrupt in-progress compactions\n                CompactionManager.instance.interruptCompactionForCFs(selfWithIndexes, interruptValidation);\n                CompactionManager.instance.waitForCessation(selfWithIndexes);\n\n                // doublecheck that we finished, instead of timing out\n                for (ColumnFamilyStore cfs : selfWithIndexes)\n                {\n                    if (!cfs.getTracker().getCompacting().isEmpty())\n                    {\n                        logger.warn(\"Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.\", metadata.cfName);\n                        return null;\n                    }\n                }\n                logger.trace(\"Compactions successfully cancelled\");\n\n                // run our task\n                try\n                {\n                    return callable.call();\n                }\n                catch (Exception e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n            finally\n            {\n                for (ColumnFamilyStore cfs : selfWithIndexes)\n                    cfs.getCompactionStrategy().resume();\n            }\n        }\n    }"
        ],
        [
            "CompactionController::getFullyExpiredSSTables(ColumnFamilyStore,Iterable,Iterable,int)",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  ",
            "    /**\n     * Finds expired sstables\n     *\n     * works something like this;\n     * 1. find \"global\" minTimestamp of overlapping sstables and compacting sstables containing any non-expired data\n     * 2. build a list of fully expired candidates\n     * 3. check if the candidates to be dropped actually can be dropped (maxTimestamp < global minTimestamp)\n     *    - if not droppable, remove from candidates\n     * 4. return candidates.\n     *\n     * @param cfStore\n     * @param compacting we take the drop-candidates from this set, it is usually the sstables included in the compaction\n     * @param overlapping the sstables that overlap the ones in compacting.\n     * @param gcBefore\n     * @return\n     */\n    public static Set<SSTableReader> getFullyExpiredSSTables(ColumnFamilyStore cfStore, Iterable<SSTableReader> compacting, Iterable<SSTableReader> overlapping, int gcBefore)\n    {\n        logger.debug(\"Checking droppable sstables in {}\", cfStore);\n\n        if (compacting == null)\n            return Collections.<SSTableReader>emptySet();\n\n        List<SSTableReader> candidates = new ArrayList<>();\n\n        long minTimestamp = Long.MAX_VALUE;\n\n        for (SSTableReader sstable : overlapping)\n        {\n            // Overlapping might include fully expired sstables. What we care about here is\n            // the min timestamp of the overlapping sstables that actually contain live data.\n            if (sstable.getSSTableMetadata().maxLocalDeletionTime >= gcBefore)\n                minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());\n        }\n\n        for (SSTableReader candidate : compacting)\n        {\n            if (candidate.getSSTableMetadata().maxLocalDeletionTime < gcBefore)\n                candidates.add(candidate);\n            else\n                minTimestamp = Math.min(minTimestamp, candidate.getMinTimestamp());\n        }\n\n        // At this point, minTimestamp denotes the lowest timestamp of any relevant\n        // SSTable that contains a constructive value. candidates contains all the\n        // candidates with no constructive values. The ones out of these that have\n        // (getMaxTimestamp() < minTimestamp) serve no purpose anymore.\n\n        Iterator<SSTableReader> iterator = candidates.iterator();\n        while (iterator.hasNext())\n        {\n            SSTableReader candidate = iterator.next();\n            if (candidate.getMaxTimestamp() >= minTimestamp)\n            {\n                iterator.remove();\n            }\n            else\n            {\n               logger.debug(\"Dropping expired SSTable {} (maxLocalDeletionTime={}, gcBefore={})\",\n                        candidate, candidate.getSSTableMetadata().maxLocalDeletionTime, gcBefore);\n            }\n        }\n        return new HashSet<>(candidates);\n    }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  ",
            "    /**\n     * Finds expired sstables\n     *\n     * works something like this;\n     * 1. find \"global\" minTimestamp of overlapping sstables and compacting sstables containing any non-expired data\n     * 2. build a list of fully expired candidates\n     * 3. check if the candidates to be dropped actually can be dropped (maxTimestamp < global minTimestamp)\n     *    - if not droppable, remove from candidates\n     * 4. return candidates.\n     *\n     * @param cfStore\n     * @param compacting we take the drop-candidates from this set, it is usually the sstables included in the compaction\n     * @param overlapping the sstables that overlap the ones in compacting.\n     * @param gcBefore\n     * @return\n     */\n    public static Set<SSTableReader> getFullyExpiredSSTables(ColumnFamilyStore cfStore, Iterable<SSTableReader> compacting, Iterable<SSTableReader> overlapping, int gcBefore)\n    {\n        logger.trace(\"Checking droppable sstables in {}\", cfStore);\n\n        if (compacting == null)\n            return Collections.<SSTableReader>emptySet();\n\n        List<SSTableReader> candidates = new ArrayList<>();\n\n        long minTimestamp = Long.MAX_VALUE;\n\n        for (SSTableReader sstable : overlapping)\n        {\n            // Overlapping might include fully expired sstables. What we care about here is\n            // the min timestamp of the overlapping sstables that actually contain live data.\n            if (sstable.getSSTableMetadata().maxLocalDeletionTime >= gcBefore)\n                minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());\n        }\n\n        for (SSTableReader candidate : compacting)\n        {\n            if (candidate.getSSTableMetadata().maxLocalDeletionTime < gcBefore)\n                candidates.add(candidate);\n            else\n                minTimestamp = Math.min(minTimestamp, candidate.getMinTimestamp());\n        }\n\n        // At this point, minTimestamp denotes the lowest timestamp of any relevant\n        // SSTable that contains a constructive value. candidates contains all the\n        // candidates with no constructive values. The ones out of these that have\n        // (getMaxTimestamp() < minTimestamp) serve no purpose anymore.\n\n        Iterator<SSTableReader> iterator = candidates.iterator();\n        while (iterator.hasNext())\n        {\n            SSTableReader candidate = iterator.next();\n            if (candidate.getMaxTimestamp() >= minTimestamp)\n            {\n                iterator.remove();\n            }\n            else\n            {\n               logger.trace(\"Dropping expired SSTable {} (maxLocalDeletionTime={}, gcBefore={})\",\n                        candidate, candidate.getSSTableMetadata().maxLocalDeletionTime, gcBefore);\n            }\n        }\n        return new HashSet<>(candidates);\n    }"
        ],
        [
            "Message::UnexpectedChannelExceptionHandler::apply(Throwable)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600 -\n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  ",
            "        @Override\n        public boolean apply(Throwable exception)\n        {\n            String message;\n            try\n            {\n                message = \"Unexpected exception during request; channel = \" + channel;\n            }\n            catch (Exception ignore)\n            {\n                // We don't want to make things worse if String.valueOf() throws an exception\n                message = \"Unexpected exception during request; channel = <unprintable>\";\n            }\n\n            if (!alwaysLogAtError && exception instanceof IOException)\n            {\n                if (ioExceptionsAtDebugLevel.contains(exception.getMessage()))\n                {\n                    // Likely unclean client disconnects\n                    logger.debug(message, exception);\n                }\n                else\n                {\n                    // Generally unhandled IO exceptions are network issues, not actual ERRORS\n                    logger.info(message, exception);\n                }\n            }\n            else\n            {\n                // Anything else is probably a bug in server of client binary protocol handling\n                logger.error(message, exception);\n            }\n\n            // We handled the exception.\n            return true;\n        }",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600 +\n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  ",
            "        @Override\n        public boolean apply(Throwable exception)\n        {\n            String message;\n            try\n            {\n                message = \"Unexpected exception during request; channel = \" + channel;\n            }\n            catch (Exception ignore)\n            {\n                // We don't want to make things worse if String.valueOf() throws an exception\n                message = \"Unexpected exception during request; channel = <unprintable>\";\n            }\n\n            if (!alwaysLogAtError && exception instanceof IOException)\n            {\n                if (ioExceptionsAtDebugLevel.contains(exception.getMessage()))\n                {\n                    // Likely unclean client disconnects\n                    logger.trace(message, exception);\n                }\n                else\n                {\n                    // Generally unhandled IO exceptions are network issues, not actual ERRORS\n                    logger.info(message, exception);\n                }\n            }\n            else\n            {\n                // Anything else is probably a bug in server of client binary protocol handling\n                logger.error(message, exception);\n            }\n\n            // We handled the exception.\n            return true;\n        }"
        ],
        [
            "HintedHandOffManager::waitForSchemaAgreement(InetAddress)",
            " 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321  \n 322  ",
            "    private int waitForSchemaAgreement(InetAddress endpoint) throws TimeoutException\n    {\n        Gossiper gossiper = Gossiper.instance;\n        int waited = 0;\n        // first, wait for schema to be gossiped.\n        while (gossiper.getEndpointStateForEndpoint(endpoint) != null && gossiper.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.SCHEMA) == null)\n        {\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n            waited += 1000;\n            if (waited > 2 * StorageService.RING_DELAY)\n                throw new TimeoutException(\"Didin't receive gossiped schema from \" + endpoint + \" in \" + 2 * StorageService.RING_DELAY + \"ms\");\n        }\n        if (gossiper.getEndpointStateForEndpoint(endpoint) == null)\n            throw new TimeoutException(\"Node \" + endpoint + \" vanished while waiting for agreement\");\n        waited = 0;\n        // then wait for the correct schema version.\n        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system keyspace.\n        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that\n        // causes the two to diverge (see CASSANDRA-2946)\n        while (gossiper.getEndpointStateForEndpoint(endpoint) != null && !gossiper.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.SCHEMA).value.equals(\n                gossiper.getEndpointStateForEndpoint(FBUtilities.getBroadcastAddress()).getApplicationState(ApplicationState.SCHEMA).value))\n        {\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n            waited += 1000;\n            if (waited > 2 * StorageService.RING_DELAY)\n                throw new TimeoutException(\"Could not reach schema agreement with \" + endpoint + \" in \" + 2 * StorageService.RING_DELAY + \"ms\");\n        }\n        if (gossiper.getEndpointStateForEndpoint(endpoint) == null)\n            throw new TimeoutException(\"Node \" + endpoint + \" vanished while waiting for agreement\");\n        logger.debug(\"schema for {} matches local schema\", endpoint);\n        return waited;\n    }",
            " 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321  \n 322  ",
            "    private int waitForSchemaAgreement(InetAddress endpoint) throws TimeoutException\n    {\n        Gossiper gossiper = Gossiper.instance;\n        int waited = 0;\n        // first, wait for schema to be gossiped.\n        while (gossiper.getEndpointStateForEndpoint(endpoint) != null && gossiper.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.SCHEMA) == null)\n        {\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n            waited += 1000;\n            if (waited > 2 * StorageService.RING_DELAY)\n                throw new TimeoutException(\"Didin't receive gossiped schema from \" + endpoint + \" in \" + 2 * StorageService.RING_DELAY + \"ms\");\n        }\n        if (gossiper.getEndpointStateForEndpoint(endpoint) == null)\n            throw new TimeoutException(\"Node \" + endpoint + \" vanished while waiting for agreement\");\n        waited = 0;\n        // then wait for the correct schema version.\n        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system keyspace.\n        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that\n        // causes the two to diverge (see CASSANDRA-2946)\n        while (gossiper.getEndpointStateForEndpoint(endpoint) != null && !gossiper.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.SCHEMA).value.equals(\n                gossiper.getEndpointStateForEndpoint(FBUtilities.getBroadcastAddress()).getApplicationState(ApplicationState.SCHEMA).value))\n        {\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n            waited += 1000;\n            if (waited > 2 * StorageService.RING_DELAY)\n                throw new TimeoutException(\"Could not reach schema agreement with \" + endpoint + \" in \" + 2 * StorageService.RING_DELAY + \"ms\");\n        }\n        if (gossiper.getEndpointStateForEndpoint(endpoint) == null)\n            throw new TimeoutException(\"Node \" + endpoint + \" vanished while waiting for agreement\");\n        logger.trace(\"schema for {} matches local schema\", endpoint);\n        return waited;\n    }"
        ],
        [
            "CommitLogSegmentManager::unusedCapacity()",
            " 396  \n 397  \n 398  \n 399  \n 400 -\n 401  \n 402  ",
            "    private long unusedCapacity()\n    {\n        long total = DatabaseDescriptor.getTotalCommitlogSpaceInMB() * 1024 * 1024;\n        long currentSize = size.get();\n        logger.debug(\"Total active commitlog segment space used is {} out of {}\", currentSize, total);\n        return total - currentSize;\n    }",
            " 396  \n 397  \n 398  \n 399  \n 400 +\n 401  \n 402  ",
            "    private long unusedCapacity()\n    {\n        long total = DatabaseDescriptor.getTotalCommitlogSpaceInMB() * 1024 * 1024;\n        long currentSize = size.get();\n        logger.trace(\"Total active commitlog segment space used is {} out of {}\", currentSize, total);\n        return total - currentSize;\n    }"
        ],
        [
            "ColumnFamilyStore::scrubDataDirectories(CFMetaData)",
            " 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573 -\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  ",
            "    /**\n     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files\n     * and compacted sstables. Files that cannot be recognized will be ignored.\n     */\n    public static void scrubDataDirectories(CFMetaData metadata)\n    {\n        Directories directories = new Directories(metadata);\n\n        // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)\n        clearEphemeralSnapshots(directories);\n\n        // remove any left-behind SSTables from failed/stalled streaming\n        FileFilter filter = new FileFilter()\n        {\n            public boolean accept(File pathname)\n            {\n                return pathname.getPath().endsWith(StreamLockfile.FILE_EXT);\n            }\n        };\n        for (File dir : directories.getCFDirectories())\n        {\n            File[] lockfiles = dir.listFiles(filter);\n            // lock files can be null if I/O error happens\n            if (lockfiles == null || lockfiles.length == 0)\n                continue;\n            logger.info(\"Removing SSTables from failed streaming session. Found {} files to cleanup.\", lockfiles.length);\n\n            for (File lockfile : lockfiles)\n            {\n                StreamLockfile streamLockfile = new StreamLockfile(lockfile);\n                streamLockfile.cleanup();\n                streamLockfile.delete();\n            }\n        }\n\n        logger.debug(\"Removing compacted SSTable files from {} (see http://wiki.apache.org/cassandra/MemtableSSTable)\", metadata.cfName);\n\n        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister().list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            Set<Component> components = sstableFiles.getValue();\n\n            if (desc.type.isTemporary)\n            {\n                SSTable.delete(desc, components);\n                continue;\n            }\n\n            File dataFile = new File(desc.filenameFor(Component.DATA));\n            if (components.contains(Component.DATA) && dataFile.length() > 0)\n                // everything appears to be in order... moving on.\n                continue;\n\n            // missing the DATA file! all components are orphaned\n            logger.warn(\"Removing orphans for {}: {}\", desc, components);\n            for (Component component : components)\n            {\n                FileUtils.deleteWithConfirm(desc.filenameFor(component));\n            }\n        }\n\n        // cleanup incomplete saved caches\n        Pattern tmpCacheFilePattern = Pattern.compile(metadata.ksName + \"-\" + metadata.cfName + \"-(Key|Row)Cache.*\\\\.tmp$\");\n        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());\n\n        if (dir.exists())\n        {\n            assert dir.isDirectory();\n            for (File file : dir.listFiles())\n                if (tmpCacheFilePattern.matcher(file.getName()).matches())\n                    if (!file.delete())\n                        logger.warn(\"could not delete {}\", file.getAbsolutePath());\n        }\n\n        // also clean out any index leftovers.\n        for (ColumnDefinition def : metadata.allColumns())\n        {\n            if (def.isIndexed())\n            {\n                CellNameType indexComparator = SecondaryIndex.getIndexComparator(metadata, def);\n                if (indexComparator != null)\n                {\n                    CFMetaData indexMetadata = CFMetaData.newIndexMetadata(metadata, def, indexComparator);\n                    scrubDataDirectories(indexMetadata);\n                }\n            }\n        }\n    }",
            " 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573 +\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  ",
            "    /**\n     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files\n     * and compacted sstables. Files that cannot be recognized will be ignored.\n     */\n    public static void scrubDataDirectories(CFMetaData metadata)\n    {\n        Directories directories = new Directories(metadata);\n\n        // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)\n        clearEphemeralSnapshots(directories);\n\n        // remove any left-behind SSTables from failed/stalled streaming\n        FileFilter filter = new FileFilter()\n        {\n            public boolean accept(File pathname)\n            {\n                return pathname.getPath().endsWith(StreamLockfile.FILE_EXT);\n            }\n        };\n        for (File dir : directories.getCFDirectories())\n        {\n            File[] lockfiles = dir.listFiles(filter);\n            // lock files can be null if I/O error happens\n            if (lockfiles == null || lockfiles.length == 0)\n                continue;\n            logger.info(\"Removing SSTables from failed streaming session. Found {} files to cleanup.\", lockfiles.length);\n\n            for (File lockfile : lockfiles)\n            {\n                StreamLockfile streamLockfile = new StreamLockfile(lockfile);\n                streamLockfile.cleanup();\n                streamLockfile.delete();\n            }\n        }\n\n        logger.trace(\"Removing compacted SSTable files from {} (see http://wiki.apache.org/cassandra/MemtableSSTable)\", metadata.cfName);\n\n        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister().list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            Set<Component> components = sstableFiles.getValue();\n\n            if (desc.type.isTemporary)\n            {\n                SSTable.delete(desc, components);\n                continue;\n            }\n\n            File dataFile = new File(desc.filenameFor(Component.DATA));\n            if (components.contains(Component.DATA) && dataFile.length() > 0)\n                // everything appears to be in order... moving on.\n                continue;\n\n            // missing the DATA file! all components are orphaned\n            logger.warn(\"Removing orphans for {}: {}\", desc, components);\n            for (Component component : components)\n            {\n                FileUtils.deleteWithConfirm(desc.filenameFor(component));\n            }\n        }\n\n        // cleanup incomplete saved caches\n        Pattern tmpCacheFilePattern = Pattern.compile(metadata.ksName + \"-\" + metadata.cfName + \"-(Key|Row)Cache.*\\\\.tmp$\");\n        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());\n\n        if (dir.exists())\n        {\n            assert dir.isDirectory();\n            for (File file : dir.listFiles())\n                if (tmpCacheFilePattern.matcher(file.getName()).matches())\n                    if (!file.delete())\n                        logger.warn(\"could not delete {}\", file.getAbsolutePath());\n        }\n\n        // also clean out any index leftovers.\n        for (ColumnDefinition def : metadata.allColumns())\n        {\n            if (def.isIndexed())\n            {\n                CellNameType indexComparator = SecondaryIndex.getIndexComparator(metadata, def);\n                if (indexComparator != null)\n                {\n                    CFMetaData indexMetadata = CFMetaData.newIndexMetadata(metadata, def, indexComparator);\n                    scrubDataDirectories(indexMetadata);\n                }\n            }\n        }\n    }"
        ],
        [
            "RingCache::refreshEndpointMap()",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  ",
            "    public void refreshEndpointMap()\n    {\n        try\n        {\n            Cassandra.Client client = ConfigHelper.getClientFromOutputAddressList(conf);\n\n            String keyspace = ConfigHelper.getOutputKeyspace(conf);\n            List<TokenRange> ring = ConfigHelper.getOutputLocalDCOnly(conf)\n                                  ? client.describe_local_ring(keyspace)\n                                  : client.describe_ring(keyspace);\n            rangeMap = ArrayListMultimap.create();\n\n            for (TokenRange range : ring)\n            {\n                Token left = partitioner.getTokenFactory().fromString(range.start_token);\n                Token right = partitioner.getTokenFactory().fromString(range.end_token);\n                Range<Token> r = new Range<Token>(left, right);\n                for (String host : range.endpoints)\n                {\n                    try\n                    {\n                        rangeMap.put(r, InetAddress.getByName(host));\n                    } catch (UnknownHostException e)\n                    {\n                        throw new AssertionError(e); // host strings are IPs\n                    }\n                }\n            }\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n        catch (TException e)\n        {\n            logger.debug(\"Error contacting seed list {} {}\", ConfigHelper.getOutputInitialAddress(conf), e.getMessage());\n        }\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  ",
            "    public void refreshEndpointMap()\n    {\n        try\n        {\n            Cassandra.Client client = ConfigHelper.getClientFromOutputAddressList(conf);\n\n            String keyspace = ConfigHelper.getOutputKeyspace(conf);\n            List<TokenRange> ring = ConfigHelper.getOutputLocalDCOnly(conf)\n                                  ? client.describe_local_ring(keyspace)\n                                  : client.describe_ring(keyspace);\n            rangeMap = ArrayListMultimap.create();\n\n            for (TokenRange range : ring)\n            {\n                Token left = partitioner.getTokenFactory().fromString(range.start_token);\n                Token right = partitioner.getTokenFactory().fromString(range.end_token);\n                Range<Token> r = new Range<Token>(left, right);\n                for (String host : range.endpoints)\n                {\n                    try\n                    {\n                        rangeMap.put(r, InetAddress.getByName(host));\n                    } catch (UnknownHostException e)\n                    {\n                        throw new AssertionError(e); // host strings are IPs\n                    }\n                }\n            }\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n        catch (TException e)\n        {\n            logger.trace(\"Error contacting seed list {} {}\", ConfigHelper.getOutputInitialAddress(conf), e.getMessage());\n        }\n    }"
        ],
        [
            "CommitLogReplayer::replayMutation(byte,int,long,CommitLogDescriptor)",
            " 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554 -\n 555 -\n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  ",
            "    /**\n     * Deserializes and replays a commit log entry.\n     */\n    void replayMutation(byte[] inputBuffer, int size,\n            final long entryLocation, final CommitLogDescriptor desc) throws IOException\n    {\n\n        final Mutation mutation;\n        try (FastByteArrayInputStream bufIn = new FastByteArrayInputStream(inputBuffer, 0, size))\n        {\n            mutation = Mutation.serializer.deserialize(new DataInputStream(bufIn),\n                                                       desc.getMessagingVersion(),\n                                                       ColumnSerializer.Flag.LOCAL);\n            // doublecheck that what we read is [still] valid for the current schema\n            for (ColumnFamily cf : mutation.getColumnFamilies())\n                for (Cell cell : cf)\n                    cf.getComparator().validate(cell.name());\n        }\n        catch (UnknownColumnFamilyException ex)\n        {\n            if (ex.cfId == null)\n                return;\n            AtomicInteger i = invalidMutations.get(ex.cfId);\n            if (i == null)\n            {\n                i = new AtomicInteger(1);\n                invalidMutations.put(ex.cfId, i);\n            }\n            else\n                i.incrementAndGet();\n            return;\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            File f = File.createTempFile(\"mutation\", \"dat\");\n\n            try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f)))\n            {\n                out.write(inputBuffer, 0, size);\n            }\n\n            // Checksum passed so this error can't be permissible.\n            handleReplayError(false,\n                              \"Unexpected error deserializing mutation; saved to %s.  \" +\n                              \"This may be caused by replaying a mutation against a table with the same name but incompatible schema.  \" +\n                              \"Exception follows: %s\",\n                              f.getAbsolutePath(),\n                              t);\n            return;\n        }\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"replaying mutation for {}.{}: {}\", mutation.getKeyspaceName(), ByteBufferUtil.bytesToHex(mutation.key()), \"{\" + StringUtils.join(mutation.getColumnFamilies().iterator(), \", \") + \"}\");\n\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow() throws IOException\n            {\n                if (Schema.instance.getKSMetaData(mutation.getKeyspaceName()) == null)\n                    return;\n                if (pointInTimeExceeded(mutation))\n                    return;\n\n                final Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName());\n\n                // Rebuild the mutation, omitting column families that\n                //    a) the user has requested that we ignore,\n                //    b) have already been flushed,\n                // or c) are part of a cf that was dropped.\n                // Keep in mind that the cf.name() is suspect. do every thing based on the cfid instead.\n                Mutation newMutation = null;\n                for (ColumnFamily columnFamily : replayFilter.filter(mutation))\n                {\n                    if (Schema.instance.getCF(columnFamily.id()) == null)\n                        continue; // dropped\n\n                    ReplayPosition rp = cfPositions.get(columnFamily.id());\n\n                    // replay if current segment is newer than last flushed one or,\n                    // if it is the last known segment, if we are after the replay position\n                    if (desc.id > rp.segment || (desc.id == rp.segment && entryLocation > rp.position))\n                    {\n                        if (newMutation == null)\n                            newMutation = new Mutation(mutation.getKeyspaceName(), mutation.key());\n                        newMutation.add(columnFamily);\n                        replayedCount.incrementAndGet();\n                    }\n                }\n                if (newMutation != null)\n                {\n                    assert !newMutation.isEmpty();\n                    Keyspace.open(newMutation.getKeyspaceName()).apply(newMutation, false);\n                    keyspacesRecovered.add(keyspace);\n                }\n            }\n        };\n        futures.add(StageManager.getStage(Stage.MUTATION).submit(runnable));\n        if (futures.size() > MAX_OUTSTANDING_REPLAY_COUNT)\n        {\n            FBUtilities.waitOnFutures(futures);\n            futures.clear();\n        }\n    }",
            " 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554 +\n 555 +\n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  ",
            "    /**\n     * Deserializes and replays a commit log entry.\n     */\n    void replayMutation(byte[] inputBuffer, int size,\n            final long entryLocation, final CommitLogDescriptor desc) throws IOException\n    {\n\n        final Mutation mutation;\n        try (FastByteArrayInputStream bufIn = new FastByteArrayInputStream(inputBuffer, 0, size))\n        {\n            mutation = Mutation.serializer.deserialize(new DataInputStream(bufIn),\n                                                       desc.getMessagingVersion(),\n                                                       ColumnSerializer.Flag.LOCAL);\n            // doublecheck that what we read is [still] valid for the current schema\n            for (ColumnFamily cf : mutation.getColumnFamilies())\n                for (Cell cell : cf)\n                    cf.getComparator().validate(cell.name());\n        }\n        catch (UnknownColumnFamilyException ex)\n        {\n            if (ex.cfId == null)\n                return;\n            AtomicInteger i = invalidMutations.get(ex.cfId);\n            if (i == null)\n            {\n                i = new AtomicInteger(1);\n                invalidMutations.put(ex.cfId, i);\n            }\n            else\n                i.incrementAndGet();\n            return;\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            File f = File.createTempFile(\"mutation\", \"dat\");\n\n            try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f)))\n            {\n                out.write(inputBuffer, 0, size);\n            }\n\n            // Checksum passed so this error can't be permissible.\n            handleReplayError(false,\n                              \"Unexpected error deserializing mutation; saved to %s.  \" +\n                              \"This may be caused by replaying a mutation against a table with the same name but incompatible schema.  \" +\n                              \"Exception follows: %s\",\n                              f.getAbsolutePath(),\n                              t);\n            return;\n        }\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"replaying mutation for {}.{}: {}\", mutation.getKeyspaceName(), ByteBufferUtil.bytesToHex(mutation.key()), \"{\" + StringUtils.join(mutation.getColumnFamilies().iterator(), \", \") + \"}\");\n\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow() throws IOException\n            {\n                if (Schema.instance.getKSMetaData(mutation.getKeyspaceName()) == null)\n                    return;\n                if (pointInTimeExceeded(mutation))\n                    return;\n\n                final Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName());\n\n                // Rebuild the mutation, omitting column families that\n                //    a) the user has requested that we ignore,\n                //    b) have already been flushed,\n                // or c) are part of a cf that was dropped.\n                // Keep in mind that the cf.name() is suspect. do every thing based on the cfid instead.\n                Mutation newMutation = null;\n                for (ColumnFamily columnFamily : replayFilter.filter(mutation))\n                {\n                    if (Schema.instance.getCF(columnFamily.id()) == null)\n                        continue; // dropped\n\n                    ReplayPosition rp = cfPositions.get(columnFamily.id());\n\n                    // replay if current segment is newer than last flushed one or,\n                    // if it is the last known segment, if we are after the replay position\n                    if (desc.id > rp.segment || (desc.id == rp.segment && entryLocation > rp.position))\n                    {\n                        if (newMutation == null)\n                            newMutation = new Mutation(mutation.getKeyspaceName(), mutation.key());\n                        newMutation.add(columnFamily);\n                        replayedCount.incrementAndGet();\n                    }\n                }\n                if (newMutation != null)\n                {\n                    assert !newMutation.isEmpty();\n                    Keyspace.open(newMutation.getKeyspaceName()).apply(newMutation, false);\n                    keyspacesRecovered.add(keyspace);\n                }\n            }\n        };\n        futures.add(StageManager.getStage(Stage.MUTATION).submit(runnable));\n        if (futures.size() > MAX_OUTSTANDING_REPLAY_COUNT)\n        {\n            FBUtilities.waitOnFutures(futures);\n            futures.clear();\n        }\n    }"
        ],
        [
            "CommitLogSegmentManager::discardSegment(CommitLogSegment,boolean)",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "    /**\n     * Indicates that a segment file should be deleted.\n     *\n     * @param segment segment to be discarded\n     */\n    private void discardSegment(final CommitLogSegment segment, final boolean deleteFile)\n    {\n        logger.debug(\"Segment {} is no longer active and will be deleted {}\", segment, deleteFile ? \"now\" : \"by the archive script\");\n\n        segmentManagementTasks.add(new Runnable()\n        {\n            public void run()\n            {\n                segment.discard(deleteFile);\n            }\n        });\n    }",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "    /**\n     * Indicates that a segment file should be deleted.\n     *\n     * @param segment segment to be discarded\n     */\n    private void discardSegment(final CommitLogSegment segment, final boolean deleteFile)\n    {\n        logger.trace(\"Segment {} is no longer active and will be deleted {}\", segment, deleteFile ? \"now\" : \"by the archive script\");\n\n        segmentManagementTasks.add(new Runnable()\n        {\n            public void run()\n            {\n                segment.discard(deleteFile);\n            }\n        });\n    }"
        ],
        [
            "Directories::getWriteableLocation(long)",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 -\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339 -\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    /**\n     * Returns a non-blacklisted data directory that _currently_ has {@code writeSize} bytes as usable space.\n     *\n     * @throws IOError if all directories are blacklisted.\n     */\n    public DataDirectory getWriteableLocation(long writeSize)\n    {\n        List<DataDirectoryCandidate> candidates = new ArrayList<>();\n\n        long totalAvailable = 0L;\n\n        // pick directories with enough space and so that resulting sstable dirs aren't blacklisted for writes.\n        boolean tooBig = false;\n        for (DataDirectory dataDir : dataDirectories)\n        {\n            if (BlacklistedDirectories.isUnwritable(getLocationForDisk(dataDir)))\n            {\n                logger.debug(\"removing blacklisted candidate {}\", dataDir.location);\n                continue;\n            }\n            DataDirectoryCandidate candidate = new DataDirectoryCandidate(dataDir);\n            // exclude directory if its total writeSize does not fit to data directory\n            if (candidate.availableSpace < writeSize)\n            {\n                logger.debug(\"removing candidate {}, usable={}, requested={}\", candidate.dataDirectory.location, candidate.availableSpace, writeSize);\n                tooBig = true;\n                continue;\n            }\n            candidates.add(candidate);\n            totalAvailable += candidate.availableSpace;\n        }\n\n        if (candidates.isEmpty())\n            if (tooBig)\n                return null;\n            else\n                throw new IOError(new IOException(\"All configured data directories have been blacklisted as unwritable for erroring out\"));\n\n        // shortcut for single data directory systems\n        if (candidates.size() == 1)\n            return candidates.get(0).dataDirectory;\n\n        sortWriteableCandidates(candidates, totalAvailable);\n\n        return pickWriteableDirectory(candidates);\n    }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339 +\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "    /**\n     * Returns a non-blacklisted data directory that _currently_ has {@code writeSize} bytes as usable space.\n     *\n     * @throws IOError if all directories are blacklisted.\n     */\n    public DataDirectory getWriteableLocation(long writeSize)\n    {\n        List<DataDirectoryCandidate> candidates = new ArrayList<>();\n\n        long totalAvailable = 0L;\n\n        // pick directories with enough space and so that resulting sstable dirs aren't blacklisted for writes.\n        boolean tooBig = false;\n        for (DataDirectory dataDir : dataDirectories)\n        {\n            if (BlacklistedDirectories.isUnwritable(getLocationForDisk(dataDir)))\n            {\n                logger.trace(\"removing blacklisted candidate {}\", dataDir.location);\n                continue;\n            }\n            DataDirectoryCandidate candidate = new DataDirectoryCandidate(dataDir);\n            // exclude directory if its total writeSize does not fit to data directory\n            if (candidate.availableSpace < writeSize)\n            {\n                logger.trace(\"removing candidate {}, usable={}, requested={}\", candidate.dataDirectory.location, candidate.availableSpace, writeSize);\n                tooBig = true;\n                continue;\n            }\n            candidates.add(candidate);\n            totalAvailable += candidate.availableSpace;\n        }\n\n        if (candidates.isEmpty())\n            if (tooBig)\n                return null;\n            else\n                throw new IOError(new IOException(\"All configured data directories have been blacklisted as unwritable for erroring out\"));\n\n        // shortcut for single data directory systems\n        if (candidates.size() == 1)\n            return candidates.get(0).dataDirectory;\n\n        sortWriteableCandidates(candidates, totalAvailable);\n\n        return pickWriteableDirectory(candidates);\n    }"
        ],
        [
            "CompactionManager::doCleanupOne(ColumnFamilyStore,LifecycleTransaction,CleanupStrategy,Collection,boolean)",
            " 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783 -\n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793 -\n 794 -\n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  ",
            "    /**\n     * This function goes over a file and removes the keys that the node is not responsible for\n     * and only keeps keys that this node is responsible for.\n     *\n     * @throws IOException\n     */\n    private void doCleanupOne(final ColumnFamilyStore cfs, LifecycleTransaction txn, CleanupStrategy cleanupStrategy, Collection<Range<Token>> ranges, boolean hasIndexes) throws IOException\n    {\n        assert !cfs.isIndex();\n\n        SSTableReader sstable = txn.onlyOne();\n\n        if (!hasIndexes && !new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(ranges))\n        {\n            txn.obsoleteOriginals();\n            txn.finish();\n            return;\n        }\n        if (!needsCleanup(sstable, ranges))\n        {\n            logger.debug(\"Skipping {} for cleanup; all rows should be kept\", sstable);\n            return;\n        }\n\n        long start = System.nanoTime();\n\n        long totalkeysWritten = 0;\n\n        long expectedBloomFilterSize = Math.max(cfs.metadata.getMinIndexInterval(),\n                                               SSTableReader.getApproximateKeyCount(txn.originals()));\n        if (logger.isDebugEnabled())\n            logger.debug(\"Expected bloom filter size : {}\", expectedBloomFilterSize);\n\n        logger.info(\"Cleaning up {}\", sstable);\n\n        File compactionFileLocation = cfs.directories.getWriteableLocationAsFile(cfs.getExpectedCompactedFileSize(txn.originals(), OperationType.CLEANUP));\n        if (compactionFileLocation == null)\n            throw new IOException(\"disk full\");\n\n        ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());\n        CleanupInfo ci = new CleanupInfo(sstable, scanner);\n\n        metrics.beginCompaction(ci);\n        List<SSTableReader> finished;\n        try (SSTableRewriter writer = new SSTableRewriter(cfs, txn, sstable.maxDataAge, false);\n             CompactionController controller = new CompactionController(cfs, txn.originals(), getDefaultGcBefore(cfs)))\n        {\n            writer.switchWriter(createWriter(cfs, compactionFileLocation, expectedBloomFilterSize, sstable.getSSTableMetadata().repairedAt, sstable));\n\n            while (scanner.hasNext())\n            {\n                if (ci.isStopRequested())\n                    throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                @SuppressWarnings(\"resource\")\n                SSTableIdentityIterator row = cleanupStrategy.cleanup((SSTableIdentityIterator) scanner.next());\n                if (row == null)\n                    continue;\n                @SuppressWarnings(\"resource\")\n                AbstractCompactedRow compactedRow = new LazilyCompactedRow(controller, Collections.singletonList(row));\n                if (writer.append(compactedRow) != null)\n                    totalkeysWritten++;\n            }\n\n            // flush to ensure we don't lose the tombstones on a restart, since they are not commitlog'd\n            cfs.indexManager.flushIndexesBlocking();\n\n            finished = writer.finish();\n        }\n        finally\n        {\n            scanner.close();\n            metrics.finishCompaction(ci);\n        }\n\n        if (!finished.isEmpty())\n        {\n            String format = \"Cleaned up to %s.  %,d to %,d (~%d%% of original) bytes for %,d keys.  Time: %,dms.\";\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = sstable.onDiskLength();\n            long endsize = 0;\n            for (SSTableReader newSstable : finished)\n                endsize += newSstable.onDiskLength();\n            double ratio = (double) endsize / (double) startsize;\n            logger.info(String.format(format, finished.get(0).getFilename(), startsize, endsize, (int) (ratio * 100), totalkeysWritten, dTime));\n        }\n\n    }",
            " 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783 +\n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793 +\n 794 +\n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  ",
            "    /**\n     * This function goes over a file and removes the keys that the node is not responsible for\n     * and only keeps keys that this node is responsible for.\n     *\n     * @throws IOException\n     */\n    private void doCleanupOne(final ColumnFamilyStore cfs, LifecycleTransaction txn, CleanupStrategy cleanupStrategy, Collection<Range<Token>> ranges, boolean hasIndexes) throws IOException\n    {\n        assert !cfs.isIndex();\n\n        SSTableReader sstable = txn.onlyOne();\n\n        if (!hasIndexes && !new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(ranges))\n        {\n            txn.obsoleteOriginals();\n            txn.finish();\n            return;\n        }\n        if (!needsCleanup(sstable, ranges))\n        {\n            logger.trace(\"Skipping {} for cleanup; all rows should be kept\", sstable);\n            return;\n        }\n\n        long start = System.nanoTime();\n\n        long totalkeysWritten = 0;\n\n        long expectedBloomFilterSize = Math.max(cfs.metadata.getMinIndexInterval(),\n                                               SSTableReader.getApproximateKeyCount(txn.originals()));\n        if (logger.isTraceEnabled())\n            logger.trace(\"Expected bloom filter size : {}\", expectedBloomFilterSize);\n\n        logger.info(\"Cleaning up {}\", sstable);\n\n        File compactionFileLocation = cfs.directories.getWriteableLocationAsFile(cfs.getExpectedCompactedFileSize(txn.originals(), OperationType.CLEANUP));\n        if (compactionFileLocation == null)\n            throw new IOException(\"disk full\");\n\n        ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());\n        CleanupInfo ci = new CleanupInfo(sstable, scanner);\n\n        metrics.beginCompaction(ci);\n        List<SSTableReader> finished;\n        try (SSTableRewriter writer = new SSTableRewriter(cfs, txn, sstable.maxDataAge, false);\n             CompactionController controller = new CompactionController(cfs, txn.originals(), getDefaultGcBefore(cfs)))\n        {\n            writer.switchWriter(createWriter(cfs, compactionFileLocation, expectedBloomFilterSize, sstable.getSSTableMetadata().repairedAt, sstable));\n\n            while (scanner.hasNext())\n            {\n                if (ci.isStopRequested())\n                    throw new CompactionInterruptedException(ci.getCompactionInfo());\n\n                @SuppressWarnings(\"resource\")\n                SSTableIdentityIterator row = cleanupStrategy.cleanup((SSTableIdentityIterator) scanner.next());\n                if (row == null)\n                    continue;\n                @SuppressWarnings(\"resource\")\n                AbstractCompactedRow compactedRow = new LazilyCompactedRow(controller, Collections.singletonList(row));\n                if (writer.append(compactedRow) != null)\n                    totalkeysWritten++;\n            }\n\n            // flush to ensure we don't lose the tombstones on a restart, since they are not commitlog'd\n            cfs.indexManager.flushIndexesBlocking();\n\n            finished = writer.finish();\n        }\n        finally\n        {\n            scanner.close();\n            metrics.finishCompaction(ci);\n        }\n\n        if (!finished.isEmpty())\n        {\n            String format = \"Cleaned up to %s.  %,d to %,d (~%d%% of original) bytes for %,d keys.  Time: %,dms.\";\n            long dTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n            long startsize = sstable.onDiskLength();\n            long endsize = 0;\n            for (SSTableReader newSstable : finished)\n                endsize += newSstable.onDiskLength();\n            double ratio = (double) endsize / (double) startsize;\n            logger.info(String.format(format, finished.get(0).getFilename(), startsize, endsize, (int) (ratio * 100), totalkeysWritten, dTime));\n        }\n\n    }"
        ],
        [
            "SliceFromReadCommand::getRow(Keyspace)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "    public Row getRow(Keyspace keyspace)\n    {\n        CFMetaData cfm = Schema.instance.getCFMetaData(ksName, cfName);\n        DecoratedKey dk = StorageService.getPartitioner().decorateKey(key);\n\n        // If we're doing a reversed query and the filter includes static columns, we need to issue two separate\n        // reads in order to guarantee that the static columns are fetched.  See CASSANDRA-8502 for more details.\n        if (filter.reversed && filter.hasStaticSlice(cfm))\n        {\n            logger.debug(\"Splitting reversed slice with static columns into two reads\");\n            Pair<SliceQueryFilter, SliceQueryFilter> newFilters = filter.splitOutStaticSlice(cfm);\n\n            Row normalResults =  keyspace.getRow(new QueryFilter(dk, cfName, newFilters.right, timestamp));\n            Row staticResults =  keyspace.getRow(new QueryFilter(dk, cfName, newFilters.left, timestamp));\n\n            // add the static results to the start of the normal results\n            if (normalResults.cf == null)\n                return staticResults;\n\n            if (staticResults.cf != null)\n                for (Cell cell : staticResults.cf.getReverseSortedColumns())\n                    normalResults.cf.addColumn(cell);\n\n            return normalResults;\n        }\n\n        return keyspace.getRow(new QueryFilter(dk, cfName, filter, timestamp));\n    }",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "    public Row getRow(Keyspace keyspace)\n    {\n        CFMetaData cfm = Schema.instance.getCFMetaData(ksName, cfName);\n        DecoratedKey dk = StorageService.getPartitioner().decorateKey(key);\n\n        // If we're doing a reversed query and the filter includes static columns, we need to issue two separate\n        // reads in order to guarantee that the static columns are fetched.  See CASSANDRA-8502 for more details.\n        if (filter.reversed && filter.hasStaticSlice(cfm))\n        {\n            logger.trace(\"Splitting reversed slice with static columns into two reads\");\n            Pair<SliceQueryFilter, SliceQueryFilter> newFilters = filter.splitOutStaticSlice(cfm);\n\n            Row normalResults =  keyspace.getRow(new QueryFilter(dk, cfName, newFilters.right, timestamp));\n            Row staticResults =  keyspace.getRow(new QueryFilter(dk, cfName, newFilters.left, timestamp));\n\n            // add the static results to the start of the normal results\n            if (normalResults.cf == null)\n                return staticResults;\n\n            if (staticResults.cf != null)\n                for (Cell cell : staticResults.cf.getReverseSortedColumns())\n                    normalResults.cf.addColumn(cell);\n\n            return normalResults;\n        }\n\n        return keyspace.getRow(new QueryFilter(dk, cfName, filter, timestamp));\n    }"
        ],
        [
            "QueryProcessor::MigrationSubscriber::onUpdateColumnFamily(String,String,boolean)",
            " 619  \n 620  \n 621 -\n 622  \n 623  \n 624  ",
            "        public void onUpdateColumnFamily(String ksName, String cfName, boolean columnsDidChange)\n        {\n            logger.debug(\"Column definitions for {}.{} changed, invalidating related prepared statements\", ksName, cfName);\n            if (columnsDidChange)\n                removeInvalidPreparedStatements(ksName, cfName);\n        }",
            " 619  \n 620  \n 621 +\n 622  \n 623  \n 624  ",
            "        public void onUpdateColumnFamily(String ksName, String cfName, boolean columnsDidChange)\n        {\n            logger.trace(\"Column definitions for {}.{} changed, invalidating related prepared statements\", ksName, cfName);\n            if (columnsDidChange)\n                removeInvalidPreparedStatements(ksName, cfName);\n        }"
        ],
        [
            "AbstractColumnFamilyInputFormat::getSplits(JobContext)",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    public List<InputSplit> getSplits(JobContext context) throws IOException\n    {\n        Configuration conf = HadoopCompat.getConfiguration(context);\n\n        validateConfiguration(conf);\n\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        partitioner = ConfigHelper.getInputPartitioner(conf);\n        logger.debug(\"partitioner is {}\", partitioner);\n\n        // canonical ranges and nodes holding replicas\n        Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(conf, keyspace);\n\n        // canonical ranges, split into pieces, fetching the splits in parallel\n        ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());\n        List<InputSplit> splits = new ArrayList<>();\n\n        try\n        {\n            List<Future<List<InputSplit>>> splitfutures = new ArrayList<>();\n            KeyRange jobKeyRange = ConfigHelper.getInputKeyRange(conf);\n            Range<Token> jobRange = null;\n            if (jobKeyRange != null)\n            {\n                if (jobKeyRange.start_key != null)\n                {\n                    if (!partitioner.preservesOrder())\n                        throw new UnsupportedOperationException(\"KeyRange based on keys can only be used with a order preserving partitioner\");\n                    if (jobKeyRange.start_token != null)\n                        throw new IllegalArgumentException(\"only start_key supported\");\n                    if (jobKeyRange.end_token != null)\n                        throw new IllegalArgumentException(\"only start_key supported\");\n                    jobRange = new Range<>(partitioner.getToken(jobKeyRange.start_key),\n                                           partitioner.getToken(jobKeyRange.end_key));\n                }\n                else if (jobKeyRange.start_token != null)\n                {\n                    jobRange = new Range<>(partitioner.getTokenFactory().fromString(jobKeyRange.start_token),\n                                           partitioner.getTokenFactory().fromString(jobKeyRange.end_token));\n                }\n                else\n                {\n                    logger.warn(\"ignoring jobKeyRange specified without start_key or start_token\");\n                }\n            }\n\n            session = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(\",\"), conf).connect();\n            Metadata metadata = session.getCluster().getMetadata();\n\n            for (TokenRange range : masterRangeNodes.keySet())\n            {\n                if (jobRange == null)\n                {\n                    // for each tokenRange, pick a live owner and ask it to compute bite-sized splits\n                    splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf)));\n                }\n                else\n                {\n                    TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange);\n                    if (range.intersects(jobTokenRange))\n                    {\n                        for (TokenRange intersection: range.intersectWith(jobTokenRange))\n                        {\n                            // for each tokenRange, pick a live owner and ask it to compute bite-sized splits\n                            splitfutures.add(executor.submit(new SplitCallable(intersection,  masterRangeNodes.get(range), conf)));\n                        }\n                    }\n                }\n            }\n\n            // wait until we have all the results back\n            for (Future<List<InputSplit>> futureInputSplits : splitfutures)\n            {\n                try\n                {\n                    splits.addAll(futureInputSplits.get());\n                }\n                catch (Exception e)\n                {\n                    throw new IOException(\"Could not get input splits\", e);\n                }\n            }\n        }\n        finally\n        {\n            executor.shutdownNow();\n        }\n\n        assert splits.size() > 0;\n        Collections.shuffle(splits, new Random(System.nanoTime()));\n        return splits;\n    }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    public List<InputSplit> getSplits(JobContext context) throws IOException\n    {\n        Configuration conf = HadoopCompat.getConfiguration(context);\n\n        validateConfiguration(conf);\n\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        partitioner = ConfigHelper.getInputPartitioner(conf);\n        logger.trace(\"partitioner is {}\", partitioner);\n\n        // canonical ranges and nodes holding replicas\n        Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(conf, keyspace);\n\n        // canonical ranges, split into pieces, fetching the splits in parallel\n        ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());\n        List<InputSplit> splits = new ArrayList<>();\n\n        try\n        {\n            List<Future<List<InputSplit>>> splitfutures = new ArrayList<>();\n            KeyRange jobKeyRange = ConfigHelper.getInputKeyRange(conf);\n            Range<Token> jobRange = null;\n            if (jobKeyRange != null)\n            {\n                if (jobKeyRange.start_key != null)\n                {\n                    if (!partitioner.preservesOrder())\n                        throw new UnsupportedOperationException(\"KeyRange based on keys can only be used with a order preserving partitioner\");\n                    if (jobKeyRange.start_token != null)\n                        throw new IllegalArgumentException(\"only start_key supported\");\n                    if (jobKeyRange.end_token != null)\n                        throw new IllegalArgumentException(\"only start_key supported\");\n                    jobRange = new Range<>(partitioner.getToken(jobKeyRange.start_key),\n                                           partitioner.getToken(jobKeyRange.end_key));\n                }\n                else if (jobKeyRange.start_token != null)\n                {\n                    jobRange = new Range<>(partitioner.getTokenFactory().fromString(jobKeyRange.start_token),\n                                           partitioner.getTokenFactory().fromString(jobKeyRange.end_token));\n                }\n                else\n                {\n                    logger.warn(\"ignoring jobKeyRange specified without start_key or start_token\");\n                }\n            }\n\n            session = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(\",\"), conf).connect();\n            Metadata metadata = session.getCluster().getMetadata();\n\n            for (TokenRange range : masterRangeNodes.keySet())\n            {\n                if (jobRange == null)\n                {\n                    // for each tokenRange, pick a live owner and ask it to compute bite-sized splits\n                    splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf)));\n                }\n                else\n                {\n                    TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange);\n                    if (range.intersects(jobTokenRange))\n                    {\n                        for (TokenRange intersection: range.intersectWith(jobTokenRange))\n                        {\n                            // for each tokenRange, pick a live owner and ask it to compute bite-sized splits\n                            splitfutures.add(executor.submit(new SplitCallable(intersection,  masterRangeNodes.get(range), conf)));\n                        }\n                    }\n                }\n            }\n\n            // wait until we have all the results back\n            for (Future<List<InputSplit>> futureInputSplits : splitfutures)\n            {\n                try\n                {\n                    splits.addAll(futureInputSplits.get());\n                }\n                catch (Exception e)\n                {\n                    throw new IOException(\"Could not get input splits\", e);\n                }\n            }\n        }\n        finally\n        {\n            executor.shutdownNow();\n        }\n\n        assert splits.size() > 0;\n        Collections.shuffle(splits, new Random(System.nanoTime()));\n        return splits;\n    }"
        ],
        [
            "CustomTThreadPoolServer::WorkerProcess::run()",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "        /**\n         * Loops on processing a client forever\n         */\n        public void run()\n        {\n            TProcessor processor = null;\n            TProtocol inputProtocol = null;\n            TProtocol outputProtocol = null;\n            SocketAddress socket = null;\n            try (TTransport inputTransport = inputTransportFactory_.getTransport(client_);\n                 TTransport outputTransport = outputTransportFactory_.getTransport(client_))\n            {\n                socket = ((TCustomSocket) client_).getSocket().getRemoteSocketAddress();\n                ThriftSessionManager.instance.setCurrentSocket(socket);\n                processor = processorFactory_.getProcessor(client_);\n\n                inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);\n                outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);\n                // we check stopped first to make sure we're not supposed to be shutting\n                // down. this is necessary for graceful shutdown.  (but not sufficient,\n                // since process() can take arbitrarily long waiting for client input.\n                // See comments at the end of serve().)\n                while (!stopped && processor.process(inputProtocol, outputProtocol))\n                {\n                    inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);\n                    outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);\n                }\n            }\n            catch (TTransportException ttx)\n            {\n                // Assume the client died and continue silently\n                // Log at debug to allow debugging of \"frame too large\" errors (see CASSANDRA-3142).\n                logger.debug(\"Thrift transport error occurred during processing of message.\", ttx);\n            }\n            catch (TException tx)\n            {\n                logger.error(\"Thrift error occurred during processing of message.\", tx);\n            }\n            catch (Exception e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                logger.error(\"Error occurred during processing of message.\", e);\n            }\n            finally\n            {\n                if (socket != null)\n                    ThriftSessionManager.instance.connectionComplete(socket);\n\n                activeClients.decrementAndGet();\n            }\n        }",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "        /**\n         * Loops on processing a client forever\n         */\n        public void run()\n        {\n            TProcessor processor = null;\n            TProtocol inputProtocol = null;\n            TProtocol outputProtocol = null;\n            SocketAddress socket = null;\n            try (TTransport inputTransport = inputTransportFactory_.getTransport(client_);\n                 TTransport outputTransport = outputTransportFactory_.getTransport(client_))\n            {\n                socket = ((TCustomSocket) client_).getSocket().getRemoteSocketAddress();\n                ThriftSessionManager.instance.setCurrentSocket(socket);\n                processor = processorFactory_.getProcessor(client_);\n\n                inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);\n                outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);\n                // we check stopped first to make sure we're not supposed to be shutting\n                // down. this is necessary for graceful shutdown.  (but not sufficient,\n                // since process() can take arbitrarily long waiting for client input.\n                // See comments at the end of serve().)\n                while (!stopped && processor.process(inputProtocol, outputProtocol))\n                {\n                    inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);\n                    outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);\n                }\n            }\n            catch (TTransportException ttx)\n            {\n                // Assume the client died and continue silently\n                // Log at debug to allow debugging of \"frame too large\" errors (see CASSANDRA-3142).\n                logger.trace(\"Thrift transport error occurred during processing of message.\", ttx);\n            }\n            catch (TException tx)\n            {\n                logger.error(\"Thrift error occurred during processing of message.\", tx);\n            }\n            catch (Exception e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                logger.error(\"Error occurred during processing of message.\", e);\n            }\n            finally\n            {\n                if (socket != null)\n                    ThriftSessionManager.instance.connectionComplete(socket);\n\n                activeClients.decrementAndGet();\n            }\n        }"
        ],
        [
            "DateTieredCompactionStrategy::getUserDefinedTask(Collection,int)",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 -\n 395  \n 396  \n 397  \n 398  \n 399  ",
            "    @Override\n    @SuppressWarnings(\"resource\")\n    public synchronized AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, int gcBefore)\n    {\n        assert !sstables.isEmpty(); // checked for by CM.submitUserDefined\n\n        LifecycleTransaction modifier = cfs.getTracker().tryModify(sstables, OperationType.COMPACTION);\n        if (modifier == null)\n        {\n            logger.debug(\"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem\", sstables);\n            return null;\n        }\n\n        return new CompactionTask(cfs, modifier, gcBefore, false).setUserDefined(true);\n    }",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 +\n 395  \n 396  \n 397  \n 398  \n 399  ",
            "    @Override\n    @SuppressWarnings(\"resource\")\n    public synchronized AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, int gcBefore)\n    {\n        assert !sstables.isEmpty(); // checked for by CM.submitUserDefined\n\n        LifecycleTransaction modifier = cfs.getTracker().tryModify(sstables, OperationType.COMPACTION);\n        if (modifier == null)\n        {\n            logger.trace(\"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem\", sstables);\n            return null;\n        }\n\n        return new CompactionTask(cfs, modifier, gcBefore, false).setUserDefined(true);\n    }"
        ],
        [
            "LegacySchemaTables::createFunctionFromFunctionRow(UntypedResultSet)",
            "1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360 -\n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  ",
            "    private static UDFunction createFunctionFromFunctionRow(UntypedResultSet.Row row)\n    {\n        String ksName = row.getString(\"keyspace_name\");\n        String functionName = row.getString(\"function_name\");\n        FunctionName name = new FunctionName(ksName, functionName);\n\n        List<ColumnIdentifier> argNames = new ArrayList<>();\n        if (row.has(\"argument_names\"))\n            for (String arg : row.getList(\"argument_names\", UTF8Type.instance))\n                argNames.add(new ColumnIdentifier(arg, true));\n\n        List<AbstractType<?>> argTypes = new ArrayList<>();\n        if (row.has(\"argument_types\"))\n            for (String type : row.getList(\"argument_types\", UTF8Type.instance))\n                argTypes.add(parseType(type));\n\n        AbstractType<?> returnType = parseType(row.getString(\"return_type\"));\n\n        String language = row.getString(\"language\");\n        String body = row.getString(\"body\");\n        boolean calledOnNullInput = row.getBoolean(\"called_on_null_input\");\n\n        org.apache.cassandra.cql3.functions.Function existing = org.apache.cassandra.cql3.functions.Functions.find(name, argTypes);\n        if (existing instanceof UDFunction)\n        {\n            // This check prevents duplicate compilation of effectively the same UDF.\n            // Duplicate compilation attempts can occur on the coordinator node handling the CREATE FUNCTION\n            // statement, since CreateFunctionStatement needs to execute UDFunction.create but schema migration\n            // also needs that (since it needs to handle its own change).\n            UDFunction udf = (UDFunction) existing;\n            if (udf.argNames().equals(argNames) && // arg types checked in Functions.find call\n                udf.returnType().equals(returnType) &&\n                !udf.isAggregate() &&\n                udf.language().equals(language) &&\n                udf.body().equals(body) &&\n                udf.isCalledOnNullInput() == calledOnNullInput)\n            {\n                logger.debug(\"Skipping duplicate compilation of already existing UDF {}\", name);\n                return udf;\n            }\n        }\n\n        try\n        {\n            return UDFunction.create(name, argNames, argTypes, returnType, calledOnNullInput, language, body);\n        }\n        catch (InvalidRequestException e)\n        {\n            logger.error(String.format(\"Cannot load function '%s' from schema: this function won't be available (on this node)\", name), e);\n            return UDFunction.createBrokenFunction(name, argNames, argTypes, returnType, calledOnNullInput, language, body, e);\n        }\n    }",
            "1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360 +\n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  ",
            "    private static UDFunction createFunctionFromFunctionRow(UntypedResultSet.Row row)\n    {\n        String ksName = row.getString(\"keyspace_name\");\n        String functionName = row.getString(\"function_name\");\n        FunctionName name = new FunctionName(ksName, functionName);\n\n        List<ColumnIdentifier> argNames = new ArrayList<>();\n        if (row.has(\"argument_names\"))\n            for (String arg : row.getList(\"argument_names\", UTF8Type.instance))\n                argNames.add(new ColumnIdentifier(arg, true));\n\n        List<AbstractType<?>> argTypes = new ArrayList<>();\n        if (row.has(\"argument_types\"))\n            for (String type : row.getList(\"argument_types\", UTF8Type.instance))\n                argTypes.add(parseType(type));\n\n        AbstractType<?> returnType = parseType(row.getString(\"return_type\"));\n\n        String language = row.getString(\"language\");\n        String body = row.getString(\"body\");\n        boolean calledOnNullInput = row.getBoolean(\"called_on_null_input\");\n\n        org.apache.cassandra.cql3.functions.Function existing = org.apache.cassandra.cql3.functions.Functions.find(name, argTypes);\n        if (existing instanceof UDFunction)\n        {\n            // This check prevents duplicate compilation of effectively the same UDF.\n            // Duplicate compilation attempts can occur on the coordinator node handling the CREATE FUNCTION\n            // statement, since CreateFunctionStatement needs to execute UDFunction.create but schema migration\n            // also needs that (since it needs to handle its own change).\n            UDFunction udf = (UDFunction) existing;\n            if (udf.argNames().equals(argNames) && // arg types checked in Functions.find call\n                udf.returnType().equals(returnType) &&\n                !udf.isAggregate() &&\n                udf.language().equals(language) &&\n                udf.body().equals(body) &&\n                udf.isCalledOnNullInput() == calledOnNullInput)\n            {\n                logger.trace(\"Skipping duplicate compilation of already existing UDF {}\", name);\n                return udf;\n            }\n        }\n\n        try\n        {\n            return UDFunction.create(name, argNames, argTypes, returnType, calledOnNullInput, language, body);\n        }\n        catch (InvalidRequestException e)\n        {\n            logger.error(String.format(\"Cannot load function '%s' from schema: this function won't be available (on this node)\", name), e);\n            return UDFunction.createBrokenFunction(name, argNames, argTypes, returnType, calledOnNullInput, language, body, e);\n        }\n    }"
        ],
        [
            "AbstractReplicationStrategy::getCachedEndpoints(Token)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "    public ArrayList<InetAddress> getCachedEndpoints(Token t)\n    {\n        long lastVersion = tokenMetadata.getRingVersion();\n\n        if (lastVersion > lastInvalidatedVersion)\n        {\n            synchronized (this)\n            {\n                if (lastVersion > lastInvalidatedVersion)\n                {\n                    logger.debug(\"clearing cached endpoints\");\n                    cachedEndpoints.clear();\n                    lastInvalidatedVersion = lastVersion;\n                }\n            }\n        }\n\n        return cachedEndpoints.get(t);\n    }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "    public ArrayList<InetAddress> getCachedEndpoints(Token t)\n    {\n        long lastVersion = tokenMetadata.getRingVersion();\n\n        if (lastVersion > lastInvalidatedVersion)\n        {\n            synchronized (this)\n            {\n                if (lastVersion > lastInvalidatedVersion)\n                {\n                    logger.trace(\"clearing cached endpoints\");\n                    cachedEndpoints.clear();\n                    lastInvalidatedVersion = lastVersion;\n                }\n            }\n        }\n\n        return cachedEndpoints.get(t);\n    }"
        ],
        [
            "CassandraServer::system_drop_column_family(String)",
            "1522  \n1523  \n1524  \n1525 -\n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  ",
            "    public String system_drop_column_family(String column_family)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"drop_column_family\");\n\n        ThriftClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.DROP);\n            MigrationManager.announceColumnFamilyDrop(keyspace, column_family);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1522  \n1523  \n1524  \n1525 +\n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  ",
            "    public String system_drop_column_family(String column_family)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"drop_column_family\");\n\n        ThriftClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.DROP);\n            MigrationManager.announceColumnFamilyDrop(keyspace, column_family);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "CommitLogReplayer::recover(File,boolean)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 -\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 -\n 396  \n 397  ",
            "    public void recover(File file, boolean tolerateTruncation) throws IOException\n    {\n        CommitLogDescriptor desc = CommitLogDescriptor.fromFileName(file.getName());\n        RandomAccessReader reader = RandomAccessReader.open(new File(file.getAbsolutePath()));\n        try\n        {\n            if (desc.version < CommitLogDescriptor.VERSION_21)\n            {\n                if (logAndCheckIfShouldSkip(file, desc))\n                    return;\n                if (globalPosition.segment == desc.id)\n                    reader.seek(globalPosition.position);\n                replaySyncSection(reader, (int) reader.getPositionLimit(), desc, desc.fileName(), tolerateTruncation);\n                return;\n            }\n\n            final long segmentId = desc.id;\n            try\n            {\n                desc = CommitLogDescriptor.readHeader(reader);\n            }\n            catch (IOException e)\n            {\n                desc = null;\n            }\n            if (desc == null) {\n                handleReplayError(false, \"Could not read commit log descriptor in file %s\", file);\n                return;\n            }\n            if (segmentId != desc.id)\n            {\n                handleReplayError(false, \"Segment id mismatch (filename %d, descriptor %d) in file %s\", segmentId, desc.id, file);\n                // continue processing if ignored.\n            }\n\n            if (logAndCheckIfShouldSkip(file, desc))\n                return;\n\n            ICompressor compressor = null;\n            if (desc.compression != null)\n            {\n                try\n                {\n                    compressor = CompressionParameters.createCompressor(desc.compression);\n                }\n                catch (ConfigurationException e)\n                {\n                    handleReplayError(false, \"Unknown compression: %s\", e.getMessage());\n                    return;\n                }\n            }\n\n            assert reader.length() <= Integer.MAX_VALUE;\n            int end = (int) reader.getFilePointer();\n            int replayEnd = end;\n\n            while ((end = readSyncMarker(desc, end, reader, tolerateTruncation)) >= 0)\n            {\n                int replayPos = replayEnd + CommitLogSegment.SYNC_MARKER_SIZE;\n\n                if (logger.isDebugEnabled())\n                    logger.trace(\"Replaying {} between {} and {}\", file, reader.getFilePointer(), end);\n                if (compressor != null)\n                {\n                    int uncompressedLength = reader.readInt();\n                    replayEnd = replayPos + uncompressedLength;\n                } else\n                {\n                    replayEnd = end;\n                }\n\n                if (segmentId == globalPosition.segment && replayEnd < globalPosition.position)\n                    // Skip over flushed section.\n                    continue;\n\n                FileDataInput sectionReader = reader;\n                String errorContext = desc.fileName();\n                // In the uncompressed case the last non-fully-flushed section can be anywhere in the file.\n                boolean tolerateErrorsInSection = tolerateTruncation;\n                if (compressor != null)\n                {\n                    // In the compressed case we know if this is the last section.\n                    tolerateErrorsInSection &= end == reader.length() || end < 0;\n\n                    int start = (int) reader.getFilePointer();\n                    try\n                    {\n                        int compressedLength = end - start;\n                        if (logger.isDebugEnabled())\n                            logger.trace(\"Decompressing {} between replay positions {} and {}\",\n                                         file,\n                                         replayPos,\n                                         replayEnd);\n                        if (compressedLength > buffer.length)\n                            buffer = new byte[(int) (1.2 * compressedLength)];\n                        reader.readFully(buffer, 0, compressedLength);\n                        int uncompressedLength = replayEnd - replayPos;\n                        if (uncompressedLength > uncompressedBuffer.length)\n                            uncompressedBuffer = new byte[(int) (1.2 * uncompressedLength)];\n                        compressedLength = compressor.uncompress(buffer, 0, compressedLength, uncompressedBuffer, 0);\n                        sectionReader = new ByteBufferDataInput(ByteBuffer.wrap(uncompressedBuffer), reader.getPath(), replayPos, 0);\n                        errorContext = \"compressed section at \" + start + \" in \" + errorContext;\n                    }\n                    catch (IOException | ArrayIndexOutOfBoundsException e)\n                    {\n                        handleReplayError(tolerateErrorsInSection,\n                                          \"Unexpected exception decompressing section at %d: %s\",\n                                          start, e);\n                        continue;\n                    }\n                }\n\n                if (!replaySyncSection(sectionReader, replayEnd, desc, errorContext, tolerateErrorsInSection))\n                    break;\n            }\n        }\n        finally\n        {\n            FileUtils.closeQuietly(reader);\n            logger.info(\"Finished reading {}\", file);\n        }\n    }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396  \n 397  ",
            "    public void recover(File file, boolean tolerateTruncation) throws IOException\n    {\n        CommitLogDescriptor desc = CommitLogDescriptor.fromFileName(file.getName());\n        RandomAccessReader reader = RandomAccessReader.open(new File(file.getAbsolutePath()));\n        try\n        {\n            if (desc.version < CommitLogDescriptor.VERSION_21)\n            {\n                if (logAndCheckIfShouldSkip(file, desc))\n                    return;\n                if (globalPosition.segment == desc.id)\n                    reader.seek(globalPosition.position);\n                replaySyncSection(reader, (int) reader.getPositionLimit(), desc, desc.fileName(), tolerateTruncation);\n                return;\n            }\n\n            final long segmentId = desc.id;\n            try\n            {\n                desc = CommitLogDescriptor.readHeader(reader);\n            }\n            catch (IOException e)\n            {\n                desc = null;\n            }\n            if (desc == null) {\n                handleReplayError(false, \"Could not read commit log descriptor in file %s\", file);\n                return;\n            }\n            if (segmentId != desc.id)\n            {\n                handleReplayError(false, \"Segment id mismatch (filename %d, descriptor %d) in file %s\", segmentId, desc.id, file);\n                // continue processing if ignored.\n            }\n\n            if (logAndCheckIfShouldSkip(file, desc))\n                return;\n\n            ICompressor compressor = null;\n            if (desc.compression != null)\n            {\n                try\n                {\n                    compressor = CompressionParameters.createCompressor(desc.compression);\n                }\n                catch (ConfigurationException e)\n                {\n                    handleReplayError(false, \"Unknown compression: %s\", e.getMessage());\n                    return;\n                }\n            }\n\n            assert reader.length() <= Integer.MAX_VALUE;\n            int end = (int) reader.getFilePointer();\n            int replayEnd = end;\n\n            while ((end = readSyncMarker(desc, end, reader, tolerateTruncation)) >= 0)\n            {\n                int replayPos = replayEnd + CommitLogSegment.SYNC_MARKER_SIZE;\n\n                if (logger.isTraceEnabled())\n                    logger.trace(\"Replaying {} between {} and {}\", file, reader.getFilePointer(), end);\n                if (compressor != null)\n                {\n                    int uncompressedLength = reader.readInt();\n                    replayEnd = replayPos + uncompressedLength;\n                } else\n                {\n                    replayEnd = end;\n                }\n\n                if (segmentId == globalPosition.segment && replayEnd < globalPosition.position)\n                    // Skip over flushed section.\n                    continue;\n\n                FileDataInput sectionReader = reader;\n                String errorContext = desc.fileName();\n                // In the uncompressed case the last non-fully-flushed section can be anywhere in the file.\n                boolean tolerateErrorsInSection = tolerateTruncation;\n                if (compressor != null)\n                {\n                    // In the compressed case we know if this is the last section.\n                    tolerateErrorsInSection &= end == reader.length() || end < 0;\n\n                    int start = (int) reader.getFilePointer();\n                    try\n                    {\n                        int compressedLength = end - start;\n                        if (logger.isTraceEnabled())\n                            logger.trace(\"Decompressing {} between replay positions {} and {}\",\n                                         file,\n                                         replayPos,\n                                         replayEnd);\n                        if (compressedLength > buffer.length)\n                            buffer = new byte[(int) (1.2 * compressedLength)];\n                        reader.readFully(buffer, 0, compressedLength);\n                        int uncompressedLength = replayEnd - replayPos;\n                        if (uncompressedLength > uncompressedBuffer.length)\n                            uncompressedBuffer = new byte[(int) (1.2 * uncompressedLength)];\n                        compressedLength = compressor.uncompress(buffer, 0, compressedLength, uncompressedBuffer, 0);\n                        sectionReader = new ByteBufferDataInput(ByteBuffer.wrap(uncompressedBuffer), reader.getPath(), replayPos, 0);\n                        errorContext = \"compressed section at \" + start + \" in \" + errorContext;\n                    }\n                    catch (IOException | ArrayIndexOutOfBoundsException e)\n                    {\n                        handleReplayError(tolerateErrorsInSection,\n                                          \"Unexpected exception decompressing section at %d: %s\",\n                                          start, e);\n                        continue;\n                    }\n                }\n\n                if (!replaySyncSection(sectionReader, replayEnd, desc, errorContext, tolerateErrorsInSection))\n                    break;\n            }\n        }\n        finally\n        {\n            FileUtils.closeQuietly(reader);\n            logger.debug(\"Finished reading {}\", file);\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::snapshotWithoutFlush(String,Predicate,boolean)",
            "2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331 -\n2332 -\n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  ",
            "    /**\n     * @param ephemeral If this flag is set to true, the snapshot will be cleaned during next startup\n     */\n    public Set<SSTableReader> snapshotWithoutFlush(String snapshotName, Predicate<SSTableReader> predicate, boolean ephemeral)\n    {\n        Set<SSTableReader> snapshottedSSTables = new HashSet<>();\n        for (ColumnFamilyStore cfs : concatWithIndexes())\n        {\n            final JSONArray filesJSONArr = new JSONArray();\n            try (RefViewFragment currentView = cfs.selectAndReference(CANONICAL_SSTABLES))\n            {\n                for (SSTableReader ssTable : currentView.sstables)\n                {\n                    if (predicate != null && !predicate.apply(ssTable))\n                        continue;\n\n                    File snapshotDirectory = Directories.getSnapshotDirectory(ssTable.descriptor, snapshotName);\n                    ssTable.createLinks(snapshotDirectory.getPath()); // hard links\n                    filesJSONArr.add(ssTable.descriptor.relativeFilenameFor(Component.DATA));\n\n                    if (logger.isDebugEnabled())\n                        logger.debug(\"Snapshot for {} keyspace data file {} created in {}\", keyspace, ssTable.getFilename(), snapshotDirectory);\n                    snapshottedSSTables.add(ssTable);\n                }\n\n                writeSnapshotManifest(filesJSONArr, snapshotName);\n            }\n        }\n        if (ephemeral)\n            createEphemeralSnapshotMarkerFile(snapshotName);\n        return snapshottedSSTables;\n    }",
            "2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331 +\n2332 +\n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  ",
            "    /**\n     * @param ephemeral If this flag is set to true, the snapshot will be cleaned during next startup\n     */\n    public Set<SSTableReader> snapshotWithoutFlush(String snapshotName, Predicate<SSTableReader> predicate, boolean ephemeral)\n    {\n        Set<SSTableReader> snapshottedSSTables = new HashSet<>();\n        for (ColumnFamilyStore cfs : concatWithIndexes())\n        {\n            final JSONArray filesJSONArr = new JSONArray();\n            try (RefViewFragment currentView = cfs.selectAndReference(CANONICAL_SSTABLES))\n            {\n                for (SSTableReader ssTable : currentView.sstables)\n                {\n                    if (predicate != null && !predicate.apply(ssTable))\n                        continue;\n\n                    File snapshotDirectory = Directories.getSnapshotDirectory(ssTable.descriptor, snapshotName);\n                    ssTable.createLinks(snapshotDirectory.getPath()); // hard links\n                    filesJSONArr.add(ssTable.descriptor.relativeFilenameFor(Component.DATA));\n\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Snapshot for {} keyspace data file {} created in {}\", keyspace, ssTable.getFilename(), snapshotDirectory);\n                    snapshottedSSTables.add(ssTable);\n                }\n\n                writeSnapshotManifest(filesJSONArr, snapshotName);\n            }\n        }\n        if (ephemeral)\n            createEphemeralSnapshotMarkerFile(snapshotName);\n        return snapshottedSSTables;\n    }"
        ],
        [
            "CqlRecordReader::initialize(InputSplit,TaskAttemptContext)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159 -\n 160  \n 161  \n 162 -\n 163  ",
            "    @Override\n    public void initialize(InputSplit split, TaskAttemptContext context) throws IOException\n    {\n        this.split = (ColumnFamilySplit) split;\n        Configuration conf = HadoopCompat.getConfiguration(context);\n        totalRowCount = (this.split.getLength() < Long.MAX_VALUE)\n                      ? (int) this.split.getLength()\n                      : ConfigHelper.getInputSplitSize(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        partitioner = ConfigHelper.getInputPartitioner(conf);\n        inputColumns = CqlConfigHelper.getInputcolumns(conf);\n        userDefinedWhereClauses = CqlConfigHelper.getInputWhereClauses(conf);\n\n        try\n        {\n            if (cluster != null)\n                return;\n\n            // create a Cluster instance\n            String[] locations = split.getLocations();\n            cluster = CqlConfigHelper.getInputCluster(locations, conf);\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        if (cluster != null)\n            session = cluster.connect(quote(keyspace));\n\n        if (session == null)\n          throw new RuntimeException(\"Can't create connection session\");\n\n        //get negotiated serialization protocol\n        nativeProtocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion().toInt();\n\n        // If the user provides a CQL query then we will use it without validation\n        // otherwise we will fall back to building a query using the:\n        //   inputColumns\n        //   whereClauses\n        cqlQuery = CqlConfigHelper.getInputCql(conf);\n        // validate that the user hasn't tried to give us a custom query along with input columns\n        // and where clauses\n        if (StringUtils.isNotEmpty(cqlQuery) && (StringUtils.isNotEmpty(inputColumns) ||\n                                                 StringUtils.isNotEmpty(userDefinedWhereClauses)))\n        {\n            throw new AssertionError(\"Cannot define a custom query with input columns and / or where clauses\");\n        }\n\n        if (StringUtils.isEmpty(cqlQuery))\n            cqlQuery = buildQuery();\n        logger.debug(\"cqlQuery {}\", cqlQuery);\n\n        rowIterator = new RowIterator();\n        logger.debug(\"created {}\", rowIterator);\n    }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159 +\n 160  \n 161  \n 162 +\n 163  ",
            "    @Override\n    public void initialize(InputSplit split, TaskAttemptContext context) throws IOException\n    {\n        this.split = (ColumnFamilySplit) split;\n        Configuration conf = HadoopCompat.getConfiguration(context);\n        totalRowCount = (this.split.getLength() < Long.MAX_VALUE)\n                      ? (int) this.split.getLength()\n                      : ConfigHelper.getInputSplitSize(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        partitioner = ConfigHelper.getInputPartitioner(conf);\n        inputColumns = CqlConfigHelper.getInputcolumns(conf);\n        userDefinedWhereClauses = CqlConfigHelper.getInputWhereClauses(conf);\n\n        try\n        {\n            if (cluster != null)\n                return;\n\n            // create a Cluster instance\n            String[] locations = split.getLocations();\n            cluster = CqlConfigHelper.getInputCluster(locations, conf);\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        if (cluster != null)\n            session = cluster.connect(quote(keyspace));\n\n        if (session == null)\n          throw new RuntimeException(\"Can't create connection session\");\n\n        //get negotiated serialization protocol\n        nativeProtocolVersion = cluster.getConfiguration().getProtocolOptions().getProtocolVersion().toInt();\n\n        // If the user provides a CQL query then we will use it without validation\n        // otherwise we will fall back to building a query using the:\n        //   inputColumns\n        //   whereClauses\n        cqlQuery = CqlConfigHelper.getInputCql(conf);\n        // validate that the user hasn't tried to give us a custom query along with input columns\n        // and where clauses\n        if (StringUtils.isNotEmpty(cqlQuery) && (StringUtils.isNotEmpty(inputColumns) ||\n                                                 StringUtils.isNotEmpty(userDefinedWhereClauses)))\n        {\n            throw new AssertionError(\"Cannot define a custom query with input columns and / or where clauses\");\n        }\n\n        if (StringUtils.isEmpty(cqlQuery))\n            cqlQuery = buildQuery();\n        logger.trace(\"cqlQuery {}\", cqlQuery);\n\n        rowIterator = new RowIterator();\n        logger.trace(\"created {}\", rowIterator);\n    }"
        ],
        [
            "PasswordAuthenticator::PlainTextSaslAuthenticator::decodeCredentials(byte)",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "        /**\n         * SASL PLAIN mechanism specifies that credentials are encoded in a\n         * sequence of UTF-8 bytes, delimited by 0 (US-ASCII NUL).\n         * The form is : {code}authzId<NUL>authnId<NUL>password<NUL>{code}\n         * authzId is optional, and in fact we don't care about it here as we'll\n         * set the authzId to match the authnId (that is, there is no concept of\n         * a user being authorized to act on behalf of another with this IAuthenticator).\n         *\n         * @param bytes encoded credentials string sent by the client\n         * @return map containing the username/password pairs in the form an IAuthenticator\n         * would expect\n         * @throws javax.security.sasl.SaslException\n         */\n        private void decodeCredentials(byte[] bytes) throws AuthenticationException\n        {\n            logger.debug(\"Decoding credentials from client token\");\n            byte[] user = null;\n            byte[] pass = null;\n            int end = bytes.length;\n            for (int i = bytes.length - 1 ; i >= 0; i--)\n            {\n                if (bytes[i] == NUL)\n                {\n                    if (pass == null)\n                        pass = Arrays.copyOfRange(bytes, i + 1, end);\n                    else if (user == null)\n                        user = Arrays.copyOfRange(bytes, i + 1, end);\n                    end = i;\n                }\n            }\n\n            if (user == null)\n                throw new AuthenticationException(\"Authentication ID must not be null\");\n            if (pass == null)\n                throw new AuthenticationException(\"Password must not be null\");\n\n            username = new String(user, StandardCharsets.UTF_8);\n            password = new String(pass, StandardCharsets.UTF_8);\n        }",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 +\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "        /**\n         * SASL PLAIN mechanism specifies that credentials are encoded in a\n         * sequence of UTF-8 bytes, delimited by 0 (US-ASCII NUL).\n         * The form is : {code}authzId<NUL>authnId<NUL>password<NUL>{code}\n         * authzId is optional, and in fact we don't care about it here as we'll\n         * set the authzId to match the authnId (that is, there is no concept of\n         * a user being authorized to act on behalf of another with this IAuthenticator).\n         *\n         * @param bytes encoded credentials string sent by the client\n         * @return map containing the username/password pairs in the form an IAuthenticator\n         * would expect\n         * @throws javax.security.sasl.SaslException\n         */\n        private void decodeCredentials(byte[] bytes) throws AuthenticationException\n        {\n            logger.trace(\"Decoding credentials from client token\");\n            byte[] user = null;\n            byte[] pass = null;\n            int end = bytes.length;\n            for (int i = bytes.length - 1 ; i >= 0; i--)\n            {\n                if (bytes[i] == NUL)\n                {\n                    if (pass == null)\n                        pass = Arrays.copyOfRange(bytes, i + 1, end);\n                    else if (user == null)\n                        user = Arrays.copyOfRange(bytes, i + 1, end);\n                    end = i;\n                }\n            }\n\n            if (user == null)\n                throw new AuthenticationException(\"Authentication ID must not be null\");\n            if (pass == null)\n                throw new AuthenticationException(\"Password must not be null\");\n\n            username = new String(user, StandardCharsets.UTF_8);\n            password = new String(pass, StandardCharsets.UTF_8);\n        }"
        ],
        [
            "ColumnFamilyRecordReader::initialize(InputSplit,TaskAttemptContext)",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 -\n 174  ",
            "    public void initialize(InputSplit split, TaskAttemptContext context) throws IOException\n    {\n        this.split = (ColumnFamilySplit) split;\n        Configuration conf = HadoopCompat.getConfiguration(context);\n        KeyRange jobRange = ConfigHelper.getInputKeyRange(conf);\n        filter = jobRange == null ? null : jobRange.row_filter;\n        predicate = ConfigHelper.getInputSlicePredicate(conf);\n        boolean widerows = ConfigHelper.getInputIsWide(conf);\n        isEmptyPredicate = isEmptyPredicate(predicate);\n        totalRowCount = (this.split.getLength() < Long.MAX_VALUE)\n                ? (int) this.split.getLength()\n                : ConfigHelper.getInputSplitSize(conf);\n        batchSize = ConfigHelper.getRangeBatchSize(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        consistencyLevel = ConsistencyLevel.valueOf(ConfigHelper.getReadConsistencyLevel(conf));\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        \n        if (batchSize < 2)\n            throw new IllegalArgumentException(\"Minimum batchSize is 2.  Suggested batchSize is 100 or more\");\n\n        try\n        {\n            if (client != null)\n                return;\n\n            // create connection using thrift\n            String location = getLocation();\n\n            int port = ConfigHelper.getInputRpcPort(conf);\n            client = ColumnFamilyInputFormat.createAuthenticatedClient(location, port, conf);\n\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        iter = widerows ? new WideRowIterator() : new StaticRowIterator();\n        logger.debug(\"created {}\", iter);\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  ",
            "    public void initialize(InputSplit split, TaskAttemptContext context) throws IOException\n    {\n        this.split = (ColumnFamilySplit) split;\n        Configuration conf = HadoopCompat.getConfiguration(context);\n        KeyRange jobRange = ConfigHelper.getInputKeyRange(conf);\n        filter = jobRange == null ? null : jobRange.row_filter;\n        predicate = ConfigHelper.getInputSlicePredicate(conf);\n        boolean widerows = ConfigHelper.getInputIsWide(conf);\n        isEmptyPredicate = isEmptyPredicate(predicate);\n        totalRowCount = (this.split.getLength() < Long.MAX_VALUE)\n                ? (int) this.split.getLength()\n                : ConfigHelper.getInputSplitSize(conf);\n        batchSize = ConfigHelper.getRangeBatchSize(conf);\n        cfName = ConfigHelper.getInputColumnFamily(conf);\n        consistencyLevel = ConsistencyLevel.valueOf(ConfigHelper.getReadConsistencyLevel(conf));\n        keyspace = ConfigHelper.getInputKeyspace(conf);\n        \n        if (batchSize < 2)\n            throw new IllegalArgumentException(\"Minimum batchSize is 2.  Suggested batchSize is 100 or more\");\n\n        try\n        {\n            if (client != null)\n                return;\n\n            // create connection using thrift\n            String location = getLocation();\n\n            int port = ConfigHelper.getInputRpcPort(conf);\n            client = ColumnFamilyInputFormat.createAuthenticatedClient(location, port, conf);\n\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n\n        iter = widerows ? new WideRowIterator() : new StaticRowIterator();\n        logger.trace(\"created {}\", iter);\n    }"
        ],
        [
            "CompactionManager::CompactionExecutor::afterExecute(Runnable,Throwable)",
            "1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420 -\n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  ",
            "        @Override\n        public void afterExecute(Runnable r, Throwable t)\n        {\n            DebuggableThreadPoolExecutor.maybeResetTraceSessionWrapper(r);\n    \n            if (t == null)\n                t = DebuggableThreadPoolExecutor.extractThrowable(r);\n\n            if (t != null)\n            {\n                if (t instanceof CompactionInterruptedException)\n                {\n                    logger.info(t.getMessage());\n                    if (t.getSuppressed() != null && t.getSuppressed().length > 0)\n                        logger.warn(\"Interruption of compaction encountered exceptions:\", t);\n                    else\n                        logger.debug(\"Full interruption stack trace:\", t);\n                }\n                else\n                {\n                    DebuggableThreadPoolExecutor.handleOrLog(t);\n                }\n            }\n\n            // Snapshots cannot be deleted on Windows while segments of the root element are mapped in NTFS. Compactions\n            // unmap those segments which could free up a snapshot for successful deletion.\n            SnapshotDeletingTask.rescheduleFailedTasks();\n        }",
            "1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420 +\n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  ",
            "        @Override\n        public void afterExecute(Runnable r, Throwable t)\n        {\n            DebuggableThreadPoolExecutor.maybeResetTraceSessionWrapper(r);\n    \n            if (t == null)\n                t = DebuggableThreadPoolExecutor.extractThrowable(r);\n\n            if (t != null)\n            {\n                if (t instanceof CompactionInterruptedException)\n                {\n                    logger.info(t.getMessage());\n                    if (t.getSuppressed() != null && t.getSuppressed().length > 0)\n                        logger.warn(\"Interruption of compaction encountered exceptions:\", t);\n                    else\n                        logger.trace(\"Full interruption stack trace:\", t);\n                }\n                else\n                {\n                    DebuggableThreadPoolExecutor.handleOrLog(t);\n                }\n            }\n\n            // Snapshots cannot be deleted on Windows while segments of the root element are mapped in NTFS. Compactions\n            // unmap those segments which could free up a snapshot for successful deletion.\n            SnapshotDeletingTask.rescheduleFailedTasks();\n        }"
        ],
        [
            "PropertyFileSnitch::reloadConfiguration(boolean)",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  \n 189  \n 190 -\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "    public void reloadConfiguration(boolean isUpdate) throws ConfigurationException\n    {\n        HashMap<InetAddress, String[]> reloadedMap = new HashMap<InetAddress, String[]>();\n\n        Properties properties = new Properties();\n        try (InputStream stream = getClass().getClassLoader().getResourceAsStream(SNITCH_PROPERTIES_FILENAME))\n        {\n            properties.load(stream);\n        }\n        catch (Exception e)\n        {\n            throw new ConfigurationException(\"Unable to read \" + SNITCH_PROPERTIES_FILENAME, e);\n        }\n\n        for (Map.Entry<Object, Object> entry : properties.entrySet())\n        {\n            String key = (String) entry.getKey();\n            String value = (String) entry.getValue();\n\n            if (key.equals(\"default\"))\n            {\n                String[] newDefault = value.split(\":\");\n                if (newDefault.length < 2)\n                    defaultDCRack = new String[] { \"default\", \"default\" };\n                else\n                    defaultDCRack = new String[] { newDefault[0].trim(), newDefault[1].trim() };\n            }\n            else\n            {\n                InetAddress host;\n                String hostString = key.replace(\"/\", \"\");\n                try\n                {\n                    host = InetAddress.getByName(hostString);\n                }\n                catch (UnknownHostException e)\n                {\n                    throw new ConfigurationException(\"Unknown host \" + hostString, e);\n                }\n                String[] token = value.split(\":\");\n                if (token.length < 2)\n                    token = new String[] { \"default\", \"default\" };\n                else\n                    token = new String[] { token[0].trim(), token[1].trim() };\n                reloadedMap.put(host, token);\n            }\n        }\n        if (defaultDCRack == null && !reloadedMap.containsKey(FBUtilities.getBroadcastAddress()))\n            throw new ConfigurationException(String.format(\"Snitch definitions at %s do not define a location for this node's broadcast address %s, nor does it provides a default\",\n                                                           SNITCH_PROPERTIES_FILENAME, FBUtilities.getBroadcastAddress()));\n\n        if (logger.isDebugEnabled())\n        {\n            StringBuilder sb = new StringBuilder();\n            for (Map.Entry<InetAddress, String[]> entry : reloadedMap.entrySet())\n                sb.append(entry.getKey()).append(\":\").append(Arrays.toString(entry.getValue())).append(\", \");\n            logger.debug(\"Loaded network topology from property file: {}\", StringUtils.removeEnd(sb.toString(), \", \"));\n        }\n\n        endpointMap = reloadedMap;\n        if (StorageService.instance != null) // null check tolerates circular dependency; see CASSANDRA-4145\n        {\n            if (isUpdate)\n                StorageService.instance.updateTopology();\n            else\n                StorageService.instance.getTokenMetadata().invalidateCachedRings();\n        }\n\n        if (gossipStarted)\n            StorageService.instance.gossipSnitchInfo();\n    }",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186  \n 187  \n 188  \n 189  \n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "    public void reloadConfiguration(boolean isUpdate) throws ConfigurationException\n    {\n        HashMap<InetAddress, String[]> reloadedMap = new HashMap<InetAddress, String[]>();\n\n        Properties properties = new Properties();\n        try (InputStream stream = getClass().getClassLoader().getResourceAsStream(SNITCH_PROPERTIES_FILENAME))\n        {\n            properties.load(stream);\n        }\n        catch (Exception e)\n        {\n            throw new ConfigurationException(\"Unable to read \" + SNITCH_PROPERTIES_FILENAME, e);\n        }\n\n        for (Map.Entry<Object, Object> entry : properties.entrySet())\n        {\n            String key = (String) entry.getKey();\n            String value = (String) entry.getValue();\n\n            if (key.equals(\"default\"))\n            {\n                String[] newDefault = value.split(\":\");\n                if (newDefault.length < 2)\n                    defaultDCRack = new String[] { \"default\", \"default\" };\n                else\n                    defaultDCRack = new String[] { newDefault[0].trim(), newDefault[1].trim() };\n            }\n            else\n            {\n                InetAddress host;\n                String hostString = key.replace(\"/\", \"\");\n                try\n                {\n                    host = InetAddress.getByName(hostString);\n                }\n                catch (UnknownHostException e)\n                {\n                    throw new ConfigurationException(\"Unknown host \" + hostString, e);\n                }\n                String[] token = value.split(\":\");\n                if (token.length < 2)\n                    token = new String[] { \"default\", \"default\" };\n                else\n                    token = new String[] { token[0].trim(), token[1].trim() };\n                reloadedMap.put(host, token);\n            }\n        }\n        if (defaultDCRack == null && !reloadedMap.containsKey(FBUtilities.getBroadcastAddress()))\n            throw new ConfigurationException(String.format(\"Snitch definitions at %s do not define a location for this node's broadcast address %s, nor does it provides a default\",\n                                                           SNITCH_PROPERTIES_FILENAME, FBUtilities.getBroadcastAddress()));\n\n        if (logger.isTraceEnabled())\n        {\n            StringBuilder sb = new StringBuilder();\n            for (Map.Entry<InetAddress, String[]> entry : reloadedMap.entrySet())\n                sb.append(entry.getKey()).append(\":\").append(Arrays.toString(entry.getValue())).append(\", \");\n            logger.trace(\"Loaded network topology from property file: {}\", StringUtils.removeEnd(sb.toString(), \", \"));\n        }\n\n        endpointMap = reloadedMap;\n        if (StorageService.instance != null) // null check tolerates circular dependency; see CASSANDRA-4145\n        {\n            if (isUpdate)\n                StorageService.instance.updateTopology();\n            else\n                StorageService.instance.getTokenMetadata().invalidateCachedRings();\n        }\n\n        if (gossipStarted)\n            StorageService.instance.gossipSnitchInfo();\n    }"
        ],
        [
            "AbstractColumnFamilyInputFormat::SplitCallable::call()",
            " 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226  \n 227  ",
            "        public List<InputSplit> call() throws Exception\n        {\n            ArrayList<InputSplit> splits = new ArrayList<>();\n            Map<TokenRange, Long> subSplits;\n            subSplits = getSubSplits(keyspace, cfName, tokenRange, conf);\n            // turn the sub-ranges into InputSplits\n            String[] endpoints = new String[hosts.size()];\n\n            // hadoop needs hostname, not ip\n            int endpointIndex = 0;\n            for (Host endpoint : hosts)\n                endpoints[endpointIndex++] = endpoint.getAddress().getHostName();\n\n            boolean partitionerIsOpp = partitioner instanceof OrderPreservingPartitioner || partitioner instanceof ByteOrderedPartitioner;\n\n            for (TokenRange subSplit : subSplits.keySet())\n            {\n                List<TokenRange> ranges = subSplit.unwrap();\n                for (TokenRange subrange : ranges)\n                {\n                    ColumnFamilySplit split =\n                            new ColumnFamilySplit(\n                                    partitionerIsOpp ?\n                                            subrange.getStart().toString().substring(2) : subrange.getStart().toString(),\n                                    partitionerIsOpp ?\n                                            subrange.getEnd().toString().substring(2) : subrange.getStart().toString(),\n                                    subSplits.get(subSplit),\n                                    endpoints);\n\n                    logger.debug(\"adding {}\", split);\n                    splits.add(split);\n                }\n            }\n            return splits;\n        }",
            " 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223  \n 224  \n 225  \n 226  \n 227  ",
            "        public List<InputSplit> call() throws Exception\n        {\n            ArrayList<InputSplit> splits = new ArrayList<>();\n            Map<TokenRange, Long> subSplits;\n            subSplits = getSubSplits(keyspace, cfName, tokenRange, conf);\n            // turn the sub-ranges into InputSplits\n            String[] endpoints = new String[hosts.size()];\n\n            // hadoop needs hostname, not ip\n            int endpointIndex = 0;\n            for (Host endpoint : hosts)\n                endpoints[endpointIndex++] = endpoint.getAddress().getHostName();\n\n            boolean partitionerIsOpp = partitioner instanceof OrderPreservingPartitioner || partitioner instanceof ByteOrderedPartitioner;\n\n            for (TokenRange subSplit : subSplits.keySet())\n            {\n                List<TokenRange> ranges = subSplit.unwrap();\n                for (TokenRange subrange : ranges)\n                {\n                    ColumnFamilySplit split =\n                            new ColumnFamilySplit(\n                                    partitionerIsOpp ?\n                                            subrange.getStart().toString().substring(2) : subrange.getStart().toString(),\n                                    partitionerIsOpp ?\n                                            subrange.getEnd().toString().substring(2) : subrange.getStart().toString(),\n                                    subSplits.get(subSplit),\n                                    endpoints);\n\n                    logger.trace(\"adding {}\", split);\n                    splits.add(split);\n                }\n            }\n            return splits;\n        }"
        ],
        [
            "HintedHandOffManager::scheduleHintDelivery(InetAddress,boolean)",
            " 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551 -\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  ",
            "    public void scheduleHintDelivery(final InetAddress to, final boolean precompact)\n    {\n        // We should not deliver hints to the same host in 2 different threads\n        if (!queuedDeliveries.add(to))\n            return;\n\n        logger.debug(\"Scheduling delivery of Hints to {}\", to);\n\n        hintDeliveryExecutor.execute(new Runnable()\n        {\n            public void run()\n            {\n                try\n                {\n                    // If it's an individual node hint replay (triggered by Gossip or via JMX), and not the global scheduled replay\n                    // (every 10 minutes), force a major compaction to get rid of the tombstones and expired hints.\n                    if (precompact)\n                        compact();\n\n                    deliverHintsToEndpoint(to);\n                }\n                finally\n                {\n                    queuedDeliveries.remove(to);\n                }\n            }\n        });\n    }",
            " 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551 +\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  ",
            "    public void scheduleHintDelivery(final InetAddress to, final boolean precompact)\n    {\n        // We should not deliver hints to the same host in 2 different threads\n        if (!queuedDeliveries.add(to))\n            return;\n\n        logger.trace(\"Scheduling delivery of Hints to {}\", to);\n\n        hintDeliveryExecutor.execute(new Runnable()\n        {\n            public void run()\n            {\n                try\n                {\n                    // If it's an individual node hint replay (triggered by Gossip or via JMX), and not the global scheduled replay\n                    // (every 10 minutes), force a major compaction to get rid of the tombstones and expired hints.\n                    if (precompact)\n                        compact();\n\n                    deliverHintsToEndpoint(to);\n                }\n                finally\n                {\n                    queuedDeliveries.remove(to);\n                }\n            }\n        });\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::newQueryPlan(String,Statement)",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  ",
            "    @Override\n    public Iterator<Host> newQueryPlan(String keyspace, Statement statement)\n    {\n        List<Host> local = new ArrayList<>(1);\n        List<Host> remote = new ArrayList<>(liveReplicaHosts.size());\n        for (Host liveReplicaHost : liveReplicaHosts)\n        {\n            if (isLocalHost(liveReplicaHost))\n            {\n                local.add(liveReplicaHost);\n            }\n            else\n            {\n                remote.add(liveReplicaHost);\n            }\n        }\n\n        Collections.shuffle(remote);\n\n        logger.debug(\"Using the following hosts order for the new query plan: {} | {}\", local, remote);\n\n        return Iterators.concat(local.iterator(), remote.iterator());\n    }",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  \n 133  ",
            "    @Override\n    public Iterator<Host> newQueryPlan(String keyspace, Statement statement)\n    {\n        List<Host> local = new ArrayList<>(1);\n        List<Host> remote = new ArrayList<>(liveReplicaHosts.size());\n        for (Host liveReplicaHost : liveReplicaHosts)\n        {\n            if (isLocalHost(liveReplicaHost))\n            {\n                local.add(liveReplicaHost);\n            }\n            else\n            {\n                remote.add(liveReplicaHost);\n            }\n        }\n\n        Collections.shuffle(remote);\n\n        logger.trace(\"Using the following hosts order for the new query plan: {} | {}\", local, remote);\n\n        return Iterators.concat(local.iterator(), remote.iterator());\n    }"
        ],
        [
            "CassandraServer::insert(ByteBuffer,ColumnParent,Column,ConsistencyLevel)",
            " 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683 -\n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  ",
            "    public void insert(ByteBuffer key, ColumnParent column_parent, Column column, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"column\", column.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"insert\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"insert\");\n        }\n\n        try\n        {\n            internal_insert(key, column_parent, column, consistency_level);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683 +\n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  ",
            "    public void insert(ByteBuffer key, ColumnParent column_parent, Column column, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"column\", column.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"insert\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"insert\");\n        }\n\n        try\n        {\n            internal_insert(key, column_parent, column, consistency_level);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "SizeTieredCompactionStrategy::getUserDefinedTask(Collection,int)",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  ",
            "    @SuppressWarnings(\"resource\")\n    public AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, final int gcBefore)\n    {\n        assert !sstables.isEmpty(); // checked for by CM.submitUserDefined\n\n        LifecycleTransaction transaction = cfs.getTracker().tryModify(sstables, OperationType.COMPACTION);\n        if (transaction == null)\n        {\n            logger.debug(\"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem\", sstables);\n            return null;\n        }\n\n        return new CompactionTask(cfs, transaction, gcBefore, false).setUserDefined(true);\n    }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  ",
            "    @SuppressWarnings(\"resource\")\n    public AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, final int gcBefore)\n    {\n        assert !sstables.isEmpty(); // checked for by CM.submitUserDefined\n\n        LifecycleTransaction transaction = cfs.getTracker().tryModify(sstables, OperationType.COMPACTION);\n        if (transaction == null)\n        {\n            logger.trace(\"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem\", sstables);\n            return null;\n        }\n\n        return new CompactionTask(cfs, transaction, gcBefore, false).setUserDefined(true);\n    }"
        ],
        [
            "CassandraServer::get_range_slices(ColumnParent,SlicePredicate,KeyRange,ConsistencyLevel)",
            "1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129 -\n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  ",
            "    public List<KeySlice> get_range_slices(ColumnParent column_parent, SlicePredicate predicate, KeyRange range, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\n                    \"column_parent\", column_parent.toString(),\n                    \"predicate\", predicate.toString(),\n                    \"range\", range.toString(),\n                    \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_range_slices\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"range_slice\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            ThriftValidation.validatePredicate(metadata, column_parent, predicate);\n            ThriftValidation.validateKeyRange(metadata, column_parent.super_column, range);\n\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            List<Row> rows = null;\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds;\n            if (range.start_key == null)\n            {\n                Token.TokenFactory tokenFactory = p.getTokenFactory();\n                Token left = tokenFactory.fromString(range.start_token);\n                Token right = tokenFactory.fromString(range.end_token);\n                bounds = Range.makeRowRange(left, right);\n            }\n            else\n            {\n                RowPosition end = range.end_key == null\n                                ? p.getTokenFactory().fromString(range.end_token).maxKeyBound()\n                                : RowPosition.ForKey.get(range.end_key, p);\n                bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(range.start_key, p), end);\n            }\n            long now = System.currentTimeMillis();\n            schedule(DatabaseDescriptor.getRangeRpcTimeout());\n            try\n            {\n                IDiskAtomFilter filter = ThriftValidation.asIFilter(predicate, metadata, column_parent.super_column);\n                rows = StorageProxy.getRangeSlice(new RangeSliceCommand(keyspace,\n                                                                        column_parent.column_family,\n                                                                        now,\n                                                                        filter,\n                                                                        bounds,\n                                                                        ThriftConversion.indexExpressionsFromThrift(range.row_filter),\n                                                                        range.count),\n                                                  consistencyLevel);\n            }\n            finally\n            {\n                release();\n            }\n            assert rows != null;\n\n            return thriftifyKeySlices(rows, column_parent, predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129 +\n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  ",
            "    public List<KeySlice> get_range_slices(ColumnParent column_parent, SlicePredicate predicate, KeyRange range, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\n                    \"column_parent\", column_parent.toString(),\n                    \"predicate\", predicate.toString(),\n                    \"range\", range.toString(),\n                    \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_range_slices\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"range_slice\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            ThriftValidation.validatePredicate(metadata, column_parent, predicate);\n            ThriftValidation.validateKeyRange(metadata, column_parent.super_column, range);\n\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            List<Row> rows = null;\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds;\n            if (range.start_key == null)\n            {\n                Token.TokenFactory tokenFactory = p.getTokenFactory();\n                Token left = tokenFactory.fromString(range.start_token);\n                Token right = tokenFactory.fromString(range.end_token);\n                bounds = Range.makeRowRange(left, right);\n            }\n            else\n            {\n                RowPosition end = range.end_key == null\n                                ? p.getTokenFactory().fromString(range.end_token).maxKeyBound()\n                                : RowPosition.ForKey.get(range.end_key, p);\n                bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(range.start_key, p), end);\n            }\n            long now = System.currentTimeMillis();\n            schedule(DatabaseDescriptor.getRangeRpcTimeout());\n            try\n            {\n                IDiskAtomFilter filter = ThriftValidation.asIFilter(predicate, metadata, column_parent.super_column);\n                rows = StorageProxy.getRangeSlice(new RangeSliceCommand(keyspace,\n                                                                        column_parent.column_family,\n                                                                        now,\n                                                                        filter,\n                                                                        bounds,\n                                                                        ThriftConversion.indexExpressionsFromThrift(range.row_filter),\n                                                                        range.count),\n                                                  consistencyLevel);\n            }\n            finally\n            {\n                release();\n            }\n            assert rows != null;\n\n            return thriftifyKeySlices(rows, column_parent, predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "CassandraServer::get(ByteBuffer,ColumnPath,ConsistencyLevel)",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 -\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  ",
            "    public ColumnOrSuperColumn get(ByteBuffer key, ColumnPath column_path, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, NotFoundException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", column_path.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"get\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family);\n            ThriftValidation.validateColumnPath(metadata, column_path);\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            ThriftValidation.validateKey(metadata, key);\n\n            IDiskAtomFilter filter;\n            if (metadata.isSuper())\n            {\n                CellNameType columnType = new SimpleDenseCellNameType(metadata.comparator.subtype(column_path.column == null ? 0 : 1));\n                SortedSet<CellName> names = new TreeSet<CellName>(columnType);\n                names.add(columnType.cellFromByteBuffer(column_path.column == null ? column_path.super_column : column_path.column));\n                filter = SuperColumns.fromSCNamesFilter(metadata.comparator, column_path.column == null ? null : column_path.bufferForSuper_column(), new NamesQueryFilter(names));\n            }\n            else\n            {\n                SortedSet<CellName> names = new TreeSet<CellName>(metadata.comparator);\n                names.add(metadata.comparator.cellFromByteBuffer(column_path.column));\n                filter = new NamesQueryFilter(names);\n            }\n\n            long now = System.currentTimeMillis();\n            ReadCommand command = ReadCommand.create(keyspace, key, column_path.column_family, now, filter);\n\n            Map<DecoratedKey, ColumnFamily> cfamilies = readColumnFamily(Arrays.asList(command), consistencyLevel, cState);\n\n            ColumnFamily cf = cfamilies.get(StorageService.getPartitioner().decorateKey(command.key));\n\n            if (cf == null)\n                throw new NotFoundException();\n            List<ColumnOrSuperColumn> tcolumns = thriftifyColumnFamily(cf, metadata.isSuper() && column_path.column != null, false, now);\n            if (tcolumns.isEmpty())\n                throw new NotFoundException();\n            assert tcolumns.size() == 1;\n            return tcolumns.get(0);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 +\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  ",
            "    public ColumnOrSuperColumn get(ByteBuffer key, ColumnPath column_path, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, NotFoundException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", column_path.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"get\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family);\n            ThriftValidation.validateColumnPath(metadata, column_path);\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            ThriftValidation.validateKey(metadata, key);\n\n            IDiskAtomFilter filter;\n            if (metadata.isSuper())\n            {\n                CellNameType columnType = new SimpleDenseCellNameType(metadata.comparator.subtype(column_path.column == null ? 0 : 1));\n                SortedSet<CellName> names = new TreeSet<CellName>(columnType);\n                names.add(columnType.cellFromByteBuffer(column_path.column == null ? column_path.super_column : column_path.column));\n                filter = SuperColumns.fromSCNamesFilter(metadata.comparator, column_path.column == null ? null : column_path.bufferForSuper_column(), new NamesQueryFilter(names));\n            }\n            else\n            {\n                SortedSet<CellName> names = new TreeSet<CellName>(metadata.comparator);\n                names.add(metadata.comparator.cellFromByteBuffer(column_path.column));\n                filter = new NamesQueryFilter(names);\n            }\n\n            long now = System.currentTimeMillis();\n            ReadCommand command = ReadCommand.create(keyspace, key, column_path.column_family, now, filter);\n\n            Map<DecoratedKey, ColumnFamily> cfamilies = readColumnFamily(Arrays.asList(command), consistencyLevel, cState);\n\n            ColumnFamily cf = cfamilies.get(StorageService.getPartitioner().decorateKey(command.key));\n\n            if (cf == null)\n                throw new NotFoundException();\n            List<ColumnOrSuperColumn> tcolumns = thriftifyColumnFamily(cf, metadata.isSuper() && column_path.column != null, false, now);\n            if (tcolumns.isEmpty())\n                throw new NotFoundException();\n            assert tcolumns.size() == 1;\n            return tcolumns.get(0);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "ReadCallback::get()",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 -\n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  ",
            "    public TResolved get() throws ReadFailureException, ReadTimeoutException, DigestMismatchException\n    {\n        if (!await(command.getTimeout(), TimeUnit.MILLISECONDS))\n        {\n            // Same as for writes, see AbstractWriteResponseHandler\n            ReadTimeoutException ex = new ReadTimeoutException(consistencyLevel, received, blockfor, resolver.isDataPresent());\n            Tracing.trace(\"Read timeout: {}\", ex.toString());\n            if (logger.isDebugEnabled())\n                logger.debug(\"Read timeout: {}\", ex.toString());\n            throw ex;\n        }\n\n        if (blockfor + failures > endpoints.size())\n        {\n            ReadFailureException ex = new ReadFailureException(consistencyLevel, received, failures, blockfor, resolver.isDataPresent());\n\n            if (logger.isDebugEnabled())\n                logger.debug(\"Read failure: {}\", ex.toString());\n            throw ex;\n        }\n\n        return blockfor == 1 ? resolver.getData() : resolver.resolve();\n    }",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 +\n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  ",
            "    public TResolved get() throws ReadFailureException, ReadTimeoutException, DigestMismatchException\n    {\n        if (!await(command.getTimeout(), TimeUnit.MILLISECONDS))\n        {\n            // Same as for writes, see AbstractWriteResponseHandler\n            ReadTimeoutException ex = new ReadTimeoutException(consistencyLevel, received, blockfor, resolver.isDataPresent());\n            Tracing.trace(\"Read timeout: {}\", ex.toString());\n            if (logger.isTraceEnabled())\n                logger.trace(\"Read timeout: {}\", ex.toString());\n            throw ex;\n        }\n\n        if (blockfor + failures > endpoints.size())\n        {\n            ReadFailureException ex = new ReadFailureException(consistencyLevel, received, failures, blockfor, resolver.isDataPresent());\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Read failure: {}\", ex.toString());\n            throw ex;\n        }\n\n        return blockfor == 1 ? resolver.getData() : resolver.resolve();\n    }"
        ],
        [
            "DefaultCompactionWriter::DefaultCompactionWriter(ColumnFamilyStore,LifecycleTransaction,Set,boolean,OperationType)",
            "  44  \n  45  \n  46  \n  47  \n  48 -\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "    @SuppressWarnings(\"resource\")\n    public DefaultCompactionWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables, boolean offline, OperationType compactionType)\n    {\n        super(cfs, txn, nonExpiredSSTables, offline);\n        logger.debug(\"Expected bloom filter size : {}\", estimatedTotalKeys);\n        long expectedWriteSize = cfs.getExpectedCompactedFileSize(nonExpiredSSTables, compactionType);\n        File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(expectedWriteSize));\n        @SuppressWarnings(\"resource\")\n        SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                    estimatedTotalKeys,\n                                                    minRepairedAt,\n                                                    cfs.metadata,\n                                                    cfs.partitioner,\n                                                    new MetadataCollector(txn.originals(), cfs.metadata.comparator, 0));\n        sstableWriter.switchWriter(writer);\n    }",
            "  44  \n  45  \n  46  \n  47  \n  48 +\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "    @SuppressWarnings(\"resource\")\n    public DefaultCompactionWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables, boolean offline, OperationType compactionType)\n    {\n        super(cfs, txn, nonExpiredSSTables, offline);\n        logger.trace(\"Expected bloom filter size : {}\", estimatedTotalKeys);\n        long expectedWriteSize = cfs.getExpectedCompactedFileSize(nonExpiredSSTables, compactionType);\n        File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(expectedWriteSize));\n        @SuppressWarnings(\"resource\")\n        SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                    estimatedTotalKeys,\n                                                    minRepairedAt,\n                                                    cfs.metadata,\n                                                    cfs.partitioner,\n                                                    new MetadataCollector(txn.originals(), cfs.metadata.comparator, 0));\n        sstableWriter.switchWriter(writer);\n    }"
        ],
        [
            "CassandraServer::get_indexed_slices(ColumnParent,IndexClause,SlicePredicate,ConsistencyLevel)",
            "1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308 -\n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  ",
            "    public List<KeySlice> get_indexed_slices(ColumnParent column_parent, IndexClause index_clause, SlicePredicate column_predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_parent\", column_parent.toString(),\n                                                                  \"index_clause\", index_clause.toString(),\n                                                                  \"slice_predicate\", column_predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_indexed_slices\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"scan\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            ThriftValidation.validatePredicate(metadata, column_parent, column_predicate);\n            ThriftValidation.validateIndexClauses(metadata, index_clause);\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(index_clause.start_key, p),\n                                                                         p.getMinimumToken().minKeyBound());\n\n            IDiskAtomFilter filter = ThriftValidation.asIFilter(column_predicate, metadata, column_parent.super_column);\n            long now = System.currentTimeMillis();\n            RangeSliceCommand command = new RangeSliceCommand(keyspace,\n                                                              column_parent.column_family,\n                                                              now,\n                                                              filter,\n                                                              bounds,\n                                                              ThriftConversion.indexExpressionsFromThrift(index_clause.expressions),\n                                                              index_clause.count);\n\n            List<Row> rows = StorageProxy.getRangeSlice(command, consistencyLevel);\n            return thriftifyKeySlices(rows, column_parent, column_predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308 +\n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  ",
            "    public List<KeySlice> get_indexed_slices(ColumnParent column_parent, IndexClause index_clause, SlicePredicate column_predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_parent\", column_parent.toString(),\n                                                                  \"index_clause\", index_clause.toString(),\n                                                                  \"slice_predicate\", column_predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_indexed_slices\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"scan\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, false);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            ThriftValidation.validatePredicate(metadata, column_parent, column_predicate);\n            ThriftValidation.validateIndexClauses(metadata, index_clause);\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(index_clause.start_key, p),\n                                                                         p.getMinimumToken().minKeyBound());\n\n            IDiskAtomFilter filter = ThriftValidation.asIFilter(column_predicate, metadata, column_parent.super_column);\n            long now = System.currentTimeMillis();\n            RangeSliceCommand command = new RangeSliceCommand(keyspace,\n                                                              column_parent.column_family,\n                                                              now,\n                                                              filter,\n                                                              bounds,\n                                                              ThriftConversion.indexExpressionsFromThrift(index_clause.expressions),\n                                                              index_clause.count);\n\n            List<Row> rows = StorageProxy.getRangeSlice(command, consistencyLevel);\n            return thriftifyKeySlices(rows, column_parent, column_predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "MessagingService::SocketThread::run()",
            " 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975 -\n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988 -\n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000 -\n1001  \n1002  \n1003  \n1004  \n1005 -\n1006  \n1007  \n1008  \n1009  \n1010 -\n1011  \n1012  \n1013  \n1014  \n1015  ",
            "        @SuppressWarnings(\"resource\")\n        public void run()\n        {\n            while (!server.isClosed())\n            {\n                Socket socket = null;\n                try\n                {\n                    socket = server.accept();\n                    if (!authenticate(socket))\n                    {\n                        logger.debug(\"remote failed to authenticate\");\n                        socket.close();\n                        continue;\n                    }\n\n                    socket.setKeepAlive(true);\n                    socket.setSoTimeout(2 * OutboundTcpConnection.WAIT_FOR_VERSION_MAX_TIME);\n                    // determine the connection type to decide whether to buffer\n                    DataInputStream in = new DataInputStream(socket.getInputStream());\n                    MessagingService.validateMagic(in.readInt());\n                    int header = in.readInt();\n                    boolean isStream = MessagingService.getBits(header, 3, 1) == 1;\n                    int version = MessagingService.getBits(header, 15, 8);\n                    logger.debug(\"Connection version {} from {}\", version, socket.getInetAddress());\n                    socket.setSoTimeout(0);\n\n                    Thread thread = isStream\n                                  ? new IncomingStreamingConnection(version, socket, connections)\n                                  : new IncomingTcpConnection(version, MessagingService.getBits(header, 2, 1) == 1, socket, connections);\n                    thread.start();\n                    connections.add((Closeable) thread);\n                }\n                catch (AsynchronousCloseException e)\n                {\n                    // this happens when another thread calls close().\n                    logger.debug(\"Asynchronous close seen by server thread\");\n                    break;\n                }\n                catch (ClosedChannelException e)\n                {\n                    logger.debug(\"MessagingService server thread already closed\");\n                    break;\n                }\n                catch (IOException e)\n                {\n                    logger.debug(\"Error reading the socket \" + socket, e);\n                    FileUtils.closeQuietly(socket);\n                }\n            }\n            logger.info(\"MessagingService has terminated the accept() thread\");\n        }",
            " 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975 +\n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988 +\n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000 +\n1001  \n1002  \n1003  \n1004  \n1005 +\n1006  \n1007  \n1008  \n1009  \n1010 +\n1011  \n1012  \n1013  \n1014  \n1015  ",
            "        @SuppressWarnings(\"resource\")\n        public void run()\n        {\n            while (!server.isClosed())\n            {\n                Socket socket = null;\n                try\n                {\n                    socket = server.accept();\n                    if (!authenticate(socket))\n                    {\n                        logger.trace(\"remote failed to authenticate\");\n                        socket.close();\n                        continue;\n                    }\n\n                    socket.setKeepAlive(true);\n                    socket.setSoTimeout(2 * OutboundTcpConnection.WAIT_FOR_VERSION_MAX_TIME);\n                    // determine the connection type to decide whether to buffer\n                    DataInputStream in = new DataInputStream(socket.getInputStream());\n                    MessagingService.validateMagic(in.readInt());\n                    int header = in.readInt();\n                    boolean isStream = MessagingService.getBits(header, 3, 1) == 1;\n                    int version = MessagingService.getBits(header, 15, 8);\n                    logger.trace(\"Connection version {} from {}\", version, socket.getInetAddress());\n                    socket.setSoTimeout(0);\n\n                    Thread thread = isStream\n                                  ? new IncomingStreamingConnection(version, socket, connections)\n                                  : new IncomingTcpConnection(version, MessagingService.getBits(header, 2, 1) == 1, socket, connections);\n                    thread.start();\n                    connections.add((Closeable) thread);\n                }\n                catch (AsynchronousCloseException e)\n                {\n                    // this happens when another thread calls close().\n                    logger.trace(\"Asynchronous close seen by server thread\");\n                    break;\n                }\n                catch (ClosedChannelException e)\n                {\n                    logger.trace(\"MessagingService server thread already closed\");\n                    break;\n                }\n                catch (IOException e)\n                {\n                    logger.trace(\"Error reading the socket \" + socket, e);\n                    FileUtils.closeQuietly(socket);\n                }\n            }\n            logger.info(\"MessagingService has terminated the accept() thread\");\n        }"
        ],
        [
            "WrappingCompactionStrategy::WrappingCompactionStrategy(ColumnFamilyStore)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67  ",
            "    public WrappingCompactionStrategy(ColumnFamilyStore cfs)\n    {\n        super(cfs, cfs.metadata.compactionStrategyOptions);\n        reloadCompactionStrategy(cfs.metadata);\n        cfs.getTracker().subscribe(this);\n        logger.debug(\"{} subscribed to the data tracker.\", this);\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67  ",
            "    public WrappingCompactionStrategy(ColumnFamilyStore cfs)\n    {\n        super(cfs, cfs.metadata.compactionStrategyOptions);\n        reloadCompactionStrategy(cfs.metadata);\n        cfs.getTracker().subscribe(this);\n        logger.trace(\"{} subscribed to the data tracker.\", this);\n    }"
        ],
        [
            "RowDataResolver::resolve()",
            "  59  \n  60  \n  61  \n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88 -\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101 -\n 102  \n 103  \n 104  ",
            "    public Row resolve() throws DigestMismatchException\n    {\n        int replyCount = replies.size();\n        if (logger.isDebugEnabled())\n            logger.debug(\"resolving {} responses\", replyCount);\n        long start = System.nanoTime();\n\n        ColumnFamily resolved;\n        if (replyCount > 1)\n        {\n            List<ColumnFamily> versions = new ArrayList<>(replyCount);\n            List<InetAddress> endpoints = new ArrayList<>(replyCount);\n\n            for (MessageIn<ReadResponse> message : replies)\n            {\n                ReadResponse response = message.payload;\n                ColumnFamily cf = response.row().cf;\n                assert !response.isDigestQuery() : \"Received digest response to repair read from \" + message.from;\n                versions.add(cf);\n                endpoints.add(message.from);\n\n                // compute maxLiveCount to prevent short reads -- see https://issues.apache.org/jira/browse/CASSANDRA-2643\n                int liveCount = cf == null ? 0 : filter.getLiveCount(cf, timestamp);\n                if (liveCount > maxLiveCount)\n                    maxLiveCount = liveCount;\n            }\n\n            resolved = resolveSuperset(versions, timestamp);\n            if (logger.isDebugEnabled())\n                logger.debug(\"versions merged\");\n\n            // send updates to any replica that was missing part of the full row\n            // (resolved can be null even if versions doesn't have all nulls because of the call to removeDeleted in resolveSuperSet)\n            if (resolved != null)\n                repairResults = scheduleRepairs(resolved, keyspaceName, key, versions, endpoints);\n        }\n        else\n        {\n            resolved = replies.get(0).payload.row().cf;\n        }\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"resolve: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n        return new Row(key, resolved);\n    }",
            "  59  \n  60  \n  61  \n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88 +\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101 +\n 102  \n 103  \n 104  ",
            "    public Row resolve() throws DigestMismatchException\n    {\n        int replyCount = replies.size();\n        if (logger.isTraceEnabled())\n            logger.trace(\"resolving {} responses\", replyCount);\n        long start = System.nanoTime();\n\n        ColumnFamily resolved;\n        if (replyCount > 1)\n        {\n            List<ColumnFamily> versions = new ArrayList<>(replyCount);\n            List<InetAddress> endpoints = new ArrayList<>(replyCount);\n\n            for (MessageIn<ReadResponse> message : replies)\n            {\n                ReadResponse response = message.payload;\n                ColumnFamily cf = response.row().cf;\n                assert !response.isDigestQuery() : \"Received digest response to repair read from \" + message.from;\n                versions.add(cf);\n                endpoints.add(message.from);\n\n                // compute maxLiveCount to prevent short reads -- see https://issues.apache.org/jira/browse/CASSANDRA-2643\n                int liveCount = cf == null ? 0 : filter.getLiveCount(cf, timestamp);\n                if (liveCount > maxLiveCount)\n                    maxLiveCount = liveCount;\n            }\n\n            resolved = resolveSuperset(versions, timestamp);\n            if (logger.isTraceEnabled())\n                logger.trace(\"versions merged\");\n\n            // send updates to any replica that was missing part of the full row\n            // (resolved can be null even if versions doesn't have all nulls because of the call to removeDeleted in resolveSuperSet)\n            if (resolved != null)\n                repairResults = scheduleRepairs(resolved, keyspaceName, key, versions, endpoints);\n        }\n        else\n        {\n            resolved = replies.get(0).payload.row().cf;\n        }\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"resolve: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n        return new Row(key, resolved);\n    }"
        ],
        [
            "OutboundTcpConnection::writeConnected(QueuedMessage,boolean)",
            " 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299 -\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  ",
            "    private void writeConnected(QueuedMessage qm, boolean flush)\n    {\n        try\n        {\n            byte[] sessionBytes = qm.message.parameters.get(Tracing.TRACE_HEADER);\n            if (sessionBytes != null)\n            {\n                UUID sessionId = UUIDGen.getUUID(ByteBuffer.wrap(sessionBytes));\n                TraceState state = Tracing.instance.get(sessionId);\n                String message = String.format(\"Sending %s message to %s\", qm.message.verb, poolReference.endPoint());\n                // session may have already finished; see CASSANDRA-5668\n                if (state == null)\n                {\n                    byte[] traceTypeBytes = qm.message.parameters.get(Tracing.TRACE_TYPE);\n                    Tracing.TraceType traceType = traceTypeBytes == null ? Tracing.TraceType.QUERY : Tracing.TraceType.deserialize(traceTypeBytes[0]);\n                    TraceState.mutateWithTracing(ByteBuffer.wrap(sessionBytes), message, -1, traceType.getTTL());\n                }\n                else\n                {\n                    state.trace(message);\n                    if (qm.message.verb == MessagingService.Verb.REQUEST_RESPONSE)\n                        Tracing.instance.doneWithNonLocalSession(state);\n                }\n            }\n\n            long timestampMillis = NanoTimeToCurrentTimeMillis.convert(qm.timestampNanos);\n            writeInternal(qm.message, qm.id, timestampMillis);\n\n            completed++;\n            if (flush)\n                out.flush();\n        }\n        catch (Exception e)\n        {\n            disconnect();\n            if (e instanceof IOException || e.getCause() instanceof IOException)\n            {\n                if (logger.isDebugEnabled())\n                    logger.debug(\"error writing to {}\", poolReference.endPoint(), e);\n\n                // if the message was important, such as a repair acknowledgement, put it back on the queue\n                // to retry after re-connecting.  See CASSANDRA-5393\n                if (qm.shouldRetry())\n                {\n                    try\n                    {\n                        backlog.put(new RetriedQueuedMessage(qm));\n                    }\n                    catch (InterruptedException e1)\n                    {\n                        throw new AssertionError(e1);\n                    }\n                }\n            }\n            else\n            {\n                // Non IO exceptions are likely a programming error so let's not silence them\n                logger.error(\"error writing to {}\", poolReference.endPoint(), e);\n            }\n        }\n    }",
            " 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  ",
            "    private void writeConnected(QueuedMessage qm, boolean flush)\n    {\n        try\n        {\n            byte[] sessionBytes = qm.message.parameters.get(Tracing.TRACE_HEADER);\n            if (sessionBytes != null)\n            {\n                UUID sessionId = UUIDGen.getUUID(ByteBuffer.wrap(sessionBytes));\n                TraceState state = Tracing.instance.get(sessionId);\n                String message = String.format(\"Sending %s message to %s\", qm.message.verb, poolReference.endPoint());\n                // session may have already finished; see CASSANDRA-5668\n                if (state == null)\n                {\n                    byte[] traceTypeBytes = qm.message.parameters.get(Tracing.TRACE_TYPE);\n                    Tracing.TraceType traceType = traceTypeBytes == null ? Tracing.TraceType.QUERY : Tracing.TraceType.deserialize(traceTypeBytes[0]);\n                    TraceState.mutateWithTracing(ByteBuffer.wrap(sessionBytes), message, -1, traceType.getTTL());\n                }\n                else\n                {\n                    state.trace(message);\n                    if (qm.message.verb == MessagingService.Verb.REQUEST_RESPONSE)\n                        Tracing.instance.doneWithNonLocalSession(state);\n                }\n            }\n\n            long timestampMillis = NanoTimeToCurrentTimeMillis.convert(qm.timestampNanos);\n            writeInternal(qm.message, qm.id, timestampMillis);\n\n            completed++;\n            if (flush)\n                out.flush();\n        }\n        catch (Exception e)\n        {\n            disconnect();\n            if (e instanceof IOException || e.getCause() instanceof IOException)\n            {\n                if (logger.isTraceEnabled())\n                    logger.trace(\"error writing to {}\", poolReference.endPoint(), e);\n\n                // if the message was important, such as a repair acknowledgement, put it back on the queue\n                // to retry after re-connecting.  See CASSANDRA-5393\n                if (qm.shouldRetry())\n                {\n                    try\n                    {\n                        backlog.put(new RetriedQueuedMessage(qm));\n                    }\n                    catch (InterruptedException e1)\n                    {\n                        throw new AssertionError(e1);\n                    }\n                }\n            }\n            else\n            {\n                // Non IO exceptions are likely a programming error so let's not silence them\n                logger.error(\"error writing to {}\", poolReference.endPoint(), e);\n            }\n        }\n    }"
        ],
        [
            "ColumnFamilyInputFormat::createAuthenticatedClient(String,int,Configuration)",
            "  63  \n  64  \n  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  ",
            "    @SuppressWarnings(\"resource\")\n    public static Cassandra.Client createAuthenticatedClient(String location, int port, Configuration conf) throws Exception\n    {\n        logger.debug(\"Creating authenticated client for CF input format\");\n        TTransport transport;\n        try\n        {\n            transport = ConfigHelper.getClientTransportFactory(conf).openTransport(location, port);\n        }\n        catch (Exception e)\n        {\n            throw new TTransportException(\"Failed to open a transport to \" + location + \":\" + port + \".\", e);\n        }\n        TProtocol binaryProtocol = new TBinaryProtocol(transport, true, true);\n        Cassandra.Client client = new Cassandra.Client(binaryProtocol);\n\n        // log in\n        client.set_keyspace(ConfigHelper.getInputKeyspace(conf));\n        if ((ConfigHelper.getInputKeyspaceUserName(conf) != null) && (ConfigHelper.getInputKeyspacePassword(conf) != null))\n        {\n            Map<String, String> creds = new HashMap<String, String>();\n            creds.put(PasswordAuthenticator.USERNAME_KEY, ConfigHelper.getInputKeyspaceUserName(conf));\n            creds.put(PasswordAuthenticator.PASSWORD_KEY, ConfigHelper.getInputKeyspacePassword(conf));\n            AuthenticationRequest authRequest = new AuthenticationRequest(creds);\n            client.login(authRequest);\n        }\n        logger.debug(\"Authenticated client for CF input format created successfully\");\n        return client;\n    }",
            "  63  \n  64  \n  65  \n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  ",
            "    @SuppressWarnings(\"resource\")\n    public static Cassandra.Client createAuthenticatedClient(String location, int port, Configuration conf) throws Exception\n    {\n        logger.trace(\"Creating authenticated client for CF input format\");\n        TTransport transport;\n        try\n        {\n            transport = ConfigHelper.getClientTransportFactory(conf).openTransport(location, port);\n        }\n        catch (Exception e)\n        {\n            throw new TTransportException(\"Failed to open a transport to \" + location + \":\" + port + \".\", e);\n        }\n        TProtocol binaryProtocol = new TBinaryProtocol(transport, true, true);\n        Cassandra.Client client = new Cassandra.Client(binaryProtocol);\n\n        // log in\n        client.set_keyspace(ConfigHelper.getInputKeyspace(conf));\n        if ((ConfigHelper.getInputKeyspaceUserName(conf) != null) && (ConfigHelper.getInputKeyspacePassword(conf) != null))\n        {\n            Map<String, String> creds = new HashMap<String, String>();\n            creds.put(PasswordAuthenticator.USERNAME_KEY, ConfigHelper.getInputKeyspaceUserName(conf));\n            creds.put(PasswordAuthenticator.PASSWORD_KEY, ConfigHelper.getInputKeyspacePassword(conf));\n            AuthenticationRequest authRequest = new AuthenticationRequest(creds);\n            client.login(authRequest);\n        }\n        logger.trace(\"Authenticated client for CF input format created successfully\");\n        return client;\n    }"
        ],
        [
            "CounterMutationVerbHandler::doVerb(MessageIn,int)",
            "  35  \n  36  \n  37  \n  38 -\n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void doVerb(final MessageIn<CounterMutation> message, final int id)\n    {\n        final CounterMutation cm = message.payload;\n        logger.debug(\"Applying forwarded {}\", cm);\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        // We should not wait for the result of the write in this thread,\n        // otherwise we could have a distributed deadlock between replicas\n        // running this VerbHandler (see #4578).\n        // Instead, we use a callback to send the response. Note that the callback\n        // will not be called if the request timeout, but this is ok\n        // because the coordinator of the counter mutation will timeout on\n        // it's own in that case.\n        StorageProxy.applyCounterMutationOnLeader(cm, localDataCenter, new Runnable()\n        {\n            public void run()\n            {\n                MessagingService.instance().sendReply(new WriteResponse().createMessage(), id, message.from);\n            }\n        });\n    }",
            "  35  \n  36  \n  37  \n  38 +\n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void doVerb(final MessageIn<CounterMutation> message, final int id)\n    {\n        final CounterMutation cm = message.payload;\n        logger.trace(\"Applying forwarded {}\", cm);\n\n        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(FBUtilities.getBroadcastAddress());\n        // We should not wait for the result of the write in this thread,\n        // otherwise we could have a distributed deadlock between replicas\n        // running this VerbHandler (see #4578).\n        // Instead, we use a callback to send the response. Note that the callback\n        // will not be called if the request timeout, but this is ok\n        // because the coordinator of the counter mutation will timeout on\n        // it's own in that case.\n        StorageProxy.applyCounterMutationOnLeader(cm, localDataCenter, new Runnable()\n        {\n            public void run()\n            {\n                MessagingService.instance().sendReply(new WriteResponse().createMessage(), id, message.from);\n            }\n        });\n    }"
        ],
        [
            "LeveledManifest::logDistribution()",
            " 452  \n 453  \n 454 -\n 455  \n 456  \n 457  \n 458  \n 459  \n 460 -\n 461  \n 462  \n 463  \n 464  \n 465  ",
            "    private void logDistribution()\n    {\n        if (logger.isDebugEnabled())\n        {\n            for (int i = 0; i < generations.length; i++)\n            {\n                if (!getLevel(i).isEmpty())\n                {\n                    logger.debug(\"L{} contains {} SSTables ({} bytes) in {}\",\n                                 i, getLevel(i).size(), SSTableReader.getTotalBytes(getLevel(i)), this);\n                }\n            }\n        }\n    }",
            " 452  \n 453  \n 454 +\n 455  \n 456  \n 457  \n 458  \n 459  \n 460 +\n 461  \n 462  \n 463  \n 464  \n 465  ",
            "    private void logDistribution()\n    {\n        if (logger.isTraceEnabled())\n        {\n            for (int i = 0; i < generations.length; i++)\n            {\n                if (!getLevel(i).isEmpty())\n                {\n                    logger.trace(\"L{} contains {} SSTables ({} bytes) in {}\",\n                                 i, getLevel(i).size(), SSTableReader.getTotalBytes(getLevel(i)), this);\n                }\n            }\n        }\n    }"
        ],
        [
            "IndexSummaryManager::adjustSamplingLevels(List,Map,double,long)",
            " 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 -\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380 -\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427 -\n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  ",
            "    private static List<SSTableReader> adjustSamplingLevels(List<SSTableReader> sstables, Map<UUID, LifecycleTransaction> transactions,\n                                                            double totalReadsPerSec, long memoryPoolCapacity) throws IOException\n    {\n\n        List<ResampleEntry> toDownsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> toUpsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> forceResample = new ArrayList<>();\n        List<ResampleEntry> forceUpsample = new ArrayList<>();\n        List<SSTableReader> newSSTables = new ArrayList<>(sstables.size());\n\n        // Going from the coldest to the hottest sstables, try to give each sstable an amount of space proportional\n        // to the number of total reads/sec it handles.\n        long remainingSpace = memoryPoolCapacity;\n        for (SSTableReader sstable : sstables)\n        {\n            int minIndexInterval = sstable.metadata.getMinIndexInterval();\n            int maxIndexInterval = sstable.metadata.getMaxIndexInterval();\n\n            double readsPerSec = sstable.getReadMeter() == null ? 0.0 : sstable.getReadMeter().fifteenMinuteRate();\n            long idealSpace = Math.round(remainingSpace * (readsPerSec / totalReadsPerSec));\n\n            // figure out how many entries our idealSpace would buy us, and pick a new sampling level based on that\n            int currentNumEntries = sstable.getIndexSummarySize();\n            double avgEntrySize = sstable.getIndexSummaryOffHeapSize() / (double) currentNumEntries;\n            long targetNumEntries = Math.max(1, Math.round(idealSpace / avgEntrySize));\n            int currentSamplingLevel = sstable.getIndexSummarySamplingLevel();\n            int maxSummarySize = sstable.getMaxIndexSummarySize();\n\n            // if the min_index_interval changed, calculate what our current sampling level would be under the new min\n            if (sstable.getMinIndexInterval() != minIndexInterval)\n            {\n                int effectiveSamplingLevel = (int) Math.round(currentSamplingLevel * (minIndexInterval / (double) sstable.getMinIndexInterval()));\n                maxSummarySize = (int) Math.round(maxSummarySize * (sstable.getMinIndexInterval() / (double) minIndexInterval));\n                logger.trace(\"min_index_interval changed from {} to {}, so the current sampling level for {} is effectively now {} (was {})\",\n                             sstable.getMinIndexInterval(), minIndexInterval, sstable, effectiveSamplingLevel, currentSamplingLevel);\n                currentSamplingLevel = effectiveSamplingLevel;\n            }\n\n            int newSamplingLevel = IndexSummaryBuilder.calculateSamplingLevel(currentSamplingLevel, currentNumEntries, targetNumEntries,\n                    minIndexInterval, maxIndexInterval);\n            int numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, maxSummarySize);\n            double effectiveIndexInterval = sstable.getEffectiveIndexInterval();\n\n            logger.trace(\"{} has {} reads/sec; ideal space for index summary: {} bytes ({} entries); considering moving \" +\n                    \"from level {} ({} entries, {} bytes) to level {} ({} entries, {} bytes)\",\n                    sstable.getFilename(), readsPerSec, idealSpace, targetNumEntries, currentSamplingLevel, currentNumEntries,\n                    currentNumEntries * avgEntrySize, newSamplingLevel, numEntriesAtNewSamplingLevel,\n                    numEntriesAtNewSamplingLevel * avgEntrySize);\n\n            if (effectiveIndexInterval < minIndexInterval)\n            {\n                // The min_index_interval was changed; re-sample to match it.\n                logger.debug(\"Forcing resample of {} because the current index interval ({}) is below min_index_interval ({})\",\n                        sstable, effectiveIndexInterval, minIndexInterval);\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceResample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else if (effectiveIndexInterval > maxIndexInterval)\n            {\n                // The max_index_interval was lowered; force an upsample to the effective minimum sampling level\n                logger.debug(\"Forcing upsample of {} because the current index interval ({}) is above max_index_interval ({})\",\n                        sstable, effectiveIndexInterval, maxIndexInterval);\n                newSamplingLevel = Math.max(1, (BASE_SAMPLING_LEVEL * minIndexInterval) / maxIndexInterval);\n                numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, sstable.getMaxIndexSummarySize());\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries >= currentNumEntries * UPSAMPLE_THRESHOLD && newSamplingLevel > currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries < currentNumEntries * DOWNSAMPLE_THESHOLD && newSamplingLevel < currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toDownsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else\n            {\n                // keep the same sampling level\n                logger.trace(\"SSTable {} is within thresholds of ideal sampling\", sstable);\n                remainingSpace -= sstable.getIndexSummaryOffHeapSize();\n                newSSTables.add(sstable);\n                transactions.get(sstable.metadata.cfId).cancel(sstable);\n            }\n            totalReadsPerSec -= readsPerSec;\n        }\n\n        if (remainingSpace > 0)\n        {\n            Pair<List<SSTableReader>, List<ResampleEntry>> result = distributeRemainingSpace(toDownsample, remainingSpace);\n            toDownsample = result.right;\n            newSSTables.addAll(result.left);\n            for (SSTableReader sstable : result.left)\n                transactions.get(sstable.metadata.cfId).cancel(sstable);\n        }\n\n        // downsample first, then upsample\n        toDownsample.addAll(forceResample);\n        toDownsample.addAll(toUpsample);\n        toDownsample.addAll(forceUpsample);\n        for (ResampleEntry entry : toDownsample)\n        {\n            SSTableReader sstable = entry.sstable;\n            logger.debug(\"Re-sampling index summary for {} from {}/{} to {}/{} of the original number of entries\",\n                         sstable, sstable.getIndexSummarySamplingLevel(), Downsampling.BASE_SAMPLING_LEVEL,\n                         entry.newSamplingLevel, Downsampling.BASE_SAMPLING_LEVEL);\n            ColumnFamilyStore cfs = Keyspace.open(sstable.metadata.ksName).getColumnFamilyStore(sstable.metadata.cfId);\n            SSTableReader replacement = sstable.cloneWithNewSummarySamplingLevel(cfs, entry.newSamplingLevel);\n            newSSTables.add(replacement);\n            transactions.get(sstable.metadata.cfId).update(replacement, true);\n        }\n\n        return newSSTables;\n    }",
            " 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 +\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380 +\n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427 +\n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  ",
            "    private static List<SSTableReader> adjustSamplingLevels(List<SSTableReader> sstables, Map<UUID, LifecycleTransaction> transactions,\n                                                            double totalReadsPerSec, long memoryPoolCapacity) throws IOException\n    {\n\n        List<ResampleEntry> toDownsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> toUpsample = new ArrayList<>(sstables.size() / 4);\n        List<ResampleEntry> forceResample = new ArrayList<>();\n        List<ResampleEntry> forceUpsample = new ArrayList<>();\n        List<SSTableReader> newSSTables = new ArrayList<>(sstables.size());\n\n        // Going from the coldest to the hottest sstables, try to give each sstable an amount of space proportional\n        // to the number of total reads/sec it handles.\n        long remainingSpace = memoryPoolCapacity;\n        for (SSTableReader sstable : sstables)\n        {\n            int minIndexInterval = sstable.metadata.getMinIndexInterval();\n            int maxIndexInterval = sstable.metadata.getMaxIndexInterval();\n\n            double readsPerSec = sstable.getReadMeter() == null ? 0.0 : sstable.getReadMeter().fifteenMinuteRate();\n            long idealSpace = Math.round(remainingSpace * (readsPerSec / totalReadsPerSec));\n\n            // figure out how many entries our idealSpace would buy us, and pick a new sampling level based on that\n            int currentNumEntries = sstable.getIndexSummarySize();\n            double avgEntrySize = sstable.getIndexSummaryOffHeapSize() / (double) currentNumEntries;\n            long targetNumEntries = Math.max(1, Math.round(idealSpace / avgEntrySize));\n            int currentSamplingLevel = sstable.getIndexSummarySamplingLevel();\n            int maxSummarySize = sstable.getMaxIndexSummarySize();\n\n            // if the min_index_interval changed, calculate what our current sampling level would be under the new min\n            if (sstable.getMinIndexInterval() != minIndexInterval)\n            {\n                int effectiveSamplingLevel = (int) Math.round(currentSamplingLevel * (minIndexInterval / (double) sstable.getMinIndexInterval()));\n                maxSummarySize = (int) Math.round(maxSummarySize * (sstable.getMinIndexInterval() / (double) minIndexInterval));\n                logger.trace(\"min_index_interval changed from {} to {}, so the current sampling level for {} is effectively now {} (was {})\",\n                             sstable.getMinIndexInterval(), minIndexInterval, sstable, effectiveSamplingLevel, currentSamplingLevel);\n                currentSamplingLevel = effectiveSamplingLevel;\n            }\n\n            int newSamplingLevel = IndexSummaryBuilder.calculateSamplingLevel(currentSamplingLevel, currentNumEntries, targetNumEntries,\n                    minIndexInterval, maxIndexInterval);\n            int numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, maxSummarySize);\n            double effectiveIndexInterval = sstable.getEffectiveIndexInterval();\n\n            logger.trace(\"{} has {} reads/sec; ideal space for index summary: {} bytes ({} entries); considering moving \" +\n                    \"from level {} ({} entries, {} bytes) to level {} ({} entries, {} bytes)\",\n                    sstable.getFilename(), readsPerSec, idealSpace, targetNumEntries, currentSamplingLevel, currentNumEntries,\n                    currentNumEntries * avgEntrySize, newSamplingLevel, numEntriesAtNewSamplingLevel,\n                    numEntriesAtNewSamplingLevel * avgEntrySize);\n\n            if (effectiveIndexInterval < minIndexInterval)\n            {\n                // The min_index_interval was changed; re-sample to match it.\n                logger.trace(\"Forcing resample of {} because the current index interval ({}) is below min_index_interval ({})\",\n                        sstable, effectiveIndexInterval, minIndexInterval);\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceResample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else if (effectiveIndexInterval > maxIndexInterval)\n            {\n                // The max_index_interval was lowered; force an upsample to the effective minimum sampling level\n                logger.trace(\"Forcing upsample of {} because the current index interval ({}) is above max_index_interval ({})\",\n                        sstable, effectiveIndexInterval, maxIndexInterval);\n                newSamplingLevel = Math.max(1, (BASE_SAMPLING_LEVEL * minIndexInterval) / maxIndexInterval);\n                numEntriesAtNewSamplingLevel = IndexSummaryBuilder.entriesAtSamplingLevel(newSamplingLevel, sstable.getMaxIndexSummarySize());\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                forceUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries >= currentNumEntries * UPSAMPLE_THRESHOLD && newSamplingLevel > currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toUpsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= avgEntrySize * numEntriesAtNewSamplingLevel;\n            }\n            else if (targetNumEntries < currentNumEntries * DOWNSAMPLE_THESHOLD && newSamplingLevel < currentSamplingLevel)\n            {\n                long spaceUsed = (long) Math.ceil(avgEntrySize * numEntriesAtNewSamplingLevel);\n                toDownsample.add(new ResampleEntry(sstable, spaceUsed, newSamplingLevel));\n                remainingSpace -= spaceUsed;\n            }\n            else\n            {\n                // keep the same sampling level\n                logger.trace(\"SSTable {} is within thresholds of ideal sampling\", sstable);\n                remainingSpace -= sstable.getIndexSummaryOffHeapSize();\n                newSSTables.add(sstable);\n                transactions.get(sstable.metadata.cfId).cancel(sstable);\n            }\n            totalReadsPerSec -= readsPerSec;\n        }\n\n        if (remainingSpace > 0)\n        {\n            Pair<List<SSTableReader>, List<ResampleEntry>> result = distributeRemainingSpace(toDownsample, remainingSpace);\n            toDownsample = result.right;\n            newSSTables.addAll(result.left);\n            for (SSTableReader sstable : result.left)\n                transactions.get(sstable.metadata.cfId).cancel(sstable);\n        }\n\n        // downsample first, then upsample\n        toDownsample.addAll(forceResample);\n        toDownsample.addAll(toUpsample);\n        toDownsample.addAll(forceUpsample);\n        for (ResampleEntry entry : toDownsample)\n        {\n            SSTableReader sstable = entry.sstable;\n            logger.trace(\"Re-sampling index summary for {} from {}/{} to {}/{} of the original number of entries\",\n                         sstable, sstable.getIndexSummarySamplingLevel(), Downsampling.BASE_SAMPLING_LEVEL,\n                         entry.newSamplingLevel, Downsampling.BASE_SAMPLING_LEVEL);\n            ColumnFamilyStore cfs = Keyspace.open(sstable.metadata.ksName).getColumnFamilyStore(sstable.metadata.cfId);\n            SSTableReader replacement = sstable.cloneWithNewSummarySamplingLevel(cfs, entry.newSamplingLevel);\n            newSSTables.add(replacement);\n            transactions.get(sstable.metadata.cfId).update(replacement, true);\n        }\n\n        return newSSTables;\n    }"
        ],
        [
            "LifecycleTransaction::doCommit(Throwable)",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "    /**\n     * point of no return: commit all changes, but leave all readers marked as compacting\n     */\n    public Throwable doCommit(Throwable accumulate)\n    {\n        assert staged.isEmpty() : \"must be no actions introduced between prepareToCommit and a commit\";\n\n        logger.debug(\"Committing update:{}, obsolete:{}\", staged.update, staged.obsolete);\n\n        // this is now the point of no return; we cannot safely rollback, so we ignore exceptions until we're done\n        // we restore state by obsoleting our obsolete files, releasing our references to them, and updating our size\n        // and notification status for the obsolete and new files\n        accumulate = markObsolete(tracker, logged.obsolete, accumulate);\n        accumulate = tracker.updateSizeTracking(logged.obsolete, logged.update, accumulate);\n        accumulate = release(selfRefs(logged.obsolete), accumulate);\n        accumulate = tracker.notifySSTablesChanged(originals, logged.update, operationType, accumulate);\n        return accumulate;\n    }",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "    /**\n     * point of no return: commit all changes, but leave all readers marked as compacting\n     */\n    public Throwable doCommit(Throwable accumulate)\n    {\n        assert staged.isEmpty() : \"must be no actions introduced between prepareToCommit and a commit\";\n\n        logger.trace(\"Committing update:{}, obsolete:{}\", staged.update, staged.obsolete);\n\n        // this is now the point of no return; we cannot safely rollback, so we ignore exceptions until we're done\n        // we restore state by obsoleting our obsolete files, releasing our references to them, and updating our size\n        // and notification status for the obsolete and new files\n        accumulate = markObsolete(tracker, logged.obsolete, accumulate);\n        accumulate = tracker.updateSizeTracking(logged.obsolete, logged.update, accumulate);\n        accumulate = release(selfRefs(logged.obsolete), accumulate);\n        accumulate = tracker.notifySSTablesChanged(originals, logged.update, operationType, accumulate);\n        return accumulate;\n    }"
        ],
        [
            "CustomTThreadPoolServer::serve()",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "    @SuppressWarnings(\"resource\")\n    public void serve()\n    {\n        try\n        {\n            serverTransport_.listen();\n        }\n        catch (TTransportException ttx)\n        {\n            logger.error(\"Error occurred during listening.\", ttx);\n            return;\n        }\n\n        stopped = false;\n        while (!stopped)\n        {\n            // block until we are under max clients\n            while (activeClients.get() >= args.maxWorkerThreads)\n            {\n                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.MILLISECONDS);\n            }\n\n            try\n            {\n                TTransport client = serverTransport_.accept();\n                activeClients.incrementAndGet();\n                WorkerProcess wp = new WorkerProcess(client);\n                executorService.execute(wp);\n            }\n            catch (TTransportException ttx)\n            {\n                if (ttx.getCause() instanceof SocketTimeoutException) // thrift sucks\n                    continue;\n\n                if (!stopped)\n                {\n                    logger.warn(\"Transport error occurred during acceptance of message.\", ttx);\n                }\n            }\n            catch (RejectedExecutionException e)\n            {\n                // worker thread decremented activeClients but hadn't finished exiting\n                logger.debug(\"Dropping client connection because our limit of {} has been reached\", args.maxWorkerThreads);\n                continue;\n            }\n\n            if (activeClients.get() >= args.maxWorkerThreads)\n                logger.warn(\"Maximum number of clients {} reached\", args.maxWorkerThreads);\n        }\n\n        executorService.shutdown();\n        // Thrift's default shutdown waits for the WorkerProcess threads to complete.  We do not,\n        // because doing that allows a client to hold our shutdown \"hostage\" by simply not sending\n        // another message after stop is called (since process will block indefinitely trying to read\n        // the next meessage header).\n        //\n        // The \"right\" fix would be to update thrift to set a socket timeout on client connections\n        // (and tolerate unintentional timeouts until stopped is set).  But this requires deep\n        // changes to the code generator, so simply setting these threads to daemon (in our custom\n        // CleaningThreadPool) and ignoring them after shutdown is good enough.\n        //\n        // Remember, our goal on shutdown is not necessarily that each client request we receive\n        // gets answered first [to do that, you should redirect clients to a different coordinator\n        // first], but rather (1) to make sure that for each update we ack as successful, we generate\n        // hints for any non-responsive replicas, and (2) to make sure that we quickly stop\n        // accepting client connections so shutdown can continue.  Not waiting for the WorkerProcess\n        // threads here accomplishes (2); MessagingService's shutdown method takes care of (1).\n        //\n        // See CASSANDRA-3335 and CASSANDRA-3727.\n    }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "    @SuppressWarnings(\"resource\")\n    public void serve()\n    {\n        try\n        {\n            serverTransport_.listen();\n        }\n        catch (TTransportException ttx)\n        {\n            logger.error(\"Error occurred during listening.\", ttx);\n            return;\n        }\n\n        stopped = false;\n        while (!stopped)\n        {\n            // block until we are under max clients\n            while (activeClients.get() >= args.maxWorkerThreads)\n            {\n                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.MILLISECONDS);\n            }\n\n            try\n            {\n                TTransport client = serverTransport_.accept();\n                activeClients.incrementAndGet();\n                WorkerProcess wp = new WorkerProcess(client);\n                executorService.execute(wp);\n            }\n            catch (TTransportException ttx)\n            {\n                if (ttx.getCause() instanceof SocketTimeoutException) // thrift sucks\n                    continue;\n\n                if (!stopped)\n                {\n                    logger.warn(\"Transport error occurred during acceptance of message.\", ttx);\n                }\n            }\n            catch (RejectedExecutionException e)\n            {\n                // worker thread decremented activeClients but hadn't finished exiting\n                logger.trace(\"Dropping client connection because our limit of {} has been reached\", args.maxWorkerThreads);\n                continue;\n            }\n\n            if (activeClients.get() >= args.maxWorkerThreads)\n                logger.warn(\"Maximum number of clients {} reached\", args.maxWorkerThreads);\n        }\n\n        executorService.shutdown();\n        // Thrift's default shutdown waits for the WorkerProcess threads to complete.  We do not,\n        // because doing that allows a client to hold our shutdown \"hostage\" by simply not sending\n        // another message after stop is called (since process will block indefinitely trying to read\n        // the next meessage header).\n        //\n        // The \"right\" fix would be to update thrift to set a socket timeout on client connections\n        // (and tolerate unintentional timeouts until stopped is set).  But this requires deep\n        // changes to the code generator, so simply setting these threads to daemon (in our custom\n        // CleaningThreadPool) and ignoring them after shutdown is good enough.\n        //\n        // Remember, our goal on shutdown is not necessarily that each client request we receive\n        // gets answered first [to do that, you should redirect clients to a different coordinator\n        // first], but rather (1) to make sure that for each update we ack as successful, we generate\n        // hints for any non-responsive replicas, and (2) to make sure that we quickly stop\n        // accepting client connections so shutdown can continue.  Not waiting for the WorkerProcess\n        // threads here accomplishes (2); MessagingService's shutdown method takes care of (1).\n        //\n        // See CASSANDRA-3335 and CASSANDRA-3727.\n    }"
        ],
        [
            "CassandraServer::get_paged_slice(String,KeyRange,ByteBuffer,ConsistencyLevel)",
            "1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213 -\n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  ",
            "    public List<KeySlice> get_paged_slice(String column_family, KeyRange range, ByteBuffer start_column, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_family\", column_family,\n                                                                  \"range\", range.toString(),\n                                                                  \"start_column\", ByteBufferUtil.bytesToHex(start_column),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_paged_slice\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"get_paged_slice\");\n        }\n\n        try\n        {\n\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_family);\n            ThriftValidation.validateKeyRange(metadata, null, range);\n\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            SlicePredicate predicate = new SlicePredicate().setSlice_range(new SliceRange(start_column, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, -1));\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds;\n            if (range.start_key == null)\n            {\n                // (token, key) is unsupported, assume (token, token)\n                Token.TokenFactory tokenFactory = p.getTokenFactory();\n                Token left = tokenFactory.fromString(range.start_token);\n                Token right = tokenFactory.fromString(range.end_token);\n                bounds = Range.makeRowRange(left, right);\n            }\n            else\n            {\n                RowPosition end = range.end_key == null\n                                ? p.getTokenFactory().fromString(range.end_token).maxKeyBound()\n                                : RowPosition.ForKey.get(range.end_key, p);\n                bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(range.start_key, p), end);\n            }\n\n            if (range.row_filter != null && !range.row_filter.isEmpty())\n                throw new InvalidRequestException(\"Cross-row paging is not supported along with index clauses\");\n\n            List<Row> rows;\n            long now = System.currentTimeMillis();\n            schedule(DatabaseDescriptor.getRangeRpcTimeout());\n            try\n            {\n                IDiskAtomFilter filter = ThriftValidation.asIFilter(predicate, metadata, null);\n                rows = StorageProxy.getRangeSlice(new RangeSliceCommand(keyspace, column_family, now, filter, bounds, null, range.count, true, true), consistencyLevel);\n            }\n            finally\n            {\n                release();\n            }\n            assert rows != null;\n\n            return thriftifyKeySlices(rows, new ColumnParent(column_family), predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213 +\n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  ",
            "    public List<KeySlice> get_paged_slice(String column_family, KeyRange range, ByteBuffer start_column, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_family\", column_family,\n                                                                  \"range\", range.toString(),\n                                                                  \"start_column\", ByteBufferUtil.bytesToHex(start_column),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_paged_slice\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"get_paged_slice\");\n        }\n\n        try\n        {\n\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_family, Permission.SELECT);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_family);\n            ThriftValidation.validateKeyRange(metadata, null, range);\n\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);\n            consistencyLevel.validateForRead(keyspace);\n\n            SlicePredicate predicate = new SlicePredicate().setSlice_range(new SliceRange(start_column, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, -1));\n\n            IPartitioner p = StorageService.getPartitioner();\n            AbstractBounds<RowPosition> bounds;\n            if (range.start_key == null)\n            {\n                // (token, key) is unsupported, assume (token, token)\n                Token.TokenFactory tokenFactory = p.getTokenFactory();\n                Token left = tokenFactory.fromString(range.start_token);\n                Token right = tokenFactory.fromString(range.end_token);\n                bounds = Range.makeRowRange(left, right);\n            }\n            else\n            {\n                RowPosition end = range.end_key == null\n                                ? p.getTokenFactory().fromString(range.end_token).maxKeyBound()\n                                : RowPosition.ForKey.get(range.end_key, p);\n                bounds = new Bounds<RowPosition>(RowPosition.ForKey.get(range.start_key, p), end);\n            }\n\n            if (range.row_filter != null && !range.row_filter.isEmpty())\n                throw new InvalidRequestException(\"Cross-row paging is not supported along with index clauses\");\n\n            List<Row> rows;\n            long now = System.currentTimeMillis();\n            schedule(DatabaseDescriptor.getRangeRpcTimeout());\n            try\n            {\n                IDiskAtomFilter filter = ThriftValidation.asIFilter(predicate, metadata, null);\n                rows = StorageProxy.getRangeSlice(new RangeSliceCommand(keyspace, column_family, now, filter, bounds, null, range.count, true, true), consistencyLevel);\n            }\n            finally\n            {\n                release();\n            }\n            assert rows != null;\n\n            return thriftifyKeySlices(rows, new ColumnParent(column_family), predicate, now);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "assureSufficientLiveNodes(Keyspace,Iterable)",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295  \n 296  \n 297  \n 298  \n 299  ",
            "    public void assureSufficientLiveNodes(Keyspace keyspace, Iterable<InetAddress> liveEndpoints) throws UnavailableException\n    {\n        int blockFor = blockFor(keyspace);\n        switch (this)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n                if (countLocalEndpoints(liveEndpoints) == 0)\n                    throw new UnavailableException(this, 1, 0);\n                break;\n            case LOCAL_QUORUM:\n                int localLive = countLocalEndpoints(liveEndpoints);\n                if (localLive < blockFor)\n                {\n                    if (logger.isDebugEnabled())\n                    {\n                        StringBuilder builder = new StringBuilder(\"Local replicas [\");\n                        for (InetAddress endpoint : liveEndpoints)\n                        {\n                            if (isLocal(endpoint))\n                                builder.append(endpoint).append(\",\");\n                        }\n                        builder.append(\"] are insufficient to satisfy LOCAL_QUORUM requirement of \").append(blockFor).append(\" live nodes in '\").append(DatabaseDescriptor.getLocalDataCenter()).append(\"'\");\n                        logger.debug(builder.toString());\n                    }\n                    throw new UnavailableException(this, blockFor, localLive);\n                }\n                break;\n            case EACH_QUORUM:\n                if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)\n                {\n                    for (Map.Entry<String, Integer> entry : countPerDCEndpoints(keyspace, liveEndpoints).entrySet())\n                    {\n                        int dcBlockFor = localQuorumFor(keyspace, entry.getKey());\n                        int dcLive = entry.getValue();\n                        if (dcLive < dcBlockFor)\n                            throw new UnavailableException(this, dcBlockFor, dcLive);\n                    }\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = Iterables.size(liveEndpoints);\n                if (live < blockFor)\n                {\n                    logger.debug(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(liveEndpoints), blockFor);\n                    throw new UnavailableException(this, blockFor, live);\n                }\n                break;\n        }\n    }",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295  \n 296  \n 297  \n 298  \n 299  ",
            "    public void assureSufficientLiveNodes(Keyspace keyspace, Iterable<InetAddress> liveEndpoints) throws UnavailableException\n    {\n        int blockFor = blockFor(keyspace);\n        switch (this)\n        {\n            case ANY:\n                // local hint is acceptable, and local node is always live\n                break;\n            case LOCAL_ONE:\n                if (countLocalEndpoints(liveEndpoints) == 0)\n                    throw new UnavailableException(this, 1, 0);\n                break;\n            case LOCAL_QUORUM:\n                int localLive = countLocalEndpoints(liveEndpoints);\n                if (localLive < blockFor)\n                {\n                    if (logger.isTraceEnabled())\n                    {\n                        StringBuilder builder = new StringBuilder(\"Local replicas [\");\n                        for (InetAddress endpoint : liveEndpoints)\n                        {\n                            if (isLocal(endpoint))\n                                builder.append(endpoint).append(\",\");\n                        }\n                        builder.append(\"] are insufficient to satisfy LOCAL_QUORUM requirement of \").append(blockFor).append(\" live nodes in '\").append(DatabaseDescriptor.getLocalDataCenter()).append(\"'\");\n                        logger.trace(builder.toString());\n                    }\n                    throw new UnavailableException(this, blockFor, localLive);\n                }\n                break;\n            case EACH_QUORUM:\n                if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)\n                {\n                    for (Map.Entry<String, Integer> entry : countPerDCEndpoints(keyspace, liveEndpoints).entrySet())\n                    {\n                        int dcBlockFor = localQuorumFor(keyspace, entry.getKey());\n                        int dcLive = entry.getValue();\n                        if (dcLive < dcBlockFor)\n                            throw new UnavailableException(this, dcBlockFor, dcLive);\n                    }\n                    break;\n                }\n                // Fallthough on purpose for SimpleStrategy\n            default:\n                int live = Iterables.size(liveEndpoints);\n                if (live < blockFor)\n                {\n                    logger.trace(\"Live nodes {} do not satisfy ConsistencyLevel ({} required)\", Iterables.toString(liveEndpoints), blockFor);\n                    throw new UnavailableException(this, blockFor, live);\n                }\n                break;\n        }\n    }"
        ],
        [
            "SizeEstimatesRecorder::run()",
            "  56  \n  57  \n  58  \n  59  \n  60 -\n  61  \n  62  \n  63  \n  64 -\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    public void run()\n    {\n        if (StorageService.instance.isStarting())\n        {\n            logger.debug(\"Node has not yet joined; not recording size estimates\");\n            return;\n        }\n\n        logger.debug(\"Recording size estimates\");\n\n        // find primary token ranges for the local node.\n        Collection<Token> localTokens = StorageService.instance.getLocalTokens();\n        Collection<Range<Token>> localRanges = StorageService.instance.getTokenMetadata().getPrimaryRangesFor(localTokens);\n\n        for (Keyspace keyspace : Keyspace.nonSystem())\n        {\n            for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n            {\n                long start = System.nanoTime();\n                recordSizeEstimates(table, localRanges);\n                long passed = System.nanoTime() - start;\n                logger.debug(\"Spent {} milliseconds on estimating {}.{} size\",\n                             TimeUnit.NANOSECONDS.toMillis(passed),\n                             table.metadata.ksName,\n                             table.metadata.cfName);\n            }\n        }\n    }",
            "  56  \n  57  \n  58  \n  59  \n  60 +\n  61  \n  62  \n  63  \n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    public void run()\n    {\n        if (StorageService.instance.isStarting())\n        {\n            logger.trace(\"Node has not yet joined; not recording size estimates\");\n            return;\n        }\n\n        logger.trace(\"Recording size estimates\");\n\n        // find primary token ranges for the local node.\n        Collection<Token> localTokens = StorageService.instance.getLocalTokens();\n        Collection<Range<Token>> localRanges = StorageService.instance.getTokenMetadata().getPrimaryRangesFor(localTokens);\n\n        for (Keyspace keyspace : Keyspace.nonSystem())\n        {\n            for (ColumnFamilyStore table : keyspace.getColumnFamilyStores())\n            {\n                long start = System.nanoTime();\n                recordSizeEstimates(table, localRanges);\n                long passed = System.nanoTime() - start;\n                logger.trace(\"Spent {} milliseconds on estimating {}.{} size\",\n                             TimeUnit.NANOSECONDS.toMillis(passed),\n                             table.metadata.ksName,\n                             table.metadata.cfName);\n            }\n        }\n    }"
        ],
        [
            "Tracing::stopSession()",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    /**\n     * Stop the session and record its complete.  Called by coodinator when request is complete.\n     */\n    public void stopSession()\n    {\n        TraceState state = this.state.get();\n        if (state == null) // inline isTracing to avoid implicit two calls to state.get()\n        {\n            logger.debug(\"request complete\");\n        }\n        else\n        {\n            final int elapsed = state.elapsed();\n            final ByteBuffer sessionId = state.sessionIdBytes;\n            final int ttl = state.ttl;\n\n            TraceState.executeMutation(TraceKeyspace.makeStopSessionMutation(sessionId, elapsed, ttl));\n\n            state.stop();\n            sessions.remove(state.sessionId);\n            this.state.set(null);\n        }\n    }",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    /**\n     * Stop the session and record its complete.  Called by coodinator when request is complete.\n     */\n    public void stopSession()\n    {\n        TraceState state = this.state.get();\n        if (state == null) // inline isTracing to avoid implicit two calls to state.get()\n        {\n            logger.trace(\"request complete\");\n        }\n        else\n        {\n            final int elapsed = state.elapsed();\n            final ByteBuffer sessionId = state.sessionIdBytes;\n            final int ttl = state.ttl;\n\n            TraceState.executeMutation(TraceKeyspace.makeStopSessionMutation(sessionId, elapsed, ttl));\n\n            state.stop();\n            sessions.remove(state.sessionId);\n            this.state.set(null);\n        }\n    }"
        ],
        [
            "AutoSavingCache::Writer::saveCache()",
            " 321  \n 322  \n 323 -\n 324  \n 325  \n 326  \n 327  \n 328 -\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  ",
            "        public void saveCache()\n        {\n            logger.debug(\"Deleting old {} files.\", cacheType);\n            deleteOldCacheFiles();\n\n            if (!keyIterator.hasNext())\n            {\n                logger.debug(\"Skipping {} save, cache is empty.\", cacheType);\n                return;\n            }\n\n            long start = System.nanoTime();\n\n            WrappedDataOutputStreamPlus writer = null;\n            Pair<File, File> cacheFilePaths = tempCacheFiles();\n            try\n            {\n                try\n                {\n                    writer = new WrappedDataOutputStreamPlus(streamFactory.getOutputStream(cacheFilePaths.left, cacheFilePaths.right));\n                }\n                catch (FileNotFoundException e)\n                {\n                    throw new RuntimeException(e);\n                }\n\n                try\n                {\n                    //Need to be able to check schema version because CF names are ambiguous\n                    UUID schemaVersion = Schema.instance.getVersion();\n                    if (schemaVersion == null)\n                    {\n                        Schema.instance.updateVersion();\n                        schemaVersion = Schema.instance.getVersion();\n                    }\n                    writer.writeLong(schemaVersion.getMostSignificantBits());\n                    writer.writeLong(schemaVersion.getLeastSignificantBits());\n\n                    while (keyIterator.hasNext())\n                    {\n                        K key = keyIterator.next();\n\n                        ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreIncludingIndexes(key.ksAndCFName);\n                        if (cfs == null)\n                            continue; // the table or 2i has been dropped.\n\n                        cacheLoader.serialize(key, writer, cfs);\n\n                        keysWritten++;\n                        if (keysWritten >= keysEstimate)\n                            break;\n                    }\n                }\n                catch (IOException e)\n                {\n                    throw new FSWriteError(e, cacheFilePaths.left);\n                }\n\n            }\n            finally\n            {\n                if (writer != null)\n                    FileUtils.closeQuietly(writer);\n            }\n\n            File cacheFile = getCacheDataPath(CURRENT_VERSION);\n            File crcFile = getCacheCrcPath(CURRENT_VERSION);\n\n            cacheFile.delete(); // ignore error if it didn't exist\n            crcFile.delete();\n\n            if (!cacheFilePaths.left.renameTo(cacheFile))\n                logger.error(\"Unable to rename {} to {}\", cacheFilePaths.left, cacheFile);\n\n            if (!cacheFilePaths.right.renameTo(crcFile))\n                logger.error(\"Unable to rename {} to {}\", cacheFilePaths.right, crcFile);\n\n            logger.info(\"Saved {} ({} items) in {} ms\", cacheType, keysWritten, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n        }",
            " 321  \n 322  \n 323 +\n 324  \n 325  \n 326  \n 327  \n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  ",
            "        public void saveCache()\n        {\n            logger.trace(\"Deleting old {} files.\", cacheType);\n            deleteOldCacheFiles();\n\n            if (!keyIterator.hasNext())\n            {\n                logger.trace(\"Skipping {} save, cache is empty.\", cacheType);\n                return;\n            }\n\n            long start = System.nanoTime();\n\n            WrappedDataOutputStreamPlus writer = null;\n            Pair<File, File> cacheFilePaths = tempCacheFiles();\n            try\n            {\n                try\n                {\n                    writer = new WrappedDataOutputStreamPlus(streamFactory.getOutputStream(cacheFilePaths.left, cacheFilePaths.right));\n                }\n                catch (FileNotFoundException e)\n                {\n                    throw new RuntimeException(e);\n                }\n\n                try\n                {\n                    //Need to be able to check schema version because CF names are ambiguous\n                    UUID schemaVersion = Schema.instance.getVersion();\n                    if (schemaVersion == null)\n                    {\n                        Schema.instance.updateVersion();\n                        schemaVersion = Schema.instance.getVersion();\n                    }\n                    writer.writeLong(schemaVersion.getMostSignificantBits());\n                    writer.writeLong(schemaVersion.getLeastSignificantBits());\n\n                    while (keyIterator.hasNext())\n                    {\n                        K key = keyIterator.next();\n\n                        ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreIncludingIndexes(key.ksAndCFName);\n                        if (cfs == null)\n                            continue; // the table or 2i has been dropped.\n\n                        cacheLoader.serialize(key, writer, cfs);\n\n                        keysWritten++;\n                        if (keysWritten >= keysEstimate)\n                            break;\n                    }\n                }\n                catch (IOException e)\n                {\n                    throw new FSWriteError(e, cacheFilePaths.left);\n                }\n\n            }\n            finally\n            {\n                if (writer != null)\n                    FileUtils.closeQuietly(writer);\n            }\n\n            File cacheFile = getCacheDataPath(CURRENT_VERSION);\n            File crcFile = getCacheCrcPath(CURRENT_VERSION);\n\n            cacheFile.delete(); // ignore error if it didn't exist\n            crcFile.delete();\n\n            if (!cacheFilePaths.left.renameTo(cacheFile))\n                logger.error(\"Unable to rename {} to {}\", cacheFilePaths.left, cacheFile);\n\n            if (!cacheFilePaths.right.renameTo(crcFile))\n                logger.error(\"Unable to rename {} to {}\", cacheFilePaths.right, crcFile);\n\n            logger.info(\"Saved {} ({} items) in {} ms\", cacheType, keysWritten, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n        }"
        ],
        [
            "SSTable::delete(Descriptor,Set)",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  ",
            "    /**\n     * We use a ReferenceQueue to manage deleting files that have been compacted\n     * and for which no more SSTable references exist.  But this is not guaranteed\n     * to run for each such file because of the semantics of the JVM gc.  So,\n     * we write a marker to `compactedFilename` when a file is compacted;\n     * if such a marker exists on startup, the file should be removed.\n     *\n     * This method will also remove SSTables that are marked as temporary.\n     *\n     * @return true if the file was deleted\n     */\n    public static boolean delete(Descriptor desc, Set<Component> components)\n    {\n        // remove the DATA component first if it exists\n        if (components.contains(Component.DATA))\n            FileUtils.deleteWithConfirm(desc.filenameFor(Component.DATA));\n        for (Component component : components)\n        {\n            if (component.equals(Component.DATA) || component.equals(Component.SUMMARY))\n                continue;\n\n            FileUtils.deleteWithConfirm(desc.filenameFor(component));\n        }\n        FileUtils.delete(desc.filenameFor(Component.SUMMARY));\n\n        logger.debug(\"Deleted {}\", desc);\n        return true;\n    }",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119  \n 120  ",
            "    /**\n     * We use a ReferenceQueue to manage deleting files that have been compacted\n     * and for which no more SSTable references exist.  But this is not guaranteed\n     * to run for each such file because of the semantics of the JVM gc.  So,\n     * we write a marker to `compactedFilename` when a file is compacted;\n     * if such a marker exists on startup, the file should be removed.\n     *\n     * This method will also remove SSTables that are marked as temporary.\n     *\n     * @return true if the file was deleted\n     */\n    public static boolean delete(Descriptor desc, Set<Component> components)\n    {\n        // remove the DATA component first if it exists\n        if (components.contains(Component.DATA))\n            FileUtils.deleteWithConfirm(desc.filenameFor(Component.DATA));\n        for (Component component : components)\n        {\n            if (component.equals(Component.DATA) || component.equals(Component.SUMMARY))\n                continue;\n\n            FileUtils.deleteWithConfirm(desc.filenameFor(component));\n        }\n        FileUtils.delete(desc.filenameFor(Component.SUMMARY));\n\n        logger.trace(\"Deleted {}\", desc);\n        return true;\n    }"
        ],
        [
            "LeveledCompactionStrategy::getNextBackgroundTask(int)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "    /**\n     * the only difference between background and maximal in LCS is that maximal is still allowed\n     * (by explicit user request) even when compaction is disabled.\n     */\n    @SuppressWarnings(\"resource\")\n    public synchronized AbstractCompactionTask getNextBackgroundTask(int gcBefore)\n    {\n        while (true)\n        {\n            OperationType op;\n            LeveledManifest.CompactionCandidate candidate = manifest.getCompactionCandidates();\n            if (candidate == null)\n            {\n                // if there is no sstable to compact in standard way, try compacting based on droppable tombstone ratio\n                SSTableReader sstable = findDroppableSSTable(gcBefore);\n                if (sstable == null)\n                {\n                    logger.debug(\"No compaction necessary for {}\", this);\n                    return null;\n                }\n                candidate = new LeveledManifest.CompactionCandidate(Collections.singleton(sstable),\n                                                                    sstable.getSSTableLevel(),\n                                                                    getMaxSSTableBytes());\n                op = OperationType.TOMBSTONE_COMPACTION;\n            }\n            else\n            {\n                op = OperationType.COMPACTION;\n            }\n\n            LifecycleTransaction txn = cfs.getTracker().tryModify(candidate.sstables, OperationType.COMPACTION);\n            if (txn != null)\n            {\n                LeveledCompactionTask newTask = new LeveledCompactionTask(cfs, txn, candidate.level, gcBefore, candidate.maxSSTableBytes, false);\n                newTask.setCompactionType(op);\n                return newTask;\n            }\n        }\n    }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "    /**\n     * the only difference between background and maximal in LCS is that maximal is still allowed\n     * (by explicit user request) even when compaction is disabled.\n     */\n    @SuppressWarnings(\"resource\")\n    public synchronized AbstractCompactionTask getNextBackgroundTask(int gcBefore)\n    {\n        while (true)\n        {\n            OperationType op;\n            LeveledManifest.CompactionCandidate candidate = manifest.getCompactionCandidates();\n            if (candidate == null)\n            {\n                // if there is no sstable to compact in standard way, try compacting based on droppable tombstone ratio\n                SSTableReader sstable = findDroppableSSTable(gcBefore);\n                if (sstable == null)\n                {\n                    logger.trace(\"No compaction necessary for {}\", this);\n                    return null;\n                }\n                candidate = new LeveledManifest.CompactionCandidate(Collections.singleton(sstable),\n                                                                    sstable.getSSTableLevel(),\n                                                                    getMaxSSTableBytes());\n                op = OperationType.TOMBSTONE_COMPACTION;\n            }\n            else\n            {\n                op = OperationType.COMPACTION;\n            }\n\n            LifecycleTransaction txn = cfs.getTracker().tryModify(candidate.sstables, OperationType.COMPACTION);\n            if (txn != null)\n            {\n                LeveledCompactionTask newTask = new LeveledCompactionTask(cfs, txn, candidate.level, gcBefore, candidate.maxSSTableBytes, false);\n                newTask.setCompactionType(op);\n                return newTask;\n            }\n        }\n    }"
        ],
        [
            "GCInspector::handleNotification(Notification,Object)",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 -\n 286 -\n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "    public void handleNotification(final Notification notification, final Object handback)\n    {\n        String type = notification.getType();\n        if (type.equals(GarbageCollectionNotificationInfo.GARBAGE_COLLECTION_NOTIFICATION))\n        {\n            // retrieve the garbage collection notification information\n            CompositeData cd = (CompositeData) notification.getUserData();\n            GarbageCollectionNotificationInfo info = GarbageCollectionNotificationInfo.from(cd);\n            String gcName = info.getGcName();\n            GcInfo gcInfo = info.getGcInfo();\n\n            long duration = gcInfo.getDuration();\n\n            /*\n             * The duration supplied in the notification info includes more than just\n             * application stopped time for concurrent GCs. Try and do a better job coming up with a good stopped time\n             * value by asking for and tracking cumulative time spent blocked in GC.\n             */\n            GCState gcState = gcStates.get(gcName);\n            if (gcState.assumeGCIsPartiallyConcurrent)\n            {\n                long previousTotal = gcState.lastGcTotalDuration;\n                long total = gcState.gcBean.getCollectionTime();\n                gcState.lastGcTotalDuration = total;\n                duration = total - previousTotal; // may be zero for a really fast collection\n            }\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(info.getGcName()).append(\" GC in \").append(duration).append(\"ms.  \");\n            long bytes = 0;\n            Map<String, MemoryUsage> beforeMemoryUsage = gcInfo.getMemoryUsageBeforeGc();\n            Map<String, MemoryUsage> afterMemoryUsage = gcInfo.getMemoryUsageAfterGc();\n            for (String key : gcState.keys(info))\n            {\n                MemoryUsage before = beforeMemoryUsage.get(key);\n                MemoryUsage after = afterMemoryUsage.get(key);\n                if (after != null && after.getUsed() != before.getUsed())\n                {\n                    sb.append(key).append(\": \").append(before.getUsed());\n                    sb.append(\" -> \");\n                    sb.append(after.getUsed());\n                    if (!key.equals(gcState.keys[gcState.keys.length - 1]))\n                        sb.append(\"; \");\n                    bytes += before.getUsed() - after.getUsed();\n                }\n            }\n\n            while (true)\n            {\n                State prev = state.get();\n                if (state.compareAndSet(prev, new State(duration, bytes, prev)))\n                    break;\n            }\n\n            String st = sb.toString();\n            if (GC_WARN_THRESHOLD_IN_MS != 0 && duration > GC_WARN_THRESHOLD_IN_MS)\n                logger.warn(st);\n            else if (duration > MIN_LOG_DURATION)\n                logger.info(st);\n            else if (logger.isDebugEnabled())\n                logger.debug(st);\n\n            if (duration > STAT_THRESHOLD)\n                StatusLogger.log();\n\n            // if we just finished an old gen collection and we're still using a lot of memory, try to reduce the pressure\n            if (gcState.assumeGCIsOldGen)\n                SSTableDeletingTask.rescheduleFailedTasks();\n        }\n    }",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 +\n 286 +\n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "    public void handleNotification(final Notification notification, final Object handback)\n    {\n        String type = notification.getType();\n        if (type.equals(GarbageCollectionNotificationInfo.GARBAGE_COLLECTION_NOTIFICATION))\n        {\n            // retrieve the garbage collection notification information\n            CompositeData cd = (CompositeData) notification.getUserData();\n            GarbageCollectionNotificationInfo info = GarbageCollectionNotificationInfo.from(cd);\n            String gcName = info.getGcName();\n            GcInfo gcInfo = info.getGcInfo();\n\n            long duration = gcInfo.getDuration();\n\n            /*\n             * The duration supplied in the notification info includes more than just\n             * application stopped time for concurrent GCs. Try and do a better job coming up with a good stopped time\n             * value by asking for and tracking cumulative time spent blocked in GC.\n             */\n            GCState gcState = gcStates.get(gcName);\n            if (gcState.assumeGCIsPartiallyConcurrent)\n            {\n                long previousTotal = gcState.lastGcTotalDuration;\n                long total = gcState.gcBean.getCollectionTime();\n                gcState.lastGcTotalDuration = total;\n                duration = total - previousTotal; // may be zero for a really fast collection\n            }\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(info.getGcName()).append(\" GC in \").append(duration).append(\"ms.  \");\n            long bytes = 0;\n            Map<String, MemoryUsage> beforeMemoryUsage = gcInfo.getMemoryUsageBeforeGc();\n            Map<String, MemoryUsage> afterMemoryUsage = gcInfo.getMemoryUsageAfterGc();\n            for (String key : gcState.keys(info))\n            {\n                MemoryUsage before = beforeMemoryUsage.get(key);\n                MemoryUsage after = afterMemoryUsage.get(key);\n                if (after != null && after.getUsed() != before.getUsed())\n                {\n                    sb.append(key).append(\": \").append(before.getUsed());\n                    sb.append(\" -> \");\n                    sb.append(after.getUsed());\n                    if (!key.equals(gcState.keys[gcState.keys.length - 1]))\n                        sb.append(\"; \");\n                    bytes += before.getUsed() - after.getUsed();\n                }\n            }\n\n            while (true)\n            {\n                State prev = state.get();\n                if (state.compareAndSet(prev, new State(duration, bytes, prev)))\n                    break;\n            }\n\n            String st = sb.toString();\n            if (GC_WARN_THRESHOLD_IN_MS != 0 && duration > GC_WARN_THRESHOLD_IN_MS)\n                logger.warn(st);\n            else if (duration > MIN_LOG_DURATION)\n                logger.info(st);\n            else if (logger.isTraceEnabled())\n                logger.trace(st);\n\n            if (duration > STAT_THRESHOLD)\n                StatusLogger.log();\n\n            // if we just finished an old gen collection and we're still using a lot of memory, try to reduce the pressure\n            if (gcState.assumeGCIsOldGen)\n                SSTableDeletingTask.rescheduleFailedTasks();\n        }\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::onRemove(Host)",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  ",
            "    @Override\n    public void onRemove(Host host)\n    {\n        if (liveReplicaHosts.remove(host))\n        {\n            logger.debug(\"Removed the host {}\", host);\n        }\n    }",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171  \n 172  ",
            "    @Override\n    public void onRemove(Host host)\n    {\n        if (liveReplicaHosts.remove(host))\n        {\n            logger.trace(\"Removed the host {}\", host);\n        }\n    }"
        ],
        [
            "CommitLogArchiver::construct()",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "    public static CommitLogArchiver construct()\n    {\n        Properties commitlog_commands = new Properties();\n        try (InputStream stream = CommitLogArchiver.class.getClassLoader().getResourceAsStream(\"commitlog_archiving.properties\"))\n        {\n            if (stream == null)\n            {\n                logger.debug(\"No commitlog_archiving properties found; archive + pitr will be disabled\");\n                return disabled();\n            }\n            else\n            {\n                commitlog_commands.load(stream);\n                String archiveCommand = commitlog_commands.getProperty(\"archive_command\");\n                String restoreCommand = commitlog_commands.getProperty(\"restore_command\");\n                String restoreDirectories = commitlog_commands.getProperty(\"restore_directories\");\n                if (restoreDirectories != null && !restoreDirectories.isEmpty())\n                {\n                    for (String dir : restoreDirectories.split(DELIMITER))\n                    {\n                        File directory = new File(dir);\n                        if (!directory.exists())\n                        {\n                            if (!directory.mkdir())\n                            {\n                                throw new RuntimeException(\"Unable to create directory: \" + dir);\n                            }\n                        }\n                    }\n                }\n                String targetTime = commitlog_commands.getProperty(\"restore_point_in_time\");\n                TimeUnit precision = TimeUnit.valueOf(commitlog_commands.getProperty(\"precision\", \"MICROSECONDS\"));\n                long restorePointInTime;\n                try\n                {\n                    restorePointInTime = Strings.isNullOrEmpty(targetTime) ? Long.MAX_VALUE : format.parse(targetTime).getTime();\n                }\n                catch (ParseException e)\n                {\n                    throw new RuntimeException(\"Unable to parse restore target time\", e);\n                }\n                return new CommitLogArchiver(archiveCommand, restoreCommand, restoreDirectories, restorePointInTime, precision);\n            }\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(\"Unable to load commitlog_archiving.properties\", e);\n        }\n\n    }",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "    public static CommitLogArchiver construct()\n    {\n        Properties commitlog_commands = new Properties();\n        try (InputStream stream = CommitLogArchiver.class.getClassLoader().getResourceAsStream(\"commitlog_archiving.properties\"))\n        {\n            if (stream == null)\n            {\n                logger.trace(\"No commitlog_archiving properties found; archive + pitr will be disabled\");\n                return disabled();\n            }\n            else\n            {\n                commitlog_commands.load(stream);\n                String archiveCommand = commitlog_commands.getProperty(\"archive_command\");\n                String restoreCommand = commitlog_commands.getProperty(\"restore_command\");\n                String restoreDirectories = commitlog_commands.getProperty(\"restore_directories\");\n                if (restoreDirectories != null && !restoreDirectories.isEmpty())\n                {\n                    for (String dir : restoreDirectories.split(DELIMITER))\n                    {\n                        File directory = new File(dir);\n                        if (!directory.exists())\n                        {\n                            if (!directory.mkdir())\n                            {\n                                throw new RuntimeException(\"Unable to create directory: \" + dir);\n                            }\n                        }\n                    }\n                }\n                String targetTime = commitlog_commands.getProperty(\"restore_point_in_time\");\n                TimeUnit precision = TimeUnit.valueOf(commitlog_commands.getProperty(\"precision\", \"MICROSECONDS\"));\n                long restorePointInTime;\n                try\n                {\n                    restorePointInTime = Strings.isNullOrEmpty(targetTime) ? Long.MAX_VALUE : format.parse(targetTime).getTime();\n                }\n                catch (ParseException e)\n                {\n                    throw new RuntimeException(\"Unable to parse restore target time\", e);\n                }\n                return new CommitLogArchiver(archiveCommand, restoreCommand, restoreDirectories, restorePointInTime, precision);\n            }\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(\"Unable to load commitlog_archiving.properties\", e);\n        }\n\n    }"
        ],
        [
            "LifecycleTransaction::checkpoint(Throwable)",
            " 222  \n 223  \n 224 -\n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  ",
            "    private Throwable checkpoint(Throwable accumulate)\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"Checkpointing update:{}, obsolete:{}\", staged.update, staged.obsolete);\n\n        if (staged.isEmpty())\n            return accumulate;\n\n        Set<SSTableReader> toUpdate = toUpdate();\n        Set<SSTableReader> fresh = copyOf(fresh());\n\n        // check the current versions of the readers we're replacing haven't somehow been replaced by someone else\n        checkNotReplaced(filterIn(toUpdate, staged.update));\n\n        // ensure any new readers are in the compacting set, since we aren't done with them yet\n        // and don't want anyone else messing with them\n        // apply atomically along with updating the live set of readers\n        tracker.apply(compose(updateCompacting(emptySet(), fresh),\n                              updateLiveSet(toUpdate, staged.update)));\n\n        // log the staged changes and our newly marked readers\n        marked.addAll(fresh);\n        logged.log(staged);\n\n        // setup our tracker, and mark our prior versions replaced, also releasing our references to them\n        // we do not replace/release obsoleted readers, since we may need to restore them on rollback\n        accumulate = setReplaced(filterOut(toUpdate, staged.obsolete), accumulate);\n        accumulate = release(selfRefs(filterOut(toUpdate, staged.obsolete)), accumulate);\n\n        staged.clear();\n        return accumulate;\n    }",
            " 222  \n 223  \n 224 +\n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  ",
            "    private Throwable checkpoint(Throwable accumulate)\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Checkpointing update:{}, obsolete:{}\", staged.update, staged.obsolete);\n\n        if (staged.isEmpty())\n            return accumulate;\n\n        Set<SSTableReader> toUpdate = toUpdate();\n        Set<SSTableReader> fresh = copyOf(fresh());\n\n        // check the current versions of the readers we're replacing haven't somehow been replaced by someone else\n        checkNotReplaced(filterIn(toUpdate, staged.update));\n\n        // ensure any new readers are in the compacting set, since we aren't done with them yet\n        // and don't want anyone else messing with them\n        // apply atomically along with updating the live set of readers\n        tracker.apply(compose(updateCompacting(emptySet(), fresh),\n                              updateLiveSet(toUpdate, staged.update)));\n\n        // log the staged changes and our newly marked readers\n        marked.addAll(fresh);\n        logged.log(staged);\n\n        // setup our tracker, and mark our prior versions replaced, also releasing our references to them\n        // we do not replace/release obsoleted readers, since we may need to restore them on rollback\n        accumulate = setReplaced(filterOut(toUpdate, staged.obsolete), accumulate);\n        accumulate = release(selfRefs(filterOut(toUpdate, staged.obsolete)), accumulate);\n\n        staged.clear();\n        return accumulate;\n    }"
        ],
        [
            "SystemKeyspace::getPreviousVersionString()",
            "1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113 -\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  ",
            "    /**\n     * Try to determine what the previous version, if any, was installed on this node.\n     * Primary source of truth is the release version in system.local. If the previous\n     * version cannot be determined by looking there then either:\n     * * the node never had a C* install before\n     * * the was a very old version (pre 1.2) installed, which did not include system.local\n     *\n     * @return either a version read from the system.local table or one of two special values\n     * indicating either no previous version (SystemUpgrade.NULL_VERSION) or an unreadable,\n     * legacy version (SystemUpgrade.UNREADABLE_VERSION).\n     */\n    private static String getPreviousVersionString()\n    {\n        String req = \"SELECT release_version FROM system.%s WHERE key='%s'\";\n        UntypedResultSet result = executeInternal(String.format(req, SystemKeyspace.LOCAL, SystemKeyspace.LOCAL));\n        if (result.isEmpty() || !result.one().has(\"release_version\"))\n        {\n            // it isn't inconceivable that one might try to upgrade a node straight from <= 1.1 to whatever\n            // the current version is. If we couldn't read a previous version from system.local we check for\n            // the existence of the legacy system.Versions table. We don't actually attempt to read a version\n            // from there, but it informs us that this isn't a completely new node.\n            for (File dataDirectory : Directories.getKSChildDirectories(SystemKeyspace.NAME))\n            {\n                if (dataDirectory.getName().equals(\"Versions\") && dataDirectory.listFiles().length > 0)\n                {\n                    logger.debug(\"Found unreadable versions info in pre 1.2 system.Versions table\");\n                    return UNREADABLE_VERSION.toString();\n                }\n            }\n\n            // no previous version information found, we can assume that this is a new node\n            return NULL_VERSION.toString();\n        }\n        // report back whatever we found in the system table\n        return result.one().getString(\"release_version\");\n    }",
            "1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113 +\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  ",
            "    /**\n     * Try to determine what the previous version, if any, was installed on this node.\n     * Primary source of truth is the release version in system.local. If the previous\n     * version cannot be determined by looking there then either:\n     * * the node never had a C* install before\n     * * the was a very old version (pre 1.2) installed, which did not include system.local\n     *\n     * @return either a version read from the system.local table or one of two special values\n     * indicating either no previous version (SystemUpgrade.NULL_VERSION) or an unreadable,\n     * legacy version (SystemUpgrade.UNREADABLE_VERSION).\n     */\n    private static String getPreviousVersionString()\n    {\n        String req = \"SELECT release_version FROM system.%s WHERE key='%s'\";\n        UntypedResultSet result = executeInternal(String.format(req, SystemKeyspace.LOCAL, SystemKeyspace.LOCAL));\n        if (result.isEmpty() || !result.one().has(\"release_version\"))\n        {\n            // it isn't inconceivable that one might try to upgrade a node straight from <= 1.1 to whatever\n            // the current version is. If we couldn't read a previous version from system.local we check for\n            // the existence of the legacy system.Versions table. We don't actually attempt to read a version\n            // from there, but it informs us that this isn't a completely new node.\n            for (File dataDirectory : Directories.getKSChildDirectories(SystemKeyspace.NAME))\n            {\n                if (dataDirectory.getName().equals(\"Versions\") && dataDirectory.listFiles().length > 0)\n                {\n                    logger.trace(\"Found unreadable versions info in pre 1.2 system.Versions table\");\n                    return UNREADABLE_VERSION.toString();\n                }\n            }\n\n            // no previous version information found, we can assume that this is a new node\n            return NULL_VERSION.toString();\n        }\n        // report back whatever we found in the system table\n        return result.one().getString(\"release_version\");\n    }"
        ],
        [
            "CommitLogReplayer::logAndCheckIfShouldSkip(File,CommitLogDescriptor)",
            " 399  \n 400  \n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409 -\n 410  \n 411  \n 412  \n 413  ",
            "    public boolean logAndCheckIfShouldSkip(File file, CommitLogDescriptor desc)\n    {\n        logger.info(\"Replaying {} (CL version {}, messaging version {}, compression {})\",\n                    file.getPath(),\n                    desc.version,\n                    desc.getMessagingVersion(),\n                    desc.compression);\n\n        if (globalPosition.segment > desc.id)\n        {\n            logger.debug(\"skipping replay of fully-flushed {}\", file);\n            return true;\n        }\n        return false;\n    }",
            " 399  \n 400  \n 401 +\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409 +\n 410  \n 411  \n 412  \n 413  ",
            "    public boolean logAndCheckIfShouldSkip(File file, CommitLogDescriptor desc)\n    {\n        logger.debug(\"Replaying {} (CL version {}, messaging version {}, compression {})\",\n                    file.getPath(),\n                    desc.version,\n                    desc.getMessagingVersion(),\n                    desc.compression);\n\n        if (globalPosition.segment > desc.id)\n        {\n            logger.trace(\"skipping replay of fully-flushed {}\", file);\n            return true;\n        }\n        return false;\n    }"
        ],
        [
            "ColumnFamilyRecordReader::WideRowIterator::maybeInit()",
            " 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 -\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "        private void maybeInit()\n        {\n            if (wideColumns != null && wideColumns.hasNext())\n                return;\n\n            KeyRange keyRange;\n            if (totalRead == 0)\n            {\n                String startToken = split.getStartToken();\n                keyRange = new KeyRange(batchSize)\n                          .setStart_token(startToken)\n                          .setEnd_token(split.getEndToken())\n                          .setRow_filter(filter);\n            }\n            else\n            {\n                KeySlice lastRow = Iterables.getLast(rows);\n                logger.debug(\"Starting with last-seen row {}\", lastRow.key);\n                keyRange = new KeyRange(batchSize)\n                          .setStart_key(lastRow.key)\n                          .setEnd_token(split.getEndToken())\n                          .setRow_filter(filter);\n            }\n\n            try\n            {\n                rows = client.get_paged_slice(cfName, keyRange, lastColumn, consistencyLevel);\n                int n = 0;\n                for (KeySlice row : rows)\n                    n += row.columns.size();\n                logger.debug(\"read {} columns in {} rows for {} starting with {}\",\n                             new Object[]{ n, rows.size(), keyRange, lastColumn });\n\n                wideColumns = Iterators.peekingIterator(new WideColumnIterator(rows));\n                if (wideColumns.hasNext() && wideColumns.peek().right.keySet().iterator().next().equals(lastColumn))\n                    wideColumns.next();\n                if (!wideColumns.hasNext())\n                    rows = null;\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n        }",
            " 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 +\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "        private void maybeInit()\n        {\n            if (wideColumns != null && wideColumns.hasNext())\n                return;\n\n            KeyRange keyRange;\n            if (totalRead == 0)\n            {\n                String startToken = split.getStartToken();\n                keyRange = new KeyRange(batchSize)\n                          .setStart_token(startToken)\n                          .setEnd_token(split.getEndToken())\n                          .setRow_filter(filter);\n            }\n            else\n            {\n                KeySlice lastRow = Iterables.getLast(rows);\n                logger.trace(\"Starting with last-seen row {}\", lastRow.key);\n                keyRange = new KeyRange(batchSize)\n                          .setStart_key(lastRow.key)\n                          .setEnd_token(split.getEndToken())\n                          .setRow_filter(filter);\n            }\n\n            try\n            {\n                rows = client.get_paged_slice(cfName, keyRange, lastColumn, consistencyLevel);\n                int n = 0;\n                for (KeySlice row : rows)\n                    n += row.columns.size();\n                logger.trace(\"read {} columns in {} rows for {} starting with {}\",\n                             new Object[]{ n, rows.size(), keyRange, lastColumn });\n\n                wideColumns = Iterators.peekingIterator(new WideColumnIterator(rows));\n                if (wideColumns.hasNext() && wideColumns.peek().right.keySet().iterator().next().equals(lastColumn))\n                    wideColumns.next();\n                if (!wideColumns.hasNext())\n                    rows = null;\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n        }"
        ],
        [
            "BootStrapper::bootstrap(StreamStateStore,boolean)",
            "  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "    public ListenableFuture<StreamState> bootstrap(StreamStateStore stateStore, boolean useStrictConsistency)\n    {\n        logger.debug(\"Beginning bootstrap process\");\n\n        RangeStreamer streamer = new RangeStreamer(tokenMetadata,\n                                                   tokens,\n                                                   address,\n                                                   \"Bootstrap\",\n                                                   useStrictConsistency,\n                                                   DatabaseDescriptor.getEndpointSnitch(),\n                                                   stateStore);\n        streamer.addSourceFilter(new RangeStreamer.FailureDetectorSourceFilter(FailureDetector.instance));\n\n        for (String keyspaceName : Schema.instance.getNonSystemKeyspaces())\n        {\n            AbstractReplicationStrategy strategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n            streamer.addRanges(keyspaceName, strategy.getPendingAddressRanges(tokenMetadata, tokens, address));\n        }\n\n        StreamResultFuture bootstrapStreamResult = streamer.fetchAsync();\n        bootstrapStreamResult.addEventListener(new StreamEventHandler()\n        {\n            private final AtomicInteger receivedFiles = new AtomicInteger();\n            private final AtomicInteger totalFilesToReceive = new AtomicInteger();\n\n            @Override\n            public void handleStreamEvent(StreamEvent event)\n            {\n                switch (event.eventType)\n                {\n                    case STREAM_PREPARED:\n                        StreamEvent.SessionPreparedEvent prepared = (StreamEvent.SessionPreparedEvent) event;\n                        int currentTotal = totalFilesToReceive.addAndGet((int) prepared.session.getTotalFilesToReceive());\n                        ProgressEvent prepareProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), currentTotal, \"prepare with \" + prepared.session.peer + \" complete\");\n                        fireProgressEvent(\"bootstrap\", prepareProgress);\n                        break;\n\n                    case FILE_PROGRESS:\n                        StreamEvent.ProgressEvent progress = (StreamEvent.ProgressEvent) event;\n                        if (progress.progress.isCompleted())\n                        {\n                            int received = receivedFiles.incrementAndGet();\n                            ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.PROGRESS, received, totalFilesToReceive.get(), \"received file \" + progress.progress.fileName);\n                            fireProgressEvent(\"bootstrap\", currentProgress);\n                        }\n                        break;\n\n                    case STREAM_COMPLETE:\n                        StreamEvent.SessionCompleteEvent completeEvent = (StreamEvent.SessionCompleteEvent) event;\n                        ProgressEvent completeProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), totalFilesToReceive.get(), \"session with \" + completeEvent.peer + \" complete\");\n                        fireProgressEvent(\"bootstrap\", completeProgress);\n                        break;\n                }\n            }\n\n            @Override\n            public void onSuccess(StreamState streamState)\n            {\n                ProgressEventType type;\n                String message;\n\n                if (streamState.hasFailedSession())\n                {\n                    type = ProgressEventType.ERROR;\n                    message = \"Some bootstrap stream failed\";\n                }\n                else\n                {\n                    type = ProgressEventType.SUCCESS;\n                    message = \"Bootstrap streaming success\";\n                }\n                ProgressEvent currentProgress = new ProgressEvent(type, receivedFiles.get(), totalFilesToReceive.get(), message);\n                fireProgressEvent(\"bootstrap\", currentProgress);\n            }\n\n            @Override\n            public void onFailure(Throwable throwable)\n            {\n                ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.ERROR, receivedFiles.get(), totalFilesToReceive.get(), throwable.getMessage());\n                fireProgressEvent(\"bootstrap\", currentProgress);\n            }\n        });\n        return bootstrapStreamResult;\n    }",
            "  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "    public ListenableFuture<StreamState> bootstrap(StreamStateStore stateStore, boolean useStrictConsistency)\n    {\n        logger.trace(\"Beginning bootstrap process\");\n\n        RangeStreamer streamer = new RangeStreamer(tokenMetadata,\n                                                   tokens,\n                                                   address,\n                                                   \"Bootstrap\",\n                                                   useStrictConsistency,\n                                                   DatabaseDescriptor.getEndpointSnitch(),\n                                                   stateStore);\n        streamer.addSourceFilter(new RangeStreamer.FailureDetectorSourceFilter(FailureDetector.instance));\n\n        for (String keyspaceName : Schema.instance.getNonSystemKeyspaces())\n        {\n            AbstractReplicationStrategy strategy = Keyspace.open(keyspaceName).getReplicationStrategy();\n            streamer.addRanges(keyspaceName, strategy.getPendingAddressRanges(tokenMetadata, tokens, address));\n        }\n\n        StreamResultFuture bootstrapStreamResult = streamer.fetchAsync();\n        bootstrapStreamResult.addEventListener(new StreamEventHandler()\n        {\n            private final AtomicInteger receivedFiles = new AtomicInteger();\n            private final AtomicInteger totalFilesToReceive = new AtomicInteger();\n\n            @Override\n            public void handleStreamEvent(StreamEvent event)\n            {\n                switch (event.eventType)\n                {\n                    case STREAM_PREPARED:\n                        StreamEvent.SessionPreparedEvent prepared = (StreamEvent.SessionPreparedEvent) event;\n                        int currentTotal = totalFilesToReceive.addAndGet((int) prepared.session.getTotalFilesToReceive());\n                        ProgressEvent prepareProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), currentTotal, \"prepare with \" + prepared.session.peer + \" complete\");\n                        fireProgressEvent(\"bootstrap\", prepareProgress);\n                        break;\n\n                    case FILE_PROGRESS:\n                        StreamEvent.ProgressEvent progress = (StreamEvent.ProgressEvent) event;\n                        if (progress.progress.isCompleted())\n                        {\n                            int received = receivedFiles.incrementAndGet();\n                            ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.PROGRESS, received, totalFilesToReceive.get(), \"received file \" + progress.progress.fileName);\n                            fireProgressEvent(\"bootstrap\", currentProgress);\n                        }\n                        break;\n\n                    case STREAM_COMPLETE:\n                        StreamEvent.SessionCompleteEvent completeEvent = (StreamEvent.SessionCompleteEvent) event;\n                        ProgressEvent completeProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), totalFilesToReceive.get(), \"session with \" + completeEvent.peer + \" complete\");\n                        fireProgressEvent(\"bootstrap\", completeProgress);\n                        break;\n                }\n            }\n\n            @Override\n            public void onSuccess(StreamState streamState)\n            {\n                ProgressEventType type;\n                String message;\n\n                if (streamState.hasFailedSession())\n                {\n                    type = ProgressEventType.ERROR;\n                    message = \"Some bootstrap stream failed\";\n                }\n                else\n                {\n                    type = ProgressEventType.SUCCESS;\n                    message = \"Bootstrap streaming success\";\n                }\n                ProgressEvent currentProgress = new ProgressEvent(type, receivedFiles.get(), totalFilesToReceive.get(), message);\n                fireProgressEvent(\"bootstrap\", currentProgress);\n            }\n\n            @Override\n            public void onFailure(Throwable throwable)\n            {\n                ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.ERROR, receivedFiles.get(), totalFilesToReceive.get(), throwable.getMessage());\n                fireProgressEvent(\"bootstrap\", currentProgress);\n            }\n        });\n        return bootstrapStreamResult;\n    }"
        ],
        [
            "LifecycleTransaction::split(Collection)",
            " 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408 -\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * remove the provided readers from this Transaction, and return a new Transaction to manage them\n     * only permitted to be called if the current Transaction has never been used\n     */\n    public LifecycleTransaction split(Collection<SSTableReader> readers)\n    {\n        logger.debug(\"Splitting {} into new transaction\", readers);\n        checkUnused();\n        for (SSTableReader reader : readers)\n            assert identities.contains(reader.instanceId) : \"may only split the same reader instance the transaction was opened with: \" + reader;\n\n        for (SSTableReader reader : readers)\n        {\n            identities.remove(reader.instanceId);\n            originals.remove(reader);\n            marked.remove(reader);\n        }\n        return new LifecycleTransaction(tracker, operationType, readers);\n    }",
            " 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408 +\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * remove the provided readers from this Transaction, and return a new Transaction to manage them\n     * only permitted to be called if the current Transaction has never been used\n     */\n    public LifecycleTransaction split(Collection<SSTableReader> readers)\n    {\n        logger.trace(\"Splitting {} into new transaction\", readers);\n        checkUnused();\n        for (SSTableReader reader : readers)\n            assert identities.contains(reader.instanceId) : \"may only split the same reader instance the transaction was opened with: \" + reader;\n\n        for (SSTableReader reader : readers)\n        {\n            identities.remove(reader.instanceId);\n            originals.remove(reader);\n            marked.remove(reader);\n        }\n        return new LifecycleTransaction(tracker, operationType, readers);\n    }"
        ],
        [
            "CassandraServer::get_multi_slice(MultiSliceRequest)",
            "1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004 -\n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  ",
            "    @Override\n    public List<ColumnOrSuperColumn> get_multi_slice(MultiSliceRequest request)\n            throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(request.key),\n                                                                  \"column_parent\", request.column_parent.toString(),\n                                                                  \"consistency_level\", request.consistency_level.name(),\n                                                                  \"count\", String.valueOf(request.count),\n                                                                  \"column_slices\", request.column_slices.toString());\n            Tracing.instance.begin(\"get_multi_slice\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"get_multi_slice\");\n        }\n        try \n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            state().hasColumnFamilyAccess(keyspace, request.getColumn_parent().column_family, Permission.SELECT);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, request.getColumn_parent().column_family);\n            if (metadata.cfType == ColumnFamilyType.Super)\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"get_multi_slice does not support super columns\");\n            ThriftValidation.validateColumnParent(metadata, request.getColumn_parent());\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(request.getConsistency_level());\n            consistencyLevel.validateForRead(keyspace);\n            List<ReadCommand> commands = new ArrayList<>(1);\n            ColumnSlice[] slices = new ColumnSlice[request.getColumn_slices().size()];\n            for (int i = 0 ; i < request.getColumn_slices().size() ; i++)\n            {\n                fixOptionalSliceParameters(request.getColumn_slices().get(i));\n                Composite start = metadata.comparator.fromByteBuffer(request.getColumn_slices().get(i).start);\n                Composite finish = metadata.comparator.fromByteBuffer(request.getColumn_slices().get(i).finish);\n                if (!start.isEmpty() && !finish.isEmpty())\n                {\n                    int compare = metadata.comparator.compare(start, finish);\n                    if (!request.reversed && compare > 0)\n                        throw new InvalidRequestException(String.format(\"Column slice at index %d had start greater than finish\", i));\n                    else if (request.reversed && compare < 0)\n                        throw new InvalidRequestException(String.format(\"Reversed column slice at index %d had start less than finish\", i));\n                }\n                slices[i] = new ColumnSlice(start, finish);\n            }\n            ColumnSlice[] deoverlapped = ColumnSlice.deoverlapSlices(slices, request.reversed ? metadata.comparator.reverseComparator() : metadata.comparator);\n            SliceQueryFilter filter = new SliceQueryFilter(deoverlapped, request.reversed, request.count);\n            ThriftValidation.validateKey(metadata, request.key);\n            commands.add(ReadCommand.create(keyspace, request.key, request.column_parent.getColumn_family(), System.currentTimeMillis(), filter));\n            return getSlice(commands, request.column_parent.isSetSuper_column(), consistencyLevel, cState).entrySet().iterator().next().getValue();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        } \n        finally \n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004 +\n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  ",
            "    @Override\n    public List<ColumnOrSuperColumn> get_multi_slice(MultiSliceRequest request)\n            throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(request.key),\n                                                                  \"column_parent\", request.column_parent.toString(),\n                                                                  \"consistency_level\", request.consistency_level.name(),\n                                                                  \"count\", String.valueOf(request.count),\n                                                                  \"column_slices\", request.column_slices.toString());\n            Tracing.instance.begin(\"get_multi_slice\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"get_multi_slice\");\n        }\n        try \n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            state().hasColumnFamilyAccess(keyspace, request.getColumn_parent().column_family, Permission.SELECT);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, request.getColumn_parent().column_family);\n            if (metadata.cfType == ColumnFamilyType.Super)\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"get_multi_slice does not support super columns\");\n            ThriftValidation.validateColumnParent(metadata, request.getColumn_parent());\n            org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(request.getConsistency_level());\n            consistencyLevel.validateForRead(keyspace);\n            List<ReadCommand> commands = new ArrayList<>(1);\n            ColumnSlice[] slices = new ColumnSlice[request.getColumn_slices().size()];\n            for (int i = 0 ; i < request.getColumn_slices().size() ; i++)\n            {\n                fixOptionalSliceParameters(request.getColumn_slices().get(i));\n                Composite start = metadata.comparator.fromByteBuffer(request.getColumn_slices().get(i).start);\n                Composite finish = metadata.comparator.fromByteBuffer(request.getColumn_slices().get(i).finish);\n                if (!start.isEmpty() && !finish.isEmpty())\n                {\n                    int compare = metadata.comparator.compare(start, finish);\n                    if (!request.reversed && compare > 0)\n                        throw new InvalidRequestException(String.format(\"Column slice at index %d had start greater than finish\", i));\n                    else if (request.reversed && compare < 0)\n                        throw new InvalidRequestException(String.format(\"Reversed column slice at index %d had start less than finish\", i));\n                }\n                slices[i] = new ColumnSlice(start, finish);\n            }\n            ColumnSlice[] deoverlapped = ColumnSlice.deoverlapSlices(slices, request.reversed ? metadata.comparator.reverseComparator() : metadata.comparator);\n            SliceQueryFilter filter = new SliceQueryFilter(deoverlapped, request.reversed, request.count);\n            ThriftValidation.validateKey(metadata, request.key);\n            commands.add(ReadCommand.create(keyspace, request.key, request.column_parent.getColumn_family(), System.currentTimeMillis(), filter));\n            return getSlice(commands, request.column_parent.isSetSuper_column(), consistencyLevel, cState).entrySet().iterator().next().getValue();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        } \n        finally \n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "CassandraServer::system_update_keyspace(KsDef)",
            "1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608 -\n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  ",
            "    /** update an existing keyspace, but do not allow column family modifications.\n     * @throws SchemaDisagreementException\n     */\n    public String system_update_keyspace(KsDef ks_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"update_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(ks_def.name);\n            state().hasKeyspaceAccess(ks_def.name, Permission.ALTER);\n            ThriftValidation.validateKeyspace(ks_def.name);\n            if (ks_def.getCf_defs() != null && ks_def.getCf_defs().size() > 0)\n                throw new InvalidRequestException(\"Keyspace update must not contain any table definitions.\");\n\n            MigrationManager.announceKeyspaceUpdate(ThriftConversion.fromThrift(ks_def));\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608 +\n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  ",
            "    /** update an existing keyspace, but do not allow column family modifications.\n     * @throws SchemaDisagreementException\n     */\n    public String system_update_keyspace(KsDef ks_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"update_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(ks_def.name);\n            state().hasKeyspaceAccess(ks_def.name, Permission.ALTER);\n            ThriftValidation.validateKeyspace(ks_def.name);\n            if (ks_def.getCf_defs() != null && ks_def.getCf_defs().size() > 0)\n                throw new InvalidRequestException(\"Keyspace update must not contain any table definitions.\");\n\n            MigrationManager.announceKeyspaceUpdate(ThriftConversion.fromThrift(ks_def));\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "CassandraServer::system_drop_keyspace(String)",
            "1583  \n1584  \n1585  \n1586 -\n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  ",
            "    public String system_drop_keyspace(String keyspace)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"drop_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(keyspace);\n            state().hasKeyspaceAccess(keyspace, Permission.DROP);\n\n            MigrationManager.announceKeyspaceDrop(keyspace);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1583  \n1584  \n1585  \n1586 +\n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  ",
            "    public String system_drop_keyspace(String keyspace)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"drop_keyspace\");\n\n        try\n        {\n            ThriftValidation.validateKeyspaceNotSystem(keyspace);\n            state().hasKeyspaceAccess(keyspace, Permission.DROP);\n\n            MigrationManager.announceKeyspaceDrop(keyspace);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "ReconnectableSnitchHelper::reconnect(InetAddress,InetAddress)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67  \n  68  ",
            "    private void reconnect(InetAddress publicAddress, InetAddress localAddress)\n    {\n        if (snitch.getDatacenter(publicAddress).equals(localDc)\n                && !MessagingService.instance().getConnectionPool(publicAddress).endPoint().equals(localAddress))\n        {\n            MessagingService.instance().getConnectionPool(publicAddress).reset(localAddress);\n            logger.debug(String.format(\"Intiated reconnect to an Internal IP %s for the %s\", localAddress, publicAddress));\n        }\n    }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67  \n  68  ",
            "    private void reconnect(InetAddress publicAddress, InetAddress localAddress)\n    {\n        if (snitch.getDatacenter(publicAddress).equals(localDc)\n                && !MessagingService.instance().getConnectionPool(publicAddress).endPoint().equals(localAddress))\n        {\n            MessagingService.instance().getConnectionPool(publicAddress).reset(localAddress);\n            logger.trace(String.format(\"Intiated reconnect to an Internal IP %s for the %s\", localAddress, publicAddress));\n        }\n    }"
        ],
        [
            "MessagingService::resetVersion(InetAddress)",
            " 848  \n 849  \n 850 -\n 851  \n 852  \n 853  \n 854  ",
            "    public void resetVersion(InetAddress endpoint)\n    {\n        logger.debug(\"Resetting version for {}\", endpoint);\n        Integer removed = versions.remove(endpoint);\n        if (removed != null && removed <= VERSION_22)\n            refreshAllNodesAtLeast22();\n    }",
            " 848  \n 849  \n 850 +\n 851  \n 852  \n 853  \n 854  ",
            "    public void resetVersion(InetAddress endpoint)\n    {\n        logger.trace(\"Resetting version for {}\", endpoint);\n        Integer removed = versions.remove(endpoint);\n        if (removed != null && removed <= VERSION_22)\n            refreshAllNodesAtLeast22();\n    }"
        ],
        [
            "CqlRecordReader::nextKeyValue()",
            " 193  \n 194  \n 195  \n 196  \n 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "    public boolean nextKeyValue() throws IOException\n    {\n        if (!rowIterator.hasNext())\n        {\n            logger.debug(\"Finished scanning {} rows (estimate was: {})\", rowIterator.totalRead, totalRowCount);\n            return false;\n        }\n\n        try\n        {\n            currentRow = rowIterator.next();\n        }\n        catch (Exception e)\n        {\n            // throw it as IOException, so client can catch it and handle it at client side\n            IOException ioe = new IOException(e.getMessage());\n            ioe.initCause(ioe.getCause());\n            throw ioe;\n        }\n        return true;\n    }",
            " 193  \n 194  \n 195  \n 196  \n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "    public boolean nextKeyValue() throws IOException\n    {\n        if (!rowIterator.hasNext())\n        {\n            logger.trace(\"Finished scanning {} rows (estimate was: {})\", rowIterator.totalRead, totalRowCount);\n            return false;\n        }\n\n        try\n        {\n            currentRow = rowIterator.next();\n        }\n        catch (Exception e)\n        {\n            // throw it as IOException, so client can catch it and handle it at client side\n            IOException ioe = new IOException(e.getMessage());\n            ioe.initCause(ioe.getCause());\n            throw ioe;\n        }\n        return true;\n    }"
        ],
        [
            "DebuggableThreadPoolExecutor::extractThrowable(Runnable)",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 -\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  ",
            "    /**\n     * @return any exception wrapped by @param runnable, i.e., if it is a FutureTask\n     */\n    public static Throwable extractThrowable(Runnable runnable)\n    {\n        // Check for exceptions wrapped by FutureTask.  We do this by calling get(), which will\n        // cause it to throw any saved exception.\n        //\n        // Complicating things, calling get() on a ScheduledFutureTask will block until the task\n        // is cancelled.  Hence, the extra isDone check beforehand.\n        if ((runnable instanceof Future<?>) && ((Future<?>) runnable).isDone())\n        {\n            try\n            {\n                ((Future<?>) runnable).get();\n            }\n            catch (InterruptedException e)\n            {\n                throw new AssertionError(e);\n            }\n            catch (CancellationException e)\n            {\n                logger.debug(\"Task cancelled\", e);\n            }\n            catch (ExecutionException e)\n            {\n                return e.getCause();\n            }\n        }\n\n        return null;\n    }",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 +\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  ",
            "    /**\n     * @return any exception wrapped by @param runnable, i.e., if it is a FutureTask\n     */\n    public static Throwable extractThrowable(Runnable runnable)\n    {\n        // Check for exceptions wrapped by FutureTask.  We do this by calling get(), which will\n        // cause it to throw any saved exception.\n        //\n        // Complicating things, calling get() on a ScheduledFutureTask will block until the task\n        // is cancelled.  Hence, the extra isDone check beforehand.\n        if ((runnable instanceof Future<?>) && ((Future<?>) runnable).isDone())\n        {\n            try\n            {\n                ((Future<?>) runnable).get();\n            }\n            catch (InterruptedException e)\n            {\n                throw new AssertionError(e);\n            }\n            catch (CancellationException e)\n            {\n                logger.trace(\"Task cancelled\", e);\n            }\n            catch (ExecutionException e)\n            {\n                return e.getCause();\n            }\n        }\n\n        return null;\n    }"
        ],
        [
            "MessagingService::setVersion(InetAddress,int)",
            " 829  \n 830  \n 831  \n 832  \n 833  \n 834 -\n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  ",
            "    /**\n     * @return the last version associated with address, or @param version if this is the first such version\n     */\n    public int setVersion(InetAddress endpoint, int version)\n    {\n        logger.debug(\"Setting version {} for {}\", version, endpoint);\n\n        if (version < VERSION_22)\n            allNodesAtLeast22 = false;\n\n        Integer v = versions.put(endpoint, version);\n\n        // if the version was increased to 2.2 or later, see if all nodes are >= 2.2 now\n        if (v != null && v < VERSION_22 && version >= VERSION_22)\n            refreshAllNodesAtLeast22();\n\n        return v == null ? version : v;\n    }",
            " 829  \n 830  \n 831  \n 832  \n 833  \n 834 +\n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  ",
            "    /**\n     * @return the last version associated with address, or @param version if this is the first such version\n     */\n    public int setVersion(InetAddress endpoint, int version)\n    {\n        logger.trace(\"Setting version {} for {}\", version, endpoint);\n\n        if (version < VERSION_22)\n            allNodesAtLeast22 = false;\n\n        Integer v = versions.put(endpoint, version);\n\n        // if the version was increased to 2.2 or later, see if all nodes are >= 2.2 now\n        if (v != null && v < VERSION_22 && version >= VERSION_22)\n            refreshAllNodesAtLeast22();\n\n        return v == null ? version : v;\n    }"
        ],
        [
            "StorageProxy::submitHint(Mutation,InetAddress,AbstractWriteResponseHandler)",
            " 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987 -\n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  ",
            "    public static Future<Void> submitHint(final Mutation mutation,\n                                          final InetAddress target,\n                                          final AbstractWriteResponseHandler<IMutation> responseHandler)\n    {\n        // local write that time out should be handled by LocalMutationRunnable\n        assert !target.equals(FBUtilities.getBroadcastAddress()) : target;\n\n        HintRunnable runnable = new HintRunnable(target)\n        {\n            public void runMayThrow()\n            {\n                int ttl = HintedHandOffManager.calculateHintTTL(mutation);\n                if (ttl > 0)\n                {\n                    logger.debug(\"Adding hint for {}\", target);\n                    writeHintForMutation(mutation, System.currentTimeMillis(), ttl, target);\n                    // Notify the handler only for CL == ANY\n                    if (responseHandler != null && responseHandler.consistencyLevel == ConsistencyLevel.ANY)\n                        responseHandler.response(null);\n                } else\n                {\n                    logger.debug(\"Skipped writing hint for {} (ttl {})\", target, ttl);\n                }\n            }\n        };\n\n        return submitHint(runnable);\n    }",
            " 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987 +\n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  ",
            "    public static Future<Void> submitHint(final Mutation mutation,\n                                          final InetAddress target,\n                                          final AbstractWriteResponseHandler<IMutation> responseHandler)\n    {\n        // local write that time out should be handled by LocalMutationRunnable\n        assert !target.equals(FBUtilities.getBroadcastAddress()) : target;\n\n        HintRunnable runnable = new HintRunnable(target)\n        {\n            public void runMayThrow()\n            {\n                int ttl = HintedHandOffManager.calculateHintTTL(mutation);\n                if (ttl > 0)\n                {\n                    logger.trace(\"Adding hint for {}\", target);\n                    writeHintForMutation(mutation, System.currentTimeMillis(), ttl, target);\n                    // Notify the handler only for CL == ANY\n                    if (responseHandler != null && responseHandler.consistencyLevel == ConsistencyLevel.ANY)\n                        responseHandler.response(null);\n                } else\n                {\n                    logger.debug(\"Skipped writing hint for {} (ttl {})\", target, ttl);\n                }\n            }\n        };\n\n        return submitHint(runnable);\n    }"
        ],
        [
            "CassandraAuthorizer::convertLegacyData()",
            " 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 -\n 443  \n 444  ",
            "    /**\n     * Copy legacy authz data from the system_auth.permissions table to the new system_auth.role_permissions table and\n     * also insert entries into the reverse lookup table.\n     * In theory, we could simply rename the existing table as the schema is structurally the same, but this would\n     * break mixed clusters during a rolling upgrade.\n     * This setup is not performed if AllowAllAuthenticator is configured (see Auth#setup).\n     */\n    private void convertLegacyData()\n    {\n        try\n        {\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"permissions\") != null)\n            {\n                logger.info(\"Converting legacy permissions data\");\n                CQLStatement insertStatement =\n                    QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (role, resource, permissions) \" +\n                                                              \"VALUES (?, ?, ?)\",\n                                                              AuthKeyspace.NAME,\n                                                              AuthKeyspace.ROLE_PERMISSIONS),\n                                                ClientState.forInternalCalls()).statement;\n                CQLStatement indexStatement =\n                    QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (resource, role) VALUES (?,?)\",\n                                                              AuthKeyspace.NAME,\n                                                              AuthKeyspace.RESOURCE_ROLE_INDEX),\n                                                ClientState.forInternalCalls()).statement;\n\n                UntypedResultSet permissions = process(\"SELECT * FROM system_auth.permissions\");\n                for (UntypedResultSet.Row row : permissions)\n                {\n                    final IResource resource = Resources.fromName(row.getString(\"resource\"));\n                    Predicate<String> isApplicable = new Predicate<String>()\n                    {\n                        public boolean apply(String s)\n                        {\n                            return resource.applicablePermissions().contains(Permission.valueOf(s));\n                        }\n                    };\n                    SetSerializer<String> serializer = SetSerializer.getInstance(UTF8Serializer.instance);\n                    Set<String> originalPerms = serializer.deserialize(row.getBytes(\"permissions\"));\n                    Set<String> filteredPerms = ImmutableSet.copyOf(Iterables.filter(originalPerms, isApplicable));\n                    insertStatement.execute(QueryState.forInternalCalls(),\n                                            QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                          Lists.newArrayList(row.getBytes(\"username\"),\n                                                                                             row.getBytes(\"resource\"),\n                                                                                             serializer.serialize(filteredPerms))));\n\n                    indexStatement.execute(QueryState.forInternalCalls(),\n                                           QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                         Lists.newArrayList(row.getBytes(\"resource\"),\n                                                                                            row.getBytes(\"username\"))));\n\n                }\n                logger.info(\"Completed conversion of legacy permissions\");\n            }\n        }\n        catch (Exception e)\n        {\n            logger.info(\"Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). \" +\n                        \"Conversion should not be considered complete\");\n            logger.debug(\"Conversion error\", e);\n        }\n    }",
            " 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443  \n 444  ",
            "    /**\n     * Copy legacy authz data from the system_auth.permissions table to the new system_auth.role_permissions table and\n     * also insert entries into the reverse lookup table.\n     * In theory, we could simply rename the existing table as the schema is structurally the same, but this would\n     * break mixed clusters during a rolling upgrade.\n     * This setup is not performed if AllowAllAuthenticator is configured (see Auth#setup).\n     */\n    private void convertLegacyData()\n    {\n        try\n        {\n            if (Schema.instance.getCFMetaData(\"system_auth\", \"permissions\") != null)\n            {\n                logger.info(\"Converting legacy permissions data\");\n                CQLStatement insertStatement =\n                    QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (role, resource, permissions) \" +\n                                                              \"VALUES (?, ?, ?)\",\n                                                              AuthKeyspace.NAME,\n                                                              AuthKeyspace.ROLE_PERMISSIONS),\n                                                ClientState.forInternalCalls()).statement;\n                CQLStatement indexStatement =\n                    QueryProcessor.getStatement(String.format(\"INSERT INTO %s.%s (resource, role) VALUES (?,?)\",\n                                                              AuthKeyspace.NAME,\n                                                              AuthKeyspace.RESOURCE_ROLE_INDEX),\n                                                ClientState.forInternalCalls()).statement;\n\n                UntypedResultSet permissions = process(\"SELECT * FROM system_auth.permissions\");\n                for (UntypedResultSet.Row row : permissions)\n                {\n                    final IResource resource = Resources.fromName(row.getString(\"resource\"));\n                    Predicate<String> isApplicable = new Predicate<String>()\n                    {\n                        public boolean apply(String s)\n                        {\n                            return resource.applicablePermissions().contains(Permission.valueOf(s));\n                        }\n                    };\n                    SetSerializer<String> serializer = SetSerializer.getInstance(UTF8Serializer.instance);\n                    Set<String> originalPerms = serializer.deserialize(row.getBytes(\"permissions\"));\n                    Set<String> filteredPerms = ImmutableSet.copyOf(Iterables.filter(originalPerms, isApplicable));\n                    insertStatement.execute(QueryState.forInternalCalls(),\n                                            QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                          Lists.newArrayList(row.getBytes(\"username\"),\n                                                                                             row.getBytes(\"resource\"),\n                                                                                             serializer.serialize(filteredPerms))));\n\n                    indexStatement.execute(QueryState.forInternalCalls(),\n                                           QueryOptions.forInternalCalls(ConsistencyLevel.ONE,\n                                                                         Lists.newArrayList(row.getBytes(\"resource\"),\n                                                                                            row.getBytes(\"username\"))));\n\n                }\n                logger.info(\"Completed conversion of legacy permissions\");\n            }\n        }\n        catch (Exception e)\n        {\n            logger.info(\"Unable to complete conversion of legacy permissions data (perhaps not enough nodes are upgraded yet). \" +\n                        \"Conversion should not be considered complete\");\n            logger.trace(\"Conversion error\", e);\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::logFlush()",
            " 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919 -\n 920  \n 921  ",
            "    private void logFlush()\n    {\n        // reclaiming includes that which we are GC-ing;\n        float onHeapRatio = 0, offHeapRatio = 0;\n        long onHeapTotal = 0, offHeapTotal = 0;\n        Memtable memtable = getTracker().getView().getCurrentMemtable();\n        onHeapRatio +=  memtable.getAllocator().onHeap().ownershipRatio();\n        offHeapRatio += memtable.getAllocator().offHeap().ownershipRatio();\n        onHeapTotal += memtable.getAllocator().onHeap().owns();\n        offHeapTotal += memtable.getAllocator().offHeap().owns();\n\n        for (SecondaryIndex index : indexManager.getIndexes())\n        {\n            if (index.getIndexCfs() != null)\n            {\n                MemtableAllocator allocator = index.getIndexCfs().getTracker().getView().getCurrentMemtable().getAllocator();\n                onHeapRatio += allocator.onHeap().ownershipRatio();\n                offHeapRatio += allocator.offHeap().ownershipRatio();\n                onHeapTotal += allocator.onHeap().owns();\n                offHeapTotal += allocator.offHeap().owns();\n            }\n        }\n\n        logger.info(\"Enqueuing flush of {}: {}\", name, String.format(\"%d (%.0f%%) on-heap, %d (%.0f%%) off-heap\",\n                                                                     onHeapTotal, onHeapRatio * 100, offHeapTotal, offHeapRatio * 100));\n    }",
            " 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919 +\n 920  \n 921  ",
            "    private void logFlush()\n    {\n        // reclaiming includes that which we are GC-ing;\n        float onHeapRatio = 0, offHeapRatio = 0;\n        long onHeapTotal = 0, offHeapTotal = 0;\n        Memtable memtable = getTracker().getView().getCurrentMemtable();\n        onHeapRatio +=  memtable.getAllocator().onHeap().ownershipRatio();\n        offHeapRatio += memtable.getAllocator().offHeap().ownershipRatio();\n        onHeapTotal += memtable.getAllocator().onHeap().owns();\n        offHeapTotal += memtable.getAllocator().offHeap().owns();\n\n        for (SecondaryIndex index : indexManager.getIndexes())\n        {\n            if (index.getIndexCfs() != null)\n            {\n                MemtableAllocator allocator = index.getIndexCfs().getTracker().getView().getCurrentMemtable().getAllocator();\n                onHeapRatio += allocator.onHeap().ownershipRatio();\n                offHeapRatio += allocator.offHeap().ownershipRatio();\n                onHeapTotal += allocator.onHeap().owns();\n                offHeapTotal += allocator.offHeap().owns();\n            }\n        }\n\n        logger.debug(\"Enqueuing flush of {}: {}\", name, String.format(\"%d (%.0f%%) on-heap, %d (%.0f%%) off-heap\",\n                                                                     onHeapTotal, onHeapRatio * 100, offHeapTotal, offHeapRatio * 100));\n    }"
        ],
        [
            "DebuggableScheduledThreadPoolExecutor::rejectedExecution(Runnable,ThreadPoolExecutor)",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "        public void rejectedExecution(Runnable task, ThreadPoolExecutor executor)\n        {\n            if (executor.isShutdown())\n            {\n                if (!StorageService.instance.isInShutdownHook())\n                    throw new RejectedExecutionException(\"ScheduledThreadPoolExecutor has shut down.\");\n\n                //Give some notification to the caller the task isn't going to run\n                if (task instanceof Future)\n                    ((Future) task).cancel(false);\n\n                logger.debug(\"ScheduledThreadPoolExecutor has shut down as part of C* shutdown\");\n            }\n            else\n            {\n                throw new AssertionError(\"Unknown rejection of ScheduledThreadPoolExecutor task\");\n            }\n        }",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "        public void rejectedExecution(Runnable task, ThreadPoolExecutor executor)\n        {\n            if (executor.isShutdown())\n            {\n                if (!StorageService.instance.isInShutdownHook())\n                    throw new RejectedExecutionException(\"ScheduledThreadPoolExecutor has shut down.\");\n\n                //Give some notification to the caller the task isn't going to run\n                if (task instanceof Future)\n                    ((Future) task).cancel(false);\n\n                logger.trace(\"ScheduledThreadPoolExecutor has shut down as part of C* shutdown\");\n            }\n            else\n            {\n                throw new AssertionError(\"Unknown rejection of ScheduledThreadPoolExecutor task\");\n            }\n        }"
        ],
        [
            "QueryProcessor::MigrationSubscriber::onDropKeyspace(String)",
            " 626  \n 627  \n 628 -\n 629  \n 630  ",
            "        public void onDropKeyspace(String ksName)\n        {\n            logger.debug(\"Keyspace {} was dropped, invalidating related prepared statements\", ksName);\n            removeInvalidPreparedStatements(ksName, null);\n        }",
            " 626  \n 627  \n 628 +\n 629  \n 630  ",
            "        public void onDropKeyspace(String ksName)\n        {\n            logger.trace(\"Keyspace {} was dropped, invalidating related prepared statements\", ksName);\n            removeInvalidPreparedStatements(ksName, null);\n        }"
        ],
        [
            "LifecycleTransaction::obsoleteOriginals()",
            " 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295  \n 296  \n 297  \n 298  \n 299  \n 300  ",
            "    /**\n     * obsolete every file in the original transaction\n     */\n    public void obsoleteOriginals()\n    {\n        logger.debug(\"Staging for obsolescence {}\", originals);\n        // if we're obsoleting, we should have no staged updates for the original files\n        assert Iterables.isEmpty(filterIn(staged.update, originals)) : staged.update;\n\n        // stage obsoletes for any currently visible versions of any original readers\n        Iterables.addAll(staged.obsolete, filterIn(current(), originals));\n    }",
            " 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295  \n 296  \n 297  \n 298  \n 299  \n 300  ",
            "    /**\n     * obsolete every file in the original transaction\n     */\n    public void obsoleteOriginals()\n    {\n        logger.trace(\"Staging for obsolescence {}\", originals);\n        // if we're obsoleting, we should have no staged updates for the original files\n        assert Iterables.isEmpty(filterIn(staged.update, originals)) : staged.update;\n\n        // stage obsoletes for any currently visible versions of any original readers\n        Iterables.addAll(staged.obsolete, filterIn(current(), originals));\n    }"
        ],
        [
            "Message::Dispatcher::channelRead0(ChannelHandlerContext,Request)",
            " 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 -\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 -\n 526  \n 527  ",
            "        @Override\n        public void channelRead0(ChannelHandlerContext ctx, Request request)\n        {\n\n            final Response response;\n            final ServerConnection connection;\n\n            try\n            {\n                assert request.connection() instanceof ServerConnection;\n                connection = (ServerConnection)request.connection();\n                if (connection.getVersion() >= Server.VERSION_4)\n                    ClientWarn.captureWarnings();\n\n                QueryState qstate = connection.validateNewMessage(request.type, connection.getVersion(), request.getStreamId());\n\n                logger.debug(\"Received: {}, v={}\", request, connection.getVersion());\n                response = request.execute(qstate);\n                response.setStreamId(request.getStreamId());\n                response.setWarnings(ClientWarn.getWarnings());\n                response.attach(connection);\n                connection.applyStateTransition(request.type, response.type);\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                UnexpectedChannelExceptionHandler handler = new UnexpectedChannelExceptionHandler(ctx.channel(), true);\n                flush(new FlushItem(ctx, ErrorMessage.fromException(t, handler).setStreamId(request.getStreamId()), request.getSourceFrame()));\n                return;\n            }\n            finally\n            {\n                ClientWarn.resetWarnings();\n            }\n\n            logger.debug(\"Responding: {}, v={}\", response, connection.getVersion());\n            flush(new FlushItem(ctx, response, request.getSourceFrame()));\n        }",
            " 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 +\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 +\n 526  \n 527  ",
            "        @Override\n        public void channelRead0(ChannelHandlerContext ctx, Request request)\n        {\n\n            final Response response;\n            final ServerConnection connection;\n\n            try\n            {\n                assert request.connection() instanceof ServerConnection;\n                connection = (ServerConnection)request.connection();\n                if (connection.getVersion() >= Server.VERSION_4)\n                    ClientWarn.captureWarnings();\n\n                QueryState qstate = connection.validateNewMessage(request.type, connection.getVersion(), request.getStreamId());\n\n                logger.trace(\"Received: {}, v={}\", request, connection.getVersion());\n                response = request.execute(qstate);\n                response.setStreamId(request.getStreamId());\n                response.setWarnings(ClientWarn.getWarnings());\n                response.attach(connection);\n                connection.applyStateTransition(request.type, response.type);\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                UnexpectedChannelExceptionHandler handler = new UnexpectedChannelExceptionHandler(ctx.channel(), true);\n                flush(new FlushItem(ctx, ErrorMessage.fromException(t, handler).setStreamId(request.getStreamId()), request.getSourceFrame()));\n                return;\n            }\n            finally\n            {\n                ClientWarn.resetWarnings();\n            }\n\n            logger.trace(\"Responding: {}, v={}\", response, connection.getVersion());\n            flush(new FlushItem(ctx, response, request.getSourceFrame()));\n        }"
        ],
        [
            "CreateIndexStatement::announceMigration(boolean)",
            " 163  \n 164  \n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "    public boolean announceMigration(boolean isLocalOnly) throws RequestValidationException\n    {\n        CFMetaData cfm = Schema.instance.getCFMetaData(keyspace(), columnFamily()).copy();\n        IndexTarget target = rawTarget.prepare(cfm);\n        logger.debug(\"Updating column {} definition for index {}\", target.column, indexName);\n        ColumnDefinition cd = cfm.getColumnDefinition(target.column);\n\n        if (cd.getIndexType() != null && ifNotExists)\n            return false;\n\n        if (properties.isCustom)\n        {\n            cd.setIndexType(IndexType.CUSTOM, properties.getOptions());\n        }\n        else if (cfm.comparator.isCompound())\n        {\n            Map<String, String> options = Collections.emptyMap();\n            // For now, we only allow indexing values for collections, but we could later allow\n            // to also index map keys, so we record that this is the values we index to make our\n            // lives easier then.\n            if (cd.type.isCollection() && cd.type.isMultiCell())\n                options = ImmutableMap.of(target.type.indexOption(), \"\");\n            cd.setIndexType(IndexType.COMPOSITES, options);\n        }\n        else\n        {\n            cd.setIndexType(IndexType.KEYS, Collections.<String, String>emptyMap());\n        }\n\n        cd.setIndexName(indexName);\n        cfm.addDefaultIndexNames();\n        MigrationManager.announceColumnFamilyUpdate(cfm, false, isLocalOnly);\n        return true;\n    }",
            " 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "    public boolean announceMigration(boolean isLocalOnly) throws RequestValidationException\n    {\n        CFMetaData cfm = Schema.instance.getCFMetaData(keyspace(), columnFamily()).copy();\n        IndexTarget target = rawTarget.prepare(cfm);\n        logger.trace(\"Updating column {} definition for index {}\", target.column, indexName);\n        ColumnDefinition cd = cfm.getColumnDefinition(target.column);\n\n        if (cd.getIndexType() != null && ifNotExists)\n            return false;\n\n        if (properties.isCustom)\n        {\n            cd.setIndexType(IndexType.CUSTOM, properties.getOptions());\n        }\n        else if (cfm.comparator.isCompound())\n        {\n            Map<String, String> options = Collections.emptyMap();\n            // For now, we only allow indexing values for collections, but we could later allow\n            // to also index map keys, so we record that this is the values we index to make our\n            // lives easier then.\n            if (cd.type.isCollection() && cd.type.isMultiCell())\n                options = ImmutableMap.of(target.type.indexOption(), \"\");\n            cd.setIndexType(IndexType.COMPOSITES, options);\n        }\n        else\n        {\n            cd.setIndexType(IndexType.KEYS, Collections.<String, String>emptyMap());\n        }\n\n        cd.setIndexName(indexName);\n        cfm.addDefaultIndexNames();\n        MigrationManager.announceColumnFamilyUpdate(cfm, false, isLocalOnly);\n        return true;\n    }"
        ],
        [
            "Directories::clearSnapshot(String,List)",
            " 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731 -\n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  ",
            "    public static void clearSnapshot(String snapshotName, List<File> snapshotDirectories)\n    {\n        // If snapshotName is empty or null, we will delete the entire snapshot directory\n        String tag = snapshotName == null ? \"\" : snapshotName;\n        for (File dir : snapshotDirectories)\n        {\n            File snapshotDir = new File(dir, join(SNAPSHOT_SUBDIR, tag));\n            if (snapshotDir.exists())\n            {\n                logger.debug(\"Removing snapshot directory {}\", snapshotDir);\n                try\n                {\n                    FileUtils.deleteRecursive(snapshotDir);\n                }\n                catch (FSWriteError e)\n                {\n                    if (FBUtilities.isWindows())\n                        SnapshotDeletingTask.addFailedSnapshot(snapshotDir);\n                    else\n                        throw e;\n                }\n            }\n        }\n    }",
            " 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731 +\n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  ",
            "    public static void clearSnapshot(String snapshotName, List<File> snapshotDirectories)\n    {\n        // If snapshotName is empty or null, we will delete the entire snapshot directory\n        String tag = snapshotName == null ? \"\" : snapshotName;\n        for (File dir : snapshotDirectories)\n        {\n            File snapshotDir = new File(dir, join(SNAPSHOT_SUBDIR, tag));\n            if (snapshotDir.exists())\n            {\n                logger.trace(\"Removing snapshot directory {}\", snapshotDir);\n                try\n                {\n                    FileUtils.deleteRecursive(snapshotDir);\n                }\n                catch (FSWriteError e)\n                {\n                    if (FBUtilities.isWindows())\n                        SnapshotDeletingTask.addFailedSnapshot(snapshotDir);\n                    else\n                        throw e;\n                }\n            }\n        }\n    }"
        ],
        [
            "Directories::Directories(CFMetaData)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  ",
            "    /**\n     * Create Directories of given ColumnFamily.\n     * SSTable directories are created under data_directories defined in cassandra.yaml if not exist at this time.\n     *\n     * @param metadata metadata of ColumnFamily\n     */\n    public Directories(final CFMetaData metadata)\n    {\n        this.metadata = metadata;\n\n        String cfId = ByteBufferUtil.bytesToHex(ByteBufferUtil.bytes(metadata.cfId));\n        int idx = metadata.cfName.indexOf(SECONDARY_INDEX_NAME_SEPARATOR);\n        String cfName = idx >= 0 ? metadata.cfName.substring(0, idx) : metadata.cfName;\n        String indexNameWithDot = idx >= 0 ? metadata.cfName.substring(idx) : null;\n\n        this.dataPaths = new File[dataDirectories.length];\n        // If upgraded from version less than 2.1, use existing directories\n        String oldSSTableRelativePath = join(metadata.ksName, cfName);\n        for (int i = 0; i < dataDirectories.length; ++i)\n        {\n            // check if old SSTable directory exists\n            dataPaths[i] = new File(dataDirectories[i].location, oldSSTableRelativePath);\n        }\n        boolean olderDirectoryExists = Iterables.any(Arrays.asList(dataPaths), new Predicate<File>()\n        {\n            public boolean apply(File file)\n            {\n                return file.exists();\n            }\n        });\n        if (!olderDirectoryExists)\n        {\n            // use 2.1+ style\n            String newSSTableRelativePath = join(metadata.ksName, cfName + '-' + cfId);\n            for (int i = 0; i < dataDirectories.length; ++i)\n                dataPaths[i] = new File(dataDirectories[i].location, newSSTableRelativePath);\n        }\n        // if index, then move to its own directory\n        if (indexNameWithDot != null)\n        {\n            for (int i = 0; i < dataDirectories.length; ++i)\n                dataPaths[i] = new File(dataPaths[i], indexNameWithDot);\n        }\n\n        for (File dir : dataPaths)\n        {\n            try\n            {\n                FileUtils.createDirectory(dir);\n            }\n            catch (FSError e)\n            {\n                // don't just let the default exception handler do this, we need the create loop to continue\n                logger.error(\"Failed to create {} directory\", dir);\n                FileUtils.handleFSError(e);\n            }\n        }\n\n        // if index, move existing older versioned SSTable files to new directory\n        if (indexNameWithDot != null)\n        {\n            for (File dataPath : dataPaths)\n            {\n                File[] indexFiles = dataPath.getParentFile().listFiles(new FileFilter()\n                {\n                    @Override\n                    public boolean accept(File file)\n                    {\n                        if (file.isDirectory())\n                            return false;\n\n                        Pair<Descriptor, Component> pair = SSTable.tryComponentFromFilename(file.getParentFile(),\n                                                                                            file.getName());\n                        return pair != null && pair.left.ksname.equals(metadata.ksName) && pair.left.cfname.equals(metadata.cfName);\n\n                    }\n                });\n                for (File indexFile : indexFiles)\n                {\n                    File destFile = new File(dataPath, indexFile.getName());\n                    logger.debug(\"Moving index file {} to {}\", indexFile, destFile);\n                    FileUtils.renameWithConfirm(indexFile, destFile);\n                }\n            }\n        }\n    }",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 +\n 262  \n 263  \n 264  \n 265  \n 266  ",
            "    /**\n     * Create Directories of given ColumnFamily.\n     * SSTable directories are created under data_directories defined in cassandra.yaml if not exist at this time.\n     *\n     * @param metadata metadata of ColumnFamily\n     */\n    public Directories(final CFMetaData metadata)\n    {\n        this.metadata = metadata;\n\n        String cfId = ByteBufferUtil.bytesToHex(ByteBufferUtil.bytes(metadata.cfId));\n        int idx = metadata.cfName.indexOf(SECONDARY_INDEX_NAME_SEPARATOR);\n        String cfName = idx >= 0 ? metadata.cfName.substring(0, idx) : metadata.cfName;\n        String indexNameWithDot = idx >= 0 ? metadata.cfName.substring(idx) : null;\n\n        this.dataPaths = new File[dataDirectories.length];\n        // If upgraded from version less than 2.1, use existing directories\n        String oldSSTableRelativePath = join(metadata.ksName, cfName);\n        for (int i = 0; i < dataDirectories.length; ++i)\n        {\n            // check if old SSTable directory exists\n            dataPaths[i] = new File(dataDirectories[i].location, oldSSTableRelativePath);\n        }\n        boolean olderDirectoryExists = Iterables.any(Arrays.asList(dataPaths), new Predicate<File>()\n        {\n            public boolean apply(File file)\n            {\n                return file.exists();\n            }\n        });\n        if (!olderDirectoryExists)\n        {\n            // use 2.1+ style\n            String newSSTableRelativePath = join(metadata.ksName, cfName + '-' + cfId);\n            for (int i = 0; i < dataDirectories.length; ++i)\n                dataPaths[i] = new File(dataDirectories[i].location, newSSTableRelativePath);\n        }\n        // if index, then move to its own directory\n        if (indexNameWithDot != null)\n        {\n            for (int i = 0; i < dataDirectories.length; ++i)\n                dataPaths[i] = new File(dataPaths[i], indexNameWithDot);\n        }\n\n        for (File dir : dataPaths)\n        {\n            try\n            {\n                FileUtils.createDirectory(dir);\n            }\n            catch (FSError e)\n            {\n                // don't just let the default exception handler do this, we need the create loop to continue\n                logger.error(\"Failed to create {} directory\", dir);\n                FileUtils.handleFSError(e);\n            }\n        }\n\n        // if index, move existing older versioned SSTable files to new directory\n        if (indexNameWithDot != null)\n        {\n            for (File dataPath : dataPaths)\n            {\n                File[] indexFiles = dataPath.getParentFile().listFiles(new FileFilter()\n                {\n                    @Override\n                    public boolean accept(File file)\n                    {\n                        if (file.isDirectory())\n                            return false;\n\n                        Pair<Descriptor, Component> pair = SSTable.tryComponentFromFilename(file.getParentFile(),\n                                                                                            file.getName());\n                        return pair != null && pair.left.ksname.equals(metadata.ksName) && pair.left.cfname.equals(metadata.cfName);\n\n                    }\n                });\n                for (File indexFile : indexFiles)\n                {\n                    File destFile = new File(dataPath, indexFile.getName());\n                    logger.trace(\"Moving index file {} to {}\", indexFile, destFile);\n                    FileUtils.renameWithConfirm(indexFile, destFile);\n                }\n            }\n        }\n    }"
        ],
        [
            "SSTableReader::markObsolete(Tracker)",
            "1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636 -\n1637 -\n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  ",
            "    /**\n     * Mark the sstable as obsolete, i.e., compacted into newer sstables.\n     *\n     * When calling this function, the caller must ensure that the SSTableReader is not referenced anywhere\n     * except for threads holding a reference.\n     *\n     * @return true if the this is the first time the file was marked obsolete.  Calling this\n     * multiple times is usually buggy (see exceptions in Tracker.unmarkCompacting and removeOldSSTablesSize).\n     */\n    public boolean markObsolete(Tracker tracker)\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"Marking {} compacted\", getFilename());\n\n        synchronized (tidy.global)\n        {\n            assert !tidy.isReplaced;\n        }\n        if (!tidy.global.isCompacted.getAndSet(true))\n        {\n            tidy.type.markObsolete(this, tracker);\n            return true;\n        }\n        return false;\n    }",
            "1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636 +\n1637 +\n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  ",
            "    /**\n     * Mark the sstable as obsolete, i.e., compacted into newer sstables.\n     *\n     * When calling this function, the caller must ensure that the SSTableReader is not referenced anywhere\n     * except for threads holding a reference.\n     *\n     * @return true if the this is the first time the file was marked obsolete.  Calling this\n     * multiple times is usually buggy (see exceptions in Tracker.unmarkCompacting and removeOldSSTablesSize).\n     */\n    public boolean markObsolete(Tracker tracker)\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Marking {} compacted\", getFilename());\n\n        synchronized (tidy.global)\n        {\n            assert !tidy.isReplaced;\n        }\n        if (!tidy.global.isCompacted.getAndSet(true))\n        {\n            tidy.type.markObsolete(this, tracker);\n            return true;\n        }\n        return false;\n    }"
        ],
        [
            "ResponseVerbHandler::doVerb(MessageIn,int)",
            "  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38 -\n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        long latency = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - MessagingService.instance().getRegisteredCallbackAge(id));\n        CallbackInfo callbackInfo = MessagingService.instance().removeRegisteredCallback(id);\n        if (callbackInfo == null)\n        {\n            String msg = \"Callback already removed for {} (from {})\";\n            logger.debug(msg, id, message.from);\n            Tracing.trace(msg, id, message.from);\n            return;\n        }\n\n        Tracing.trace(\"Processing response from {}\", message.from);\n        IAsyncCallback cb = callbackInfo.callback;\n        if (message.isFailureResponse())\n        {\n            ((IAsyncCallbackWithFailure) cb).onFailure(message.from);\n        }\n        else\n        {\n            //TODO: Should we add latency only in success cases?\n            MessagingService.instance().maybeAddLatency(cb, message.from, latency);\n            cb.response(message);\n        }\n    }",
            "  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38 +\n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        long latency = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - MessagingService.instance().getRegisteredCallbackAge(id));\n        CallbackInfo callbackInfo = MessagingService.instance().removeRegisteredCallback(id);\n        if (callbackInfo == null)\n        {\n            String msg = \"Callback already removed for {} (from {})\";\n            logger.trace(msg, id, message.from);\n            Tracing.trace(msg, id, message.from);\n            return;\n        }\n\n        Tracing.trace(\"Processing response from {}\", message.from);\n        IAsyncCallback cb = callbackInfo.callback;\n        if (message.isFailureResponse())\n        {\n            ((IAsyncCallbackWithFailure) cb).onFailure(message.from);\n        }\n        else\n        {\n            //TODO: Should we add latency only in success cases?\n            MessagingService.instance().maybeAddLatency(cb, message.from, latency);\n            cb.response(message);\n        }\n    }"
        ],
        [
            "LogOutput::debug(String)",
            "  45  \n  46  \n  47 -\n  48  ",
            "        public void debug(String msg)\n        {\n            logger.debug(msg);\n        }",
            "  45  \n  46  \n  47 +\n  48  ",
            "        public void debug(String msg)\n        {\n            logger.trace(msg);\n        }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::onDown(Host)",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161  \n 162  ",
            "    @Override\n    public void onDown(Host host)\n    {\n        if (liveReplicaHosts.remove(host))\n        {\n            logger.debug(\"The host {} is now down\", host);\n        }\n    }",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  ",
            "    @Override\n    public void onDown(Host host)\n    {\n        if (liveReplicaHosts.remove(host))\n        {\n            logger.trace(\"The host {} is now down\", host);\n        }\n    }"
        ],
        [
            "StorageProxy::fetchRows(List,ConsistencyLevel)",
            "1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393 -\n1394 -\n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472 -\n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491 -\n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  ",
            "    /**\n     * This function executes local and remote reads, and blocks for the results:\n     *\n     * 1. Get the replica locations, sorted by response time according to the snitch\n     * 2. Send a data request to the closest replica, and digest requests to either\n     *    a) all the replicas, if read repair is enabled\n     *    b) the closest R-1 replicas, where R is the number required to satisfy the ConsistencyLevel\n     * 3. Wait for a response from R replicas\n     * 4. If the digests (if any) match the data return the data\n     * 5. else carry out read repair by getting data from all the nodes.\n     */\n    private static List<Row> fetchRows(List<ReadCommand> initialCommands, ConsistencyLevel consistencyLevel)\n    throws UnavailableException, ReadFailureException, ReadTimeoutException\n    {\n        List<Row> rows = new ArrayList<>(initialCommands.size());\n        // (avoid allocating a new list in the common case of nothing-to-retry)\n        List<ReadCommand> commandsToRetry = Collections.emptyList();\n\n        do\n        {\n            List<ReadCommand> commands = commandsToRetry.isEmpty() ? initialCommands : commandsToRetry;\n            AbstractReadExecutor[] readExecutors = new AbstractReadExecutor[commands.size()];\n\n            if (!commandsToRetry.isEmpty())\n                Tracing.trace(\"Retrying {} commands\", commandsToRetry.size());\n\n            // send out read requests\n            for (int i = 0; i < commands.size(); i++)\n            {\n                ReadCommand command = commands.get(i);\n                assert !command.isDigestQuery();\n\n                AbstractReadExecutor exec = AbstractReadExecutor.getReadExecutor(command, consistencyLevel);\n                exec.executeAsync();\n                readExecutors[i] = exec;\n            }\n\n            for (AbstractReadExecutor exec : readExecutors)\n                exec.maybeTryAdditionalReplicas();\n\n            // read results and make a second pass for any digest mismatches\n            List<ReadCommand> repairCommands = null;\n            List<ReadCallback<ReadResponse, Row>> repairResponseHandlers = null;\n            for (AbstractReadExecutor exec: readExecutors)\n            {\n                try\n                {\n                    Row row = exec.get();\n                    if (row != null)\n                    {\n                        row = exec.command.maybeTrim(row);\n                        rows.add(row);\n                    }\n\n                    if (logger.isDebugEnabled())\n                        logger.debug(\"Read: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - exec.handler.start));\n                }\n                catch (ReadTimeoutException|ReadFailureException ex)\n                {\n                    int blockFor = consistencyLevel.blockFor(Keyspace.open(exec.command.getKeyspace()));\n                    int responseCount = exec.handler.getReceivedCount();\n                    String gotData = responseCount > 0\n                                   ? exec.resolver.isDataPresent() ? \" (including data)\" : \" (only digests)\"\n                                   : \"\";\n\n                    boolean isTimeout = ex instanceof ReadTimeoutException;\n                    if (Tracing.isTracing())\n                    {\n                        Tracing.trace(\"{}; received {} of {} responses{}\",\n                                      isTimeout ? \"Timed out\" : \"Failed\", responseCount, blockFor, gotData);\n                    }\n                    else if (logger.isDebugEnabled())\n                    {\n                        logger.debug(\"Read {}; received {} of {} responses{}\", (isTimeout ? \"timeout\" : \"failure\"), responseCount, blockFor, gotData);\n                    }\n                    throw ex;\n                }\n                catch (DigestMismatchException ex)\n                {\n                    Tracing.trace(\"Digest mismatch: {}\", ex);\n\n                    ReadRepairMetrics.repairedBlocking.mark();\n\n                    // Do a full data read to resolve the correct response (and repair node that need be)\n                    RowDataResolver resolver = new RowDataResolver(exec.command.ksName, exec.command.key, exec.command.filter(), exec.command.timestamp, exec.handler.endpoints.size());\n                    ReadCallback<ReadResponse, Row> repairHandler = new ReadCallback<>(resolver,\n                                                                                       ConsistencyLevel.ALL,\n                                                                                       exec.getContactedReplicas().size(),\n                                                                                       exec.command,\n                                                                                       Keyspace.open(exec.command.getKeyspace()),\n                                                                                       exec.handler.endpoints);\n\n                    if (repairCommands == null)\n                    {\n                        repairCommands = new ArrayList<>();\n                        repairResponseHandlers = new ArrayList<>();\n                    }\n                    repairCommands.add(exec.command);\n                    repairResponseHandlers.add(repairHandler);\n\n                    MessageOut<ReadCommand> message = exec.command.createMessage();\n                    for (InetAddress endpoint : exec.getContactedReplicas())\n                    {\n                        Tracing.trace(\"Enqueuing full data read to {}\", endpoint);\n                        MessagingService.instance().sendRRWithFailure(message, endpoint, repairHandler);\n                    }\n                }\n            }\n\n            commandsToRetry.clear();\n\n            // read the results for the digest mismatch retries\n            if (repairResponseHandlers != null)\n            {\n                for (int i = 0; i < repairCommands.size(); i++)\n                {\n                    ReadCommand command = repairCommands.get(i);\n                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);\n\n                    Row row;\n                    try\n                    {\n                        row = handler.get();\n                    }\n                    catch (DigestMismatchException e)\n                    {\n                        throw new AssertionError(e); // full data requested from each node here, no digests should be sent\n                    }\n                    catch (ReadTimeoutException e)\n                    {\n                        if (Tracing.isTracing())\n                            Tracing.trace(\"Timed out waiting on digest mismatch repair requests\");\n                        else\n                            logger.debug(\"Timed out waiting on digest mismatch repair requests\");\n                        // the caught exception here will have CL.ALL from the repair command,\n                        // not whatever CL the initial command was at (CASSANDRA-7947)\n                        int blockFor = consistencyLevel.blockFor(Keyspace.open(command.getKeyspace()));\n                        throw new ReadTimeoutException(consistencyLevel, blockFor-1, blockFor, true);\n                    }\n\n                    RowDataResolver resolver = (RowDataResolver)handler.resolver;\n                    try\n                    {\n                        // wait for the repair writes to be acknowledged, to minimize impact on any replica that's\n                        // behind on writes in case the out-of-sync row is read multiple times in quick succession\n                        FBUtilities.waitOnFutures(resolver.repairResults, DatabaseDescriptor.getWriteRpcTimeout());\n                    }\n                    catch (TimeoutException e)\n                    {\n                        if (Tracing.isTracing())\n                            Tracing.trace(\"Timed out waiting on digest mismatch repair acknowledgements\");\n                        else\n                            logger.debug(\"Timed out waiting on digest mismatch repair acknowledgements\");\n                        int blockFor = consistencyLevel.blockFor(Keyspace.open(command.getKeyspace()));\n                        throw new ReadTimeoutException(consistencyLevel, blockFor-1, blockFor, true);\n                    }\n\n                    // retry any potential short reads\n                    ReadCommand retryCommand = command.maybeGenerateRetryCommand(resolver, row);\n                    if (retryCommand != null)\n                    {\n                        Tracing.trace(\"Issuing retry for read command\");\n                        if (commandsToRetry == Collections.EMPTY_LIST)\n                            commandsToRetry = new ArrayList<>();\n                        commandsToRetry.add(retryCommand);\n                        continue;\n                    }\n\n                    if (row != null)\n                    {\n                        row = command.maybeTrim(row);\n                        rows.add(row);\n                    }\n                }\n            }\n        } while (!commandsToRetry.isEmpty());\n\n        return rows;\n    }",
            "1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393 +\n1394 +\n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472 +\n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491 +\n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  ",
            "    /**\n     * This function executes local and remote reads, and blocks for the results:\n     *\n     * 1. Get the replica locations, sorted by response time according to the snitch\n     * 2. Send a data request to the closest replica, and digest requests to either\n     *    a) all the replicas, if read repair is enabled\n     *    b) the closest R-1 replicas, where R is the number required to satisfy the ConsistencyLevel\n     * 3. Wait for a response from R replicas\n     * 4. If the digests (if any) match the data return the data\n     * 5. else carry out read repair by getting data from all the nodes.\n     */\n    private static List<Row> fetchRows(List<ReadCommand> initialCommands, ConsistencyLevel consistencyLevel)\n    throws UnavailableException, ReadFailureException, ReadTimeoutException\n    {\n        List<Row> rows = new ArrayList<>(initialCommands.size());\n        // (avoid allocating a new list in the common case of nothing-to-retry)\n        List<ReadCommand> commandsToRetry = Collections.emptyList();\n\n        do\n        {\n            List<ReadCommand> commands = commandsToRetry.isEmpty() ? initialCommands : commandsToRetry;\n            AbstractReadExecutor[] readExecutors = new AbstractReadExecutor[commands.size()];\n\n            if (!commandsToRetry.isEmpty())\n                Tracing.trace(\"Retrying {} commands\", commandsToRetry.size());\n\n            // send out read requests\n            for (int i = 0; i < commands.size(); i++)\n            {\n                ReadCommand command = commands.get(i);\n                assert !command.isDigestQuery();\n\n                AbstractReadExecutor exec = AbstractReadExecutor.getReadExecutor(command, consistencyLevel);\n                exec.executeAsync();\n                readExecutors[i] = exec;\n            }\n\n            for (AbstractReadExecutor exec : readExecutors)\n                exec.maybeTryAdditionalReplicas();\n\n            // read results and make a second pass for any digest mismatches\n            List<ReadCommand> repairCommands = null;\n            List<ReadCallback<ReadResponse, Row>> repairResponseHandlers = null;\n            for (AbstractReadExecutor exec: readExecutors)\n            {\n                try\n                {\n                    Row row = exec.get();\n                    if (row != null)\n                    {\n                        row = exec.command.maybeTrim(row);\n                        rows.add(row);\n                    }\n\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Read: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - exec.handler.start));\n                }\n                catch (ReadTimeoutException|ReadFailureException ex)\n                {\n                    int blockFor = consistencyLevel.blockFor(Keyspace.open(exec.command.getKeyspace()));\n                    int responseCount = exec.handler.getReceivedCount();\n                    String gotData = responseCount > 0\n                                   ? exec.resolver.isDataPresent() ? \" (including data)\" : \" (only digests)\"\n                                   : \"\";\n\n                    boolean isTimeout = ex instanceof ReadTimeoutException;\n                    if (Tracing.isTracing())\n                    {\n                        Tracing.trace(\"{}; received {} of {} responses{}\",\n                                      isTimeout ? \"Timed out\" : \"Failed\", responseCount, blockFor, gotData);\n                    }\n                    else if (logger.isDebugEnabled())\n                    {\n                        logger.debug(\"Read {}; received {} of {} responses{}\", (isTimeout ? \"timeout\" : \"failure\"), responseCount, blockFor, gotData);\n                    }\n                    throw ex;\n                }\n                catch (DigestMismatchException ex)\n                {\n                    Tracing.trace(\"Digest mismatch: {}\", ex);\n\n                    ReadRepairMetrics.repairedBlocking.mark();\n\n                    // Do a full data read to resolve the correct response (and repair node that need be)\n                    RowDataResolver resolver = new RowDataResolver(exec.command.ksName, exec.command.key, exec.command.filter(), exec.command.timestamp, exec.handler.endpoints.size());\n                    ReadCallback<ReadResponse, Row> repairHandler = new ReadCallback<>(resolver,\n                                                                                       ConsistencyLevel.ALL,\n                                                                                       exec.getContactedReplicas().size(),\n                                                                                       exec.command,\n                                                                                       Keyspace.open(exec.command.getKeyspace()),\n                                                                                       exec.handler.endpoints);\n\n                    if (repairCommands == null)\n                    {\n                        repairCommands = new ArrayList<>();\n                        repairResponseHandlers = new ArrayList<>();\n                    }\n                    repairCommands.add(exec.command);\n                    repairResponseHandlers.add(repairHandler);\n\n                    MessageOut<ReadCommand> message = exec.command.createMessage();\n                    for (InetAddress endpoint : exec.getContactedReplicas())\n                    {\n                        Tracing.trace(\"Enqueuing full data read to {}\", endpoint);\n                        MessagingService.instance().sendRRWithFailure(message, endpoint, repairHandler);\n                    }\n                }\n            }\n\n            commandsToRetry.clear();\n\n            // read the results for the digest mismatch retries\n            if (repairResponseHandlers != null)\n            {\n                for (int i = 0; i < repairCommands.size(); i++)\n                {\n                    ReadCommand command = repairCommands.get(i);\n                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);\n\n                    Row row;\n                    try\n                    {\n                        row = handler.get();\n                    }\n                    catch (DigestMismatchException e)\n                    {\n                        throw new AssertionError(e); // full data requested from each node here, no digests should be sent\n                    }\n                    catch (ReadTimeoutException e)\n                    {\n                        if (Tracing.isTracing())\n                            Tracing.trace(\"Timed out waiting on digest mismatch repair requests\");\n                        else\n                            logger.trace(\"Timed out waiting on digest mismatch repair requests\");\n                        // the caught exception here will have CL.ALL from the repair command,\n                        // not whatever CL the initial command was at (CASSANDRA-7947)\n                        int blockFor = consistencyLevel.blockFor(Keyspace.open(command.getKeyspace()));\n                        throw new ReadTimeoutException(consistencyLevel, blockFor-1, blockFor, true);\n                    }\n\n                    RowDataResolver resolver = (RowDataResolver)handler.resolver;\n                    try\n                    {\n                        // wait for the repair writes to be acknowledged, to minimize impact on any replica that's\n                        // behind on writes in case the out-of-sync row is read multiple times in quick succession\n                        FBUtilities.waitOnFutures(resolver.repairResults, DatabaseDescriptor.getWriteRpcTimeout());\n                    }\n                    catch (TimeoutException e)\n                    {\n                        if (Tracing.isTracing())\n                            Tracing.trace(\"Timed out waiting on digest mismatch repair acknowledgements\");\n                        else\n                            logger.trace(\"Timed out waiting on digest mismatch repair acknowledgements\");\n                        int blockFor = consistencyLevel.blockFor(Keyspace.open(command.getKeyspace()));\n                        throw new ReadTimeoutException(consistencyLevel, blockFor-1, blockFor, true);\n                    }\n\n                    // retry any potential short reads\n                    ReadCommand retryCommand = command.maybeGenerateRetryCommand(resolver, row);\n                    if (retryCommand != null)\n                    {\n                        Tracing.trace(\"Issuing retry for read command\");\n                        if (commandsToRetry == Collections.EMPTY_LIST)\n                            commandsToRetry = new ArrayList<>();\n                        commandsToRetry.add(retryCommand);\n                        continue;\n                    }\n\n                    if (row != null)\n                    {\n                        row = command.maybeTrim(row);\n                        rows.add(row);\n                    }\n                }\n            }\n        } while (!commandsToRetry.isEmpty());\n\n        return rows;\n    }"
        ],
        [
            "MetadataSerializer::mutateLevel(Descriptor,int)",
            " 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "    public void mutateLevel(Descriptor descriptor, int newLevel) throws IOException\n    {\n        logger.debug(\"Mutating {} to level {}\", descriptor.filenameFor(Component.STATS), newLevel);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateLevel(newLevel));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }",
            " 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "    public void mutateLevel(Descriptor descriptor, int newLevel) throws IOException\n    {\n        logger.trace(\"Mutating {} to level {}\", descriptor.filenameFor(Component.STATS), newLevel);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateLevel(newLevel));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }"
        ],
        [
            "CustomClassLoader::loadClassInternal(String)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "    public synchronized Class<?> loadClassInternal(String name) throws ClassNotFoundException\n    {\n        try\n        {\n            return parent.loadClass(name);\n        }\n        catch (ClassNotFoundException ex)\n        {\n            logger.debug(\"Class not found using parent class loader,\", ex);\n            // Don't throw the exception here, try triggers directory.\n        }\n        Class<?> clazz = this.findClass(name);\n        cache.put(name, clazz);\n        return clazz;\n    }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 +\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "    public synchronized Class<?> loadClassInternal(String name) throws ClassNotFoundException\n    {\n        try\n        {\n            return parent.loadClass(name);\n        }\n        catch (ClassNotFoundException ex)\n        {\n            logger.trace(\"Class not found using parent class loader,\", ex);\n            // Don't throw the exception here, try triggers directory.\n        }\n        Class<?> clazz = this.findClass(name);\n        cache.put(name, clazz);\n        return clazz;\n    }"
        ],
        [
            "ThriftValidation::validateColumnData(CFMetaData,ByteBuffer,ByteBuffer,Column)",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 -\n 455 -\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "    /**\n     * Validates the data part of the column (everything in the column object but the name, which is assumed to be valid)\n     */\n    public static void validateColumnData(CFMetaData metadata, ByteBuffer key, ByteBuffer scName, Column column) throws org.apache.cassandra.exceptions.InvalidRequestException\n    {\n        validateTtl(column);\n        if (!column.isSetValue())\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Column value is required\");\n        if (!column.isSetTimestamp())\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Column timestamp is required\");\n\n        CellName cn = scName == null\n                    ? metadata.comparator.cellFromByteBuffer(column.name)\n                    : metadata.comparator.makeCellName(scName, column.name);\n        try\n        {\n            AbstractType<?> validator = metadata.getValueValidator(cn);\n            if (validator != null)\n                validator.validate(column.value);\n        }\n        catch (MarshalException me)\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"rejecting invalid value {}\", ByteBufferUtil.bytesToHex(summarize(column.value)));\n\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"(%s) [%s][%s][%s] failed validation\",\n                                                                      me.getMessage(),\n                                                                      metadata.ksName,\n                                                                      metadata.cfName,\n                                                                      (SuperColumns.getComparatorFor(metadata, scName != null)).getString(column.name)));\n        }\n\n        // Indexed column values cannot be larger than 64K.  See CASSANDRA-3057/4240 for more details\n        SecondaryIndex failedIndex = Keyspace.open(metadata.ksName).getColumnFamilyStore(metadata.cfName).indexManager.validate(key, asDBColumn(cn, column));\n        if (failedIndex != null)\n                    throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"Can't index column value of size %d for index %s in CF %s of KS %s\",\n                                                                              column.value.remaining(),\n                                                                              failedIndex.getIndexName(),\n                                                                              metadata.cfName,\n                                                                              metadata.ksName));\n    }",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 +\n 455 +\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "    /**\n     * Validates the data part of the column (everything in the column object but the name, which is assumed to be valid)\n     */\n    public static void validateColumnData(CFMetaData metadata, ByteBuffer key, ByteBuffer scName, Column column) throws org.apache.cassandra.exceptions.InvalidRequestException\n    {\n        validateTtl(column);\n        if (!column.isSetValue())\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Column value is required\");\n        if (!column.isSetTimestamp())\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Column timestamp is required\");\n\n        CellName cn = scName == null\n                    ? metadata.comparator.cellFromByteBuffer(column.name)\n                    : metadata.comparator.makeCellName(scName, column.name);\n        try\n        {\n            AbstractType<?> validator = metadata.getValueValidator(cn);\n            if (validator != null)\n                validator.validate(column.value);\n        }\n        catch (MarshalException me)\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"rejecting invalid value {}\", ByteBufferUtil.bytesToHex(summarize(column.value)));\n\n            throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"(%s) [%s][%s][%s] failed validation\",\n                                                                      me.getMessage(),\n                                                                      metadata.ksName,\n                                                                      metadata.cfName,\n                                                                      (SuperColumns.getComparatorFor(metadata, scName != null)).getString(column.name)));\n        }\n\n        // Indexed column values cannot be larger than 64K.  See CASSANDRA-3057/4240 for more details\n        SecondaryIndex failedIndex = Keyspace.open(metadata.ksName).getColumnFamilyStore(metadata.cfName).indexManager.validate(key, asDBColumn(cn, column));\n        if (failedIndex != null)\n                    throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"Can't index column value of size %d for index %s in CF %s of KS %s\",\n                                                                              column.value.remaining(),\n                                                                              failedIndex.getIndexName(),\n                                                                              metadata.cfName,\n                                                                              metadata.ksName));\n    }"
        ],
        [
            "ColumnFamilyOutputFormat::createAuthenticatedClient(String,int,Configuration)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  ",
            "    /**\n     * Connects to the given server:port and returns a client based on the given socket that points to the configured\n     * keyspace, and is logged in with the configured credentials.\n     *\n     * @param host fully qualified host name to connect to\n     * @param port RPC port of the server\n     * @param conf a job configuration\n     * @return a cassandra client\n     * @throws Exception set of thrown exceptions may be implementation defined,\n     *                   depending on the used transport factory\n     */\n    @SuppressWarnings(\"resource\")\n    public static Cassandra.Client createAuthenticatedClient(String host, int port, Configuration conf) throws Exception\n    {\n        logger.debug(\"Creating authenticated client for CF output format\");\n        TTransport transport = ConfigHelper.getClientTransportFactory(conf).openTransport(host, port);\n        TProtocol binaryProtocol = new TBinaryProtocol(transport, true, true);\n        Cassandra.Client client = new Cassandra.Client(binaryProtocol);\n        client.set_keyspace(ConfigHelper.getOutputKeyspace(conf));\n        String user = ConfigHelper.getOutputKeyspaceUserName(conf);\n        String password = ConfigHelper.getOutputKeyspacePassword(conf);\n        if ((user != null) && (password != null))\n            login(user, password, client);\n\n        logger.debug(\"Authenticated client for CF output format created successfully\");\n        return client;\n    }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  ",
            "    /**\n     * Connects to the given server:port and returns a client based on the given socket that points to the configured\n     * keyspace, and is logged in with the configured credentials.\n     *\n     * @param host fully qualified host name to connect to\n     * @param port RPC port of the server\n     * @param conf a job configuration\n     * @return a cassandra client\n     * @throws Exception set of thrown exceptions may be implementation defined,\n     *                   depending on the used transport factory\n     */\n    @SuppressWarnings(\"resource\")\n    public static Cassandra.Client createAuthenticatedClient(String host, int port, Configuration conf) throws Exception\n    {\n        logger.trace(\"Creating authenticated client for CF output format\");\n        TTransport transport = ConfigHelper.getClientTransportFactory(conf).openTransport(host, port);\n        TProtocol binaryProtocol = new TBinaryProtocol(transport, true, true);\n        Cassandra.Client client = new Cassandra.Client(binaryProtocol);\n        client.set_keyspace(ConfigHelper.getOutputKeyspace(conf));\n        String user = ConfigHelper.getOutputKeyspaceUserName(conf);\n        String password = ConfigHelper.getOutputKeyspacePassword(conf);\n        if ((user != null) && (password != null))\n            login(user, password, client);\n\n        logger.trace(\"Authenticated client for CF output format created successfully\");\n        return client;\n    }"
        ],
        [
            "ColumnFamilyStore::getOverlappingSSTables(Iterable)",
            "1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346 -\n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  ",
            "    /**\n     * @param sstables\n     * @return sstables whose key range overlaps with that of the given sstables, not including itself.\n     * (The given sstables may or may not overlap with each other.)\n     */\n    public Collection<SSTableReader> getOverlappingSSTables(Iterable<SSTableReader> sstables)\n    {\n        logger.debug(\"Checking for sstables overlapping {}\", sstables);\n\n        // a normal compaction won't ever have an empty sstables list, but we create a skeleton\n        // compaction controller for streaming, and that passes an empty list.\n        if (!sstables.iterator().hasNext())\n            return ImmutableSet.of();\n\n\n\n        List<SSTableReader> sortedByFirst = Lists.newArrayList(sstables);\n        Collections.sort(sortedByFirst, new Comparator<SSTableReader>()\n        {\n            @Override\n            public int compare(SSTableReader o1, SSTableReader o2)\n            {\n                return o1.first.compareTo(o2.first);\n            }\n        });\n        List<Interval<RowPosition, SSTableReader>> intervals = new ArrayList<>();\n        DecoratedKey first = null, last = null;\n        /*\n        normalize the intervals covered by the sstables\n        assume we have sstables like this (brackets representing first/last key in the sstable);\n        [   ] [   ]    [   ]   [  ]\n           [   ]         [       ]\n        then we can, instead of searching the interval tree 6 times, normalize the intervals and\n        only query the tree 2 times, for these intervals;\n        [         ]    [          ]\n         */\n        for (SSTableReader sstable : sortedByFirst)\n        {\n            if (first == null)\n            {\n                first = sstable.first;\n                last = sstable.last;\n            }\n            else\n            {\n                if (sstable.first.compareTo(last) <= 0) // we do overlap\n                {\n                    if (sstable.last.compareTo(last) > 0)\n                        last = sstable.last;\n                }\n                else\n                {\n                    intervals.add(Interval.<RowPosition, SSTableReader>create(first, last));\n                    first = sstable.first;\n                    last = sstable.last;\n                }\n            }\n        }\n        intervals.add(Interval.<RowPosition, SSTableReader>create(first, last));\n        SSTableIntervalTree tree = data.getView().intervalTree;\n        Set<SSTableReader> results = new HashSet<>();\n\n        for (Interval<RowPosition, SSTableReader> interval : intervals)\n            results.addAll(tree.search(interval));\n\n        return Sets.difference(results, ImmutableSet.copyOf(sstables));\n    }",
            "1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346 +\n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  ",
            "    /**\n     * @param sstables\n     * @return sstables whose key range overlaps with that of the given sstables, not including itself.\n     * (The given sstables may or may not overlap with each other.)\n     */\n    public Collection<SSTableReader> getOverlappingSSTables(Iterable<SSTableReader> sstables)\n    {\n        logger.trace(\"Checking for sstables overlapping {}\", sstables);\n\n        // a normal compaction won't ever have an empty sstables list, but we create a skeleton\n        // compaction controller for streaming, and that passes an empty list.\n        if (!sstables.iterator().hasNext())\n            return ImmutableSet.of();\n\n\n\n        List<SSTableReader> sortedByFirst = Lists.newArrayList(sstables);\n        Collections.sort(sortedByFirst, new Comparator<SSTableReader>()\n        {\n            @Override\n            public int compare(SSTableReader o1, SSTableReader o2)\n            {\n                return o1.first.compareTo(o2.first);\n            }\n        });\n        List<Interval<RowPosition, SSTableReader>> intervals = new ArrayList<>();\n        DecoratedKey first = null, last = null;\n        /*\n        normalize the intervals covered by the sstables\n        assume we have sstables like this (brackets representing first/last key in the sstable);\n        [   ] [   ]    [   ]   [  ]\n           [   ]         [       ]\n        then we can, instead of searching the interval tree 6 times, normalize the intervals and\n        only query the tree 2 times, for these intervals;\n        [         ]    [          ]\n         */\n        for (SSTableReader sstable : sortedByFirst)\n        {\n            if (first == null)\n            {\n                first = sstable.first;\n                last = sstable.last;\n            }\n            else\n            {\n                if (sstable.first.compareTo(last) <= 0) // we do overlap\n                {\n                    if (sstable.last.compareTo(last) > 0)\n                        last = sstable.last;\n                }\n                else\n                {\n                    intervals.add(Interval.<RowPosition, SSTableReader>create(first, last));\n                    first = sstable.first;\n                    last = sstable.last;\n                }\n            }\n        }\n        intervals.add(Interval.<RowPosition, SSTableReader>create(first, last));\n        SSTableIntervalTree tree = data.getView().intervalTree;\n        Set<SSTableReader> results = new HashSet<>();\n\n        for (Interval<RowPosition, SSTableReader> interval : intervals)\n            results.addAll(tree.search(interval));\n\n        return Sets.difference(results, ImmutableSet.copyOf(sstables));\n    }"
        ],
        [
            "CompositesIndex::delete(IndexedEntry,OpOrder)",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146 -\n 147  ",
            "    public void delete(IndexedEntry entry, OpOrder.Group opGroup)\n    {\n        int localDeletionTime = (int) (System.currentTimeMillis() / 1000);\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata);\n        cfi.addTombstone(entry.indexEntry, localDeletionTime, entry.timestamp);\n        indexCfs.apply(entry.indexValue, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n        if (logger.isDebugEnabled())\n            logger.debug(\"removed index entry for cleaned-up value {}:{}\", entry.indexValue, cfi);\n    }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146 +\n 147  ",
            "    public void delete(IndexedEntry entry, OpOrder.Group opGroup)\n    {\n        int localDeletionTime = (int) (System.currentTimeMillis() / 1000);\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata);\n        cfi.addTombstone(entry.indexEntry, localDeletionTime, entry.timestamp);\n        indexCfs.apply(entry.indexValue, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n        if (logger.isTraceEnabled())\n            logger.trace(\"removed index entry for cleaned-up value {}:{}\", entry.indexValue, cfi);\n    }"
        ],
        [
            "CassandraServer::remove_counter(ByteBuffer,ColumnPath,ConsistencyLevel)",
            "1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800 -\n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  ",
            "    public void remove_counter(ByteBuffer key, ColumnPath path, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", path.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"remove_counter\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"remove_counter\");\n        }\n\n        try\n        {\n            internal_remove(key, path, System.currentTimeMillis(), consistency_level, true);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800 +\n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  ",
            "    public void remove_counter(ByteBuffer key, ColumnPath path, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_path\", path.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"remove_counter\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"remove_counter\");\n        }\n\n        try\n        {\n            internal_remove(key, path, System.currentTimeMillis(), consistency_level, true);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::getSnapshotSSTableReader(String)",
            "2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412 -\n2413 -\n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420 -\n2421  \n2422 -\n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  ",
            "    public Refs<SSTableReader> getSnapshotSSTableReader(String tag) throws IOException\n    {\n        Map<Integer, SSTableReader> active = new HashMap<>();\n        for (SSTableReader sstable : data.getView().sstables)\n            active.put(sstable.descriptor.generation, sstable);\n        Map<Descriptor, Set<Component>> snapshots = directories.sstableLister().snapshots(tag).list();\n        Refs<SSTableReader> refs = new Refs<>();\n        try\n        {\n            for (Map.Entry<Descriptor, Set<Component>> entries : snapshots.entrySet())\n            {\n                // Try acquire reference to an active sstable instead of snapshot if it exists,\n                // to avoid opening new sstables. If it fails, use the snapshot reference instead.\n                SSTableReader sstable = active.get(entries.getKey().generation);\n                if (sstable == null || !refs.tryRef(sstable))\n                {\n                    if (logger.isDebugEnabled())\n                        logger.debug(\"using snapshot sstable {}\", entries.getKey());\n                    // open without tracking hotness\n                    sstable = SSTableReader.open(entries.getKey(), entries.getValue(), metadata, partitioner, true, false);\n                    refs.tryRef(sstable);\n                    // release the self ref as we never add the snapshot sstable to DataTracker where it is otherwise released\n                    sstable.selfRef().release();\n                }\n                else if (logger.isDebugEnabled())\n                {\n                    logger.debug(\"using active sstable {}\", entries.getKey());\n                }\n            }\n        }\n        catch (IOException | RuntimeException e)\n        {\n            // In case one of the snapshot sstables fails to open,\n            // we must release the references to the ones we opened so far\n            refs.release();\n            throw e;\n        }\n        return refs;\n    }",
            "2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412 +\n2413 +\n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420 +\n2421  \n2422 +\n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  ",
            "    public Refs<SSTableReader> getSnapshotSSTableReader(String tag) throws IOException\n    {\n        Map<Integer, SSTableReader> active = new HashMap<>();\n        for (SSTableReader sstable : data.getView().sstables)\n            active.put(sstable.descriptor.generation, sstable);\n        Map<Descriptor, Set<Component>> snapshots = directories.sstableLister().snapshots(tag).list();\n        Refs<SSTableReader> refs = new Refs<>();\n        try\n        {\n            for (Map.Entry<Descriptor, Set<Component>> entries : snapshots.entrySet())\n            {\n                // Try acquire reference to an active sstable instead of snapshot if it exists,\n                // to avoid opening new sstables. If it fails, use the snapshot reference instead.\n                SSTableReader sstable = active.get(entries.getKey().generation);\n                if (sstable == null || !refs.tryRef(sstable))\n                {\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"using snapshot sstable {}\", entries.getKey());\n                    // open without tracking hotness\n                    sstable = SSTableReader.open(entries.getKey(), entries.getValue(), metadata, partitioner, true, false);\n                    refs.tryRef(sstable);\n                    // release the self ref as we never add the snapshot sstable to DataTracker where it is otherwise released\n                    sstable.selfRef().release();\n                }\n                else if (logger.isTraceEnabled())\n                {\n                    logger.trace(\"using active sstable {}\", entries.getKey());\n                }\n            }\n        }\n        catch (IOException | RuntimeException e)\n        {\n            // In case one of the snapshot sstables fails to open,\n            // we must release the references to the ones we opened so far\n            refs.release();\n            throw e;\n        }\n        return refs;\n    }"
        ],
        [
            "KeysSearcher::getIndexedIterator(OpOrder,ExtendedFilter,IndexExpression,SecondaryIndex)",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  ",
            "    private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final OpOrder.Group writeOp, final ExtendedFilter filter, final IndexExpression primary, final SecondaryIndex index)\n    {\n\n        // Start with the most-restrictive indexed clause, then apply remaining clauses\n        // to each row matching that clause.\n        // TODO: allow merge join instead of just one index + loop\n        assert index != null;\n        assert index.getIndexCfs() != null;\n        final DecoratedKey indexKey = index.getIndexKeyFor(primary.value);\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"Most-selective indexed predicate is {}\",\n                         ((AbstractSimplePerColumnSecondaryIndex) index).expressionString(primary));\n\n        /*\n         * XXX: If the range requested is a token range, we'll have to start at the beginning (and stop at the end) of\n         * the indexed row unfortunately (which will be inefficient), because we have not way to intuit the small\n         * possible key having a given token. A fix would be to actually store the token along the key in the\n         * indexed row.\n         */\n        final AbstractBounds<RowPosition> range = filter.dataRange.keyRange();\n        CellNameType type = index.getIndexCfs().getComparator();\n        final Composite startKey = range.left instanceof DecoratedKey ? type.make(((DecoratedKey)range.left).getKey()) : Composites.EMPTY;\n        final Composite endKey = range.right instanceof DecoratedKey ? type.make(((DecoratedKey)range.right).getKey()) : Composites.EMPTY;\n\n        final CellName primaryColumn = baseCfs.getComparator().cellFromByteBuffer(primary.column);\n\n        return new ColumnFamilyStore.AbstractScanIterator()\n        {\n            private Composite lastSeenKey = startKey;\n            private Iterator<Cell> indexColumns;\n            private int columnsRead = Integer.MAX_VALUE;\n\n            protected Row computeNext()\n            {\n                int meanColumns = Math.max(index.getIndexCfs().getMeanColumns(), 1);\n                // We shouldn't fetch only 1 row as this provides buggy paging in case the first row doesn't satisfy all clauses\n                int rowsPerQuery = Math.max(Math.min(filter.maxRows(), filter.maxColumns() / meanColumns), 2);\n                while (true)\n                {\n                    if (indexColumns == null || !indexColumns.hasNext())\n                    {\n                        if (columnsRead < rowsPerQuery)\n                        {\n                            logger.trace(\"Read only {} (< {}) last page through, must be done\", columnsRead, rowsPerQuery);\n                            return endOfData();\n                        }\n\n                        if (logger.isTraceEnabled() && (index instanceof AbstractSimplePerColumnSecondaryIndex))\n                            logger.trace(\"Scanning index {} starting with {}\",\n                                         ((AbstractSimplePerColumnSecondaryIndex)index).expressionString(primary), index.getBaseCfs().metadata.getKeyValidator().getString(startKey.toByteBuffer()));\n\n                        QueryFilter indexFilter = QueryFilter.getSliceFilter(indexKey,\n                                                                             index.getIndexCfs().name,\n                                                                             lastSeenKey,\n                                                                             endKey,\n                                                                             false,\n                                                                             rowsPerQuery,\n                                                                             filter.timestamp);\n                        ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);\n                        logger.trace(\"fetched {}\", indexRow);\n                        if (indexRow == null)\n                        {\n                            logger.trace(\"no data, all done\");\n                            return endOfData();\n                        }\n\n                        Collection<Cell> sortedCells = indexRow.getSortedColumns();\n                        columnsRead = sortedCells.size();\n                        indexColumns = sortedCells.iterator();\n                        Cell firstCell = sortedCells.iterator().next();\n\n                        // Paging is racy, so it is possible the first column of a page is not the last seen one.\n                        if (lastSeenKey != startKey && lastSeenKey.equals(firstCell.name()))\n                        {\n                            // skip the row we already saw w/ the last page of results\n                            indexColumns.next();\n                            logger.trace(\"Skipping {}\", baseCfs.metadata.getKeyValidator().getString(firstCell.name().toByteBuffer()));\n                        }\n                        else if (range instanceof Range && indexColumns.hasNext() && firstCell.name().equals(startKey))\n                        {\n                            // skip key excluded by range\n                            indexColumns.next();\n                            logger.trace(\"Skipping first key as range excludes it\");\n                        }\n                    }\n\n                    while (indexColumns.hasNext())\n                    {\n                        Cell cell = indexColumns.next();\n                        lastSeenKey = cell.name();\n                        if (!cell.isLive(filter.timestamp))\n                        {\n                            logger.trace(\"skipping {}\", cell.name());\n                            continue;\n                        }\n\n                        DecoratedKey dk = baseCfs.partitioner.decorateKey(lastSeenKey.toByteBuffer());\n                        if (!range.right.isMinimum() && range.right.compareTo(dk) < 0)\n                        {\n                            logger.trace(\"Reached end of assigned scan range\");\n                            return endOfData();\n                        }\n                        if (!range.contains(dk))\n                        {\n                            logger.trace(\"Skipping entry {} outside of assigned scan range\", dk.getToken());\n                            continue;\n                        }\n\n                        logger.trace(\"Returning index hit for {}\", dk);\n                        ColumnFamily data = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, filter.columnFilter(lastSeenKey.toByteBuffer()), filter.timestamp));\n                        // While the column family we'll get in the end should contains the primary clause cell, the initialFilter may not have found it and can thus be null\n                        if (data == null)\n                            data = ArrayBackedSortedColumns.factory.create(baseCfs.metadata);\n\n                        // as in CFS.filter - extend the filter to ensure we include the columns\n                        // from the index expressions, just in case they weren't included in the initialFilter\n                        IDiskAtomFilter extraFilter = filter.getExtraFilter(dk, data);\n                        if (extraFilter != null)\n                        {\n                            ColumnFamily cf = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, extraFilter, filter.timestamp));\n                            if (cf != null)\n                                data.addAll(cf);\n                        }\n\n                        if (((KeysIndex)index).isIndexEntryStale(indexKey.getKey(), data, filter.timestamp))\n                        {\n                            // delete the index entry w/ its own timestamp\n                            Cell dummyCell = new BufferCell(primaryColumn, indexKey.getKey(), cell.timestamp());\n                            ((PerColumnSecondaryIndex)index).delete(dk.getKey(), dummyCell, writeOp);\n                            continue;\n                        }\n                        return new Row(dk, data);\n                    }\n                 }\n             }\n\n            public void close() throws IOException {}\n        };\n    }",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 +\n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  ",
            "    private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final OpOrder.Group writeOp, final ExtendedFilter filter, final IndexExpression primary, final SecondaryIndex index)\n    {\n\n        // Start with the most-restrictive indexed clause, then apply remaining clauses\n        // to each row matching that clause.\n        // TODO: allow merge join instead of just one index + loop\n        assert index != null;\n        assert index.getIndexCfs() != null;\n        final DecoratedKey indexKey = index.getIndexKeyFor(primary.value);\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"Most-selective indexed predicate is {}\",\n                         ((AbstractSimplePerColumnSecondaryIndex) index).expressionString(primary));\n\n        /*\n         * XXX: If the range requested is a token range, we'll have to start at the beginning (and stop at the end) of\n         * the indexed row unfortunately (which will be inefficient), because we have not way to intuit the small\n         * possible key having a given token. A fix would be to actually store the token along the key in the\n         * indexed row.\n         */\n        final AbstractBounds<RowPosition> range = filter.dataRange.keyRange();\n        CellNameType type = index.getIndexCfs().getComparator();\n        final Composite startKey = range.left instanceof DecoratedKey ? type.make(((DecoratedKey)range.left).getKey()) : Composites.EMPTY;\n        final Composite endKey = range.right instanceof DecoratedKey ? type.make(((DecoratedKey)range.right).getKey()) : Composites.EMPTY;\n\n        final CellName primaryColumn = baseCfs.getComparator().cellFromByteBuffer(primary.column);\n\n        return new ColumnFamilyStore.AbstractScanIterator()\n        {\n            private Composite lastSeenKey = startKey;\n            private Iterator<Cell> indexColumns;\n            private int columnsRead = Integer.MAX_VALUE;\n\n            protected Row computeNext()\n            {\n                int meanColumns = Math.max(index.getIndexCfs().getMeanColumns(), 1);\n                // We shouldn't fetch only 1 row as this provides buggy paging in case the first row doesn't satisfy all clauses\n                int rowsPerQuery = Math.max(Math.min(filter.maxRows(), filter.maxColumns() / meanColumns), 2);\n                while (true)\n                {\n                    if (indexColumns == null || !indexColumns.hasNext())\n                    {\n                        if (columnsRead < rowsPerQuery)\n                        {\n                            logger.trace(\"Read only {} (< {}) last page through, must be done\", columnsRead, rowsPerQuery);\n                            return endOfData();\n                        }\n\n                        if (logger.isTraceEnabled() && (index instanceof AbstractSimplePerColumnSecondaryIndex))\n                            logger.trace(\"Scanning index {} starting with {}\",\n                                         ((AbstractSimplePerColumnSecondaryIndex)index).expressionString(primary), index.getBaseCfs().metadata.getKeyValidator().getString(startKey.toByteBuffer()));\n\n                        QueryFilter indexFilter = QueryFilter.getSliceFilter(indexKey,\n                                                                             index.getIndexCfs().name,\n                                                                             lastSeenKey,\n                                                                             endKey,\n                                                                             false,\n                                                                             rowsPerQuery,\n                                                                             filter.timestamp);\n                        ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);\n                        logger.trace(\"fetched {}\", indexRow);\n                        if (indexRow == null)\n                        {\n                            logger.trace(\"no data, all done\");\n                            return endOfData();\n                        }\n\n                        Collection<Cell> sortedCells = indexRow.getSortedColumns();\n                        columnsRead = sortedCells.size();\n                        indexColumns = sortedCells.iterator();\n                        Cell firstCell = sortedCells.iterator().next();\n\n                        // Paging is racy, so it is possible the first column of a page is not the last seen one.\n                        if (lastSeenKey != startKey && lastSeenKey.equals(firstCell.name()))\n                        {\n                            // skip the row we already saw w/ the last page of results\n                            indexColumns.next();\n                            logger.trace(\"Skipping {}\", baseCfs.metadata.getKeyValidator().getString(firstCell.name().toByteBuffer()));\n                        }\n                        else if (range instanceof Range && indexColumns.hasNext() && firstCell.name().equals(startKey))\n                        {\n                            // skip key excluded by range\n                            indexColumns.next();\n                            logger.trace(\"Skipping first key as range excludes it\");\n                        }\n                    }\n\n                    while (indexColumns.hasNext())\n                    {\n                        Cell cell = indexColumns.next();\n                        lastSeenKey = cell.name();\n                        if (!cell.isLive(filter.timestamp))\n                        {\n                            logger.trace(\"skipping {}\", cell.name());\n                            continue;\n                        }\n\n                        DecoratedKey dk = baseCfs.partitioner.decorateKey(lastSeenKey.toByteBuffer());\n                        if (!range.right.isMinimum() && range.right.compareTo(dk) < 0)\n                        {\n                            logger.trace(\"Reached end of assigned scan range\");\n                            return endOfData();\n                        }\n                        if (!range.contains(dk))\n                        {\n                            logger.trace(\"Skipping entry {} outside of assigned scan range\", dk.getToken());\n                            continue;\n                        }\n\n                        logger.trace(\"Returning index hit for {}\", dk);\n                        ColumnFamily data = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, filter.columnFilter(lastSeenKey.toByteBuffer()), filter.timestamp));\n                        // While the column family we'll get in the end should contains the primary clause cell, the initialFilter may not have found it and can thus be null\n                        if (data == null)\n                            data = ArrayBackedSortedColumns.factory.create(baseCfs.metadata);\n\n                        // as in CFS.filter - extend the filter to ensure we include the columns\n                        // from the index expressions, just in case they weren't included in the initialFilter\n                        IDiskAtomFilter extraFilter = filter.getExtraFilter(dk, data);\n                        if (extraFilter != null)\n                        {\n                            ColumnFamily cf = baseCfs.getColumnFamily(new QueryFilter(dk, baseCfs.name, extraFilter, filter.timestamp));\n                            if (cf != null)\n                                data.addAll(cf);\n                        }\n\n                        if (((KeysIndex)index).isIndexEntryStale(indexKey.getKey(), data, filter.timestamp))\n                        {\n                            // delete the index entry w/ its own timestamp\n                            Cell dummyCell = new BufferCell(primaryColumn, indexKey.getKey(), cell.timestamp());\n                            ((PerColumnSecondaryIndex)index).delete(dk.getKey(), dummyCell, writeOp);\n                            continue;\n                        }\n                        return new Row(dk, data);\n                    }\n                 }\n             }\n\n            public void close() throws IOException {}\n        };\n    }"
        ],
        [
            "CommitLogSegmentManager::start()",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "    void start()\n    {\n        // The run loop for the manager thread\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow() throws Exception\n            {\n                while (run)\n                {\n                    try\n                    {\n                        Runnable task = segmentManagementTasks.poll();\n                        if (task == null)\n                        {\n                            // if we have no more work to do, check if we should create a new segment\n                            if (availableSegments.isEmpty() && (activeSegments.isEmpty() || createReserveSegments))\n                            {\n                                logger.debug(\"No segments in reserve; creating a fresh one\");\n                                // TODO : some error handling in case we fail to create a new segment\n                                availableSegments.add(CommitLogSegment.createSegment(commitLog));\n                                hasAvailableSegments.signalAll();\n                            }\n\n                            // flush old Cfs if we're full\n                            long unused = unusedCapacity();\n                            if (unused < 0)\n                            {\n                                List<CommitLogSegment> segmentsToRecycle = new ArrayList<>();\n                                long spaceToReclaim = 0;\n                                for (CommitLogSegment segment : activeSegments)\n                                {\n                                    if (segment == allocatingFrom)\n                                        break;\n                                    segmentsToRecycle.add(segment);\n                                    spaceToReclaim += DatabaseDescriptor.getCommitLogSegmentSize();\n                                    if (spaceToReclaim + unused >= 0)\n                                        break;\n                                }\n                                flushDataFrom(segmentsToRecycle, false);\n                            }\n\n                            try\n                            {\n                                // wait for new work to be provided\n                                task = segmentManagementTasks.take();\n                            }\n                            catch (InterruptedException e)\n                            {\n                                throw new AssertionError();\n                            }\n                        }\n\n                        task.run();\n                    }\n                    catch (Throwable t)\n                    {\n                        JVMStabilityInspector.inspectThrowable(t);\n                        if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                            return;\n                        // sleep some arbitrary period to avoid spamming CL\n                        Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n                    }\n                }\n            }\n        };\n\n        run = true;\n\n        managerThread = new Thread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n        managerThread.start();\n    }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "    void start()\n    {\n        // The run loop for the manager thread\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow() throws Exception\n            {\n                while (run)\n                {\n                    try\n                    {\n                        Runnable task = segmentManagementTasks.poll();\n                        if (task == null)\n                        {\n                            // if we have no more work to do, check if we should create a new segment\n                            if (availableSegments.isEmpty() && (activeSegments.isEmpty() || createReserveSegments))\n                            {\n                                logger.trace(\"No segments in reserve; creating a fresh one\");\n                                // TODO : some error handling in case we fail to create a new segment\n                                availableSegments.add(CommitLogSegment.createSegment(commitLog));\n                                hasAvailableSegments.signalAll();\n                            }\n\n                            // flush old Cfs if we're full\n                            long unused = unusedCapacity();\n                            if (unused < 0)\n                            {\n                                List<CommitLogSegment> segmentsToRecycle = new ArrayList<>();\n                                long spaceToReclaim = 0;\n                                for (CommitLogSegment segment : activeSegments)\n                                {\n                                    if (segment == allocatingFrom)\n                                        break;\n                                    segmentsToRecycle.add(segment);\n                                    spaceToReclaim += DatabaseDescriptor.getCommitLogSegmentSize();\n                                    if (spaceToReclaim + unused >= 0)\n                                        break;\n                                }\n                                flushDataFrom(segmentsToRecycle, false);\n                            }\n\n                            try\n                            {\n                                // wait for new work to be provided\n                                task = segmentManagementTasks.take();\n                            }\n                            catch (InterruptedException e)\n                            {\n                                throw new AssertionError();\n                            }\n                        }\n\n                        task.run();\n                    }\n                    catch (Throwable t)\n                    {\n                        JVMStabilityInspector.inspectThrowable(t);\n                        if (!CommitLog.handleCommitError(\"Failed managing commit log segments\", t))\n                            return;\n                        // sleep some arbitrary period to avoid spamming CL\n                        Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n                    }\n                }\n            }\n        };\n\n        run = true;\n\n        managerThread = new Thread(runnable, \"COMMIT-LOG-ALLOCATOR\");\n        managerThread.start();\n    }"
        ],
        [
            "Keyspace::indexRow(DecoratedKey,ColumnFamilyStore,Set)",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 -\n 424 -\n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  ",
            "    /**\n     * @param key row to index\n     * @param cfs ColumnFamily to index row in\n     * @param idxNames columns to index, in comparator order\n     */\n    public static void indexRow(DecoratedKey key, ColumnFamilyStore cfs, Set<String> idxNames)\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"Indexing row {} \", cfs.metadata.getKeyValidator().getString(key.getKey()));\n\n        try (OpOrder.Group opGroup = cfs.keyspace.writeOrder.start())\n        {\n            Set<SecondaryIndex> indexes = cfs.indexManager.getIndexesByNames(idxNames);\n\n            Iterator<ColumnFamily> pager = QueryPagers.pageRowLocally(cfs, key.getKey(), DEFAULT_PAGE_SIZE);\n            while (pager.hasNext())\n            {\n                ColumnFamily cf = pager.next();\n                ColumnFamily cf2 = cf.cloneMeShallow();\n                for (Cell cell : cf)\n                {\n                    if (cfs.indexManager.indexes(cell.name(), indexes))\n                        cf2.addColumn(cell);\n                }\n                cfs.indexManager.indexRow(key.getKey(), cf2, opGroup);\n            }\n        }\n    }",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424 +\n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  ",
            "    /**\n     * @param key row to index\n     * @param cfs ColumnFamily to index row in\n     * @param idxNames columns to index, in comparator order\n     */\n    public static void indexRow(DecoratedKey key, ColumnFamilyStore cfs, Set<String> idxNames)\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Indexing row {} \", cfs.metadata.getKeyValidator().getString(key.getKey()));\n\n        try (OpOrder.Group opGroup = cfs.keyspace.writeOrder.start())\n        {\n            Set<SecondaryIndex> indexes = cfs.indexManager.getIndexesByNames(idxNames);\n\n            Iterator<ColumnFamily> pager = QueryPagers.pageRowLocally(cfs, key.getKey(), DEFAULT_PAGE_SIZE);\n            while (pager.hasNext())\n            {\n                ColumnFamily cf = pager.next();\n                ColumnFamily cf2 = cf.cloneMeShallow();\n                for (Cell cell : cf)\n                {\n                    if (cfs.indexManager.indexes(cell.name(), indexes))\n                        cf2.addColumn(cell);\n                }\n                cfs.indexManager.indexRow(key.getKey(), cf2, opGroup);\n            }\n        }\n    }"
        ],
        [
            "LeveledManifest::add(SSTableReader)",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "    public synchronized void add(SSTableReader reader)\n    {\n        int level = reader.getSSTableLevel();\n\n        assert level < generations.length : \"Invalid level \" + level + \" out of \" + (generations.length - 1);\n        logDistribution();\n        if (canAddSSTable(reader))\n        {\n            // adding the sstable does not cause overlap in the level\n            logger.debug(\"Adding {} to L{}\", reader, level);\n            generations[level].add(reader);\n        }\n        else\n        {\n            // this can happen if:\n            // * a compaction has promoted an overlapping sstable to the given level, or\n            //   was also supposed to add an sstable at the given level.\n            // * we are moving sstables from unrepaired to repaired and the sstable\n            //   would cause overlap\n            //\n            // The add(..):ed sstable will be sent to level 0\n            try\n            {\n                reader.descriptor.getMetadataSerializer().mutateLevel(reader.descriptor, 0);\n                reader.reloadSSTableMetadata();\n            }\n            catch (IOException e)\n            {\n                logger.error(\"Could not change sstable level - adding it at level 0 anyway, we will find it at restart.\", e);\n            }\n            generations[0].add(reader);\n        }\n    }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "    public synchronized void add(SSTableReader reader)\n    {\n        int level = reader.getSSTableLevel();\n\n        assert level < generations.length : \"Invalid level \" + level + \" out of \" + (generations.length - 1);\n        logDistribution();\n        if (canAddSSTable(reader))\n        {\n            // adding the sstable does not cause overlap in the level\n            logger.trace(\"Adding {} to L{}\", reader, level);\n            generations[level].add(reader);\n        }\n        else\n        {\n            // this can happen if:\n            // * a compaction has promoted an overlapping sstable to the given level, or\n            //   was also supposed to add an sstable at the given level.\n            // * we are moving sstables from unrepaired to repaired and the sstable\n            //   would cause overlap\n            //\n            // The add(..):ed sstable will be sent to level 0\n            try\n            {\n                reader.descriptor.getMetadataSerializer().mutateLevel(reader.descriptor, 0);\n                reader.reloadSSTableMetadata();\n            }\n            catch (IOException e)\n            {\n                logger.error(\"Could not change sstable level - adding it at level 0 anyway, we will find it at restart.\", e);\n            }\n            generations[0].add(reader);\n        }\n    }"
        ],
        [
            "SSTableReader::markSuspect()",
            "1656  \n1657  \n1658 -\n1659 -\n1660  \n1661  \n1662  ",
            "    public void markSuspect()\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"Marking {} as a suspect for blacklisting.\", getFilename());\n\n        isSuspect.getAndSet(true);\n    }",
            "1656  \n1657  \n1658 +\n1659 +\n1660  \n1661  \n1662  ",
            "    public void markSuspect()\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Marking {} as a suspect for blacklisting.\", getFilename());\n\n        isSuspect.getAndSet(true);\n    }"
        ],
        [
            "RangeStreamer::addRanges(String,Collection)",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150  \n 151  \n 152 -\n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159  \n 160 -\n 161  \n 162  \n 163  \n 164  ",
            "    /**\n     * Add ranges to be streamed for given keyspace.\n     *\n     * @param keyspaceName keyspace name\n     * @param ranges ranges to be streamed\n     */\n    public void addRanges(String keyspaceName, Collection<Range<Token>> ranges)\n    {\n        Multimap<Range<Token>, InetAddress> rangesForKeyspace = useStrictSourcesForRanges(keyspaceName)\n                ? getAllRangesWithStrictSourcesFor(keyspaceName, ranges) : getAllRangesWithSourcesFor(keyspaceName, ranges);\n\n        if (logger.isDebugEnabled())\n        {\n            for (Map.Entry<Range<Token>, InetAddress> entry : rangesForKeyspace.entries())\n                logger.debug(String.format(\"%s: range %s exists on %s\", description, entry.getKey(), entry.getValue()));\n        }\n\n        for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : getRangeFetchMap(rangesForKeyspace, sourceFilters, keyspaceName).asMap().entrySet())\n        {\n            if (logger.isDebugEnabled())\n            {\n                for (Range<Token> r : entry.getValue())\n                    logger.debug(String.format(\"%s: range %s from source %s for keyspace %s\", description, r, entry.getKey(), keyspaceName));\n            }\n            toFetch.put(keyspaceName, entry);\n        }\n    }",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150  \n 151  \n 152 +\n 153  \n 154  \n 155  \n 156  \n 157 +\n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164  ",
            "    /**\n     * Add ranges to be streamed for given keyspace.\n     *\n     * @param keyspaceName keyspace name\n     * @param ranges ranges to be streamed\n     */\n    public void addRanges(String keyspaceName, Collection<Range<Token>> ranges)\n    {\n        Multimap<Range<Token>, InetAddress> rangesForKeyspace = useStrictSourcesForRanges(keyspaceName)\n                ? getAllRangesWithStrictSourcesFor(keyspaceName, ranges) : getAllRangesWithSourcesFor(keyspaceName, ranges);\n\n        if (logger.isTraceEnabled())\n        {\n            for (Map.Entry<Range<Token>, InetAddress> entry : rangesForKeyspace.entries())\n                logger.trace(String.format(\"%s: range %s exists on %s\", description, entry.getKey(), entry.getValue()));\n        }\n\n        for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : getRangeFetchMap(rangesForKeyspace, sourceFilters, keyspaceName).asMap().entrySet())\n        {\n            if (logger.isTraceEnabled())\n            {\n                for (Range<Token> r : entry.getValue())\n                    logger.trace(String.format(\"%s: range %s from source %s for keyspace %s\", description, r, entry.getKey(), keyspaceName));\n            }\n            toFetch.put(keyspaceName, entry);\n        }\n    }"
        ],
        [
            "PropertyFileSnitch::getRawEndpointInfo(InetAddress)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  ",
            "    private String[] getRawEndpointInfo(InetAddress endpoint)\n    {\n        String[] value = endpointMap.get(endpoint);\n        if (value == null)\n        {\n            logger.debug(\"Could not find end point information for {}, will use default\", endpoint);\n            return defaultDCRack;\n        }\n        return value;\n    }",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  ",
            "    private String[] getRawEndpointInfo(InetAddress endpoint)\n    {\n        String[] value = endpointMap.get(endpoint);\n        if (value == null)\n        {\n            logger.trace(\"Could not find end point information for {}, will use default\", endpoint);\n            return defaultDCRack;\n        }\n        return value;\n    }"
        ],
        [
            "SizeTieredCompactionStrategy::getNextBackgroundSSTables(int)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    private List<SSTableReader> getNextBackgroundSSTables(final int gcBefore)\n    {\n        // make local copies so they can't be changed out from under us mid-method\n        int minThreshold = cfs.getMinimumCompactionThreshold();\n        int maxThreshold = cfs.getMaximumCompactionThreshold();\n\n        Iterable<SSTableReader> candidates = filterSuspectSSTables(Sets.intersection(cfs.getUncompactingSSTables(), sstables));\n\n        List<List<SSTableReader>> buckets = getBuckets(createSSTableAndLengthPairs(candidates), sizeTieredOptions.bucketHigh, sizeTieredOptions.bucketLow, sizeTieredOptions.minSSTableSize);\n        logger.debug(\"Compaction buckets are {}\", buckets);\n        updateEstimatedCompactionsByTasks(buckets);\n        List<SSTableReader> mostInteresting = mostInterestingBucket(buckets, minThreshold, maxThreshold);\n        if (!mostInteresting.isEmpty())\n            return mostInteresting;\n\n        // if there is no sstable to compact in standard way, try compacting single sstable whose droppable tombstone\n        // ratio is greater than threshold.\n        List<SSTableReader> sstablesWithTombstones = new ArrayList<>();\n        for (SSTableReader sstable : candidates)\n        {\n            if (worthDroppingTombstones(sstable, gcBefore))\n                sstablesWithTombstones.add(sstable);\n        }\n        if (sstablesWithTombstones.isEmpty())\n            return Collections.emptyList();\n\n        Collections.sort(sstablesWithTombstones, new SSTableReader.SizeComparator());\n        return Collections.singletonList(sstablesWithTombstones.get(0));\n    }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    private List<SSTableReader> getNextBackgroundSSTables(final int gcBefore)\n    {\n        // make local copies so they can't be changed out from under us mid-method\n        int minThreshold = cfs.getMinimumCompactionThreshold();\n        int maxThreshold = cfs.getMaximumCompactionThreshold();\n\n        Iterable<SSTableReader> candidates = filterSuspectSSTables(Sets.intersection(cfs.getUncompactingSSTables(), sstables));\n\n        List<List<SSTableReader>> buckets = getBuckets(createSSTableAndLengthPairs(candidates), sizeTieredOptions.bucketHigh, sizeTieredOptions.bucketLow, sizeTieredOptions.minSSTableSize);\n        logger.trace(\"Compaction buckets are {}\", buckets);\n        updateEstimatedCompactionsByTasks(buckets);\n        List<SSTableReader> mostInteresting = mostInterestingBucket(buckets, minThreshold, maxThreshold);\n        if (!mostInteresting.isEmpty())\n            return mostInteresting;\n\n        // if there is no sstable to compact in standard way, try compacting single sstable whose droppable tombstone\n        // ratio is greater than threshold.\n        List<SSTableReader> sstablesWithTombstones = new ArrayList<>();\n        for (SSTableReader sstable : candidates)\n        {\n            if (worthDroppingTombstones(sstable, gcBefore))\n                sstablesWithTombstones.add(sstable);\n        }\n        if (sstablesWithTombstones.isEmpty())\n            return Collections.emptyList();\n\n        Collections.sort(sstablesWithTombstones, new SSTableReader.SizeComparator());\n        return Collections.singletonList(sstablesWithTombstones.get(0));\n    }"
        ],
        [
            "AutoSavingCache::loadSaved()",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260 -\n 261  \n 262  \n 263  ",
            "    public int loadSaved()\n    {\n        int count = 0;\n        long start = System.nanoTime();\n\n        // modern format, allows both key and value (so key cache load can be purely sequential)\n        File dataPath = getCacheDataPath(CURRENT_VERSION);\n        File crcPath = getCacheCrcPath(CURRENT_VERSION);\n        if (dataPath.exists() && crcPath.exists())\n        {\n            DataInputStream in = null;\n            try\n            {\n                logger.info(String.format(\"reading saved cache %s\", dataPath));\n                in = new DataInputStream(new LengthAvailableInputStream(new BufferedInputStream(streamFactory.getInputStream(dataPath, crcPath)), dataPath.length()));\n\n                //Check the schema has not changed since CFs are looked up by name which is ambiguous\n                UUID schemaVersion = new UUID(in.readLong(), in.readLong());\n                if (!schemaVersion.equals(Schema.instance.getVersion()))\n                    throw new RuntimeException(\"Cache schema version \"\n                                              + schemaVersion.toString()\n                                              + \" does not match current schema version \"\n                                              + Schema.instance.getVersion());\n\n                ArrayDeque<Future<Pair<K, V>>> futures = new ArrayDeque<Future<Pair<K, V>>>();\n                while (in.available() > 0)\n                {\n                    //ksname and cfname are serialized by the serializers in CacheService\n                    //That is delegated there because there are serializer specific conditions\n                    //where a cache key is skipped and not written\n                    String ksname = in.readUTF();\n                    String cfname = in.readUTF();\n\n                    ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreIncludingIndexes(Pair.create(ksname, cfname));\n\n                    Future<Pair<K, V>> entryFuture = cacheLoader.deserialize(in, cfs);\n                    // Key cache entry can return null, if the SSTable doesn't exist.\n                    if (entryFuture == null)\n                        continue;\n\n                    futures.offer(entryFuture);\n                    count++;\n\n                    /*\n                     * Kind of unwise to accrue an unbounded number of pending futures\n                     * So now there is this loop to keep a bounded number pending.\n                     */\n                    do\n                    {\n                        while (futures.peek() != null && futures.peek().isDone())\n                        {\n                            Future<Pair<K, V>> future = futures.poll();\n                            Pair<K, V> entry = future.get();\n                            if (entry != null && entry.right != null)\n                                put(entry.left, entry.right);\n                        }\n\n                        if (futures.size() > 1000)\n                            Thread.yield();\n                    } while(futures.size() > 1000);\n                }\n\n                Future<Pair<K, V>> future = null;\n                while ((future = futures.poll()) != null)\n                {\n                    Pair<K, V> entry = future.get();\n                    if (entry != null && entry.right != null)\n                        put(entry.left, entry.right);\n                }\n            }\n            catch (CorruptFileException e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                logger.warn(String.format(\"Non-fatal checksum error reading saved cache %s\", dataPath.getAbsolutePath()), e);\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                logger.info(String.format(\"Harmless error reading saved cache %s\", dataPath.getAbsolutePath()), t);\n            }\n            finally\n            {\n                FileUtils.closeQuietly(in);\n            }\n        }\n        if (logger.isDebugEnabled())\n            logger.debug(\"completed reading ({} ms; {} keys) saved cache {}\",\n                    TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start), count, dataPath);\n        return count;\n    }",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260 +\n 261  \n 262  \n 263  ",
            "    public int loadSaved()\n    {\n        int count = 0;\n        long start = System.nanoTime();\n\n        // modern format, allows both key and value (so key cache load can be purely sequential)\n        File dataPath = getCacheDataPath(CURRENT_VERSION);\n        File crcPath = getCacheCrcPath(CURRENT_VERSION);\n        if (dataPath.exists() && crcPath.exists())\n        {\n            DataInputStream in = null;\n            try\n            {\n                logger.info(String.format(\"reading saved cache %s\", dataPath));\n                in = new DataInputStream(new LengthAvailableInputStream(new BufferedInputStream(streamFactory.getInputStream(dataPath, crcPath)), dataPath.length()));\n\n                //Check the schema has not changed since CFs are looked up by name which is ambiguous\n                UUID schemaVersion = new UUID(in.readLong(), in.readLong());\n                if (!schemaVersion.equals(Schema.instance.getVersion()))\n                    throw new RuntimeException(\"Cache schema version \"\n                                              + schemaVersion.toString()\n                                              + \" does not match current schema version \"\n                                              + Schema.instance.getVersion());\n\n                ArrayDeque<Future<Pair<K, V>>> futures = new ArrayDeque<Future<Pair<K, V>>>();\n                while (in.available() > 0)\n                {\n                    //ksname and cfname are serialized by the serializers in CacheService\n                    //That is delegated there because there are serializer specific conditions\n                    //where a cache key is skipped and not written\n                    String ksname = in.readUTF();\n                    String cfname = in.readUTF();\n\n                    ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreIncludingIndexes(Pair.create(ksname, cfname));\n\n                    Future<Pair<K, V>> entryFuture = cacheLoader.deserialize(in, cfs);\n                    // Key cache entry can return null, if the SSTable doesn't exist.\n                    if (entryFuture == null)\n                        continue;\n\n                    futures.offer(entryFuture);\n                    count++;\n\n                    /*\n                     * Kind of unwise to accrue an unbounded number of pending futures\n                     * So now there is this loop to keep a bounded number pending.\n                     */\n                    do\n                    {\n                        while (futures.peek() != null && futures.peek().isDone())\n                        {\n                            Future<Pair<K, V>> future = futures.poll();\n                            Pair<K, V> entry = future.get();\n                            if (entry != null && entry.right != null)\n                                put(entry.left, entry.right);\n                        }\n\n                        if (futures.size() > 1000)\n                            Thread.yield();\n                    } while(futures.size() > 1000);\n                }\n\n                Future<Pair<K, V>> future = null;\n                while ((future = futures.poll()) != null)\n                {\n                    Pair<K, V> entry = future.get();\n                    if (entry != null && entry.right != null)\n                        put(entry.left, entry.right);\n                }\n            }\n            catch (CorruptFileException e)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                logger.warn(String.format(\"Non-fatal checksum error reading saved cache %s\", dataPath.getAbsolutePath()), e);\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                logger.info(String.format(\"Harmless error reading saved cache %s\", dataPath.getAbsolutePath()), t);\n            }\n            finally\n            {\n                FileUtils.closeQuietly(in);\n            }\n        }\n        if (logger.isTraceEnabled())\n            logger.trace(\"completed reading ({} ms; {} keys) saved cache {}\",\n                    TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start), count, dataPath);\n        return count;\n    }"
        ],
        [
            "DateTieredCompactionStrategy::DateTieredCompactionStrategy(ColumnFamilyStore,Map)",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53 -\n  54  \n  55  \n  56 -\n  57  \n  58  ",
            "    public DateTieredCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        super(cfs, options);\n        this.estimatedRemainingTasks = 0;\n        this.options = new DateTieredCompactionStrategyOptions(options);\n        if (!options.containsKey(AbstractCompactionStrategy.TOMBSTONE_COMPACTION_INTERVAL_OPTION) && !options.containsKey(AbstractCompactionStrategy.TOMBSTONE_THRESHOLD_OPTION))\n        {\n            disableTombstoneCompactions = true;\n            logger.debug(\"Disabling tombstone compactions for DTCS\");\n        }\n        else\n            logger.debug(\"Enabling tombstone compactions for DTCS\");\n\n    }",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53 +\n  54  \n  55  \n  56 +\n  57  \n  58  ",
            "    public DateTieredCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)\n    {\n        super(cfs, options);\n        this.estimatedRemainingTasks = 0;\n        this.options = new DateTieredCompactionStrategyOptions(options);\n        if (!options.containsKey(AbstractCompactionStrategy.TOMBSTONE_COMPACTION_INTERVAL_OPTION) && !options.containsKey(AbstractCompactionStrategy.TOMBSTONE_THRESHOLD_OPTION))\n        {\n            disableTombstoneCompactions = true;\n            logger.trace(\"Disabling tombstone compactions for DTCS\");\n        }\n        else\n            logger.trace(\"Enabling tombstone compactions for DTCS\");\n\n    }"
        ],
        [
            "TopKSampler::addSample(T,long,int)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "    /**\n     * Adds a sample to statistics collection. This method is non-blocking and will\n     * use the \"Sampler\" thread pool to record results if the sampler is enabled.  If not\n     * sampling this is a NOOP\n     */\n    public void addSample(final T item, final long hash, final int value)\n    {\n        if (enabled)\n        {\n            final Object lock = this;\n            samplerExecutor.execute(new Runnable()\n            {\n                public void run()\n                {\n                    // samplerExecutor is single threaded but still need\n                    // synchronization against jmx calls to finishSampling\n                    synchronized (lock)\n                    {\n                        if (enabled)\n                        {\n                            try\n                            {\n                                summary.offer(item, value);\n                                hll.offerHashed(hash);\n                            } catch (Exception e)\n                            {\n                                logger.debug(\"Failure to offer sample\", e);\n                            }\n                        }\n                    }\n                }\n            });\n        }\n    }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "    /**\n     * Adds a sample to statistics collection. This method is non-blocking and will\n     * use the \"Sampler\" thread pool to record results if the sampler is enabled.  If not\n     * sampling this is a NOOP\n     */\n    public void addSample(final T item, final long hash, final int value)\n    {\n        if (enabled)\n        {\n            final Object lock = this;\n            samplerExecutor.execute(new Runnable()\n            {\n                public void run()\n                {\n                    // samplerExecutor is single threaded but still need\n                    // synchronization against jmx calls to finishSampling\n                    synchronized (lock)\n                    {\n                        if (enabled)\n                        {\n                            try\n                            {\n                                summary.offer(item, value);\n                                hll.offerHashed(hash);\n                            } catch (Exception e)\n                            {\n                                logger.trace(\"Failure to offer sample\", e);\n                            }\n                        }\n                    }\n                }\n            });\n        }\n    }"
        ],
        [
            "FileCacheService::put(CacheKey,RandomAccessReader)",
            " 146  \n 147  \n 148  \n 149  \n 150 -\n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    @SuppressWarnings(\"resource\")\n    public void put(CacheKey cacheKey, RandomAccessReader instance)\n    {\n        int memoryUsed = memoryUsage.get();\n        if (logger.isDebugEnabled())\n            logger.debug(\"Estimated memory usage is {} compared to actual usage {}\", memoryUsed, sizeInBytes());\n\n        CacheBucket bucket = cache.getIfPresent(cacheKey);\n        if (memoryUsed >= MEMORY_USAGE_THRESHOLD || bucket == null)\n        {\n            instance.deallocate();\n        }\n        else\n        {\n            memoryUsage.addAndGet(instance.getTotalBufferSize());\n            bucket.queue.add(instance);\n            if (bucket.discarded)\n            {\n                RandomAccessReader reader = bucket.queue.poll();\n                if (reader != null)\n                {\n                    memoryUsage.addAndGet(-1 * reader.getTotalBufferSize());\n                    reader.deallocate();\n                }\n            }\n        }\n    }",
            " 146  \n 147  \n 148  \n 149  \n 150 +\n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    @SuppressWarnings(\"resource\")\n    public void put(CacheKey cacheKey, RandomAccessReader instance)\n    {\n        int memoryUsed = memoryUsage.get();\n        if (logger.isTraceEnabled())\n            logger.trace(\"Estimated memory usage is {} compared to actual usage {}\", memoryUsed, sizeInBytes());\n\n        CacheBucket bucket = cache.getIfPresent(cacheKey);\n        if (memoryUsed >= MEMORY_USAGE_THRESHOLD || bucket == null)\n        {\n            instance.deallocate();\n        }\n        else\n        {\n            memoryUsage.addAndGet(instance.getTotalBufferSize());\n            bucket.queue.add(instance);\n            if (bucket.discarded)\n            {\n                RandomAccessReader reader = bucket.queue.poll();\n                if (reader != null)\n                {\n                    memoryUsage.addAndGet(-1 * reader.getTotalBufferSize());\n                    reader.deallocate();\n                }\n            }\n        }\n    }"
        ],
        [
            "CassandraServer::multiget_slice(List,ColumnParent,SlicePredicate,ConsistencyLevel)",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 -\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  ",
            "    public Map<ByteBuffer, List<ColumnOrSuperColumn>> multiget_slice(List<ByteBuffer> keys, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            List<String> keysList = Lists.newArrayList();\n            for (ByteBuffer key : keys)\n                keysList.add(ByteBufferUtil.bytesToHex(key));\n            Map<String, String> traceParameters = ImmutableMap.of(\"keys\", keysList.toString(),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"multiget_slice\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"multiget_slice\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            return multigetSliceInternal(keyspace, keys, column_parent, System.currentTimeMillis(), predicate, consistency_level, cState);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 +\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  ",
            "    public Map<ByteBuffer, List<ColumnOrSuperColumn>> multiget_slice(List<ByteBuffer> keys, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            List<String> keysList = Lists.newArrayList();\n            for (ByteBuffer key : keys)\n                keysList.add(ByteBufferUtil.bytesToHex(key));\n            Map<String, String> traceParameters = ImmutableMap.of(\"keys\", keysList.toString(),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"multiget_slice\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"multiget_slice\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            return multigetSliceInternal(keyspace, keys, column_parent, System.currentTimeMillis(), predicate, consistency_level, cState);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "CassandraServer::prepare_cql3_query(ByteBuffer,Compression)",
            "1921  \n1922  \n1923 -\n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  ",
            "    public CqlPreparedResult prepare_cql3_query(ByteBuffer query, Compression compression) throws TException\n    {\n        logger.debug(\"prepare_cql3_query\");\n\n        String queryString = uncompress(query, compression);\n        ThriftClientState cState = state();\n\n        try\n        {\n            cState.validateLogin();\n            return ClientState.getCQLQueryHandler().prepare(queryString,\n                                                       cState.getQueryState(),\n                                                       null).toThriftPreparedResult();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1921  \n1922  \n1923 +\n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  ",
            "    public CqlPreparedResult prepare_cql3_query(ByteBuffer query, Compression compression) throws TException\n    {\n        logger.trace(\"prepare_cql3_query\");\n\n        String queryString = uncompress(query, compression);\n        ThriftClientState cState = state();\n\n        try\n        {\n            cState.validateLogin();\n            return ClientState.getCQLQueryHandler().prepare(queryString,\n                                                       cState.getQueryState(),\n                                                       null).toThriftPreparedResult();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "DefinitionsUpdateVerbHandler::doVerb(MessageIn,int)",
            "  42  \n  43  \n  44 -\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  ",
            "    public void doVerb(final MessageIn<Collection<Mutation>> message, int id)\n    {\n        logger.debug(\"Received schema mutation push from {}\", message.from);\n\n        StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable()\n        {\n            public void runMayThrow() throws Exception\n            {\n                LegacySchemaTables.mergeSchema(message.payload);\n            }\n        });\n    }",
            "  42  \n  43  \n  44 +\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  ",
            "    public void doVerb(final MessageIn<Collection<Mutation>> message, int id)\n    {\n        logger.trace(\"Received schema mutation push from {}\", message.from);\n\n        StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable()\n        {\n            public void runMayThrow() throws Exception\n            {\n                LegacySchemaTables.mergeSchema(message.payload);\n            }\n        });\n    }"
        ],
        [
            "JavaSourceUDFFactory::generateArguments(Class,List)",
            " 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "    private static String generateArguments(Class<?>[] paramTypes, List<ColumnIdentifier> argNames)\n    {\n        StringBuilder code = new StringBuilder(64 * paramTypes.length);\n        for (int i = 0; i < paramTypes.length; i++)\n        {\n            if (i > 0)\n                code.append(\",\\n\");\n\n            if (logger.isDebugEnabled())\n                code.append(\"                /* parameter '\").append(argNames.get(i)).append(\"' */\\n\");\n\n            code\n                // cast to Java type\n                .append(\"                (\").append(javaSourceName(paramTypes[i])).append(\") \")\n                // generate object representation of input parameter (call UDFunction.compose)\n                .append(composeMethod(paramTypes[i])).append(\"(protocolVersion, \").append(i).append(\", params.get(\").append(i).append(\"))\");\n        }\n        return code.toString();\n    }",
            " 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "    private static String generateArguments(Class<?>[] paramTypes, List<ColumnIdentifier> argNames)\n    {\n        StringBuilder code = new StringBuilder(64 * paramTypes.length);\n        for (int i = 0; i < paramTypes.length; i++)\n        {\n            if (i > 0)\n                code.append(\",\\n\");\n\n            if (logger.isTraceEnabled())\n                code.append(\"                /* parameter '\").append(argNames.get(i)).append(\"' */\\n\");\n\n            code\n                // cast to Java type\n                .append(\"                (\").append(javaSourceName(paramTypes[i])).append(\") \")\n                // generate object representation of input parameter (call UDFunction.compose)\n                .append(composeMethod(paramTypes[i])).append(\"(protocolVersion, \").append(i).append(\", params.get(\").append(i).append(\"))\");\n        }\n        return code.toString();\n    }"
        ],
        [
            "QueryProcessor::MigrationSubscriber::onDropColumnFamily(String,String)",
            " 632  \n 633  \n 634 -\n 635  \n 636  ",
            "        public void onDropColumnFamily(String ksName, String cfName)\n        {\n            logger.debug(\"Table {}.{} was dropped, invalidating related prepared statements\", ksName, cfName);\n            removeInvalidPreparedStatements(ksName, cfName);\n        }",
            " 632  \n 633  \n 634 +\n 635  \n 636  ",
            "        public void onDropColumnFamily(String ksName, String cfName)\n        {\n            logger.trace(\"Table {}.{} was dropped, invalidating related prepared statements\", ksName, cfName);\n            removeInvalidPreparedStatements(ksName, cfName);\n        }"
        ],
        [
            "CassandraServer::system_update_column_family(CfDef)",
            "1627  \n1628  \n1629  \n1630 -\n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  ",
            "    public String system_update_column_family(CfDef cf_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"update_column_family\");\n\n        try\n        {\n            if (cf_def.keyspace == null || cf_def.name == null)\n                throw new InvalidRequestException(\"Keyspace and CF name must be set.\");\n\n            state().hasColumnFamilyAccess(cf_def.keyspace, cf_def.name, Permission.ALTER);\n            CFMetaData oldCfm = Schema.instance.getCFMetaData(cf_def.keyspace, cf_def.name);\n\n            if (oldCfm == null)\n                throw new InvalidRequestException(\"Could not find table definition to modify.\");\n\n            if (!oldCfm.isThriftCompatible())\n                throw new InvalidRequestException(\"Cannot modify CQL3 table \" + oldCfm.cfName + \" as it may break the schema. You should use cqlsh to modify CQL3 tables instead.\");\n\n            CFMetaData cfm = ThriftConversion.fromThriftForUpdate(cf_def, oldCfm);\n            CFMetaData.validateCompactionOptions(cfm.compactionStrategyClass, cfm.compactionStrategyOptions);\n            cfm.addDefaultIndexNames();\n\n            if (!oldCfm.getTriggers().equals(cfm.getTriggers()))\n                state().ensureIsSuper(\"Only superusers are allowed to add or remove triggers.\");\n\n            MigrationManager.announceColumnFamilyUpdate(cfm, true);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1627  \n1628  \n1629  \n1630 +\n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  ",
            "    public String system_update_column_family(CfDef cf_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"update_column_family\");\n\n        try\n        {\n            if (cf_def.keyspace == null || cf_def.name == null)\n                throw new InvalidRequestException(\"Keyspace and CF name must be set.\");\n\n            state().hasColumnFamilyAccess(cf_def.keyspace, cf_def.name, Permission.ALTER);\n            CFMetaData oldCfm = Schema.instance.getCFMetaData(cf_def.keyspace, cf_def.name);\n\n            if (oldCfm == null)\n                throw new InvalidRequestException(\"Could not find table definition to modify.\");\n\n            if (!oldCfm.isThriftCompatible())\n                throw new InvalidRequestException(\"Cannot modify CQL3 table \" + oldCfm.cfName + \" as it may break the schema. You should use cqlsh to modify CQL3 tables instead.\");\n\n            CFMetaData cfm = ThriftConversion.fromThriftForUpdate(cf_def, oldCfm);\n            CFMetaData.validateCompactionOptions(cfm.compactionStrategyClass, cfm.compactionStrategyOptions);\n            cfm.addDefaultIndexNames();\n\n            if (!oldCfm.getTriggers().equals(cfm.getTriggers()))\n                state().ensureIsSuper(\"Only superusers are allowed to add or remove triggers.\");\n\n            MigrationManager.announceColumnFamilyUpdate(cfm, true);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "IncomingTcpConnection::receiveMessage(DataInput,int)",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  ",
            "    private InetAddress receiveMessage(DataInput input, int version) throws IOException\n    {\n        int id;\n        if (version < MessagingService.VERSION_20)\n            id = Integer.parseInt(input.readUTF());\n        else\n            id = input.readInt();\n\n        long timestamp = System.currentTimeMillis();\n        boolean isCrossNodeTimestamp = false;\n        // make sure to readInt, even if cross_node_to is not enabled\n        int partial = input.readInt();\n        if (DatabaseDescriptor.hasCrossNodeTimeout())\n        {\n            long crossNodeTimestamp = (timestamp & 0xFFFFFFFF00000000L) | (((partial & 0xFFFFFFFFL) << 2) >> 2);\n            isCrossNodeTimestamp = (timestamp != crossNodeTimestamp);\n            timestamp = crossNodeTimestamp;\n        }\n\n        MessageIn message = MessageIn.read(input, version, id);\n        if (message == null)\n        {\n            // callback expired; nothing to do\n            return null;\n        }\n        if (version <= MessagingService.current_version)\n        {\n            MessagingService.instance().receive(message, id, timestamp, isCrossNodeTimestamp);\n        }\n        else\n        {\n            logger.debug(\"Received connection from newer protocol version {}. Ignoring message\", version);\n        }\n        return message.from;\n    }",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  ",
            "    private InetAddress receiveMessage(DataInput input, int version) throws IOException\n    {\n        int id;\n        if (version < MessagingService.VERSION_20)\n            id = Integer.parseInt(input.readUTF());\n        else\n            id = input.readInt();\n\n        long timestamp = System.currentTimeMillis();\n        boolean isCrossNodeTimestamp = false;\n        // make sure to readInt, even if cross_node_to is not enabled\n        int partial = input.readInt();\n        if (DatabaseDescriptor.hasCrossNodeTimeout())\n        {\n            long crossNodeTimestamp = (timestamp & 0xFFFFFFFF00000000L) | (((partial & 0xFFFFFFFFL) << 2) >> 2);\n            isCrossNodeTimestamp = (timestamp != crossNodeTimestamp);\n            timestamp = crossNodeTimestamp;\n        }\n\n        MessageIn message = MessageIn.read(input, version, id);\n        if (message == null)\n        {\n            // callback expired; nothing to do\n            return null;\n        }\n        if (version <= MessagingService.current_version)\n        {\n            MessagingService.instance().receive(message, id, timestamp, isCrossNodeTimestamp);\n        }\n        else\n        {\n            logger.trace(\"Received connection from newer protocol version {}. Ignoring message\", version);\n        }\n        return message.from;\n    }"
        ],
        [
            "Mx4jTool::maybeLoad()",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45 -\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "    /**\n     * Starts a JMX over http interface if and mx4j-tools.jar is in the classpath.\n     * @return true if successfully loaded.\n     */\n    public static boolean maybeLoad()\n    {\n        try\n        {\n            logger.debug(\"Will try to load mx4j now, if it's in the classpath\");\n            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n            ObjectName processorName = new ObjectName(\"Server:name=XSLTProcessor\");\n\n            Class<?> httpAdaptorClass = Class.forName(\"mx4j.tools.adaptor.http.HttpAdaptor\");\n            Object httpAdaptor = httpAdaptorClass.newInstance();\n            httpAdaptorClass.getMethod(\"setHost\", String.class).invoke(httpAdaptor, getAddress());\n            httpAdaptorClass.getMethod(\"setPort\", Integer.TYPE).invoke(httpAdaptor, getPort());\n\n            ObjectName httpName = new ObjectName(\"system:name=http\");\n            mbs.registerMBean(httpAdaptor, httpName);\n\n            Class<?> xsltProcessorClass = Class.forName(\"mx4j.tools.adaptor.http.XSLTProcessor\");\n            Object xsltProcessor = xsltProcessorClass.newInstance();\n            httpAdaptorClass.getMethod(\"setProcessor\", Class.forName(\"mx4j.tools.adaptor.http.ProcessorMBean\")).\n                    invoke(httpAdaptor, xsltProcessor);\n            mbs.registerMBean(xsltProcessor, processorName);\n            httpAdaptorClass.getMethod(\"start\").invoke(httpAdaptor);\n            logger.info(\"mx4j successfuly loaded\");\n            return true;\n        }\n        catch (ClassNotFoundException e)\n        {\n            logger.debug(\"Will not load MX4J, mx4j-tools.jar is not in the classpath\");\n        }\n        catch(Exception e)\n        {\n            logger.warn(\"Could not start register mbean in JMX\", e);\n        }\n        return false;\n    }",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45 +\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "    /**\n     * Starts a JMX over http interface if and mx4j-tools.jar is in the classpath.\n     * @return true if successfully loaded.\n     */\n    public static boolean maybeLoad()\n    {\n        try\n        {\n            logger.trace(\"Will try to load mx4j now, if it's in the classpath\");\n            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n            ObjectName processorName = new ObjectName(\"Server:name=XSLTProcessor\");\n\n            Class<?> httpAdaptorClass = Class.forName(\"mx4j.tools.adaptor.http.HttpAdaptor\");\n            Object httpAdaptor = httpAdaptorClass.newInstance();\n            httpAdaptorClass.getMethod(\"setHost\", String.class).invoke(httpAdaptor, getAddress());\n            httpAdaptorClass.getMethod(\"setPort\", Integer.TYPE).invoke(httpAdaptor, getPort());\n\n            ObjectName httpName = new ObjectName(\"system:name=http\");\n            mbs.registerMBean(httpAdaptor, httpName);\n\n            Class<?> xsltProcessorClass = Class.forName(\"mx4j.tools.adaptor.http.XSLTProcessor\");\n            Object xsltProcessor = xsltProcessorClass.newInstance();\n            httpAdaptorClass.getMethod(\"setProcessor\", Class.forName(\"mx4j.tools.adaptor.http.ProcessorMBean\")).\n                    invoke(httpAdaptor, xsltProcessor);\n            mbs.registerMBean(xsltProcessor, processorName);\n            httpAdaptorClass.getMethod(\"start\").invoke(httpAdaptor);\n            logger.info(\"mx4j successfuly loaded\");\n            return true;\n        }\n        catch (ClassNotFoundException e)\n        {\n            logger.trace(\"Will not load MX4J, mx4j-tools.jar is not in the classpath\");\n        }\n        catch(Exception e)\n        {\n            logger.warn(\"Could not start register mbean in JMX\", e);\n        }\n        return false;\n    }"
        ],
        [
            "CommitLogSegmentManager::recycleSegment(File)",
            " 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 -\n 358  \n 359  ",
            "    /**\n     * Differs from the above because it can work on any file instead of just existing\n     * commit log segments managed by this manager.\n     *\n     * @param file segment file that is no longer in use.\n     */\n    void recycleSegment(final File file)\n    {\n        // (don't decrease managed size, since this was never a \"live\" segment)\n        logger.debug(\"(Unopened) segment {} is no longer needed and will be deleted now\", file);\n        FileUtils.deleteWithConfirm(file);\n    }",
            " 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 +\n 358  \n 359  ",
            "    /**\n     * Differs from the above because it can work on any file instead of just existing\n     * commit log segments managed by this manager.\n     *\n     * @param file segment file that is no longer in use.\n     */\n    void recycleSegment(final File file)\n    {\n        // (don't decrease managed size, since this was never a \"live\" segment)\n        logger.trace(\"(Unopened) segment {} is no longer needed and will be deleted now\", file);\n        FileUtils.deleteWithConfirm(file);\n    }"
        ],
        [
            "Tracker::updateSizeTracking(Iterable,Iterable,Throwable)",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141 -\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155 -\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "    Throwable updateSizeTracking(Iterable<SSTableReader> oldSSTables, Iterable<SSTableReader> newSSTables, Throwable accumulate)\n    {\n        if (isDummy())\n            return accumulate;\n\n        long add = 0;\n        for (SSTableReader sstable : newSSTables)\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"adding {} to list of files tracked for {}.{}\", sstable.descriptor, cfstore.keyspace.getName(), cfstore.name);\n            try\n            {\n                add += sstable.bytesOnDisk();\n            }\n            catch (Throwable t)\n            {\n                accumulate = merge(accumulate, t);\n            }\n        }\n        long subtract = 0;\n        for (SSTableReader sstable : oldSSTables)\n        {\n            if (logger.isDebugEnabled())\n                logger.debug(\"removing {} from list of files tracked for {}.{}\", sstable.descriptor, cfstore.keyspace.getName(), cfstore.name);\n            try\n            {\n                subtract += sstable.bytesOnDisk();\n            }\n            catch (Throwable t)\n            {\n                accumulate = merge(accumulate, t);\n            }\n        }\n        StorageMetrics.load.inc(add - subtract);\n        cfstore.metric.liveDiskSpaceUsed.inc(add - subtract);\n        // we don't subtract from total until the sstable is deleted\n        cfstore.metric.totalDiskSpaceUsed.inc(add);\n        return accumulate;\n    }",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 +\n 141 +\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 +\n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "    Throwable updateSizeTracking(Iterable<SSTableReader> oldSSTables, Iterable<SSTableReader> newSSTables, Throwable accumulate)\n    {\n        if (isDummy())\n            return accumulate;\n\n        long add = 0;\n        for (SSTableReader sstable : newSSTables)\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"adding {} to list of files tracked for {}.{}\", sstable.descriptor, cfstore.keyspace.getName(), cfstore.name);\n            try\n            {\n                add += sstable.bytesOnDisk();\n            }\n            catch (Throwable t)\n            {\n                accumulate = merge(accumulate, t);\n            }\n        }\n        long subtract = 0;\n        for (SSTableReader sstable : oldSSTables)\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"removing {} from list of files tracked for {}.{}\", sstable.descriptor, cfstore.keyspace.getName(), cfstore.name);\n            try\n            {\n                subtract += sstable.bytesOnDisk();\n            }\n            catch (Throwable t)\n            {\n                accumulate = merge(accumulate, t);\n            }\n        }\n        StorageMetrics.load.inc(add - subtract);\n        cfstore.metric.liveDiskSpaceUsed.inc(add - subtract);\n        // we don't subtract from total until the sstable is deleted\n        cfstore.metric.totalDiskSpaceUsed.inc(add);\n        return accumulate;\n    }"
        ],
        [
            "LifecycleTransaction::cancel(SSTableReader)",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "    /**\n     * remove the reader from the set we're modifying\n     */\n    public void cancel(SSTableReader cancel)\n    {\n        logger.debug(\"Cancelling {} from transaction\", cancel);\n        assert originals.contains(cancel) : \"may only cancel a reader in the 'original' set: \" + cancel + \" vs \" + originals;\n        assert !(staged.contains(cancel) || logged.contains(cancel)) : \"may only cancel a reader that has not been updated or obsoleted in this transaction: \" + cancel;\n        originals.remove(cancel);\n        marked.remove(cancel);\n        identities.remove(cancel.instanceId);\n        maybeFail(unmarkCompacting(singleton(cancel), null));\n    }",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384 +\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "    /**\n     * remove the reader from the set we're modifying\n     */\n    public void cancel(SSTableReader cancel)\n    {\n        logger.trace(\"Cancelling {} from transaction\", cancel);\n        assert originals.contains(cancel) : \"may only cancel a reader in the 'original' set: \" + cancel + \" vs \" + originals;\n        assert !(staged.contains(cancel) || logged.contains(cancel)) : \"may only cancel a reader that has not been updated or obsoleted in this transaction: \" + cancel;\n        originals.remove(cancel);\n        marked.remove(cancel);\n        identities.remove(cancel.instanceId);\n        maybeFail(unmarkCompacting(singleton(cancel), null));\n    }"
        ],
        [
            "PermissionsCache::initCache(LoadingCache)",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "    private LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> initCache(\n                                                             LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> existing)\n    {\n        if (authorizer instanceof AllowAllAuthorizer)\n            return null;\n\n        if (DatabaseDescriptor.getPermissionsValidity() <= 0)\n            return null;\n\n        LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> newcache = CacheBuilder.newBuilder()\n                           .refreshAfterWrite(DatabaseDescriptor.getPermissionsUpdateInterval(), TimeUnit.MILLISECONDS)\n                           .expireAfterWrite(DatabaseDescriptor.getPermissionsValidity(), TimeUnit.MILLISECONDS)\n                           .maximumSize(DatabaseDescriptor.getPermissionsCacheMaxEntries())\n                           .build(new CacheLoader<Pair<AuthenticatedUser, IResource>, Set<Permission>>()\n                           {\n                               public Set<Permission> load(Pair<AuthenticatedUser, IResource> userResource)\n                               {\n                                   return authorizer.authorize(userResource.left, userResource.right);\n                               }\n\n                               public ListenableFuture<Set<Permission>> reload(final Pair<AuthenticatedUser, IResource> userResource,\n                                                                               final Set<Permission> oldValue)\n                               {\n                                   ListenableFutureTask<Set<Permission>> task = ListenableFutureTask.create(new Callable<Set<Permission>>()\n                                   {\n                                       public Set<Permission>call() throws Exception\n                                       {\n                                           try\n                                           {\n                                               return authorizer.authorize(userResource.left, userResource.right);\n                                           }\n                                           catch (Exception e)\n                                           {\n                                               logger.debug(\"Error performing async refresh of user permissions\", e);\n                                               throw e;\n                                           }\n                                       }\n                                   });\n                                   cacheRefreshExecutor.execute(task);\n                                   return task;\n                               }\n                           });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 +\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "    private LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> initCache(\n                                                             LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> existing)\n    {\n        if (authorizer instanceof AllowAllAuthorizer)\n            return null;\n\n        if (DatabaseDescriptor.getPermissionsValidity() <= 0)\n            return null;\n\n        LoadingCache<Pair<AuthenticatedUser, IResource>, Set<Permission>> newcache = CacheBuilder.newBuilder()\n                           .refreshAfterWrite(DatabaseDescriptor.getPermissionsUpdateInterval(), TimeUnit.MILLISECONDS)\n                           .expireAfterWrite(DatabaseDescriptor.getPermissionsValidity(), TimeUnit.MILLISECONDS)\n                           .maximumSize(DatabaseDescriptor.getPermissionsCacheMaxEntries())\n                           .build(new CacheLoader<Pair<AuthenticatedUser, IResource>, Set<Permission>>()\n                           {\n                               public Set<Permission> load(Pair<AuthenticatedUser, IResource> userResource)\n                               {\n                                   return authorizer.authorize(userResource.left, userResource.right);\n                               }\n\n                               public ListenableFuture<Set<Permission>> reload(final Pair<AuthenticatedUser, IResource> userResource,\n                                                                               final Set<Permission> oldValue)\n                               {\n                                   ListenableFutureTask<Set<Permission>> task = ListenableFutureTask.create(new Callable<Set<Permission>>()\n                                   {\n                                       public Set<Permission>call() throws Exception\n                                       {\n                                           try\n                                           {\n                                               return authorizer.authorize(userResource.left, userResource.right);\n                                           }\n                                           catch (Exception e)\n                                           {\n                                               logger.trace(\"Error performing async refresh of user permissions\", e);\n                                               throw e;\n                                           }\n                                       }\n                                   });\n                                   cacheRefreshExecutor.execute(task);\n                                   return task;\n                               }\n                           });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }"
        ],
        [
            "ColumnFamilyStore::scheduleFlush()",
            " 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "    void scheduleFlush()\n    {\n        int period = metadata.getMemtableFlushPeriod();\n        if (period > 0)\n        {\n            logger.debug(\"scheduling flush in {} ms\", period);\n            WrappedRunnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow() throws Exception\n                {\n                    synchronized (data)\n                    {\n                        Memtable current = data.getView().getCurrentMemtable();\n                        // if we're not expired, we've been hit by a scheduled flush for an already flushed memtable, so ignore\n                        if (current.isExpired())\n                        {\n                            if (current.isClean())\n                            {\n                                // if we're still clean, instead of swapping just reschedule a flush for later\n                                scheduleFlush();\n                            }\n                            else\n                            {\n                                // we'll be rescheduled by the constructor of the Memtable.\n                                forceFlush();\n                            }\n                        }\n                    }\n                }\n            };\n            ScheduledExecutors.scheduledTasks.schedule(runnable, period, TimeUnit.MILLISECONDS);\n        }\n    }",
            " 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "    void scheduleFlush()\n    {\n        int period = metadata.getMemtableFlushPeriod();\n        if (period > 0)\n        {\n            logger.trace(\"scheduling flush in {} ms\", period);\n            WrappedRunnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow() throws Exception\n                {\n                    synchronized (data)\n                    {\n                        Memtable current = data.getView().getCurrentMemtable();\n                        // if we're not expired, we've been hit by a scheduled flush for an already flushed memtable, so ignore\n                        if (current.isExpired())\n                        {\n                            if (current.isClean())\n                            {\n                                // if we're still clean, instead of swapping just reschedule a flush for later\n                                scheduleFlush();\n                            }\n                            else\n                            {\n                                // we'll be rescheduled by the constructor of the Memtable.\n                                forceFlush();\n                            }\n                        }\n                    }\n                }\n            };\n            ScheduledExecutors.scheduledTasks.schedule(runnable, period, TimeUnit.MILLISECONDS);\n        }\n    }"
        ],
        [
            "CassandraServer::atomic_batch_mutate(Map,ConsistencyLevel)",
            " 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983 -\n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  ",
            "    public void atomic_batch_mutate(Map<ByteBuffer,Map<String,List<Mutation>>> mutation_map, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = Maps.newLinkedHashMap();\n            for (Map.Entry<ByteBuffer, Map<String, List<Mutation>>> mutationEntry : mutation_map.entrySet())\n            {\n                traceParameters.put(ByteBufferUtil.bytesToHex(mutationEntry.getKey()),\n                                    Joiner.on(\";\").withKeyValueSeparator(\":\").join(mutationEntry.getValue()));\n            }\n            traceParameters.put(\"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"atomic_batch_mutate\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"atomic_batch_mutate\");\n        }\n\n        try\n        {\n            doInsert(consistency_level, createMutationList(consistency_level, mutation_map, false), true);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983 +\n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  ",
            "    public void atomic_batch_mutate(Map<ByteBuffer,Map<String,List<Mutation>>> mutation_map, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = Maps.newLinkedHashMap();\n            for (Map.Entry<ByteBuffer, Map<String, List<Mutation>>> mutationEntry : mutation_map.entrySet())\n            {\n                traceParameters.put(ByteBufferUtil.bytesToHex(mutationEntry.getKey()),\n                                    Joiner.on(\";\").withKeyValueSeparator(\":\").join(mutationEntry.getValue()));\n            }\n            traceParameters.put(\"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"atomic_batch_mutate\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"atomic_batch_mutate\");\n        }\n\n        try\n        {\n            doInsert(consistency_level, createMutationList(consistency_level, mutation_map, false), true);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "ExtendedFilter::WithClauses::needsExtraQuery(ByteBuffer,ColumnFamily)",
            " 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264 -\n 265  \n 266  \n 267  \n 268  \n 269  ",
            "        private boolean needsExtraQuery(ByteBuffer rowKey, ColumnFamily data)\n        {\n            IDiskAtomFilter filter = columnFilter(rowKey);\n            if (filter instanceof SliceQueryFilter && DataRange.isFullRowSlice((SliceQueryFilter)filter))\n                return false;\n\n            for (IndexExpression expr : clause)\n            {\n                if (data.getColumn(data.getComparator().cellFromByteBuffer(expr.column)) == null)\n                {\n                    logger.debug(\"adding extraFilter to cover additional expressions\");\n                    return true;\n                }\n            }\n            return false;\n        }",
            " 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264 +\n 265  \n 266  \n 267  \n 268  \n 269  ",
            "        private boolean needsExtraQuery(ByteBuffer rowKey, ColumnFamily data)\n        {\n            IDiskAtomFilter filter = columnFilter(rowKey);\n            if (filter instanceof SliceQueryFilter && DataRange.isFullRowSlice((SliceQueryFilter)filter))\n                return false;\n\n            for (IndexExpression expr : clause)\n            {\n                if (data.getColumn(data.getComparator().cellFromByteBuffer(expr.column)) == null)\n                {\n                    logger.trace(\"adding extraFilter to cover additional expressions\");\n                    return true;\n                }\n            }\n            return false;\n        }"
        ],
        [
            "EstimatedHistogram::log(Logger)",
            " 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 -\n 269  \n 270  ",
            "    /**\n     * log.debug() every record in the histogram\n     *\n     * @param log\n     */\n    public void log(Logger log)\n    {\n        // only print overflow if there is any\n        int nameCount;\n        if (buckets.get(buckets.length() - 1) == 0)\n            nameCount = buckets.length() - 1;\n        else\n            nameCount = buckets.length();\n        String[] names = new String[nameCount];\n\n        int maxNameLength = 0;\n        for (int i = 0; i < nameCount; i++)\n        {\n            names[i] = nameOfRange(bucketOffsets, i);\n            maxNameLength = Math.max(maxNameLength, names[i].length());\n        }\n\n        // emit log records\n        String formatstr = \"%\" + maxNameLength + \"s: %d\";\n        for (int i = 0; i < nameCount; i++)\n        {\n            long count = buckets.get(i);\n            // sort-of-hack to not print empty ranges at the start that are only used to demarcate the\n            // first populated range. for code clarity we don't omit this record from the maxNameLength\n            // calculation, and accept the unnecessary whitespace prefixes that will occasionally occur\n            if (i == 0 && count == 0)\n                continue;\n            log.debug(String.format(formatstr, names[i], count));\n        }\n    }",
            " 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  ",
            "    /**\n     * log.trace() every record in the histogram\n     *\n     * @param log\n     */\n    public void log(Logger log)\n    {\n        // only print overflow if there is any\n        int nameCount;\n        if (buckets.get(buckets.length() - 1) == 0)\n            nameCount = buckets.length() - 1;\n        else\n            nameCount = buckets.length();\n        String[] names = new String[nameCount];\n\n        int maxNameLength = 0;\n        for (int i = 0; i < nameCount; i++)\n        {\n            names[i] = nameOfRange(bucketOffsets, i);\n            maxNameLength = Math.max(maxNameLength, names[i].length());\n        }\n\n        // emit log records\n        String formatstr = \"%\" + maxNameLength + \"s: %d\";\n        for (int i = 0; i < nameCount; i++)\n        {\n            long count = buckets.get(i);\n            // sort-of-hack to not print empty ranges at the start that are only used to demarcate the\n            // first populated range. for code clarity we don't omit this record from the maxNameLength\n            // calculation, and accept the unnecessary whitespace prefixes that will occasionally occur\n            if (i == 0 && count == 0)\n                continue;\n            log.trace(String.format(formatstr, names[i], count));\n        }\n    }"
        ],
        [
            "AbstractSimplePerColumnSecondaryIndex::insert(ByteBuffer,Cell,OpOrder)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123 -\n 124  \n 125  \n 126  ",
            "    public void insert(ByteBuffer rowKey, Cell cell, OpOrder.Group opGroup)\n    {\n        DecoratedKey valueKey = getIndexKeyFor(getIndexedValue(rowKey, cell));\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata, false, 1);\n        CellName name = makeIndexColumnName(rowKey, cell);\n        if (cell instanceof ExpiringCell)\n        {\n            ExpiringCell ec = (ExpiringCell) cell;\n            cfi.addColumn(new BufferExpiringCell(name, ByteBufferUtil.EMPTY_BYTE_BUFFER, ec.timestamp(), ec.getTimeToLive(), ec.getLocalDeletionTime()));\n        }\n        else\n        {\n            cfi.addColumn(new BufferCell(name, ByteBufferUtil.EMPTY_BYTE_BUFFER, cell.timestamp()));\n        }\n        if (logger.isDebugEnabled())\n            logger.debug(\"applying index row {} in {}\", indexCfs.metadata.getKeyValidator().getString(valueKey.getKey()), cfi);\n\n        indexCfs.apply(valueKey, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n    }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  \n 125  \n 126  ",
            "    public void insert(ByteBuffer rowKey, Cell cell, OpOrder.Group opGroup)\n    {\n        DecoratedKey valueKey = getIndexKeyFor(getIndexedValue(rowKey, cell));\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata, false, 1);\n        CellName name = makeIndexColumnName(rowKey, cell);\n        if (cell instanceof ExpiringCell)\n        {\n            ExpiringCell ec = (ExpiringCell) cell;\n            cfi.addColumn(new BufferExpiringCell(name, ByteBufferUtil.EMPTY_BYTE_BUFFER, ec.timestamp(), ec.getTimeToLive(), ec.getLocalDeletionTime()));\n        }\n        else\n        {\n            cfi.addColumn(new BufferCell(name, ByteBufferUtil.EMPTY_BYTE_BUFFER, cell.timestamp()));\n        }\n        if (logger.isTraceEnabled())\n            logger.trace(\"applying index row {} in {}\", indexCfs.metadata.getKeyValidator().getString(valueKey.getKey()), cfi);\n\n        indexCfs.apply(valueKey, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n    }"
        ],
        [
            "CassandraServer::describe_schema_versions()",
            "1724  \n1725  \n1726 -\n1727  \n1728  ",
            "    public Map<String, List<String>> describe_schema_versions() throws TException, InvalidRequestException\n    {\n        logger.debug(\"checking schema agreement\");\n        return StorageProxy.describeSchemaVersions();\n    }",
            "1724  \n1725  \n1726 +\n1727  \n1728  ",
            "    public Map<String, List<String>> describe_schema_versions() throws TException, InvalidRequestException\n    {\n        logger.trace(\"checking schema agreement\");\n        return StorageProxy.describeSchemaVersions();\n    }"
        ],
        [
            "BatchlogManager::Batch::replay(RateLimiter)",
            " 275  \n 276  \n 277 -\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "        public int replay(RateLimiter rateLimiter) throws IOException\n        {\n            logger.debug(\"Replaying batch {}\", id);\n\n            List<Mutation> mutations = replayingMutations();\n\n            if (mutations.isEmpty())\n                return 0;\n\n            int ttl = calculateHintTTL(mutations);\n            if (ttl <= 0)\n                return 0;\n\n            replayHandlers = sendReplays(mutations, writtenAt, ttl);\n\n            rateLimiter.acquire(data.remaining()); // acquire afterwards, to not mess up ttl calculation.\n\n            return replayHandlers.size();\n        }",
            " 275  \n 276  \n 277 +\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "        public int replay(RateLimiter rateLimiter) throws IOException\n        {\n            logger.trace(\"Replaying batch {}\", id);\n\n            List<Mutation> mutations = replayingMutations();\n\n            if (mutations.isEmpty())\n                return 0;\n\n            int ttl = calculateHintTTL(mutations);\n            if (ttl <= 0)\n                return 0;\n\n            replayHandlers = sendReplays(mutations, writtenAt, ttl);\n\n            rateLimiter.acquire(data.remaining()); // acquire afterwards, to not mess up ttl calculation.\n\n            return replayHandlers.size();\n        }"
        ],
        [
            "CassandraServer::multiget_count(List,ColumnParent,SlicePredicate,ConsistencyLevel)",
            " 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 -\n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "    public Map<ByteBuffer, Integer> multiget_count(List<ByteBuffer> keys, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            List<String> keysList = Lists.newArrayList();\n            for (ByteBuffer key : keys)\n            {\n                keysList.add(ByteBufferUtil.bytesToHex(key));\n            }\n            Map<String, String> traceParameters = ImmutableMap.of(\"keys\", keysList.toString(),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"multiget_count\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"multiget_count\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n\n            Map<ByteBuffer, Integer> counts = new HashMap<ByteBuffer, Integer>();\n            Map<ByteBuffer, List<ColumnOrSuperColumn>> columnFamiliesMap = multigetSliceInternal(keyspace,\n                                                                                                 keys,\n                                                                                                 column_parent,\n                                                                                                 System.currentTimeMillis(),\n                                                                                                 predicate,\n                                                                                                 consistency_level,\n                                                                                                 cState);\n\n            for (Map.Entry<ByteBuffer, List<ColumnOrSuperColumn>> cf : columnFamiliesMap.entrySet())\n                counts.put(cf.getKey(), cf.getValue().size());\n            return counts;\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 +\n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "    public Map<ByteBuffer, Integer> multiget_count(List<ByteBuffer> keys, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            List<String> keysList = Lists.newArrayList();\n            for (ByteBuffer key : keys)\n            {\n                keysList.add(ByteBufferUtil.bytesToHex(key));\n            }\n            Map<String, String> traceParameters = ImmutableMap.of(\"keys\", keysList.toString(),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"multiget_count\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"multiget_count\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n\n            Map<ByteBuffer, Integer> counts = new HashMap<ByteBuffer, Integer>();\n            Map<ByteBuffer, List<ColumnOrSuperColumn>> columnFamiliesMap = multigetSliceInternal(keyspace,\n                                                                                                 keys,\n                                                                                                 column_parent,\n                                                                                                 System.currentTimeMillis(),\n                                                                                                 predicate,\n                                                                                                 consistency_level,\n                                                                                                 cState);\n\n            for (Map.Entry<ByteBuffer, List<ColumnOrSuperColumn>> cf : columnFamiliesMap.entrySet())\n                counts.put(cf.getKey(), cf.getValue().size());\n            return counts;\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "IncomingStreamingConnection::close()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "    @Override\n    public void close()\n    {\n        try\n        {\n            if (!socket.isClosed())\n            {\n                socket.close();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"Error closing socket\", e);\n        }\n        finally\n        {\n            group.remove(this);\n        }\n    }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "    @Override\n    public void close()\n    {\n        try\n        {\n            if (!socket.isClosed())\n            {\n                socket.close();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"Error closing socket\", e);\n        }\n        finally\n        {\n            group.remove(this);\n        }\n    }"
        ],
        [
            "JavaSourceUDFFactory::buildUDF(FunctionName,List,List,AbstractType,boolean,String)",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "    static UDFunction buildUDF(FunctionName name,\n                               List<ColumnIdentifier> argNames,\n                               List<AbstractType<?>> argTypes,\n                               AbstractType<?> returnType,\n                               boolean calledOnNullInput,\n                               String body)\n    throws InvalidRequestException\n    {\n        // argDataTypes is just the C* internal argTypes converted to the Java Driver DataType\n        DataType[] argDataTypes = UDHelper.driverTypes(argTypes);\n        // returnDataType is just the C* internal returnType converted to the Java Driver DataType\n        DataType returnDataType = UDHelper.driverType(returnType);\n        // javaParamTypes is just the Java representation for argTypes resp. argDataTypes\n        Class<?>[] javaParamTypes = UDHelper.javaTypes(argDataTypes, calledOnNullInput);\n        // javaReturnType is just the Java representation for returnType resp. returnDataType\n        Class<?> javaReturnType = returnDataType.asJavaClass();\n\n        String clsName = generateClassName(name);\n\n        StringBuilder javaSourceBuilder = new StringBuilder();\n        int lineOffset = 1;\n        for (int i = 0; i < javaSourceTemplate.length; i++)\n        {\n            String s = javaSourceTemplate[i];\n\n            // strings at odd indexes are 'instructions'\n            if ((i & 1) == 1)\n            {\n                switch (s)\n                {\n                    case \"class_name\":\n                        s = clsName;\n                        break;\n                    case \"body\":\n                        lineOffset = countNewlines(javaSourceBuilder);\n                        s = body;\n                        break;\n                    case \"arguments\":\n                        s = generateArguments(javaParamTypes, argNames);\n                        break;\n                    case \"argument_list\":\n                        s = generateArgumentList(javaParamTypes, argNames);\n                        break;\n                    case \"return_type\":\n                        s = javaSourceName(javaReturnType);\n                        break;\n                }\n            }\n\n            javaSourceBuilder.append(s);\n        }\n\n        String targetClassName = GENERATED_PACKAGE + '.' + clsName;\n\n        String javaSource = javaSourceBuilder.toString();\n\n        logger.debug(\"Compiling Java source UDF '{}' as class '{}' using source:\\n{}\", name, targetClassName, javaSource);\n\n        try\n        {\n            EcjCompilationUnit compilationUnit = new EcjCompilationUnit(javaSource, targetClassName);\n\n            Compiler compiler = new Compiler(compilationUnit,\n                                             errorHandlingPolicy,\n                                             compilerOptions,\n                                             compilationUnit,\n                                             problemFactory);\n            compiler.compile(new ICompilationUnit[]{ compilationUnit });\n\n            if (compilationUnit.problemList != null && !compilationUnit.problemList.isEmpty())\n            {\n                boolean fullSource = false;\n                StringBuilder problems = new StringBuilder();\n                for (IProblem problem : compilationUnit.problemList)\n                {\n                    long ln = problem.getSourceLineNumber() - lineOffset;\n                    if (ln < 1L)\n                    {\n                        if (problem.isError())\n                        {\n                            // if generated source around UDF source provided by the user is buggy,\n                            // this code is appended.\n                            problems.append(\"GENERATED SOURCE ERROR: line \")\n                                    .append(problem.getSourceLineNumber())\n                                    .append(\" (in generated source): \")\n                                    .append(problem.getMessage())\n                                    .append('\\n');\n                            fullSource = true;\n                        }\n                    }\n                    else\n                    {\n                        problems.append(\"Line \")\n                                .append(Long.toString(ln))\n                                .append(\": \")\n                                .append(problem.getMessage())\n                                .append('\\n');\n                    }\n                }\n\n                if (fullSource)\n                    throw new InvalidRequestException(\"Java source compilation failed:\\n\" + problems + \"\\n generated source:\\n\" + javaSource);\n                else\n                    throw new InvalidRequestException(\"Java source compilation failed:\\n\" + problems);\n            }\n\n            Class cls = targetClassLoader.loadClass(targetClassName);\n\n            if (cls.getDeclaredMethods().length != 2 || cls.getDeclaredConstructors().length != 1)\n                throw new InvalidRequestException(\"Check your source to not define additional Java methods or constructors\");\n            MethodType methodType = MethodType.methodType(void.class)\n                                              .appendParameterTypes(FunctionName.class, List.class, List.class, DataType[].class,\n                                                                    AbstractType.class, DataType.class,\n                                                                    boolean.class, String.class);\n            MethodHandle ctor = MethodHandles.lookup().findConstructor(cls, methodType);\n            return (UDFunction) ctor.invokeWithArguments(name, argNames, argTypes, argDataTypes,\n                                                         returnType, returnDataType,\n                                                         calledOnNullInput, body);\n        }\n        catch (InvocationTargetException e)\n        {\n            // in case of an ITE, use the cause\n            throw new InvalidRequestException(String.format(\"Could not compile function '%s' from Java source: %s\", name, e.getCause()));\n        }\n        catch (VirtualMachineError e)\n        {\n            throw e;\n        }\n        catch (Throwable e)\n        {\n            throw new InvalidRequestException(String.format(\"Could not compile function '%s' from Java source: %s\", name, e));\n        }\n    }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "    static UDFunction buildUDF(FunctionName name,\n                               List<ColumnIdentifier> argNames,\n                               List<AbstractType<?>> argTypes,\n                               AbstractType<?> returnType,\n                               boolean calledOnNullInput,\n                               String body)\n    throws InvalidRequestException\n    {\n        // argDataTypes is just the C* internal argTypes converted to the Java Driver DataType\n        DataType[] argDataTypes = UDHelper.driverTypes(argTypes);\n        // returnDataType is just the C* internal returnType converted to the Java Driver DataType\n        DataType returnDataType = UDHelper.driverType(returnType);\n        // javaParamTypes is just the Java representation for argTypes resp. argDataTypes\n        Class<?>[] javaParamTypes = UDHelper.javaTypes(argDataTypes, calledOnNullInput);\n        // javaReturnType is just the Java representation for returnType resp. returnDataType\n        Class<?> javaReturnType = returnDataType.asJavaClass();\n\n        String clsName = generateClassName(name);\n\n        StringBuilder javaSourceBuilder = new StringBuilder();\n        int lineOffset = 1;\n        for (int i = 0; i < javaSourceTemplate.length; i++)\n        {\n            String s = javaSourceTemplate[i];\n\n            // strings at odd indexes are 'instructions'\n            if ((i & 1) == 1)\n            {\n                switch (s)\n                {\n                    case \"class_name\":\n                        s = clsName;\n                        break;\n                    case \"body\":\n                        lineOffset = countNewlines(javaSourceBuilder);\n                        s = body;\n                        break;\n                    case \"arguments\":\n                        s = generateArguments(javaParamTypes, argNames);\n                        break;\n                    case \"argument_list\":\n                        s = generateArgumentList(javaParamTypes, argNames);\n                        break;\n                    case \"return_type\":\n                        s = javaSourceName(javaReturnType);\n                        break;\n                }\n            }\n\n            javaSourceBuilder.append(s);\n        }\n\n        String targetClassName = GENERATED_PACKAGE + '.' + clsName;\n\n        String javaSource = javaSourceBuilder.toString();\n\n        logger.trace(\"Compiling Java source UDF '{}' as class '{}' using source:\\n{}\", name, targetClassName, javaSource);\n\n        try\n        {\n            EcjCompilationUnit compilationUnit = new EcjCompilationUnit(javaSource, targetClassName);\n\n            Compiler compiler = new Compiler(compilationUnit,\n                                             errorHandlingPolicy,\n                                             compilerOptions,\n                                             compilationUnit,\n                                             problemFactory);\n            compiler.compile(new ICompilationUnit[]{ compilationUnit });\n\n            if (compilationUnit.problemList != null && !compilationUnit.problemList.isEmpty())\n            {\n                boolean fullSource = false;\n                StringBuilder problems = new StringBuilder();\n                for (IProblem problem : compilationUnit.problemList)\n                {\n                    long ln = problem.getSourceLineNumber() - lineOffset;\n                    if (ln < 1L)\n                    {\n                        if (problem.isError())\n                        {\n                            // if generated source around UDF source provided by the user is buggy,\n                            // this code is appended.\n                            problems.append(\"GENERATED SOURCE ERROR: line \")\n                                    .append(problem.getSourceLineNumber())\n                                    .append(\" (in generated source): \")\n                                    .append(problem.getMessage())\n                                    .append('\\n');\n                            fullSource = true;\n                        }\n                    }\n                    else\n                    {\n                        problems.append(\"Line \")\n                                .append(Long.toString(ln))\n                                .append(\": \")\n                                .append(problem.getMessage())\n                                .append('\\n');\n                    }\n                }\n\n                if (fullSource)\n                    throw new InvalidRequestException(\"Java source compilation failed:\\n\" + problems + \"\\n generated source:\\n\" + javaSource);\n                else\n                    throw new InvalidRequestException(\"Java source compilation failed:\\n\" + problems);\n            }\n\n            Class cls = targetClassLoader.loadClass(targetClassName);\n\n            if (cls.getDeclaredMethods().length != 2 || cls.getDeclaredConstructors().length != 1)\n                throw new InvalidRequestException(\"Check your source to not define additional Java methods or constructors\");\n            MethodType methodType = MethodType.methodType(void.class)\n                                              .appendParameterTypes(FunctionName.class, List.class, List.class, DataType[].class,\n                                                                    AbstractType.class, DataType.class,\n                                                                    boolean.class, String.class);\n            MethodHandle ctor = MethodHandles.lookup().findConstructor(cls, methodType);\n            return (UDFunction) ctor.invokeWithArguments(name, argNames, argTypes, argDataTypes,\n                                                         returnType, returnDataType,\n                                                         calledOnNullInput, body);\n        }\n        catch (InvocationTargetException e)\n        {\n            // in case of an ITE, use the cause\n            throw new InvalidRequestException(String.format(\"Could not compile function '%s' from Java source: %s\", name, e.getCause()));\n        }\n        catch (VirtualMachineError e)\n        {\n            throw e;\n        }\n        catch (Throwable e)\n        {\n            throw new InvalidRequestException(String.format(\"Could not compile function '%s' from Java source: %s\", name, e));\n        }\n    }"
        ],
        [
            "SSTableReader::saveSummary(Descriptor,DecoratedKey,DecoratedKey,SegmentedFile,SegmentedFile,IndexSummary)",
            " 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948 -\n 949  \n 950  \n 951  \n 952  \n 953  \n 954  ",
            "    /**\n     * Save index summary to Summary.db file.\n     */\n    public static void saveSummary(Descriptor descriptor, DecoratedKey first, DecoratedKey last,\n                                   SegmentedFile.Builder ibuilder, SegmentedFile.Builder dbuilder, IndexSummary summary)\n    {\n        File summariesFile = new File(descriptor.filenameFor(Component.SUMMARY));\n        if (summariesFile.exists())\n            FileUtils.deleteWithConfirm(summariesFile);\n\n        try (DataOutputStreamPlus oStream = new BufferedDataOutputStreamPlus(new FileOutputStream(summariesFile));)\n        {\n            IndexSummary.serializer.serialize(summary, oStream, descriptor.version.hasSamplingLevel());\n            ByteBufferUtil.writeWithLength(first.getKey(), oStream);\n            ByteBufferUtil.writeWithLength(last.getKey(), oStream);\n            ibuilder.serializeBounds(oStream);\n            dbuilder.serializeBounds(oStream);\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"Cannot save SSTable Summary: \", e);\n\n            // corrupted hence delete it and let it load it now.\n            if (summariesFile.exists())\n                FileUtils.deleteWithConfirm(summariesFile);\n        }\n    }",
            " 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948 +\n 949  \n 950  \n 951  \n 952  \n 953  \n 954  ",
            "    /**\n     * Save index summary to Summary.db file.\n     */\n    public static void saveSummary(Descriptor descriptor, DecoratedKey first, DecoratedKey last,\n                                   SegmentedFile.Builder ibuilder, SegmentedFile.Builder dbuilder, IndexSummary summary)\n    {\n        File summariesFile = new File(descriptor.filenameFor(Component.SUMMARY));\n        if (summariesFile.exists())\n            FileUtils.deleteWithConfirm(summariesFile);\n\n        try (DataOutputStreamPlus oStream = new BufferedDataOutputStreamPlus(new FileOutputStream(summariesFile));)\n        {\n            IndexSummary.serializer.serialize(summary, oStream, descriptor.version.hasSamplingLevel());\n            ByteBufferUtil.writeWithLength(first.getKey(), oStream);\n            ByteBufferUtil.writeWithLength(last.getKey(), oStream);\n            ibuilder.serializeBounds(oStream);\n            dbuilder.serializeBounds(oStream);\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"Cannot save SSTable Summary: \", e);\n\n            // corrupted hence delete it and let it load it now.\n            if (summariesFile.exists())\n                FileUtils.deleteWithConfirm(summariesFile);\n        }\n    }"
        ],
        [
            "HintedHandOffManager::deliverHintsToEndpoint(InetAddress)",
            " 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 -\n 333  \n 334  \n 335  \n 336 -\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348 -\n 349  \n 350  \n 351  \n 352  \n 353  ",
            "    private void deliverHintsToEndpoint(InetAddress endpoint)\n    {\n        if (hintStore.isEmpty())\n            return; // nothing to do, don't confuse users by logging a no-op handoff\n\n        // check if hints delivery has been paused\n        if (hintedHandOffPaused)\n        {\n            logger.debug(\"Hints delivery process is paused, aborting\");\n            return;\n        }\n\n        logger.debug(\"Checking remote({}) schema before delivering hints\", endpoint);\n        try\n        {\n            waitForSchemaAgreement(endpoint);\n        }\n        catch (TimeoutException e)\n        {\n            return;\n        }\n\n        if (!FailureDetector.instance.isAlive(endpoint))\n        {\n            logger.debug(\"Endpoint {} died before hint delivery, aborting\", endpoint);\n            return;\n        }\n\n        doDeliverHintsToEndpoint(endpoint);\n    }",
            " 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333  \n 334  \n 335  \n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348 +\n 349  \n 350  \n 351  \n 352  \n 353  ",
            "    private void deliverHintsToEndpoint(InetAddress endpoint)\n    {\n        if (hintStore.isEmpty())\n            return; // nothing to do, don't confuse users by logging a no-op handoff\n\n        // check if hints delivery has been paused\n        if (hintedHandOffPaused)\n        {\n            logger.trace(\"Hints delivery process is paused, aborting\");\n            return;\n        }\n\n        logger.trace(\"Checking remote({}) schema before delivering hints\", endpoint);\n        try\n        {\n            waitForSchemaAgreement(endpoint);\n        }\n        catch (TimeoutException e)\n        {\n            return;\n        }\n\n        if (!FailureDetector.instance.isAlive(endpoint))\n        {\n            logger.trace(\"Endpoint {} died before hint delivery, aborting\", endpoint);\n            return;\n        }\n\n        doDeliverHintsToEndpoint(endpoint);\n    }"
        ],
        [
            "CompactionManager::doValidationCompaction(ColumnFamilyStore,Validator)",
            " 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113 -\n1114  \n1115  \n1116  \n1117 -\n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  ",
            "    /**\n     * Performs a readonly \"compaction\" of all sstables in order to validate complete rows,\n     * but without writing the merge result\n     */\n    @SuppressWarnings(\"resource\")\n    private void doValidationCompaction(ColumnFamilyStore cfs, Validator validator) throws IOException\n    {\n        // this isn't meant to be race-proof, because it's not -- it won't cause bugs for a CFS to be dropped\n        // mid-validation, or to attempt to validate a droped CFS.  this is just a best effort to avoid useless work,\n        // particularly in the scenario where a validation is submitted before the drop, and there are compactions\n        // started prior to the drop keeping some sstables alive.  Since validationCompaction can run\n        // concurrently with other compactions, it would otherwise go ahead and scan those again.\n        if (!cfs.isValid())\n            return;\n\n        Refs<SSTableReader> sstables = null;\n        try\n        {\n\n            String snapshotName = validator.desc.sessionId.toString();\n            int gcBefore;\n            boolean isSnapshotValidation = cfs.snapshotExists(snapshotName);\n            if (isSnapshotValidation)\n            {\n                // If there is a snapshot created for the session then read from there.\n                // note that we populate the parent repair session when creating the snapshot, meaning the sstables in the snapshot are the ones we\n                // are supposed to validate.\n                sstables = cfs.getSnapshotSSTableReader(snapshotName);\n\n\n                // Computing gcbefore based on the current time wouldn't be very good because we know each replica will execute\n                // this at a different time (that's the whole purpose of repair with snaphsot). So instead we take the creation\n                // time of the snapshot, which should give us roughtly the same time on each replica (roughtly being in that case\n                // 'as good as in the non-snapshot' case)\n                gcBefore = cfs.gcBefore(cfs.getSnapshotCreationTime(snapshotName));\n            }\n            else\n            {\n                // flush first so everyone is validating data that is as similar as possible\n                StorageService.instance.forceKeyspaceFlush(cfs.keyspace.getName(), cfs.name);\n                ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(validator.desc.parentSessionId);\n                ColumnFamilyStore.RefViewFragment sstableCandidates = cfs.selectAndReference(prs.isIncremental ? ColumnFamilyStore.UNREPAIRED_SSTABLES : ColumnFamilyStore.CANONICAL_SSTABLES);\n                Set<SSTableReader> sstablesToValidate = new HashSet<>();\n\n                for (SSTableReader sstable : sstableCandidates.sstables)\n                {\n                    if (new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(Collections.singletonList(validator.desc.range)))\n                    {\n                        sstablesToValidate.add(sstable);\n                    }\n                }\n\n                Set<SSTableReader> currentlyRepairing = ActiveRepairService.instance.currentlyRepairing(cfs.metadata.cfId, validator.desc.parentSessionId);\n\n                if (!Sets.intersection(currentlyRepairing, sstablesToValidate).isEmpty())\n                {\n                    logger.error(\"Cannot start multiple repair sessions over the same sstables\");\n                    throw new RuntimeException(\"Cannot start multiple repair sessions over the same sstables\");\n                }\n\n                sstables = Refs.tryRef(sstablesToValidate);\n                if (sstables == null)\n                {\n                    logger.error(\"Could not reference sstables\");\n                    throw new RuntimeException(\"Could not reference sstables\");\n                }\n                sstableCandidates.release();\n                prs.addSSTables(cfs.metadata.cfId, sstablesToValidate);\n\n                if (validator.gcBefore > 0)\n                    gcBefore = validator.gcBefore;\n                else\n                    gcBefore = getDefaultGcBefore(cfs);\n            }\n\n            // Create Merkle tree suitable to hold estimated partitions for given range.\n            // We blindly assume that partition is evenly distributed on all sstables for now.\n            long numPartitions = 0;\n            for (SSTableReader sstable : sstables)\n            {\n                numPartitions += sstable.estimatedKeysForRanges(singleton(validator.desc.range));\n            }\n            // determine tree depth from number of partitions, but cap at 20 to prevent large tree.\n            int depth = numPartitions > 0 ? (int) Math.min(Math.floor(Math.log(numPartitions)), 20) : 0;\n            MerkleTree tree = new MerkleTree(cfs.partitioner, validator.desc.range, MerkleTree.RECOMMENDED_DEPTH, (int) Math.pow(2, depth));\n\n            long start = System.nanoTime();\n            try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategy().getScanners(sstables, validator.desc.range))\n            {\n                CompactionIterable ci = new ValidationCompactionIterable(cfs, scanners.scanners, gcBefore);\n                Iterator<AbstractCompactedRow> iter = ci.iterator();\n                metrics.beginCompaction(ci);\n                try\n                {\n                    // validate the CF as we iterate over it\n                    validator.prepare(cfs, tree);\n                    while (iter.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n                        AbstractCompactedRow row = iter.next();\n                        validator.add(row);\n                    }\n                    validator.complete();\n                }\n                finally\n                {\n                    if (isSnapshotValidation)\n                    {\n                        cfs.clearSnapshot(snapshotName);\n                    }\n\n                    metrics.finishCompaction(ci);\n                }\n            }\n\n            if (logger.isDebugEnabled())\n            {\n                // MT serialize may take time\n                long duration = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n                logger.debug(\"Validation finished in {} msec, depth {} for {} keys, serialized size {} bytes for {}\",\n                             duration,\n                             depth,\n                             numPartitions,\n                             MerkleTree.serializer.serializedSize(tree, 0),\n                             validator.desc);\n            }\n        }\n        finally\n        {\n            if (sstables != null)\n                sstables.release();\n        }\n    }",
            " 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113 +\n1114  \n1115  \n1116  \n1117 +\n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  ",
            "    /**\n     * Performs a readonly \"compaction\" of all sstables in order to validate complete rows,\n     * but without writing the merge result\n     */\n    @SuppressWarnings(\"resource\")\n    private void doValidationCompaction(ColumnFamilyStore cfs, Validator validator) throws IOException\n    {\n        // this isn't meant to be race-proof, because it's not -- it won't cause bugs for a CFS to be dropped\n        // mid-validation, or to attempt to validate a droped CFS.  this is just a best effort to avoid useless work,\n        // particularly in the scenario where a validation is submitted before the drop, and there are compactions\n        // started prior to the drop keeping some sstables alive.  Since validationCompaction can run\n        // concurrently with other compactions, it would otherwise go ahead and scan those again.\n        if (!cfs.isValid())\n            return;\n\n        Refs<SSTableReader> sstables = null;\n        try\n        {\n\n            String snapshotName = validator.desc.sessionId.toString();\n            int gcBefore;\n            boolean isSnapshotValidation = cfs.snapshotExists(snapshotName);\n            if (isSnapshotValidation)\n            {\n                // If there is a snapshot created for the session then read from there.\n                // note that we populate the parent repair session when creating the snapshot, meaning the sstables in the snapshot are the ones we\n                // are supposed to validate.\n                sstables = cfs.getSnapshotSSTableReader(snapshotName);\n\n\n                // Computing gcbefore based on the current time wouldn't be very good because we know each replica will execute\n                // this at a different time (that's the whole purpose of repair with snaphsot). So instead we take the creation\n                // time of the snapshot, which should give us roughtly the same time on each replica (roughtly being in that case\n                // 'as good as in the non-snapshot' case)\n                gcBefore = cfs.gcBefore(cfs.getSnapshotCreationTime(snapshotName));\n            }\n            else\n            {\n                // flush first so everyone is validating data that is as similar as possible\n                StorageService.instance.forceKeyspaceFlush(cfs.keyspace.getName(), cfs.name);\n                ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(validator.desc.parentSessionId);\n                ColumnFamilyStore.RefViewFragment sstableCandidates = cfs.selectAndReference(prs.isIncremental ? ColumnFamilyStore.UNREPAIRED_SSTABLES : ColumnFamilyStore.CANONICAL_SSTABLES);\n                Set<SSTableReader> sstablesToValidate = new HashSet<>();\n\n                for (SSTableReader sstable : sstableCandidates.sstables)\n                {\n                    if (new Bounds<>(sstable.first.getToken(), sstable.last.getToken()).intersects(Collections.singletonList(validator.desc.range)))\n                    {\n                        sstablesToValidate.add(sstable);\n                    }\n                }\n\n                Set<SSTableReader> currentlyRepairing = ActiveRepairService.instance.currentlyRepairing(cfs.metadata.cfId, validator.desc.parentSessionId);\n\n                if (!Sets.intersection(currentlyRepairing, sstablesToValidate).isEmpty())\n                {\n                    logger.error(\"Cannot start multiple repair sessions over the same sstables\");\n                    throw new RuntimeException(\"Cannot start multiple repair sessions over the same sstables\");\n                }\n\n                sstables = Refs.tryRef(sstablesToValidate);\n                if (sstables == null)\n                {\n                    logger.error(\"Could not reference sstables\");\n                    throw new RuntimeException(\"Could not reference sstables\");\n                }\n                sstableCandidates.release();\n                prs.addSSTables(cfs.metadata.cfId, sstablesToValidate);\n\n                if (validator.gcBefore > 0)\n                    gcBefore = validator.gcBefore;\n                else\n                    gcBefore = getDefaultGcBefore(cfs);\n            }\n\n            // Create Merkle tree suitable to hold estimated partitions for given range.\n            // We blindly assume that partition is evenly distributed on all sstables for now.\n            long numPartitions = 0;\n            for (SSTableReader sstable : sstables)\n            {\n                numPartitions += sstable.estimatedKeysForRanges(singleton(validator.desc.range));\n            }\n            // determine tree depth from number of partitions, but cap at 20 to prevent large tree.\n            int depth = numPartitions > 0 ? (int) Math.min(Math.floor(Math.log(numPartitions)), 20) : 0;\n            MerkleTree tree = new MerkleTree(cfs.partitioner, validator.desc.range, MerkleTree.RECOMMENDED_DEPTH, (int) Math.pow(2, depth));\n\n            long start = System.nanoTime();\n            try (AbstractCompactionStrategy.ScannerList scanners = cfs.getCompactionStrategy().getScanners(sstables, validator.desc.range))\n            {\n                CompactionIterable ci = new ValidationCompactionIterable(cfs, scanners.scanners, gcBefore);\n                Iterator<AbstractCompactedRow> iter = ci.iterator();\n                metrics.beginCompaction(ci);\n                try\n                {\n                    // validate the CF as we iterate over it\n                    validator.prepare(cfs, tree);\n                    while (iter.hasNext())\n                    {\n                        if (ci.isStopRequested())\n                            throw new CompactionInterruptedException(ci.getCompactionInfo());\n                        AbstractCompactedRow row = iter.next();\n                        validator.add(row);\n                    }\n                    validator.complete();\n                }\n                finally\n                {\n                    if (isSnapshotValidation)\n                    {\n                        cfs.clearSnapshot(snapshotName);\n                    }\n\n                    metrics.finishCompaction(ci);\n                }\n            }\n\n            if (logger.isTraceEnabled())\n            {\n                // MT serialize may take time\n                long duration = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n                logger.trace(\"Validation finished in {} msec, depth {} for {} keys, serialized size {} bytes for {}\",\n                             duration,\n                             depth,\n                             numPartitions,\n                             MerkleTree.serializer.serializedSize(tree, 0),\n                             validator.desc);\n            }\n        }\n        finally\n        {\n            if (sstables != null)\n                sstables.release();\n        }\n    }"
        ],
        [
            "CassandraServer::execute_prepared_cql3_query(int,List,ConsistencyLevel)",
            "1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955 -\n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  ",
            "    public CqlResult execute_prepared_cql3_query(int itemId, List<ByteBuffer> bindVariables, ConsistencyLevel cLevel) throws TException\n    {\n        if (startSessionIfRequested())\n        {\n            // TODO we don't have [typed] access to CQL bind variables here.  CASSANDRA-4560 is open to add support.\n            Tracing.instance.begin(\"execute_prepared_cql3_query\", ImmutableMap.of(\"consistency_level\", cLevel.name()));\n        }\n        else\n        {\n            logger.debug(\"execute_prepared_cql3_query\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            ParsedStatement.Prepared prepared = ClientState.getCQLQueryHandler().getPreparedForThrift(itemId);\n\n            if (prepared == null)\n                throw new InvalidRequestException(String.format(\"Prepared query with ID %d not found\" +\n                                                                \" (either the query was not prepared on this host (maybe the host has been restarted?)\" +\n                                                                \" or you have prepared too many queries and it has been evicted from the internal cache)\",\n                                                                itemId));\n            logger.trace(\"Retrieved prepared statement #{} with {} bind markers\", itemId, prepared.statement.getBoundTerms());\n\n            return ClientState.getCQLQueryHandler().processPrepared(prepared.statement,\n                                                                    cState.getQueryState(),\n                                                                    QueryOptions.fromProtocolV2(ThriftConversion.fromThrift(cLevel), bindVariables),\n                                                                    null).toThriftResult();\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955 +\n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  ",
            "    public CqlResult execute_prepared_cql3_query(int itemId, List<ByteBuffer> bindVariables, ConsistencyLevel cLevel) throws TException\n    {\n        if (startSessionIfRequested())\n        {\n            // TODO we don't have [typed] access to CQL bind variables here.  CASSANDRA-4560 is open to add support.\n            Tracing.instance.begin(\"execute_prepared_cql3_query\", ImmutableMap.of(\"consistency_level\", cLevel.name()));\n        }\n        else\n        {\n            logger.trace(\"execute_prepared_cql3_query\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            ParsedStatement.Prepared prepared = ClientState.getCQLQueryHandler().getPreparedForThrift(itemId);\n\n            if (prepared == null)\n                throw new InvalidRequestException(String.format(\"Prepared query with ID %d not found\" +\n                                                                \" (either the query was not prepared on this host (maybe the host has been restarted?)\" +\n                                                                \" or you have prepared too many queries and it has been evicted from the internal cache)\",\n                                                                itemId));\n            logger.trace(\"Retrieved prepared statement #{} with {} bind markers\", itemId, prepared.statement.getBoundTerms());\n\n            return ClientState.getCQLQueryHandler().processPrepared(prepared.statement,\n                                                                    cState.getQueryState(),\n                                                                    QueryOptions.fromProtocolV2(ThriftConversion.fromThrift(cLevel), bindVariables),\n                                                                    null).toThriftResult();\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::truncateBlocking()",
            "2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637 -\n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663 -\n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677 -\n2678  \n2679  \n2680  \n2681  \n2682  \n2683 -\n2684  ",
            "    /**\n     * Truncate deletes the entire column family's data with no expensive tombstone creation\n     */\n    public void truncateBlocking()\n    {\n        // We have two goals here:\n        // - truncate should delete everything written before truncate was invoked\n        // - but not delete anything that isn't part of the snapshot we create.\n        // We accomplish this by first flushing manually, then snapshotting, and\n        // recording the timestamp IN BETWEEN those actions. Any sstables created\n        // with this timestamp or greater time, will not be marked for delete.\n        //\n        // Bonus complication: since we store replay position in sstable metadata,\n        // truncating those sstables means we will replay any CL segments from the\n        // beginning if we restart before they [the CL segments] are discarded for\n        // normal reasons post-truncate.  To prevent this, we store truncation\n        // position in the System keyspace.\n        logger.debug(\"truncating {}\", name);\n\n        if (keyspace.getMetadata().durableWrites || DatabaseDescriptor.isAutoSnapshot())\n        {\n            // flush the CF being truncated before forcing the new segment\n            forceBlockingFlush();\n\n            // sleep a little to make sure that our truncatedAt comes after any sstable\n            // that was part of the flushed we forced; otherwise on a tie, it won't get deleted.\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            // just nuke the memtable data w/o writing to disk first\n            synchronized (data)\n            {\n                final Flush flush = new Flush(true);\n                flushExecutor.execute(flush);\n                postFlushExecutor.submit(flush.postFlush);\n            }\n        }\n\n        Runnable truncateRunnable = new Runnable()\n        {\n            public void run()\n            {\n                logger.debug(\"Discarding sstable data for truncated CF + indexes\");\n\n                final long truncatedAt = System.currentTimeMillis();\n                data.notifyTruncated(truncatedAt);\n\n                if (DatabaseDescriptor.isAutoSnapshot())\n                    snapshot(Keyspace.getTimestampedSnapshotName(name));\n\n                ReplayPosition replayAfter = discardSSTables(truncatedAt);\n\n                for (SecondaryIndex index : indexManager.getIndexes())\n                    index.truncateBlocking(truncatedAt);\n\n                SystemKeyspace.saveTruncationRecord(ColumnFamilyStore.this, truncatedAt, replayAfter);\n                logger.debug(\"cleaning out row cache\");\n                invalidateCaches();\n            }\n        };\n\n        runWithCompactionsDisabled(Executors.callable(truncateRunnable), true);\n        logger.debug(\"truncate complete\");\n    }",
            "2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637 +\n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663 +\n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677 +\n2678  \n2679  \n2680  \n2681  \n2682  \n2683 +\n2684  ",
            "    /**\n     * Truncate deletes the entire column family's data with no expensive tombstone creation\n     */\n    public void truncateBlocking()\n    {\n        // We have two goals here:\n        // - truncate should delete everything written before truncate was invoked\n        // - but not delete anything that isn't part of the snapshot we create.\n        // We accomplish this by first flushing manually, then snapshotting, and\n        // recording the timestamp IN BETWEEN those actions. Any sstables created\n        // with this timestamp or greater time, will not be marked for delete.\n        //\n        // Bonus complication: since we store replay position in sstable metadata,\n        // truncating those sstables means we will replay any CL segments from the\n        // beginning if we restart before they [the CL segments] are discarded for\n        // normal reasons post-truncate.  To prevent this, we store truncation\n        // position in the System keyspace.\n        logger.trace(\"truncating {}\", name);\n\n        if (keyspace.getMetadata().durableWrites || DatabaseDescriptor.isAutoSnapshot())\n        {\n            // flush the CF being truncated before forcing the new segment\n            forceBlockingFlush();\n\n            // sleep a little to make sure that our truncatedAt comes after any sstable\n            // that was part of the flushed we forced; otherwise on a tie, it won't get deleted.\n            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            // just nuke the memtable data w/o writing to disk first\n            synchronized (data)\n            {\n                final Flush flush = new Flush(true);\n                flushExecutor.execute(flush);\n                postFlushExecutor.submit(flush.postFlush);\n            }\n        }\n\n        Runnable truncateRunnable = new Runnable()\n        {\n            public void run()\n            {\n                logger.trace(\"Discarding sstable data for truncated CF + indexes\");\n\n                final long truncatedAt = System.currentTimeMillis();\n                data.notifyTruncated(truncatedAt);\n\n                if (DatabaseDescriptor.isAutoSnapshot())\n                    snapshot(Keyspace.getTimestampedSnapshotName(name));\n\n                ReplayPosition replayAfter = discardSSTables(truncatedAt);\n\n                for (SecondaryIndex index : indexManager.getIndexes())\n                    index.truncateBlocking(truncatedAt);\n\n                SystemKeyspace.saveTruncationRecord(ColumnFamilyStore.this, truncatedAt, replayAfter);\n                logger.trace(\"cleaning out row cache\");\n                invalidateCaches();\n            }\n        };\n\n        runWithCompactionsDisabled(Executors.callable(truncateRunnable), true);\n        logger.trace(\"truncate complete\");\n    }"
        ],
        [
            "ColumnFamilyStore::ColumnFamilyStore(Keyspace,String,IPartitioner,int,CFMetaData,Directories,boolean,boolean)",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425 -\n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              IPartitioner partitioner,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        name = columnFamilyName;\n        this.metadata = metadata;\n        this.minCompactionThreshold = new DefaultInteger(metadata.getMinCompactionThreshold());\n        this.maxCompactionThreshold = new DefaultInteger(metadata.getMaxCompactionThreshold());\n        this.partitioner = partitioner;\n        this.directories = directories;\n        this.indexManager = new SecondaryIndexManager(this);\n        this.metric = new ColumnFamilyMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister().skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata, this.partitioner);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        this.compactionStrategyWrapper = new WrappingCompactionStrategy(this);\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyWrapper.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (ColumnDefinition info : metadata.allColumns())\n        {\n            if (info.getIndexType() != null)\n                indexManager.addIndexedColumn(info);\n        }\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            String type = this.partitioner instanceof LocalPartitioner ? \"IndexColumnFamilies\" : \"ColumnFamilies\";\n            mbeanName = \"org.apache.cassandra.db:type=\" + type + \",keyspace=\" + this.keyspace.getName() + \",columnfamily=\" + name;\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName nameObj = new ObjectName(mbeanName);\n                mbs.registerMBean(this, nameObj);\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.debug(\"retryPolicy for {} is {}\", name, this.metadata.getSpeculativeRetry());\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetry retryPolicy = ColumnFamilyStore.this.metadata.getSpeculativeRetry();\n                    switch (retryPolicy.type)\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.value) * 1000d);\n                            break;\n                        case CUSTOM:\n                            // convert to nanos, since configuration is in millisecond\n                            sampleLatencyNanos = (long) (retryPolicy.value * 1000d * 1000d);\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n        }\n    }",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425 +\n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  ",
            "    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                              String columnFamilyName,\n                              IPartitioner partitioner,\n                              int generation,\n                              CFMetaData metadata,\n                              Directories directories,\n                              boolean loadSSTables,\n                              boolean registerBookkeeping)\n    {\n        assert metadata != null : \"null metadata for \" + keyspace + \":\" + columnFamilyName;\n\n        this.keyspace = keyspace;\n        name = columnFamilyName;\n        this.metadata = metadata;\n        this.minCompactionThreshold = new DefaultInteger(metadata.getMinCompactionThreshold());\n        this.maxCompactionThreshold = new DefaultInteger(metadata.getMaxCompactionThreshold());\n        this.partitioner = partitioner;\n        this.directories = directories;\n        this.indexManager = new SecondaryIndexManager(this);\n        this.metric = new ColumnFamilyMetrics(this);\n        fileIndexGenerator.set(generation);\n        sampleLatencyNanos = DatabaseDescriptor.getReadRpcTimeout() / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // scan for sstables corresponding to this cf and load them\n        data = new Tracker(this, loadSSTables);\n\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister().skipTemporary(true);\n            Collection<SSTableReader> sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata, this.partitioner);\n            data.addInitialSSTables(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        this.compactionStrategyWrapper = new WrappingCompactionStrategy(this);\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyWrapper.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        for (ColumnDefinition info : metadata.allColumns())\n        {\n            if (info.getIndexType() != null)\n                indexManager.addIndexedColumn(info);\n        }\n\n        if (registerBookkeeping)\n        {\n            // register the mbean\n            String type = this.partitioner instanceof LocalPartitioner ? \"IndexColumnFamilies\" : \"ColumnFamilies\";\n            mbeanName = \"org.apache.cassandra.db:type=\" + type + \",keyspace=\" + this.keyspace.getName() + \",columnfamily=\" + name;\n            try\n            {\n                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();\n                ObjectName nameObj = new ObjectName(mbeanName);\n                mbs.registerMBean(this, nameObj);\n            }\n            catch (Exception e)\n            {\n                throw new RuntimeException(e);\n            }\n            logger.trace(\"retryPolicy for {} is {}\", name, this.metadata.getSpeculativeRetry());\n            latencyCalculator = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(new Runnable()\n            {\n                public void run()\n                {\n                    SpeculativeRetry retryPolicy = ColumnFamilyStore.this.metadata.getSpeculativeRetry();\n                    switch (retryPolicy.type)\n                    {\n                        case PERCENTILE:\n                            // get percentile in nanos\n                            sampleLatencyNanos = (long) (metric.coordinatorReadLatency.getSnapshot().getValue(retryPolicy.value) * 1000d);\n                            break;\n                        case CUSTOM:\n                            // convert to nanos, since configuration is in millisecond\n                            sampleLatencyNanos = (long) (retryPolicy.value * 1000d * 1000d);\n                            break;\n                        default:\n                            sampleLatencyNanos = Long.MAX_VALUE;\n                            break;\n                    }\n                }\n            }, DatabaseDescriptor.getReadRpcTimeout(), DatabaseDescriptor.getReadRpcTimeout(), TimeUnit.MILLISECONDS);\n        }\n        else\n        {\n            latencyCalculator = ScheduledExecutors.optionalTasks.schedule(Runnables.doNothing(), 0, TimeUnit.NANOSECONDS);\n            mbeanName = null;\n        }\n    }"
        ],
        [
            "CassandraServer::batch_mutate(Map,ConsistencyLevel)",
            " 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950 -\n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  ",
            "    public void batch_mutate(Map<ByteBuffer,Map<String,List<Mutation>>> mutation_map, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = Maps.newLinkedHashMap();\n            for (Map.Entry<ByteBuffer, Map<String, List<Mutation>>> mutationEntry : mutation_map.entrySet())\n            {\n                traceParameters.put(ByteBufferUtil.bytesToHex(mutationEntry.getKey()),\n                                    Joiner.on(\";\").withKeyValueSeparator(\":\").join(mutationEntry.getValue()));\n            }\n            traceParameters.put(\"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"batch_mutate\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"batch_mutate\");\n        }\n\n        try\n        {\n            doInsert(consistency_level, createMutationList(consistency_level, mutation_map, true));\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950 +\n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  ",
            "    public void batch_mutate(Map<ByteBuffer,Map<String,List<Mutation>>> mutation_map, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = Maps.newLinkedHashMap();\n            for (Map.Entry<ByteBuffer, Map<String, List<Mutation>>> mutationEntry : mutation_map.entrySet())\n            {\n                traceParameters.put(ByteBufferUtil.bytesToHex(mutationEntry.getKey()),\n                                    Joiner.on(\";\").withKeyValueSeparator(\":\").join(mutationEntry.getValue()));\n            }\n            traceParameters.put(\"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"batch_mutate\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"batch_mutate\");\n        }\n\n        try\n        {\n            doInsert(consistency_level, createMutationList(consistency_level, mutation_map, true));\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::forceFlush(ReplayPosition)",
            " 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958 -\n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  ",
            "    /**\n     * Flush if there is unflushed data that was written to the CommitLog before @param flushIfDirtyBefore\n     * (inclusive).  If @param flushIfDirtyBefore is null, flush if there is any unflushed data.\n     *\n     * @return a Future such that when the future completes, all data inserted before forceFlush was called,\n     * will be flushed.\n     */\n    public ListenableFuture<?> forceFlush(ReplayPosition flushIfDirtyBefore)\n    {\n        // we synchronize on the data tracker to ensure we don't race against other calls to switchMemtable(),\n        // unnecessarily queueing memtables that are about to be made clean\n        synchronized (data)\n        {\n            // during index build, 2ary index memtables can be dirty even if parent is not.  if so,\n            // we want to flush the 2ary index ones too.\n            boolean clean = true;\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                clean &= cfs.data.getView().getCurrentMemtable().isCleanAfter(flushIfDirtyBefore);\n\n            if (clean)\n            {\n                // We could have a memtable for this column family that is being\n                // flushed. Make sure the future returned wait for that so callers can\n                // assume that any data inserted prior to the call are fully flushed\n                // when the future returns (see #5241).\n                ListenableFutureTask<?> task = ListenableFutureTask.create(new Runnable()\n                {\n                    public void run()\n                    {\n                        logger.debug(\"forceFlush requested but everything is clean in {}\", name);\n                    }\n                }, null);\n                postFlushExecutor.execute(task);\n                return task;\n            }\n\n            return switchMemtable();\n        }\n    }",
            " 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958 +\n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  ",
            "    /**\n     * Flush if there is unflushed data that was written to the CommitLog before @param flushIfDirtyBefore\n     * (inclusive).  If @param flushIfDirtyBefore is null, flush if there is any unflushed data.\n     *\n     * @return a Future such that when the future completes, all data inserted before forceFlush was called,\n     * will be flushed.\n     */\n    public ListenableFuture<?> forceFlush(ReplayPosition flushIfDirtyBefore)\n    {\n        // we synchronize on the data tracker to ensure we don't race against other calls to switchMemtable(),\n        // unnecessarily queueing memtables that are about to be made clean\n        synchronized (data)\n        {\n            // during index build, 2ary index memtables can be dirty even if parent is not.  if so,\n            // we want to flush the 2ary index ones too.\n            boolean clean = true;\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                clean &= cfs.data.getView().getCurrentMemtable().isCleanAfter(flushIfDirtyBefore);\n\n            if (clean)\n            {\n                // We could have a memtable for this column family that is being\n                // flushed. Make sure the future returned wait for that so callers can\n                // assume that any data inserted prior to the call are fully flushed\n                // when the future returns (see #5241).\n                ListenableFutureTask<?> task = ListenableFutureTask.create(new Runnable()\n                {\n                    public void run()\n                    {\n                        logger.trace(\"forceFlush requested but everything is clean in {}\", name);\n                    }\n                }, null);\n                postFlushExecutor.execute(task);\n                return task;\n            }\n\n            return switchMemtable();\n        }\n    }"
        ],
        [
            "ColumnFamilyStore::clearEphemeralSnapshots(Directories)",
            "2387  \n2388  \n2389  \n2390  \n2391 -\n2392  \n2393  \n2394  ",
            "    protected static void clearEphemeralSnapshots(Directories directories)\n    {\n        for (String ephemeralSnapshot : directories.listEphemeralSnapshots())\n        {\n            logger.debug(\"Clearing ephemeral snapshot {} leftover from previous session.\", ephemeralSnapshot);\n            Directories.clearSnapshot(ephemeralSnapshot, directories.getCFDirectories());\n        }\n    }",
            "2387  \n2388  \n2389  \n2390  \n2391 +\n2392  \n2393  \n2394  ",
            "    protected static void clearEphemeralSnapshots(Directories directories)\n    {\n        for (String ephemeralSnapshot : directories.listEphemeralSnapshots())\n        {\n            logger.trace(\"Clearing ephemeral snapshot {} leftover from previous session.\", ephemeralSnapshot);\n            Directories.clearSnapshot(ephemeralSnapshot, directories.getCFDirectories());\n        }\n    }"
        ],
        [
            "SplittingSizeTieredCompactionWriter::append(AbstractCompactedRow)",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118  \n 119  \n 120  ",
            "    @Override\n    public boolean append(AbstractCompactedRow row)\n    {\n        RowIndexEntry rie = sstableWriter.append(row);\n        if (sstableWriter.currentWriter().getOnDiskFilePointer() > currentBytesToWrite && currentRatioIndex < ratios.length - 1) // if we underestimate how many keys we have, the last sstable might get more than we expect\n        {\n            currentRatioIndex++;\n            currentBytesToWrite = Math.round(totalSize * ratios[currentRatioIndex]);\n            long currentPartitionsToWrite = Math.round(ratios[currentRatioIndex] * estimatedTotalKeys);\n            File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(Math.round(totalSize * ratios[currentRatioIndex])));\n            @SuppressWarnings(\"resource\")\n            SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                                                currentPartitionsToWrite,\n                                                                                minRepairedAt,\n                                                                                cfs.metadata,\n                                                                                cfs.partitioner,\n                                                                                new MetadataCollector(allSSTables, cfs.metadata.comparator, 0));\n            sstableWriter.switchWriter(writer);\n            logger.debug(\"Switching writer, currentPartitionsToWrite = {}\", currentPartitionsToWrite);\n        }\n        return rie != null;\n    }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120  ",
            "    @Override\n    public boolean append(AbstractCompactedRow row)\n    {\n        RowIndexEntry rie = sstableWriter.append(row);\n        if (sstableWriter.currentWriter().getOnDiskFilePointer() > currentBytesToWrite && currentRatioIndex < ratios.length - 1) // if we underestimate how many keys we have, the last sstable might get more than we expect\n        {\n            currentRatioIndex++;\n            currentBytesToWrite = Math.round(totalSize * ratios[currentRatioIndex]);\n            long currentPartitionsToWrite = Math.round(ratios[currentRatioIndex] * estimatedTotalKeys);\n            File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(Math.round(totalSize * ratios[currentRatioIndex])));\n            @SuppressWarnings(\"resource\")\n            SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                                                currentPartitionsToWrite,\n                                                                                minRepairedAt,\n                                                                                cfs.metadata,\n                                                                                cfs.partitioner,\n                                                                                new MetadataCollector(allSSTables, cfs.metadata.comparator, 0));\n            sstableWriter.switchWriter(writer);\n            logger.trace(\"Switching writer, currentPartitionsToWrite = {}\", currentPartitionsToWrite);\n        }\n        return rie != null;\n    }"
        ],
        [
            "Memtable::FlushRunnable::writeSortedContents(ReplayPosition,File)",
            " 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369 -\n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 -\n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 -\n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  ",
            "        private SSTableReader writeSortedContents(ReplayPosition context, File sstableDirectory)\n        {\n            logger.info(\"Writing {}\", Memtable.this.toString());\n\n            SSTableReader ssTable;\n            // errors when creating the writer that may leave empty temp files.\n            try (SSTableWriter writer = createFlushWriter(cfs.getTempSSTablePath(sstableDirectory)))\n            {\n                boolean trackContention = logger.isDebugEnabled();\n                int heavilyContendedRowCount = 0;\n                // (we can't clear out the map as-we-go to free up memory,\n                //  since the memtable is being used for queries in the \"pending flush\" category)\n                for (Map.Entry<RowPosition, AtomicBTreeColumns> entry : rows.entrySet())\n                {\n                    AtomicBTreeColumns cf = entry.getValue();\n\n                    if (cf.isMarkedForDelete() && cf.hasColumns())\n                    {\n                        // When every node is up, there's no reason to write batchlog data out to sstables\n                        // (which in turn incurs cost like compaction) since the BL write + delete cancel each other out,\n                        // and BL data is strictly local, so we don't need to preserve tombstones for repair.\n                        // If we have a data row + row level tombstone, then writing it is effectively an expensive no-op so we skip it.\n                        // See CASSANDRA-4667.\n                        if (cfs.name.equals(SystemKeyspace.BATCHLOG) && cfs.keyspace.getName().equals(SystemKeyspace.NAME))\n                            continue;\n                    }\n\n                    if (trackContention && cf.usePessimisticLocking())\n                        heavilyContendedRowCount++;\n\n                    if (!cf.isEmpty())\n                        writer.append((DecoratedKey)entry.getKey(), cf);\n                }\n\n                if (writer.getFilePointer() > 0)\n                {\n                    logger.info(String.format(\"Completed flushing %s (%s) for commitlog position %s\",\n                                              writer.getFilename(),\n                                              FBUtilities.prettyPrintMemory(writer.getOnDiskFilePointer()),\n                                              context));\n\n                    // temp sstables should contain non-repaired data.\n                    ssTable = writer.finish(true);\n                }\n                else\n                {\n                    logger.info(\"Completed flushing {}; nothing needed to be retained.  Commitlog position was {}\",\n                                writer.getFilename(), context);\n                    writer.abort();\n                    ssTable = null;\n                }\n\n                if (heavilyContendedRowCount > 0)\n                    logger.debug(String.format(\"High update contention in %d/%d partitions of %s \", heavilyContendedRowCount, rows.size(), Memtable.this.toString()));\n\n                return ssTable;\n            }\n        }",
            " 361  \n 362  \n 363 +\n 364  \n 365  \n 366  \n 367  \n 368  \n 369 +\n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 +\n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 +\n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 +\n 415  \n 416  \n 417  \n 418  ",
            "        private SSTableReader writeSortedContents(ReplayPosition context, File sstableDirectory)\n        {\n            logger.debug(\"Writing {}\", Memtable.this.toString());\n\n            SSTableReader ssTable;\n            // errors when creating the writer that may leave empty temp files.\n            try (SSTableWriter writer = createFlushWriter(cfs.getTempSSTablePath(sstableDirectory)))\n            {\n                boolean trackContention = logger.isTraceEnabled();\n                int heavilyContendedRowCount = 0;\n                // (we can't clear out the map as-we-go to free up memory,\n                //  since the memtable is being used for queries in the \"pending flush\" category)\n                for (Map.Entry<RowPosition, AtomicBTreeColumns> entry : rows.entrySet())\n                {\n                    AtomicBTreeColumns cf = entry.getValue();\n\n                    if (cf.isMarkedForDelete() && cf.hasColumns())\n                    {\n                        // When every node is up, there's no reason to write batchlog data out to sstables\n                        // (which in turn incurs cost like compaction) since the BL write + delete cancel each other out,\n                        // and BL data is strictly local, so we don't need to preserve tombstones for repair.\n                        // If we have a data row + row level tombstone, then writing it is effectively an expensive no-op so we skip it.\n                        // See CASSANDRA-4667.\n                        if (cfs.name.equals(SystemKeyspace.BATCHLOG) && cfs.keyspace.getName().equals(SystemKeyspace.NAME))\n                            continue;\n                    }\n\n                    if (trackContention && cf.usePessimisticLocking())\n                        heavilyContendedRowCount++;\n\n                    if (!cf.isEmpty())\n                        writer.append((DecoratedKey)entry.getKey(), cf);\n                }\n\n                if (writer.getFilePointer() > 0)\n                {\n                    logger.debug(String.format(\"Completed flushing %s (%s) for commitlog position %s\",\n                                              writer.getFilename(),\n                                              FBUtilities.prettyPrintMemory(writer.getOnDiskFilePointer()),\n                                              context));\n\n                    // temp sstables should contain non-repaired data.\n                    ssTable = writer.finish(true);\n                }\n                else\n                {\n                    logger.debug(\"Completed flushing {}; nothing needed to be retained.  Commitlog position was {}\",\n                                writer.getFilename(), context);\n                    writer.abort();\n                    ssTable = null;\n                }\n\n                if (heavilyContendedRowCount > 0)\n                    logger.trace(String.format(\"High update contention in %d/%d partitions of %s \", heavilyContendedRowCount, rows.size(), Memtable.this.toString()));\n\n                return ssTable;\n            }\n        }"
        ],
        [
            "CassandraServer::truncate(String)",
            "1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677 -\n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  ",
            "    public void truncate(String cfname) throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        ClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, cfname, Permission.MODIFY);\n\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"truncate\", ImmutableMap.of(\"cf\", cfname, \"ks\", keyspace));\n            }\n            else\n            {\n                logger.debug(\"truncating {}.{}\", cState.getKeyspace(), cfname);\n            }\n\n            schedule(DatabaseDescriptor.getTruncateRpcTimeout());\n            try\n            {\n                StorageProxy.truncateBlocking(cState.getKeyspace(), cfname);\n            }\n            finally\n            {\n                release();\n            }\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (org.apache.cassandra.exceptions.UnavailableException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (TimeoutException e)\n        {\n            throw new TimedOutException();\n        }\n        catch (IOException e)\n        {\n            throw (UnavailableException) new UnavailableException().initCause(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677 +\n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  ",
            "    public void truncate(String cfname) throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        ClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, cfname, Permission.MODIFY);\n\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"truncate\", ImmutableMap.of(\"cf\", cfname, \"ks\", keyspace));\n            }\n            else\n            {\n                logger.trace(\"truncating {}.{}\", cState.getKeyspace(), cfname);\n            }\n\n            schedule(DatabaseDescriptor.getTruncateRpcTimeout());\n            try\n            {\n                StorageProxy.truncateBlocking(cState.getKeyspace(), cfname);\n            }\n            finally\n            {\n                release();\n            }\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (org.apache.cassandra.exceptions.UnavailableException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (TimeoutException e)\n        {\n            throw new TimedOutException();\n        }\n        catch (IOException e)\n        {\n            throw (UnavailableException) new UnavailableException().initCause(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "PasswordAuthenticator::authenticate(String,String)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  ",
            "    private AuthenticatedUser authenticate(String username, String password) throws AuthenticationException\n    {\n        try\n        {\n            // If the legacy users table exists try to verify credentials there. This is to handle the case\n            // where the cluster is being upgraded and so is running with mixed versions of the authn tables\n            SelectStatement authenticationStatement = Schema.instance.getCFMetaData(AuthKeyspace.NAME, LEGACY_CREDENTIALS_TABLE) == null\n                                                    ? authenticateStatement\n                                                    : legacyAuthenticateStatement;\n            return doAuthenticate(username, password, authenticationStatement);\n        }\n        catch (RequestExecutionException e)\n        {\n            logger.debug(\"Error performing internal authentication\", e);\n            throw new AuthenticationException(e.toString());\n        }\n    }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  ",
            "    private AuthenticatedUser authenticate(String username, String password) throws AuthenticationException\n    {\n        try\n        {\n            // If the legacy users table exists try to verify credentials there. This is to handle the case\n            // where the cluster is being upgraded and so is running with mixed versions of the authn tables\n            SelectStatement authenticationStatement = Schema.instance.getCFMetaData(AuthKeyspace.NAME, LEGACY_CREDENTIALS_TABLE) == null\n                                                    ? authenticateStatement\n                                                    : legacyAuthenticateStatement;\n            return doAuthenticate(username, password, authenticationStatement);\n        }\n        catch (RequestExecutionException e)\n        {\n            logger.trace(\"Error performing internal authentication\", e);\n            throw new AuthenticationException(e.toString());\n        }\n    }"
        ],
        [
            "SchemaCheckVerbHandler::doVerb(MessageIn,int)",
            "  36  \n  37  \n  38 -\n  39  \n  40  \n  41  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        logger.debug(\"Received schema check request.\");\n        MessageOut<UUID> response = new MessageOut<UUID>(MessagingService.Verb.INTERNAL_RESPONSE, Schema.instance.getVersion(), UUIDSerializer.serializer);\n        MessagingService.instance().sendReply(response, id, message.from);\n    }",
            "  36  \n  37  \n  38 +\n  39  \n  40  \n  41  ",
            "    public void doVerb(MessageIn message, int id)\n    {\n        logger.trace(\"Received schema check request.\");\n        MessageOut<UUID> response = new MessageOut<UUID>(MessagingService.Verb.INTERNAL_RESPONSE, Schema.instance.getVersion(), UUIDSerializer.serializer);\n        MessagingService.instance().sendReply(response, id, message.from);\n    }"
        ],
        [
            "BatchlogManager::Batch::finish()",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307 -\n 308  \n 309  \n 310  \n 311  \n 312  \n 313  ",
            "        public void finish()\n        {\n            for (int i = 0; i < replayHandlers.size(); i++)\n            {\n                ReplayWriteResponseHandler<Mutation> handler = replayHandlers.get(i);\n                try\n                {\n                    handler.get();\n                }\n                catch (WriteTimeoutException|WriteFailureException e)\n                {\n                    logger.debug(\"Failed replaying a batched mutation to a node, will write a hint\");\n                    logger.debug(\"Failure was : {}\", e.getMessage());\n                    // writing hints for the rest to hints, starting from i\n                    writeHintsForUndeliveredEndpoints(i);\n                    return;\n                }\n            }\n        }",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307 +\n 308  \n 309  \n 310  \n 311  \n 312  \n 313  ",
            "        public void finish()\n        {\n            for (int i = 0; i < replayHandlers.size(); i++)\n            {\n                ReplayWriteResponseHandler<Mutation> handler = replayHandlers.get(i);\n                try\n                {\n                    handler.get();\n                }\n                catch (WriteTimeoutException|WriteFailureException e)\n                {\n                    logger.trace(\"Failed replaying a batched mutation to a node, will write a hint\");\n                    logger.trace(\"Failure was : {}\", e.getMessage());\n                    // writing hints for the rest to hints, starting from i\n                    writeHintsForUndeliveredEndpoints(i);\n                    return;\n                }\n            }\n        }"
        ],
        [
            "CommitLogReplayer::replaySyncSection(FileDataInput,int,CommitLogDescriptor,String,boolean)",
            " 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 -\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437 -\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  ",
            "    /**\n     * Replays a sync section containing a list of mutations.\n     *\n     * @return Whether replay should continue with the next section.\n     */\n    private boolean replaySyncSection(FileDataInput reader, int end, CommitLogDescriptor desc, String errorContext, boolean tolerateErrors) throws IOException\n    {\n         /* read the logs populate Mutation and apply */\n        while (reader.getFilePointer() < end && !reader.isEOF())\n        {\n            long mutationStart = reader.getFilePointer();\n            if (logger.isDebugEnabled())\n                logger.trace(\"Reading mutation at {}\", mutationStart);\n\n            long claimedCRC32;\n            int serializedSize;\n            try\n            {\n                // any of the reads may hit EOF\n                serializedSize = reader.readInt();\n                if (serializedSize == LEGACY_END_OF_SEGMENT_MARKER)\n                {\n                    logger.debug(\"Encountered end of segment marker at {}\", reader.getFilePointer());\n                    return false;\n                }\n\n                // Mutation must be at LEAST 10 bytes:\n                // 3 each for a non-empty Keyspace and Key (including the\n                // 2-byte length from writeUTF/writeWithShortLength) and 4 bytes for column count.\n                // This prevents CRC by being fooled by special-case garbage in the file; see CASSANDRA-2128\n                if (serializedSize < 10)\n                {\n                    handleReplayError(tolerateErrors,\n                                      \"Invalid mutation size %d at %d in %s\",\n                                      serializedSize, mutationStart, errorContext);\n                    return false;\n                }\n\n                long claimedSizeChecksum;\n                if (desc.version < CommitLogDescriptor.VERSION_21)\n                    claimedSizeChecksum = reader.readLong();\n                else\n                    claimedSizeChecksum = reader.readInt() & 0xffffffffL;\n                checksum.reset();\n                if (desc.version < CommitLogDescriptor.VERSION_20)\n                    checksum.update(serializedSize);\n                else\n                    checksum.updateInt(serializedSize);\n\n                if (checksum.getValue() != claimedSizeChecksum)\n                {\n                    handleReplayError(tolerateErrors,\n                                      \"Mutation size checksum failure at %d in %s\",\n                                      mutationStart, errorContext);\n                    return false;\n                }\n                // ok.\n\n                if (serializedSize > buffer.length)\n                    buffer = new byte[(int) (1.2 * serializedSize)];\n                reader.readFully(buffer, 0, serializedSize);\n                if (desc.version < CommitLogDescriptor.VERSION_21)\n                    claimedCRC32 = reader.readLong();\n                else\n                    claimedCRC32 = reader.readInt() & 0xffffffffL;\n            }\n            catch (EOFException eof)\n            {\n                handleReplayError(tolerateErrors,\n                                  \"Unexpected end of segment\",\n                                  mutationStart, errorContext);\n                return false; // last CL entry didn't get completely written. that's ok.\n            }\n\n            checksum.update(buffer, 0, serializedSize);\n            if (claimedCRC32 != checksum.getValue())\n            {\n                handleReplayError(tolerateErrors,\n                                  \"Mutation checksum failure at %d in %s\",\n                                  mutationStart, errorContext);\n                continue;\n            }\n            replayMutation(buffer, serializedSize, reader.getFilePointer(), desc);\n        }\n        return true;\n    }",
            " 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 +\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437 +\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  ",
            "    /**\n     * Replays a sync section containing a list of mutations.\n     *\n     * @return Whether replay should continue with the next section.\n     */\n    private boolean replaySyncSection(FileDataInput reader, int end, CommitLogDescriptor desc, String errorContext, boolean tolerateErrors) throws IOException\n    {\n         /* read the logs populate Mutation and apply */\n        while (reader.getFilePointer() < end && !reader.isEOF())\n        {\n            long mutationStart = reader.getFilePointer();\n            if (logger.isTraceEnabled())\n                logger.trace(\"Reading mutation at {}\", mutationStart);\n\n            long claimedCRC32;\n            int serializedSize;\n            try\n            {\n                // any of the reads may hit EOF\n                serializedSize = reader.readInt();\n                if (serializedSize == LEGACY_END_OF_SEGMENT_MARKER)\n                {\n                    logger.trace(\"Encountered end of segment marker at {}\", reader.getFilePointer());\n                    return false;\n                }\n\n                // Mutation must be at LEAST 10 bytes:\n                // 3 each for a non-empty Keyspace and Key (including the\n                // 2-byte length from writeUTF/writeWithShortLength) and 4 bytes for column count.\n                // This prevents CRC by being fooled by special-case garbage in the file; see CASSANDRA-2128\n                if (serializedSize < 10)\n                {\n                    handleReplayError(tolerateErrors,\n                                      \"Invalid mutation size %d at %d in %s\",\n                                      serializedSize, mutationStart, errorContext);\n                    return false;\n                }\n\n                long claimedSizeChecksum;\n                if (desc.version < CommitLogDescriptor.VERSION_21)\n                    claimedSizeChecksum = reader.readLong();\n                else\n                    claimedSizeChecksum = reader.readInt() & 0xffffffffL;\n                checksum.reset();\n                if (desc.version < CommitLogDescriptor.VERSION_20)\n                    checksum.update(serializedSize);\n                else\n                    checksum.updateInt(serializedSize);\n\n                if (checksum.getValue() != claimedSizeChecksum)\n                {\n                    handleReplayError(tolerateErrors,\n                                      \"Mutation size checksum failure at %d in %s\",\n                                      mutationStart, errorContext);\n                    return false;\n                }\n                // ok.\n\n                if (serializedSize > buffer.length)\n                    buffer = new byte[(int) (1.2 * serializedSize)];\n                reader.readFully(buffer, 0, serializedSize);\n                if (desc.version < CommitLogDescriptor.VERSION_21)\n                    claimedCRC32 = reader.readLong();\n                else\n                    claimedCRC32 = reader.readInt() & 0xffffffffL;\n            }\n            catch (EOFException eof)\n            {\n                handleReplayError(tolerateErrors,\n                                  \"Unexpected end of segment\",\n                                  mutationStart, errorContext);\n                return false; // last CL entry didn't get completely written. that's ok.\n            }\n\n            checksum.update(buffer, 0, serializedSize);\n            if (claimedCRC32 != checksum.getValue())\n            {\n                handleReplayError(tolerateErrors,\n                                  \"Mutation checksum failure at %d in %s\",\n                                  mutationStart, errorContext);\n                continue;\n            }\n            replayMutation(buffer, serializedSize, reader.getFilePointer(), desc);\n        }\n        return true;\n    }"
        ],
        [
            "AbstractSimplePerColumnSecondaryIndex::deleteForCleanup(ByteBuffer,Cell,OpOrder)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105 -\n 106  ",
            "    public void deleteForCleanup(ByteBuffer rowKey, Cell cell, OpOrder.Group opGroup)\n    {\n        if (!cell.isLive())\n            return;\n\n        DecoratedKey valueKey = getIndexKeyFor(getIndexedValue(rowKey, cell));\n        int localDeletionTime = (int) (System.currentTimeMillis() / 1000);\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata, false, 1);\n        cfi.addTombstone(makeIndexColumnName(rowKey, cell), localDeletionTime, cell.timestamp());\n        indexCfs.apply(valueKey, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n        if (logger.isDebugEnabled())\n            logger.debug(\"removed index entry for cleaned-up value {}:{}\", valueKey, cfi);\n    }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106  ",
            "    public void deleteForCleanup(ByteBuffer rowKey, Cell cell, OpOrder.Group opGroup)\n    {\n        if (!cell.isLive())\n            return;\n\n        DecoratedKey valueKey = getIndexKeyFor(getIndexedValue(rowKey, cell));\n        int localDeletionTime = (int) (System.currentTimeMillis() / 1000);\n        ColumnFamily cfi = ArrayBackedSortedColumns.factory.create(indexCfs.metadata, false, 1);\n        cfi.addTombstone(makeIndexColumnName(rowKey, cell), localDeletionTime, cell.timestamp());\n        indexCfs.apply(valueKey, cfi, SecondaryIndexManager.nullUpdater, opGroup, null);\n        if (logger.isTraceEnabled())\n            logger.trace(\"removed index entry for cleaned-up value {}:{}\", valueKey, cfi);\n    }"
        ],
        [
            "IncomingTcpConnection::receiveMessages()",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    private void receiveMessages() throws IOException\n    {\n        // handshake (true) endpoint versions\n        DataOutputStream out = new DataOutputStream(socket.getOutputStream());\n        // if this version is < the MS version the other node is trying\n        // to connect with, the other node will disconnect\n        out.writeInt(MessagingService.current_version);\n        out.flush();\n        DataInput in = new DataInputStream(socket.getInputStream());\n        int maxVersion = in.readInt();\n        // outbound side will reconnect if necessary to upgrade version\n        assert version <= MessagingService.current_version;\n        from = CompactEndpointSerializationHelper.deserialize(in);\n        // record the (true) version of the endpoint\n        MessagingService.instance().setVersion(from, maxVersion);\n        logger.debug(\"Set version for {} to {} (will use {})\", from, maxVersion, MessagingService.instance().getVersion(from));\n\n        if (compressed)\n        {\n            logger.debug(\"Upgrading incoming connection to be compressed\");\n            if (version < MessagingService.VERSION_21)\n            {\n                in = new DataInputStream(new SnappyInputStream(socket.getInputStream()));\n            }\n            else\n            {\n                LZ4FastDecompressor decompressor = LZ4Factory.fastestInstance().fastDecompressor();\n                Checksum checksum = XXHashFactory.fastestInstance().newStreamingHash32(OutboundTcpConnection.LZ4_HASH_SEED).asChecksum();\n                in = new DataInputStream(new LZ4BlockInputStream(socket.getInputStream(),\n                                                                 decompressor,\n                                                                 checksum));\n            }\n        }\n        else\n        {\n            ReadableByteChannel channel = socket.getChannel();\n            in = new NIODataInputStream(channel != null ? channel : Channels.newChannel(socket.getInputStream()), BUFFER_SIZE);\n        }\n\n        while (true)\n        {\n            MessagingService.validateMagic(in.readInt());\n            receiveMessage(in, version);\n        }\n    }",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    private void receiveMessages() throws IOException\n    {\n        // handshake (true) endpoint versions\n        DataOutputStream out = new DataOutputStream(socket.getOutputStream());\n        // if this version is < the MS version the other node is trying\n        // to connect with, the other node will disconnect\n        out.writeInt(MessagingService.current_version);\n        out.flush();\n        DataInput in = new DataInputStream(socket.getInputStream());\n        int maxVersion = in.readInt();\n        // outbound side will reconnect if necessary to upgrade version\n        assert version <= MessagingService.current_version;\n        from = CompactEndpointSerializationHelper.deserialize(in);\n        // record the (true) version of the endpoint\n        MessagingService.instance().setVersion(from, maxVersion);\n        logger.trace(\"Set version for {} to {} (will use {})\", from, maxVersion, MessagingService.instance().getVersion(from));\n\n        if (compressed)\n        {\n            logger.trace(\"Upgrading incoming connection to be compressed\");\n            if (version < MessagingService.VERSION_21)\n            {\n                in = new DataInputStream(new SnappyInputStream(socket.getInputStream()));\n            }\n            else\n            {\n                LZ4FastDecompressor decompressor = LZ4Factory.fastestInstance().fastDecompressor();\n                Checksum checksum = XXHashFactory.fastestInstance().newStreamingHash32(OutboundTcpConnection.LZ4_HASH_SEED).asChecksum();\n                in = new DataInputStream(new LZ4BlockInputStream(socket.getInputStream(),\n                                                                 decompressor,\n                                                                 checksum));\n            }\n        }\n        else\n        {\n            ReadableByteChannel channel = socket.getChannel();\n            in = new NIODataInputStream(channel != null ? channel : Channels.newChannel(socket.getInputStream()), BUFFER_SIZE);\n        }\n\n        while (true)\n        {\n            MessagingService.validateMagic(in.readInt());\n            receiveMessage(in, version);\n        }\n    }"
        ],
        [
            "CommitLogReplayer::construct(CommitLog)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  ",
            "    public static CommitLogReplayer construct(CommitLog commitLog)\n    {\n        // compute per-CF and global replay positions\n        Map<UUID, ReplayPosition> cfPositions = new HashMap<UUID, ReplayPosition>();\n        Ordering<ReplayPosition> replayPositionOrdering = Ordering.from(ReplayPosition.comparator);\n        ReplayFilter replayFilter = ReplayFilter.create();\n        for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n        {\n            // it's important to call RP.gRP per-cf, before aggregating all the positions w/ the Ordering.min call\n            // below: gRP will return NONE if there are no flushed sstables, which is important to have in the\n            // list (otherwise we'll just start replay from the first flush position that we do have, which is not correct).\n            ReplayPosition rp = ReplayPosition.getReplayPosition(cfs.getSSTables());\n\n            // but, if we've truncated the cf in question, then we need to need to start replay after the truncation\n            ReplayPosition truncatedAt = SystemKeyspace.getTruncatedPosition(cfs.metadata.cfId);\n            if (truncatedAt != null)\n            {\n                // Point in time restore is taken to mean that the tables need to be recovered even if they were\n                // deleted at a later point in time. Any truncation record after that point must thus be cleared prior\n                // to recovery (CASSANDRA-9195).\n                long restoreTime = commitLog.archiver.restorePointInTime;\n                long truncatedTime = SystemKeyspace.getTruncatedAt(cfs.metadata.cfId);\n                if (truncatedTime > restoreTime)\n                {\n                    if (replayFilter.includes(cfs.metadata))\n                    {\n                        logger.info(\"Restore point in time is before latest truncation of table {}.{}. Clearing truncation record.\",\n                                    cfs.metadata.ksName,\n                                    cfs.metadata.cfName);\n                        SystemKeyspace.removeTruncationRecord(cfs.metadata.cfId);\n                    }\n                }\n                else\n                {\n                    rp = replayPositionOrdering.max(Arrays.asList(rp, truncatedAt));\n                }\n            }\n\n            cfPositions.put(cfs.metadata.cfId, rp);\n        }\n        ReplayPosition globalPosition = replayPositionOrdering.min(cfPositions.values());\n        logger.debug(\"Global replay position is {} from columnfamilies {}\", globalPosition, FBUtilities.toString(cfPositions));\n        return new CommitLogReplayer(commitLog, globalPosition, cfPositions, replayFilter);\n    }",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140  \n 141  ",
            "    public static CommitLogReplayer construct(CommitLog commitLog)\n    {\n        // compute per-CF and global replay positions\n        Map<UUID, ReplayPosition> cfPositions = new HashMap<UUID, ReplayPosition>();\n        Ordering<ReplayPosition> replayPositionOrdering = Ordering.from(ReplayPosition.comparator);\n        ReplayFilter replayFilter = ReplayFilter.create();\n        for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n        {\n            // it's important to call RP.gRP per-cf, before aggregating all the positions w/ the Ordering.min call\n            // below: gRP will return NONE if there are no flushed sstables, which is important to have in the\n            // list (otherwise we'll just start replay from the first flush position that we do have, which is not correct).\n            ReplayPosition rp = ReplayPosition.getReplayPosition(cfs.getSSTables());\n\n            // but, if we've truncated the cf in question, then we need to need to start replay after the truncation\n            ReplayPosition truncatedAt = SystemKeyspace.getTruncatedPosition(cfs.metadata.cfId);\n            if (truncatedAt != null)\n            {\n                // Point in time restore is taken to mean that the tables need to be recovered even if they were\n                // deleted at a later point in time. Any truncation record after that point must thus be cleared prior\n                // to recovery (CASSANDRA-9195).\n                long restoreTime = commitLog.archiver.restorePointInTime;\n                long truncatedTime = SystemKeyspace.getTruncatedAt(cfs.metadata.cfId);\n                if (truncatedTime > restoreTime)\n                {\n                    if (replayFilter.includes(cfs.metadata))\n                    {\n                        logger.info(\"Restore point in time is before latest truncation of table {}.{}. Clearing truncation record.\",\n                                    cfs.metadata.ksName,\n                                    cfs.metadata.cfName);\n                        SystemKeyspace.removeTruncationRecord(cfs.metadata.cfId);\n                    }\n                }\n                else\n                {\n                    rp = replayPositionOrdering.max(Arrays.asList(rp, truncatedAt));\n                }\n            }\n\n            cfPositions.put(cfs.metadata.cfId, rp);\n        }\n        ReplayPosition globalPosition = replayPositionOrdering.min(cfPositions.values());\n        logger.trace(\"Global replay position is {} from columnfamilies {}\", globalPosition, FBUtilities.toString(cfPositions));\n        return new CommitLogReplayer(commitLog, globalPosition, cfPositions, replayFilter);\n    }"
        ],
        [
            "SSTableReader::openForBatch(Descriptor,Set,CFMetaData,IPartitioner)",
            " 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402 -\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * Open SSTable reader to be used in batch mode(such as sstableloader).\n     *\n     * @param descriptor\n     * @param components\n     * @param metadata\n     * @param partitioner\n     * @return opened SSTableReader\n     * @throws IOException\n     */\n    public static SSTableReader openForBatch(Descriptor descriptor, Set<Component> components, CFMetaData metadata, IPartitioner partitioner) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        Map<MetadataType, MetadataComponent> sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor,\n                EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS));\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.info(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor, components, metadata, partitioner, System.currentTimeMillis(),\n                statsMetadata, OpenReason.NORMAL);\n\n        // special implementation of load to use non-pooled SegmentedFile builders\n        try(SegmentedFile.Builder ibuilder = new BufferedSegmentedFile.Builder();\n            SegmentedFile.Builder dbuilder = sstable.compression\n                ? new CompressedSegmentedFile.Builder(null)\n                : new BufferedSegmentedFile.Builder())\n        {\n            if (!sstable.loadSummary(ibuilder, dbuilder))\n                sstable.buildSummary(false, ibuilder, dbuilder, false, Downsampling.BASE_SAMPLING_LEVEL);\n            sstable.ifile = ibuilder.complete(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX));\n            sstable.dfile = dbuilder.complete(sstable.descriptor.filenameFor(Component.DATA));\n            sstable.bf = FilterFactory.AlwaysPresent;\n            sstable.setup(false);\n            return sstable;\n        }\n    }",
            " 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    /**\n     * Open SSTable reader to be used in batch mode(such as sstableloader).\n     *\n     * @param descriptor\n     * @param components\n     * @param metadata\n     * @param partitioner\n     * @return opened SSTableReader\n     * @throws IOException\n     */\n    public static SSTableReader openForBatch(Descriptor descriptor, Set<Component> components, CFMetaData metadata, IPartitioner partitioner) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        Map<MetadataType, MetadataComponent> sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor,\n                EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS));\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.debug(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor, components, metadata, partitioner, System.currentTimeMillis(),\n                statsMetadata, OpenReason.NORMAL);\n\n        // special implementation of load to use non-pooled SegmentedFile builders\n        try(SegmentedFile.Builder ibuilder = new BufferedSegmentedFile.Builder();\n            SegmentedFile.Builder dbuilder = sstable.compression\n                ? new CompressedSegmentedFile.Builder(null)\n                : new BufferedSegmentedFile.Builder())\n        {\n            if (!sstable.loadSummary(ibuilder, dbuilder))\n                sstable.buildSummary(false, ibuilder, dbuilder, false, Downsampling.BASE_SAMPLING_LEVEL);\n            sstable.ifile = ibuilder.complete(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX));\n            sstable.dfile = dbuilder.complete(sstable.descriptor.filenameFor(Component.DATA));\n            sstable.bf = FilterFactory.AlwaysPresent;\n            sstable.setup(false);\n            return sstable;\n        }\n    }"
        ],
        [
            "LifecycleTransaction::obsolete(SSTableReader)",
            " 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 -\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  ",
            "    /**\n     * mark this reader as for obsoletion. this does not actually obsolete the reader until commit() is called,\n     * but on checkpoint() the reader will be removed from the live set\n     */\n    public void obsolete(SSTableReader reader)\n    {\n        logger.debug(\"Staging for obsolescence {}\", reader);\n        // check this is: a reader guarded by the transaction, an instance we have already worked with\n        // and that we haven't already obsoleted it, nor do we have other changes staged for it\n        assert identities.contains(reader.instanceId) : \"only reader instances that have previously been provided may be obsoleted: \" + reader;\n        assert originals.contains(reader) : \"only readers in the 'original' set may be obsoleted: \" + reader + \" vs \" + originals;\n        assert !(logged.obsolete.contains(reader) || staged.obsolete.contains(reader)) : \"may not obsolete a reader that has already been obsoleted: \" + reader;\n        assert !staged.update.contains(reader) : \"may not obsolete a reader that has a staged update (must checkpoint first): \" + reader;\n        assert current(reader) == reader : \"may only obsolete the latest version of the reader: \" + reader;\n        staged.obsolete.add(reader);\n    }",
            " 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 +\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  ",
            "    /**\n     * mark this reader as for obsoletion. this does not actually obsolete the reader until commit() is called,\n     * but on checkpoint() the reader will be removed from the live set\n     */\n    public void obsolete(SSTableReader reader)\n    {\n        logger.trace(\"Staging for obsolescence {}\", reader);\n        // check this is: a reader guarded by the transaction, an instance we have already worked with\n        // and that we haven't already obsoleted it, nor do we have other changes staged for it\n        assert identities.contains(reader.instanceId) : \"only reader instances that have previously been provided may be obsoleted: \" + reader;\n        assert originals.contains(reader) : \"only readers in the 'original' set may be obsoleted: \" + reader + \" vs \" + originals;\n        assert !(logged.obsolete.contains(reader) || staged.obsolete.contains(reader)) : \"may not obsolete a reader that has already been obsoleted: \" + reader;\n        assert !staged.update.contains(reader) : \"may not obsolete a reader that has a staged update (must checkpoint first): \" + reader;\n        assert current(reader) == reader : \"may only obsolete the latest version of the reader: \" + reader;\n        staged.obsolete.add(reader);\n    }"
        ],
        [
            "FileUtils::atomicMoveWithFallback(Path,Path)",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 -\n 202  \n 203  \n 204  \n 205  ",
            "    /**\n     * Move a file atomically, if it fails, it falls back to a non-atomic operation\n     * @param from\n     * @param to\n     * @throws IOException\n     */\n    private static void atomicMoveWithFallback(Path from, Path to) throws IOException\n    {\n        try\n        {\n            Files.move(from, to, StandardCopyOption.REPLACE_EXISTING, StandardCopyOption.ATOMIC_MOVE);\n        }\n        catch (AtomicMoveNotSupportedException e)\n        {\n            logger.debug(\"Could not do an atomic move\", e);\n            Files.move(from, to, StandardCopyOption.REPLACE_EXISTING);\n        }\n\n    }",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202  \n 203  \n 204  \n 205  ",
            "    /**\n     * Move a file atomically, if it fails, it falls back to a non-atomic operation\n     * @param from\n     * @param to\n     * @throws IOException\n     */\n    private static void atomicMoveWithFallback(Path from, Path to) throws IOException\n    {\n        try\n        {\n            Files.move(from, to, StandardCopyOption.REPLACE_EXISTING, StandardCopyOption.ATOMIC_MOVE);\n        }\n        catch (AtomicMoveNotSupportedException e)\n        {\n            logger.trace(\"Could not do an atomic move\", e);\n            Files.move(from, to, StandardCopyOption.REPLACE_EXISTING);\n        }\n\n    }"
        ],
        [
            "CompactionManager::performAnticompaction(ColumnFamilyStore,Collection,Refs,LifecycleTransaction,long)",
            " 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 -\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt) throws InterruptedException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.debug(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken());\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            cfs.getTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatuses);\n            txn.cancel(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }",
            " 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 +\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  ",
            "    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getAndReferenceSSTables(..)).\n     *\n     * @param cfs\n     * @param ranges Ranges that the repair was carried out on\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      Collection<Range<Token>> ranges,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      long repairedAt) throws InterruptedException, IOException\n    {\n        logger.info(\"Starting anticompaction for {}.{} on {}/{} sstables\", cfs.keyspace.getName(), cfs.getColumnFamilyName(), validatedForRepair.size(), cfs.getSSTables().size());\n        logger.trace(\"Starting anticompaction for ranges {}\", ranges);\n        Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n        Set<SSTableReader> mutatedRepairStatuses = new HashSet<>();\n        Set<SSTableReader> nonAnticompacting = new HashSet<>();\n        Iterator<SSTableReader> sstableIterator = sstables.iterator();\n        try\n        {\n            List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n            while (sstableIterator.hasNext())\n            {\n                SSTableReader sstable = sstableIterator.next();\n\n                Range<Token> sstableRange = new Range<>(sstable.first.getToken(), sstable.last.getToken());\n\n                boolean shouldAnticompact = false;\n\n                for (Range<Token> r : normalizedRanges)\n                {\n                    if (r.contains(sstableRange))\n                    {\n                        logger.info(\"SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting\", sstable, r);\n                        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, repairedAt);\n                        sstable.reloadSSTableMetadata();\n                        mutatedRepairStatuses.add(sstable);\n                        sstableIterator.remove();\n                        shouldAnticompact = true;\n                        break;\n                    }\n                    else if (sstableRange.intersects(r))\n                    {\n                        logger.info(\"SSTable {} ({}) will be anticompacted on range {}\", sstable, sstableRange, r);\n                        shouldAnticompact = true;\n                    }\n                }\n\n                if (!shouldAnticompact)\n                {\n                    logger.info(\"SSTable {} ({}) does not intersect repaired ranges {}, not touching repairedAt.\", sstable, sstableRange, normalizedRanges);\n                    nonAnticompacting.add(sstable);\n                    sstableIterator.remove();\n                }\n            }\n            cfs.getTracker().notifySSTableRepairedStatusChanged(mutatedRepairStatuses);\n            txn.cancel(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            validatedForRepair.release(Sets.union(nonAnticompacting, mutatedRepairStatuses));\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, ranges, txn, repairedAt);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"Completed anticompaction successfully\");\n    }"
        ],
        [
            "HintedHandOffManager::scheduleAllDeliveries()",
            " 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 -\n 538  ",
            "    /**\n     * Attempt delivery to any node for which we have hints.  Necessary since we can generate hints even for\n     * nodes which are never officially down/failed.\n     */\n    private void scheduleAllDeliveries()\n    {\n        logger.debug(\"Started scheduleAllDeliveries\");\n\n        // Force a major compaction to get rid of the tombstones and expired hints. Do it once, before we schedule any\n        // individual replay, to avoid N - 1 redundant individual compactions (when N is the number of nodes with hints\n        // to deliver to).\n        compact();\n\n        IPartitioner p = StorageService.getPartitioner();\n        RowPosition minPos = p.getMinimumToken().minKeyBound();\n        Range<RowPosition> range = new Range<>(minPos, minPos);\n        IDiskAtomFilter filter = new NamesQueryFilter(ImmutableSortedSet.<CellName>of());\n        List<Row> rows = hintStore.getRangeSlice(range, null, filter, Integer.MAX_VALUE, System.currentTimeMillis());\n        for (Row row : rows)\n        {\n            UUID hostId = UUIDGen.getUUID(row.key.getKey());\n            InetAddress target = StorageService.instance.getTokenMetadata().getEndpointForHostId(hostId);\n            // token may have since been removed (in which case we have just read back a tombstone)\n            if (target != null)\n                scheduleHintDelivery(target, false);\n        }\n\n        logger.debug(\"Finished scheduleAllDeliveries\");\n    }",
            " 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 +\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 +\n 538  ",
            "    /**\n     * Attempt delivery to any node for which we have hints.  Necessary since we can generate hints even for\n     * nodes which are never officially down/failed.\n     */\n    private void scheduleAllDeliveries()\n    {\n        logger.trace(\"Started scheduleAllDeliveries\");\n\n        // Force a major compaction to get rid of the tombstones and expired hints. Do it once, before we schedule any\n        // individual replay, to avoid N - 1 redundant individual compactions (when N is the number of nodes with hints\n        // to deliver to).\n        compact();\n\n        IPartitioner p = StorageService.getPartitioner();\n        RowPosition minPos = p.getMinimumToken().minKeyBound();\n        Range<RowPosition> range = new Range<>(minPos, minPos);\n        IDiskAtomFilter filter = new NamesQueryFilter(ImmutableSortedSet.<CellName>of());\n        List<Row> rows = hintStore.getRangeSlice(range, null, filter, Integer.MAX_VALUE, System.currentTimeMillis());\n        for (Row row : rows)\n        {\n            UUID hostId = UUIDGen.getUUID(row.key.getKey());\n            InetAddress target = StorageService.instance.getTokenMetadata().getEndpointForHostId(hostId);\n            // token may have since been removed (in which case we have just read back a tombstone)\n            if (target != null)\n                scheduleHintDelivery(target, false);\n        }\n\n        logger.trace(\"Finished scheduleAllDeliveries\");\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::init(Cluster,Collection)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  ",
            "    @Override\n    public void init(Cluster cluster, Collection<Host> hosts)\n    {\n        List<Host> replicaHosts = new ArrayList<>();\n        for (Host host : hosts)\n        {\n            if (replicaAddresses.contains(host.getAddress()))\n            {\n                replicaHosts.add(host);\n            }\n        }\n        liveReplicaHosts.addAll(replicaHosts);\n        logger.debug(\"Initialized with replica hosts: {}\", replicaHosts);\n    }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  ",
            "    @Override\n    public void init(Cluster cluster, Collection<Host> hosts)\n    {\n        List<Host> replicaHosts = new ArrayList<>();\n        for (Host host : hosts)\n        {\n            if (replicaAddresses.contains(host.getAddress()))\n            {\n                replicaHosts.add(host);\n            }\n        }\n        liveReplicaHosts.addAll(replicaHosts);\n        logger.trace(\"Initialized with replica hosts: {}\", replicaHosts);\n    }"
        ],
        [
            "IncomingTcpConnection::close()",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "    @Override\n    public void close()\n    {\n        try\n        {\n            if (!socket.isClosed())\n            {\n                socket.close();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"Error closing socket\", e);\n        }\n        finally\n        {\n            group.remove(this);\n        }\n    }",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "    @Override\n    public void close()\n    {\n        try\n        {\n            if (!socket.isClosed())\n            {\n                socket.close();\n            }\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"Error closing socket\", e);\n        }\n        finally\n        {\n            group.remove(this);\n        }\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::onUp(Host)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  ",
            "    @Override\n    public void onUp(Host host)\n    {\n        if (replicaAddresses.contains(host.getAddress()))\n        {\n            liveReplicaHosts.add(host);\n            logger.debug(\"The host {} is now up\", host);\n        }\n    }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  ",
            "    @Override\n    public void onUp(Host host)\n    {\n        if (replicaAddresses.contains(host.getAddress()))\n        {\n            liveReplicaHosts.add(host);\n            logger.trace(\"The host {} is now up\", host);\n        }\n    }"
        ],
        [
            "OutboundTcpConnection::connect()",
            " 371  \n 372  \n 373  \n 374 -\n 375 -\n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421 -\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 -\n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  ",
            "    @SuppressWarnings(\"resource\")\n    private boolean connect()\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"attempting to connect to {}\", poolReference.endPoint());\n\n        long start = System.nanoTime();\n        long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getRpcTimeout());\n        while (System.nanoTime() - start < timeout)\n        {\n            targetVersion = MessagingService.instance().getVersion(poolReference.endPoint());\n            try\n            {\n                socket = poolReference.newSocket();\n                socket.setKeepAlive(true);\n                if (isLocalDC(poolReference.endPoint()))\n                {\n                    socket.setTcpNoDelay(INTRADC_TCP_NODELAY);\n                }\n                else\n                {\n                    socket.setTcpNoDelay(DatabaseDescriptor.getInterDCTcpNoDelay());\n                }\n                if (DatabaseDescriptor.getInternodeSendBufferSize() != null)\n                {\n                    try\n                    {\n                        socket.setSendBufferSize(DatabaseDescriptor.getInternodeSendBufferSize());\n                    }\n                    catch (SocketException se)\n                    {\n                        logger.warn(\"Failed to set send buffer size on internode socket.\", se);\n                    }\n                }\n\n                // SocketChannel may be null when using SSL\n                WritableByteChannel ch = socket.getChannel();\n                out = new BufferedDataOutputStreamPlus(ch != null ? ch : Channels.newChannel(socket.getOutputStream()), BUFFER_SIZE);\n\n                out.writeInt(MessagingService.PROTOCOL_MAGIC);\n                writeHeader(out, targetVersion, shouldCompressConnection());\n                out.flush();\n\n                DataInputStream in = new DataInputStream(socket.getInputStream());\n                int maxTargetVersion = handshakeVersion(in);\n                if (maxTargetVersion == NO_VERSION)\n                {\n                    // no version is returned, so disconnect an try again: we will either get\n                    // a different target version (targetVersion < MessagingService.VERSION_12)\n                    // or if the same version the handshake will finally succeed\n                    logger.debug(\"Target max version is {}; no version information yet, will retry\", maxTargetVersion);\n                    if (DatabaseDescriptor.getSeeds().contains(poolReference.endPoint()))\n                        logger.warn(\"Seed gossip version is {}; will not connect with that version\", maxTargetVersion);\n                    disconnect();\n                    continue;\n                }\n                else\n                {\n                    MessagingService.instance().setVersion(poolReference.endPoint(), maxTargetVersion);\n                }\n\n                if (targetVersion > maxTargetVersion)\n                {\n                    logger.debug(\"Target max version is {}; will reconnect with that version\", maxTargetVersion);\n                    disconnect();\n                    return false;\n                }\n\n                if (targetVersion < maxTargetVersion && targetVersion < MessagingService.current_version)\n                {\n                    logger.trace(\"Detected higher max version {} (using {}); will reconnect when queued messages are done\",\n                                 maxTargetVersion, targetVersion);\n                    softCloseSocket();\n                }\n\n                out.writeInt(MessagingService.current_version);\n                CompactEndpointSerializationHelper.serialize(FBUtilities.getBroadcastAddress(), out);\n                if (shouldCompressConnection())\n                {\n                    out.flush();\n                    logger.trace(\"Upgrading OutputStream to be compressed\");\n                    if (targetVersion < MessagingService.VERSION_21)\n                    {\n                        // Snappy is buffered, so no need for extra buffering output stream\n                        out = new WrappedDataOutputStreamPlus(new SnappyOutputStream(socket.getOutputStream()));\n                    }\n                    else\n                    {\n                        // TODO: custom LZ4 OS that supports BB write methods\n                        LZ4Compressor compressor = LZ4Factory.fastestInstance().fastCompressor();\n                        Checksum checksum = XXHashFactory.fastestInstance().newStreamingHash32(LZ4_HASH_SEED).asChecksum();\n                        out = new WrappedDataOutputStreamPlus(new LZ4BlockOutputStream(socket.getOutputStream(),\n                                                                            1 << 14,  // 16k block size\n                                                                            compressor,\n                                                                            checksum,\n                                                                            true)); // no async flushing\n                    }\n                }\n\n                return true;\n            }\n            catch (IOException e)\n            {\n                socket = null;\n                if (logger.isTraceEnabled())\n                    logger.trace(\"unable to connect to \" + poolReference.endPoint(), e);\n                Uninterruptibles.sleepUninterruptibly(OPEN_RETRY_DELAY, TimeUnit.MILLISECONDS);\n            }\n        }\n        return false;\n    }",
            " 371  \n 372  \n 373  \n 374 +\n 375 +\n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421 +\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 +\n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  ",
            "    @SuppressWarnings(\"resource\")\n    private boolean connect()\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"attempting to connect to {}\", poolReference.endPoint());\n\n        long start = System.nanoTime();\n        long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getRpcTimeout());\n        while (System.nanoTime() - start < timeout)\n        {\n            targetVersion = MessagingService.instance().getVersion(poolReference.endPoint());\n            try\n            {\n                socket = poolReference.newSocket();\n                socket.setKeepAlive(true);\n                if (isLocalDC(poolReference.endPoint()))\n                {\n                    socket.setTcpNoDelay(INTRADC_TCP_NODELAY);\n                }\n                else\n                {\n                    socket.setTcpNoDelay(DatabaseDescriptor.getInterDCTcpNoDelay());\n                }\n                if (DatabaseDescriptor.getInternodeSendBufferSize() != null)\n                {\n                    try\n                    {\n                        socket.setSendBufferSize(DatabaseDescriptor.getInternodeSendBufferSize());\n                    }\n                    catch (SocketException se)\n                    {\n                        logger.warn(\"Failed to set send buffer size on internode socket.\", se);\n                    }\n                }\n\n                // SocketChannel may be null when using SSL\n                WritableByteChannel ch = socket.getChannel();\n                out = new BufferedDataOutputStreamPlus(ch != null ? ch : Channels.newChannel(socket.getOutputStream()), BUFFER_SIZE);\n\n                out.writeInt(MessagingService.PROTOCOL_MAGIC);\n                writeHeader(out, targetVersion, shouldCompressConnection());\n                out.flush();\n\n                DataInputStream in = new DataInputStream(socket.getInputStream());\n                int maxTargetVersion = handshakeVersion(in);\n                if (maxTargetVersion == NO_VERSION)\n                {\n                    // no version is returned, so disconnect an try again: we will either get\n                    // a different target version (targetVersion < MessagingService.VERSION_12)\n                    // or if the same version the handshake will finally succeed\n                    logger.trace(\"Target max version is {}; no version information yet, will retry\", maxTargetVersion);\n                    if (DatabaseDescriptor.getSeeds().contains(poolReference.endPoint()))\n                        logger.warn(\"Seed gossip version is {}; will not connect with that version\", maxTargetVersion);\n                    disconnect();\n                    continue;\n                }\n                else\n                {\n                    MessagingService.instance().setVersion(poolReference.endPoint(), maxTargetVersion);\n                }\n\n                if (targetVersion > maxTargetVersion)\n                {\n                    logger.trace(\"Target max version is {}; will reconnect with that version\", maxTargetVersion);\n                    disconnect();\n                    return false;\n                }\n\n                if (targetVersion < maxTargetVersion && targetVersion < MessagingService.current_version)\n                {\n                    logger.trace(\"Detected higher max version {} (using {}); will reconnect when queued messages are done\",\n                                 maxTargetVersion, targetVersion);\n                    softCloseSocket();\n                }\n\n                out.writeInt(MessagingService.current_version);\n                CompactEndpointSerializationHelper.serialize(FBUtilities.getBroadcastAddress(), out);\n                if (shouldCompressConnection())\n                {\n                    out.flush();\n                    logger.trace(\"Upgrading OutputStream to be compressed\");\n                    if (targetVersion < MessagingService.VERSION_21)\n                    {\n                        // Snappy is buffered, so no need for extra buffering output stream\n                        out = new WrappedDataOutputStreamPlus(new SnappyOutputStream(socket.getOutputStream()));\n                    }\n                    else\n                    {\n                        // TODO: custom LZ4 OS that supports BB write methods\n                        LZ4Compressor compressor = LZ4Factory.fastestInstance().fastCompressor();\n                        Checksum checksum = XXHashFactory.fastestInstance().newStreamingHash32(LZ4_HASH_SEED).asChecksum();\n                        out = new WrappedDataOutputStreamPlus(new LZ4BlockOutputStream(socket.getOutputStream(),\n                                                                            1 << 14,  // 16k block size\n                                                                            compressor,\n                                                                            checksum,\n                                                                            true)); // no async flushing\n                    }\n                }\n\n                return true;\n            }\n            catch (IOException e)\n            {\n                socket = null;\n                if (logger.isTraceEnabled())\n                    logger.trace(\"unable to connect to \" + poolReference.endPoint(), e);\n                Uninterruptibles.sleepUninterruptibly(OPEN_RETRY_DELAY, TimeUnit.MILLISECONDS);\n            }\n        }\n        return false;\n    }"
        ],
        [
            "BootStrapper::getBootstrapTokens(TokenMetadata)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * if initialtoken was specified, use that (split on comma).\n     * otherwise, if num_tokens == 1, pick a token to assume half the load of the most-loaded node.\n     * else choose num_tokens tokens at random\n     */\n    public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata) throws ConfigurationException\n    {\n        Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens();\n        // if user specified tokens, use those\n        if (initialTokens.size() > 0)\n        {\n            logger.debug(\"tokens manually specified as {}\",  initialTokens);\n            List<Token> tokens = new ArrayList<>(initialTokens.size());\n            for (String tokenString : initialTokens)\n            {\n                Token token = StorageService.getPartitioner().getTokenFactory().fromString(tokenString);\n                if (metadata.getEndpoint(token) != null)\n                    throw new ConfigurationException(\"Bootstrapping to existing token \" + tokenString + \" is not allowed (decommission/removenode the old node first).\");\n                tokens.add(token);\n            }\n            return tokens;\n        }\n\n        int numTokens = DatabaseDescriptor.getNumTokens();\n        if (numTokens < 1)\n            throw new ConfigurationException(\"num_tokens must be >= 1\");\n\n        if (numTokens == 1)\n            logger.warn(\"Picking random token for a single vnode.  You should probably add more vnodes; failing that, you should probably specify the token manually\");\n\n        return getRandomTokens(metadata, numTokens);\n    }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * if initialtoken was specified, use that (split on comma).\n     * otherwise, if num_tokens == 1, pick a token to assume half the load of the most-loaded node.\n     * else choose num_tokens tokens at random\n     */\n    public static Collection<Token> getBootstrapTokens(final TokenMetadata metadata) throws ConfigurationException\n    {\n        Collection<String> initialTokens = DatabaseDescriptor.getInitialTokens();\n        // if user specified tokens, use those\n        if (initialTokens.size() > 0)\n        {\n            logger.trace(\"tokens manually specified as {}\",  initialTokens);\n            List<Token> tokens = new ArrayList<>(initialTokens.size());\n            for (String tokenString : initialTokens)\n            {\n                Token token = StorageService.getPartitioner().getTokenFactory().fromString(tokenString);\n                if (metadata.getEndpoint(token) != null)\n                    throw new ConfigurationException(\"Bootstrapping to existing token \" + tokenString + \" is not allowed (decommission/removenode the old node first).\");\n                tokens.add(token);\n            }\n            return tokens;\n        }\n\n        int numTokens = DatabaseDescriptor.getNumTokens();\n        if (numTokens < 1)\n            throw new ConfigurationException(\"num_tokens must be >= 1\");\n\n        if (numTokens == 1)\n            logger.warn(\"Picking random token for a single vnode.  You should probably add more vnodes; failing that, you should probably specify the token manually\");\n\n        return getRandomTokens(metadata, numTokens);\n    }"
        ],
        [
            "CqlNativeStorage::setLocation(String,Job)",
            " 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629 -\n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  ",
            "    /** set read configuration settings */\n    public void setLocation(String location, Job job) throws IOException\n    {\n        conf = job.getConfiguration();\n        setLocationFromUri(location);\n\n        if (username != null && password != null)\n        {\n            ConfigHelper.setInputKeyspaceUserNameAndPassword(conf, username, password);\n            CqlConfigHelper.setUserNameAndPassword(conf, username, password);\n        }\n        if (splitSize > 0)\n            ConfigHelper.setInputSplitSize(conf, splitSize);\n        if (partitionerClass!= null)\n            ConfigHelper.setInputPartitioner(conf, partitionerClass);\n        if (initHostAddress != null)\n            ConfigHelper.setInputInitialAddress(conf, initHostAddress);\n        if (rpcPort != null)\n            ConfigHelper.setInputRpcPort(conf, rpcPort);\n        if (nativePort != null)\n            CqlConfigHelper.setInputNativePort(conf, nativePort);\n        if (nativeCoreConnections != null)\n            CqlConfigHelper.setInputCoreConnections(conf, nativeCoreConnections);\n        if (nativeMaxConnections != null)\n            CqlConfigHelper.setInputMaxConnections(conf, nativeMaxConnections);\n        if (nativeMaxSimultReqs != null)\n            CqlConfigHelper.setInputMaxSimultReqPerConnections(conf, nativeMaxSimultReqs);\n        if (nativeConnectionTimeout != null)\n            CqlConfigHelper.setInputNativeConnectionTimeout(conf, nativeConnectionTimeout);\n        if (nativeReadConnectionTimeout != null)\n            CqlConfigHelper.setInputNativeReadConnectionTimeout(conf, nativeReadConnectionTimeout);\n        if (nativeReceiveBufferSize != null)\n            CqlConfigHelper.setInputNativeReceiveBufferSize(conf, nativeReceiveBufferSize);\n        if (nativeSendBufferSize != null)\n            CqlConfigHelper.setInputNativeSendBufferSize(conf, nativeSendBufferSize);\n        if (nativeSolinger != null)\n            CqlConfigHelper.setInputNativeSolinger(conf, nativeSolinger);\n        if (nativeTcpNodelay != null)\n            CqlConfigHelper.setInputNativeTcpNodelay(conf, nativeTcpNodelay);\n        if (nativeReuseAddress != null)\n            CqlConfigHelper.setInputNativeReuseAddress(conf, nativeReuseAddress);\n        if (nativeKeepAlive != null)\n            CqlConfigHelper.setInputNativeKeepAlive(conf, nativeKeepAlive);\n        if (nativeAuthProvider != null)\n            CqlConfigHelper.setInputNativeAuthProvider(conf, nativeAuthProvider);\n        if (nativeSSLTruststorePath != null)\n            CqlConfigHelper.setInputNativeSSLTruststorePath(conf, nativeSSLTruststorePath);\n        if (nativeSSLKeystorePath != null)\n            CqlConfigHelper.setInputNativeSSLKeystorePath(conf, nativeSSLKeystorePath);\n        if (nativeSSLTruststorePassword != null)\n            CqlConfigHelper.setInputNativeSSLTruststorePassword(conf, nativeSSLTruststorePassword);\n        if (nativeSSLKeystorePassword != null)\n            CqlConfigHelper.setInputNativeSSLKeystorePassword(conf, nativeSSLKeystorePassword);\n        if (nativeSSLCipherSuites != null)\n            CqlConfigHelper.setInputNativeSSLCipherSuites(conf, nativeSSLCipherSuites);\n\n        ConfigHelper.setInputColumnFamily(conf, keyspace, column_family);\n        setConnectionInformation();\n\n        CqlConfigHelper.setInputCQLPageRowSize(conf, String.valueOf(pageSize));\n        if (inputCql != null)\n            CqlConfigHelper.setInputCql(conf, inputCql);\n        if (columns != null)\n            CqlConfigHelper.setInputColumns(conf, columns);\n        if (whereClause != null)\n            CqlConfigHelper.setInputWhereClauses(conf, whereClause);\n\n        String whereClauseForPartitionFilter = getWhereClauseForPartitionFilter();\n        String wc = whereClause != null && !whereClause.trim().isEmpty()\n                               ? whereClauseForPartitionFilter == null ? whereClause: String.format(\"%s AND %s\", whereClause.trim(), whereClauseForPartitionFilter)\n                               : whereClauseForPartitionFilter;\n\n        if (wc != null)\n        {\n            logger.debug(\"where clause: {}\", wc);\n            CqlConfigHelper.setInputWhereClauses(conf, wc);\n        }\n        if (System.getenv(StorageHelper.PIG_INPUT_SPLIT_SIZE) != null)\n        {\n            try\n            {\n                ConfigHelper.setInputSplitSize(conf, Integer.parseInt(System.getenv(StorageHelper.PIG_INPUT_SPLIT_SIZE)));\n            }\n            catch (NumberFormatException e)\n            {\n                throw new IOException(\"PIG_INPUT_SPLIT_SIZE is not a number\", e);\n            }\n        }\n\n        if (ConfigHelper.getInputInitialAddress(conf) == null)\n            throw new IOException(\"PIG_INPUT_INITIAL_ADDRESS or PIG_INITIAL_ADDRESS environment variable not set\");\n        if (ConfigHelper.getInputPartitioner(conf) == null)\n            throw new IOException(\"PIG_INPUT_PARTITIONER or PIG_PARTITIONER environment variable not set\");\n        if (loadSignature == null)\n            loadSignature = location;\n\n        initSchema(loadSignature);\n    }",
            " 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629 +\n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  ",
            "    /** set read configuration settings */\n    public void setLocation(String location, Job job) throws IOException\n    {\n        conf = job.getConfiguration();\n        setLocationFromUri(location);\n\n        if (username != null && password != null)\n        {\n            ConfigHelper.setInputKeyspaceUserNameAndPassword(conf, username, password);\n            CqlConfigHelper.setUserNameAndPassword(conf, username, password);\n        }\n        if (splitSize > 0)\n            ConfigHelper.setInputSplitSize(conf, splitSize);\n        if (partitionerClass!= null)\n            ConfigHelper.setInputPartitioner(conf, partitionerClass);\n        if (initHostAddress != null)\n            ConfigHelper.setInputInitialAddress(conf, initHostAddress);\n        if (rpcPort != null)\n            ConfigHelper.setInputRpcPort(conf, rpcPort);\n        if (nativePort != null)\n            CqlConfigHelper.setInputNativePort(conf, nativePort);\n        if (nativeCoreConnections != null)\n            CqlConfigHelper.setInputCoreConnections(conf, nativeCoreConnections);\n        if (nativeMaxConnections != null)\n            CqlConfigHelper.setInputMaxConnections(conf, nativeMaxConnections);\n        if (nativeMaxSimultReqs != null)\n            CqlConfigHelper.setInputMaxSimultReqPerConnections(conf, nativeMaxSimultReqs);\n        if (nativeConnectionTimeout != null)\n            CqlConfigHelper.setInputNativeConnectionTimeout(conf, nativeConnectionTimeout);\n        if (nativeReadConnectionTimeout != null)\n            CqlConfigHelper.setInputNativeReadConnectionTimeout(conf, nativeReadConnectionTimeout);\n        if (nativeReceiveBufferSize != null)\n            CqlConfigHelper.setInputNativeReceiveBufferSize(conf, nativeReceiveBufferSize);\n        if (nativeSendBufferSize != null)\n            CqlConfigHelper.setInputNativeSendBufferSize(conf, nativeSendBufferSize);\n        if (nativeSolinger != null)\n            CqlConfigHelper.setInputNativeSolinger(conf, nativeSolinger);\n        if (nativeTcpNodelay != null)\n            CqlConfigHelper.setInputNativeTcpNodelay(conf, nativeTcpNodelay);\n        if (nativeReuseAddress != null)\n            CqlConfigHelper.setInputNativeReuseAddress(conf, nativeReuseAddress);\n        if (nativeKeepAlive != null)\n            CqlConfigHelper.setInputNativeKeepAlive(conf, nativeKeepAlive);\n        if (nativeAuthProvider != null)\n            CqlConfigHelper.setInputNativeAuthProvider(conf, nativeAuthProvider);\n        if (nativeSSLTruststorePath != null)\n            CqlConfigHelper.setInputNativeSSLTruststorePath(conf, nativeSSLTruststorePath);\n        if (nativeSSLKeystorePath != null)\n            CqlConfigHelper.setInputNativeSSLKeystorePath(conf, nativeSSLKeystorePath);\n        if (nativeSSLTruststorePassword != null)\n            CqlConfigHelper.setInputNativeSSLTruststorePassword(conf, nativeSSLTruststorePassword);\n        if (nativeSSLKeystorePassword != null)\n            CqlConfigHelper.setInputNativeSSLKeystorePassword(conf, nativeSSLKeystorePassword);\n        if (nativeSSLCipherSuites != null)\n            CqlConfigHelper.setInputNativeSSLCipherSuites(conf, nativeSSLCipherSuites);\n\n        ConfigHelper.setInputColumnFamily(conf, keyspace, column_family);\n        setConnectionInformation();\n\n        CqlConfigHelper.setInputCQLPageRowSize(conf, String.valueOf(pageSize));\n        if (inputCql != null)\n            CqlConfigHelper.setInputCql(conf, inputCql);\n        if (columns != null)\n            CqlConfigHelper.setInputColumns(conf, columns);\n        if (whereClause != null)\n            CqlConfigHelper.setInputWhereClauses(conf, whereClause);\n\n        String whereClauseForPartitionFilter = getWhereClauseForPartitionFilter();\n        String wc = whereClause != null && !whereClause.trim().isEmpty()\n                               ? whereClauseForPartitionFilter == null ? whereClause: String.format(\"%s AND %s\", whereClause.trim(), whereClauseForPartitionFilter)\n                               : whereClauseForPartitionFilter;\n\n        if (wc != null)\n        {\n            logger.trace(\"where clause: {}\", wc);\n            CqlConfigHelper.setInputWhereClauses(conf, wc);\n        }\n        if (System.getenv(StorageHelper.PIG_INPUT_SPLIT_SIZE) != null)\n        {\n            try\n            {\n                ConfigHelper.setInputSplitSize(conf, Integer.parseInt(System.getenv(StorageHelper.PIG_INPUT_SPLIT_SIZE)));\n            }\n            catch (NumberFormatException e)\n            {\n                throw new IOException(\"PIG_INPUT_SPLIT_SIZE is not a number\", e);\n            }\n        }\n\n        if (ConfigHelper.getInputInitialAddress(conf) == null)\n            throw new IOException(\"PIG_INPUT_INITIAL_ADDRESS or PIG_INITIAL_ADDRESS environment variable not set\");\n        if (ConfigHelper.getInputPartitioner(conf) == null)\n            throw new IOException(\"PIG_INPUT_PARTITIONER or PIG_PARTITIONER environment variable not set\");\n        if (loadSignature == null)\n            loadSignature = location;\n\n        initSchema(loadSignature);\n    }"
        ],
        [
            "LeveledManifest::replace(Collection,Collection)",
            " 145  \n 146  \n 147  \n 148  \n 149 -\n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    public synchronized void replace(Collection<SSTableReader> removed, Collection<SSTableReader> added)\n    {\n        assert !removed.isEmpty(); // use add() instead of promote when adding new sstables\n        logDistribution();\n        if (logger.isDebugEnabled())\n            logger.debug(\"Replacing [{}]\", toString(removed));\n\n        // the level for the added sstables is the max of the removed ones,\n        // plus one if the removed were all on the same level\n        int minLevel = Integer.MAX_VALUE;\n\n        for (SSTableReader sstable : removed)\n        {\n            int thisLevel = remove(sstable);\n            minLevel = Math.min(minLevel, thisLevel);\n        }\n\n        // it's valid to do a remove w/o an add (e.g. on truncate)\n        if (added.isEmpty())\n            return;\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"Adding [{}]\", toString(added));\n\n        for (SSTableReader ssTableReader : added)\n            add(ssTableReader);\n        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;\n    }",
            " 145  \n 146  \n 147  \n 148  \n 149 +\n 150 +\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "    public synchronized void replace(Collection<SSTableReader> removed, Collection<SSTableReader> added)\n    {\n        assert !removed.isEmpty(); // use add() instead of promote when adding new sstables\n        logDistribution();\n        if (logger.isTraceEnabled())\n            logger.trace(\"Replacing [{}]\", toString(removed));\n\n        // the level for the added sstables is the max of the removed ones,\n        // plus one if the removed were all on the same level\n        int minLevel = Integer.MAX_VALUE;\n\n        for (SSTableReader sstable : removed)\n        {\n            int thisLevel = remove(sstable);\n            minLevel = Math.min(minLevel, thisLevel);\n        }\n\n        // it's valid to do a remove w/o an add (e.g. on truncate)\n        if (added.isEmpty())\n            return;\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"Adding [{}]\", toString(added));\n\n        for (SSTableReader ssTableReader : added)\n            add(ssTableReader);\n        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::LimitedLocalNodeFirstLocalBalancingPolicy(String)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 -\n  75  ",
            "    public LimitedLocalNodeFirstLocalBalancingPolicy(String[] replicas)\n    {\n        for (String replica : replicas)\n        {\n            try\n            {\n                InetAddress[] addresses = InetAddress.getAllByName(replica);\n                Collections.addAll(replicaAddresses, addresses);\n            }\n            catch (UnknownHostException e)\n            {\n                logger.warn(\"Invalid replica host name: {}, skipping it\", replica);\n            }\n        }\n        logger.debug(\"Created instance with the following replicas: {}\", Arrays.asList(replicas));\n    }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75  ",
            "    public LimitedLocalNodeFirstLocalBalancingPolicy(String[] replicas)\n    {\n        for (String replica : replicas)\n        {\n            try\n            {\n                InetAddress[] addresses = InetAddress.getAllByName(replica);\n                Collections.addAll(replicaAddresses, addresses);\n            }\n            catch (UnknownHostException e)\n            {\n                logger.warn(\"Invalid replica host name: {}, skipping it\", replica);\n            }\n        }\n        logger.trace(\"Created instance with the following replicas: {}\", Arrays.asList(replicas));\n    }"
        ],
        [
            "CassandraServer::get_count(ByteBuffer,ColumnParent,SlicePredicate,ConsistencyLevel)",
            " 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 -\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544 -\n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  ",
            "    public int get_count(ByteBuffer key, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_count\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"get_count\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            Keyspace keyspaceName = Keyspace.open(keyspace);\n            ColumnFamilyStore cfs = keyspaceName.getColumnFamilyStore(column_parent.column_family);\n            long timestamp = System.currentTimeMillis();\n\n            if (predicate.column_names != null)\n                return getSliceInternal(keyspace, key, column_parent, timestamp, predicate, consistency_level, cState).size();\n\n            int pageSize;\n            // request by page if this is a large row\n            if (cfs.getMeanColumns() > 0)\n            {\n                int averageColumnSize = (int) (cfs.metric.meanRowSize.getValue() / cfs.getMeanColumns());\n                pageSize = Math.min(COUNT_PAGE_SIZE, 4 * 1024 * 1024 / averageColumnSize);\n                pageSize = Math.max(2, pageSize);\n                logger.debug(\"average row column size is {}; using pageSize of {}\", averageColumnSize, pageSize);\n            }\n            else\n            {\n                pageSize = COUNT_PAGE_SIZE;\n            }\n\n            SliceRange sliceRange = predicate.slice_range == null\n                                  ? new SliceRange(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, Integer.MAX_VALUE)\n                                  : predicate.slice_range;\n            SliceQueryFilter filter = toInternalFilter(cfs.metadata, column_parent, sliceRange);\n\n            return QueryPagers.countPaged(keyspace,\n                                          column_parent.column_family,\n                                          key,\n                                          filter,\n                                          ThriftConversion.fromThrift(consistency_level),\n                                          cState,\n                                          pageSize,\n                                          timestamp);\n        }\n        catch (IllegalArgumentException e)\n        {\n            // CASSANDRA-5701\n            throw new InvalidRequestException(e.getMessage());\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 +\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544 +\n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  ",
            "    public int get_count(ByteBuffer key, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_count\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"get_count\");\n        }\n\n        try\n        {\n            ThriftClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            Keyspace keyspaceName = Keyspace.open(keyspace);\n            ColumnFamilyStore cfs = keyspaceName.getColumnFamilyStore(column_parent.column_family);\n            long timestamp = System.currentTimeMillis();\n\n            if (predicate.column_names != null)\n                return getSliceInternal(keyspace, key, column_parent, timestamp, predicate, consistency_level, cState).size();\n\n            int pageSize;\n            // request by page if this is a large row\n            if (cfs.getMeanColumns() > 0)\n            {\n                int averageColumnSize = (int) (cfs.metric.meanRowSize.getValue() / cfs.getMeanColumns());\n                pageSize = Math.min(COUNT_PAGE_SIZE, 4 * 1024 * 1024 / averageColumnSize);\n                pageSize = Math.max(2, pageSize);\n                logger.trace(\"average row column size is {}; using pageSize of {}\", averageColumnSize, pageSize);\n            }\n            else\n            {\n                pageSize = COUNT_PAGE_SIZE;\n            }\n\n            SliceRange sliceRange = predicate.slice_range == null\n                                  ? new SliceRange(ByteBufferUtil.EMPTY_BYTE_BUFFER, ByteBufferUtil.EMPTY_BYTE_BUFFER, false, Integer.MAX_VALUE)\n                                  : predicate.slice_range;\n            SliceQueryFilter filter = toInternalFilter(cfs.metadata, column_parent, sliceRange);\n\n            return QueryPagers.countPaged(keyspace,\n                                          column_parent.column_family,\n                                          key,\n                                          filter,\n                                          ThriftConversion.fromThrift(consistency_level),\n                                          cState,\n                                          pageSize,\n                                          timestamp);\n        }\n        catch (IllegalArgumentException e)\n        {\n            // CASSANDRA-5701\n            throw new InvalidRequestException(e.getMessage());\n        }\n        catch (RequestExecutionException e)\n        {\n            throw ThriftConversion.rethrow(e);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "DateTieredCompactionStrategy::getNextBackgroundSSTables(int)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     *\n     * @param gcBefore\n     * @return\n     */\n    private List<SSTableReader> getNextBackgroundSSTables(final int gcBefore)\n    {\n        if (cfs.getSSTables().isEmpty())\n            return Collections.emptyList();\n\n        Set<SSTableReader> uncompacting = Sets.intersection(sstables, cfs.getUncompactingSSTables());\n\n        Set<SSTableReader> expired = Collections.emptySet();\n        // we only check for expired sstables every 10 minutes (by default) due to it being an expensive operation\n        if (System.currentTimeMillis() - lastExpiredCheck > options.expiredSSTableCheckFrequency)\n        {\n            // Find fully expired SSTables. Those will be included no matter what.\n            expired = CompactionController.getFullyExpiredSSTables(cfs, uncompacting, cfs.getOverlappingSSTables(uncompacting), gcBefore);\n            lastExpiredCheck = System.currentTimeMillis();\n        }\n        Set<SSTableReader> candidates = Sets.newHashSet(filterSuspectSSTables(uncompacting));\n\n        List<SSTableReader> compactionCandidates = new ArrayList<>(getNextNonExpiredSSTables(Sets.difference(candidates, expired), gcBefore));\n        if (!expired.isEmpty())\n        {\n            logger.debug(\"Including expired sstables: {}\", expired);\n            compactionCandidates.addAll(expired);\n        }\n        return compactionCandidates;\n    }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     *\n     * @param gcBefore\n     * @return\n     */\n    private List<SSTableReader> getNextBackgroundSSTables(final int gcBefore)\n    {\n        if (cfs.getSSTables().isEmpty())\n            return Collections.emptyList();\n\n        Set<SSTableReader> uncompacting = Sets.intersection(sstables, cfs.getUncompactingSSTables());\n\n        Set<SSTableReader> expired = Collections.emptySet();\n        // we only check for expired sstables every 10 minutes (by default) due to it being an expensive operation\n        if (System.currentTimeMillis() - lastExpiredCheck > options.expiredSSTableCheckFrequency)\n        {\n            // Find fully expired SSTables. Those will be included no matter what.\n            expired = CompactionController.getFullyExpiredSSTables(cfs, uncompacting, cfs.getOverlappingSSTables(uncompacting), gcBefore);\n            lastExpiredCheck = System.currentTimeMillis();\n        }\n        Set<SSTableReader> candidates = Sets.newHashSet(filterSuspectSSTables(uncompacting));\n\n        List<SSTableReader> compactionCandidates = new ArrayList<>(getNextNonExpiredSSTables(Sets.difference(candidates, expired), gcBefore));\n        if (!expired.isEmpty())\n        {\n            logger.trace(\"Including expired sstables: {}\", expired);\n            compactionCandidates.addAll(expired);\n        }\n        return compactionCandidates;\n    }"
        ],
        [
            "MetadataSerializer::mutateRepairedAt(Descriptor,long)",
            " 140  \n 141  \n 142 -\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "    public void mutateRepairedAt(Descriptor descriptor, long newRepairedAt) throws IOException\n    {\n        logger.debug(\"Mutating {} to repairedAt time {}\", descriptor.filenameFor(Component.STATS), newRepairedAt);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }",
            " 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "    public void mutateRepairedAt(Descriptor descriptor, long newRepairedAt) throws IOException\n    {\n        logger.trace(\"Mutating {} to repairedAt time {}\", descriptor.filenameFor(Component.STATS), newRepairedAt);\n        Map<MetadataType, MetadataComponent> currentComponents = deserialize(descriptor, EnumSet.allOf(MetadataType.class));\n        StatsMetadata stats = (StatsMetadata) currentComponents.remove(MetadataType.STATS);\n        // mutate level\n        currentComponents.put(MetadataType.STATS, stats.mutateRepairedAt(newRepairedAt));\n        rewriteSSTableMetadata(descriptor, currentComponents);\n    }"
        ],
        [
            "SSTableReader::estimateCompactionGain(Set)",
            " 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316  \n 317  ",
            "    /**\n     * Estimates how much of the keys we would keep if the sstables were compacted together\n     */\n    public static double estimateCompactionGain(Set<SSTableReader> overlapping)\n    {\n        Set<ICardinality> cardinalities = new HashSet<>(overlapping.size());\n        for (SSTableReader sstable : overlapping)\n        {\n            try\n            {\n                ICardinality cardinality = ((CompactionMetadata) sstable.descriptor.getMetadataSerializer().deserialize(sstable.descriptor, MetadataType.COMPACTION)).cardinalityEstimator;\n                if (cardinality != null)\n                    cardinalities.add(cardinality);\n                else\n                    logger.debug(\"Got a null cardinality estimator in: {}\", sstable.getFilename());\n            }\n            catch (IOException e)\n            {\n                logger.warn(\"Could not read up compaction metadata for {}\", sstable, e);\n            }\n        }\n        long totalKeyCountBefore = 0;\n        for (ICardinality cardinality : cardinalities)\n        {\n            totalKeyCountBefore += cardinality.cardinality();\n        }\n        if (totalKeyCountBefore == 0)\n            return 1;\n\n        long totalKeyCountAfter = mergeCardinalities(cardinalities).cardinality();\n        logger.debug(\"Estimated compaction gain: {}/{}={}\", totalKeyCountAfter, totalKeyCountBefore, ((double)totalKeyCountAfter)/totalKeyCountBefore);\n        return ((double)totalKeyCountAfter)/totalKeyCountBefore;\n    }",
            " 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 +\n 316  \n 317  ",
            "    /**\n     * Estimates how much of the keys we would keep if the sstables were compacted together\n     */\n    public static double estimateCompactionGain(Set<SSTableReader> overlapping)\n    {\n        Set<ICardinality> cardinalities = new HashSet<>(overlapping.size());\n        for (SSTableReader sstable : overlapping)\n        {\n            try\n            {\n                ICardinality cardinality = ((CompactionMetadata) sstable.descriptor.getMetadataSerializer().deserialize(sstable.descriptor, MetadataType.COMPACTION)).cardinalityEstimator;\n                if (cardinality != null)\n                    cardinalities.add(cardinality);\n                else\n                    logger.trace(\"Got a null cardinality estimator in: {}\", sstable.getFilename());\n            }\n            catch (IOException e)\n            {\n                logger.warn(\"Could not read up compaction metadata for {}\", sstable, e);\n            }\n        }\n        long totalKeyCountBefore = 0;\n        for (ICardinality cardinality : cardinalities)\n        {\n            totalKeyCountBefore += cardinality.cardinality();\n        }\n        if (totalKeyCountBefore == 0)\n            return 1;\n\n        long totalKeyCountAfter = mergeCardinalities(cardinalities).cardinality();\n        logger.trace(\"Estimated compaction gain: {}/{}={}\", totalKeyCountAfter, totalKeyCountBefore, ((double)totalKeyCountAfter)/totalKeyCountBefore);\n        return ((double)totalKeyCountAfter)/totalKeyCountBefore;\n    }"
        ],
        [
            "CassandraServer::get_slice(ByteBuffer,ColumnParent,SlicePredicate,ConsistencyLevel)",
            " 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "    public List<ColumnOrSuperColumn> get_slice(ByteBuffer key, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_slice\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"get_slice\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            state().hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            return getSliceInternal(keyspace, key, column_parent, System.currentTimeMillis(), predicate, consistency_level, cState);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            " 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "    public List<ColumnOrSuperColumn> get_slice(ByteBuffer key, ColumnParent column_parent, SlicePredicate predicate, ConsistencyLevel consistency_level)\n    throws InvalidRequestException, UnavailableException, TimedOutException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"key\", ByteBufferUtil.bytesToHex(key),\n                                                                  \"column_parent\", column_parent.toString(),\n                                                                  \"predicate\", predicate.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"get_slice\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"get_slice\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            state().hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.SELECT);\n            return getSliceInternal(keyspace, key, column_parent, System.currentTimeMillis(), predicate, consistency_level, cState);\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "IncomingTcpConnection::run()",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "    /**\n     * A new connection will either stream or message for its entire lifetime: because streaming\n     * bypasses the InputStream implementations to use sendFile, we cannot begin buffering until\n     * we've determined the type of the connection.\n     */\n    @Override\n    public void run()\n    {\n        try\n        {\n            if (version < MessagingService.VERSION_20)\n                throw new UnsupportedOperationException(String.format(\"Unable to read obsolete message version %s; \"\n                                                                      + \"The earliest version supported is 2.0.0\",\n                                                                      version));\n\n            receiveMessages();\n        }\n        catch (EOFException e)\n        {\n            logger.trace(\"eof reading from socket; closing\", e);\n            // connection will be reset so no need to throw an exception.\n        }\n        catch (UnknownColumnFamilyException e)\n        {\n            logger.warn(\"UnknownColumnFamilyException reading from socket; closing\", e);\n        }\n        catch (IOException e)\n        {\n            logger.debug(\"IOException reading from socket; closing\", e);\n        }\n        finally\n        {\n            close();\n        }\n    }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "    /**\n     * A new connection will either stream or message for its entire lifetime: because streaming\n     * bypasses the InputStream implementations to use sendFile, we cannot begin buffering until\n     * we've determined the type of the connection.\n     */\n    @Override\n    public void run()\n    {\n        try\n        {\n            if (version < MessagingService.VERSION_20)\n                throw new UnsupportedOperationException(String.format(\"Unable to read obsolete message version %s; \"\n                                                                      + \"The earliest version supported is 2.0.0\",\n                                                                      version));\n\n            receiveMessages();\n        }\n        catch (EOFException e)\n        {\n            logger.trace(\"eof reading from socket; closing\", e);\n            // connection will be reset so no need to throw an exception.\n        }\n        catch (UnknownColumnFamilyException e)\n        {\n            logger.warn(\"UnknownColumnFamilyException reading from socket; closing\", e);\n        }\n        catch (IOException e)\n        {\n            logger.trace(\"IOException reading from socket; closing\", e);\n        }\n        finally\n        {\n            close();\n        }\n    }"
        ],
        [
            "SSTableReader::open(Descriptor,Set,CFMetaData,IPartitioner,boolean,boolean)",
            " 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 -\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 -\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "    public static SSTableReader open(Descriptor descriptor,\n                                      Set<Component> components,\n                                      CFMetaData metadata,\n                                      IPartitioner partitioner,\n                                      boolean validate,\n                                      boolean trackHotness) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        Map<MetadataType, MetadataComponent> sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor,\n                                                                                                               EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS));\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.info(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor, components, metadata, partitioner, System.currentTimeMillis(),\n                                             statsMetadata, OpenReason.NORMAL);\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata);\n            logger.debug(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(trackHotness);\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.debug(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw t;\n        }\n    }",
            " 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 +\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 +\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "    public static SSTableReader open(Descriptor descriptor,\n                                      Set<Component> components,\n                                      CFMetaData metadata,\n                                      IPartitioner partitioner,\n                                      boolean validate,\n                                      boolean trackHotness) throws IOException\n    {\n        // Minimum components without which we can't do anything\n        assert components.contains(Component.DATA) : \"Data component is missing for sstable \" + descriptor;\n        assert !validate || components.contains(Component.PRIMARY_INDEX) : \"Primary index component is missing for sstable \" + descriptor;\n\n        Map<MetadataType, MetadataComponent> sstableMetadata = descriptor.getMetadataSerializer().deserialize(descriptor,\n                                                                                                               EnumSet.of(MetadataType.VALIDATION, MetadataType.STATS));\n        ValidationMetadata validationMetadata = (ValidationMetadata) sstableMetadata.get(MetadataType.VALIDATION);\n        StatsMetadata statsMetadata = (StatsMetadata) sstableMetadata.get(MetadataType.STATS);\n\n        // Check if sstable is created using same partitioner.\n        // Partitioner can be null, which indicates older version of sstable or no stats available.\n        // In that case, we skip the check.\n        String partitionerName = partitioner.getClass().getCanonicalName();\n        if (validationMetadata != null && !partitionerName.equals(validationMetadata.partitioner))\n        {\n            logger.error(String.format(\"Cannot open %s; partitioner %s does not match system partitioner %s.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.\",\n                    descriptor, validationMetadata.partitioner, partitionerName));\n            System.exit(1);\n        }\n\n        logger.debug(\"Opening {} ({} bytes)\", descriptor, new File(descriptor.filenameFor(Component.DATA)).length());\n        SSTableReader sstable = internalOpen(descriptor, components, metadata, partitioner, System.currentTimeMillis(),\n                                             statsMetadata, OpenReason.NORMAL);\n        try\n        {\n            // load index and filter\n            long start = System.nanoTime();\n            sstable.load(validationMetadata);\n            logger.trace(\"INDEX LOAD TIME for {}: {} ms.\", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            sstable.setup(trackHotness);\n            if (validate)\n                sstable.validate();\n\n            if (sstable.getKeyCache() != null)\n                logger.trace(\"key cache contains {}/{} keys\", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());\n\n            return sstable;\n        }\n        catch (Throwable t)\n        {\n            sstable.selfRef().release();\n            throw t;\n        }\n    }"
        ],
        [
            "MetadataSerializer::deserialize(Descriptor,EnumSet)",
            "  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException\n    {\n        Map<MetadataType, MetadataComponent> components;\n        logger.debug(\"Load metadata for {}\", descriptor);\n        File statsFile = new File(descriptor.filenameFor(Component.STATS));\n        if (!statsFile.exists())\n        {\n            logger.debug(\"No sstable stats for {}\", descriptor);\n            components = Maps.newHashMap();\n            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());\n        }\n        else\n        {\n            try (RandomAccessReader r = RandomAccessReader.open(statsFile))\n            {\n                components = deserialize(descriptor, r, types);\n            }\n        }\n        return components;\n    }",
            "  78  \n  79  \n  80  \n  81 +\n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    public Map<MetadataType, MetadataComponent> deserialize(Descriptor descriptor, EnumSet<MetadataType> types) throws IOException\n    {\n        Map<MetadataType, MetadataComponent> components;\n        logger.trace(\"Load metadata for {}\", descriptor);\n        File statsFile = new File(descriptor.filenameFor(Component.STATS));\n        if (!statsFile.exists())\n        {\n            logger.trace(\"No sstable stats for {}\", descriptor);\n            components = Maps.newHashMap();\n            components.put(MetadataType.STATS, MetadataCollector.defaultStatsMetadata());\n        }\n        else\n        {\n            try (RandomAccessReader r = RandomAccessReader.open(statsFile))\n            {\n                components = deserialize(descriptor, r, types);\n            }\n        }\n        return components;\n    }"
        ],
        [
            "FileUtils::renameWithConfirm(File,File)",
            " 170  \n 171  \n 172  \n 173 -\n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "    public static void renameWithConfirm(File from, File to)\n    {\n        assert from.exists();\n        if (logger.isDebugEnabled())\n            logger.debug((String.format(\"Renaming %s to %s\", from.getPath(), to.getPath())));\n        // this is not FSWE because usually when we see it it's because we didn't close the file before renaming it,\n        // and Windows is picky about that.\n        try\n        {\n            atomicMoveWithFallback(from.toPath(), to.toPath());\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(String.format(\"Failed to rename %s to %s\", from.getPath(), to.getPath()), e);\n        }\n    }",
            " 170  \n 171  \n 172  \n 173 +\n 174 +\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "    public static void renameWithConfirm(File from, File to)\n    {\n        assert from.exists();\n        if (logger.isTraceEnabled())\n            logger.trace((String.format(\"Renaming %s to %s\", from.getPath(), to.getPath())));\n        // this is not FSWE because usually when we see it it's because we didn't close the file before renaming it,\n        // and Windows is picky about that.\n        try\n        {\n            atomicMoveWithFallback(from.toPath(), to.toPath());\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(String.format(\"Failed to rename %s to %s\", from.getPath(), to.getPath()), e);\n        }\n    }"
        ],
        [
            "ReadCallback::AsyncRepairRunner::run()",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "        public void run()\n        {\n            // If the resolver is a RowDigestResolver, we need to do a full data read if there is a mismatch.\n            // Otherwise, resolve will send the repairs directly if needs be (and in that case we should never\n            // get a digest mismatch)\n            try\n            {\n                resolver.resolve();\n            }\n            catch (DigestMismatchException e)\n            {\n                assert resolver instanceof RowDigestResolver;\n\n                if (traceState != null)\n                    traceState.trace(\"Digest mismatch: {}\", e.toString());\n                if (logger.isDebugEnabled())\n                    logger.debug(\"Digest mismatch:\", e);\n                \n                ReadRepairMetrics.repairedBackground.mark();\n                \n                ReadCommand readCommand = (ReadCommand) command;\n                final RowDataResolver repairResolver = new RowDataResolver(readCommand.ksName, readCommand.key, readCommand.filter(), readCommand.timestamp, endpoints.size());\n                AsyncRepairCallback repairHandler = new AsyncRepairCallback(repairResolver, endpoints.size());\n\n                MessageOut<ReadCommand> message = ((ReadCommand) command).createMessage();\n                for (InetAddress endpoint : endpoints)\n                    MessagingService.instance().sendRR(message, endpoint, repairHandler);\n            }\n        }",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "        public void run()\n        {\n            // If the resolver is a RowDigestResolver, we need to do a full data read if there is a mismatch.\n            // Otherwise, resolve will send the repairs directly if needs be (and in that case we should never\n            // get a digest mismatch)\n            try\n            {\n                resolver.resolve();\n            }\n            catch (DigestMismatchException e)\n            {\n                assert resolver instanceof RowDigestResolver;\n\n                if (traceState != null)\n                    traceState.trace(\"Digest mismatch: {}\", e.toString());\n                if (logger.isTraceEnabled())\n                    logger.trace(\"Digest mismatch:\", e);\n                \n                ReadRepairMetrics.repairedBackground.mark();\n                \n                ReadCommand readCommand = (ReadCommand) command;\n                final RowDataResolver repairResolver = new RowDataResolver(readCommand.ksName, readCommand.key, readCommand.filter(), readCommand.timestamp, endpoints.size());\n                AsyncRepairCallback repairHandler = new AsyncRepairCallback(repairResolver, endpoints.size());\n\n                MessageOut<ReadCommand> message = ((ReadCommand) command).createMessage();\n                for (InetAddress endpoint : endpoints)\n                    MessagingService.instance().sendRR(message, endpoint, repairHandler);\n            }\n        }"
        ],
        [
            "ScriptBasedUDF::executeUserDefined(int,List)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147  \n 148  \n 149  ",
            "    public ByteBuffer executeUserDefined(int protocolVersion, List<ByteBuffer> parameters) throws InvalidRequestException\n    {\n        Object[] params = new Object[argTypes.size()];\n        for (int i = 0; i < params.length; i++)\n            params[i] = compose(protocolVersion, i, parameters.get(i));\n\n        try\n        {\n            Bindings bindings = new SimpleBindings();\n            for (int i = 0; i < params.length; i++)\n                bindings.put(argNames.get(i).toString(), params[i]);\n\n            Object result = script.eval(bindings);\n            if (result == null)\n                return null;\n\n            Class<?> javaReturnType = returnDataType.asJavaClass();\n            Class<?> resultType = result.getClass();\n            if (!javaReturnType.isAssignableFrom(resultType))\n            {\n                if (result instanceof Number)\n                {\n                    Number rNumber = (Number) result;\n                    if (javaReturnType == Integer.class)\n                        result = rNumber.intValue();\n                    else if (javaReturnType == Short.class)\n                        result = rNumber.shortValue();\n                    else if (javaReturnType == Byte.class)\n                        result = rNumber.byteValue();\n                    else if (javaReturnType == Long.class)\n                        result = rNumber.longValue();\n                    else if (javaReturnType == Float.class)\n                        result = rNumber.floatValue();\n                    else if (javaReturnType == Double.class)\n                        result = rNumber.doubleValue();\n                    else if (javaReturnType == BigInteger.class)\n                    {\n                        if (rNumber instanceof BigDecimal)\n                            result = ((BigDecimal)rNumber).toBigInteger();\n                        else if (rNumber instanceof Double || rNumber instanceof Float)\n                            result = new BigDecimal(rNumber.toString()).toBigInteger();\n                        else\n                            result = BigInteger.valueOf(rNumber.longValue());\n                    }\n                    else if (javaReturnType == BigDecimal.class)\n                        // String c'tor of BigDecimal is more accurate than valueOf(double)\n                        result = new BigDecimal(rNumber.toString());\n                }\n            }\n\n            return decompose(protocolVersion, result);\n        }\n        catch (RuntimeException | ScriptException e)\n        {\n            logger.debug(\"Execution of UDF '{}' failed\", name, e);\n            throw FunctionExecutionException.create(this, e);\n        }\n    }",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  \n 148  \n 149  ",
            "    public ByteBuffer executeUserDefined(int protocolVersion, List<ByteBuffer> parameters) throws InvalidRequestException\n    {\n        Object[] params = new Object[argTypes.size()];\n        for (int i = 0; i < params.length; i++)\n            params[i] = compose(protocolVersion, i, parameters.get(i));\n\n        try\n        {\n            Bindings bindings = new SimpleBindings();\n            for (int i = 0; i < params.length; i++)\n                bindings.put(argNames.get(i).toString(), params[i]);\n\n            Object result = script.eval(bindings);\n            if (result == null)\n                return null;\n\n            Class<?> javaReturnType = returnDataType.asJavaClass();\n            Class<?> resultType = result.getClass();\n            if (!javaReturnType.isAssignableFrom(resultType))\n            {\n                if (result instanceof Number)\n                {\n                    Number rNumber = (Number) result;\n                    if (javaReturnType == Integer.class)\n                        result = rNumber.intValue();\n                    else if (javaReturnType == Short.class)\n                        result = rNumber.shortValue();\n                    else if (javaReturnType == Byte.class)\n                        result = rNumber.byteValue();\n                    else if (javaReturnType == Long.class)\n                        result = rNumber.longValue();\n                    else if (javaReturnType == Float.class)\n                        result = rNumber.floatValue();\n                    else if (javaReturnType == Double.class)\n                        result = rNumber.doubleValue();\n                    else if (javaReturnType == BigInteger.class)\n                    {\n                        if (rNumber instanceof BigDecimal)\n                            result = ((BigDecimal)rNumber).toBigInteger();\n                        else if (rNumber instanceof Double || rNumber instanceof Float)\n                            result = new BigDecimal(rNumber.toString()).toBigInteger();\n                        else\n                            result = BigInteger.valueOf(rNumber.longValue());\n                    }\n                    else if (javaReturnType == BigDecimal.class)\n                        // String c'tor of BigDecimal is more accurate than valueOf(double)\n                        result = new BigDecimal(rNumber.toString());\n                }\n            }\n\n            return decompose(protocolVersion, result);\n        }\n        catch (RuntimeException | ScriptException e)\n        {\n            logger.trace(\"Execution of UDF '{}' failed\", name, e);\n            throw FunctionExecutionException.create(this, e);\n        }\n    }"
        ],
        [
            "CassandraRoleManager::scheduleSetupTask(Callable)",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379 -\n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  ",
            "    private void scheduleSetupTask(final Callable<Void> setupTask)\n    {\n        // The delay is to give the node a chance to see its peers before attempting the operation\n        ScheduledExecutors.optionalTasks.schedule(new Runnable()\n        {\n            public void run()\n            {\n                // If not all nodes are on 2.2, we don't want to initialize the role manager as this will confuse 2.1\n                // nodes (see CASSANDRA-9761 for details). So we re-schedule the setup for later, hoping that the upgrade\n                // will be finished by then.\n                if (!MessagingService.instance().areAllNodesAtLeast22())\n                {\n                    logger.debug(\"Not all nodes are upgraded to a version that supports Roles yet, rescheduling setup task\");\n                    scheduleSetupTask(setupTask);\n                    return;\n                }\n\n                isClusterReady = true;\n                try\n                {\n                    setupTask.call();\n                }\n                catch (Exception e)\n                {\n                    logger.info(\"Setup task failed with error, rescheduling\");\n                    scheduleSetupTask(setupTask);\n                }\n            }\n        }, AuthKeyspace.SUPERUSER_SETUP_DELAY, TimeUnit.MILLISECONDS);\n    }",
            " 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379 +\n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  ",
            "    private void scheduleSetupTask(final Callable<Void> setupTask)\n    {\n        // The delay is to give the node a chance to see its peers before attempting the operation\n        ScheduledExecutors.optionalTasks.schedule(new Runnable()\n        {\n            public void run()\n            {\n                // If not all nodes are on 2.2, we don't want to initialize the role manager as this will confuse 2.1\n                // nodes (see CASSANDRA-9761 for details). So we re-schedule the setup for later, hoping that the upgrade\n                // will be finished by then.\n                if (!MessagingService.instance().areAllNodesAtLeast22())\n                {\n                    logger.trace(\"Not all nodes are upgraded to a version that supports Roles yet, rescheduling setup task\");\n                    scheduleSetupTask(setupTask);\n                    return;\n                }\n\n                isClusterReady = true;\n                try\n                {\n                    setupTask.call();\n                }\n                catch (Exception e)\n                {\n                    logger.info(\"Setup task failed with error, rescheduling\");\n                    scheduleSetupTask(setupTask);\n                }\n            }\n        }, AuthKeyspace.SUPERUSER_SETUP_DELAY, TimeUnit.MILLISECONDS);\n    }"
        ],
        [
            "LeveledManifest::getCompactionCandidates()",
            " 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330 -\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342 -\n 343  \n 344  \n 345  \n 346  \n 347 -\n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "    /**\n     * @return highest-priority sstables to compact, and level to compact them to\n     * If no compactions are necessary, will return null\n     */\n    public synchronized CompactionCandidate getCompactionCandidates()\n    {\n        // during bootstrap we only do size tiering in L0 to make sure\n        // the streamed files can be placed in their original levels\n        if (StorageService.instance.isBootstrapMode())\n        {\n            List<SSTableReader> mostInteresting = getSSTablesForSTCS(getLevel(0));\n            if (!mostInteresting.isEmpty())\n            {\n                logger.info(\"Bootstrapping - doing STCS in L0\");\n                return new CompactionCandidate(mostInteresting, 0, Long.MAX_VALUE);\n            }\n            return null;\n        }\n        // LevelDB gives each level a score of how much data it contains vs its ideal amount, and\n        // compacts the level with the highest score. But this falls apart spectacularly once you\n        // get behind.  Consider this set of levels:\n        // L0: 988 [ideal: 4]\n        // L1: 117 [ideal: 10]\n        // L2: 12  [ideal: 100]\n        //\n        // The problem is that L0 has a much higher score (almost 250) than L1 (11), so what we'll\n        // do is compact a batch of MAX_COMPACTING_L0 sstables with all 117 L1 sstables, and put the\n        // result (say, 120 sstables) in L1. Then we'll compact the next batch of MAX_COMPACTING_L0,\n        // and so forth.  So we spend most of our i/o rewriting the L1 data with each batch.\n        //\n        // If we could just do *all* L0 a single time with L1, that would be ideal.  But we can't\n        // -- see the javadoc for MAX_COMPACTING_L0.\n        //\n        // LevelDB's way around this is to simply block writes if L0 compaction falls behind.\n        // We don't have that luxury.\n        //\n        // So instead, we\n        // 1) force compacting higher levels first, which minimizes the i/o needed to compact\n        //    optimially which gives us a long term win, and\n        // 2) if L0 falls behind, we will size-tiered compact it to reduce read overhead until\n        //    we can catch up on the higher levels.\n        //\n        // This isn't a magic wand -- if you are consistently writing too fast for LCS to keep\n        // up, you're still screwed.  But if instead you have intermittent bursts of activity,\n        // it can help a lot.\n        for (int i = generations.length - 1; i > 0; i--)\n        {\n            List<SSTableReader> sstables = getLevel(i);\n            if (sstables.isEmpty())\n                continue; // mostly this just avoids polluting the debug log with zero scores\n            // we want to calculate score excluding compacting ones\n            Set<SSTableReader> sstablesInLevel = Sets.newHashSet(sstables);\n            Set<SSTableReader> remaining = Sets.difference(sstablesInLevel, cfs.getTracker().getCompacting());\n            double score = (double) SSTableReader.getTotalBytes(remaining) / (double)maxBytesForLevel(i, maxSSTableSizeInBytes);\n            logger.debug(\"Compaction score for level {} is {}\", i, score);\n\n            if (score > 1.001)\n            {\n                // before proceeding with a higher level, let's see if L0 is far enough behind to warrant STCS\n                if (!DatabaseDescriptor.getDisableSTCSInL0() && getLevel(0).size() > MAX_COMPACTING_L0)\n                {\n                    List<SSTableReader> mostInteresting = getSSTablesForSTCS(getLevel(0));\n                    if (!mostInteresting.isEmpty())\n                    {\n                        logger.debug(\"L0 is too far behind, performing size-tiering there first\");\n                        return new CompactionCandidate(mostInteresting, 0, Long.MAX_VALUE);\n                    }\n                }\n\n                // L0 is fine, proceed with this level\n                Collection<SSTableReader> candidates = getCandidatesFor(i);\n                if (!candidates.isEmpty())\n                {\n                    int nextLevel = getNextLevel(candidates);\n                    candidates = getOverlappingStarvedSSTables(nextLevel, candidates);\n                    if (logger.isDebugEnabled())\n                        logger.debug(\"Compaction candidates for L{} are {}\", i, toString(candidates));\n                    return new CompactionCandidate(candidates, nextLevel, cfs.getCompactionStrategy().getMaxSSTableBytes());\n                }\n                else\n                {\n                    logger.debug(\"No compaction candidates for L{}\", i);\n                }\n            }\n        }\n\n        // Higher levels are happy, time for a standard, non-STCS L0 compaction\n        if (getLevel(0).isEmpty())\n            return null;\n        Collection<SSTableReader> candidates = getCandidatesFor(0);\n        if (candidates.isEmpty())\n            return null;\n        return new CompactionCandidate(candidates, getNextLevel(candidates), cfs.getCompactionStrategy().getMaxSSTableBytes());\n    }",
            " 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330 +\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 +\n 342 +\n 343  \n 344  \n 345  \n 346  \n 347 +\n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "    /**\n     * @return highest-priority sstables to compact, and level to compact them to\n     * If no compactions are necessary, will return null\n     */\n    public synchronized CompactionCandidate getCompactionCandidates()\n    {\n        // during bootstrap we only do size tiering in L0 to make sure\n        // the streamed files can be placed in their original levels\n        if (StorageService.instance.isBootstrapMode())\n        {\n            List<SSTableReader> mostInteresting = getSSTablesForSTCS(getLevel(0));\n            if (!mostInteresting.isEmpty())\n            {\n                logger.info(\"Bootstrapping - doing STCS in L0\");\n                return new CompactionCandidate(mostInteresting, 0, Long.MAX_VALUE);\n            }\n            return null;\n        }\n        // LevelDB gives each level a score of how much data it contains vs its ideal amount, and\n        // compacts the level with the highest score. But this falls apart spectacularly once you\n        // get behind.  Consider this set of levels:\n        // L0: 988 [ideal: 4]\n        // L1: 117 [ideal: 10]\n        // L2: 12  [ideal: 100]\n        //\n        // The problem is that L0 has a much higher score (almost 250) than L1 (11), so what we'll\n        // do is compact a batch of MAX_COMPACTING_L0 sstables with all 117 L1 sstables, and put the\n        // result (say, 120 sstables) in L1. Then we'll compact the next batch of MAX_COMPACTING_L0,\n        // and so forth.  So we spend most of our i/o rewriting the L1 data with each batch.\n        //\n        // If we could just do *all* L0 a single time with L1, that would be ideal.  But we can't\n        // -- see the javadoc for MAX_COMPACTING_L0.\n        //\n        // LevelDB's way around this is to simply block writes if L0 compaction falls behind.\n        // We don't have that luxury.\n        //\n        // So instead, we\n        // 1) force compacting higher levels first, which minimizes the i/o needed to compact\n        //    optimially which gives us a long term win, and\n        // 2) if L0 falls behind, we will size-tiered compact it to reduce read overhead until\n        //    we can catch up on the higher levels.\n        //\n        // This isn't a magic wand -- if you are consistently writing too fast for LCS to keep\n        // up, you're still screwed.  But if instead you have intermittent bursts of activity,\n        // it can help a lot.\n        for (int i = generations.length - 1; i > 0; i--)\n        {\n            List<SSTableReader> sstables = getLevel(i);\n            if (sstables.isEmpty())\n                continue; // mostly this just avoids polluting the debug log with zero scores\n            // we want to calculate score excluding compacting ones\n            Set<SSTableReader> sstablesInLevel = Sets.newHashSet(sstables);\n            Set<SSTableReader> remaining = Sets.difference(sstablesInLevel, cfs.getTracker().getCompacting());\n            double score = (double) SSTableReader.getTotalBytes(remaining) / (double)maxBytesForLevel(i, maxSSTableSizeInBytes);\n            logger.trace(\"Compaction score for level {} is {}\", i, score);\n\n            if (score > 1.001)\n            {\n                // before proceeding with a higher level, let's see if L0 is far enough behind to warrant STCS\n                if (!DatabaseDescriptor.getDisableSTCSInL0() && getLevel(0).size() > MAX_COMPACTING_L0)\n                {\n                    List<SSTableReader> mostInteresting = getSSTablesForSTCS(getLevel(0));\n                    if (!mostInteresting.isEmpty())\n                    {\n                        logger.trace(\"L0 is too far behind, performing size-tiering there first\");\n                        return new CompactionCandidate(mostInteresting, 0, Long.MAX_VALUE);\n                    }\n                }\n\n                // L0 is fine, proceed with this level\n                Collection<SSTableReader> candidates = getCandidatesFor(i);\n                if (!candidates.isEmpty())\n                {\n                    int nextLevel = getNextLevel(candidates);\n                    candidates = getOverlappingStarvedSSTables(nextLevel, candidates);\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Compaction candidates for L{} are {}\", i, toString(candidates));\n                    return new CompactionCandidate(candidates, nextLevel, cfs.getCompactionStrategy().getMaxSSTableBytes());\n                }\n                else\n                {\n                    logger.trace(\"No compaction candidates for L{}\", i);\n                }\n            }\n        }\n\n        // Higher levels are happy, time for a standard, non-STCS L0 compaction\n        if (getLevel(0).isEmpty())\n            return null;\n        Collection<SSTableReader> candidates = getCandidatesFor(0);\n        if (candidates.isEmpty())\n            return null;\n        return new CompactionCandidate(candidates, getNextLevel(candidates), cfs.getCompactionStrategy().getMaxSSTableBytes());\n    }"
        ],
        [
            "MessagingService::waitUntilListening()",
            " 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541 -\n 542  \n 543  ",
            "    public void waitUntilListening()\n    {\n        try\n        {\n            listenGate.await();\n        }\n        catch (InterruptedException ie)\n        {\n            logger.debug(\"await interrupted\");\n        }\n    }",
            " 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541 +\n 542  \n 543  ",
            "    public void waitUntilListening()\n    {\n        try\n        {\n            listenGate.await();\n        }\n        catch (InterruptedException ie)\n        {\n            logger.trace(\"await interrupted\");\n        }\n    }"
        ],
        [
            "CommitLogArchiver::maybeRestoreArchive()",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "    public void maybeRestoreArchive()\n    {\n        if (Strings.isNullOrEmpty(restoreDirectories))\n            return;\n\n        for (String dir : restoreDirectories.split(DELIMITER))\n        {\n            File[] files = new File(dir).listFiles();\n            if (files == null)\n            {\n                throw new RuntimeException(\"Unable to list directory \" + dir);\n            }\n            for (File fromFile : files)\n            {\n                CommitLogDescriptor fromHeader = CommitLogDescriptor.fromHeader(fromFile);\n                CommitLogDescriptor fromName = CommitLogDescriptor.isValid(fromFile.getName()) ? CommitLogDescriptor.fromFileName(fromFile.getName()) : null;\n                CommitLogDescriptor descriptor;\n                if (fromHeader == null && fromName == null)\n                    throw new IllegalStateException(\"Cannot safely construct descriptor for segment, either from its name or its header: \" + fromFile.getPath());\n                else if (fromHeader != null && fromName != null && !fromHeader.equalsIgnoringCompression(fromName))\n                    throw new IllegalStateException(String.format(\"Cannot safely construct descriptor for segment, as name and header descriptors do not match (%s vs %s): %s\", fromHeader, fromName, fromFile.getPath()));\n                else if (fromName != null && fromHeader == null && fromName.version >= CommitLogDescriptor.VERSION_21)\n                    throw new IllegalStateException(\"Cannot safely construct descriptor for segment, as name descriptor implies a version that should contain a header descriptor, but that descriptor could not be read: \" + fromFile.getPath());\n                else if (fromHeader != null)\n                    descriptor = fromHeader;\n                else descriptor = fromName;\n\n                if (descriptor.version > CommitLogDescriptor.VERSION_22)\n                    throw new IllegalStateException(\"Unsupported commit log version: \" + descriptor.version);\n\n                if (descriptor.compression != null) {\n                    try\n                    {\n                        CompressionParameters.createCompressor(descriptor.compression);\n                    }\n                    catch (ConfigurationException e)\n                    {\n                        throw new IllegalStateException(\"Unknown compression\", e);\n                    }\n                }\n\n                File toFile = new File(DatabaseDescriptor.getCommitLogLocation(), descriptor.fileName());\n                if (toFile.exists())\n                {\n                    logger.debug(\"Skipping restore of archive {} as the segment already exists in the restore location {}\",\n                                 fromFile.getPath(), toFile.getPath());\n                    continue;\n                }\n\n                String command = restoreCommand.replace(\"%from\", fromFile.getPath());\n                command = command.replace(\"%to\", toFile.getPath());\n                try\n                {\n                    exec(command);\n                }\n                catch (IOException e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n    }",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "    public void maybeRestoreArchive()\n    {\n        if (Strings.isNullOrEmpty(restoreDirectories))\n            return;\n\n        for (String dir : restoreDirectories.split(DELIMITER))\n        {\n            File[] files = new File(dir).listFiles();\n            if (files == null)\n            {\n                throw new RuntimeException(\"Unable to list directory \" + dir);\n            }\n            for (File fromFile : files)\n            {\n                CommitLogDescriptor fromHeader = CommitLogDescriptor.fromHeader(fromFile);\n                CommitLogDescriptor fromName = CommitLogDescriptor.isValid(fromFile.getName()) ? CommitLogDescriptor.fromFileName(fromFile.getName()) : null;\n                CommitLogDescriptor descriptor;\n                if (fromHeader == null && fromName == null)\n                    throw new IllegalStateException(\"Cannot safely construct descriptor for segment, either from its name or its header: \" + fromFile.getPath());\n                else if (fromHeader != null && fromName != null && !fromHeader.equalsIgnoringCompression(fromName))\n                    throw new IllegalStateException(String.format(\"Cannot safely construct descriptor for segment, as name and header descriptors do not match (%s vs %s): %s\", fromHeader, fromName, fromFile.getPath()));\n                else if (fromName != null && fromHeader == null && fromName.version >= CommitLogDescriptor.VERSION_21)\n                    throw new IllegalStateException(\"Cannot safely construct descriptor for segment, as name descriptor implies a version that should contain a header descriptor, but that descriptor could not be read: \" + fromFile.getPath());\n                else if (fromHeader != null)\n                    descriptor = fromHeader;\n                else descriptor = fromName;\n\n                if (descriptor.version > CommitLogDescriptor.VERSION_22)\n                    throw new IllegalStateException(\"Unsupported commit log version: \" + descriptor.version);\n\n                if (descriptor.compression != null) {\n                    try\n                    {\n                        CompressionParameters.createCompressor(descriptor.compression);\n                    }\n                    catch (ConfigurationException e)\n                    {\n                        throw new IllegalStateException(\"Unknown compression\", e);\n                    }\n                }\n\n                File toFile = new File(DatabaseDescriptor.getCommitLogLocation(), descriptor.fileName());\n                if (toFile.exists())\n                {\n                    logger.trace(\"Skipping restore of archive {} as the segment already exists in the restore location {}\",\n                                 fromFile.getPath(), toFile.getPath());\n                    continue;\n                }\n\n                String command = restoreCommand.replace(\"%from\", fromFile.getPath());\n                command = command.replace(\"%to\", toFile.getPath());\n                try\n                {\n                    exec(command);\n                }\n                catch (IOException e)\n                {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n    }"
        ],
        [
            "SplittingSizeTieredCompactionWriter::SplittingSizeTieredCompactionWriter(ColumnFamilyStore,LifecycleTransaction,Set,OperationType,long)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  ",
            "    @SuppressWarnings(\"resource\")\n    public SplittingSizeTieredCompactionWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables, OperationType compactionType, long smallestSSTable)\n    {\n        super(cfs, txn, nonExpiredSSTables, false);\n        this.allSSTables = txn.originals();\n        totalSize = cfs.getExpectedCompactedFileSize(nonExpiredSSTables, compactionType);\n        double[] potentialRatios = new double[20];\n        double currentRatio = 1;\n        for (int i = 0; i < potentialRatios.length; i++)\n        {\n            currentRatio /= 2;\n            potentialRatios[i] = currentRatio;\n        }\n\n        int noPointIndex = 0;\n        // find how many sstables we should create - 50MB min sstable size\n        for (double ratio : potentialRatios)\n        {\n            noPointIndex++;\n            if (ratio * totalSize < smallestSSTable)\n            {\n                break;\n            }\n        }\n        ratios = Arrays.copyOfRange(potentialRatios, 0, noPointIndex);\n        File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(Math.round(totalSize * ratios[currentRatioIndex])));\n        long currentPartitionsToWrite = Math.round(estimatedTotalKeys * ratios[currentRatioIndex]);\n        currentBytesToWrite = Math.round(totalSize * ratios[currentRatioIndex]);\n        @SuppressWarnings(\"resource\")\n        SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                                            currentPartitionsToWrite,\n                                                                            minRepairedAt,\n                                                                            cfs.metadata,\n                                                                            cfs.partitioner,\n                                                                            new MetadataCollector(allSSTables, cfs.metadata.comparator, 0));\n\n        sstableWriter.switchWriter(writer);\n        logger.debug(\"Ratios={}, expectedKeys = {}, totalSize = {}, currentPartitionsToWrite = {}, currentBytesToWrite = {}\", ratios, estimatedTotalKeys, totalSize, currentPartitionsToWrite, currentBytesToWrite);\n    }",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  ",
            "    @SuppressWarnings(\"resource\")\n    public SplittingSizeTieredCompactionWriter(ColumnFamilyStore cfs, LifecycleTransaction txn, Set<SSTableReader> nonExpiredSSTables, OperationType compactionType, long smallestSSTable)\n    {\n        super(cfs, txn, nonExpiredSSTables, false);\n        this.allSSTables = txn.originals();\n        totalSize = cfs.getExpectedCompactedFileSize(nonExpiredSSTables, compactionType);\n        double[] potentialRatios = new double[20];\n        double currentRatio = 1;\n        for (int i = 0; i < potentialRatios.length; i++)\n        {\n            currentRatio /= 2;\n            potentialRatios[i] = currentRatio;\n        }\n\n        int noPointIndex = 0;\n        // find how many sstables we should create - 50MB min sstable size\n        for (double ratio : potentialRatios)\n        {\n            noPointIndex++;\n            if (ratio * totalSize < smallestSSTable)\n            {\n                break;\n            }\n        }\n        ratios = Arrays.copyOfRange(potentialRatios, 0, noPointIndex);\n        File sstableDirectory = cfs.directories.getLocationForDisk(getWriteDirectory(Math.round(totalSize * ratios[currentRatioIndex])));\n        long currentPartitionsToWrite = Math.round(estimatedTotalKeys * ratios[currentRatioIndex]);\n        currentBytesToWrite = Math.round(totalSize * ratios[currentRatioIndex]);\n        @SuppressWarnings(\"resource\")\n        SSTableWriter writer = SSTableWriter.create(Descriptor.fromFilename(cfs.getTempSSTablePath(sstableDirectory)),\n                                                                            currentPartitionsToWrite,\n                                                                            minRepairedAt,\n                                                                            cfs.metadata,\n                                                                            cfs.partitioner,\n                                                                            new MetadataCollector(allSSTables, cfs.metadata.comparator, 0));\n\n        sstableWriter.switchWriter(writer);\n        logger.trace(\"Ratios={}, expectedKeys = {}, totalSize = {}, currentPartitionsToWrite = {}, currentBytesToWrite = {}\", ratios, estimatedTotalKeys, totalSize, currentPartitionsToWrite, currentBytesToWrite);\n    }"
        ],
        [
            "DateTieredCompactionStrategy::getCompactionCandidates(Iterable,long,int)",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    private List<SSTableReader> getCompactionCandidates(Iterable<SSTableReader> candidateSSTables, long now, int base)\n    {\n        Iterable<SSTableReader> candidates = filterOldSSTables(Lists.newArrayList(candidateSSTables), options.maxSSTableAge, now);\n\n        List<List<SSTableReader>> buckets = getBuckets(createSSTableAndMinTimestampPairs(candidates), options.baseTime, base, now);\n        logger.debug(\"Compaction buckets are {}\", buckets);\n        updateEstimatedCompactionsByTasks(buckets);\n        List<SSTableReader> mostInteresting = newestBucket(buckets,\n                                                           cfs.getMinimumCompactionThreshold(),\n                                                           cfs.getMaximumCompactionThreshold(),\n                                                           now,\n                                                           options.baseTime);\n        if (!mostInteresting.isEmpty())\n            return mostInteresting;\n        return null;\n    }",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137 +\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    private List<SSTableReader> getCompactionCandidates(Iterable<SSTableReader> candidateSSTables, long now, int base)\n    {\n        Iterable<SSTableReader> candidates = filterOldSSTables(Lists.newArrayList(candidateSSTables), options.maxSSTableAge, now);\n\n        List<List<SSTableReader>> buckets = getBuckets(createSSTableAndMinTimestampPairs(candidates), options.baseTime, base, now);\n        logger.trace(\"Compaction buckets are {}\", buckets);\n        updateEstimatedCompactionsByTasks(buckets);\n        List<SSTableReader> mostInteresting = newestBucket(buckets,\n                                                           cfs.getMinimumCompactionThreshold(),\n                                                           cfs.getMaximumCompactionThreshold(),\n                                                           now,\n                                                           options.baseTime);\n        if (!mostInteresting.isEmpty())\n            return mostInteresting;\n        return null;\n    }"
        ],
        [
            "ColumnFamilyStore::viewFilter(Collection,boolean)",
            "1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975 -\n1976  \n1977  \n1978  \n1979  ",
            "    /**\n     * @return a ViewFragment containing the sstables and memtables that may need to be merged\n     * for rows for all of @param rowBoundsCollection, inclusive, according to the interval tree.\n     */\n    public Function<View, List<SSTableReader>> viewFilter(final Collection<AbstractBounds<RowPosition>> rowBoundsCollection, final boolean includeRepaired)\n    {\n        assert AbstractBounds.noneStrictlyWrapsAround(rowBoundsCollection);\n        return new Function<View, List<SSTableReader>>()\n        {\n            public List<SSTableReader> apply(View view)\n            {\n                Set<SSTableReader> sstables = Sets.newHashSet();\n                for (AbstractBounds<RowPosition> rowBounds : rowBoundsCollection)\n                {\n                    // Note that View.sstablesInBounds always includes it's bound while rowBounds may not. This is ok however\n                    // because the fact we restrict the sstables returned by this function is an optimization in the first\n                    // place and the returned sstables will (almost) never cover *exactly* rowBounds anyway. It's also\n                    // *very* unlikely that a sstable is included *just* because we consider one of the bound inclusively\n                    // instead of exclusively, so the performance impact is negligible in practice.\n                    for (SSTableReader sstable : view.sstablesInBounds(rowBounds.left, rowBounds.right))\n                    {\n                        if (includeRepaired || !sstable.isRepaired())\n                            sstables.add(sstable);\n                    }\n                }\n\n                logger.debug(\"ViewFilter for {}/{} sstables\", sstables.size(), getSSTables().size());\n                return ImmutableList.copyOf(sstables);\n            }\n        };\n    }",
            "1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975 +\n1976  \n1977  \n1978  \n1979  ",
            "    /**\n     * @return a ViewFragment containing the sstables and memtables that may need to be merged\n     * for rows for all of @param rowBoundsCollection, inclusive, according to the interval tree.\n     */\n    public Function<View, List<SSTableReader>> viewFilter(final Collection<AbstractBounds<RowPosition>> rowBoundsCollection, final boolean includeRepaired)\n    {\n        assert AbstractBounds.noneStrictlyWrapsAround(rowBoundsCollection);\n        return new Function<View, List<SSTableReader>>()\n        {\n            public List<SSTableReader> apply(View view)\n            {\n                Set<SSTableReader> sstables = Sets.newHashSet();\n                for (AbstractBounds<RowPosition> rowBounds : rowBoundsCollection)\n                {\n                    // Note that View.sstablesInBounds always includes it's bound while rowBounds may not. This is ok however\n                    // because the fact we restrict the sstables returned by this function is an optimization in the first\n                    // place and the returned sstables will (almost) never cover *exactly* rowBounds anyway. It's also\n                    // *very* unlikely that a sstable is included *just* because we consider one of the bound inclusively\n                    // instead of exclusively, so the performance impact is negligible in practice.\n                    for (SSTableReader sstable : view.sstablesInBounds(rowBounds.left, rowBounds.right))\n                    {\n                        if (includeRepaired || !sstable.isRepaired())\n                            sstables.add(sstable);\n                    }\n                }\n\n                logger.trace(\"ViewFilter for {}/{} sstables\", sstables.size(), getSSTables().size());\n                return ImmutableList.copyOf(sstables);\n            }\n        };\n    }"
        ],
        [
            "StorageProxy::getRangeSlice(AbstractRangeCommand,ConsistencyLevel)",
            "1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713 -\n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899 -\n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  ",
            "    public static List<Row> getRangeSlice(AbstractRangeCommand command, ConsistencyLevel consistency_level)\n    throws UnavailableException, ReadFailureException, ReadTimeoutException\n    {\n        Tracing.trace(\"Computing ranges to query\");\n        long startTime = System.nanoTime();\n\n        Keyspace keyspace = Keyspace.open(command.keyspace);\n        List<Row> rows;\n        // now scan until we have enough results\n        try\n        {\n            int liveRowCount = 0;\n            boolean countLiveRows = command.countCQL3Rows() || command.ignoredTombstonedPartitions();\n            rows = new ArrayList<>();\n\n            // when dealing with LocalStrategy keyspaces, we can skip the range splitting and merging (which can be\n            // expensive in clusters with vnodes)\n            List<? extends AbstractBounds<RowPosition>> ranges;\n            if (keyspace.getReplicationStrategy() instanceof LocalStrategy)\n                ranges = command.keyRange.unwrap();\n            else\n                ranges = getRestrictedRanges(command.keyRange);\n\n            // determine the number of rows to be fetched and the concurrency factor\n            int rowsToBeFetched = command.limit();\n            int concurrencyFactor;\n            if (command.requiresScanningAllRanges())\n            {\n                // all nodes must be queried\n                rowsToBeFetched *= ranges.size();\n                concurrencyFactor = ranges.size();\n                logger.debug(\"Requested rows: {}, ranges.size(): {}; concurrent range requests: {}\",\n                             command.limit(),\n                             ranges.size(),\n                             concurrencyFactor);\n                Tracing.trace(\"Submitting range requests on {} ranges with a concurrency of {}\",\n                              ranges.size(), concurrencyFactor);\n            }\n            else\n            {\n                // our estimate of how many result rows there will be per-range\n                float resultRowsPerRange = estimateResultRowsPerRange(command, keyspace);\n                // underestimate how many rows we will get per-range in order to increase the likelihood that we'll\n                // fetch enough rows in the first round\n                resultRowsPerRange -= resultRowsPerRange * CONCURRENT_SUBREQUESTS_MARGIN;\n                concurrencyFactor = resultRowsPerRange == 0.0\n                                  ? 1\n                                  : Math.max(1, Math.min(ranges.size(), (int) Math.ceil(command.limit() / resultRowsPerRange)));\n\n                logger.debug(\"Estimated result rows per range: {}; requested rows: {}, ranges.size(): {}; concurrent range requests: {}\",\n                             resultRowsPerRange,\n                             command.limit(),\n                             ranges.size(),\n                             concurrencyFactor);\n                Tracing.trace(\"Submitting range requests on {} ranges with a concurrency of {} ({} rows per range expected)\",\n                              ranges.size(),\n                              concurrencyFactor,\n                              resultRowsPerRange);\n            }\n\n            boolean haveSufficientRows = false;\n            int i = 0;\n            AbstractBounds<RowPosition> nextRange = null;\n            List<InetAddress> nextEndpoints = null;\n            List<InetAddress> nextFilteredEndpoints = null;\n            while (i < ranges.size())\n            {\n                List<Pair<AbstractRangeCommand, ReadCallback<RangeSliceReply, Iterable<Row>>>> scanHandlers = new ArrayList<>(concurrencyFactor);\n                int concurrentFetchStartingIndex = i;\n                int concurrentRequests = 0;\n                while ((i - concurrentFetchStartingIndex) < concurrencyFactor)\n                {\n                    AbstractBounds<RowPosition> range = nextRange == null\n                                                      ? ranges.get(i)\n                                                      : nextRange;\n                    List<InetAddress> liveEndpoints = nextEndpoints == null\n                                                    ? getLiveSortedEndpoints(keyspace, range.right)\n                                                    : nextEndpoints;\n                    List<InetAddress> filteredEndpoints = nextFilteredEndpoints == null\n                                                        ? consistency_level.filterForQuery(keyspace, liveEndpoints)\n                                                        : nextFilteredEndpoints;\n                    ++i;\n                    ++concurrentRequests;\n\n                    // getRestrictedRange has broken the queried range into per-[vnode] token ranges, but this doesn't take\n                    // the replication factor into account. If the intersection of live endpoints for 2 consecutive ranges\n                    // still meets the CL requirements, then we can merge both ranges into the same RangeSliceCommand.\n                    while (i < ranges.size())\n                    {\n                        nextRange = ranges.get(i);\n                        nextEndpoints = getLiveSortedEndpoints(keyspace, nextRange.right);\n                        nextFilteredEndpoints = consistency_level.filterForQuery(keyspace, nextEndpoints);\n\n                        // If the current range right is the min token, we should stop merging because CFS.getRangeSlice\n                        // don't know how to deal with a wrapping range.\n                        // Note: it would be slightly more efficient to have CFS.getRangeSlice on the destination nodes unwraps\n                        // the range if necessary and deal with it. However, we can't start sending wrapped range without breaking\n                        // wire compatibility, so It's likely easier not to bother;\n                        if (range.right.isMinimum())\n                            break;\n\n                        List<InetAddress> merged = intersection(liveEndpoints, nextEndpoints);\n\n                        // Check if there is enough endpoint for the merge to be possible.\n                        if (!consistency_level.isSufficientLiveNodes(keyspace, merged))\n                            break;\n\n                        List<InetAddress> filteredMerged = consistency_level.filterForQuery(keyspace, merged);\n\n                        // Estimate whether merging will be a win or not\n                        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(filteredMerged, filteredEndpoints, nextFilteredEndpoints))\n                            break;\n\n                        // If we get there, merge this range and the next one\n                        range = range.withNewRight(nextRange.right);\n                        liveEndpoints = merged;\n                        filteredEndpoints = filteredMerged;\n                        ++i;\n                    }\n\n                    AbstractRangeCommand nodeCmd = command.forSubRange(range);\n\n                    // collect replies and resolve according to consistency level\n                    RangeSliceResponseResolver resolver = new RangeSliceResponseResolver(nodeCmd.keyspace, command.timestamp);\n                    List<InetAddress> minimalEndpoints = filteredEndpoints.subList(0, Math.min(filteredEndpoints.size(), consistency_level.blockFor(keyspace)));\n                    ReadCallback<RangeSliceReply, Iterable<Row>> handler = new ReadCallback<>(resolver, consistency_level, nodeCmd, minimalEndpoints);\n                    handler.assureSufficientLiveNodes();\n                    resolver.setSources(filteredEndpoints);\n                    if (filteredEndpoints.size() == 1\n                        && filteredEndpoints.get(0).equals(FBUtilities.getBroadcastAddress())\n                        && OPTIMIZE_LOCAL_REQUESTS)\n                    {\n                        StageManager.getStage(Stage.READ).execute(new LocalRangeSliceRunnable(nodeCmd, handler), Tracing.instance.get());\n                    }\n                    else\n                    {\n                        MessageOut<? extends AbstractRangeCommand> message = nodeCmd.createMessage();\n                        for (InetAddress endpoint : filteredEndpoints)\n                        {\n                            Tracing.trace(\"Enqueuing request to {}\", endpoint);\n                            MessagingService.instance().sendRRWithFailure(message, endpoint, handler);\n                        }\n                    }\n                    scanHandlers.add(Pair.create(nodeCmd, handler));\n                }\n                Tracing.trace(\"Submitted {} concurrent range requests covering {} ranges\", concurrentRequests, i - concurrentFetchStartingIndex);\n\n                List<AsyncOneResponse> repairResponses = new ArrayList<>();\n                for (Pair<AbstractRangeCommand, ReadCallback<RangeSliceReply, Iterable<Row>>> cmdPairHandler : scanHandlers)\n                {\n                    ReadCallback<RangeSliceReply, Iterable<Row>> handler = cmdPairHandler.right;\n                    RangeSliceResponseResolver resolver = (RangeSliceResponseResolver)handler.resolver;\n\n                    try\n                    {\n                        for (Row row : handler.get())\n                        {\n                            rows.add(row);\n                            if (countLiveRows)\n                                liveRowCount += row.getLiveCount(command.predicate, command.timestamp);\n                        }\n                        repairResponses.addAll(resolver.repairResults);\n                    }\n                    catch (ReadTimeoutException|ReadFailureException ex)\n                    {\n                        // we timed out or failed waiting for responses\n                        int blockFor = consistency_level.blockFor(keyspace);\n                        int responseCount = resolver.responses.size();\n                        String gotData = responseCount > 0\n                                         ? resolver.isDataPresent() ? \" (including data)\" : \" (only digests)\"\n                                         : \"\";\n\n                        boolean isTimeout = ex instanceof ReadTimeoutException;\n                        if (Tracing.isTracing())\n                        {\n                            Tracing.trace(\"{}; received {} of {} responses{} for range {} of {}\",\n                                          (isTimeout ? \"Timed out\" : \"Failed\"), responseCount, blockFor, gotData, i, ranges.size());\n                        }\n                        else if (logger.isDebugEnabled())\n                        {\n                            logger.debug(\"Range slice {}; received {} of {} responses{} for range {} of {}\",\n                                         (isTimeout ? \"timeout\" : \"failure\"), responseCount, blockFor, gotData, i, ranges.size());\n                        }\n                        throw ex;\n                    }\n                    catch (DigestMismatchException e)\n                    {\n                        throw new AssertionError(e); // no digests in range slices yet\n                    }\n\n                    // if we're done, great, otherwise, move to the next range\n                    int count = countLiveRows ? liveRowCount : rows.size();\n                    if (count >= rowsToBeFetched)\n                    {\n                        haveSufficientRows = true;\n                        break;\n                    }\n                }\n\n                try\n                {\n                    FBUtilities.waitOnFutures(repairResponses, DatabaseDescriptor.getWriteRpcTimeout());\n                }\n                catch (TimeoutException ex)\n                {\n                    // We got all responses, but timed out while repairing\n                    int blockFor = consistency_level.blockFor(keyspace);\n                    if (Tracing.isTracing())\n                        Tracing.trace(\"Timed out while read-repairing after receiving all {} data and digest responses\", blockFor);\n                    else\n                        logger.debug(\"Range slice timeout while read-repairing after receiving all {} data and digest responses\", blockFor);\n                    throw new ReadTimeoutException(consistency_level, blockFor-1, blockFor, true);\n                }\n\n                if (haveSufficientRows)\n                    return command.postReconciliationProcessing(rows);\n\n                // we didn't get enough rows in our concurrent fetch; recalculate our concurrency factor\n                // based on the results we've seen so far (as long as we still have ranges left to query)\n                if (i < ranges.size())\n                {\n                    float fetchedRows = countLiveRows ? liveRowCount : rows.size();\n                    float remainingRows = rowsToBeFetched - fetchedRows;\n                    float actualRowsPerRange;\n                    if (fetchedRows == 0.0)\n                    {\n                        // we haven't actually gotten any results, so query all remaining ranges at once\n                        actualRowsPerRange = 0.0f;\n                        concurrencyFactor = ranges.size() - i;\n                    }\n                    else\n                    {\n                        actualRowsPerRange = fetchedRows / i;\n                        concurrencyFactor = Math.max(1, Math.min(ranges.size() - i, Math.round(remainingRows / actualRowsPerRange)));\n                    }\n                    logger.debug(\"Didn't get enough response rows; actual rows per range: {}; remaining rows: {}, new concurrent requests: {}\",\n                                 actualRowsPerRange, (int) remainingRows, concurrencyFactor);\n                }\n            }\n        }\n        finally\n        {\n            long latency = System.nanoTime() - startTime;\n            rangeMetrics.addNano(latency);\n            Keyspace.open(command.keyspace).getColumnFamilyStore(command.columnFamily).metric.coordinatorScanLatency.update(latency, TimeUnit.NANOSECONDS);\n        }\n        return command.postReconciliationProcessing(rows);\n    }",
            "1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713 +\n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899 +\n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  ",
            "    public static List<Row> getRangeSlice(AbstractRangeCommand command, ConsistencyLevel consistency_level)\n    throws UnavailableException, ReadFailureException, ReadTimeoutException\n    {\n        Tracing.trace(\"Computing ranges to query\");\n        long startTime = System.nanoTime();\n\n        Keyspace keyspace = Keyspace.open(command.keyspace);\n        List<Row> rows;\n        // now scan until we have enough results\n        try\n        {\n            int liveRowCount = 0;\n            boolean countLiveRows = command.countCQL3Rows() || command.ignoredTombstonedPartitions();\n            rows = new ArrayList<>();\n\n            // when dealing with LocalStrategy keyspaces, we can skip the range splitting and merging (which can be\n            // expensive in clusters with vnodes)\n            List<? extends AbstractBounds<RowPosition>> ranges;\n            if (keyspace.getReplicationStrategy() instanceof LocalStrategy)\n                ranges = command.keyRange.unwrap();\n            else\n                ranges = getRestrictedRanges(command.keyRange);\n\n            // determine the number of rows to be fetched and the concurrency factor\n            int rowsToBeFetched = command.limit();\n            int concurrencyFactor;\n            if (command.requiresScanningAllRanges())\n            {\n                // all nodes must be queried\n                rowsToBeFetched *= ranges.size();\n                concurrencyFactor = ranges.size();\n                logger.debug(\"Requested rows: {}, ranges.size(): {}; concurrent range requests: {}\",\n                             command.limit(),\n                             ranges.size(),\n                             concurrencyFactor);\n                Tracing.trace(\"Submitting range requests on {} ranges with a concurrency of {}\",\n                              ranges.size(), concurrencyFactor);\n            }\n            else\n            {\n                // our estimate of how many result rows there will be per-range\n                float resultRowsPerRange = estimateResultRowsPerRange(command, keyspace);\n                // underestimate how many rows we will get per-range in order to increase the likelihood that we'll\n                // fetch enough rows in the first round\n                resultRowsPerRange -= resultRowsPerRange * CONCURRENT_SUBREQUESTS_MARGIN;\n                concurrencyFactor = resultRowsPerRange == 0.0\n                                  ? 1\n                                  : Math.max(1, Math.min(ranges.size(), (int) Math.ceil(command.limit() / resultRowsPerRange)));\n\n                logger.trace(\"Estimated result rows per range: {}; requested rows: {}, ranges.size(): {}; concurrent range requests: {}\",\n                             resultRowsPerRange,\n                             command.limit(),\n                             ranges.size(),\n                             concurrencyFactor);\n                Tracing.trace(\"Submitting range requests on {} ranges with a concurrency of {} ({} rows per range expected)\",\n                              ranges.size(),\n                              concurrencyFactor,\n                              resultRowsPerRange);\n            }\n\n            boolean haveSufficientRows = false;\n            int i = 0;\n            AbstractBounds<RowPosition> nextRange = null;\n            List<InetAddress> nextEndpoints = null;\n            List<InetAddress> nextFilteredEndpoints = null;\n            while (i < ranges.size())\n            {\n                List<Pair<AbstractRangeCommand, ReadCallback<RangeSliceReply, Iterable<Row>>>> scanHandlers = new ArrayList<>(concurrencyFactor);\n                int concurrentFetchStartingIndex = i;\n                int concurrentRequests = 0;\n                while ((i - concurrentFetchStartingIndex) < concurrencyFactor)\n                {\n                    AbstractBounds<RowPosition> range = nextRange == null\n                                                      ? ranges.get(i)\n                                                      : nextRange;\n                    List<InetAddress> liveEndpoints = nextEndpoints == null\n                                                    ? getLiveSortedEndpoints(keyspace, range.right)\n                                                    : nextEndpoints;\n                    List<InetAddress> filteredEndpoints = nextFilteredEndpoints == null\n                                                        ? consistency_level.filterForQuery(keyspace, liveEndpoints)\n                                                        : nextFilteredEndpoints;\n                    ++i;\n                    ++concurrentRequests;\n\n                    // getRestrictedRange has broken the queried range into per-[vnode] token ranges, but this doesn't take\n                    // the replication factor into account. If the intersection of live endpoints for 2 consecutive ranges\n                    // still meets the CL requirements, then we can merge both ranges into the same RangeSliceCommand.\n                    while (i < ranges.size())\n                    {\n                        nextRange = ranges.get(i);\n                        nextEndpoints = getLiveSortedEndpoints(keyspace, nextRange.right);\n                        nextFilteredEndpoints = consistency_level.filterForQuery(keyspace, nextEndpoints);\n\n                        // If the current range right is the min token, we should stop merging because CFS.getRangeSlice\n                        // don't know how to deal with a wrapping range.\n                        // Note: it would be slightly more efficient to have CFS.getRangeSlice on the destination nodes unwraps\n                        // the range if necessary and deal with it. However, we can't start sending wrapped range without breaking\n                        // wire compatibility, so It's likely easier not to bother;\n                        if (range.right.isMinimum())\n                            break;\n\n                        List<InetAddress> merged = intersection(liveEndpoints, nextEndpoints);\n\n                        // Check if there is enough endpoint for the merge to be possible.\n                        if (!consistency_level.isSufficientLiveNodes(keyspace, merged))\n                            break;\n\n                        List<InetAddress> filteredMerged = consistency_level.filterForQuery(keyspace, merged);\n\n                        // Estimate whether merging will be a win or not\n                        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(filteredMerged, filteredEndpoints, nextFilteredEndpoints))\n                            break;\n\n                        // If we get there, merge this range and the next one\n                        range = range.withNewRight(nextRange.right);\n                        liveEndpoints = merged;\n                        filteredEndpoints = filteredMerged;\n                        ++i;\n                    }\n\n                    AbstractRangeCommand nodeCmd = command.forSubRange(range);\n\n                    // collect replies and resolve according to consistency level\n                    RangeSliceResponseResolver resolver = new RangeSliceResponseResolver(nodeCmd.keyspace, command.timestamp);\n                    List<InetAddress> minimalEndpoints = filteredEndpoints.subList(0, Math.min(filteredEndpoints.size(), consistency_level.blockFor(keyspace)));\n                    ReadCallback<RangeSliceReply, Iterable<Row>> handler = new ReadCallback<>(resolver, consistency_level, nodeCmd, minimalEndpoints);\n                    handler.assureSufficientLiveNodes();\n                    resolver.setSources(filteredEndpoints);\n                    if (filteredEndpoints.size() == 1\n                        && filteredEndpoints.get(0).equals(FBUtilities.getBroadcastAddress())\n                        && OPTIMIZE_LOCAL_REQUESTS)\n                    {\n                        StageManager.getStage(Stage.READ).execute(new LocalRangeSliceRunnable(nodeCmd, handler), Tracing.instance.get());\n                    }\n                    else\n                    {\n                        MessageOut<? extends AbstractRangeCommand> message = nodeCmd.createMessage();\n                        for (InetAddress endpoint : filteredEndpoints)\n                        {\n                            Tracing.trace(\"Enqueuing request to {}\", endpoint);\n                            MessagingService.instance().sendRRWithFailure(message, endpoint, handler);\n                        }\n                    }\n                    scanHandlers.add(Pair.create(nodeCmd, handler));\n                }\n                Tracing.trace(\"Submitted {} concurrent range requests covering {} ranges\", concurrentRequests, i - concurrentFetchStartingIndex);\n\n                List<AsyncOneResponse> repairResponses = new ArrayList<>();\n                for (Pair<AbstractRangeCommand, ReadCallback<RangeSliceReply, Iterable<Row>>> cmdPairHandler : scanHandlers)\n                {\n                    ReadCallback<RangeSliceReply, Iterable<Row>> handler = cmdPairHandler.right;\n                    RangeSliceResponseResolver resolver = (RangeSliceResponseResolver)handler.resolver;\n\n                    try\n                    {\n                        for (Row row : handler.get())\n                        {\n                            rows.add(row);\n                            if (countLiveRows)\n                                liveRowCount += row.getLiveCount(command.predicate, command.timestamp);\n                        }\n                        repairResponses.addAll(resolver.repairResults);\n                    }\n                    catch (ReadTimeoutException|ReadFailureException ex)\n                    {\n                        // we timed out or failed waiting for responses\n                        int blockFor = consistency_level.blockFor(keyspace);\n                        int responseCount = resolver.responses.size();\n                        String gotData = responseCount > 0\n                                         ? resolver.isDataPresent() ? \" (including data)\" : \" (only digests)\"\n                                         : \"\";\n\n                        boolean isTimeout = ex instanceof ReadTimeoutException;\n                        if (Tracing.isTracing())\n                        {\n                            Tracing.trace(\"{}; received {} of {} responses{} for range {} of {}\",\n                                          (isTimeout ? \"Timed out\" : \"Failed\"), responseCount, blockFor, gotData, i, ranges.size());\n                        }\n                        else if (logger.isDebugEnabled())\n                        {\n                            logger.debug(\"Range slice {}; received {} of {} responses{} for range {} of {}\",\n                                         (isTimeout ? \"timeout\" : \"failure\"), responseCount, blockFor, gotData, i, ranges.size());\n                        }\n                        throw ex;\n                    }\n                    catch (DigestMismatchException e)\n                    {\n                        throw new AssertionError(e); // no digests in range slices yet\n                    }\n\n                    // if we're done, great, otherwise, move to the next range\n                    int count = countLiveRows ? liveRowCount : rows.size();\n                    if (count >= rowsToBeFetched)\n                    {\n                        haveSufficientRows = true;\n                        break;\n                    }\n                }\n\n                try\n                {\n                    FBUtilities.waitOnFutures(repairResponses, DatabaseDescriptor.getWriteRpcTimeout());\n                }\n                catch (TimeoutException ex)\n                {\n                    // We got all responses, but timed out while repairing\n                    int blockFor = consistency_level.blockFor(keyspace);\n                    if (Tracing.isTracing())\n                        Tracing.trace(\"Timed out while read-repairing after receiving all {} data and digest responses\", blockFor);\n                    else\n                        logger.debug(\"Range slice timeout while read-repairing after receiving all {} data and digest responses\", blockFor);\n                    throw new ReadTimeoutException(consistency_level, blockFor-1, blockFor, true);\n                }\n\n                if (haveSufficientRows)\n                    return command.postReconciliationProcessing(rows);\n\n                // we didn't get enough rows in our concurrent fetch; recalculate our concurrency factor\n                // based on the results we've seen so far (as long as we still have ranges left to query)\n                if (i < ranges.size())\n                {\n                    float fetchedRows = countLiveRows ? liveRowCount : rows.size();\n                    float remainingRows = rowsToBeFetched - fetchedRows;\n                    float actualRowsPerRange;\n                    if (fetchedRows == 0.0)\n                    {\n                        // we haven't actually gotten any results, so query all remaining ranges at once\n                        actualRowsPerRange = 0.0f;\n                        concurrencyFactor = ranges.size() - i;\n                    }\n                    else\n                    {\n                        actualRowsPerRange = fetchedRows / i;\n                        concurrencyFactor = Math.max(1, Math.min(ranges.size() - i, Math.round(remainingRows / actualRowsPerRange)));\n                    }\n                    logger.trace(\"Didn't get enough response rows; actual rows per range: {}; remaining rows: {}, new concurrent requests: {}\",\n                                 actualRowsPerRange, (int) remainingRows, concurrencyFactor);\n                }\n            }\n        }\n        finally\n        {\n            long latency = System.nanoTime() - startTime;\n            rangeMetrics.addNano(latency);\n            Keyspace.open(command.keyspace).getColumnFamilyStore(command.columnFamily).metric.coordinatorScanLatency.update(latency, TimeUnit.NANOSECONDS);\n        }\n        return command.postReconciliationProcessing(rows);\n    }"
        ],
        [
            "CassandraServer::system_add_column_family(CfDef)",
            "1495  \n1496  \n1497  \n1498 -\n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  ",
            "    public String system_add_column_family(CfDef cf_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.debug(\"add_column_family\");\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasKeyspaceAccess(keyspace, Permission.CREATE);\n            cf_def.unsetId(); // explicitly ignore any id set by client (Hector likes to set zero)\n            CFMetaData cfm = ThriftConversion.fromThrift(cf_def);\n            CFMetaData.validateCompactionOptions(cfm.compactionStrategyClass, cfm.compactionStrategyOptions);\n            cfm.addDefaultIndexNames();\n\n            if (!cfm.getTriggers().isEmpty())\n                state().ensureIsSuper(\"Only superusers are allowed to add triggers.\");\n\n            MigrationManager.announceNewColumnFamily(cfm);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }",
            "1495  \n1496  \n1497  \n1498 +\n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  ",
            "    public String system_add_column_family(CfDef cf_def)\n    throws InvalidRequestException, SchemaDisagreementException, TException\n    {\n        logger.trace(\"add_column_family\");\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n            cState.hasKeyspaceAccess(keyspace, Permission.CREATE);\n            cf_def.unsetId(); // explicitly ignore any id set by client (Hector likes to set zero)\n            CFMetaData cfm = ThriftConversion.fromThrift(cf_def);\n            CFMetaData.validateCompactionOptions(cfm.compactionStrategyClass, cfm.compactionStrategyOptions);\n            cfm.addDefaultIndexNames();\n\n            if (!cfm.getTriggers().isEmpty())\n                state().ensureIsSuper(\"Only superusers are allowed to add triggers.\");\n\n            MigrationManager.announceNewColumnFamily(cfm);\n            return Schema.instance.getVersion().toString();\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n    }"
        ],
        [
            "MessagingService::SocketThread::close()",
            "1017  \n1018  \n1019 -\n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  ",
            "        void close() throws IOException\n        {\n            logger.debug(\"Closing accept() thread\");\n\n            try\n            {\n                server.close();\n            }\n            catch (IOException e)\n            {\n                // dirty hack for clean shutdown on OSX w/ Java >= 1.8.0_20\n                // see https://issues.apache.org/jira/browse/CASSANDRA-8220\n                // see https://bugs.openjdk.java.net/browse/JDK-8050499\n                if (!\"Unknown error: 316\".equals(e.getMessage()) || !\"Mac OS X\".equals(System.getProperty(\"os.name\")))\n                    throw e;\n            }\n\n            for (Closeable connection : connections)\n            {\n                connection.close();\n            }\n        }",
            "1017  \n1018  \n1019 +\n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  ",
            "        void close() throws IOException\n        {\n            logger.trace(\"Closing accept() thread\");\n\n            try\n            {\n                server.close();\n            }\n            catch (IOException e)\n            {\n                // dirty hack for clean shutdown on OSX w/ Java >= 1.8.0_20\n                // see https://issues.apache.org/jira/browse/CASSANDRA-8220\n                // see https://bugs.openjdk.java.net/browse/JDK-8050499\n                if (!\"Unknown error: 316\".equals(e.getMessage()) || !\"Mac OS X\".equals(System.getProperty(\"os.name\")))\n                    throw e;\n            }\n\n            for (Closeable connection : connections)\n            {\n                connection.close();\n            }\n        }"
        ],
        [
            "LeveledManifest::getEstimatedTasks()",
            " 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706 -\n 707  \n 708  \n 709  ",
            "    public synchronized int getEstimatedTasks()\n    {\n        long tasks = 0;\n        long[] estimated = new long[generations.length];\n\n        for (int i = generations.length - 1; i >= 0; i--)\n        {\n            List<SSTableReader> sstables = getLevel(i);\n            // If there is 1 byte over TBL - (MBL * 1.001), there is still a task left, so we need to round up.\n            estimated[i] = (long)Math.ceil((double)Math.max(0L, SSTableReader.getTotalBytes(sstables) - (long)(maxBytesForLevel(i, maxSSTableSizeInBytes) * 1.001)) / (double)maxSSTableSizeInBytes);\n            tasks += estimated[i];\n        }\n\n        logger.debug(\"Estimating {} compactions to do for {}.{}\",\n                     Arrays.toString(estimated), cfs.keyspace.getName(), cfs.name);\n        return Ints.checkedCast(tasks);\n    }",
            " 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706 +\n 707  \n 708  \n 709  ",
            "    public synchronized int getEstimatedTasks()\n    {\n        long tasks = 0;\n        long[] estimated = new long[generations.length];\n\n        for (int i = generations.length - 1; i >= 0; i--)\n        {\n            List<SSTableReader> sstables = getLevel(i);\n            // If there is 1 byte over TBL - (MBL * 1.001), there is still a task left, so we need to round up.\n            estimated[i] = (long)Math.ceil((double)Math.max(0L, SSTableReader.getTotalBytes(sstables) - (long)(maxBytesForLevel(i, maxSSTableSizeInBytes) * 1.001)) / (double)maxSSTableSizeInBytes);\n            tasks += estimated[i];\n        }\n\n        logger.trace(\"Estimating {} compactions to do for {}.{}\",\n                     Arrays.toString(estimated), cfs.keyspace.getName(), cfs.name);\n        return Ints.checkedCast(tasks);\n    }"
        ],
        [
            "LifecycleTransaction::doAbort(Throwable)",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171 -\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "    /**\n     * undo all of the changes made by this transaction, resetting the state to its original form\n     */\n    public Throwable doAbort(Throwable accumulate)\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"Aborting transaction over {}, with ({},{}) logged and ({},{}) staged\", originals, logged.update, logged.obsolete, staged.update, staged.obsolete);\n\n        if (logged.isEmpty() && staged.isEmpty())\n            return accumulate;\n\n        // mark obsolete all readers that are not versions of those present in the original set\n        Iterable<SSTableReader> obsolete = filterOut(concatUniq(staged.update, logged.update), originals);\n        logger.debug(\"Obsoleting {}\", obsolete);\n        // we don't pass the tracker in for the obsoletion, since these readers have never been notified externally\n        // nor had their size accounting affected\n        accumulate = markObsolete(null, obsolete, accumulate);\n\n        // replace all updated readers with a version restored to its original state\n        accumulate = tracker.apply(updateLiveSet(logged.update, restoreUpdatedOriginals()), accumulate);\n        // setReplaced immediately preceding versions that have not been obsoleted\n        accumulate = setReplaced(logged.update, accumulate);\n        // we have replaced all of logged.update and never made visible staged.update,\n        // and the files we have logged as obsolete we clone fresh versions of, so they are no longer needed either\n        // any _staged_ obsoletes should either be in staged.update already, and dealt with there,\n        // or is still in its original form (so left as is); in either case no extra action is needed\n        accumulate = release(selfRefs(concat(staged.update, logged.update, logged.obsolete)), accumulate);\n        logged.clear();\n        staged.clear();\n        return accumulate;\n    }",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "    /**\n     * undo all of the changes made by this transaction, resetting the state to its original form\n     */\n    public Throwable doAbort(Throwable accumulate)\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"Aborting transaction over {}, with ({},{}) logged and ({},{}) staged\", originals, logged.update, logged.obsolete, staged.update, staged.obsolete);\n\n        if (logged.isEmpty() && staged.isEmpty())\n            return accumulate;\n\n        // mark obsolete all readers that are not versions of those present in the original set\n        Iterable<SSTableReader> obsolete = filterOut(concatUniq(staged.update, logged.update), originals);\n        logger.trace(\"Obsoleting {}\", obsolete);\n        // we don't pass the tracker in for the obsoletion, since these readers have never been notified externally\n        // nor had their size accounting affected\n        accumulate = markObsolete(null, obsolete, accumulate);\n\n        // replace all updated readers with a version restored to its original state\n        accumulate = tracker.apply(updateLiveSet(logged.update, restoreUpdatedOriginals()), accumulate);\n        // setReplaced immediately preceding versions that have not been obsoleted\n        accumulate = setReplaced(logged.update, accumulate);\n        // we have replaced all of logged.update and never made visible staged.update,\n        // and the files we have logged as obsolete we clone fresh versions of, so they are no longer needed either\n        // any _staged_ obsoletes should either be in staged.update already, and dealt with there,\n        // or is still in its original form (so left as is); in either case no extra action is needed\n        accumulate = release(selfRefs(concat(staged.update, logged.update, logged.obsolete)), accumulate);\n        logged.clear();\n        staged.clear();\n        return accumulate;\n    }"
        ],
        [
            "LimitedLocalNodeFirstLocalBalancingPolicy::onAdd(Host)",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  ",
            "    @Override\n    public void onAdd(Host host)\n    {\n        if (replicaAddresses.contains(host.getAddress()))\n        {\n            liveReplicaHosts.add(host);\n            logger.debug(\"Added a new host {}\", host);\n        }\n    }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  ",
            "    @Override\n    public void onAdd(Host host)\n    {\n        if (replicaAddresses.contains(host.getAddress()))\n        {\n            liveReplicaHosts.add(host);\n            logger.trace(\"Added a new host {}\", host);\n        }\n    }"
        ],
        [
            "RowDigestResolver::resolve()",
            "  64  \n  65  \n  66 -\n  67 -\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102 -\n 103  \n 104  ",
            "    public Row resolve() throws DigestMismatchException\n    {\n        if (logger.isDebugEnabled())\n            logger.debug(\"resolving {} responses\", replies.size());\n\n        long start = System.nanoTime();\n\n        // validate digests against each other; throw immediately on mismatch.\n        // also extract the data reply, if any.\n        ColumnFamily data = null;\n        ByteBuffer digest = null;\n\n        for (MessageIn<ReadResponse> message : replies)\n        {\n            ReadResponse response = message.payload;\n\n            ByteBuffer newDigest;\n            if (response.isDigestQuery())\n            {\n                newDigest = response.digest();\n            }\n            else\n            {\n                // note that this allows for multiple data replies, post-CASSANDRA-5932\n                data = response.row().cf;\n                if (response.digest() == null)\n                    message.payload.setDigest(ColumnFamily.digest(data));\n\n                newDigest = response.digest();\n            }\n\n            if (digest == null)\n                digest = newDigest;\n            else if (!digest.equals(newDigest))\n                throw new DigestMismatchException(key, digest, newDigest);\n        }\n\n        if (logger.isDebugEnabled())\n            logger.debug(\"resolve: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n        return new Row(key, data);\n    }",
            "  64  \n  65  \n  66 +\n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102 +\n 103  \n 104  ",
            "    public Row resolve() throws DigestMismatchException\n    {\n        if (logger.isTraceEnabled())\n            logger.trace(\"resolving {} responses\", replies.size());\n\n        long start = System.nanoTime();\n\n        // validate digests against each other; throw immediately on mismatch.\n        // also extract the data reply, if any.\n        ColumnFamily data = null;\n        ByteBuffer digest = null;\n\n        for (MessageIn<ReadResponse> message : replies)\n        {\n            ReadResponse response = message.payload;\n\n            ByteBuffer newDigest;\n            if (response.isDigestQuery())\n            {\n                newDigest = response.digest();\n            }\n            else\n            {\n                // note that this allows for multiple data replies, post-CASSANDRA-5932\n                data = response.row().cf;\n                if (response.digest() == null)\n                    message.payload.setDigest(ColumnFamily.digest(data));\n\n                newDigest = response.digest();\n            }\n\n            if (digest == null)\n                digest = newDigest;\n            else if (!digest.equals(newDigest))\n                throw new DigestMismatchException(key, digest, newDigest);\n        }\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"resolve: {} ms.\", TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n        return new Row(key, data);\n    }"
        ],
        [
            "SSTableReader::loadSummary(SegmentedFile,SegmentedFile)",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846 -\n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  ",
            "    /**\n     * Load index summary from Summary.db file if it exists.\n     *\n     * if loaded index summary has different index interval from current value stored in schema,\n     * then Summary.db file will be deleted and this returns false to rebuild summary.\n     *\n     * @param ibuilder\n     * @param dbuilder\n     * @return true if index summary is loaded successfully from Summary.db file.\n     */\n    @SuppressWarnings(\"resource\")\n    public boolean loadSummary(SegmentedFile.Builder ibuilder, SegmentedFile.Builder dbuilder)\n    {\n        File summariesFile = new File(descriptor.filenameFor(Component.SUMMARY));\n        if (!summariesFile.exists())\n            return false;\n\n        DataInputStream iStream = null;\n        try\n        {\n            iStream = new DataInputStream(new FileInputStream(summariesFile));\n            indexSummary = IndexSummary.serializer.deserialize(\n                    iStream, partitioner, descriptor.version.hasSamplingLevel(),\n                    metadata.getMinIndexInterval(), metadata.getMaxIndexInterval());\n            first = partitioner.decorateKey(ByteBufferUtil.readWithLength(iStream));\n            last = partitioner.decorateKey(ByteBufferUtil.readWithLength(iStream));\n            ibuilder.deserializeBounds(iStream);\n            dbuilder.deserializeBounds(iStream);\n        }\n        catch (IOException e)\n        {\n            if (indexSummary != null)\n                indexSummary.close();\n            logger.debug(\"Cannot deserialize SSTable Summary File {}: {}\", summariesFile.getPath(), e.getMessage());\n            // corrupted; delete it and fall back to creating a new summary\n            FileUtils.closeQuietly(iStream);\n            // delete it and fall back to creating a new summary\n            FileUtils.deleteWithConfirm(summariesFile);\n            return false;\n        }\n        finally\n        {\n            FileUtils.closeQuietly(iStream);\n        }\n\n        return true;\n    }",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846 +\n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  ",
            "    /**\n     * Load index summary from Summary.db file if it exists.\n     *\n     * if loaded index summary has different index interval from current value stored in schema,\n     * then Summary.db file will be deleted and this returns false to rebuild summary.\n     *\n     * @param ibuilder\n     * @param dbuilder\n     * @return true if index summary is loaded successfully from Summary.db file.\n     */\n    @SuppressWarnings(\"resource\")\n    public boolean loadSummary(SegmentedFile.Builder ibuilder, SegmentedFile.Builder dbuilder)\n    {\n        File summariesFile = new File(descriptor.filenameFor(Component.SUMMARY));\n        if (!summariesFile.exists())\n            return false;\n\n        DataInputStream iStream = null;\n        try\n        {\n            iStream = new DataInputStream(new FileInputStream(summariesFile));\n            indexSummary = IndexSummary.serializer.deserialize(\n                    iStream, partitioner, descriptor.version.hasSamplingLevel(),\n                    metadata.getMinIndexInterval(), metadata.getMaxIndexInterval());\n            first = partitioner.decorateKey(ByteBufferUtil.readWithLength(iStream));\n            last = partitioner.decorateKey(ByteBufferUtil.readWithLength(iStream));\n            ibuilder.deserializeBounds(iStream);\n            dbuilder.deserializeBounds(iStream);\n        }\n        catch (IOException e)\n        {\n            if (indexSummary != null)\n                indexSummary.close();\n            logger.trace(\"Cannot deserialize SSTable Summary File {}: {}\", summariesFile.getPath(), e.getMessage());\n            // corrupted; delete it and fall back to creating a new summary\n            FileUtils.closeQuietly(iStream);\n            // delete it and fall back to creating a new summary\n            FileUtils.deleteWithConfirm(summariesFile);\n            return false;\n        }\n        finally\n        {\n            FileUtils.closeQuietly(iStream);\n        }\n\n        return true;\n    }"
        ],
        [
            "CassandraServer::add(ByteBuffer,ColumnParent,CounterColumn,ConsistencyLevel)",
            "1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744 -\n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  ",
            "    public void add(ByteBuffer key, ColumnParent column_parent, CounterColumn column, ConsistencyLevel consistency_level)\n            throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_parent\", column_parent.toString(),\n                                                                  \"column\", column.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"add\", traceParameters);\n        }\n        else\n        {\n            logger.debug(\"add\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.MODIFY);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, true);\n            ThriftValidation.validateKey(metadata, key);\n            ThriftConversion.fromThrift(consistency_level).validateCounterForWrite(metadata);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            // SuperColumn field is usually optional, but not when we're adding\n            if (metadata.cfType == ColumnFamilyType.Super && column_parent.super_column == null)\n                throw new InvalidRequestException(\"missing mandatory super column name for super CF \" + column_parent.column_family);\n\n            ThriftValidation.validateColumnNames(metadata, column_parent, Arrays.asList(column.name));\n\n            org.apache.cassandra.db.Mutation mutation = new org.apache.cassandra.db.Mutation(keyspace, key);\n            try\n            {\n                if (metadata.isSuper())\n                    mutation.addCounter(column_parent.column_family, metadata.comparator.makeCellName(column_parent.super_column, column.name), column.value);\n                else\n                    mutation.addCounter(column_parent.column_family, metadata.comparator.cellFromByteBuffer(column.name), column.value);\n            }\n            catch (MarshalException e)\n            {\n                throw new InvalidRequestException(e.getMessage());\n            }\n            doInsert(consistency_level, Arrays.asList(new CounterMutation(mutation, ThriftConversion.fromThrift(consistency_level))));\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744 +\n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  ",
            "    public void add(ByteBuffer key, ColumnParent column_parent, CounterColumn column, ConsistencyLevel consistency_level)\n            throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        if (startSessionIfRequested())\n        {\n            Map<String, String> traceParameters = ImmutableMap.of(\"column_parent\", column_parent.toString(),\n                                                                  \"column\", column.toString(),\n                                                                  \"consistency_level\", consistency_level.name());\n            Tracing.instance.begin(\"add\", traceParameters);\n        }\n        else\n        {\n            logger.trace(\"add\");\n        }\n\n        try\n        {\n            ClientState cState = state();\n            String keyspace = cState.getKeyspace();\n\n            cState.hasColumnFamilyAccess(keyspace, column_parent.column_family, Permission.MODIFY);\n\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_parent.column_family, true);\n            ThriftValidation.validateKey(metadata, key);\n            ThriftConversion.fromThrift(consistency_level).validateCounterForWrite(metadata);\n            ThriftValidation.validateColumnParent(metadata, column_parent);\n            // SuperColumn field is usually optional, but not when we're adding\n            if (metadata.cfType == ColumnFamilyType.Super && column_parent.super_column == null)\n                throw new InvalidRequestException(\"missing mandatory super column name for super CF \" + column_parent.column_family);\n\n            ThriftValidation.validateColumnNames(metadata, column_parent, Arrays.asList(column.name));\n\n            org.apache.cassandra.db.Mutation mutation = new org.apache.cassandra.db.Mutation(keyspace, key);\n            try\n            {\n                if (metadata.isSuper())\n                    mutation.addCounter(column_parent.column_family, metadata.comparator.makeCellName(column_parent.super_column, column.name), column.value);\n                else\n                    mutation.addCounter(column_parent.column_family, metadata.comparator.cellFromByteBuffer(column.name), column.value);\n            }\n            catch (MarshalException e)\n            {\n                throw new InvalidRequestException(e.getMessage());\n            }\n            doInsert(consistency_level, Arrays.asList(new CounterMutation(mutation, ThriftConversion.fromThrift(consistency_level))));\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "CommitLogSegmentManager::flushDataFrom(List,boolean)",
            " 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 -\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "    /**\n     * Force a flush on all CFs that are still dirty in @param segments.\n     *\n     * @return a Future that will finish when all the flushes are complete.\n     */\n    private Future<?> flushDataFrom(List<CommitLogSegment> segments, boolean force)\n    {\n        if (segments.isEmpty())\n            return Futures.immediateFuture(null);\n        final ReplayPosition maxReplayPosition = segments.get(segments.size() - 1).getContext();\n\n        // a map of CfId -> forceFlush() to ensure we only queue one flush per cf\n        final Map<UUID, ListenableFuture<?>> flushes = new LinkedHashMap<>();\n\n        for (CommitLogSegment segment : segments)\n        {\n            for (UUID dirtyCFId : segment.getDirtyCFIDs())\n            {\n                Pair<String,String> pair = Schema.instance.getCF(dirtyCFId);\n                if (pair == null)\n                {\n                    // even though we remove the schema entry before a final flush when dropping a CF,\n                    // it's still possible for a writer to race and finish his append after the flush.\n                    logger.debug(\"Marking clean CF {} that doesn't exist anymore\", dirtyCFId);\n                    segment.markClean(dirtyCFId, segment.getContext());\n                }\n                else if (!flushes.containsKey(dirtyCFId))\n                {\n                    String keyspace = pair.left;\n                    final ColumnFamilyStore cfs = Keyspace.open(keyspace).getColumnFamilyStore(dirtyCFId);\n                    // can safely call forceFlush here as we will only ever block (briefly) for other attempts to flush,\n                    // no deadlock possibility since switchLock removal\n                    flushes.put(dirtyCFId, force ? cfs.forceFlush() : cfs.forceFlush(maxReplayPosition));\n                }\n            }\n        }\n\n        return Futures.allAsList(flushes.values());\n    }",
            " 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "    /**\n     * Force a flush on all CFs that are still dirty in @param segments.\n     *\n     * @return a Future that will finish when all the flushes are complete.\n     */\n    private Future<?> flushDataFrom(List<CommitLogSegment> segments, boolean force)\n    {\n        if (segments.isEmpty())\n            return Futures.immediateFuture(null);\n        final ReplayPosition maxReplayPosition = segments.get(segments.size() - 1).getContext();\n\n        // a map of CfId -> forceFlush() to ensure we only queue one flush per cf\n        final Map<UUID, ListenableFuture<?>> flushes = new LinkedHashMap<>();\n\n        for (CommitLogSegment segment : segments)\n        {\n            for (UUID dirtyCFId : segment.getDirtyCFIDs())\n            {\n                Pair<String,String> pair = Schema.instance.getCF(dirtyCFId);\n                if (pair == null)\n                {\n                    // even though we remove the schema entry before a final flush when dropping a CF,\n                    // it's still possible for a writer to race and finish his append after the flush.\n                    logger.trace(\"Marking clean CF {} that doesn't exist anymore\", dirtyCFId);\n                    segment.markClean(dirtyCFId, segment.getContext());\n                }\n                else if (!flushes.containsKey(dirtyCFId))\n                {\n                    String keyspace = pair.left;\n                    final ColumnFamilyStore cfs = Keyspace.open(keyspace).getColumnFamilyStore(dirtyCFId);\n                    // can safely call forceFlush here as we will only ever block (briefly) for other attempts to flush,\n                    // no deadlock possibility since switchLock removal\n                    flushes.put(dirtyCFId, force ? cfs.forceFlush() : cfs.forceFlush(maxReplayPosition));\n                }\n            }\n        }\n\n        return Futures.allAsList(flushes.values());\n    }"
        ],
        [
            "RangeStreamer::fetchAsync()",
            " 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342 -\n 343 -\n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "    public StreamResultFuture fetchAsync()\n    {\n        for (Map.Entry<String, Map.Entry<InetAddress, Collection<Range<Token>>>> entry : toFetch.entries())\n        {\n            String keyspace = entry.getKey();\n            InetAddress source = entry.getValue().getKey();\n            InetAddress preferred = SystemKeyspace.getPreferredIP(source);\n            Collection<Range<Token>> ranges = entry.getValue().getValue();\n\n            // filter out already streamed ranges\n            Set<Range<Token>> availableRanges = stateStore.getAvailableRanges(keyspace, StorageService.getPartitioner());\n            if (ranges.removeAll(availableRanges))\n            {\n                logger.info(\"Some ranges of {} are already available. Skipping streaming those ranges.\", availableRanges);\n            }\n\n            if (logger.isDebugEnabled())\n                logger.debug(\"{}ing from {} ranges {}\", description, source, StringUtils.join(ranges, \", \"));\n            /* Send messages to respective folks to stream data over to me */\n            streamPlan.requestRanges(source, preferred, keyspace, ranges);\n        }\n\n        return streamPlan.execute();\n    }",
            " 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342 +\n 343 +\n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "    public StreamResultFuture fetchAsync()\n    {\n        for (Map.Entry<String, Map.Entry<InetAddress, Collection<Range<Token>>>> entry : toFetch.entries())\n        {\n            String keyspace = entry.getKey();\n            InetAddress source = entry.getValue().getKey();\n            InetAddress preferred = SystemKeyspace.getPreferredIP(source);\n            Collection<Range<Token>> ranges = entry.getValue().getValue();\n\n            // filter out already streamed ranges\n            Set<Range<Token>> availableRanges = stateStore.getAvailableRanges(keyspace, StorageService.getPartitioner());\n            if (ranges.removeAll(availableRanges))\n            {\n                logger.info(\"Some ranges of {} are already available. Skipping streaming those ranges.\", availableRanges);\n            }\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"{}ing from {} ranges {}\", description, source, StringUtils.join(ranges, \", \"));\n            /* Send messages to respective folks to stream data over to me */\n            streamPlan.requestRanges(source, preferred, keyspace, ranges);\n        }\n\n        return streamPlan.execute();\n    }"
        ],
        [
            "CommitLogReplayer::blockForWrites()",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "    public int blockForWrites()\n    {\n        for (Map.Entry<UUID, AtomicInteger> entry : invalidMutations.entrySet())\n            logger.warn(String.format(\"Skipped %d mutations from unknown (probably removed) CF with id %s\", entry.getValue().intValue(), entry.getKey()));\n\n        // wait for all the writes to finish on the mutation stage\n        FBUtilities.waitOnFutures(futures);\n        logger.debug(\"Finished waiting on mutations from recovery\");\n\n        // flush replayed keyspaces\n        futures.clear();\n        for (Keyspace keyspace : keyspacesRecovered)\n            futures.addAll(keyspace.flush());\n        FBUtilities.waitOnFutures(futures);\n        return replayedCount.get();\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 +\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "    public int blockForWrites()\n    {\n        for (Map.Entry<UUID, AtomicInteger> entry : invalidMutations.entrySet())\n            logger.warn(String.format(\"Skipped %d mutations from unknown (probably removed) CF with id %s\", entry.getValue().intValue(), entry.getKey()));\n\n        // wait for all the writes to finish on the mutation stage\n        FBUtilities.waitOnFutures(futures);\n        logger.trace(\"Finished waiting on mutations from recovery\");\n\n        // flush replayed keyspaces\n        futures.clear();\n        for (Keyspace keyspace : keyspacesRecovered)\n            futures.addAll(keyspace.flush());\n        FBUtilities.waitOnFutures(futures);\n        return replayedCount.get();\n    }"
        ]
    ],
    "5f1e4bcad7d36feb97096c6149ad3c43f26f0fd9": [
        [
            "PaxosState::commit(Commit)",
            " 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "    public static void commit(Commit proposal)\n    {\n        long start = System.nanoTime();\n        try\n        {\n            // There is no guarantee we will see commits in the right order, because messages\n            // can get delayed, so a proposal can be older than our current most recent ballot/commit.\n            // Committing it is however always safe due to column timestamps, so always do it. However,\n            // if our current in-progress ballot is strictly greater than the proposal one, we shouldn't\n            // erase the in-progress update.\n            // The table may have been truncated since the proposal was initiated. In that case, we\n            // don't want to perform the mutation and potentially resurrect truncated data\n            if (UUIDGen.unixTimestamp(proposal.ballot) >= SystemKeyspace.getTruncatedAt(proposal.update.metadata().cfId))\n            {\n                Tracing.trace(\"Committing proposal {}\", proposal);\n                Mutation mutation = proposal.makeMutation();\n                try\n                {\n                    Uninterruptibles.getUninterruptibly(Keyspace.open(mutation.getKeyspaceName()).apply(mutation, true));\n                }\n                catch (ExecutionException e)\n                {\n                    throw new RuntimeException(e.getCause());\n                }\n            }\n            else\n            {\n                Tracing.trace(\"Not committing proposal {} as ballot timestamp predates last truncation time\", proposal);\n            }\n            // We don't need to lock, we're just blindly updating\n            SystemKeyspace.savePaxosCommit(proposal);\n        }\n        finally\n        {\n            Keyspace.open(proposal.update.metadata().ksName).getColumnFamilyStore(proposal.update.metadata().cfId).metric.casCommit.addNano(System.nanoTime() - start);\n        }\n    }",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "    public static void commit(Commit proposal)\n    {\n        long start = System.nanoTime();\n        try\n        {\n            // There is no guarantee we will see commits in the right order, because messages\n            // can get delayed, so a proposal can be older than our current most recent ballot/commit.\n            // Committing it is however always safe due to column timestamps, so always do it. However,\n            // if our current in-progress ballot is strictly greater than the proposal one, we shouldn't\n            // erase the in-progress update.\n            // The table may have been truncated since the proposal was initiated. In that case, we\n            // don't want to perform the mutation and potentially resurrect truncated data\n            if (UUIDGen.unixTimestamp(proposal.ballot) >= SystemKeyspace.getTruncatedAt(proposal.update.metadata().cfId))\n            {\n                Tracing.trace(\"Committing proposal {}\", proposal);\n                Mutation mutation = proposal.makeMutation();\n                try\n                {\n                    Uninterruptibles.getUninterruptibly(Keyspace.open(mutation.getKeyspaceName()).apply(mutation, true));\n                }\n                catch (ExecutionException e)\n                {\n                    Throwables.propagate(e.getCause());\n                }\n            }\n            else\n            {\n                Tracing.trace(\"Not committing proposal {} as ballot timestamp predates last truncation time\", proposal);\n            }\n            // We don't need to lock, we're just blindly updating\n            SystemKeyspace.savePaxosCommit(proposal);\n        }\n        finally\n        {\n            Keyspace.open(proposal.update.metadata().ksName).getColumnFamilyStore(proposal.update.metadata().cfId).metric.casCommit.addNano(System.nanoTime() - start);\n        }\n    }"
        ],
        [
            "CommitLogReplayer::replayMutation(byte,int,long,CommitLogDescriptor)",
            " 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571 -\n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606 -\n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  ",
            "    /**\n     * Deserializes and replays a commit log entry.\n     */\n    void replayMutation(byte[] inputBuffer, int size,\n            final long entryLocation, final CommitLogDescriptor desc) throws IOException\n    {\n\n        final Mutation mutation;\n        try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size))\n        {\n            mutation = Mutation.serializer.deserialize(bufIn,\n                                                       desc.getMessagingVersion(),\n                                                       SerializationHelper.Flag.LOCAL);\n            // doublecheck that what we read is [still] valid for the current schema\n            for (PartitionUpdate upd : mutation.getPartitionUpdates())\n                upd.validate();\n        }\n        catch (UnknownColumnFamilyException ex)\n        {\n            if (ex.cfId == null)\n                return;\n            AtomicInteger i = invalidMutations.get(ex.cfId);\n            if (i == null)\n            {\n                i = new AtomicInteger(1);\n                invalidMutations.put(ex.cfId, i);\n            }\n            else\n                i.incrementAndGet();\n            return;\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            File f = File.createTempFile(\"mutation\", \"dat\");\n\n            try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f)))\n            {\n                out.write(inputBuffer, 0, size);\n            }\n\n            // Checksum passed so this error can't be permissible.\n            handleReplayError(false,\n                              \"Unexpected error deserializing mutation; saved to %s.  \" +\n                              \"This may be caused by replaying a mutation against a table with the same name but incompatible schema.  \" +\n                              \"Exception follows: %s\",\n                              f.getAbsolutePath(),\n                              t);\n            return;\n        }\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"replaying mutation for {}.{}: {}\", mutation.getKeyspaceName(), mutation.key(), \"{\" + StringUtils.join(mutation.getPartitionUpdates().iterator(), \", \") + \"}\");\n\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow() throws ExecutionException\n            {\n                if (Schema.instance.getKSMetaData(mutation.getKeyspaceName()) == null)\n                    return;\n                if (pointInTimeExceeded(mutation))\n                    return;\n\n                final Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName());\n\n                // Rebuild the mutation, omitting column families that\n                //    a) the user has requested that we ignore,\n                //    b) have already been flushed,\n                // or c) are part of a cf that was dropped.\n                // Keep in mind that the cf.name() is suspect. do every thing based on the cfid instead.\n                Mutation newMutation = null;\n                for (PartitionUpdate update : replayFilter.filter(mutation))\n                {\n                    if (Schema.instance.getCF(update.metadata().cfId) == null)\n                        continue; // dropped\n\n                    ReplayPosition rp = cfPositions.get(update.metadata().cfId);\n\n                    // replay if current segment is newer than last flushed one or,\n                    // if it is the last known segment, if we are after the replay position\n                    if (desc.id > rp.segment || (desc.id == rp.segment && entryLocation > rp.position))\n                    {\n                        if (newMutation == null)\n                            newMutation = new Mutation(mutation.getKeyspaceName(), mutation.key());\n                        newMutation.add(update);\n                        replayedCount.incrementAndGet();\n                    }\n                }\n                if (newMutation != null)\n                {\n                    assert !newMutation.isEmpty();\n                    Uninterruptibles.getUninterruptibly(Keyspace.open(newMutation.getKeyspaceName()).applyFromCommitLog(newMutation));\n                    keyspacesRecovered.add(keyspace);\n                }\n            }\n        };\n        futures.add(StageManager.getStage(Stage.MUTATION).submit(runnable));\n        if (futures.size() > MAX_OUTSTANDING_REPLAY_COUNT)\n        {\n            FBUtilities.waitOnFutures(futures);\n            futures.clear();\n        }\n    }",
            " 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572 +\n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607 +\n 608 +\n 609 +\n 610 +\n 611 +\n 612 +\n 613 +\n 614 +\n 615 +\n 616 +\n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  ",
            "    /**\n     * Deserializes and replays a commit log entry.\n     */\n    void replayMutation(byte[] inputBuffer, int size,\n            final long entryLocation, final CommitLogDescriptor desc) throws IOException\n    {\n\n        final Mutation mutation;\n        try (RebufferingInputStream bufIn = new DataInputBuffer(inputBuffer, 0, size))\n        {\n            mutation = Mutation.serializer.deserialize(bufIn,\n                                                       desc.getMessagingVersion(),\n                                                       SerializationHelper.Flag.LOCAL);\n            // doublecheck that what we read is [still] valid for the current schema\n            for (PartitionUpdate upd : mutation.getPartitionUpdates())\n                upd.validate();\n        }\n        catch (UnknownColumnFamilyException ex)\n        {\n            if (ex.cfId == null)\n                return;\n            AtomicInteger i = invalidMutations.get(ex.cfId);\n            if (i == null)\n            {\n                i = new AtomicInteger(1);\n                invalidMutations.put(ex.cfId, i);\n            }\n            else\n                i.incrementAndGet();\n            return;\n        }\n        catch (Throwable t)\n        {\n            JVMStabilityInspector.inspectThrowable(t);\n            File f = File.createTempFile(\"mutation\", \"dat\");\n\n            try (DataOutputStream out = new DataOutputStream(new FileOutputStream(f)))\n            {\n                out.write(inputBuffer, 0, size);\n            }\n\n            // Checksum passed so this error can't be permissible.\n            handleReplayError(false,\n                              \"Unexpected error deserializing mutation; saved to %s.  \" +\n                              \"This may be caused by replaying a mutation against a table with the same name but incompatible schema.  \" +\n                              \"Exception follows: %s\",\n                              f.getAbsolutePath(),\n                              t);\n            return;\n        }\n\n        if (logger.isTraceEnabled())\n            logger.trace(\"replaying mutation for {}.{}: {}\", mutation.getKeyspaceName(), mutation.key(), \"{\" + StringUtils.join(mutation.getPartitionUpdates().iterator(), \", \") + \"}\");\n\n        Runnable runnable = new WrappedRunnable()\n        {\n            public void runMayThrow()\n            {\n                if (Schema.instance.getKSMetaData(mutation.getKeyspaceName()) == null)\n                    return;\n                if (pointInTimeExceeded(mutation))\n                    return;\n\n                final Keyspace keyspace = Keyspace.open(mutation.getKeyspaceName());\n\n                // Rebuild the mutation, omitting column families that\n                //    a) the user has requested that we ignore,\n                //    b) have already been flushed,\n                // or c) are part of a cf that was dropped.\n                // Keep in mind that the cf.name() is suspect. do every thing based on the cfid instead.\n                Mutation newMutation = null;\n                for (PartitionUpdate update : replayFilter.filter(mutation))\n                {\n                    if (Schema.instance.getCF(update.metadata().cfId) == null)\n                        continue; // dropped\n\n                    ReplayPosition rp = cfPositions.get(update.metadata().cfId);\n\n                    // replay if current segment is newer than last flushed one or,\n                    // if it is the last known segment, if we are after the replay position\n                    if (desc.id > rp.segment || (desc.id == rp.segment && entryLocation > rp.position))\n                    {\n                        if (newMutation == null)\n                            newMutation = new Mutation(mutation.getKeyspaceName(), mutation.key());\n                        newMutation.add(update);\n                        replayedCount.incrementAndGet();\n                    }\n                }\n                if (newMutation != null)\n                {\n                    assert !newMutation.isEmpty();\n\n                    try\n                    {\n                        Uninterruptibles.getUninterruptibly(Keyspace.open(newMutation.getKeyspaceName()).applyFromCommitLog(newMutation));\n                    }\n                    catch (ExecutionException e)\n                    {\n                        Throwables.propagate(e.getCause());\n                    }\n\n                    keyspacesRecovered.add(keyspace);\n                }\n            }\n        };\n        futures.add(StageManager.getStage(Stage.MUTATION).submit(runnable));\n        if (futures.size() > MAX_OUTSTANDING_REPLAY_COUNT)\n        {\n            FBUtilities.waitOnFutures(futures);\n            futures.clear();\n        }\n    }"
        ],
        [
            "Mutation::apply(boolean)",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221  \n 222  ",
            "    public void apply(boolean durableWrites)\n    {\n        try\n        {\n            Uninterruptibles.getUninterruptibly(applyFuture(durableWrites));\n        }\n        catch (ExecutionException e)\n        {\n            throw new RuntimeException(e.getCause());\n        }\n    }",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  ",
            "    public void apply(boolean durableWrites)\n    {\n        try\n        {\n            Uninterruptibles.getUninterruptibly(applyFuture(durableWrites));\n        }\n        catch (ExecutionException e)\n        {\n            Throwables.propagate(e.getCause());\n        }\n    }"
        ]
    ],
    "fed476f9c049128674841d1c46b868979352b1a5": [
        [
            "Verifier::Verifier(ColumnFamilyStore,SSTableReader,boolean)",
            "  65 -\n  66  \n  67  \n  68  ",
            "    public Verifier(ColumnFamilyStore cfs, SSTableReader sstable, boolean isOffline) throws IOException\n    {\n        this(cfs, sstable, new OutputHandler.LogOutput(), isOffline);\n    }",
            "  65 +\n  66  \n  67  \n  68  ",
            "    public Verifier(ColumnFamilyStore cfs, SSTableReader sstable, boolean isOffline)\n    {\n        this(cfs, sstable, new OutputHandler.LogOutput(), isOffline);\n    }"
        ],
        [
            "CompactionManager::relocateSSTables(ColumnFamilyStore,int)",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 -\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  ",
            "    public AllSSTableOpStatus relocateSSTables(final ColumnFamilyStore cfs, int jobs) throws ExecutionException, InterruptedException\n    {\n        if (!cfs.getPartitioner().splitter().isPresent())\n        {\n            logger.info(\"Partitioner does not support splitting\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n        final Collection<Range<Token>> r = StorageService.instance.getLocalRanges(cfs.keyspace.getName());\n\n        if (r.isEmpty())\n        {\n            logger.info(\"Relocate cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n\n        final List<Range<Token>> localRanges = Range.sort(r);\n        final Directories.DataDirectory[] locations = cfs.getDirectories().getWriteableLocations();\n        final List<PartitionPosition> diskBoundaries = StorageService.getDiskBoundaries(localRanges, cfs.getPartitioner(), locations);\n\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Set<SSTableReader> originals = Sets.newHashSet(transaction.originals());\n                Set<SSTableReader> needsRelocation = originals.stream().filter(s -> !inCorrectLocation(s)).collect(Collectors.toSet());\n                transaction.cancel(Sets.difference(originals, needsRelocation));\n\n                Map<Integer, List<SSTableReader>> groupedByDisk = needsRelocation.stream().collect(Collectors.groupingBy((s) ->\n                        CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), s)));\n\n                int maxSize = 0;\n                for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                    maxSize = Math.max(maxSize, diskSSTables.size());\n\n                List<SSTableReader> mixedSSTables = new ArrayList<>();\n\n                for (int i = 0; i < maxSize; i++)\n                    for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                        if (i < diskSSTables.size())\n                            mixedSSTables.add(diskSSTables.get(i));\n\n                return mixedSSTables;\n            }\n\n            private boolean inCorrectLocation(SSTableReader sstable)\n            {\n                if (!cfs.getPartitioner().splitter().isPresent())\n                    return true;\n                int directoryIndex = CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), sstable);\n                Directories.DataDirectory[] locations = cfs.getDirectories().getWriteableLocations();\n\n                Directories.DataDirectory location = locations[directoryIndex];\n                PartitionPosition diskLast = diskBoundaries.get(directoryIndex);\n                // the location we get from directoryIndex is based on the first key in the sstable\n                // now we need to make sure the last key is less than the boundary as well:\n                return sstable.descriptor.directory.getAbsolutePath().startsWith(location.location.getAbsolutePath()) && sstable.last.compareTo(diskLast) <= 0;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                logger.debug(\"Relocating {}\", txn.originals());\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.RELOCATE);\n                task.execute(metrics);\n            }\n        }, jobs, OperationType.RELOCATE);\n    }",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 +\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  ",
            "    public AllSSTableOpStatus relocateSSTables(final ColumnFamilyStore cfs, int jobs) throws ExecutionException, InterruptedException\n    {\n        if (!cfs.getPartitioner().splitter().isPresent())\n        {\n            logger.info(\"Partitioner does not support splitting\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n        final Collection<Range<Token>> r = StorageService.instance.getLocalRanges(cfs.keyspace.getName());\n\n        if (r.isEmpty())\n        {\n            logger.info(\"Relocate cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n\n        final List<Range<Token>> localRanges = Range.sort(r);\n        final Directories.DataDirectory[] locations = cfs.getDirectories().getWriteableLocations();\n        final List<PartitionPosition> diskBoundaries = StorageService.getDiskBoundaries(localRanges, cfs.getPartitioner(), locations);\n\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Set<SSTableReader> originals = Sets.newHashSet(transaction.originals());\n                Set<SSTableReader> needsRelocation = originals.stream().filter(s -> !inCorrectLocation(s)).collect(Collectors.toSet());\n                transaction.cancel(Sets.difference(originals, needsRelocation));\n\n                Map<Integer, List<SSTableReader>> groupedByDisk = needsRelocation.stream().collect(Collectors.groupingBy((s) ->\n                        CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), s)));\n\n                int maxSize = 0;\n                for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                    maxSize = Math.max(maxSize, diskSSTables.size());\n\n                List<SSTableReader> mixedSSTables = new ArrayList<>();\n\n                for (int i = 0; i < maxSize; i++)\n                    for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                        if (i < diskSSTables.size())\n                            mixedSSTables.add(diskSSTables.get(i));\n\n                return mixedSSTables;\n            }\n\n            private boolean inCorrectLocation(SSTableReader sstable)\n            {\n                if (!cfs.getPartitioner().splitter().isPresent())\n                    return true;\n                int directoryIndex = CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), sstable);\n                Directories.DataDirectory[] locations = cfs.getDirectories().getWriteableLocations();\n\n                Directories.DataDirectory location = locations[directoryIndex];\n                PartitionPosition diskLast = diskBoundaries.get(directoryIndex);\n                // the location we get from directoryIndex is based on the first key in the sstable\n                // now we need to make sure the last key is less than the boundary as well:\n                return sstable.descriptor.directory.getAbsolutePath().startsWith(location.location.getAbsolutePath()) && sstable.last.compareTo(diskLast) <= 0;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn)\n            {\n                logger.debug(\"Relocating {}\", txn.originals());\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.RELOCATE);\n                task.execute(metrics);\n            }\n        }, jobs, OperationType.RELOCATE);\n    }"
        ],
        [
            "SafeMemory::MemoryTidy::tidy()",
            "  87 -\n  88  \n  89  \n  90  \n  91  \n  92  ",
            "        public void tidy() throws Exception\n        {\n            /** see {@link Memory#Memory(long)} re: null pointers*/\n            if (peer != 0)\n                MemoryUtil.free(peer);\n        }",
            "  87 +\n  88  \n  89  \n  90  \n  91  \n  92  ",
            "        public void tidy()\n        {\n            /** see {@link Memory#Memory(long)} re: null pointers*/\n            if (peer != 0)\n                MemoryUtil.free(peer);\n        }"
        ],
        [
            "CompactionManager::submitMaximal(ColumnFamilyStore,int,boolean)",
            " 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676 -\n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "    public List<Future<?>> submitMaximal(final ColumnFamilyStore cfStore, final int gcBefore, boolean splitOutput)\n    {\n        // here we compute the task off the compaction executor, so having that present doesn't\n        // confuse runWithCompactionsDisabled -- i.e., we don't want to deadlock ourselves, waiting\n        // for ourselves to finish/acknowledge cancellation before continuing.\n        final Collection<AbstractCompactionTask> tasks = cfStore.getCompactionStrategyManager().getMaximalTasks(gcBefore, splitOutput);\n\n        if (tasks == null)\n            return Collections.emptyList();\n\n        List<Future<?>> futures = new ArrayList<>();\n        int nonEmptyTasks = 0;\n        for (final AbstractCompactionTask task : tasks)\n        {\n            if (task.transaction.originals().size() > 0)\n                nonEmptyTasks++;\n            Runnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow() throws IOException\n                {\n                    task.execute(metrics);\n                }\n            };\n            if (executor.isShutdown())\n            {\n                logger.info(\"Compaction executor has shut down, not submitting task\");\n                return Collections.emptyList();\n            }\n            futures.add(executor.submit(runnable));\n        }\n        if (nonEmptyTasks > 1)\n            logger.info(\"Major compaction will not result in a single sstable - repaired and unrepaired data is kept separate and compaction runs per data_file_directory.\");\n        return futures;\n    }",
            " 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676 +\n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "    public List<Future<?>> submitMaximal(final ColumnFamilyStore cfStore, final int gcBefore, boolean splitOutput)\n    {\n        // here we compute the task off the compaction executor, so having that present doesn't\n        // confuse runWithCompactionsDisabled -- i.e., we don't want to deadlock ourselves, waiting\n        // for ourselves to finish/acknowledge cancellation before continuing.\n        final Collection<AbstractCompactionTask> tasks = cfStore.getCompactionStrategyManager().getMaximalTasks(gcBefore, splitOutput);\n\n        if (tasks == null)\n            return Collections.emptyList();\n\n        List<Future<?>> futures = new ArrayList<>();\n        int nonEmptyTasks = 0;\n        for (final AbstractCompactionTask task : tasks)\n        {\n            if (task.transaction.originals().size() > 0)\n                nonEmptyTasks++;\n            Runnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow()\n                {\n                    task.execute(metrics);\n                }\n            };\n            if (executor.isShutdown())\n            {\n                logger.info(\"Compaction executor has shut down, not submitting task\");\n                return Collections.emptyList();\n            }\n            futures.add(executor.submit(runnable));\n        }\n        if (nonEmptyTasks > 1)\n            logger.info(\"Major compaction will not result in a single sstable - repaired and unrepaired data is kept separate and compaction runs per data_file_directory.\");\n        return futures;\n    }"
        ],
        [
            "CassandraServer::truncate(String)",
            "2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113 -\n2114 -\n2115 -\n2116 -\n2117  \n2118  \n2119  \n2120  \n2121  ",
            "    public void truncate(String cfname) throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        ClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, cfname, Permission.MODIFY);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, cfname, false);\n            if (metadata.isView())\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Cannot truncate Materialized Views\");\n\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"truncate\", ImmutableMap.of(\"cf\", cfname, \"ks\", keyspace));\n            }\n            else\n            {\n                logger.trace(\"truncating {}.{}\", cState.getKeyspace(), cfname);\n            }\n\n            schedule(DatabaseDescriptor.getTruncateRpcTimeout());\n            try\n            {\n                StorageProxy.truncateBlocking(cState.getKeyspace(), cfname);\n            }\n            finally\n            {\n                release();\n            }\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (org.apache.cassandra.exceptions.UnavailableException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (TimeoutException e)\n        {\n            throw new TimedOutException();\n        }\n        catch (IOException e)\n        {\n            throw (UnavailableException) new UnavailableException().initCause(e);\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }",
            "2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  ",
            "    public void truncate(String cfname) throws InvalidRequestException, UnavailableException, TimedOutException, TException\n    {\n        ClientState cState = state();\n\n        try\n        {\n            String keyspace = cState.getKeyspace();\n            cState.hasColumnFamilyAccess(keyspace, cfname, Permission.MODIFY);\n            CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, cfname, false);\n            if (metadata.isView())\n                throw new org.apache.cassandra.exceptions.InvalidRequestException(\"Cannot truncate Materialized Views\");\n\n            if (startSessionIfRequested())\n            {\n                Tracing.instance.begin(\"truncate\", ImmutableMap.of(\"cf\", cfname, \"ks\", keyspace));\n            }\n            else\n            {\n                logger.trace(\"truncating {}.{}\", cState.getKeyspace(), cfname);\n            }\n\n            schedule(DatabaseDescriptor.getTruncateRpcTimeout());\n            try\n            {\n                StorageProxy.truncateBlocking(cState.getKeyspace(), cfname);\n            }\n            finally\n            {\n                release();\n            }\n        }\n        catch (RequestValidationException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (org.apache.cassandra.exceptions.UnavailableException e)\n        {\n            throw ThriftConversion.toThrift(e);\n        }\n        catch (TimeoutException e)\n        {\n            throw new TimedOutException();\n        }\n        finally\n        {\n            Tracing.instance.stopSession();\n        }\n    }"
        ],
        [
            "StopWordFactory::load(String)",
            "  55 -\n  56  \n  57  \n  58  ",
            "                public Set<String> load(String s) throws Exception\n                {\n                    return getStopWordsFromResource(s);\n                }",
            "  55 +\n  56  \n  57  \n  58  ",
            "                public Set<String> load(String s)\n                {\n                    return getStopWordsFromResource(s);\n                }"
        ],
        [
            "TruncateStatement::execute(QueryState,QueryOptions)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    public ResultMessage execute(QueryState state, QueryOptions options) throws InvalidRequestException, TruncateException\n    {\n        try\n        {\n            CFMetaData metaData = Schema.instance.getCFMetaData(keyspace(), columnFamily());\n            if (metaData.isView())\n                throw new InvalidRequestException(\"Cannot TRUNCATE materialized view directly; must truncate base table instead\");\n\n            StorageProxy.truncateBlocking(keyspace(), columnFamily());\n        }\n        catch (UnavailableException | TimeoutException | IOException e)\n        {\n            throw new TruncateException(e);\n        }\n        return null;\n    }",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    public ResultMessage execute(QueryState state, QueryOptions options) throws InvalidRequestException, TruncateException\n    {\n        try\n        {\n            CFMetaData metaData = Schema.instance.getCFMetaData(keyspace(), columnFamily());\n            if (metaData.isView())\n                throw new InvalidRequestException(\"Cannot TRUNCATE materialized view directly; must truncate base table instead\");\n\n            StorageProxy.truncateBlocking(keyspace(), columnFamily());\n        }\n        catch (UnavailableException | TimeoutException e)\n        {\n            throw new TruncateException(e);\n        }\n        return null;\n    }"
        ],
        [
            "AsyncRepairCallback::response(MessageIn)",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void response(MessageIn<ReadResponse> message)\n    {\n        repairResolver.preprocess(message);\n        if (received.incrementAndGet() == blockfor)\n        {\n            StageManager.getStage(Stage.READ_REPAIR).execute(new WrappedRunnable()\n            {\n                protected void runMayThrow() throws DigestMismatchException, IOException\n                {\n                    repairResolver.resolve();\n                }\n            });\n        }\n    }",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  ",
            "    public void response(MessageIn<ReadResponse> message)\n    {\n        repairResolver.preprocess(message);\n        if (received.incrementAndGet() == blockfor)\n        {\n            StageManager.getStage(Stage.READ_REPAIR).execute(new WrappedRunnable()\n            {\n                protected void runMayThrow()\n                {\n                    repairResolver.resolve();\n                }\n            });\n        }\n    }"
        ],
        [
            "Scrubber::Scrubber(ColumnFamilyStore,LifecycleTransaction,boolean,OutputHandler,boolean)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "    @SuppressWarnings(\"resource\")\n    public Scrubber(ColumnFamilyStore cfs,\n                    LifecycleTransaction transaction,\n                    boolean skipCorrupted,\n                    OutputHandler outputHandler,\n                    boolean checkData) throws IOException\n    {\n        this.cfs = cfs;\n        this.transaction = transaction;\n        this.sstable = transaction.onlyOne();\n        this.outputHandler = outputHandler;\n        this.skipCorrupted = skipCorrupted;\n        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata,\n                                                                                                        sstable.descriptor.version,\n                                                                                                        sstable.header);\n\n        List<SSTableReader> toScrub = Collections.singletonList(sstable);\n\n        int locIndex = CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), sstable);\n        this.destination = cfs.getDirectories().getLocationForDisk(cfs.getDirectories().getWriteableLocations()[locIndex]);\n        this.isCommutative = cfs.metadata.isCounter();\n\n        boolean hasIndexFile = (new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX))).exists();\n        this.isIndex = cfs.isIndex();\n        if (!hasIndexFile)\n        {\n            // if there's any corruption in the -Data.db then rows can't be skipped over. but it's worth a shot.\n            outputHandler.warn(\"Missing component: \" + sstable.descriptor.filenameFor(Component.PRIMARY_INDEX));\n        }\n        this.checkData = checkData && !this.isIndex; //LocalByPartitionerType does not support validation\n        this.expectedBloomFilterSize = Math.max(\n            cfs.metadata.params.minIndexInterval,\n            hasIndexFile ? SSTableReader.getApproximateKeyCount(toScrub) : 0);\n\n        // loop through each row, deserializing to check for damage.\n        // we'll also loop through the index at the same time, using the position from the index to recover if the\n        // row header (key or data size) is corrupt. (This means our position in the index file will be one row\n        // \"ahead\" of the data file.)\n        this.dataFile = transaction.isOffline()\n                        ? sstable.openDataReader()\n                        : sstable.openDataReader(CompactionManager.instance.getRateLimiter());\n\n        this.indexFile = hasIndexFile\n                ? RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)))\n                : null;\n\n        this.scrubInfo = new ScrubInfo(dataFile, sstable);\n\n        this.currentRowPositionFromIndex = 0;\n        this.nextRowPositionFromIndex = 0;\n    }",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "    @SuppressWarnings(\"resource\")\n    public Scrubber(ColumnFamilyStore cfs,\n                    LifecycleTransaction transaction,\n                    boolean skipCorrupted,\n                    OutputHandler outputHandler,\n                    boolean checkData)\n    {\n        this.cfs = cfs;\n        this.transaction = transaction;\n        this.sstable = transaction.onlyOne();\n        this.outputHandler = outputHandler;\n        this.skipCorrupted = skipCorrupted;\n        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata,\n                                                                                                        sstable.descriptor.version,\n                                                                                                        sstable.header);\n\n        List<SSTableReader> toScrub = Collections.singletonList(sstable);\n\n        int locIndex = CompactionStrategyManager.getCompactionStrategyIndex(cfs, cfs.getDirectories(), sstable);\n        this.destination = cfs.getDirectories().getLocationForDisk(cfs.getDirectories().getWriteableLocations()[locIndex]);\n        this.isCommutative = cfs.metadata.isCounter();\n\n        boolean hasIndexFile = (new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX))).exists();\n        this.isIndex = cfs.isIndex();\n        if (!hasIndexFile)\n        {\n            // if there's any corruption in the -Data.db then rows can't be skipped over. but it's worth a shot.\n            outputHandler.warn(\"Missing component: \" + sstable.descriptor.filenameFor(Component.PRIMARY_INDEX));\n        }\n        this.checkData = checkData && !this.isIndex; //LocalByPartitionerType does not support validation\n        this.expectedBloomFilterSize = Math.max(\n            cfs.metadata.params.minIndexInterval,\n            hasIndexFile ? SSTableReader.getApproximateKeyCount(toScrub) : 0);\n\n        // loop through each row, deserializing to check for damage.\n        // we'll also loop through the index at the same time, using the position from the index to recover if the\n        // row header (key or data size) is corrupt. (This means our position in the index file will be one row\n        // \"ahead\" of the data file.)\n        this.dataFile = transaction.isOffline()\n                        ? sstable.openDataReader()\n                        : sstable.openDataReader(CompactionManager.instance.getRateLimiter());\n\n        this.indexFile = hasIndexFile\n                ? RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)))\n                : null;\n\n        this.scrubInfo = new ScrubInfo(dataFile, sstable);\n\n        this.currentRowPositionFromIndex = 0;\n        this.nextRowPositionFromIndex = 0;\n    }"
        ],
        [
            "StartupChecks::execute()",
            " 117 -\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "        public void execute() throws StartupException\n        {\n            if (FBUtilities.isWindows())\n                return;\n            String jemalloc = System.getProperty(\"cassandra.libjemalloc\");\n            if (jemalloc == null)\n                logger.warn(\"jemalloc shared library could not be preloaded to speed up memory allocations\");\n            else if (\"-\".equals(jemalloc))\n                logger.info(\"jemalloc preload explicitly disabled\");\n            else\n                logger.info(\"jemalloc seems to be preloaded from {}\", jemalloc);\n        }",
            " 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "        public void execute()\n        {\n            if (FBUtilities.isWindows())\n                return;\n            String jemalloc = System.getProperty(\"cassandra.libjemalloc\");\n            if (jemalloc == null)\n                logger.warn(\"jemalloc shared library could not be preloaded to speed up memory allocations\");\n            else if (\"-\".equals(jemalloc))\n                logger.info(\"jemalloc preload explicitly disabled\");\n            else\n                logger.info(\"jemalloc seems to be preloaded from {}\", jemalloc);\n        }"
        ],
        [
            "ColumnFamilyStore::markAllCompacting(OperationType)",
            "2231  \n2232  \n2233  \n2234  \n2235 -\n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  ",
            "    public LifecycleTransaction markAllCompacting(final OperationType operationType)\n    {\n        Callable<LifecycleTransaction> callable = new Callable<LifecycleTransaction>()\n        {\n            public LifecycleTransaction call() throws Exception\n            {\n                assert data.getCompacting().isEmpty() : data.getCompacting();\n                Iterable<SSTableReader> sstables = getPermittedToCompactSSTables();\n                sstables = AbstractCompactionStrategy.filterSuspectSSTables(sstables);\n                sstables = ImmutableList.copyOf(sstables);\n                LifecycleTransaction modifier = data.tryModify(sstables, operationType);\n                assert modifier != null: \"something marked things compacting while compactions are disabled\";\n                return modifier;\n            }\n        };\n\n        return runWithCompactionsDisabled(callable, false, false);\n    }",
            "2231  \n2232  \n2233  \n2234  \n2235 +\n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  ",
            "    public LifecycleTransaction markAllCompacting(final OperationType operationType)\n    {\n        Callable<LifecycleTransaction> callable = new Callable<LifecycleTransaction>()\n        {\n            public LifecycleTransaction call()\n            {\n                assert data.getCompacting().isEmpty() : data.getCompacting();\n                Iterable<SSTableReader> sstables = getPermittedToCompactSSTables();\n                sstables = AbstractCompactionStrategy.filterSuspectSSTables(sstables);\n                sstables = ImmutableList.copyOf(sstables);\n                LifecycleTransaction modifier = data.tryModify(sstables, operationType);\n                assert modifier != null: \"something marked things compacting while compactions are disabled\";\n                return modifier;\n            }\n        };\n\n        return runWithCompactionsDisabled(callable, false, false);\n    }"
        ],
        [
            "Scrubber::Scrubber(ColumnFamilyStore,LifecycleTransaction,boolean,boolean)",
            "  81 -\n  82  \n  83  \n  84  ",
            "    public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, boolean checkData) throws IOException\n    {\n        this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData);\n    }",
            "  81 +\n  82  \n  83  \n  84  ",
            "    public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, boolean checkData)\n    {\n        this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData);\n    }"
        ],
        [
            "AutoSavingCache::loadSavedAsync()",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    public ListenableFuture<Integer> loadSavedAsync()\n    {\n        final ListeningExecutorService es = MoreExecutors.listeningDecorator(Executors.newSingleThreadExecutor());\n        final long start = System.nanoTime();\n\n        ListenableFuture<Integer> cacheLoad = es.submit(new Callable<Integer>()\n        {\n            @Override\n            public Integer call() throws Exception\n            {\n                return loadSaved();\n            }\n        });\n        cacheLoad.addListener(new Runnable() {\n            @Override\n            public void run()\n            {\n                if (size() > 0)\n                    logger.info(\"Completed loading ({} ms; {} keys) {} cache\",\n                            TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start),\n                            CacheService.instance.keyCache.size(),\n                            cacheType);\n                es.shutdown();\n            }\n        }, MoreExecutors.directExecutor());\n\n        return cacheLoad;\n    }",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 +\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "    public ListenableFuture<Integer> loadSavedAsync()\n    {\n        final ListeningExecutorService es = MoreExecutors.listeningDecorator(Executors.newSingleThreadExecutor());\n        final long start = System.nanoTime();\n\n        ListenableFuture<Integer> cacheLoad = es.submit(new Callable<Integer>()\n        {\n            @Override\n            public Integer call()\n            {\n                return loadSaved();\n            }\n        });\n        cacheLoad.addListener(new Runnable() {\n            @Override\n            public void run()\n            {\n                if (size() > 0)\n                    logger.info(\"Completed loading ({} ms; {} keys) {} cache\",\n                            TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start),\n                            CacheService.instance.keyCache.size(),\n                            cacheType);\n                es.shutdown();\n            }\n        }, MoreExecutors.directExecutor());\n\n        return cacheLoad;\n    }"
        ],
        [
            "Verifier::Verifier(ColumnFamilyStore,SSTableReader,OutputHandler,boolean)",
            "  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "    public Verifier(ColumnFamilyStore cfs, SSTableReader sstable, OutputHandler outputHandler, boolean isOffline) throws IOException\n    {\n        this.cfs = cfs;\n        this.sstable = sstable;\n        this.outputHandler = outputHandler;\n        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata, sstable.descriptor.version, sstable.header);\n\n        this.controller = new VerifyController(cfs);\n\n        this.dataFile = isOffline\n                        ? sstable.openDataReader()\n                        : sstable.openDataReader(CompactionManager.instance.getRateLimiter());\n        this.indexFile = RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)));\n        this.verifyInfo = new VerifyInfo(dataFile, sstable);\n    }",
            "  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "    public Verifier(ColumnFamilyStore cfs, SSTableReader sstable, OutputHandler outputHandler, boolean isOffline)\n    {\n        this.cfs = cfs;\n        this.sstable = sstable;\n        this.outputHandler = outputHandler;\n        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata, sstable.descriptor.version, sstable.header);\n\n        this.controller = new VerifyController(cfs);\n\n        this.dataFile = isOffline\n                        ? sstable.openDataReader()\n                        : sstable.openDataReader(CompactionManager.instance.getRateLimiter());\n        this.indexFile = RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)));\n        this.verifyInfo = new VerifyInfo(dataFile, sstable);\n    }"
        ],
        [
            "DataOutputStreamPlus::newDefaultChannel()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "    protected WritableByteChannel newDefaultChannel()\n    {\n        return new WritableByteChannel()\n        {\n\n            @Override\n            public boolean isOpen()\n            {\n                return true;\n            }\n\n            @Override\n            public void close() throws IOException\n            {\n            }\n\n            @Override\n            public int write(ByteBuffer src) throws IOException\n            {\n                int toWrite = src.remaining();\n\n                if (src.hasArray())\n                {\n                    DataOutputStreamPlus.this.write(src.array(), src.arrayOffset() + src.position(), src.remaining());\n                    src.position(src.limit());\n                    return toWrite;\n                }\n\n                if (toWrite < 16)\n                {\n                    int offset = src.position();\n                    for (int i = 0 ; i < toWrite ; i++)\n                        DataOutputStreamPlus.this.write(src.get(i + offset));\n                    src.position(src.limit());\n                    return toWrite;\n                }\n\n                byte[] buf = retrieveTemporaryBuffer(toWrite);\n\n                int totalWritten = 0;\n                while (totalWritten < toWrite)\n                {\n                    int toWriteThisTime = Math.min(buf.length, toWrite - totalWritten);\n\n                    ByteBufferUtil.arrayCopy(src, src.position() + totalWritten, buf, 0, toWriteThisTime);\n\n                    DataOutputStreamPlus.this.write(buf, 0, toWriteThisTime);\n\n                    totalWritten += toWriteThisTime;\n                }\n\n                src.position(src.limit());\n                return totalWritten;\n            }\n\n        };\n    }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "    protected WritableByteChannel newDefaultChannel()\n    {\n        return new WritableByteChannel()\n        {\n\n            @Override\n            public boolean isOpen()\n            {\n                return true;\n            }\n\n            @Override\n            public void close()\n            {\n            }\n\n            @Override\n            public int write(ByteBuffer src) throws IOException\n            {\n                int toWrite = src.remaining();\n\n                if (src.hasArray())\n                {\n                    DataOutputStreamPlus.this.write(src.array(), src.arrayOffset() + src.position(), src.remaining());\n                    src.position(src.limit());\n                    return toWrite;\n                }\n\n                if (toWrite < 16)\n                {\n                    int offset = src.position();\n                    for (int i = 0 ; i < toWrite ; i++)\n                        DataOutputStreamPlus.this.write(src.get(i + offset));\n                    src.position(src.limit());\n                    return toWrite;\n                }\n\n                byte[] buf = retrieveTemporaryBuffer(toWrite);\n\n                int totalWritten = 0;\n                while (totalWritten < toWrite)\n                {\n                    int toWriteThisTime = Math.min(buf.length, toWrite - totalWritten);\n\n                    ByteBufferUtil.arrayCopy(src, src.position() + totalWritten, buf, 0, toWriteThisTime);\n\n                    DataOutputStreamPlus.this.write(buf, 0, toWriteThisTime);\n\n                    totalWritten += toWriteThisTime;\n                }\n\n                src.position(src.limit());\n                return totalWritten;\n            }\n\n        };\n    }"
        ],
        [
            "StorageProxy::truncateBlocking(String,String)",
            "2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367 -\n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  ",
            "    /**\n     * Performs the truncate operatoin, which effectively deletes all data from\n     * the column family cfname\n     * @param keyspace\n     * @param cfname\n     * @throws UnavailableException If some of the hosts in the ring are down.\n     * @throws TimeoutException\n     * @throws IOException\n     */\n    public static void truncateBlocking(String keyspace, String cfname) throws UnavailableException, TimeoutException, IOException\n    {\n        logger.debug(\"Starting a blocking truncate operation on keyspace {}, CF {}\", keyspace, cfname);\n        if (isAnyStorageHostDown())\n        {\n            logger.info(\"Cannot perform truncate, some hosts are down\");\n            // Since the truncate operation is so aggressive and is typically only\n            // invoked by an admin, for simplicity we require that all nodes are up\n            // to perform the operation.\n            int liveMembers = Gossiper.instance.getLiveMembers().size();\n            throw new UnavailableException(ConsistencyLevel.ALL, liveMembers + Gossiper.instance.getUnreachableMembers().size(), liveMembers);\n        }\n\n        Set<InetAddress> allEndpoints = StorageService.instance.getLiveRingMembers(true);\n\n        int blockFor = allEndpoints.size();\n        final TruncateResponseHandler responseHandler = new TruncateResponseHandler(blockFor);\n\n        // Send out the truncate calls and track the responses with the callbacks.\n        Tracing.trace(\"Enqueuing truncate messages to hosts {}\", allEndpoints);\n        final Truncation truncation = new Truncation(keyspace, cfname);\n        MessageOut<Truncation> message = truncation.createMessage();\n        for (InetAddress endpoint : allEndpoints)\n            MessagingService.instance().sendRR(message, endpoint, responseHandler);\n\n        // Wait for all\n        try\n        {\n            responseHandler.get();\n        }\n        catch (TimeoutException e)\n        {\n            Tracing.trace(\"Timed out\");\n            throw e;\n        }\n    }",
            "2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366 +\n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  ",
            "    /**\n     * Performs the truncate operatoin, which effectively deletes all data from\n     * the column family cfname\n     * @param keyspace\n     * @param cfname\n     * @throws UnavailableException If some of the hosts in the ring are down.\n     * @throws TimeoutException\n     */\n    public static void truncateBlocking(String keyspace, String cfname) throws UnavailableException, TimeoutException\n    {\n        logger.debug(\"Starting a blocking truncate operation on keyspace {}, CF {}\", keyspace, cfname);\n        if (isAnyStorageHostDown())\n        {\n            logger.info(\"Cannot perform truncate, some hosts are down\");\n            // Since the truncate operation is so aggressive and is typically only\n            // invoked by an admin, for simplicity we require that all nodes are up\n            // to perform the operation.\n            int liveMembers = Gossiper.instance.getLiveMembers().size();\n            throw new UnavailableException(ConsistencyLevel.ALL, liveMembers + Gossiper.instance.getUnreachableMembers().size(), liveMembers);\n        }\n\n        Set<InetAddress> allEndpoints = StorageService.instance.getLiveRingMembers(true);\n\n        int blockFor = allEndpoints.size();\n        final TruncateResponseHandler responseHandler = new TruncateResponseHandler(blockFor);\n\n        // Send out the truncate calls and track the responses with the callbacks.\n        Tracing.trace(\"Enqueuing truncate messages to hosts {}\", allEndpoints);\n        final Truncation truncation = new Truncation(keyspace, cfname);\n        MessageOut<Truncation> message = truncation.createMessage();\n        for (InetAddress endpoint : allEndpoints)\n            MessagingService.instance().sendRR(message, endpoint, responseHandler);\n\n        // Wait for all\n        try\n        {\n            responseHandler.get();\n        }\n        catch (TimeoutException e)\n        {\n            Tracing.trace(\"Timed out\");\n            throw e;\n        }\n    }"
        ],
        [
            "StorageService::deliverHints(String)",
            "2673 -\n2674  \n2675  \n2676  ",
            "    public final void deliverHints(String host) throws UnknownHostException\n    {\n        throw new UnsupportedOperationException();\n    }",
            "2673 +\n2674  \n2675  \n2676  ",
            "    public final void deliverHints(String host)\n    {\n        throw new UnsupportedOperationException();\n    }"
        ],
        [
            "CompactionManager::performSSTableRewrite(ColumnFamilyStore,boolean,int)",
            " 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412 -\n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    public AllSSTableOpStatus performSSTableRewrite(final ColumnFamilyStore cfs, final boolean excludeCurrentVersion, int jobs) throws InterruptedException, ExecutionException\n    {\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Iterable<SSTableReader> sstables = new ArrayList<>(transaction.originals());\n                Iterator<SSTableReader> iter = sstables.iterator();\n                while (iter.hasNext())\n                {\n                    SSTableReader sstable = iter.next();\n                    if (excludeCurrentVersion && sstable.descriptor.version.equals(sstable.descriptor.getFormat().getLatestVersion()))\n                    {\n                        transaction.cancel(sstable);\n                        iter.remove();\n                    }\n                }\n                return sstables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.UPGRADE_SSTABLES);\n                task.execute(metrics);\n            }\n        }, jobs, OperationType.UPGRADE_SSTABLES);\n    }",
            " 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412 +\n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "    public AllSSTableOpStatus performSSTableRewrite(final ColumnFamilyStore cfs, final boolean excludeCurrentVersion, int jobs) throws InterruptedException, ExecutionException\n    {\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Iterable<SSTableReader> sstables = new ArrayList<>(transaction.originals());\n                Iterator<SSTableReader> iter = sstables.iterator();\n                while (iter.hasNext())\n                {\n                    SSTableReader sstable = iter.next();\n                    if (excludeCurrentVersion && sstable.descriptor.version.equals(sstable.descriptor.getFormat().getLatestVersion()))\n                    {\n                        transaction.cancel(sstable);\n                        iter.remove();\n                    }\n                }\n                return sstables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn)\n            {\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.UPGRADE_SSTABLES);\n                task.execute(metrics);\n            }\n        }, jobs, OperationType.UPGRADE_SSTABLES);\n    }"
        ],
        [
            "CqlInputFormat::getSubSplits(String,String,TokenRange,Configuration,Session)",
            " 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "    private Map<TokenRange, Long> getSubSplits(String keyspace, String cfName, TokenRange range, Configuration conf, Session session) throws IOException\n    {\n        int splitSize = ConfigHelper.getInputSplitSize(conf);\n        int splitSizeMb = ConfigHelper.getInputSplitSizeInMb(conf);\n        try\n        {\n            return describeSplits(keyspace, cfName, range, splitSize, splitSizeMb, session);\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n    }",
            " 216 +\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "    private Map<TokenRange, Long> getSubSplits(String keyspace, String cfName, TokenRange range, Configuration conf, Session session)\n    {\n        int splitSize = ConfigHelper.getInputSplitSize(conf);\n        int splitSizeMb = ConfigHelper.getInputSplitSizeInMb(conf);\n        try\n        {\n            return describeSplits(keyspace, cfName, range, splitSize, splitSizeMb, session);\n        }\n        catch (Exception e)\n        {\n            throw new RuntimeException(e);\n        }\n    }"
        ],
        [
            "RepairJob::run()",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 -\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints) throws Exception\n                {\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees) throws Exception\n            {\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "    /**\n     * Runs repair job.\n     *\n     * This sets up necessary task and runs them on given {@code taskExecutor}.\n     * After submitting all tasks, waits until validation with replica completes.\n     */\n    public void run()\n    {\n        List<InetAddress> allEndpoints = new ArrayList<>(session.endpoints);\n        allEndpoints.add(FBUtilities.getBroadcastAddress());\n\n        ListenableFuture<List<TreeResponse>> validations;\n        // Create a snapshot at all nodes unless we're using pure parallel repairs\n        if (parallelismDegree != RepairParallelism.PARALLEL)\n        {\n            // Request snapshot to all replica\n            List<ListenableFuture<InetAddress>> snapshotTasks = new ArrayList<>(allEndpoints.size());\n            for (InetAddress endpoint : allEndpoints)\n            {\n                SnapshotTask snapshotTask = new SnapshotTask(desc, endpoint);\n                snapshotTasks.add(snapshotTask);\n                taskExecutor.execute(snapshotTask);\n            }\n            // When all snapshot complete, send validation requests\n            ListenableFuture<List<InetAddress>> allSnapshotTasks = Futures.allAsList(snapshotTasks);\n            validations = Futures.transform(allSnapshotTasks, new AsyncFunction<List<InetAddress>, List<TreeResponse>>()\n            {\n                public ListenableFuture<List<TreeResponse>> apply(List<InetAddress> endpoints)\n                {\n                    if (parallelismDegree == RepairParallelism.SEQUENTIAL)\n                        return sendSequentialValidationRequest(endpoints);\n                    else\n                        return sendDCAwareValidationRequest(endpoints);\n                }\n            }, taskExecutor);\n        }\n        else\n        {\n            // If not sequential, just send validation request to all replica\n            validations = sendValidationRequest(allEndpoints);\n        }\n\n        // When all validations complete, submit sync tasks\n        ListenableFuture<List<SyncStat>> syncResults = Futures.transform(validations, new AsyncFunction<List<TreeResponse>, List<SyncStat>>()\n        {\n            public ListenableFuture<List<SyncStat>> apply(List<TreeResponse> trees)\n            {\n                InetAddress local = FBUtilities.getLocalAddress();\n\n                List<SyncTask> syncTasks = new ArrayList<>();\n                // We need to difference all trees one against another\n                for (int i = 0; i < trees.size() - 1; ++i)\n                {\n                    TreeResponse r1 = trees.get(i);\n                    for (int j = i + 1; j < trees.size(); ++j)\n                    {\n                        TreeResponse r2 = trees.get(j);\n                        SyncTask task;\n                        if (r1.endpoint.equals(local) || r2.endpoint.equals(local))\n                        {\n                            task = new LocalSyncTask(desc, r1, r2, repairedAt);\n                        }\n                        else\n                        {\n                            task = new RemoteSyncTask(desc, r1, r2);\n                            // RemoteSyncTask expects SyncComplete message sent back.\n                            // Register task to RepairSession to receive response.\n                            session.waitForSync(Pair.create(desc, new NodePair(r1.endpoint, r2.endpoint)), (RemoteSyncTask) task);\n                        }\n                        syncTasks.add(task);\n                        taskExecutor.submit(task);\n                    }\n                }\n                return Futures.allAsList(syncTasks);\n            }\n        }, taskExecutor);\n\n        // When all sync complete, set the final result\n        Futures.addCallback(syncResults, new FutureCallback<List<SyncStat>>()\n        {\n            public void onSuccess(List<SyncStat> stats)\n            {\n                logger.info(String.format(\"[repair #%s] %s is fully synced\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.successfulRepairJob(session.getId(), desc.keyspace, desc.columnFamily);\n                set(new RepairResult(desc, stats));\n            }\n\n            /**\n             * Snapshot, validation and sync failures are all handled here\n             */\n            public void onFailure(Throwable t)\n            {\n                logger.warn(String.format(\"[repair #%s] %s sync failed\", session.getId(), desc.columnFamily));\n                SystemDistributedKeyspace.failedRepairJob(session.getId(), desc.keyspace, desc.columnFamily, t);\n                setException(t);\n            }\n        }, taskExecutor);\n\n        // Wait for validation to complete\n        Futures.getUnchecked(validations);\n    }"
        ],
        [
            "ColumnFamilyStore::scheduleFlush()",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "    void scheduleFlush()\n    {\n        int period = metadata.params.memtableFlushPeriodInMs;\n        if (period > 0)\n        {\n            logger.trace(\"scheduling flush in {} ms\", period);\n            WrappedRunnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow() throws Exception\n                {\n                    synchronized (data)\n                    {\n                        Memtable current = data.getView().getCurrentMemtable();\n                        // if we're not expired, we've been hit by a scheduled flush for an already flushed memtable, so ignore\n                        if (current.isExpired())\n                        {\n                            if (current.isClean())\n                            {\n                                // if we're still clean, instead of swapping just reschedule a flush for later\n                                scheduleFlush();\n                            }\n                            else\n                            {\n                                // we'll be rescheduled by the constructor of the Memtable.\n                                forceFlush();\n                            }\n                        }\n                    }\n                }\n            };\n            ScheduledExecutors.scheduledTasks.schedule(runnable, period, TimeUnit.MILLISECONDS);\n        }\n    }",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "    void scheduleFlush()\n    {\n        int period = metadata.params.memtableFlushPeriodInMs;\n        if (period > 0)\n        {\n            logger.trace(\"scheduling flush in {} ms\", period);\n            WrappedRunnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow()\n                {\n                    synchronized (data)\n                    {\n                        Memtable current = data.getView().getCurrentMemtable();\n                        // if we're not expired, we've been hit by a scheduled flush for an already flushed memtable, so ignore\n                        if (current.isExpired())\n                        {\n                            if (current.isClean())\n                            {\n                                // if we're still clean, instead of swapping just reschedule a flush for later\n                                scheduleFlush();\n                            }\n                            else\n                            {\n                                // we'll be rescheduled by the constructor of the Memtable.\n                                forceFlush();\n                            }\n                        }\n                    }\n                }\n            };\n            ScheduledExecutors.scheduledTasks.schedule(runnable, period, TimeUnit.MILLISECONDS);\n        }\n    }"
        ],
        [
            "CompactionManager::submitUserDefined(ColumnFamilyStore,Collection,int)",
            " 774  \n 775  \n 776  \n 777  \n 778 -\n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  ",
            "    public Future<?> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)\n    {\n        Runnable runnable = new WrappedRunnable()\n        {\n            protected void runMayThrow() throws IOException\n            {\n                // look up the sstables now that we're on the compaction executor, so we don't try to re-compact\n                // something that was already being compacted earlier.\n                Collection<SSTableReader> sstables = new ArrayList<>(dataFiles.size());\n                for (Descriptor desc : dataFiles)\n                {\n                    // inefficient but not in a performance sensitive path\n                    SSTableReader sstable = lookupSSTable(cfs, desc);\n                    if (sstable == null)\n                    {\n                        logger.info(\"Will not compact {}: it is not an active sstable\", desc);\n                    }\n                    else\n                    {\n                        sstables.add(sstable);\n                    }\n                }\n\n                if (sstables.isEmpty())\n                {\n                    logger.info(\"No files to compact for user defined compaction\");\n                }\n                else\n                {\n                    List<AbstractCompactionTask> tasks = cfs.getCompactionStrategyManager().getUserDefinedTasks(sstables, gcBefore);\n                    for (AbstractCompactionTask task : tasks)\n                    {\n                        if (task != null)\n                            task.execute(metrics);\n                    }\n                }\n            }\n        };\n        if (executor.isShutdown())\n        {\n            logger.info(\"Compaction executor has shut down, not submitting task\");\n            return Futures.immediateCancelledFuture();\n        }\n\n        return executor.submit(runnable);\n    }",
            " 774  \n 775  \n 776  \n 777  \n 778 +\n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  ",
            "    public Future<?> submitUserDefined(final ColumnFamilyStore cfs, final Collection<Descriptor> dataFiles, final int gcBefore)\n    {\n        Runnable runnable = new WrappedRunnable()\n        {\n            protected void runMayThrow()\n            {\n                // look up the sstables now that we're on the compaction executor, so we don't try to re-compact\n                // something that was already being compacted earlier.\n                Collection<SSTableReader> sstables = new ArrayList<>(dataFiles.size());\n                for (Descriptor desc : dataFiles)\n                {\n                    // inefficient but not in a performance sensitive path\n                    SSTableReader sstable = lookupSSTable(cfs, desc);\n                    if (sstable == null)\n                    {\n                        logger.info(\"Will not compact {}: it is not an active sstable\", desc);\n                    }\n                    else\n                    {\n                        sstables.add(sstable);\n                    }\n                }\n\n                if (sstables.isEmpty())\n                {\n                    logger.info(\"No files to compact for user defined compaction\");\n                }\n                else\n                {\n                    List<AbstractCompactionTask> tasks = cfs.getCompactionStrategyManager().getUserDefinedTasks(sstables, gcBefore);\n                    for (AbstractCompactionTask task : tasks)\n                    {\n                        if (task != null)\n                            task.execute(metrics);\n                    }\n                }\n            }\n        };\n        if (executor.isShutdown())\n        {\n            logger.info(\"Compaction executor has shut down, not submitting task\");\n            return Futures.immediateCancelledFuture();\n        }\n\n        return executor.submit(runnable);\n    }"
        ],
        [
            "AuthCache::initCache(LoadingCache)",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    private LoadingCache<K, V> initCache(LoadingCache<K, V> existing)\n    {\n        if (!enableCache.get())\n            return null;\n\n        if (getValidity() <= 0)\n            return null;\n\n        logger.info(\"(Re)initializing {} (validity period/update interval/max entries) ({}/{}/{})\",\n                    name, getValidity(), getUpdateInterval(), getMaxEntries());\n\n        LoadingCache<K, V> newcache = CacheBuilder.newBuilder()\n                           .refreshAfterWrite(getUpdateInterval(), TimeUnit.MILLISECONDS)\n                           .expireAfterWrite(getValidity(), TimeUnit.MILLISECONDS)\n                           .maximumSize(getMaxEntries())\n                           .build(new CacheLoader<K, V>()\n                           {\n                               public V load(K k) throws Exception\n                               {\n                                   return loadFunction.apply(k);\n                               }\n\n                               public ListenableFuture<V> reload(final K k, final V oldV)\n                               {\n                                   ListenableFutureTask<V> task = ListenableFutureTask.create(() -> {\n                                       try\n                                       {\n                                           return loadFunction.apply(k);\n                                       }\n                                       catch (Exception e)\n                                       {\n                                           logger.trace(\"Error performing async refresh of auth data in {}\", name, e);\n                                           throw e;\n                                       }\n                                   });\n                                   cacheRefreshExecutor.execute(task);\n                                   return task;\n                               }\n                           });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    private LoadingCache<K, V> initCache(LoadingCache<K, V> existing)\n    {\n        if (!enableCache.get())\n            return null;\n\n        if (getValidity() <= 0)\n            return null;\n\n        logger.info(\"(Re)initializing {} (validity period/update interval/max entries) ({}/{}/{})\",\n                    name, getValidity(), getUpdateInterval(), getMaxEntries());\n\n        LoadingCache<K, V> newcache = CacheBuilder.newBuilder()\n                           .refreshAfterWrite(getUpdateInterval(), TimeUnit.MILLISECONDS)\n                           .expireAfterWrite(getValidity(), TimeUnit.MILLISECONDS)\n                           .maximumSize(getMaxEntries())\n                           .build(new CacheLoader<K, V>()\n                           {\n                               public V load(K k)\n                               {\n                                   return loadFunction.apply(k);\n                               }\n\n                               public ListenableFuture<V> reload(final K k, final V oldV)\n                               {\n                                   ListenableFutureTask<V> task = ListenableFutureTask.create(() -> {\n                                       try\n                                       {\n                                           return loadFunction.apply(k);\n                                       }\n                                       catch (Exception e)\n                                       {\n                                           logger.trace(\"Error performing async refresh of auth data in {}\", name, e);\n                                           throw e;\n                                       }\n                                   });\n                                   cacheRefreshExecutor.execute(task);\n                                   return task;\n                               }\n                           });\n        if (existing != null)\n            newcache.putAll(existing.asMap());\n        return newcache;\n    }"
        ],
        [
            "CompactionStrategyManager::getMaximalTasks(int,boolean)",
            " 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704 -\n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  ",
            "    public Collection<AbstractCompactionTask> getMaximalTasks(final int gcBefore, final boolean splitOutput)\n    {\n        maybeReload(cfs.metadata);\n        // runWithCompactionsDisabled cancels active compactions and disables them, then we are able\n        // to make the repaired/unrepaired strategies mark their own sstables as compacting. Once the\n        // sstables are marked the compactions are re-enabled\n        return cfs.runWithCompactionsDisabled(new Callable<Collection<AbstractCompactionTask>>()\n        {\n            @Override\n            public Collection<AbstractCompactionTask> call() throws Exception\n            {\n                List<AbstractCompactionTask> tasks = new ArrayList<>();\n                readLock.lock();\n                try\n                {\n                    for (AbstractCompactionStrategy strategy : repaired)\n                    {\n                        Collection<AbstractCompactionTask> task = strategy.getMaximalTask(gcBefore, splitOutput);\n                        if (task != null)\n                            tasks.addAll(task);\n                    }\n                    for (AbstractCompactionStrategy strategy : unrepaired)\n                    {\n                        Collection<AbstractCompactionTask> task = strategy.getMaximalTask(gcBefore, splitOutput);\n                        if (task != null)\n                            tasks.addAll(task);\n                    }\n                }\n                finally\n                {\n                    readLock.unlock();\n                }\n                if (tasks.isEmpty())\n                    return null;\n                return tasks;\n            }\n        }, false, false);\n    }",
            " 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704 +\n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  ",
            "    public Collection<AbstractCompactionTask> getMaximalTasks(final int gcBefore, final boolean splitOutput)\n    {\n        maybeReload(cfs.metadata);\n        // runWithCompactionsDisabled cancels active compactions and disables them, then we are able\n        // to make the repaired/unrepaired strategies mark their own sstables as compacting. Once the\n        // sstables are marked the compactions are re-enabled\n        return cfs.runWithCompactionsDisabled(new Callable<Collection<AbstractCompactionTask>>()\n        {\n            @Override\n            public Collection<AbstractCompactionTask> call()\n            {\n                List<AbstractCompactionTask> tasks = new ArrayList<>();\n                readLock.lock();\n                try\n                {\n                    for (AbstractCompactionStrategy strategy : repaired)\n                    {\n                        Collection<AbstractCompactionTask> task = strategy.getMaximalTask(gcBefore, splitOutput);\n                        if (task != null)\n                            tasks.addAll(task);\n                    }\n                    for (AbstractCompactionStrategy strategy : unrepaired)\n                    {\n                        Collection<AbstractCompactionTask> task = strategy.getMaximalTask(gcBefore, splitOutput);\n                        if (task != null)\n                            tasks.addAll(task);\n                    }\n                }\n                finally\n                {\n                    readLock.unlock();\n                }\n                if (tasks.isEmpty())\n                    return null;\n                return tasks;\n            }\n        }, false, false);\n    }"
        ],
        [
            "TraceStateImpl::executeMutation(Mutation)",
            "  51  \n  52  \n  53  \n  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  ",
            "    static void executeMutation(final Mutation mutation)\n    {\n        StageManager.getStage(Stage.TRACING).execute(new WrappedRunnable()\n        {\n            protected void runMayThrow() throws Exception\n            {\n                mutateWithCatch(mutation);\n            }\n        });\n    }",
            "  51  \n  52  \n  53  \n  54  \n  55 +\n  56  \n  57  \n  58  \n  59  \n  60  ",
            "    static void executeMutation(final Mutation mutation)\n    {\n        StageManager.getStage(Stage.TRACING).execute(new WrappedRunnable()\n        {\n            protected void runMayThrow()\n            {\n                mutateWithCatch(mutation);\n            }\n        });\n    }"
        ],
        [
            "OutgoingFileMessage::deserialize(ReadableByteChannel,int,StreamSession)",
            "  41 -\n  42  \n  43  \n  44  ",
            "        public OutgoingFileMessage deserialize(ReadableByteChannel in, int version, StreamSession session) throws IOException\n        {\n            throw new UnsupportedOperationException(\"Not allowed to call deserialize on an outgoing file\");\n        }",
            "  41 +\n  42  \n  43  \n  44  ",
            "        public OutgoingFileMessage deserialize(ReadableByteChannel in, int version, StreamSession session)\n        {\n            throw new UnsupportedOperationException(\"Not allowed to call deserialize on an outgoing file\");\n        }"
        ],
        [
            "IncomingFileMessage::serialize(IncomingFileMessage,DataOutputStreamPlus,int,StreamSession)",
            "  75 -\n  76  \n  77  \n  78  ",
            "        public void serialize(IncomingFileMessage message, DataOutputStreamPlus out, int version, StreamSession session) throws IOException\n        {\n            throw new UnsupportedOperationException(\"Not allowed to call serialize on an incoming file\");\n        }",
            "  75 +\n  76  \n  77  \n  78  ",
            "        public void serialize(IncomingFileMessage message, DataOutputStreamPlus out, int version, StreamSession session)\n        {\n            throw new UnsupportedOperationException(\"Not allowed to call serialize on an incoming file\");\n        }"
        ],
        [
            "RepairRunnable::runMayThrow()",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 -\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 3 + options.getRanges().size(); // calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace,\n                                                                                                columnFamilies);\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results) throws Exception\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 +\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  ",
            "    protected void runMayThrow() throws Exception\n    {\n        final TraceState traceState;\n\n        final String tag = \"repair:\" + cmd;\n\n        final AtomicInteger progress = new AtomicInteger();\n        final int totalProgress = 3 + options.getRanges().size(); // calculate neighbors, validation, prepare for repair + number of ranges to repair\n\n        String[] columnFamilies = options.getColumnFamilies().toArray(new String[options.getColumnFamilies().size()]);\n        Iterable<ColumnFamilyStore> validColumnFamilies = storageService.getValidColumnFamilies(false, false, keyspace,\n                                                                                                columnFamilies);\n\n        final long startTime = System.currentTimeMillis();\n        String message = String.format(\"Starting repair command #%d, repairing keyspace %s with %s\", cmd, keyspace,\n                                       options);\n        logger.info(message);\n        fireProgressEvent(tag, new ProgressEvent(ProgressEventType.START, 0, 100, message));\n        if (options.isTraced())\n        {\n            StringBuilder cfsb = new StringBuilder();\n            for (ColumnFamilyStore cfs : validColumnFamilies)\n                cfsb.append(\", \").append(cfs.keyspace.getName()).append(\".\").append(cfs.name);\n\n            UUID sessionId = Tracing.instance.newSession(Tracing.TraceType.REPAIR);\n            traceState = Tracing.instance.begin(\"repair\", ImmutableMap.of(\"keyspace\", keyspace, \"columnFamilies\",\n                                                                          cfsb.substring(2)));\n            Tracing.traceRepair(message);\n            traceState.enableActivityNotification(tag);\n            for (ProgressListener listener : listeners)\n                traceState.addProgressListener(listener);\n            Thread queryThread = createQueryThread(cmd, sessionId);\n            queryThread.setName(\"RepairTracePolling\");\n            queryThread.start();\n        }\n        else\n        {\n            traceState = null;\n        }\n\n        final Set<InetAddress> allNeighbors = new HashSet<>();\n        List<Pair<Set<InetAddress>, ? extends Collection<Range<Token>>>> commonRanges = new ArrayList<>();\n        try\n        {\n            for (Range<Token> range : options.getRanges())\n            {\n                Set<InetAddress> neighbors = ActiveRepairService.getNeighbors(keyspace, range,\n                                                                              options.getDataCenters(),\n                                                                              options.getHosts());\n\n                addRangeToNeighbors(commonRanges, range, neighbors);\n                allNeighbors.addAll(neighbors);\n            }\n\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            logger.error(\"Repair failed:\", e);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        // Validate columnfamilies\n        List<ColumnFamilyStore> columnFamilyStores = new ArrayList<>();\n        try\n        {\n            Iterables.addAll(columnFamilyStores, validColumnFamilies);\n            progress.incrementAndGet();\n        }\n        catch (IllegalArgumentException e)\n        {\n            fireErrorAndComplete(tag, progress.get(), totalProgress, e.getMessage());\n            return;\n        }\n\n        String[] cfnames = new String[columnFamilyStores.size()];\n        for (int i = 0; i < columnFamilyStores.size(); i++)\n        {\n            cfnames[i] = columnFamilyStores.get(i).name;\n        }\n\n        final UUID parentSession = UUIDGen.getTimeUUID();\n        SystemDistributedKeyspace.startParentRepair(parentSession, keyspace, cfnames, options);\n        long repairedAt;\n        try\n        {\n            ActiveRepairService.instance.prepareForRepair(parentSession, FBUtilities.getBroadcastAddress(), allNeighbors, options, columnFamilyStores);\n            repairedAt = ActiveRepairService.instance.getParentRepairSession(parentSession).getRepairedAt();\n            progress.incrementAndGet();\n        }\n        catch (Throwable t)\n        {\n            SystemDistributedKeyspace.failParentRepair(parentSession, t);\n            fireErrorAndComplete(tag, progress.get(), totalProgress, t.getMessage());\n            return;\n        }\n\n        // Set up RepairJob executor for this repair command.\n        final ListeningExecutorService executor = MoreExecutors.listeningDecorator(new JMXConfigurableThreadPoolExecutor(options.getJobThreads(),\n                                                                                                                         Integer.MAX_VALUE,\n                                                                                                                         TimeUnit.SECONDS,\n                                                                                                                         new LinkedBlockingQueue<Runnable>(),\n                                                                                                                         new NamedThreadFactory(\"Repair#\" + cmd),\n                                                                                                                         \"internal\"));\n\n        List<ListenableFuture<RepairSessionResult>> futures = new ArrayList<>(options.getRanges().size());\n        for (Pair<Set<InetAddress>, ? extends Collection<Range<Token>>> p : commonRanges)\n        {\n            final RepairSession session = ActiveRepairService.instance.submitRepairSession(parentSession,\n                                                              p.right,\n                                                              keyspace,\n                                                              options.getParallelism(),\n                                                              p.left,\n                                                              repairedAt,\n                                                              executor,\n                                                              cfnames);\n            if (session == null)\n                continue;\n            // After repair session completes, notify client its result\n            Futures.addCallback(session, new FutureCallback<RepairSessionResult>()\n            {\n                public void onSuccess(RepairSessionResult result)\n                {\n                    /**\n                     * If the success message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s finished\", session.getId(),\n                                                   session.getRanges().toString());\n                    logger.info(message);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n\n                public void onFailure(Throwable t)\n                {\n                    /**\n                     * If the failure message below is modified, it must also be updated on\n                     * {@link org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport}\n                     * for backward-compatibility support.\n                     */\n                    String message = String.format(\"Repair session %s for range %s failed with error %s\",\n                                                   session.getId(), session.getRanges().toString(), t.getMessage());\n                    logger.error(message, t);\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.PROGRESS,\n                                                             progress.incrementAndGet(),\n                                                             totalProgress,\n                                                             message));\n                }\n            });\n            futures.add(session);\n        }\n\n        // After all repair sessions completes(successful or not),\n        // run anticompaction if necessary and send finish notice back to client\n        final Collection<Range<Token>> successfulRanges = new ArrayList<>();\n        final AtomicBoolean hasFailure = new AtomicBoolean();\n        final ListenableFuture<List<RepairSessionResult>> allSessions = Futures.successfulAsList(futures);\n        ListenableFuture anticompactionResult = Futures.transform(allSessions, new AsyncFunction<List<RepairSessionResult>, Object>()\n        {\n            @SuppressWarnings(\"unchecked\")\n            public ListenableFuture apply(List<RepairSessionResult> results)\n            {\n                // filter out null(=failed) results and get successful ranges\n                for (RepairSessionResult sessionResult : results)\n                {\n                    if (sessionResult != null)\n                    {\n                        successfulRanges.addAll(sessionResult.ranges);\n                    }\n                    else\n                    {\n                        hasFailure.compareAndSet(false, true);\n                    }\n                }\n                return ActiveRepairService.instance.finishParentSession(parentSession, allNeighbors, successfulRanges);\n            }\n        });\n        Futures.addCallback(anticompactionResult, new FutureCallback<Object>()\n        {\n            public void onSuccess(Object result)\n            {\n                SystemDistributedKeyspace.successfulParentRepair(parentSession, successfulRanges);\n                if (hasFailure.get())\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress,\n                                                             \"Some repair failed\"));\n                }\n                else\n                {\n                    fireProgressEvent(tag, new ProgressEvent(ProgressEventType.SUCCESS, progress.get(), totalProgress,\n                                                             \"Repair completed successfully\"));\n                }\n                repairComplete();\n            }\n\n            public void onFailure(Throwable t)\n            {\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.ERROR, progress.get(), totalProgress, t.getMessage()));\n                SystemDistributedKeyspace.failParentRepair(parentSession, t);\n                repairComplete();\n            }\n\n            private void repairComplete()\n            {\n                String duration = DurationFormatUtils.formatDurationWords(System.currentTimeMillis() - startTime,\n                                                                          true, true);\n                String message = String.format(\"Repair command #%d finished in %s\", cmd, duration);\n                fireProgressEvent(tag, new ProgressEvent(ProgressEventType.COMPLETE, progress.get(), totalProgress, message));\n                logger.info(message);\n                if (options.isTraced() && traceState != null)\n                {\n                    for (ProgressListener listener : listeners)\n                        traceState.removeProgressListener(listener);\n                    // Because DebuggableThreadPoolExecutor#afterExecute and this callback\n                    // run in a nondeterministic order (within the same thread), the\n                    // TraceState may have been nulled out at this point. The TraceState\n                    // should be traceState, so just set it without bothering to check if it\n                    // actually was nulled out.\n                    Tracing.instance.set(traceState);\n                    Tracing.traceRepair(message);\n                    Tracing.instance.stopSession();\n                }\n                executor.shutdownNow();\n            }\n        });\n    }"
        ],
        [
            "NodeProbe::getRMIClientSocketFactory()",
            " 229 -\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "    private RMIClientSocketFactory getRMIClientSocketFactory() throws IOException\n    {\n        if (Boolean.parseBoolean(System.getProperty(\"ssl.enable\")))\n            return new SslRMIClientSocketFactory();\n        else\n            return RMISocketFactory.getDefaultSocketFactory();\n    }",
            " 229 +\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "    private RMIClientSocketFactory getRMIClientSocketFactory()\n    {\n        if (Boolean.parseBoolean(System.getProperty(\"ssl.enable\")))\n            return new SslRMIClientSocketFactory();\n        else\n            return RMISocketFactory.getDefaultSocketFactory();\n    }"
        ],
        [
            "ColumnFamilyStore::Flush::reclaim(Memtable)",
            "1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208 -\n1209  \n1210  \n1211  \n1212  \n1213  \n1214  ",
            "        private void reclaim(final Memtable memtable)\n        {\n            // issue a read barrier for reclaiming the memory, and offload the wait to another thread\n            final OpOrder.Barrier readBarrier = readOrdering.newBarrier();\n            readBarrier.issue();\n            reclaimExecutor.execute(new WrappedRunnable()\n            {\n                public void runMayThrow() throws InterruptedException, ExecutionException\n                {\n                    readBarrier.await();\n                    memtable.setDiscarded();\n                }\n            });\n        }",
            "1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208 +\n1209  \n1210  \n1211  \n1212  \n1213  \n1214  ",
            "        private void reclaim(final Memtable memtable)\n        {\n            // issue a read barrier for reclaiming the memory, and offload the wait to another thread\n            final OpOrder.Barrier readBarrier = readOrdering.newBarrier();\n            readBarrier.issue();\n            reclaimExecutor.execute(new WrappedRunnable()\n            {\n                public void runMayThrow()\n                {\n                    readBarrier.await();\n                    memtable.setDiscarded();\n                }\n            });\n        }"
        ]
    ]
}