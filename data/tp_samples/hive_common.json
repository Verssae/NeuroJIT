{
    "8f5dee8c45ccf249b56609edb4a66d0211e2a00c": [
        [
            "TestTxnUtils::testSQLGenerator()",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "  @Test\n  public void testSQLGenerator() throws Exception {\n    //teseted on Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production\n    TxnHandler.SQLGenerator sqlGenerator =\n      new TxnHandler.SQLGenerator(TxnHandler.DatabaseProduct.ORACLE, conf);\n    List<String> rows = new ArrayList<>();\n    rows.add(\"'yellow', 1\");\n    List<String> sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1)\", sql.get(0));\n    rows.add(\"'red', 2\");\n    rows.add(\"'orange', 3\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    \n    Assert.assertEquals(\"Wrong stmt\", \n      \"insert all into colors(name, category) values('yellow', 1) into colors(name, category) values('red', 2) into colors(name, category) values('orange', 3) select * from dual\", sql.get(0));\n    for(int i = 0; i < conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE); i++) {\n      rows.add(\"\\'G\\',\" + i);\n    }\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 2, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert all into colors(name, category) values('yellow', 1) into colors(name, category) values('red', 2) into colors(name, category) values('orange', 3) into colors(name, category) values('G',0) into colors(name, category) values('G',1) into colors(name, category) values('G',2) into colors(name, category) values('G',3) into colors(name, category) values('G',4) into colors(name, category) values('G',5) into colors(name, category) values('G',6) into colors(name, category) values('G',7) into colors(name, category) values('G',8) into colors(name, category) values('G',9) into colors(name, category) values('G',10) into colors(name, category) values('G',11) into colors(name, category) values('G',12) into colors(name, category) values('G',13) into colors(name, category) values('G',14) into colors(name, category) values('G',15) into colors(name, category) values('G',16) into colors(name, category) values('G',17) into colors(name, category) values('G',18) into colors(name, category) values('G',19) into colors(name, category) values('G',20) into colors(name, category) values('G',21) into colors(name, category) values('G',22) into colors(name, category) values('G',23) into colors(name, category) values('G',24) into colors(name, category) values('G',25) into colors(name, category) values('G',26) into colors(name, category) values('G',27) into colors(name, category) values('G',28) into colors(name, category) values('G',29) into colors(name, category) values('G',30) into colors(name, category) values('G',31) into colors(name, category) values('G',32) into colors(name, category) values('G',33) into colors(name, category) values('G',34) into colors(name, category) values('G',35) into colors(name, category) values('G',36) into colors(name, category) values('G',37) into colors(name, category) values('G',38) into colors(name, category) values('G',39) into colors(name, category) values('G',40) into colors(name, category) values('G',41) into colors(name, category) values('G',42) into colors(name, category) values('G',43) into colors(name, category) values('G',44) into colors(name, category) values('G',45) into colors(name, category) values('G',46) into colors(name, category) values('G',47) into colors(name, category) values('G',48) into colors(name, category) values('G',49) into colors(name, category) values('G',50) into colors(name, category) values('G',51) into colors(name, category) values('G',52) into colors(name, category) values('G',53) into colors(name, category) values('G',54) into colors(name, category) values('G',55) into colors(name, category) values('G',56) into colors(name, category) values('G',57) into colors(name, category) values('G',58) into colors(name, category) values('G',59) into colors(name, category) values('G',60) into colors(name, category) values('G',61) into colors(name, category) values('G',62) into colors(name, category) values('G',63) into colors(name, category) values('G',64) into colors(name, category) values('G',65) into colors(name, category) values('G',66) into colors(name, category) values('G',67) into colors(name, category) values('G',68) into colors(name, category) values('G',69) into colors(name, category) values('G',70) into colors(name, category) values('G',71) into colors(name, category) values('G',72) into colors(name, category) values('G',73) into colors(name, category) values('G',74) into colors(name, category) values('G',75) into colors(name, category) values('G',76) into colors(name, category) values('G',77) into colors(name, category) values('G',78) into colors(name, category) values('G',79) into colors(name, category) values('G',80) into colors(name, category) values('G',81) into colors(name, category) values('G',82) into colors(name, category) values('G',83) into colors(name, category) values('G',84) into colors(name, category) values('G',85) into colors(name, category) values('G',86) into colors(name, category) values('G',87) into colors(name, category) values('G',88) into colors(name, category) values('G',89) into colors(name, category) values('G',90) into colors(name, category) values('G',91) into colors(name, category) values('G',92) into colors(name, category) values('G',93) into colors(name, category) values('G',94) into colors(name, category) values('G',95) into colors(name, category) values('G',96) into colors(name, category) values('G',97) into colors(name, category) values('G',98) into colors(name, category) values('G',99) into colors(name, category) values('G',100) into colors(name, category) values('G',101) into colors(name, category) values('G',102) into colors(name, category) values('G',103) into colors(name, category) values('G',104) into colors(name, category) values('G',105) into colors(name, category) values('G',106) into colors(name, category) values('G',107) into colors(name, category) values('G',108) into colors(name, category) values('G',109) into colors(name, category) values('G',110) into colors(name, category) values('G',111) into colors(name, category) values('G',112) into colors(name, category) values('G',113) into colors(name, category) values('G',114) into colors(name, category) values('G',115) into colors(name, category) values('G',116) into colors(name, category) values('G',117) into colors(name, category) values('G',118) into colors(name, category) values('G',119) into colors(name, category) values('G',120) into colors(name, category) values('G',121) into colors(name, category) values('G',122) into colors(name, category) values('G',123) into colors(name, category) values('G',124) into colors(name, category) values('G',125) into colors(name, category) values('G',126) into colors(name, category) values('G',127) into colors(name, category) values('G',128) into colors(name, category) values('G',129) into colors(name, category) values('G',130) into colors(name, category) values('G',131) into colors(name, category) values('G',132) into colors(name, category) values('G',133) into colors(name, category) values('G',134) into colors(name, category) values('G',135) into colors(name, category) values('G',136) into colors(name, category) values('G',137) into colors(name, category) values('G',138) into colors(name, category) values('G',139) into colors(name, category) values('G',140) into colors(name, category) values('G',141) into colors(name, category) values('G',142) into colors(name, category) values('G',143) into colors(name, category) values('G',144) into colors(name, category) values('G',145) into colors(name, category) values('G',146) into colors(name, category) values('G',147) into colors(name, category) values('G',148) into colors(name, category) values('G',149) into colors(name, category) values('G',150) into colors(name, category) values('G',151) into colors(name, category) values('G',152) into colors(name, category) values('G',153) into colors(name, category) values('G',154) into colors(name, category) values('G',155) into colors(name, category) values('G',156) into colors(name, category) values('G',157) into colors(name, category) values('G',158) into colors(name, category) values('G',159) into colors(name, category) values('G',160) into colors(name, category) values('G',161) into colors(name, category) values('G',162) into colors(name, category) values('G',163) into colors(name, category) values('G',164) into colors(name, category) values('G',165) into colors(name, category) values('G',166) into colors(name, category) values('G',167) into colors(name, category) values('G',168) into colors(name, category) values('G',169) into colors(name, category) values('G',170) into colors(name, category) values('G',171) into colors(name, category) values('G',172) into colors(name, category) values('G',173) into colors(name, category) values('G',174) into colors(name, category) values('G',175) into colors(name, category) values('G',176) into colors(name, category) values('G',177) into colors(name, category) values('G',178) into colors(name, category) values('G',179) into colors(name, category) values('G',180) into colors(name, category) values('G',181) into colors(name, category) values('G',182) into colors(name, category) values('G',183) into colors(name, category) values('G',184) into colors(name, category) values('G',185) into colors(name, category) values('G',186) into colors(name, category) values('G',187) into colors(name, category) values('G',188) into colors(name, category) values('G',189) into colors(name, category) values('G',190) into colors(name, category) values('G',191) into colors(name, category) values('G',192) into colors(name, category) values('G',193) into colors(name, category) values('G',194) into colors(name, category) values('G',195) into colors(name, category) values('G',196) into colors(name, category) values('G',197) into colors(name, category) values('G',198) into colors(name, category) values('G',199) into colors(name, category) values('G',200) into colors(name, category) values('G',201) into colors(name, category) values('G',202) into colors(name, category) values('G',203) into colors(name, category) values('G',204) into colors(name, category) values('G',205) into colors(name, category) values('G',206) into colors(name, category) values('G',207) into colors(name, category) values('G',208) into colors(name, category) values('G',209) into colors(name, category) values('G',210) into colors(name, category) values('G',211) into colors(name, category) values('G',212) into colors(name, category) values('G',213) into colors(name, category) values('G',214) into colors(name, category) values('G',215) into colors(name, category) values('G',216) into colors(name, category) values('G',217) into colors(name, category) values('G',218) into colors(name, category) values('G',219) into colors(name, category) values('G',220) into colors(name, category) values('G',221) into colors(name, category) values('G',222) into colors(name, category) values('G',223) into colors(name, category) values('G',224) into colors(name, category) values('G',225) into colors(name, category) values('G',226) into colors(name, category) values('G',227) into colors(name, category) values('G',228) into colors(name, category) values('G',229) into colors(name, category) values('G',230) into colors(name, category) values('G',231) into colors(name, category) values('G',232) into colors(name, category) values('G',233) into colors(name, category) values('G',234) into colors(name, category) values('G',235) into colors(name, category) values('G',236) into colors(name, category) values('G',237) into colors(name, category) values('G',238) into colors(name, category) values('G',239) into colors(name, category) values('G',240) into colors(name, category) values('G',241) into colors(name, category) values('G',242) into colors(name, category) values('G',243) into colors(name, category) values('G',244) into colors(name, category) values('G',245) into colors(name, category) values('G',246) into colors(name, category) values('G',247) into colors(name, category) values('G',248) into colors(name, category) values('G',249) into colors(name, category) values('G',250) into colors(name, category) values('G',251) into colors(name, category) values('G',252) into colors(name, category) values('G',253) into colors(name, category) values('G',254) into colors(name, category) values('G',255) into colors(name, category) values('G',256) into colors(name, category) values('G',257) into colors(name, category) values('G',258) into colors(name, category) values('G',259) into colors(name, category) values('G',260) into colors(name, category) values('G',261) into colors(name, category) values('G',262) into colors(name, category) values('G',263) into colors(name, category) values('G',264) into colors(name, category) values('G',265) into colors(name, category) values('G',266) into colors(name, category) values('G',267) into colors(name, category) values('G',268) into colors(name, category) values('G',269) into colors(name, category) values('G',270) into colors(name, category) values('G',271) into colors(name, category) values('G',272) into colors(name, category) values('G',273) into colors(name, category) values('G',274) into colors(name, category) values('G',275) into colors(name, category) values('G',276) into colors(name, category) values('G',277) into colors(name, category) values('G',278) into colors(name, category) values('G',279) into colors(name, category) values('G',280) into colors(name, category) values('G',281) into colors(name, category) values('G',282) into colors(name, category) values('G',283) into colors(name, category) values('G',284) into colors(name, category) values('G',285) into colors(name, category) values('G',286) into colors(name, category) values('G',287) into colors(name, category) values('G',288) into colors(name, category) values('G',289) into colors(name, category) values('G',290) into colors(name, category) values('G',291) into colors(name, category) values('G',292) into colors(name, category) values('G',293) into colors(name, category) values('G',294) into colors(name, category) values('G',295) into colors(name, category) values('G',296) into colors(name, category) values('G',297) into colors(name, category) values('G',298) into colors(name, category) values('G',299) into colors(name, category) values('G',300) into colors(name, category) values('G',301) into colors(name, category) values('G',302) into colors(name, category) values('G',303) into colors(name, category) values('G',304) into colors(name, category) values('G',305) into colors(name, category) values('G',306) into colors(name, category) values('G',307) into colors(name, category) values('G',308) into colors(name, category) values('G',309) into colors(name, category) values('G',310) into colors(name, category) values('G',311) into colors(name, category) values('G',312) into colors(name, category) values('G',313) into colors(name, category) values('G',314) into colors(name, category) values('G',315) into colors(name, category) values('G',316) into colors(name, category) values('G',317) into colors(name, category) values('G',318) into colors(name, category) values('G',319) into colors(name, category) values('G',320) into colors(name, category) values('G',321) into colors(name, category) values('G',322) into colors(name, category) values('G',323) into colors(name, category) values('G',324) into colors(name, category) values('G',325) into colors(name, category) values('G',326) into colors(name, category) values('G',327) into colors(name, category) values('G',328) into colors(name, category) values('G',329) into colors(name, category) values('G',330) into colors(name, category) values('G',331) into colors(name, category) values('G',332) into colors(name, category) values('G',333) into colors(name, category) values('G',334) into colors(name, category) values('G',335) into colors(name, category) values('G',336) into colors(name, category) values('G',337) into colors(name, category) values('G',338) into colors(name, category) values('G',339) into colors(name, category) values('G',340) into colors(name, category) values('G',341) into colors(name, category) values('G',342) into colors(name, category) values('G',343) into colors(name, category) values('G',344) into colors(name, category) values('G',345) into colors(name, category) values('G',346) into colors(name, category) values('G',347) into colors(name, category) values('G',348) into colors(name, category) values('G',349) into colors(name, category) values('G',350) into colors(name, category) values('G',351) into colors(name, category) values('G',352) into colors(name, category) values('G',353) into colors(name, category) values('G',354) into colors(name, category) values('G',355) into colors(name, category) values('G',356) into colors(name, category) values('G',357) into colors(name, category) values('G',358) into colors(name, category) values('G',359) into colors(name, category) values('G',360) into colors(name, category) values('G',361) into colors(name, category) values('G',362) into colors(name, category) values('G',363) into colors(name, category) values('G',364) into colors(name, category) values('G',365) into colors(name, category) values('G',366) into colors(name, category) values('G',367) into colors(name, category) values('G',368) into colors(name, category) values('G',369) into colors(name, category) values('G',370) into colors(name, category) values('G',371) into colors(name, category) values('G',372) into colors(name, category) values('G',373) into colors(name, category) values('G',374) into colors(name, category) values('G',375) into colors(name, category) values('G',376) into colors(name, category) values('G',377) into colors(name, category) values('G',378) into colors(name, category) values('G',379) into colors(name, category) values('G',380) into colors(name, category) values('G',381) into colors(name, category) values('G',382) into colors(name, category) values('G',383) into colors(name, category) values('G',384) into colors(name, category) values('G',385) into colors(name, category) values('G',386) into colors(name, category) values('G',387) into colors(name, category) values('G',388) into colors(name, category) values('G',389) into colors(name, category) values('G',390) into colors(name, category) values('G',391) into colors(name, category) values('G',392) into colors(name, category) values('G',393) into colors(name, category) values('G',394) into colors(name, category) values('G',395) into colors(name, category) values('G',396) into colors(name, category) values('G',397) into colors(name, category) values('G',398) into colors(name, category) values('G',399) into colors(name, category) values('G',400) into colors(name, category) values('G',401) into colors(name, category) values('G',402) into colors(name, category) values('G',403) into colors(name, category) values('G',404) into colors(name, category) values('G',405) into colors(name, category) values('G',406) into colors(name, category) values('G',407) into colors(name, category) values('G',408) into colors(name, category) values('G',409) into colors(name, category) values('G',410) into colors(name, category) values('G',411) into colors(name, category) values('G',412) into colors(name, category) values('G',413) into colors(name, category) values('G',414) into colors(name, category) values('G',415) into colors(name, category) values('G',416) into colors(name, category) values('G',417) into colors(name, category) values('G',418) into colors(name, category) values('G',419) into colors(name, category) values('G',420) into colors(name, category) values('G',421) into colors(name, category) values('G',422) into colors(name, category) values('G',423) into colors(name, category) values('G',424) into colors(name, category) values('G',425) into colors(name, category) values('G',426) into colors(name, category) values('G',427) into colors(name, category) values('G',428) into colors(name, category) values('G',429) into colors(name, category) values('G',430) into colors(name, category) values('G',431) into colors(name, category) values('G',432) into colors(name, category) values('G',433) into colors(name, category) values('G',434) into colors(name, category) values('G',435) into colors(name, category) values('G',436) into colors(name, category) values('G',437) into colors(name, category) values('G',438) into colors(name, category) values('G',439) into colors(name, category) values('G',440) into colors(name, category) values('G',441) into colors(name, category) values('G',442) into colors(name, category) values('G',443) into colors(name, category) values('G',444) into colors(name, category) values('G',445) into colors(name, category) values('G',446) into colors(name, category) values('G',447) into colors(name, category) values('G',448) into colors(name, category) values('G',449) into colors(name, category) values('G',450) into colors(name, category) values('G',451) into colors(name, category) values('G',452) into colors(name, category) values('G',453) into colors(name, category) values('G',454) into colors(name, category) values('G',455) into colors(name, category) values('G',456) into colors(name, category) values('G',457) into colors(name, category) values('G',458) into colors(name, category) values('G',459) into colors(name, category) values('G',460) into colors(name, category) values('G',461) into colors(name, category) values('G',462) into colors(name, category) values('G',463) into colors(name, category) values('G',464) into colors(name, category) values('G',465) into colors(name, category) values('G',466) into colors(name, category) values('G',467) into colors(name, category) values('G',468) into colors(name, category) values('G',469) into colors(name, category) values('G',470) into colors(name, category) values('G',471) into colors(name, category) values('G',472) into colors(name, category) values('G',473) into colors(name, category) values('G',474) into colors(name, category) values('G',475) into colors(name, category) values('G',476) into colors(name, category) values('G',477) into colors(name, category) values('G',478) into colors(name, category) values('G',479) into colors(name, category) values('G',480) into colors(name, category) values('G',481) into colors(name, category) values('G',482) into colors(name, category) values('G',483) into colors(name, category) values('G',484) into colors(name, category) values('G',485) into colors(name, category) values('G',486) into colors(name, category) values('G',487) into colors(name, category) values('G',488) into colors(name, category) values('G',489) into colors(name, category) values('G',490) into colors(name, category) values('G',491) into colors(name, category) values('G',492) into colors(name, category) values('G',493) into colors(name, category) values('G',494) into colors(name, category) values('G',495) into colors(name, category) values('G',496) into colors(name, category) values('G',497) into colors(name, category) values('G',498) into colors(name, category) values('G',499) into colors(name, category) values('G',500) into colors(name, category) values('G',501) into colors(name, category) values('G',502) into colors(name, category) values('G',503) into colors(name, category) values('G',504) into colors(name, category) values('G',505) into colors(name, category) values('G',506) into colors(name, category) values('G',507) into colors(name, category) values('G',508) into colors(name, category) values('G',509) into colors(name, category) values('G',510) into colors(name, category) values('G',511) into colors(name, category) values('G',512) into colors(name, category) values('G',513) into colors(name, category) values('G',514) into colors(name, category) values('G',515) into colors(name, category) values('G',516) into colors(name, category) values('G',517) into colors(name, category) values('G',518) into colors(name, category) values('G',519) into colors(name, category) values('G',520) into colors(name, category) values('G',521) into colors(name, category) values('G',522) into colors(name, category) values('G',523) into colors(name, category) values('G',524) into colors(name, category) values('G',525) into colors(name, category) values('G',526) into colors(name, category) values('G',527) into colors(name, category) values('G',528) into colors(name, category) values('G',529) into colors(name, category) values('G',530) into colors(name, category) values('G',531) into colors(name, category) values('G',532) into colors(name, category) values('G',533) into colors(name, category) values('G',534) into colors(name, category) values('G',535) into colors(name, category) values('G',536) into colors(name, category) values('G',537) into colors(name, category) values('G',538) into colors(name, category) values('G',539) into colors(name, category) values('G',540) into colors(name, category) values('G',541) into colors(name, category) values('G',542) into colors(name, category) values('G',543) into colors(name, category) values('G',544) into colors(name, category) values('G',545) into colors(name, category) values('G',546) into colors(name, category) values('G',547) into colors(name, category) values('G',548) into colors(name, category) values('G',549) into colors(name, category) values('G',550) into colors(name, category) values('G',551) into colors(name, category) values('G',552) into colors(name, category) values('G',553) into colors(name, category) values('G',554) into colors(name, category) values('G',555) into colors(name, category) values('G',556) into colors(name, category) values('G',557) into colors(name, category) values('G',558) into colors(name, category) values('G',559) into colors(name, category) values('G',560) into colors(name, category) values('G',561) into colors(name, category) values('G',562) into colors(name, category) values('G',563) into colors(name, category) values('G',564) into colors(name, category) values('G',565) into colors(name, category) values('G',566) into colors(name, category) values('G',567) into colors(name, category) values('G',568) into colors(name, category) values('G',569) into colors(name, category) values('G',570) into colors(name, category) values('G',571) into colors(name, category) values('G',572) into colors(name, category) values('G',573) into colors(name, category) values('G',574) into colors(name, category) values('G',575) into colors(name, category) values('G',576) into colors(name, category) values('G',577) into colors(name, category) values('G',578) into colors(name, category) values('G',579) into colors(name, category) values('G',580) into colors(name, category) values('G',581) into colors(name, category) values('G',582) into colors(name, category) values('G',583) into colors(name, category) values('G',584) into colors(name, category) values('G',585) into colors(name, category) values('G',586) into colors(name, category) values('G',587) into colors(name, category) values('G',588) into colors(name, category) values('G',589) into colors(name, category) values('G',590) into colors(name, category) values('G',591) into colors(name, category) values('G',592) into colors(name, category) values('G',593) into colors(name, category) values('G',594) into colors(name, category) values('G',595) into colors(name, category) values('G',596) into colors(name, category) values('G',597) into colors(name, category) values('G',598) into colors(name, category) values('G',599) into colors(name, category) values('G',600) into colors(name, category) values('G',601) into colors(name, category) values('G',602) into colors(name, category) values('G',603) into colors(name, category) values('G',604) into colors(name, category) values('G',605) into colors(name, category) values('G',606) into colors(name, category) values('G',607) into colors(name, category) values('G',608) into colors(name, category) values('G',609) into colors(name, category) values('G',610) into colors(name, category) values('G',611) into colors(name, category) values('G',612) into colors(name, category) values('G',613) into colors(name, category) values('G',614) into colors(name, category) values('G',615) into colors(name, category) values('G',616) into colors(name, category) values('G',617) into colors(name, category) values('G',618) into colors(name, category) values('G',619) into colors(name, category) values('G',620) into colors(name, category) values('G',621) into colors(name, category) values('G',622) into colors(name, category) values('G',623) into colors(name, category) values('G',624) into colors(name, category) values('G',625) into colors(name, category) values('G',626) into colors(name, category) values('G',627) into colors(name, category) values('G',628) into colors(name, category) values('G',629) into colors(name, category) values('G',630) into colors(name, category) values('G',631) into colors(name, category) values('G',632) into colors(name, category) values('G',633) into colors(name, category) values('G',634) into colors(name, category) values('G',635) into colors(name, category) values('G',636) into colors(name, category) values('G',637) into colors(name, category) values('G',638) into colors(name, category) values('G',639) into colors(name, category) values('G',640) into colors(name, category) values('G',641) into colors(name, category) values('G',642) into colors(name, category) values('G',643) into colors(name, category) values('G',644) into colors(name, category) values('G',645) into colors(name, category) values('G',646) into colors(name, category) values('G',647) into colors(name, category) values('G',648) into colors(name, category) values('G',649) into colors(name, category) values('G',650) into colors(name, category) values('G',651) into colors(name, category) values('G',652) into colors(name, category) values('G',653) into colors(name, category) values('G',654) into colors(name, category) values('G',655) into colors(name, category) values('G',656) into colors(name, category) values('G',657) into colors(name, category) values('G',658) into colors(name, category) values('G',659) into colors(name, category) values('G',660) into colors(name, category) values('G',661) into colors(name, category) values('G',662) into colors(name, category) values('G',663) into colors(name, category) values('G',664) into colors(name, category) values('G',665) into colors(name, category) values('G',666) into colors(name, category) values('G',667) into colors(name, category) values('G',668) into colors(name, category) values('G',669) into colors(name, category) values('G',670) into colors(name, category) values('G',671) into colors(name, category) values('G',672) into colors(name, category) values('G',673) into colors(name, category) values('G',674) into colors(name, category) values('G',675) into colors(name, category) values('G',676) into colors(name, category) values('G',677) into colors(name, category) values('G',678) into colors(name, category) values('G',679) into colors(name, category) values('G',680) into colors(name, category) values('G',681) into colors(name, category) values('G',682) into colors(name, category) values('G',683) into colors(name, category) values('G',684) into colors(name, category) values('G',685) into colors(name, category) values('G',686) into colors(name, category) values('G',687) into colors(name, category) values('G',688) into colors(name, category) values('G',689) into colors(name, category) values('G',690) into colors(name, category) values('G',691) into colors(name, category) values('G',692) into colors(name, category) values('G',693) into colors(name, category) values('G',694) into colors(name, category) values('G',695) into colors(name, category) values('G',696) into colors(name, category) values('G',697) into colors(name, category) values('G',698) into colors(name, category) values('G',699) into colors(name, category) values('G',700) into colors(name, category) values('G',701) into colors(name, category) values('G',702) into colors(name, category) values('G',703) into colors(name, category) values('G',704) into colors(name, category) values('G',705) into colors(name, category) values('G',706) into colors(name, category) values('G',707) into colors(name, category) values('G',708) into colors(name, category) values('G',709) into colors(name, category) values('G',710) into colors(name, category) values('G',711) into colors(name, category) values('G',712) into colors(name, category) values('G',713) into colors(name, category) values('G',714) into colors(name, category) values('G',715) into colors(name, category) values('G',716) into colors(name, category) values('G',717) into colors(name, category) values('G',718) into colors(name, category) values('G',719) into colors(name, category) values('G',720) into colors(name, category) values('G',721) into colors(name, category) values('G',722) into colors(name, category) values('G',723) into colors(name, category) values('G',724) into colors(name, category) values('G',725) into colors(name, category) values('G',726) into colors(name, category) values('G',727) into colors(name, category) values('G',728) into colors(name, category) values('G',729) into colors(name, category) values('G',730) into colors(name, category) values('G',731) into colors(name, category) values('G',732) into colors(name, category) values('G',733) into colors(name, category) values('G',734) into colors(name, category) values('G',735) into colors(name, category) values('G',736) into colors(name, category) values('G',737) into colors(name, category) values('G',738) into colors(name, category) values('G',739) into colors(name, category) values('G',740) into colors(name, category) values('G',741) into colors(name, category) values('G',742) into colors(name, category) values('G',743) into colors(name, category) values('G',744) into colors(name, category) values('G',745) into colors(name, category) values('G',746) into colors(name, category) values('G',747) into colors(name, category) values('G',748) into colors(name, category) values('G',749) into colors(name, category) values('G',750) into colors(name, category) values('G',751) into colors(name, category) values('G',752) into colors(name, category) values('G',753) into colors(name, category) values('G',754) into colors(name, category) values('G',755) into colors(name, category) values('G',756) into colors(name, category) values('G',757) into colors(name, category) values('G',758) into colors(name, category) values('G',759) into colors(name, category) values('G',760) into colors(name, category) values('G',761) into colors(name, category) values('G',762) into colors(name, category) values('G',763) into colors(name, category) values('G',764) into colors(name, category) values('G',765) into colors(name, category) values('G',766) into colors(name, category) values('G',767) into colors(name, category) values('G',768) into colors(name, category) values('G',769) into colors(name, category) values('G',770) into colors(name, category) values('G',771) into colors(name, category) values('G',772) into colors(name, category) values('G',773) into colors(name, category) values('G',774) into colors(name, category) values('G',775) into colors(name, category) values('G',776) into colors(name, category) values('G',777) into colors(name, category) values('G',778) into colors(name, category) values('G',779) into colors(name, category) values('G',780) into colors(name, category) values('G',781) into colors(name, category) values('G',782) into colors(name, category) values('G',783) into colors(name, category) values('G',784) into colors(name, category) values('G',785) into colors(name, category) values('G',786) into colors(name, category) values('G',787) into colors(name, category) values('G',788) into colors(name, category) values('G',789) into colors(name, category) values('G',790) into colors(name, category) values('G',791) into colors(name, category) values('G',792) into colors(name, category) values('G',793) into colors(name, category) values('G',794) into colors(name, category) values('G',795) into colors(name, category) values('G',796) into colors(name, category) values('G',797) into colors(name, category) values('G',798) into colors(name, category) values('G',799) into colors(name, category) values('G',800) into colors(name, category) values('G',801) into colors(name, category) values('G',802) into colors(name, category) values('G',803) into colors(name, category) values('G',804) into colors(name, category) values('G',805) into colors(name, category) values('G',806) into colors(name, category) values('G',807) into colors(name, category) values('G',808) into colors(name, category) values('G',809) into colors(name, category) values('G',810) into colors(name, category) values('G',811) into colors(name, category) values('G',812) into colors(name, category) values('G',813) into colors(name, category) values('G',814) into colors(name, category) values('G',815) into colors(name, category) values('G',816) into colors(name, category) values('G',817) into colors(name, category) values('G',818) into colors(name, category) values('G',819) into colors(name, category) values('G',820) into colors(name, category) values('G',821) into colors(name, category) values('G',822) into colors(name, category) values('G',823) into colors(name, category) values('G',824) into colors(name, category) values('G',825) into colors(name, category) values('G',826) into colors(name, category) values('G',827) into colors(name, category) values('G',828) into colors(name, category) values('G',829) into colors(name, category) values('G',830) into colors(name, category) values('G',831) into colors(name, category) values('G',832) into colors(name, category) values('G',833) into colors(name, category) values('G',834) into colors(name, category) values('G',835) into colors(name, category) values('G',836) into colors(name, category) values('G',837) into colors(name, category) values('G',838) into colors(name, category) values('G',839) into colors(name, category) values('G',840) into colors(name, category) values('G',841) into colors(name, category) values('G',842) into colors(name, category) values('G',843) into colors(name, category) values('G',844) into colors(name, category) values('G',845) into colors(name, category) values('G',846) into colors(name, category) values('G',847) into colors(name, category) values('G',848) into colors(name, category) values('G',849) into colors(name, category) values('G',850) into colors(name, category) values('G',851) into colors(name, category) values('G',852) into colors(name, category) values('G',853) into colors(name, category) values('G',854) into colors(name, category) values('G',855) into colors(name, category) values('G',856) into colors(name, category) values('G',857) into colors(name, category) values('G',858) into colors(name, category) values('G',859) into colors(name, category) values('G',860) into colors(name, category) values('G',861) into colors(name, category) values('G',862) into colors(name, category) values('G',863) into colors(name, category) values('G',864) into colors(name, category) values('G',865) into colors(name, category) values('G',866) into colors(name, category) values('G',867) into colors(name, category) values('G',868) into colors(name, category) values('G',869) into colors(name, category) values('G',870) into colors(name, category) values('G',871) into colors(name, category) values('G',872) into colors(name, category) values('G',873) into colors(name, category) values('G',874) into colors(name, category) values('G',875) into colors(name, category) values('G',876) into colors(name, category) values('G',877) into colors(name, category) values('G',878) into colors(name, category) values('G',879) into colors(name, category) values('G',880) into colors(name, category) values('G',881) into colors(name, category) values('G',882) into colors(name, category) values('G',883) into colors(name, category) values('G',884) into colors(name, category) values('G',885) into colors(name, category) values('G',886) into colors(name, category) values('G',887) into colors(name, category) values('G',888) into colors(name, category) values('G',889) into colors(name, category) values('G',890) into colors(name, category) values('G',891) into colors(name, category) values('G',892) into colors(name, category) values('G',893) into colors(name, category) values('G',894) into colors(name, category) values('G',895) into colors(name, category) values('G',896) into colors(name, category) values('G',897) into colors(name, category) values('G',898) into colors(name, category) values('G',899) into colors(name, category) values('G',900) into colors(name, category) values('G',901) into colors(name, category) values('G',902) into colors(name, category) values('G',903) into colors(name, category) values('G',904) into colors(name, category) values('G',905) into colors(name, category) values('G',906) into colors(name, category) values('G',907) into colors(name, category) values('G',908) into colors(name, category) values('G',909) into colors(name, category) values('G',910) into colors(name, category) values('G',911) into colors(name, category) values('G',912) into colors(name, category) values('G',913) into colors(name, category) values('G',914) into colors(name, category) values('G',915) into colors(name, category) values('G',916) into colors(name, category) values('G',917) into colors(name, category) values('G',918) into colors(name, category) values('G',919) into colors(name, category) values('G',920) into colors(name, category) values('G',921) into colors(name, category) values('G',922) into colors(name, category) values('G',923) into colors(name, category) values('G',924) into colors(name, category) values('G',925) into colors(name, category) values('G',926) into colors(name, category) values('G',927) into colors(name, category) values('G',928) into colors(name, category) values('G',929) into colors(name, category) values('G',930) into colors(name, category) values('G',931) into colors(name, category) values('G',932) into colors(name, category) values('G',933) into colors(name, category) values('G',934) into colors(name, category) values('G',935) into colors(name, category) values('G',936) into colors(name, category) values('G',937) into colors(name, category) values('G',938) into colors(name, category) values('G',939) into colors(name, category) values('G',940) into colors(name, category) values('G',941) into colors(name, category) values('G',942) into colors(name, category) values('G',943) into colors(name, category) values('G',944) into colors(name, category) values('G',945) into colors(name, category) values('G',946) into colors(name, category) values('G',947) into colors(name, category) values('G',948) into colors(name, category) values('G',949) into colors(name, category) values('G',950) into colors(name, category) values('G',951) into colors(name, category) values('G',952) into colors(name, category) values('G',953) into colors(name, category) values('G',954) into colors(name, category) values('G',955) into colors(name, category) values('G',956) into colors(name, category) values('G',957) into colors(name, category) values('G',958) into colors(name, category) values('G',959) into colors(name, category) values('G',960) into colors(name, category) values('G',961) into colors(name, category) values('G',962) into colors(name, category) values('G',963) into colors(name, category) values('G',964) into colors(name, category) values('G',965) into colors(name, category) values('G',966) into colors(name, category) values('G',967) into colors(name, category) values('G',968) into colors(name, category) values('G',969) into colors(name, category) values('G',970) into colors(name, category) values('G',971) into colors(name, category) values('G',972) into colors(name, category) values('G',973) into colors(name, category) values('G',974) into colors(name, category) values('G',975) into colors(name, category) values('G',976) into colors(name, category) values('G',977) into colors(name, category) values('G',978) into colors(name, category) values('G',979) into colors(name, category) values('G',980) into colors(name, category) values('G',981) into colors(name, category) values('G',982) into colors(name, category) values('G',983) into colors(name, category) values('G',984) into colors(name, category) values('G',985) into colors(name, category) values('G',986) into colors(name, category) values('G',987) into colors(name, category) values('G',988) into colors(name, category) values('G',989) into colors(name, category) values('G',990) into colors(name, category) values('G',991) into colors(name, category) values('G',992) into colors(name, category) values('G',993) into colors(name, category) values('G',994) into colors(name, category) values('G',995) into colors(name, category) values('G',996)  select * from dual\", sql.get(0));\n    Assert.assertEquals(\"Wrong stmt\", \"insert all into colors(name, category) values('G',997) into colors(name, category) values('G',998) into colors(name, category) values('G',999) select * from dual\", sql.get(1));\n    \n    sqlGenerator =\n      new TxnHandler.SQLGenerator(TxnHandler.DatabaseProduct.MYSQL, conf);\n    rows.clear();\n    rows.add(\"'yellow', 1\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1)\", sql.get(0));\n    rows.add(\"'red', 2\");\n    rows.add(\"'orange', 3\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1),('red', 2),('orange', 3)\", sql.get(0));\n    for(int i = 0; i < conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE); i++) {\n      rows.add(\"\\'G\\',\" + i);\n    }\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 2, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1),('red', 2),('orange', 3),('G',0),('G',1),('G',2),('G',3),('G',4),('G',5),('G',6),('G',7),('G',8),('G',9),('G',10),('G',11),('G',12),('G',13),('G',14),('G',15),('G',16),('G',17),('G',18),('G',19),('G',20),('G',21),('G',22),('G',23),('G',24),('G',25),('G',26),('G',27),('G',28),('G',29),('G',30),('G',31),('G',32),('G',33),('G',34),('G',35),('G',36),('G',37),('G',38),('G',39),('G',40),('G',41),('G',42),('G',43),('G',44),('G',45),('G',46),('G',47),('G',48),('G',49),('G',50),('G',51),('G',52),('G',53),('G',54),('G',55),('G',56),('G',57),('G',58),('G',59),('G',60),('G',61),('G',62),('G',63),('G',64),('G',65),('G',66),('G',67),('G',68),('G',69),('G',70),('G',71),('G',72),('G',73),('G',74),('G',75),('G',76),('G',77),('G',78),('G',79),('G',80),('G',81),('G',82),('G',83),('G',84),('G',85),('G',86),('G',87),('G',88),('G',89),('G',90),('G',91),('G',92),('G',93),('G',94),('G',95),('G',96),('G',97),('G',98),('G',99),('G',100),('G',101),('G',102),('G',103),('G',104),('G',105),('G',106),('G',107),('G',108),('G',109),('G',110),('G',111),('G',112),('G',113),('G',114),('G',115),('G',116),('G',117),('G',118),('G',119),('G',120),('G',121),('G',122),('G',123),('G',124),('G',125),('G',126),('G',127),('G',128),('G',129),('G',130),('G',131),('G',132),('G',133),('G',134),('G',135),('G',136),('G',137),('G',138),('G',139),('G',140),('G',141),('G',142),('G',143),('G',144),('G',145),('G',146),('G',147),('G',148),('G',149),('G',150),('G',151),('G',152),('G',153),('G',154),('G',155),('G',156),('G',157),('G',158),('G',159),('G',160),('G',161),('G',162),('G',163),('G',164),('G',165),('G',166),('G',167),('G',168),('G',169),('G',170),('G',171),('G',172),('G',173),('G',174),('G',175),('G',176),('G',177),('G',178),('G',179),('G',180),('G',181),('G',182),('G',183),('G',184),('G',185),('G',186),('G',187),('G',188),('G',189),('G',190),('G',191),('G',192),('G',193),('G',194),('G',195),('G',196),('G',197),('G',198),('G',199),('G',200),('G',201),('G',202),('G',203),('G',204),('G',205),('G',206),('G',207),('G',208),('G',209),('G',210),('G',211),('G',212),('G',213),('G',214),('G',215),('G',216),('G',217),('G',218),('G',219),('G',220),('G',221),('G',222),('G',223),('G',224),('G',225),('G',226),('G',227),('G',228),('G',229),('G',230),('G',231),('G',232),('G',233),('G',234),('G',235),('G',236),('G',237),('G',238),('G',239),('G',240),('G',241),('G',242),('G',243),('G',244),('G',245),('G',246),('G',247),('G',248),('G',249),('G',250),('G',251),('G',252),('G',253),('G',254),('G',255),('G',256),('G',257),('G',258),('G',259),('G',260),('G',261),('G',262),('G',263),('G',264),('G',265),('G',266),('G',267),('G',268),('G',269),('G',270),('G',271),('G',272),('G',273),('G',274),('G',275),('G',276),('G',277),('G',278),('G',279),('G',280),('G',281),('G',282),('G',283),('G',284),('G',285),('G',286),('G',287),('G',288),('G',289),('G',290),('G',291),('G',292),('G',293),('G',294),('G',295),('G',296),('G',297),('G',298),('G',299),('G',300),('G',301),('G',302),('G',303),('G',304),('G',305),('G',306),('G',307),('G',308),('G',309),('G',310),('G',311),('G',312),('G',313),('G',314),('G',315),('G',316),('G',317),('G',318),('G',319),('G',320),('G',321),('G',322),('G',323),('G',324),('G',325),('G',326),('G',327),('G',328),('G',329),('G',330),('G',331),('G',332),('G',333),('G',334),('G',335),('G',336),('G',337),('G',338),('G',339),('G',340),('G',341),('G',342),('G',343),('G',344),('G',345),('G',346),('G',347),('G',348),('G',349),('G',350),('G',351),('G',352),('G',353),('G',354),('G',355),('G',356),('G',357),('G',358),('G',359),('G',360),('G',361),('G',362),('G',363),('G',364),('G',365),('G',366),('G',367),('G',368),('G',369),('G',370),('G',371),('G',372),('G',373),('G',374),('G',375),('G',376),('G',377),('G',378),('G',379),('G',380),('G',381),('G',382),('G',383),('G',384),('G',385),('G',386),('G',387),('G',388),('G',389),('G',390),('G',391),('G',392),('G',393),('G',394),('G',395),('G',396),('G',397),('G',398),('G',399),('G',400),('G',401),('G',402),('G',403),('G',404),('G',405),('G',406),('G',407),('G',408),('G',409),('G',410),('G',411),('G',412),('G',413),('G',414),('G',415),('G',416),('G',417),('G',418),('G',419),('G',420),('G',421),('G',422),('G',423),('G',424),('G',425),('G',426),('G',427),('G',428),('G',429),('G',430),('G',431),('G',432),('G',433),('G',434),('G',435),('G',436),('G',437),('G',438),('G',439),('G',440),('G',441),('G',442),('G',443),('G',444),('G',445),('G',446),('G',447),('G',448),('G',449),('G',450),('G',451),('G',452),('G',453),('G',454),('G',455),('G',456),('G',457),('G',458),('G',459),('G',460),('G',461),('G',462),('G',463),('G',464),('G',465),('G',466),('G',467),('G',468),('G',469),('G',470),('G',471),('G',472),('G',473),('G',474),('G',475),('G',476),('G',477),('G',478),('G',479),('G',480),('G',481),('G',482),('G',483),('G',484),('G',485),('G',486),('G',487),('G',488),('G',489),('G',490),('G',491),('G',492),('G',493),('G',494),('G',495),('G',496),('G',497),('G',498),('G',499),('G',500),('G',501),('G',502),('G',503),('G',504),('G',505),('G',506),('G',507),('G',508),('G',509),('G',510),('G',511),('G',512),('G',513),('G',514),('G',515),('G',516),('G',517),('G',518),('G',519),('G',520),('G',521),('G',522),('G',523),('G',524),('G',525),('G',526),('G',527),('G',528),('G',529),('G',530),('G',531),('G',532),('G',533),('G',534),('G',535),('G',536),('G',537),('G',538),('G',539),('G',540),('G',541),('G',542),('G',543),('G',544),('G',545),('G',546),('G',547),('G',548),('G',549),('G',550),('G',551),('G',552),('G',553),('G',554),('G',555),('G',556),('G',557),('G',558),('G',559),('G',560),('G',561),('G',562),('G',563),('G',564),('G',565),('G',566),('G',567),('G',568),('G',569),('G',570),('G',571),('G',572),('G',573),('G',574),('G',575),('G',576),('G',577),('G',578),('G',579),('G',580),('G',581),('G',582),('G',583),('G',584),('G',585),('G',586),('G',587),('G',588),('G',589),('G',590),('G',591),('G',592),('G',593),('G',594),('G',595),('G',596),('G',597),('G',598),('G',599),('G',600),('G',601),('G',602),('G',603),('G',604),('G',605),('G',606),('G',607),('G',608),('G',609),('G',610),('G',611),('G',612),('G',613),('G',614),('G',615),('G',616),('G',617),('G',618),('G',619),('G',620),('G',621),('G',622),('G',623),('G',624),('G',625),('G',626),('G',627),('G',628),('G',629),('G',630),('G',631),('G',632),('G',633),('G',634),('G',635),('G',636),('G',637),('G',638),('G',639),('G',640),('G',641),('G',642),('G',643),('G',644),('G',645),('G',646),('G',647),('G',648),('G',649),('G',650),('G',651),('G',652),('G',653),('G',654),('G',655),('G',656),('G',657),('G',658),('G',659),('G',660),('G',661),('G',662),('G',663),('G',664),('G',665),('G',666),('G',667),('G',668),('G',669),('G',670),('G',671),('G',672),('G',673),('G',674),('G',675),('G',676),('G',677),('G',678),('G',679),('G',680),('G',681),('G',682),('G',683),('G',684),('G',685),('G',686),('G',687),('G',688),('G',689),('G',690),('G',691),('G',692),('G',693),('G',694),('G',695),('G',696),('G',697),('G',698),('G',699),('G',700),('G',701),('G',702),('G',703),('G',704),('G',705),('G',706),('G',707),('G',708),('G',709),('G',710),('G',711),('G',712),('G',713),('G',714),('G',715),('G',716),('G',717),('G',718),('G',719),('G',720),('G',721),('G',722),('G',723),('G',724),('G',725),('G',726),('G',727),('G',728),('G',729),('G',730),('G',731),('G',732),('G',733),('G',734),('G',735),('G',736),('G',737),('G',738),('G',739),('G',740),('G',741),('G',742),('G',743),('G',744),('G',745),('G',746),('G',747),('G',748),('G',749),('G',750),('G',751),('G',752),('G',753),('G',754),('G',755),('G',756),('G',757),('G',758),('G',759),('G',760),('G',761),('G',762),('G',763),('G',764),('G',765),('G',766),('G',767),('G',768),('G',769),('G',770),('G',771),('G',772),('G',773),('G',774),('G',775),('G',776),('G',777),('G',778),('G',779),('G',780),('G',781),('G',782),('G',783),('G',784),('G',785),('G',786),('G',787),('G',788),('G',789),('G',790),('G',791),('G',792),('G',793),('G',794),('G',795),('G',796),('G',797),('G',798),('G',799),('G',800),('G',801),('G',802),('G',803),('G',804),('G',805),('G',806),('G',807),('G',808),('G',809),('G',810),('G',811),('G',812),('G',813),('G',814),('G',815),('G',816),('G',817),('G',818),('G',819),('G',820),('G',821),('G',822),('G',823),('G',824),('G',825),('G',826),('G',827),('G',828),('G',829),('G',830),('G',831),('G',832),('G',833),('G',834),('G',835),('G',836),('G',837),('G',838),('G',839),('G',840),('G',841),('G',842),('G',843),('G',844),('G',845),('G',846),('G',847),('G',848),('G',849),('G',850),('G',851),('G',852),('G',853),('G',854),('G',855),('G',856),('G',857),('G',858),('G',859),('G',860),('G',861),('G',862),('G',863),('G',864),('G',865),('G',866),('G',867),('G',868),('G',869),('G',870),('G',871),('G',872),('G',873),('G',874),('G',875),('G',876),('G',877),('G',878),('G',879),('G',880),('G',881),('G',882),('G',883),('G',884),('G',885),('G',886),('G',887),('G',888),('G',889),('G',890),('G',891),('G',892),('G',893),('G',894),('G',895),('G',896),('G',897),('G',898),('G',899),('G',900),('G',901),('G',902),('G',903),('G',904),('G',905),('G',906),('G',907),('G',908),('G',909),('G',910),('G',911),('G',912),('G',913),('G',914),('G',915),('G',916),('G',917),('G',918),('G',919),('G',920),('G',921),('G',922),('G',923),('G',924),('G',925),('G',926),('G',927),('G',928),('G',929),('G',930),('G',931),('G',932),('G',933),('G',934),('G',935),('G',936),('G',937),('G',938),('G',939),('G',940),('G',941),('G',942),('G',943),('G',944),('G',945),('G',946),('G',947),('G',948),('G',949),('G',950),('G',951),('G',952),('G',953),('G',954),('G',955),('G',956),('G',957),('G',958),('G',959),('G',960),('G',961),('G',962),('G',963),('G',964),('G',965),('G',966),('G',967),('G',968),('G',969),('G',970),('G',971),('G',972),('G',973),('G',974),('G',975),('G',976),('G',977),('G',978),('G',979),('G',980),('G',981),('G',982),('G',983),('G',984),('G',985),('G',986),('G',987),('G',988),('G',989),('G',990),('G',991),('G',992),('G',993),('G',994),('G',995),('G',996)\", sql.get(0));\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('G',997),('G',998),('G',999)\", sql.get(1));\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167  ",
            "  @Test\n  public void testSQLGenerator() throws Exception {\n    //teseted on Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production\n    TxnHandler.SQLGenerator sqlGenerator =\n      new TxnHandler.SQLGenerator(TxnHandler.DatabaseProduct.ORACLE, conf);\n    List<String> rows = new ArrayList<>();\n    rows.add(\"'yellow', 1\");\n    List<String> sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1)\", sql.get(0));\n    rows.add(\"'red', 2\");\n    rows.add(\"'orange', 3\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    \n    Assert.assertEquals(\"Wrong stmt\", \n      \"insert all into colors(name, category) values('yellow', 1) into colors(name, category) values('red', 2) into colors(name, category) values('orange', 3) select * from dual\", sql.get(0));\n    for(int i = 0; i < conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE); i++) {\n      rows.add(\"\\'G\\',\" + i);\n    }\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 2, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert all into colors(name, category) values('yellow', 1) into colors(name, category) values('red', 2) into colors(name, category) values('orange', 3) into colors(name, category) values('G',0) into colors(name, category) values('G',1) into colors(name, category) values('G',2) into colors(name, category) values('G',3) into colors(name, category) values('G',4) into colors(name, category) values('G',5) into colors(name, category) values('G',6) into colors(name, category) values('G',7) into colors(name, category) values('G',8) into colors(name, category) values('G',9) into colors(name, category) values('G',10) into colors(name, category) values('G',11) into colors(name, category) values('G',12) into colors(name, category) values('G',13) into colors(name, category) values('G',14) into colors(name, category) values('G',15) into colors(name, category) values('G',16) into colors(name, category) values('G',17) into colors(name, category) values('G',18) into colors(name, category) values('G',19) into colors(name, category) values('G',20) into colors(name, category) values('G',21) into colors(name, category) values('G',22) into colors(name, category) values('G',23) into colors(name, category) values('G',24) into colors(name, category) values('G',25) into colors(name, category) values('G',26) into colors(name, category) values('G',27) into colors(name, category) values('G',28) into colors(name, category) values('G',29) into colors(name, category) values('G',30) into colors(name, category) values('G',31) into colors(name, category) values('G',32) into colors(name, category) values('G',33) into colors(name, category) values('G',34) into colors(name, category) values('G',35) into colors(name, category) values('G',36) into colors(name, category) values('G',37) into colors(name, category) values('G',38) into colors(name, category) values('G',39) into colors(name, category) values('G',40) into colors(name, category) values('G',41) into colors(name, category) values('G',42) into colors(name, category) values('G',43) into colors(name, category) values('G',44) into colors(name, category) values('G',45) into colors(name, category) values('G',46) into colors(name, category) values('G',47) into colors(name, category) values('G',48) into colors(name, category) values('G',49) into colors(name, category) values('G',50) into colors(name, category) values('G',51) into colors(name, category) values('G',52) into colors(name, category) values('G',53) into colors(name, category) values('G',54) into colors(name, category) values('G',55) into colors(name, category) values('G',56) into colors(name, category) values('G',57) into colors(name, category) values('G',58) into colors(name, category) values('G',59) into colors(name, category) values('G',60) into colors(name, category) values('G',61) into colors(name, category) values('G',62) into colors(name, category) values('G',63) into colors(name, category) values('G',64) into colors(name, category) values('G',65) into colors(name, category) values('G',66) into colors(name, category) values('G',67) into colors(name, category) values('G',68) into colors(name, category) values('G',69) into colors(name, category) values('G',70) into colors(name, category) values('G',71) into colors(name, category) values('G',72) into colors(name, category) values('G',73) into colors(name, category) values('G',74) into colors(name, category) values('G',75) into colors(name, category) values('G',76) into colors(name, category) values('G',77) into colors(name, category) values('G',78) into colors(name, category) values('G',79) into colors(name, category) values('G',80) into colors(name, category) values('G',81) into colors(name, category) values('G',82) into colors(name, category) values('G',83) into colors(name, category) values('G',84) into colors(name, category) values('G',85) into colors(name, category) values('G',86) into colors(name, category) values('G',87) into colors(name, category) values('G',88) into colors(name, category) values('G',89) into colors(name, category) values('G',90) into colors(name, category) values('G',91) into colors(name, category) values('G',92) into colors(name, category) values('G',93) into colors(name, category) values('G',94) into colors(name, category) values('G',95) into colors(name, category) values('G',96) into colors(name, category) values('G',97) into colors(name, category) values('G',98) into colors(name, category) values('G',99) into colors(name, category) values('G',100) into colors(name, category) values('G',101) into colors(name, category) values('G',102) into colors(name, category) values('G',103) into colors(name, category) values('G',104) into colors(name, category) values('G',105) into colors(name, category) values('G',106) into colors(name, category) values('G',107) into colors(name, category) values('G',108) into colors(name, category) values('G',109) into colors(name, category) values('G',110) into colors(name, category) values('G',111) into colors(name, category) values('G',112) into colors(name, category) values('G',113) into colors(name, category) values('G',114) into colors(name, category) values('G',115) into colors(name, category) values('G',116) into colors(name, category) values('G',117) into colors(name, category) values('G',118) into colors(name, category) values('G',119) into colors(name, category) values('G',120) into colors(name, category) values('G',121) into colors(name, category) values('G',122) into colors(name, category) values('G',123) into colors(name, category) values('G',124) into colors(name, category) values('G',125) into colors(name, category) values('G',126) into colors(name, category) values('G',127) into colors(name, category) values('G',128) into colors(name, category) values('G',129) into colors(name, category) values('G',130) into colors(name, category) values('G',131) into colors(name, category) values('G',132) into colors(name, category) values('G',133) into colors(name, category) values('G',134) into colors(name, category) values('G',135) into colors(name, category) values('G',136) into colors(name, category) values('G',137) into colors(name, category) values('G',138) into colors(name, category) values('G',139) into colors(name, category) values('G',140) into colors(name, category) values('G',141) into colors(name, category) values('G',142) into colors(name, category) values('G',143) into colors(name, category) values('G',144) into colors(name, category) values('G',145) into colors(name, category) values('G',146) into colors(name, category) values('G',147) into colors(name, category) values('G',148) into colors(name, category) values('G',149) into colors(name, category) values('G',150) into colors(name, category) values('G',151) into colors(name, category) values('G',152) into colors(name, category) values('G',153) into colors(name, category) values('G',154) into colors(name, category) values('G',155) into colors(name, category) values('G',156) into colors(name, category) values('G',157) into colors(name, category) values('G',158) into colors(name, category) values('G',159) into colors(name, category) values('G',160) into colors(name, category) values('G',161) into colors(name, category) values('G',162) into colors(name, category) values('G',163) into colors(name, category) values('G',164) into colors(name, category) values('G',165) into colors(name, category) values('G',166) into colors(name, category) values('G',167) into colors(name, category) values('G',168) into colors(name, category) values('G',169) into colors(name, category) values('G',170) into colors(name, category) values('G',171) into colors(name, category) values('G',172) into colors(name, category) values('G',173) into colors(name, category) values('G',174) into colors(name, category) values('G',175) into colors(name, category) values('G',176) into colors(name, category) values('G',177) into colors(name, category) values('G',178) into colors(name, category) values('G',179) into colors(name, category) values('G',180) into colors(name, category) values('G',181) into colors(name, category) values('G',182) into colors(name, category) values('G',183) into colors(name, category) values('G',184) into colors(name, category) values('G',185) into colors(name, category) values('G',186) into colors(name, category) values('G',187) into colors(name, category) values('G',188) into colors(name, category) values('G',189) into colors(name, category) values('G',190) into colors(name, category) values('G',191) into colors(name, category) values('G',192) into colors(name, category) values('G',193) into colors(name, category) values('G',194) into colors(name, category) values('G',195) into colors(name, category) values('G',196) into colors(name, category) values('G',197) into colors(name, category) values('G',198) into colors(name, category) values('G',199) into colors(name, category) values('G',200) into colors(name, category) values('G',201) into colors(name, category) values('G',202) into colors(name, category) values('G',203) into colors(name, category) values('G',204) into colors(name, category) values('G',205) into colors(name, category) values('G',206) into colors(name, category) values('G',207) into colors(name, category) values('G',208) into colors(name, category) values('G',209) into colors(name, category) values('G',210) into colors(name, category) values('G',211) into colors(name, category) values('G',212) into colors(name, category) values('G',213) into colors(name, category) values('G',214) into colors(name, category) values('G',215) into colors(name, category) values('G',216) into colors(name, category) values('G',217) into colors(name, category) values('G',218) into colors(name, category) values('G',219) into colors(name, category) values('G',220) into colors(name, category) values('G',221) into colors(name, category) values('G',222) into colors(name, category) values('G',223) into colors(name, category) values('G',224) into colors(name, category) values('G',225) into colors(name, category) values('G',226) into colors(name, category) values('G',227) into colors(name, category) values('G',228) into colors(name, category) values('G',229) into colors(name, category) values('G',230) into colors(name, category) values('G',231) into colors(name, category) values('G',232) into colors(name, category) values('G',233) into colors(name, category) values('G',234) into colors(name, category) values('G',235) into colors(name, category) values('G',236) into colors(name, category) values('G',237) into colors(name, category) values('G',238) into colors(name, category) values('G',239) into colors(name, category) values('G',240) into colors(name, category) values('G',241) into colors(name, category) values('G',242) into colors(name, category) values('G',243) into colors(name, category) values('G',244) into colors(name, category) values('G',245) into colors(name, category) values('G',246) into colors(name, category) values('G',247) into colors(name, category) values('G',248) into colors(name, category) values('G',249) into colors(name, category) values('G',250) into colors(name, category) values('G',251) into colors(name, category) values('G',252) into colors(name, category) values('G',253) into colors(name, category) values('G',254) into colors(name, category) values('G',255) into colors(name, category) values('G',256) into colors(name, category) values('G',257) into colors(name, category) values('G',258) into colors(name, category) values('G',259) into colors(name, category) values('G',260) into colors(name, category) values('G',261) into colors(name, category) values('G',262) into colors(name, category) values('G',263) into colors(name, category) values('G',264) into colors(name, category) values('G',265) into colors(name, category) values('G',266) into colors(name, category) values('G',267) into colors(name, category) values('G',268) into colors(name, category) values('G',269) into colors(name, category) values('G',270) into colors(name, category) values('G',271) into colors(name, category) values('G',272) into colors(name, category) values('G',273) into colors(name, category) values('G',274) into colors(name, category) values('G',275) into colors(name, category) values('G',276) into colors(name, category) values('G',277) into colors(name, category) values('G',278) into colors(name, category) values('G',279) into colors(name, category) values('G',280) into colors(name, category) values('G',281) into colors(name, category) values('G',282) into colors(name, category) values('G',283) into colors(name, category) values('G',284) into colors(name, category) values('G',285) into colors(name, category) values('G',286) into colors(name, category) values('G',287) into colors(name, category) values('G',288) into colors(name, category) values('G',289) into colors(name, category) values('G',290) into colors(name, category) values('G',291) into colors(name, category) values('G',292) into colors(name, category) values('G',293) into colors(name, category) values('G',294) into colors(name, category) values('G',295) into colors(name, category) values('G',296) into colors(name, category) values('G',297) into colors(name, category) values('G',298) into colors(name, category) values('G',299) into colors(name, category) values('G',300) into colors(name, category) values('G',301) into colors(name, category) values('G',302) into colors(name, category) values('G',303) into colors(name, category) values('G',304) into colors(name, category) values('G',305) into colors(name, category) values('G',306) into colors(name, category) values('G',307) into colors(name, category) values('G',308) into colors(name, category) values('G',309) into colors(name, category) values('G',310) into colors(name, category) values('G',311) into colors(name, category) values('G',312) into colors(name, category) values('G',313) into colors(name, category) values('G',314) into colors(name, category) values('G',315) into colors(name, category) values('G',316) into colors(name, category) values('G',317) into colors(name, category) values('G',318) into colors(name, category) values('G',319) into colors(name, category) values('G',320) into colors(name, category) values('G',321) into colors(name, category) values('G',322) into colors(name, category) values('G',323) into colors(name, category) values('G',324) into colors(name, category) values('G',325) into colors(name, category) values('G',326) into colors(name, category) values('G',327) into colors(name, category) values('G',328) into colors(name, category) values('G',329) into colors(name, category) values('G',330) into colors(name, category) values('G',331) into colors(name, category) values('G',332) into colors(name, category) values('G',333) into colors(name, category) values('G',334) into colors(name, category) values('G',335) into colors(name, category) values('G',336) into colors(name, category) values('G',337) into colors(name, category) values('G',338) into colors(name, category) values('G',339) into colors(name, category) values('G',340) into colors(name, category) values('G',341) into colors(name, category) values('G',342) into colors(name, category) values('G',343) into colors(name, category) values('G',344) into colors(name, category) values('G',345) into colors(name, category) values('G',346) into colors(name, category) values('G',347) into colors(name, category) values('G',348) into colors(name, category) values('G',349) into colors(name, category) values('G',350) into colors(name, category) values('G',351) into colors(name, category) values('G',352) into colors(name, category) values('G',353) into colors(name, category) values('G',354) into colors(name, category) values('G',355) into colors(name, category) values('G',356) into colors(name, category) values('G',357) into colors(name, category) values('G',358) into colors(name, category) values('G',359) into colors(name, category) values('G',360) into colors(name, category) values('G',361) into colors(name, category) values('G',362) into colors(name, category) values('G',363) into colors(name, category) values('G',364) into colors(name, category) values('G',365) into colors(name, category) values('G',366) into colors(name, category) values('G',367) into colors(name, category) values('G',368) into colors(name, category) values('G',369) into colors(name, category) values('G',370) into colors(name, category) values('G',371) into colors(name, category) values('G',372) into colors(name, category) values('G',373) into colors(name, category) values('G',374) into colors(name, category) values('G',375) into colors(name, category) values('G',376) into colors(name, category) values('G',377) into colors(name, category) values('G',378) into colors(name, category) values('G',379) into colors(name, category) values('G',380) into colors(name, category) values('G',381) into colors(name, category) values('G',382) into colors(name, category) values('G',383) into colors(name, category) values('G',384) into colors(name, category) values('G',385) into colors(name, category) values('G',386) into colors(name, category) values('G',387) into colors(name, category) values('G',388) into colors(name, category) values('G',389) into colors(name, category) values('G',390) into colors(name, category) values('G',391) into colors(name, category) values('G',392) into colors(name, category) values('G',393) into colors(name, category) values('G',394) into colors(name, category) values('G',395) into colors(name, category) values('G',396) into colors(name, category) values('G',397) into colors(name, category) values('G',398) into colors(name, category) values('G',399) into colors(name, category) values('G',400) into colors(name, category) values('G',401) into colors(name, category) values('G',402) into colors(name, category) values('G',403) into colors(name, category) values('G',404) into colors(name, category) values('G',405) into colors(name, category) values('G',406) into colors(name, category) values('G',407) into colors(name, category) values('G',408) into colors(name, category) values('G',409) into colors(name, category) values('G',410) into colors(name, category) values('G',411) into colors(name, category) values('G',412) into colors(name, category) values('G',413) into colors(name, category) values('G',414) into colors(name, category) values('G',415) into colors(name, category) values('G',416) into colors(name, category) values('G',417) into colors(name, category) values('G',418) into colors(name, category) values('G',419) into colors(name, category) values('G',420) into colors(name, category) values('G',421) into colors(name, category) values('G',422) into colors(name, category) values('G',423) into colors(name, category) values('G',424) into colors(name, category) values('G',425) into colors(name, category) values('G',426) into colors(name, category) values('G',427) into colors(name, category) values('G',428) into colors(name, category) values('G',429) into colors(name, category) values('G',430) into colors(name, category) values('G',431) into colors(name, category) values('G',432) into colors(name, category) values('G',433) into colors(name, category) values('G',434) into colors(name, category) values('G',435) into colors(name, category) values('G',436) into colors(name, category) values('G',437) into colors(name, category) values('G',438) into colors(name, category) values('G',439) into colors(name, category) values('G',440) into colors(name, category) values('G',441) into colors(name, category) values('G',442) into colors(name, category) values('G',443) into colors(name, category) values('G',444) into colors(name, category) values('G',445) into colors(name, category) values('G',446) into colors(name, category) values('G',447) into colors(name, category) values('G',448) into colors(name, category) values('G',449) into colors(name, category) values('G',450) into colors(name, category) values('G',451) into colors(name, category) values('G',452) into colors(name, category) values('G',453) into colors(name, category) values('G',454) into colors(name, category) values('G',455) into colors(name, category) values('G',456) into colors(name, category) values('G',457) into colors(name, category) values('G',458) into colors(name, category) values('G',459) into colors(name, category) values('G',460) into colors(name, category) values('G',461) into colors(name, category) values('G',462) into colors(name, category) values('G',463) into colors(name, category) values('G',464) into colors(name, category) values('G',465) into colors(name, category) values('G',466) into colors(name, category) values('G',467) into colors(name, category) values('G',468) into colors(name, category) values('G',469) into colors(name, category) values('G',470) into colors(name, category) values('G',471) into colors(name, category) values('G',472) into colors(name, category) values('G',473) into colors(name, category) values('G',474) into colors(name, category) values('G',475) into colors(name, category) values('G',476) into colors(name, category) values('G',477) into colors(name, category) values('G',478) into colors(name, category) values('G',479) into colors(name, category) values('G',480) into colors(name, category) values('G',481) into colors(name, category) values('G',482) into colors(name, category) values('G',483) into colors(name, category) values('G',484) into colors(name, category) values('G',485) into colors(name, category) values('G',486) into colors(name, category) values('G',487) into colors(name, category) values('G',488) into colors(name, category) values('G',489) into colors(name, category) values('G',490) into colors(name, category) values('G',491) into colors(name, category) values('G',492) into colors(name, category) values('G',493) into colors(name, category) values('G',494) into colors(name, category) values('G',495) into colors(name, category) values('G',496) into colors(name, category) values('G',497) into colors(name, category) values('G',498) into colors(name, category) values('G',499) into colors(name, category) values('G',500) into colors(name, category) values('G',501) into colors(name, category) values('G',502) into colors(name, category) values('G',503) into colors(name, category) values('G',504) into colors(name, category) values('G',505) into colors(name, category) values('G',506) into colors(name, category) values('G',507) into colors(name, category) values('G',508) into colors(name, category) values('G',509) into colors(name, category) values('G',510) into colors(name, category) values('G',511) into colors(name, category) values('G',512) into colors(name, category) values('G',513) into colors(name, category) values('G',514) into colors(name, category) values('G',515) into colors(name, category) values('G',516) into colors(name, category) values('G',517) into colors(name, category) values('G',518) into colors(name, category) values('G',519) into colors(name, category) values('G',520) into colors(name, category) values('G',521) into colors(name, category) values('G',522) into colors(name, category) values('G',523) into colors(name, category) values('G',524) into colors(name, category) values('G',525) into colors(name, category) values('G',526) into colors(name, category) values('G',527) into colors(name, category) values('G',528) into colors(name, category) values('G',529) into colors(name, category) values('G',530) into colors(name, category) values('G',531) into colors(name, category) values('G',532) into colors(name, category) values('G',533) into colors(name, category) values('G',534) into colors(name, category) values('G',535) into colors(name, category) values('G',536) into colors(name, category) values('G',537) into colors(name, category) values('G',538) into colors(name, category) values('G',539) into colors(name, category) values('G',540) into colors(name, category) values('G',541) into colors(name, category) values('G',542) into colors(name, category) values('G',543) into colors(name, category) values('G',544) into colors(name, category) values('G',545) into colors(name, category) values('G',546) into colors(name, category) values('G',547) into colors(name, category) values('G',548) into colors(name, category) values('G',549) into colors(name, category) values('G',550) into colors(name, category) values('G',551) into colors(name, category) values('G',552) into colors(name, category) values('G',553) into colors(name, category) values('G',554) into colors(name, category) values('G',555) into colors(name, category) values('G',556) into colors(name, category) values('G',557) into colors(name, category) values('G',558) into colors(name, category) values('G',559) into colors(name, category) values('G',560) into colors(name, category) values('G',561) into colors(name, category) values('G',562) into colors(name, category) values('G',563) into colors(name, category) values('G',564) into colors(name, category) values('G',565) into colors(name, category) values('G',566) into colors(name, category) values('G',567) into colors(name, category) values('G',568) into colors(name, category) values('G',569) into colors(name, category) values('G',570) into colors(name, category) values('G',571) into colors(name, category) values('G',572) into colors(name, category) values('G',573) into colors(name, category) values('G',574) into colors(name, category) values('G',575) into colors(name, category) values('G',576) into colors(name, category) values('G',577) into colors(name, category) values('G',578) into colors(name, category) values('G',579) into colors(name, category) values('G',580) into colors(name, category) values('G',581) into colors(name, category) values('G',582) into colors(name, category) values('G',583) into colors(name, category) values('G',584) into colors(name, category) values('G',585) into colors(name, category) values('G',586) into colors(name, category) values('G',587) into colors(name, category) values('G',588) into colors(name, category) values('G',589) into colors(name, category) values('G',590) into colors(name, category) values('G',591) into colors(name, category) values('G',592) into colors(name, category) values('G',593) into colors(name, category) values('G',594) into colors(name, category) values('G',595) into colors(name, category) values('G',596) into colors(name, category) values('G',597) into colors(name, category) values('G',598) into colors(name, category) values('G',599) into colors(name, category) values('G',600) into colors(name, category) values('G',601) into colors(name, category) values('G',602) into colors(name, category) values('G',603) into colors(name, category) values('G',604) into colors(name, category) values('G',605) into colors(name, category) values('G',606) into colors(name, category) values('G',607) into colors(name, category) values('G',608) into colors(name, category) values('G',609) into colors(name, category) values('G',610) into colors(name, category) values('G',611) into colors(name, category) values('G',612) into colors(name, category) values('G',613) into colors(name, category) values('G',614) into colors(name, category) values('G',615) into colors(name, category) values('G',616) into colors(name, category) values('G',617) into colors(name, category) values('G',618) into colors(name, category) values('G',619) into colors(name, category) values('G',620) into colors(name, category) values('G',621) into colors(name, category) values('G',622) into colors(name, category) values('G',623) into colors(name, category) values('G',624) into colors(name, category) values('G',625) into colors(name, category) values('G',626) into colors(name, category) values('G',627) into colors(name, category) values('G',628) into colors(name, category) values('G',629) into colors(name, category) values('G',630) into colors(name, category) values('G',631) into colors(name, category) values('G',632) into colors(name, category) values('G',633) into colors(name, category) values('G',634) into colors(name, category) values('G',635) into colors(name, category) values('G',636) into colors(name, category) values('G',637) into colors(name, category) values('G',638) into colors(name, category) values('G',639) into colors(name, category) values('G',640) into colors(name, category) values('G',641) into colors(name, category) values('G',642) into colors(name, category) values('G',643) into colors(name, category) values('G',644) into colors(name, category) values('G',645) into colors(name, category) values('G',646) into colors(name, category) values('G',647) into colors(name, category) values('G',648) into colors(name, category) values('G',649) into colors(name, category) values('G',650) into colors(name, category) values('G',651) into colors(name, category) values('G',652) into colors(name, category) values('G',653) into colors(name, category) values('G',654) into colors(name, category) values('G',655) into colors(name, category) values('G',656) into colors(name, category) values('G',657) into colors(name, category) values('G',658) into colors(name, category) values('G',659) into colors(name, category) values('G',660) into colors(name, category) values('G',661) into colors(name, category) values('G',662) into colors(name, category) values('G',663) into colors(name, category) values('G',664) into colors(name, category) values('G',665) into colors(name, category) values('G',666) into colors(name, category) values('G',667) into colors(name, category) values('G',668) into colors(name, category) values('G',669) into colors(name, category) values('G',670) into colors(name, category) values('G',671) into colors(name, category) values('G',672) into colors(name, category) values('G',673) into colors(name, category) values('G',674) into colors(name, category) values('G',675) into colors(name, category) values('G',676) into colors(name, category) values('G',677) into colors(name, category) values('G',678) into colors(name, category) values('G',679) into colors(name, category) values('G',680) into colors(name, category) values('G',681) into colors(name, category) values('G',682) into colors(name, category) values('G',683) into colors(name, category) values('G',684) into colors(name, category) values('G',685) into colors(name, category) values('G',686) into colors(name, category) values('G',687) into colors(name, category) values('G',688) into colors(name, category) values('G',689) into colors(name, category) values('G',690) into colors(name, category) values('G',691) into colors(name, category) values('G',692) into colors(name, category) values('G',693) into colors(name, category) values('G',694) into colors(name, category) values('G',695) into colors(name, category) values('G',696) into colors(name, category) values('G',697) into colors(name, category) values('G',698) into colors(name, category) values('G',699) into colors(name, category) values('G',700) into colors(name, category) values('G',701) into colors(name, category) values('G',702) into colors(name, category) values('G',703) into colors(name, category) values('G',704) into colors(name, category) values('G',705) into colors(name, category) values('G',706) into colors(name, category) values('G',707) into colors(name, category) values('G',708) into colors(name, category) values('G',709) into colors(name, category) values('G',710) into colors(name, category) values('G',711) into colors(name, category) values('G',712) into colors(name, category) values('G',713) into colors(name, category) values('G',714) into colors(name, category) values('G',715) into colors(name, category) values('G',716) into colors(name, category) values('G',717) into colors(name, category) values('G',718) into colors(name, category) values('G',719) into colors(name, category) values('G',720) into colors(name, category) values('G',721) into colors(name, category) values('G',722) into colors(name, category) values('G',723) into colors(name, category) values('G',724) into colors(name, category) values('G',725) into colors(name, category) values('G',726) into colors(name, category) values('G',727) into colors(name, category) values('G',728) into colors(name, category) values('G',729) into colors(name, category) values('G',730) into colors(name, category) values('G',731) into colors(name, category) values('G',732) into colors(name, category) values('G',733) into colors(name, category) values('G',734) into colors(name, category) values('G',735) into colors(name, category) values('G',736) into colors(name, category) values('G',737) into colors(name, category) values('G',738) into colors(name, category) values('G',739) into colors(name, category) values('G',740) into colors(name, category) values('G',741) into colors(name, category) values('G',742) into colors(name, category) values('G',743) into colors(name, category) values('G',744) into colors(name, category) values('G',745) into colors(name, category) values('G',746) into colors(name, category) values('G',747) into colors(name, category) values('G',748) into colors(name, category) values('G',749) into colors(name, category) values('G',750) into colors(name, category) values('G',751) into colors(name, category) values('G',752) into colors(name, category) values('G',753) into colors(name, category) values('G',754) into colors(name, category) values('G',755) into colors(name, category) values('G',756) into colors(name, category) values('G',757) into colors(name, category) values('G',758) into colors(name, category) values('G',759) into colors(name, category) values('G',760) into colors(name, category) values('G',761) into colors(name, category) values('G',762) into colors(name, category) values('G',763) into colors(name, category) values('G',764) into colors(name, category) values('G',765) into colors(name, category) values('G',766) into colors(name, category) values('G',767) into colors(name, category) values('G',768) into colors(name, category) values('G',769) into colors(name, category) values('G',770) into colors(name, category) values('G',771) into colors(name, category) values('G',772) into colors(name, category) values('G',773) into colors(name, category) values('G',774) into colors(name, category) values('G',775) into colors(name, category) values('G',776) into colors(name, category) values('G',777) into colors(name, category) values('G',778) into colors(name, category) values('G',779) into colors(name, category) values('G',780) into colors(name, category) values('G',781) into colors(name, category) values('G',782) into colors(name, category) values('G',783) into colors(name, category) values('G',784) into colors(name, category) values('G',785) into colors(name, category) values('G',786) into colors(name, category) values('G',787) into colors(name, category) values('G',788) into colors(name, category) values('G',789) into colors(name, category) values('G',790) into colors(name, category) values('G',791) into colors(name, category) values('G',792) into colors(name, category) values('G',793) into colors(name, category) values('G',794) into colors(name, category) values('G',795) into colors(name, category) values('G',796) into colors(name, category) values('G',797) into colors(name, category) values('G',798) into colors(name, category) values('G',799) into colors(name, category) values('G',800) into colors(name, category) values('G',801) into colors(name, category) values('G',802) into colors(name, category) values('G',803) into colors(name, category) values('G',804) into colors(name, category) values('G',805) into colors(name, category) values('G',806) into colors(name, category) values('G',807) into colors(name, category) values('G',808) into colors(name, category) values('G',809) into colors(name, category) values('G',810) into colors(name, category) values('G',811) into colors(name, category) values('G',812) into colors(name, category) values('G',813) into colors(name, category) values('G',814) into colors(name, category) values('G',815) into colors(name, category) values('G',816) into colors(name, category) values('G',817) into colors(name, category) values('G',818) into colors(name, category) values('G',819) into colors(name, category) values('G',820) into colors(name, category) values('G',821) into colors(name, category) values('G',822) into colors(name, category) values('G',823) into colors(name, category) values('G',824) into colors(name, category) values('G',825) into colors(name, category) values('G',826) into colors(name, category) values('G',827) into colors(name, category) values('G',828) into colors(name, category) values('G',829) into colors(name, category) values('G',830) into colors(name, category) values('G',831) into colors(name, category) values('G',832) into colors(name, category) values('G',833) into colors(name, category) values('G',834) into colors(name, category) values('G',835) into colors(name, category) values('G',836) into colors(name, category) values('G',837) into colors(name, category) values('G',838) into colors(name, category) values('G',839) into colors(name, category) values('G',840) into colors(name, category) values('G',841) into colors(name, category) values('G',842) into colors(name, category) values('G',843) into colors(name, category) values('G',844) into colors(name, category) values('G',845) into colors(name, category) values('G',846) into colors(name, category) values('G',847) into colors(name, category) values('G',848) into colors(name, category) values('G',849) into colors(name, category) values('G',850) into colors(name, category) values('G',851) into colors(name, category) values('G',852) into colors(name, category) values('G',853) into colors(name, category) values('G',854) into colors(name, category) values('G',855) into colors(name, category) values('G',856) into colors(name, category) values('G',857) into colors(name, category) values('G',858) into colors(name, category) values('G',859) into colors(name, category) values('G',860) into colors(name, category) values('G',861) into colors(name, category) values('G',862) into colors(name, category) values('G',863) into colors(name, category) values('G',864) into colors(name, category) values('G',865) into colors(name, category) values('G',866) into colors(name, category) values('G',867) into colors(name, category) values('G',868) into colors(name, category) values('G',869) into colors(name, category) values('G',870) into colors(name, category) values('G',871) into colors(name, category) values('G',872) into colors(name, category) values('G',873) into colors(name, category) values('G',874) into colors(name, category) values('G',875) into colors(name, category) values('G',876) into colors(name, category) values('G',877) into colors(name, category) values('G',878) into colors(name, category) values('G',879) into colors(name, category) values('G',880) into colors(name, category) values('G',881) into colors(name, category) values('G',882) into colors(name, category) values('G',883) into colors(name, category) values('G',884) into colors(name, category) values('G',885) into colors(name, category) values('G',886) into colors(name, category) values('G',887) into colors(name, category) values('G',888) into colors(name, category) values('G',889) into colors(name, category) values('G',890) into colors(name, category) values('G',891) into colors(name, category) values('G',892) into colors(name, category) values('G',893) into colors(name, category) values('G',894) into colors(name, category) values('G',895) into colors(name, category) values('G',896) into colors(name, category) values('G',897) into colors(name, category) values('G',898) into colors(name, category) values('G',899) into colors(name, category) values('G',900) into colors(name, category) values('G',901) into colors(name, category) values('G',902) into colors(name, category) values('G',903) into colors(name, category) values('G',904) into colors(name, category) values('G',905) into colors(name, category) values('G',906) into colors(name, category) values('G',907) into colors(name, category) values('G',908) into colors(name, category) values('G',909) into colors(name, category) values('G',910) into colors(name, category) values('G',911) into colors(name, category) values('G',912) into colors(name, category) values('G',913) into colors(name, category) values('G',914) into colors(name, category) values('G',915) into colors(name, category) values('G',916) into colors(name, category) values('G',917) into colors(name, category) values('G',918) into colors(name, category) values('G',919) into colors(name, category) values('G',920) into colors(name, category) values('G',921) into colors(name, category) values('G',922) into colors(name, category) values('G',923) into colors(name, category) values('G',924) into colors(name, category) values('G',925) into colors(name, category) values('G',926) into colors(name, category) values('G',927) into colors(name, category) values('G',928) into colors(name, category) values('G',929) into colors(name, category) values('G',930) into colors(name, category) values('G',931) into colors(name, category) values('G',932) into colors(name, category) values('G',933) into colors(name, category) values('G',934) into colors(name, category) values('G',935) into colors(name, category) values('G',936) into colors(name, category) values('G',937) into colors(name, category) values('G',938) into colors(name, category) values('G',939) into colors(name, category) values('G',940) into colors(name, category) values('G',941) into colors(name, category) values('G',942) into colors(name, category) values('G',943) into colors(name, category) values('G',944) into colors(name, category) values('G',945) into colors(name, category) values('G',946) into colors(name, category) values('G',947) into colors(name, category) values('G',948) into colors(name, category) values('G',949) into colors(name, category) values('G',950) into colors(name, category) values('G',951) into colors(name, category) values('G',952) into colors(name, category) values('G',953) into colors(name, category) values('G',954) into colors(name, category) values('G',955) into colors(name, category) values('G',956) into colors(name, category) values('G',957) into colors(name, category) values('G',958) into colors(name, category) values('G',959) into colors(name, category) values('G',960) into colors(name, category) values('G',961) into colors(name, category) values('G',962) into colors(name, category) values('G',963) into colors(name, category) values('G',964) into colors(name, category) values('G',965) into colors(name, category) values('G',966) into colors(name, category) values('G',967) into colors(name, category) values('G',968) into colors(name, category) values('G',969) into colors(name, category) values('G',970) into colors(name, category) values('G',971) into colors(name, category) values('G',972) into colors(name, category) values('G',973) into colors(name, category) values('G',974) into colors(name, category) values('G',975) into colors(name, category) values('G',976) into colors(name, category) values('G',977) into colors(name, category) values('G',978) into colors(name, category) values('G',979) into colors(name, category) values('G',980) into colors(name, category) values('G',981) into colors(name, category) values('G',982) into colors(name, category) values('G',983) into colors(name, category) values('G',984) into colors(name, category) values('G',985) into colors(name, category) values('G',986) into colors(name, category) values('G',987) into colors(name, category) values('G',988) into colors(name, category) values('G',989) into colors(name, category) values('G',990) into colors(name, category) values('G',991) into colors(name, category) values('G',992) into colors(name, category) values('G',993) into colors(name, category) values('G',994) into colors(name, category) values('G',995) into colors(name, category) values('G',996)  select * from dual\", sql.get(0));\n    Assert.assertEquals(\"Wrong stmt\", \"insert all into colors(name, category) values('G',997) into colors(name, category) values('G',998) into colors(name, category) values('G',999) select * from dual\", sql.get(1));\n    \n    sqlGenerator =\n      new TxnHandler.SQLGenerator(TxnHandler.DatabaseProduct.MYSQL, conf);\n    rows.clear();\n    rows.add(\"'yellow', 1\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1)\", sql.get(0));\n    rows.add(\"'red', 2\");\n    rows.add(\"'orange', 3\");\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 1, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1),('red', 2),('orange', 3)\", sql.get(0));\n    for(int i = 0; i < conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE); i++) {\n      rows.add(\"\\'G\\',\" + i);\n    }\n    sql = sqlGenerator.createInsertValuesStmt(\"colors(name, category)\", rows);\n    Assert.assertEquals(\"Number of stmts\", 2, sql.size());\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('yellow', 1),('red', 2),('orange', 3),('G',0),('G',1),('G',2),('G',3),('G',4),('G',5),('G',6),('G',7),('G',8),('G',9),('G',10),('G',11),('G',12),('G',13),('G',14),('G',15),('G',16),('G',17),('G',18),('G',19),('G',20),('G',21),('G',22),('G',23),('G',24),('G',25),('G',26),('G',27),('G',28),('G',29),('G',30),('G',31),('G',32),('G',33),('G',34),('G',35),('G',36),('G',37),('G',38),('G',39),('G',40),('G',41),('G',42),('G',43),('G',44),('G',45),('G',46),('G',47),('G',48),('G',49),('G',50),('G',51),('G',52),('G',53),('G',54),('G',55),('G',56),('G',57),('G',58),('G',59),('G',60),('G',61),('G',62),('G',63),('G',64),('G',65),('G',66),('G',67),('G',68),('G',69),('G',70),('G',71),('G',72),('G',73),('G',74),('G',75),('G',76),('G',77),('G',78),('G',79),('G',80),('G',81),('G',82),('G',83),('G',84),('G',85),('G',86),('G',87),('G',88),('G',89),('G',90),('G',91),('G',92),('G',93),('G',94),('G',95),('G',96),('G',97),('G',98),('G',99),('G',100),('G',101),('G',102),('G',103),('G',104),('G',105),('G',106),('G',107),('G',108),('G',109),('G',110),('G',111),('G',112),('G',113),('G',114),('G',115),('G',116),('G',117),('G',118),('G',119),('G',120),('G',121),('G',122),('G',123),('G',124),('G',125),('G',126),('G',127),('G',128),('G',129),('G',130),('G',131),('G',132),('G',133),('G',134),('G',135),('G',136),('G',137),('G',138),('G',139),('G',140),('G',141),('G',142),('G',143),('G',144),('G',145),('G',146),('G',147),('G',148),('G',149),('G',150),('G',151),('G',152),('G',153),('G',154),('G',155),('G',156),('G',157),('G',158),('G',159),('G',160),('G',161),('G',162),('G',163),('G',164),('G',165),('G',166),('G',167),('G',168),('G',169),('G',170),('G',171),('G',172),('G',173),('G',174),('G',175),('G',176),('G',177),('G',178),('G',179),('G',180),('G',181),('G',182),('G',183),('G',184),('G',185),('G',186),('G',187),('G',188),('G',189),('G',190),('G',191),('G',192),('G',193),('G',194),('G',195),('G',196),('G',197),('G',198),('G',199),('G',200),('G',201),('G',202),('G',203),('G',204),('G',205),('G',206),('G',207),('G',208),('G',209),('G',210),('G',211),('G',212),('G',213),('G',214),('G',215),('G',216),('G',217),('G',218),('G',219),('G',220),('G',221),('G',222),('G',223),('G',224),('G',225),('G',226),('G',227),('G',228),('G',229),('G',230),('G',231),('G',232),('G',233),('G',234),('G',235),('G',236),('G',237),('G',238),('G',239),('G',240),('G',241),('G',242),('G',243),('G',244),('G',245),('G',246),('G',247),('G',248),('G',249),('G',250),('G',251),('G',252),('G',253),('G',254),('G',255),('G',256),('G',257),('G',258),('G',259),('G',260),('G',261),('G',262),('G',263),('G',264),('G',265),('G',266),('G',267),('G',268),('G',269),('G',270),('G',271),('G',272),('G',273),('G',274),('G',275),('G',276),('G',277),('G',278),('G',279),('G',280),('G',281),('G',282),('G',283),('G',284),('G',285),('G',286),('G',287),('G',288),('G',289),('G',290),('G',291),('G',292),('G',293),('G',294),('G',295),('G',296),('G',297),('G',298),('G',299),('G',300),('G',301),('G',302),('G',303),('G',304),('G',305),('G',306),('G',307),('G',308),('G',309),('G',310),('G',311),('G',312),('G',313),('G',314),('G',315),('G',316),('G',317),('G',318),('G',319),('G',320),('G',321),('G',322),('G',323),('G',324),('G',325),('G',326),('G',327),('G',328),('G',329),('G',330),('G',331),('G',332),('G',333),('G',334),('G',335),('G',336),('G',337),('G',338),('G',339),('G',340),('G',341),('G',342),('G',343),('G',344),('G',345),('G',346),('G',347),('G',348),('G',349),('G',350),('G',351),('G',352),('G',353),('G',354),('G',355),('G',356),('G',357),('G',358),('G',359),('G',360),('G',361),('G',362),('G',363),('G',364),('G',365),('G',366),('G',367),('G',368),('G',369),('G',370),('G',371),('G',372),('G',373),('G',374),('G',375),('G',376),('G',377),('G',378),('G',379),('G',380),('G',381),('G',382),('G',383),('G',384),('G',385),('G',386),('G',387),('G',388),('G',389),('G',390),('G',391),('G',392),('G',393),('G',394),('G',395),('G',396),('G',397),('G',398),('G',399),('G',400),('G',401),('G',402),('G',403),('G',404),('G',405),('G',406),('G',407),('G',408),('G',409),('G',410),('G',411),('G',412),('G',413),('G',414),('G',415),('G',416),('G',417),('G',418),('G',419),('G',420),('G',421),('G',422),('G',423),('G',424),('G',425),('G',426),('G',427),('G',428),('G',429),('G',430),('G',431),('G',432),('G',433),('G',434),('G',435),('G',436),('G',437),('G',438),('G',439),('G',440),('G',441),('G',442),('G',443),('G',444),('G',445),('G',446),('G',447),('G',448),('G',449),('G',450),('G',451),('G',452),('G',453),('G',454),('G',455),('G',456),('G',457),('G',458),('G',459),('G',460),('G',461),('G',462),('G',463),('G',464),('G',465),('G',466),('G',467),('G',468),('G',469),('G',470),('G',471),('G',472),('G',473),('G',474),('G',475),('G',476),('G',477),('G',478),('G',479),('G',480),('G',481),('G',482),('G',483),('G',484),('G',485),('G',486),('G',487),('G',488),('G',489),('G',490),('G',491),('G',492),('G',493),('G',494),('G',495),('G',496),('G',497),('G',498),('G',499),('G',500),('G',501),('G',502),('G',503),('G',504),('G',505),('G',506),('G',507),('G',508),('G',509),('G',510),('G',511),('G',512),('G',513),('G',514),('G',515),('G',516),('G',517),('G',518),('G',519),('G',520),('G',521),('G',522),('G',523),('G',524),('G',525),('G',526),('G',527),('G',528),('G',529),('G',530),('G',531),('G',532),('G',533),('G',534),('G',535),('G',536),('G',537),('G',538),('G',539),('G',540),('G',541),('G',542),('G',543),('G',544),('G',545),('G',546),('G',547),('G',548),('G',549),('G',550),('G',551),('G',552),('G',553),('G',554),('G',555),('G',556),('G',557),('G',558),('G',559),('G',560),('G',561),('G',562),('G',563),('G',564),('G',565),('G',566),('G',567),('G',568),('G',569),('G',570),('G',571),('G',572),('G',573),('G',574),('G',575),('G',576),('G',577),('G',578),('G',579),('G',580),('G',581),('G',582),('G',583),('G',584),('G',585),('G',586),('G',587),('G',588),('G',589),('G',590),('G',591),('G',592),('G',593),('G',594),('G',595),('G',596),('G',597),('G',598),('G',599),('G',600),('G',601),('G',602),('G',603),('G',604),('G',605),('G',606),('G',607),('G',608),('G',609),('G',610),('G',611),('G',612),('G',613),('G',614),('G',615),('G',616),('G',617),('G',618),('G',619),('G',620),('G',621),('G',622),('G',623),('G',624),('G',625),('G',626),('G',627),('G',628),('G',629),('G',630),('G',631),('G',632),('G',633),('G',634),('G',635),('G',636),('G',637),('G',638),('G',639),('G',640),('G',641),('G',642),('G',643),('G',644),('G',645),('G',646),('G',647),('G',648),('G',649),('G',650),('G',651),('G',652),('G',653),('G',654),('G',655),('G',656),('G',657),('G',658),('G',659),('G',660),('G',661),('G',662),('G',663),('G',664),('G',665),('G',666),('G',667),('G',668),('G',669),('G',670),('G',671),('G',672),('G',673),('G',674),('G',675),('G',676),('G',677),('G',678),('G',679),('G',680),('G',681),('G',682),('G',683),('G',684),('G',685),('G',686),('G',687),('G',688),('G',689),('G',690),('G',691),('G',692),('G',693),('G',694),('G',695),('G',696),('G',697),('G',698),('G',699),('G',700),('G',701),('G',702),('G',703),('G',704),('G',705),('G',706),('G',707),('G',708),('G',709),('G',710),('G',711),('G',712),('G',713),('G',714),('G',715),('G',716),('G',717),('G',718),('G',719),('G',720),('G',721),('G',722),('G',723),('G',724),('G',725),('G',726),('G',727),('G',728),('G',729),('G',730),('G',731),('G',732),('G',733),('G',734),('G',735),('G',736),('G',737),('G',738),('G',739),('G',740),('G',741),('G',742),('G',743),('G',744),('G',745),('G',746),('G',747),('G',748),('G',749),('G',750),('G',751),('G',752),('G',753),('G',754),('G',755),('G',756),('G',757),('G',758),('G',759),('G',760),('G',761),('G',762),('G',763),('G',764),('G',765),('G',766),('G',767),('G',768),('G',769),('G',770),('G',771),('G',772),('G',773),('G',774),('G',775),('G',776),('G',777),('G',778),('G',779),('G',780),('G',781),('G',782),('G',783),('G',784),('G',785),('G',786),('G',787),('G',788),('G',789),('G',790),('G',791),('G',792),('G',793),('G',794),('G',795),('G',796),('G',797),('G',798),('G',799),('G',800),('G',801),('G',802),('G',803),('G',804),('G',805),('G',806),('G',807),('G',808),('G',809),('G',810),('G',811),('G',812),('G',813),('G',814),('G',815),('G',816),('G',817),('G',818),('G',819),('G',820),('G',821),('G',822),('G',823),('G',824),('G',825),('G',826),('G',827),('G',828),('G',829),('G',830),('G',831),('G',832),('G',833),('G',834),('G',835),('G',836),('G',837),('G',838),('G',839),('G',840),('G',841),('G',842),('G',843),('G',844),('G',845),('G',846),('G',847),('G',848),('G',849),('G',850),('G',851),('G',852),('G',853),('G',854),('G',855),('G',856),('G',857),('G',858),('G',859),('G',860),('G',861),('G',862),('G',863),('G',864),('G',865),('G',866),('G',867),('G',868),('G',869),('G',870),('G',871),('G',872),('G',873),('G',874),('G',875),('G',876),('G',877),('G',878),('G',879),('G',880),('G',881),('G',882),('G',883),('G',884),('G',885),('G',886),('G',887),('G',888),('G',889),('G',890),('G',891),('G',892),('G',893),('G',894),('G',895),('G',896),('G',897),('G',898),('G',899),('G',900),('G',901),('G',902),('G',903),('G',904),('G',905),('G',906),('G',907),('G',908),('G',909),('G',910),('G',911),('G',912),('G',913),('G',914),('G',915),('G',916),('G',917),('G',918),('G',919),('G',920),('G',921),('G',922),('G',923),('G',924),('G',925),('G',926),('G',927),('G',928),('G',929),('G',930),('G',931),('G',932),('G',933),('G',934),('G',935),('G',936),('G',937),('G',938),('G',939),('G',940),('G',941),('G',942),('G',943),('G',944),('G',945),('G',946),('G',947),('G',948),('G',949),('G',950),('G',951),('G',952),('G',953),('G',954),('G',955),('G',956),('G',957),('G',958),('G',959),('G',960),('G',961),('G',962),('G',963),('G',964),('G',965),('G',966),('G',967),('G',968),('G',969),('G',970),('G',971),('G',972),('G',973),('G',974),('G',975),('G',976),('G',977),('G',978),('G',979),('G',980),('G',981),('G',982),('G',983),('G',984),('G',985),('G',986),('G',987),('G',988),('G',989),('G',990),('G',991),('G',992),('G',993),('G',994),('G',995),('G',996)\", sql.get(0));\n    Assert.assertEquals(\"Wrong stmt\", \"insert into colors(name, category) values('G',997),('G',998),('G',999)\", sql.get(1));\n\n    sqlGenerator = new TxnHandler.SQLGenerator(TxnHandler.DatabaseProduct.SQLSERVER, conf);\n    String modSql = sqlGenerator.addForUpdateClause(\"select nl_next from NEXT_LOCK_ID\");\n    Assert.assertEquals(\"select nl_next from NEXT_LOCK_ID with (updlock)\", modSql);\n    modSql = sqlGenerator.addForUpdateClause(\"select MT_COMMENT from AUX_TABLE where MT_KEY1='CheckLock' and MT_KEY2=0\");\n    Assert.assertEquals(\"select MT_COMMENT from AUX_TABLE with (updlock) where MT_KEY1='CheckLock' and MT_KEY2=0\", modSql);\n  }"
        ],
        [
            "TxnHandler::SQLGenerator::addForUpdateClause(String)",
            "3373  \n3374  \n3375  \n3376  \n3377 -\n3378  \n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393 -\n3394  \n3395  \n3396  \n3397  \n3398  \n3399  ",
            "    /**\n     * Given a {@code selectStatement}, decorated it with FOR UPDATE or semantically equivalent\n     * construct.  If the DB doesn't support, return original select.\n     */\n    private String addForUpdateClause(String selectStatement) throws MetaException {\n      switch (dbProduct) {\n        case DERBY:\n          //https://db.apache.org/derby/docs/10.1/ref/rrefsqlj31783.html\n          //sadly in Derby, FOR UPDATE doesn't meant what it should\n          return selectStatement;\n        case MYSQL:\n          //http://dev.mysql.com/doc/refman/5.7/en/select.html\n        case ORACLE:\n          //https://docs.oracle.com/cd/E17952_01/refman-5.6-en/select.html\n        case POSTGRES:\n          //http://www.postgresql.org/docs/9.0/static/sql-select.html\n          return selectStatement + \" for update\";\n        case SQLSERVER:\n          //https://msdn.microsoft.com/en-us/library/ms189499.aspx\n          //https://msdn.microsoft.com/en-us/library/ms187373.aspx\n          return selectStatement + \" with(updlock)\";\n        default:\n          String msg = \"Unrecognized database product name <\" + dbProduct + \">\";\n          LOG.error(msg);\n          throw new MetaException(msg);\n      }\n    }",
            "3374  \n3375  \n3376  \n3377  \n3378 +\n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394 +\n3395 +\n3396 +\n3397 +\n3398 +\n3399 +\n3400 +\n3401  \n3402  \n3403  \n3404  \n3405  \n3406  ",
            "    /**\n     * Given a {@code selectStatement}, decorated it with FOR UPDATE or semantically equivalent\n     * construct.  If the DB doesn't support, return original select.\n     */\n    String addForUpdateClause(String selectStatement) throws MetaException {\n      switch (dbProduct) {\n        case DERBY:\n          //https://db.apache.org/derby/docs/10.1/ref/rrefsqlj31783.html\n          //sadly in Derby, FOR UPDATE doesn't meant what it should\n          return selectStatement;\n        case MYSQL:\n          //http://dev.mysql.com/doc/refman/5.7/en/select.html\n        case ORACLE:\n          //https://docs.oracle.com/cd/E17952_01/refman-5.6-en/select.html\n        case POSTGRES:\n          //http://www.postgresql.org/docs/9.0/static/sql-select.html\n          return selectStatement + \" for update\";\n        case SQLSERVER:\n          //https://msdn.microsoft.com/en-us/library/ms189499.aspx\n          //https://msdn.microsoft.com/en-us/library/ms187373.aspx\n          String modifier = \" with (updlock)\";\n          int wherePos = selectStatement.toUpperCase().indexOf(\" WHERE \");\n          if(wherePos < 0) {\n            return selectStatement + modifier;\n          }\n          return selectStatement.substring(0, wherePos) + modifier +\n            selectStatement.substring(wherePos, selectStatement.length());\n        default:\n          String msg = \"Unrecognized database product name <\" + dbProduct + \">\";\n          LOG.error(msg);\n          throw new MetaException(msg);\n      }\n    }"
        ]
    ],
    "1d926ae8180857033704610fe065415893660ee8": [
        [
            "DDLTask::msck(Hive,MsckDesc)",
            "1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880 -\n1881  \n1882  \n1883 -\n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  ",
            "  /**\n   * MetastoreCheck, see if the data in the metastore matches what is on the\n   * dfs. Current version checks for tables and partitions that are either\n   * missing on disk on in the metastore.\n   *\n   * @param db\n   *          The database in question.\n   * @param msckDesc\n   *          Information about the tables and partitions we want to check for.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   */\n  private int msck(Hive db, MsckDesc msckDesc) {\n    CheckResult result = new CheckResult();\n    List<String> repairOutput = new ArrayList<String>();\n    try {\n      HiveMetaStoreChecker checker = new HiveMetaStoreChecker(db);\n      String[] names = Utilities.getDbTableName(msckDesc.getTableName());\n      checker.checkMetastore(names[0], names[1], msckDesc.getPartSpecs(), result);\n      List<CheckResult.PartitionResult> partsNotInMs = result.getPartitionsNotInMs();\n      if (msckDesc.isRepairPartitions() && !partsNotInMs.isEmpty()) {\n        AbstractList<String> vals = null;\n        String settingStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION);\n        boolean doValidate = !(\"ignore\".equals(settingStr));\n        boolean doSkip = doValidate && \"skip\".equals(settingStr);\n        // The default setting is \"throw\"; assume doValidate && !doSkip means throw.\n        if (doValidate) {\n          // Validate that we can add partition without escaping. Escaping was originally intended\n          // to avoid creating invalid HDFS paths; however, if we escape the HDFS path (that we\n          // deem invalid but HDFS actually supports - it is possible to create HDFS paths with\n          // unprintable characters like ASCII 7), metastore will create another directory instead\n          // of the one we are trying to \"repair\" here.\n          Iterator<CheckResult.PartitionResult> iter = partsNotInMs.iterator();\n          while (iter.hasNext()) {\n            CheckResult.PartitionResult part = iter.next();\n            try {\n              vals = Warehouse.makeValsFromName(part.getPartitionName(), vals);\n            } catch (MetaException ex) {\n              throw new HiveException(ex);\n            }\n            for (String val : vals) {\n              String escapedPath = FileUtils.escapePathName(val);\n              assert escapedPath != null;\n              if (escapedPath.equals(val)) continue;\n              String errorMsg = \"Repair: Cannot add partition \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName() + \" due to invalid characters in the name\";\n              if (doSkip) {\n                repairOutput.add(errorMsg);\n                iter.remove();\n              } else {\n                throw new HiveException(errorMsg);\n              }\n            }\n          }\n        }\n        Table table = db.getTable(msckDesc.getTableName());\n        AddPartitionDesc apd = new AddPartitionDesc(\n            table.getDbName(), table.getTableName(), false);\n        try {\n          int batch_size = conf.getIntVar(ConfVars.HIVE_MSCK_REPAIR_BATCH_SIZE);\n          if (batch_size > 0 && partsNotInMs.size() > batch_size) {\n            int counter = 0;\n            for (CheckResult.PartitionResult part : partsNotInMs) {\n              counter++;\n              apd.addPartition(Warehouse.makeSpecFromName(part.getPartitionName()), null);\n              repairOutput.add(\"Repair: Added partition to metastore \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName());\n              if (counter == batch_size) {\n                db.createPartitions(apd);\n                apd = new AddPartitionDesc(table.getDbName(), table.getTableName(), false);\n                counter = 0;\n              }\n            }\n          } else {\n            for (CheckResult.PartitionResult part : partsNotInMs) {\n              apd.addPartition(Warehouse.makeSpecFromName(part.getPartitionName()), null);\n              repairOutput.add(\"Repair: Added partition to metastore \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName());\n            }\n            db.createPartitions(apd);\n          }\n        } catch (Exception e) {\n          LOG.info(\"Could not bulk-add partitions to metastore; trying one by one\", e);\n          repairOutput.clear();\n          msckAddPartitionsOneByOne(db, table, partsNotInMs, repairOutput);\n        }\n      }\n    } catch (HiveException e) {\n      LOG.warn(\"Failed to run metacheck: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"Failed to run metacheck: \", e);\n      return 1;\n    } finally {\n      BufferedWriter resultOut = null;\n      try {\n        Path resFile = new Path(msckDesc.getResFile());\n        FileSystem fs = resFile.getFileSystem(conf);\n        resultOut = new BufferedWriter(new OutputStreamWriter(fs\n            .create(resFile)));\n\n        boolean firstWritten = false;\n        firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n            \"Tables not in metastore:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n            \"Tables missing on filesystem:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n            \"Partitions not in metastore:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n        for (String rout : repairOutput) {\n          if (firstWritten) {\n            resultOut.write(terminator);\n          } else {\n            firstWritten = true;\n          }\n          resultOut.write(rout);\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed to save metacheck output: \", e);\n        return 1;\n      } finally {\n        if (resultOut != null) {\n          try {\n            resultOut.close();\n          } catch (IOException e) {\n            LOG.warn(\"Failed to close output file: \", e);\n            return 1;\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
            "1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880 +\n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  ",
            "  /**\n   * MetastoreCheck, see if the data in the metastore matches what is on the\n   * dfs. Current version checks for tables and partitions that are either\n   * missing on disk on in the metastore.\n   *\n   * @param db\n   *          The database in question.\n   * @param msckDesc\n   *          Information about the tables and partitions we want to check for.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   */\n  private int msck(Hive db, MsckDesc msckDesc) {\n    CheckResult result = new CheckResult();\n    List<String> repairOutput = new ArrayList<String>();\n    try {\n      HiveMetaStoreChecker checker = new HiveMetaStoreChecker(db);\n      String[] names = Utilities.getDbTableName(msckDesc.getTableName());\n      checker.checkMetastore(names[0], names[1], msckDesc.getPartSpecs(), result);\n      List<CheckResult.PartitionResult> partsNotInMs = result.getPartitionsNotInMs();\n      if (msckDesc.isRepairPartitions() && !partsNotInMs.isEmpty()) {\n        AbstractList<String> vals = null;\n        String settingStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION);\n        boolean doValidate = !(\"ignore\".equals(settingStr));\n        boolean doSkip = doValidate && \"skip\".equals(settingStr);\n        // The default setting is \"throw\"; assume doValidate && !doSkip means throw.\n        if (doValidate) {\n          // Validate that we can add partition without escaping. Escaping was originally intended\n          // to avoid creating invalid HDFS paths; however, if we escape the HDFS path (that we\n          // deem invalid but HDFS actually supports - it is possible to create HDFS paths with\n          // unprintable characters like ASCII 7), metastore will create another directory instead\n          // of the one we are trying to \"repair\" here.\n          Iterator<CheckResult.PartitionResult> iter = partsNotInMs.iterator();\n          while (iter.hasNext()) {\n            CheckResult.PartitionResult part = iter.next();\n            try {\n              vals = Warehouse.makeValsFromName(part.getPartitionName(), vals);\n            } catch (MetaException ex) {\n              throw new HiveException(ex);\n            }\n            for (String val : vals) {\n              String escapedPath = FileUtils.escapePathName(val);\n              assert escapedPath != null;\n              if (escapedPath.equals(val)) continue;\n              String errorMsg = \"Repair: Cannot add partition \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName() + \" due to invalid characters in the name\";\n              if (doSkip) {\n                repairOutput.add(errorMsg);\n                iter.remove();\n              } else {\n                throw new HiveException(errorMsg);\n              }\n            }\n          }\n        }\n        Table table = db.getTable(msckDesc.getTableName());\n        AddPartitionDesc apd = new AddPartitionDesc(\n            table.getDbName(), table.getTableName(), false);\n        try {\n          int batch_size = conf.getIntVar(ConfVars.HIVE_MSCK_REPAIR_BATCH_SIZE);\n          if (batch_size > 0 && partsNotInMs.size() > batch_size) {\n            int counter = 0;\n            for (CheckResult.PartitionResult part : partsNotInMs) {\n              counter++;\n              apd.addPartition(Warehouse.makeSpecFromName(part.getPartitionName()), null);\n              repairOutput.add(\"Repair: Added partition to metastore \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName());\n              if (counter % batch_size == 0 || counter == partsNotInMs.size()) {\n                db.createPartitions(apd);\n                apd = new AddPartitionDesc(table.getDbName(), table.getTableName(), false);\n              }\n            }\n          } else {\n            for (CheckResult.PartitionResult part : partsNotInMs) {\n              apd.addPartition(Warehouse.makeSpecFromName(part.getPartitionName()), null);\n              repairOutput.add(\"Repair: Added partition to metastore \" + msckDesc.getTableName()\n                  + ':' + part.getPartitionName());\n            }\n            db.createPartitions(apd);\n          }\n        } catch (Exception e) {\n          LOG.info(\"Could not bulk-add partitions to metastore; trying one by one\", e);\n          repairOutput.clear();\n          msckAddPartitionsOneByOne(db, table, partsNotInMs, repairOutput);\n        }\n      }\n    } catch (HiveException e) {\n      LOG.warn(\"Failed to run metacheck: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"Failed to run metacheck: \", e);\n      return 1;\n    } finally {\n      BufferedWriter resultOut = null;\n      try {\n        Path resFile = new Path(msckDesc.getResFile());\n        FileSystem fs = resFile.getFileSystem(conf);\n        resultOut = new BufferedWriter(new OutputStreamWriter(fs\n            .create(resFile)));\n\n        boolean firstWritten = false;\n        firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n            \"Tables not in metastore:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n            \"Tables missing on filesystem:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n            \"Partitions not in metastore:\", resultOut, firstWritten);\n        firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n        for (String rout : repairOutput) {\n          if (firstWritten) {\n            resultOut.write(terminator);\n          } else {\n            firstWritten = true;\n          }\n          resultOut.write(rout);\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed to save metacheck output: \", e);\n        return 1;\n      } finally {\n        if (resultOut != null) {\n          try {\n            resultOut.close();\n          } catch (IOException e) {\n            LOG.warn(\"Failed to close output file: \", e);\n            return 1;\n          }\n        }\n      }\n    }\n\n    return 0;\n  }"
        ]
    ],
    "cd6c3cdf91c466a01cd08b108601f8a654a192a8": [
        [
            "LlapTaskSchedulerService::selectHost(TaskInfo)",
            " 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784 -\n 785 -\n 786  \n 787  \n 788  \n 789 -\n 790 -\n 791 -\n 792 -\n 793 -\n 794 -\n 795 -\n 796 -\n 797 -\n 798 -\n 799 -\n 800 -\n 801 -\n 802 -\n 803  \n 804 -\n 805  \n 806  \n 807  \n 808  ",
            "  /**\n   * @param request the list of preferred hosts. null implies any host\n   * @return\n   */\n  private SelectHostResult selectHost(TaskInfo request) {\n    String[] requestedHosts = request.requestedHosts;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"selectingHost for task={} on hosts={}\", request.task, Arrays.toString(requestedHosts));\n    }\n    long schedulerAttemptTime = clock.getTime();\n    readLock.lock(); // Read-lock. Not updating any stats at the moment.\n    try {\n      // If there's no memory available, fail\n      if (getTotalResources().getMemory() <= 0) {\n        return SELECT_HOST_RESULT_INADEQUATE_TOTAL_CAPACITY;\n      }\n\n      boolean shouldDelayForLocality = request.shouldDelayForLocality(schedulerAttemptTime);\n      LOG.debug(\"ShouldDelayForLocality={} for task={} on hosts={}\", shouldDelayForLocality,\n          request.task, Arrays.toString(requestedHosts));\n      if (requestedHosts != null && requestedHosts.length > 0) {\n        int prefHostCount = -1;\n        boolean requestedHostsWillBecomeAvailable = false;\n        for (String host : requestedHosts) {\n          prefHostCount++;\n          // Pick the first host always. Weak attempt at cache affinity.\n          Set<ServiceInstance> instances = activeInstances.getByHost(host);\n          if (!instances.isEmpty()) {\n            for (ServiceInstance inst : instances) {\n              NodeInfo nodeInfo = instanceToNodeMap.get(inst.getWorkerIdentity());\n              if (nodeInfo != null) {\n                if  (nodeInfo.canAcceptTask()) {\n                  // Successfully scheduled.\n                  LOG.info(\n                      \"Assigning \" + nodeToString(inst, nodeInfo) + \" when looking for \" + host +\n                          \". local=true\" + \" FirstRequestedHost=\" + (prefHostCount == 0) +\n                          (requestedHosts.length > 1 ? \", #prefLocations=\" + requestedHosts.length :\n                              \"\"));\n                  return new SelectHostResult(inst, nodeInfo);\n                } else {\n                  // The node cannot accept a task at the moment.\n                  if (shouldDelayForLocality) {\n                    // Perform some checks on whether the node will become available or not.\n                    if (request.shouldForceLocality()) {\n                      requestedHostsWillBecomeAvailable = true;\n                    } else {\n                      if (nodeInfo.getEnableTime() > request.getLocalityDelayTimeout() &&\n                          nodeInfo.isDisabled() && nodeInfo.hadCommFailure()) {\n                        LOG.debug(\"Host={} will not become available within requested timeout\", nodeInfo);\n                        // This node will likely be activated after the task timeout expires.\n                      } else {\n                        // Worth waiting for the timeout.\n                        requestedHostsWillBecomeAvailable = true;\n                      }\n                    }\n                  }\n                }\n              } else {\n                LOG.warn(\n                    \"Null NodeInfo when attempting to get host with worker identity {}, and host {}\",\n                    inst.getWorkerIdentity(), host);\n                // Leave requestedHostWillBecomeAvailable as is. If some other host is found - delay,\n                // else ends up allocating to a random host immediately.\n              }\n            }\n          }\n        }\n        // Check if forcing the location is required.\n        if (shouldDelayForLocality) {\n          if (requestedHostsWillBecomeAvailable) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Delaying local allocation for [\" + request.task +\n                  \"] when trying to allocate on [\" + Arrays.toString(requestedHosts) + \"]\" +\n                  \". ScheduleAttemptTime=\" + schedulerAttemptTime + \", taskDelayTimeout=\" +\n                  request.getLocalityDelayTimeout());\n            }\n            return SELECT_HOST_RESULT_DELAYED_LOCALITY;\n          } else {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping local allocation for [\" + request.task +\n                  \"] when trying to allocate on [\" + Arrays.toString(requestedHosts) +\n                  \"] since none of these hosts are part of the known list\");\n            }\n          }\n        }\n      }\n      /* fall through - miss in locality (random scheduling) or no locality-requested */\n      Entry<String, NodeInfo>[] all = instanceToNodeMap.entrySet().toArray(new Entry[0]);\n      // Check again\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Attempting random allocation for task={}\", request.task);\n      }\n      if (all.length > 0) {\n        int n = random.nextInt(all.length);\n        // start at random offset and iterate whole list\n        for (int i = 0; i < all.length; i++) {\n          Entry<String, NodeInfo> inst = all[(i + n) % all.length];\n          if (inst.getValue().canAcceptTask()) {\n            LOG.info(\n                \"Assigning \" + nodeToString(inst.getValue().getServiceInstance(), inst.getValue()) +\n                    \" when looking for any host, from #hosts=\" + all.length + \", requestedHosts=\" +\n                    ((requestedHosts == null || requestedHosts.length == 0) ? \"null\" :\n                        Arrays.toString(requestedHosts)));\n            return new SelectHostResult(inst.getValue().getServiceInstance(), inst.getValue());\n          }\n        }\n      }\n      return SELECT_HOST_RESULT_DELAYED_RESOURCES;\n    } finally {\n      readLock.unlock();\n    }\n  }",
            " 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786 +\n 787 +\n 788 +\n 789 +\n 790 +\n 791 +\n 792 +\n 793 +\n 794  \n 795  \n 796  \n 797 +\n 798 +\n 799  \n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805 +\n 806  \n 807  \n 808  \n 809  ",
            "  /**\n   * @param request the list of preferred hosts. null implies any host\n   * @return\n   */\n  private SelectHostResult selectHost(TaskInfo request) {\n    String[] requestedHosts = request.requestedHosts;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"selectingHost for task={} on hosts={}\", request.task, Arrays.toString(requestedHosts));\n    }\n    long schedulerAttemptTime = clock.getTime();\n    readLock.lock(); // Read-lock. Not updating any stats at the moment.\n    try {\n      // If there's no memory available, fail\n      if (getTotalResources().getMemory() <= 0) {\n        return SELECT_HOST_RESULT_INADEQUATE_TOTAL_CAPACITY;\n      }\n\n      boolean shouldDelayForLocality = request.shouldDelayForLocality(schedulerAttemptTime);\n      LOG.debug(\"ShouldDelayForLocality={} for task={} on hosts={}\", shouldDelayForLocality,\n          request.task, Arrays.toString(requestedHosts));\n      if (requestedHosts != null && requestedHosts.length > 0) {\n        int prefHostCount = -1;\n        boolean requestedHostsWillBecomeAvailable = false;\n        for (String host : requestedHosts) {\n          prefHostCount++;\n          // Pick the first host always. Weak attempt at cache affinity.\n          Set<ServiceInstance> instances = activeInstances.getByHost(host);\n          if (!instances.isEmpty()) {\n            for (ServiceInstance inst : instances) {\n              NodeInfo nodeInfo = instanceToNodeMap.get(inst.getWorkerIdentity());\n              if (nodeInfo != null) {\n                if  (nodeInfo.canAcceptTask()) {\n                  // Successfully scheduled.\n                  LOG.info(\n                      \"Assigning \" + nodeToString(inst, nodeInfo) + \" when looking for \" + host +\n                          \". local=true\" + \" FirstRequestedHost=\" + (prefHostCount == 0) +\n                          (requestedHosts.length > 1 ? \", #prefLocations=\" + requestedHosts.length :\n                              \"\"));\n                  return new SelectHostResult(inst, nodeInfo);\n                } else {\n                  // The node cannot accept a task at the moment.\n                  if (shouldDelayForLocality) {\n                    // Perform some checks on whether the node will become available or not.\n                    if (request.shouldForceLocality()) {\n                      requestedHostsWillBecomeAvailable = true;\n                    } else {\n                      if (nodeInfo.getEnableTime() > request.getLocalityDelayTimeout() &&\n                          nodeInfo.isDisabled() && nodeInfo.hadCommFailure()) {\n                        LOG.debug(\"Host={} will not become available within requested timeout\", nodeInfo);\n                        // This node will likely be activated after the task timeout expires.\n                      } else {\n                        // Worth waiting for the timeout.\n                        requestedHostsWillBecomeAvailable = true;\n                      }\n                    }\n                  }\n                }\n              } else {\n                LOG.warn(\n                    \"Null NodeInfo when attempting to get host with worker identity {}, and host {}\",\n                    inst.getWorkerIdentity(), host);\n                // Leave requestedHostWillBecomeAvailable as is. If some other host is found - delay,\n                // else ends up allocating to a random host immediately.\n              }\n            }\n          }\n        }\n        // Check if forcing the location is required.\n        if (shouldDelayForLocality) {\n          if (requestedHostsWillBecomeAvailable) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Delaying local allocation for [\" + request.task +\n                  \"] when trying to allocate on [\" + Arrays.toString(requestedHosts) + \"]\" +\n                  \". ScheduleAttemptTime=\" + schedulerAttemptTime + \", taskDelayTimeout=\" +\n                  request.getLocalityDelayTimeout());\n            }\n            return SELECT_HOST_RESULT_DELAYED_LOCALITY;\n          } else {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Skipping local allocation for [\" + request.task +\n                  \"] when trying to allocate on [\" + Arrays.toString(requestedHosts) +\n                  \"] since none of these hosts are part of the known list\");\n            }\n          }\n        }\n      }\n      /* fall through - miss in locality (random scheduling) or no locality-requested */\n      Collection<ServiceInstance> instances = activeInstances.getAll();\n      ArrayList<NodeInfo> all = new ArrayList<>(instances.size());\n      for (ServiceInstance inst : instances) {\n        NodeInfo nodeInfo = instanceToNodeMap.get(inst.getWorkerIdentity());\n        if (nodeInfo != null && nodeInfo.canAcceptTask()) {\n          all.add(nodeInfo);\n        }\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Attempting random allocation for task={}\", request.task);\n      }\n      if (all.isEmpty()) {\n        return SELECT_HOST_RESULT_DELAYED_RESOURCES;\n      }\n      NodeInfo randomNode = all.get(random.nextInt(all.size()));\n      LOG.info(\"Assigning \" + nodeToString(randomNode.getServiceInstance(), randomNode)\n          + \" when looking for any host, from #hosts=\" + all.size() + \", requestedHosts=\"\n          + ((requestedHosts == null || requestedHosts.length == 0)\n              ? \"null\" : Arrays.toString(requestedHosts)));\n      return new SelectHostResult(randomNode.getServiceInstance(), randomNode);\n    } finally {\n      readLock.unlock();\n    }\n  }"
        ]
    ],
    "0a6d30b3e89d97028b3cd4174ec92e1f5a56d49f": [
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n      List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n      ArrayList<ExprNodeDesc> bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 +\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208 +\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }"
        ]
    ],
    "7dd1d3694856a2fd55eba1355e81e5fa71477b0d": [
        [
            "HiveMetaStore::HMSHandler::delete_table_column_statistics(String,String,String)",
            "4718  \n4719  \n4720  \n4721  \n4722  \n4723  \n4724  \n4725  \n4726  \n4727  \n4728 -\n4729 -\n4730  \n4731  \n4732  \n4733  \n4734  \n4735 -\n4736  \n4737  \n4738  ",
            "    @Override\n    public boolean delete_table_column_statistics(String dbName, String tableName, String colName)\n      throws NoSuchObjectException, MetaException, InvalidObjectException, TException,\n      InvalidInputException {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n\n      if (colName != null) {\n        colName = colName.toLowerCase();\n      }\n      startFunction(\"delete_column_statistics_by_table: db=\" + dbName + \" table=\" + tableName +\n                    \" column=\" + colName);\n\n      boolean ret = false;\n      try {\n        ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n      } finally {\n        endFunction(\"delete_column_statistics_by_table: \", ret != false, null, tableName);\n      }\n      return ret;\n    }",
            "4721  \n4722  \n4723  \n4724  \n4725  \n4726  \n4727  \n4728  \n4729  \n4730  \n4731 +\n4732 +\n4733  \n4734  \n4735  \n4736  \n4737  \n4738 +\n4739  \n4740  \n4741  ",
            "    @Override\n    public boolean delete_table_column_statistics(String dbName, String tableName, String colName)\n      throws NoSuchObjectException, MetaException, InvalidObjectException, TException,\n      InvalidInputException {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n\n      if (colName != null) {\n        colName = colName.toLowerCase();\n      }\n      startFunction(\"delete_column_statistics_by_table\", \": db=\" + dbName\n          + \" table=\" + tableName + \" column=\" + colName);\n\n      boolean ret = false;\n      try {\n        ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n      } finally {\n        endFunction(\"delete_column_statistics_by_table\", ret != false, null, tableName);\n      }\n      return ret;\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::updatePartitonColStats(Table,ColumnStatistics)",
            "4641  \n4642  \n4643  \n4644  \n4645  \n4646  \n4647  \n4648  \n4649  \n4650  \n4651  \n4652  \n4653  \n4654  \n4655  \n4656  \n4657  \n4658  \n4659  \n4660  \n4661  \n4662  \n4663  \n4664  \n4665 -\n4666 -\n4667  \n4668  \n4669  \n4670  \n4671  \n4672  \n4673  \n4674  \n4675  \n4676  \n4677  \n4678  \n4679  \n4680  \n4681  \n4682 -\n4683  \n4684  ",
            "    private boolean updatePartitonColStats(Table tbl, ColumnStatistics colStats)\n        throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n      String dbName = null;\n      String tableName = null;\n      String partName = null;\n      String colName = null;\n\n      ColumnStatisticsDesc statsDesc = colStats.getStatsDesc();\n      dbName = statsDesc.getDbName().toLowerCase();\n      tableName = statsDesc.getTableName().toLowerCase();\n      partName = lowerCaseConvertPartName(statsDesc.getPartName());\n\n      statsDesc.setDbName(dbName);\n      statsDesc.setTableName(tableName);\n      statsDesc.setPartName(partName);\n\n      long time = System.currentTimeMillis() / 1000;\n      statsDesc.setLastAnalyzed(time);\n\n      List<ColumnStatisticsObj> statsObjs =  colStats.getStatsObj();\n\n      for (ColumnStatisticsObj statsObj:statsObjs) {\n        colName = statsObj.getColName().toLowerCase();\n        statsObj.setColName(colName);\n        startFunction(\"write_partition_column_statistics:  db=\" + dbName + \" table=\" + tableName +\n            \" part=\" + partName + \"column=\" + colName);\n      }\n\n      colStats.setStatsDesc(statsDesc);\n      colStats.setStatsObj(statsObjs);\n\n      boolean ret = false;\n\n      try {\n        if (tbl == null) {\n          tbl = getTable(dbName, tableName);\n        }\n        List<String> partVals = getPartValsFromName(tbl, partName);\n        ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n        return ret;\n      } finally {\n        endFunction(\"write_partition_column_statistics: \", ret != false, null, tableName);\n      }\n    }",
            "4642  \n4643  \n4644  \n4645  \n4646  \n4647  \n4648  \n4649  \n4650  \n4651  \n4652  \n4653  \n4654  \n4655  \n4656  \n4657  \n4658  \n4659  \n4660  \n4661  \n4662  \n4663 +\n4664 +\n4665 +\n4666  \n4667  \n4668  \n4669  \n4670  \n4671  \n4672  \n4673  \n4674  \n4675  \n4676  \n4677  \n4678  \n4679  \n4680  \n4681  \n4682  \n4683  \n4684 +\n4685  \n4686  ",
            "    private boolean updatePartitonColStats(Table tbl, ColumnStatistics colStats)\n        throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n      String dbName = null;\n      String tableName = null;\n      String partName = null;\n      String colName = null;\n\n      ColumnStatisticsDesc statsDesc = colStats.getStatsDesc();\n      dbName = statsDesc.getDbName().toLowerCase();\n      tableName = statsDesc.getTableName().toLowerCase();\n      partName = lowerCaseConvertPartName(statsDesc.getPartName());\n\n      statsDesc.setDbName(dbName);\n      statsDesc.setTableName(tableName);\n      statsDesc.setPartName(partName);\n\n      long time = System.currentTimeMillis() / 1000;\n      statsDesc.setLastAnalyzed(time);\n\n      List<ColumnStatisticsObj> statsObjs =  colStats.getStatsObj();\n\n      startFunction(\"write_partition_column_statistics\",\n          \":  db=\" + dbName + \" table=\" + tableName\n              + \" part=\" + partName + \"column=\" + colName);\n      for (ColumnStatisticsObj statsObj:statsObjs) {\n        colName = statsObj.getColName().toLowerCase();\n        statsObj.setColName(colName);\n      }\n\n      colStats.setStatsDesc(statsDesc);\n      colStats.setStatsObj(statsObjs);\n\n      boolean ret = false;\n\n      try {\n        if (tbl == null) {\n          tbl = getTable(dbName, tableName);\n        }\n        List<String> partVals = getPartValsFromName(tbl, partName);\n        ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n        return ret;\n      } finally {\n        endFunction(\"write_partition_column_statistics\", ret != false, null, tableName);\n      }\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::get_partitions_statistics_req(PartitionsStatsRequest)",
            "4571  \n4572  \n4573  \n4574  \n4575  \n4576 -\n4577  \n4578  \n4579  \n4580  \n4581  \n4582  \n4583  \n4584  \n4585  \n4586  \n4587  \n4588  \n4589  \n4590  \n4591  \n4592  \n4593  \n4594  \n4595  \n4596  \n4597 -\n4598  \n4599  \n4600  ",
            "    @Override\n    public PartitionsStatsResult get_partitions_statistics_req(PartitionsStatsRequest request)\n        throws MetaException, NoSuchObjectException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_partitions_statistics_req: db=\" + dbName + \" table=\" + tblName);\n\n      PartitionsStatsResult result = null;\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      List<String> lowerCasePartNames = new ArrayList<String>(request.getPartNames().size());\n      for (String partName : request.getPartNames()) {\n        lowerCasePartNames.add(lowerCaseConvertPartName(partName));\n      }\n      try {\n        List<ColumnStatistics> stats = getMS().getPartitionColumnStatistics(\n            dbName, tblName, lowerCasePartNames, lowerCaseColNames);\n        Map<String, List<ColumnStatisticsObj>> map =\n            new HashMap<String, List<ColumnStatisticsObj>>();\n        for (ColumnStatistics stat : stats) {\n          map.put(stat.getStatsDesc().getPartName(), stat.getStatsObj());\n        }\n        result = new PartitionsStatsResult(map);\n      } finally {\n        endFunction(\"get_partitions_statistics_req: \", result == null, null, tblName);\n      }\n      return result;\n    }",
            "4572  \n4573  \n4574  \n4575  \n4576  \n4577 +\n4578  \n4579  \n4580  \n4581  \n4582  \n4583  \n4584  \n4585  \n4586  \n4587  \n4588  \n4589  \n4590  \n4591  \n4592  \n4593  \n4594  \n4595  \n4596  \n4597  \n4598 +\n4599  \n4600  \n4601  ",
            "    @Override\n    public PartitionsStatsResult get_partitions_statistics_req(PartitionsStatsRequest request)\n        throws MetaException, NoSuchObjectException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_partitions_statistics_req\", \": db=\" + dbName + \" table=\" + tblName);\n\n      PartitionsStatsResult result = null;\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      List<String> lowerCasePartNames = new ArrayList<String>(request.getPartNames().size());\n      for (String partName : request.getPartNames()) {\n        lowerCasePartNames.add(lowerCaseConvertPartName(partName));\n      }\n      try {\n        List<ColumnStatistics> stats = getMS().getPartitionColumnStatistics(\n            dbName, tblName, lowerCasePartNames, lowerCaseColNames);\n        Map<String, List<ColumnStatisticsObj>> map =\n            new HashMap<String, List<ColumnStatisticsObj>>();\n        for (ColumnStatistics stat : stats) {\n          map.put(stat.getStatsDesc().getPartName(), stat.getStatsObj());\n        }\n        result = new PartitionsStatsResult(map);\n      } finally {\n        endFunction(\"get_partitions_statistics_req\", result == null, null, tblName);\n      }\n      return result;\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::update_table_column_statistics(ColumnStatistics)",
            "4602  \n4603  \n4604  \n4605  \n4606  \n4607  \n4608  \n4609  \n4610  \n4611  \n4612  \n4613  \n4614  \n4615  \n4616  \n4617  \n4618  \n4619  \n4620  \n4621  \n4622  \n4623  \n4624 -\n4625 -\n4626  \n4627  \n4628  \n4629  \n4630  \n4631  \n4632  \n4633  \n4634  \n4635  \n4636  \n4637 -\n4638  \n4639  ",
            "    @Override\n    public boolean update_table_column_statistics(ColumnStatistics colStats)\n      throws NoSuchObjectException,InvalidObjectException,MetaException,TException,\n      InvalidInputException\n    {\n      String dbName = null;\n      String tableName = null;\n      String colName = null;\n      ColumnStatisticsDesc statsDesc = colStats.getStatsDesc();\n      dbName = statsDesc.getDbName().toLowerCase();\n      tableName = statsDesc.getTableName().toLowerCase();\n\n      statsDesc.setDbName(dbName);\n      statsDesc.setTableName(tableName);\n      long time = System.currentTimeMillis() / 1000;\n      statsDesc.setLastAnalyzed(time);\n\n      List<ColumnStatisticsObj> statsObjs =  colStats.getStatsObj();\n\n      for (ColumnStatisticsObj statsObj:statsObjs) {\n        colName = statsObj.getColName().toLowerCase();\n        statsObj.setColName(colName);\n        startFunction(\"write_column_statistics:  db=\" + dbName + \" table=\" + tableName +\n          \" column=\" + colName);\n      }\n\n     colStats.setStatsDesc(statsDesc);\n     colStats.setStatsObj(statsObjs);\n\n     boolean ret = false;\n\n      try {\n        ret = getMS().updateTableColumnStatistics(colStats);\n        return ret;\n      } finally {\n        endFunction(\"write_column_statistics: \", ret != false, null, tableName);\n      }\n    }",
            "4603  \n4604  \n4605  \n4606  \n4607  \n4608  \n4609  \n4610  \n4611  \n4612  \n4613  \n4614  \n4615  \n4616  \n4617  \n4618  \n4619  \n4620  \n4621  \n4622 +\n4623 +\n4624  \n4625  \n4626  \n4627  \n4628  \n4629  \n4630  \n4631  \n4632  \n4633  \n4634  \n4635  \n4636  \n4637  \n4638 +\n4639  \n4640  ",
            "    @Override\n    public boolean update_table_column_statistics(ColumnStatistics colStats)\n      throws NoSuchObjectException,InvalidObjectException,MetaException,TException,\n      InvalidInputException\n    {\n      String dbName = null;\n      String tableName = null;\n      String colName = null;\n      ColumnStatisticsDesc statsDesc = colStats.getStatsDesc();\n      dbName = statsDesc.getDbName().toLowerCase();\n      tableName = statsDesc.getTableName().toLowerCase();\n\n      statsDesc.setDbName(dbName);\n      statsDesc.setTableName(tableName);\n      long time = System.currentTimeMillis() / 1000;\n      statsDesc.setLastAnalyzed(time);\n\n      List<ColumnStatisticsObj> statsObjs =  colStats.getStatsObj();\n\n      startFunction(\"write_column_statistics\", \":  db=\" + dbName\n          + \" table=\" + tableName + \" column=\" + colName);\n      for (ColumnStatisticsObj statsObj:statsObjs) {\n        colName = statsObj.getColName().toLowerCase();\n        statsObj.setColName(colName);\n      }\n\n     colStats.setStatsDesc(statsDesc);\n     colStats.setStatsObj(statsObjs);\n\n     boolean ret = false;\n\n      try {\n        ret = getMS().updateTableColumnStatistics(colStats);\n        return ret;\n      } finally {\n        endFunction(\"write_column_statistics\", ret != false, null, tableName);\n      }\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::get_table_statistics_req(TableStatsRequest)",
            "4524  \n4525  \n4526  \n4527  \n4528  \n4529 -\n4530  \n4531  \n4532  \n4533  \n4534  \n4535  \n4536  \n4537  \n4538  \n4539  \n4540 -\n4541  \n4542  \n4543  ",
            "    @Override\n    public TableStatsResult get_table_statistics_req(TableStatsRequest request)\n        throws MetaException, NoSuchObjectException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_table_statistics_req: db=\" + dbName + \" table=\" + tblName);\n      TableStatsResult result = null;\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      try {\n        ColumnStatistics cs = getMS().getTableColumnStatistics(dbName, tblName, lowerCaseColNames);\n        result = new TableStatsResult((cs == null || cs.getStatsObj() == null)\n            ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n      } finally {\n        endFunction(\"get_table_statistics_req: \", result == null, null, tblName);\n      }\n      return result;\n    }",
            "4524  \n4525  \n4526  \n4527  \n4528  \n4529 +\n4530  \n4531  \n4532  \n4533  \n4534  \n4535  \n4536  \n4537  \n4538  \n4539  \n4540 +\n4541  \n4542  \n4543  ",
            "    @Override\n    public TableStatsResult get_table_statistics_req(TableStatsRequest request)\n        throws MetaException, NoSuchObjectException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_table_statistics_req\", \": db=\" + dbName + \" table=\" + tblName);\n      TableStatsResult result = null;\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      try {\n        ColumnStatistics cs = getMS().getTableColumnStatistics(dbName, tblName, lowerCaseColNames);\n        result = new TableStatsResult((cs == null || cs.getStatsObj() == null)\n            ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n      } finally {\n        endFunction(\"get_table_statistics_req\", result == null, null, tblName);\n      }\n      return result;\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::delete_partition_column_statistics(String,String,String,String)",
            "4693  \n4694  \n4695  \n4696  \n4697  \n4698  \n4699  \n4700  \n4701  \n4702  \n4703  \n4704 -\n4705 -\n4706  \n4707  \n4708  \n4709  \n4710  \n4711  \n4712  \n4713 -\n4714  \n4715  \n4716  ",
            "    @Override\n    public boolean delete_partition_column_statistics(String dbName, String tableName,\n      String partName, String colName) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, TException, InvalidInputException\n    {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      if (colName != null) {\n        colName = colName.toLowerCase();\n      }\n      String convertedPartName = lowerCaseConvertPartName(partName);\n      startFunction(\"delete_column_statistics_by_partition: db=\" + dbName + \" table=\" + tableName +\n                    \" partition=\" + convertedPartName + \" column=\" + colName);\n      boolean ret = false;\n\n      try {\n        List<String> partVals = getPartValsFromName(getMS(), dbName, tableName, convertedPartName);\n        ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                      convertedPartName, partVals, colName);\n      } finally {\n        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null, tableName);\n      }\n      return ret;\n    }",
            "4695  \n4696  \n4697  \n4698  \n4699  \n4700  \n4701  \n4702  \n4703  \n4704  \n4705  \n4706 +\n4707 +\n4708 +\n4709  \n4710  \n4711  \n4712  \n4713  \n4714  \n4715  \n4716 +\n4717  \n4718  \n4719  ",
            "    @Override\n    public boolean delete_partition_column_statistics(String dbName, String tableName,\n      String partName, String colName) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, TException, InvalidInputException\n    {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      if (colName != null) {\n        colName = colName.toLowerCase();\n      }\n      String convertedPartName = lowerCaseConvertPartName(partName);\n      startFunction(\"delete_column_statistics_by_partition\",\": db=\" + dbName\n          + \" table=\" + tableName + \" partition=\" + convertedPartName\n          + \" column=\" + colName);\n      boolean ret = false;\n\n      try {\n        List<String> partVals = getPartValsFromName(getMS(), dbName, tableName, convertedPartName);\n        ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                      convertedPartName, partVals, colName);\n      } finally {\n        endFunction(\"delete_column_statistics_by_partition\", ret != false, null, tableName);\n      }\n      return ret;\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::get_table_column_statistics(String,String,String)",
            "4501  \n4502  \n4503  \n4504  \n4505  \n4506  \n4507  \n4508  \n4509 -\n4510  \n4511  \n4512  \n4513  \n4514  \n4515  \n4516  \n4517  \n4518  \n4519  \n4520 -\n4521  \n4522  ",
            "    @Override\n    public ColumnStatistics get_table_column_statistics(String dbName, String tableName,\n      String colName) throws NoSuchObjectException, MetaException, TException,\n      InvalidInputException, InvalidObjectException\n    {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      colName = colName.toLowerCase();\n      startFunction(\"get_column_statistics_by_table: db=\" + dbName + \" table=\" + tableName +\n                    \" column=\" + colName);\n      ColumnStatistics statsObj = null;\n      try {\n        statsObj = getMS().getTableColumnStatistics(\n            dbName, tableName, Lists.newArrayList(colName));\n        if (statsObj != null) {\n          assert statsObj.getStatsObjSize() <= 1;\n        }\n        return statsObj;\n      } finally {\n        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null, tableName);\n      }\n    }",
            "4501  \n4502  \n4503  \n4504  \n4505  \n4506  \n4507  \n4508  \n4509 +\n4510  \n4511  \n4512  \n4513  \n4514  \n4515  \n4516  \n4517  \n4518  \n4519  \n4520 +\n4521  \n4522  ",
            "    @Override\n    public ColumnStatistics get_table_column_statistics(String dbName, String tableName,\n      String colName) throws NoSuchObjectException, MetaException, TException,\n      InvalidInputException, InvalidObjectException\n    {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      colName = colName.toLowerCase();\n      startFunction(\"get_column_statistics_by_table\", \": db=\" + dbName + \" table=\" + tableName +\n                    \" column=\" + colName);\n      ColumnStatistics statsObj = null;\n      try {\n        statsObj = getMS().getTableColumnStatistics(\n            dbName, tableName, Lists.newArrayList(colName));\n        if (statsObj != null) {\n          assert statsObj.getStatsObjSize() <= 1;\n        }\n        return statsObj;\n      } finally {\n        endFunction(\"get_column_statistics_by_table\", statsObj != null, null, tableName);\n      }\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::get_partition_column_statistics(String,String,String,String)",
            "4545  \n4546  \n4547  \n4548  \n4549  \n4550  \n4551  \n4552  \n4553 -\n4554 -\n4555  \n4556  \n4557  \n4558  \n4559  \n4560  \n4561  \n4562  \n4563  \n4564  \n4565  \n4566 -\n4567  \n4568  \n4569  ",
            "    @Override\n    public ColumnStatistics get_partition_column_statistics(String dbName, String tableName,\n      String partName, String colName) throws NoSuchObjectException, MetaException,\n      InvalidInputException, TException, InvalidObjectException {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      colName = colName.toLowerCase();\n      String convertedPartName = lowerCaseConvertPartName(partName);\n      startFunction(\"get_column_statistics_by_partition: db=\" + dbName + \" table=\" + tableName +\n          \" partition=\" + convertedPartName + \" column=\" + colName);\n      ColumnStatistics statsObj = null;\n\n      try {\n        List<ColumnStatistics> list = getMS().getPartitionColumnStatistics(dbName, tableName,\n            Lists.newArrayList(convertedPartName), Lists.newArrayList(colName));\n        if (list.isEmpty()) return null;\n        if (list.size() != 1) {\n          throw new MetaException(list.size() + \" statistics for single column and partition\");\n        }\n        statsObj = list.get(0);\n      } finally {\n        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null, tableName);\n      }\n      return statsObj;\n    }",
            "4545  \n4546  \n4547  \n4548  \n4549  \n4550  \n4551  \n4552  \n4553 +\n4554 +\n4555 +\n4556  \n4557  \n4558  \n4559  \n4560  \n4561  \n4562  \n4563  \n4564  \n4565  \n4566  \n4567 +\n4568  \n4569  \n4570  ",
            "    @Override\n    public ColumnStatistics get_partition_column_statistics(String dbName, String tableName,\n      String partName, String colName) throws NoSuchObjectException, MetaException,\n      InvalidInputException, TException, InvalidObjectException {\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n      colName = colName.toLowerCase();\n      String convertedPartName = lowerCaseConvertPartName(partName);\n      startFunction(\"get_column_statistics_by_partition\",\n              \": db=\" + dbName + \" table=\" + tableName\n              + \" partition=\" + convertedPartName + \" column=\" + colName);\n      ColumnStatistics statsObj = null;\n\n      try {\n        List<ColumnStatistics> list = getMS().getPartitionColumnStatistics(dbName, tableName,\n            Lists.newArrayList(convertedPartName), Lists.newArrayList(colName));\n        if (list.isEmpty()) return null;\n        if (list.size() != 1) {\n          throw new MetaException(list.size() + \" statistics for single column and partition\");\n        }\n        statsObj = list.get(0);\n      } finally {\n        endFunction(\"get_column_statistics_by_partition\", statsObj != null, null, tableName);\n      }\n      return statsObj;\n    }"
        ],
        [
            "HiveMetaStore::HMSHandler::get_aggr_stats_for(PartitionsStatsRequest)",
            "6100  \n6101  \n6102  \n6103  \n6104  \n6105 -\n6106  \n6107  \n6108  \n6109  \n6110  \n6111  \n6112  \n6113  \n6114  \n6115  \n6116  \n6117  \n6118  \n6119  \n6120  \n6121  \n6122 -\n6123  \n6124  \n6125  ",
            "    @Override\n    public AggrStats get_aggr_stats_for(PartitionsStatsRequest request)\n        throws NoSuchObjectException, MetaException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_aggr_stats_for: db=\" + request.getDbName() + \" table=\" + request.getTblName());\n\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      List<String> lowerCasePartNames = new ArrayList<String>(request.getPartNames().size());\n      for (String partName : request.getPartNames()) {\n        lowerCasePartNames.add(lowerCaseConvertPartName(partName));\n      }\n      AggrStats aggrStats = null;\n\n      try {\n        aggrStats = new AggrStats(getMS().get_aggr_stats_for(dbName, tblName, lowerCasePartNames,\n            lowerCaseColNames));\n        return aggrStats;\n      } finally {\n          endFunction(\"get_partitions_statistics_req: \", aggrStats == null, null, request.getTblName());\n      }\n\n    }",
            "6103  \n6104  \n6105  \n6106  \n6107  \n6108 +\n6109 +\n6110  \n6111  \n6112  \n6113  \n6114  \n6115  \n6116  \n6117  \n6118  \n6119  \n6120  \n6121  \n6122  \n6123  \n6124  \n6125  \n6126 +\n6127  \n6128  \n6129  ",
            "    @Override\n    public AggrStats get_aggr_stats_for(PartitionsStatsRequest request)\n        throws NoSuchObjectException, MetaException, TException {\n      String dbName = request.getDbName().toLowerCase();\n      String tblName = request.getTblName().toLowerCase();\n      startFunction(\"get_aggr_stats_for\", \": db=\" + request.getDbName()\n          + \" table=\" + request.getTblName());\n\n      List<String> lowerCaseColNames = new ArrayList<String>(request.getColNames().size());\n      for (String colName : request.getColNames()) {\n        lowerCaseColNames.add(colName.toLowerCase());\n      }\n      List<String> lowerCasePartNames = new ArrayList<String>(request.getPartNames().size());\n      for (String partName : request.getPartNames()) {\n        lowerCasePartNames.add(lowerCaseConvertPartName(partName));\n      }\n      AggrStats aggrStats = null;\n\n      try {\n        aggrStats = new AggrStats(getMS().get_aggr_stats_for(dbName, tblName, lowerCasePartNames,\n            lowerCaseColNames));\n        return aggrStats;\n      } finally {\n          endFunction(\"get_aggr_stats_for\", aggrStats == null, null, request.getTblName());\n      }\n\n    }"
        ]
    ],
    "36ea6831f480173befaf28138aa9bf19f8366010": [
        [
            "HiveMetaStoreChecker::checkPartitionDirs(Path,Set,int)",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 -\n 424  \n 425  ",
            "   * Assume that depth is 2, i.e., partition columns are a and b\n   * tblPath/a=1  => throw exception\n   * tblPath/a=1/file => throw exception\n   * tblPath/a=1/b=2/file => return a=1/b=2\n   * tblPath/a=1/b=2/c=3 => return a=1/b=2\n   * tblPath/a=1/b=2/c=3/file => return a=1/b=2\n   *\n   * @param basePath\n   *          Start directory\n   * @param allDirs\n   *          This set will contain the leaf paths at the end.\n   * @param maxDepth\n   *          Specify how deep the search goes.\n   * @throws IOException\n   *           Thrown if we can't get lists from the fs.\n   * @throws HiveException \n   */\n\n  private void checkPartitionDirs(Path basePath, Set<Path> allDirs, int maxDepth) throws IOException, HiveException {\n    ConcurrentLinkedQueue<Path> basePaths = new ConcurrentLinkedQueue<>();\n    basePaths.add(basePath);\n    Set<Path> dirSet = Collections.newSetFromMap(new ConcurrentHashMap<Path, Boolean>());    \n    // Here we just reuse the THREAD_COUNT configuration for\n    // HIVE_MOVE_FILES_THREAD_COUNT\n    final ExecutorService pool = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25) > 0 ? Executors\n        .newFixedThreadPool(conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25),\n            new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"MSCK-GetPaths-%d\").build())\n            : null;\n    if (pool == null) {\n      LOG.debug(\"Not-using threaded version of MSCK-GetPaths\");\n    } else {\n      LOG.debug(\"Using threaded version of MSCK-GetPaths with number of threads \"\n          + ((ThreadPoolExecutor) pool).getPoolSize());\n    }\n    checkPartitionDirs(pool, basePaths, dirSet, basePath.getFileSystem(conf), maxDepth, maxDepth);\n    pool.shutdown();\n    allDirs.addAll(dirSet);\n  }",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424 +\n 425 +\n 426  \n 427  ",
            "   * Assume that depth is 2, i.e., partition columns are a and b\n   * tblPath/a=1  => throw exception\n   * tblPath/a=1/file => throw exception\n   * tblPath/a=1/b=2/file => return a=1/b=2\n   * tblPath/a=1/b=2/c=3 => return a=1/b=2\n   * tblPath/a=1/b=2/c=3/file => return a=1/b=2\n   *\n   * @param basePath\n   *          Start directory\n   * @param allDirs\n   *          This set will contain the leaf paths at the end.\n   * @param maxDepth\n   *          Specify how deep the search goes.\n   * @throws IOException\n   *           Thrown if we can't get lists from the fs.\n   * @throws HiveException \n   */\n\n  private void checkPartitionDirs(Path basePath, Set<Path> allDirs, int maxDepth) throws IOException, HiveException {\n    ConcurrentLinkedQueue<Path> basePaths = new ConcurrentLinkedQueue<>();\n    basePaths.add(basePath);\n    Set<Path> dirSet = Collections.newSetFromMap(new ConcurrentHashMap<Path, Boolean>());    \n    // Here we just reuse the THREAD_COUNT configuration for\n    // HIVE_MOVE_FILES_THREAD_COUNT\n    final ExecutorService pool = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25) > 0 ? Executors\n        .newFixedThreadPool(conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25),\n            new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"MSCK-GetPaths-%d\").build())\n            : null;\n    if (pool == null) {\n      LOG.debug(\"Not-using threaded version of MSCK-GetPaths\");\n    } else {\n      LOG.debug(\"Using threaded version of MSCK-GetPaths with number of threads \"\n          + ((ThreadPoolExecutor) pool).getPoolSize());\n    }\n    checkPartitionDirs(pool, basePaths, dirSet, basePath.getFileSystem(conf), maxDepth, maxDepth);\n    if (pool != null) {\n      pool.shutdown();\n    }\n    allDirs.addAll(dirSet);\n  }"
        ]
    ],
    "6536e30f2ee9707e16d4cd8d7c8321c4073a30b9": [
        [
            "Vectorizer::VectorizationDispatcher::validateInputFormatAndSchemaEvolution(MapWork,String,TableScanOperator,VectorTaskColumnInfo)",
            " 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752 -\n 753 -\n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  ",
            "    private boolean validateInputFormatAndSchemaEvolution(MapWork mapWork, String alias,\n        TableScanOperator tableScanOperator, VectorTaskColumnInfo vectorTaskColumnInfo)\n            throws SemanticException {\n\n      boolean isAcidTable = tableScanOperator.getConf().isAcidTable();\n\n      // These names/types are the data columns plus partition columns.\n      final List<String> allColumnNameList = new ArrayList<String>();\n      final List<TypeInfo> allTypeInfoList = new ArrayList<TypeInfo>();\n\n      getTableScanOperatorSchemaInfo(tableScanOperator, allColumnNameList, allTypeInfoList);\n\n      final List<Integer> dataColumnNums = new ArrayList<Integer>();\n\n      final int allColumnCount = allColumnNameList.size();\n\n      /*\n       * Validate input formats of all the partitions can be vectorized.\n       */\n      boolean isFirst = true;\n      int dataColumnCount = 0;\n      int partitionColumnCount = 0;\n\n      List<String> tableDataColumnList = null;\n      List<TypeInfo> tableDataTypeInfoList = null;\n\n      LinkedHashMap<Path, ArrayList<String>> pathToAliases = mapWork.getPathToAliases();\n      LinkedHashMap<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\n      for (Entry<Path, ArrayList<String>> entry: pathToAliases.entrySet()) {\n        Path path = entry.getKey();\n        List<String> aliases = entry.getValue();\n        boolean isPresent = (aliases != null && aliases.indexOf(alias) != -1);\n        if (!isPresent) {\n          LOG.info(\"Alias \" + alias + \" not present in aliases \" + aliases);\n          return false;\n        }\n        PartitionDesc partDesc = pathToPartitionInfo.get(path);\n        if (partDesc.getVectorPartitionDesc() != null) {\n          // We seen this already.\n          continue;\n        }\n        if (!verifyAndSetVectorPartDesc(partDesc, isAcidTable)) {\n          return false;\n        }\n        VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();\n        if (LOG.isInfoEnabled()) {\n          LOG.info(\"Vectorizer path: \" + path + \", \" + vectorPartDesc.toString() +\n              \", aliases \" + aliases);\n        }\n\n        if (isFirst) {\n\n          // Determine the data and partition columns using the first partition descriptor.\n\n          LinkedHashMap<String, String> partSpec = partDesc.getPartSpec();\n          if (partSpec != null && partSpec.size() > 0) {\n            partitionColumnCount = partSpec.size();\n            dataColumnCount = allColumnCount - partitionColumnCount;\n          } else {\n            partitionColumnCount = 0;\n            dataColumnCount = allColumnCount;\n          }\n\n          determineDataColumnNums(tableScanOperator, allColumnNameList, dataColumnCount,\n              dataColumnNums);\n\n          tableDataColumnList = allColumnNameList.subList(0, dataColumnCount);\n          tableDataTypeInfoList = allTypeInfoList.subList(0, dataColumnCount);\n\n          isFirst = false;\n        }\n\n        // We need to get the partition's column names from the partition serde.\n        // (e.g. Avro provides the table schema and ignores the partition schema..).\n        //\n        Deserializer deserializer;\n        StructObjectInspector partObjectInspector;\n        try {\n          deserializer = partDesc.getDeserializer(hiveConf);\n          partObjectInspector = (StructObjectInspector) deserializer.getObjectInspector();\n        } catch (Exception e) {\n          throw new SemanticException(e);\n        }\n        String nextDataColumnsString = ObjectInspectorUtils.getFieldNames(partObjectInspector);\n        String[] nextDataColumns = nextDataColumnsString.split(\",\");\n        List<String> nextDataColumnList = Arrays.asList(nextDataColumns);\n\n        /*\n         * Validate the column names that are present are the same.  Missing columns will be\n         * implicitly defaulted to null.\n         */\n        if (nextDataColumnList.size() > tableDataColumnList.size()) {\n          LOG.info(\n              String.format(\n                  \"Could not vectorize partition %s \" +\n                  \"(deserializer \" + deserializer.getClass().getName() + \")\" +\n                  \"The partition column names %d is greater than the number of table columns %d\",\n                  path, nextDataColumnList.size(), tableDataColumnList.size()));\n          return false;\n        }\n        if (!(deserializer instanceof NullStructSerDe)) {\n\n          // (Don't insist NullStructSerDe produce correct column names).\n          for (int i = 0; i < nextDataColumnList.size(); i++) {\n            String nextColumnName = nextDataColumnList.get(i);\n            String tableColumnName = tableDataColumnList.get(i);\n            if (!nextColumnName.equals(tableColumnName)) {\n              LOG.info(\n                  String.format(\n                      \"Could not vectorize partition %s \" +\n                      \"(deserializer \" + deserializer.getClass().getName() + \")\" +\n                      \"The partition column name %s is does not match table column name %s\",\n                      path, nextColumnName, tableColumnName));\n              return false;\n            }\n          }\n        }\n\n        List<TypeInfo> nextDataTypeInfoList;\n        if (vectorPartDesc.getIsInputFileFormatSelfDescribing()) {\n\n          /*\n           * Self-Describing Input Format will convert its data to the table schema.\n           */\n          nextDataTypeInfoList = tableDataTypeInfoList;\n\n        } else {\n          String nextDataTypesString = ObjectInspectorUtils.getFieldTypes(partObjectInspector);\n\n          // We convert to an array of TypeInfo using a library routine since it parses the information\n          // and can handle use of different separators, etc.  We cannot use the raw type string\n          // for comparison in the map because of the different separators used.\n          nextDataTypeInfoList =\n              TypeInfoUtils.getTypeInfosFromTypeString(nextDataTypesString);\n        }\n\n        vectorPartDesc.setDataTypeInfos(nextDataTypeInfoList);\n      }\n\n      vectorTaskColumnInfo.setAllColumnNames(allColumnNameList);\n      vectorTaskColumnInfo.setAllTypeInfos(allTypeInfoList);\n      vectorTaskColumnInfo.setDataColumnNums(dataColumnNums);\n      vectorTaskColumnInfo.setPartitionColumnCount(partitionColumnCount);\n      vectorTaskColumnInfo.setUseVectorizedInputFileFormat(useVectorizedInputFileFormat);\n\n      // Helps to keep this for debugging.\n      vectorTaskColumnInfo.setTableScanOperator(tableScanOperator);\n\n      return true;\n    }",
            " 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752 +\n 753 +\n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  ",
            "    private boolean validateInputFormatAndSchemaEvolution(MapWork mapWork, String alias,\n        TableScanOperator tableScanOperator, VectorTaskColumnInfo vectorTaskColumnInfo)\n            throws SemanticException {\n\n      boolean isAcidTable = tableScanOperator.getConf().isAcidTable();\n\n      // These names/types are the data columns plus partition columns.\n      final List<String> allColumnNameList = new ArrayList<String>();\n      final List<TypeInfo> allTypeInfoList = new ArrayList<TypeInfo>();\n\n      getTableScanOperatorSchemaInfo(tableScanOperator, allColumnNameList, allTypeInfoList);\n\n      final List<Integer> dataColumnNums = new ArrayList<Integer>();\n\n      final int allColumnCount = allColumnNameList.size();\n\n      /*\n       * Validate input formats of all the partitions can be vectorized.\n       */\n      boolean isFirst = true;\n      int dataColumnCount = 0;\n      int partitionColumnCount = 0;\n\n      List<String> tableDataColumnList = null;\n      List<TypeInfo> tableDataTypeInfoList = null;\n\n      LinkedHashMap<Path, ArrayList<String>> pathToAliases = mapWork.getPathToAliases();\n      LinkedHashMap<Path, PartitionDesc> pathToPartitionInfo = mapWork.getPathToPartitionInfo();\n      for (Entry<Path, ArrayList<String>> entry: pathToAliases.entrySet()) {\n        Path path = entry.getKey();\n        List<String> aliases = entry.getValue();\n        boolean isPresent = (aliases != null && aliases.indexOf(alias) != -1);\n        if (!isPresent) {\n          LOG.info(\"Alias \" + alias + \" not present in aliases \" + aliases);\n          return false;\n        }\n        PartitionDesc partDesc = pathToPartitionInfo.get(path);\n        if (partDesc.getVectorPartitionDesc() != null) {\n          // We seen this already.\n          continue;\n        }\n        if (!verifyAndSetVectorPartDesc(partDesc, isAcidTable)) {\n          return false;\n        }\n        VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();\n          if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Vectorizer path: \" + path + \", \" + vectorPartDesc.toString() +\n              \", aliases \" + aliases);\n        }\n\n        if (isFirst) {\n\n          // Determine the data and partition columns using the first partition descriptor.\n\n          LinkedHashMap<String, String> partSpec = partDesc.getPartSpec();\n          if (partSpec != null && partSpec.size() > 0) {\n            partitionColumnCount = partSpec.size();\n            dataColumnCount = allColumnCount - partitionColumnCount;\n          } else {\n            partitionColumnCount = 0;\n            dataColumnCount = allColumnCount;\n          }\n\n          determineDataColumnNums(tableScanOperator, allColumnNameList, dataColumnCount,\n              dataColumnNums);\n\n          tableDataColumnList = allColumnNameList.subList(0, dataColumnCount);\n          tableDataTypeInfoList = allTypeInfoList.subList(0, dataColumnCount);\n\n          isFirst = false;\n        }\n\n        // We need to get the partition's column names from the partition serde.\n        // (e.g. Avro provides the table schema and ignores the partition schema..).\n        //\n        Deserializer deserializer;\n        StructObjectInspector partObjectInspector;\n        try {\n          deserializer = partDesc.getDeserializer(hiveConf);\n          partObjectInspector = (StructObjectInspector) deserializer.getObjectInspector();\n        } catch (Exception e) {\n          throw new SemanticException(e);\n        }\n        String nextDataColumnsString = ObjectInspectorUtils.getFieldNames(partObjectInspector);\n        String[] nextDataColumns = nextDataColumnsString.split(\",\");\n        List<String> nextDataColumnList = Arrays.asList(nextDataColumns);\n\n        /*\n         * Validate the column names that are present are the same.  Missing columns will be\n         * implicitly defaulted to null.\n         */\n        if (nextDataColumnList.size() > tableDataColumnList.size()) {\n          LOG.info(\n              String.format(\n                  \"Could not vectorize partition %s \" +\n                  \"(deserializer \" + deserializer.getClass().getName() + \")\" +\n                  \"The partition column names %d is greater than the number of table columns %d\",\n                  path, nextDataColumnList.size(), tableDataColumnList.size()));\n          return false;\n        }\n        if (!(deserializer instanceof NullStructSerDe)) {\n\n          // (Don't insist NullStructSerDe produce correct column names).\n          for (int i = 0; i < nextDataColumnList.size(); i++) {\n            String nextColumnName = nextDataColumnList.get(i);\n            String tableColumnName = tableDataColumnList.get(i);\n            if (!nextColumnName.equals(tableColumnName)) {\n              LOG.info(\n                  String.format(\n                      \"Could not vectorize partition %s \" +\n                      \"(deserializer \" + deserializer.getClass().getName() + \")\" +\n                      \"The partition column name %s is does not match table column name %s\",\n                      path, nextColumnName, tableColumnName));\n              return false;\n            }\n          }\n        }\n\n        List<TypeInfo> nextDataTypeInfoList;\n        if (vectorPartDesc.getIsInputFileFormatSelfDescribing()) {\n\n          /*\n           * Self-Describing Input Format will convert its data to the table schema.\n           */\n          nextDataTypeInfoList = tableDataTypeInfoList;\n\n        } else {\n          String nextDataTypesString = ObjectInspectorUtils.getFieldTypes(partObjectInspector);\n\n          // We convert to an array of TypeInfo using a library routine since it parses the information\n          // and can handle use of different separators, etc.  We cannot use the raw type string\n          // for comparison in the map because of the different separators used.\n          nextDataTypeInfoList =\n              TypeInfoUtils.getTypeInfosFromTypeString(nextDataTypesString);\n        }\n\n        vectorPartDesc.setDataTypeInfos(nextDataTypeInfoList);\n      }\n\n      vectorTaskColumnInfo.setAllColumnNames(allColumnNameList);\n      vectorTaskColumnInfo.setAllTypeInfos(allTypeInfoList);\n      vectorTaskColumnInfo.setDataColumnNums(dataColumnNums);\n      vectorTaskColumnInfo.setPartitionColumnCount(partitionColumnCount);\n      vectorTaskColumnInfo.setUseVectorizedInputFileFormat(useVectorizedInputFileFormat);\n\n      // Helps to keep this for debugging.\n      vectorTaskColumnInfo.setTableScanOperator(tableScanOperator);\n\n      return true;\n    }"
        ],
        [
            "RelOptHiveTable::getColStat(List,boolean)",
            " 413  \n 414  \n 415 -\n 416  \n 417 -\n 418  \n 419 -\n 420  \n 421  \n 422  \n 423  \n 424 -\n 425  \n 426 -\n 427 -\n 428 -\n 429  \n 430  \n 431  \n 432  \n 433  ",
            "  public List<ColStatistics> getColStat(List<Integer> projIndxLst, boolean allowNullColumnForMissingStats) {\n    List<ColStatistics> colStatsBldr = Lists.newArrayList();\n\n    if (projIndxLst != null) {\n      updateColStats(new HashSet<Integer>(projIndxLst), allowNullColumnForMissingStats);\n      for (Integer i : projIndxLst) {\n        colStatsBldr.add(hiveColStatsMap.get(i));\n      }\n    } else {\n      List<Integer> pILst = new ArrayList<Integer>();\n      for (Integer i = 0; i < noOfNonVirtualCols; i++) {\n        pILst.add(i);\n      }\n      updateColStats(new HashSet<Integer>(pILst), allowNullColumnForMissingStats);\n      for (Integer pi : pILst) {\n        colStatsBldr.add(hiveColStatsMap.get(pi));\n      }\n    }\n\n    return colStatsBldr;\n  }",
            " 413  \n 414  \n 415 +\n 416  \n 417  \n 418 +\n 419 +\n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428  \n 429  \n 430  \n 431  \n 432 +\n 433 +\n 434 +\n 435  \n 436 +\n 437 +\n 438 +\n 439 +\n 440 +\n 441  \n 442  \n 443  \n 444  \n 445  ",
            "  public List<ColStatistics> getColStat(List<Integer> projIndxLst, boolean allowNullColumnForMissingStats) {\n    List<ColStatistics> colStatsBldr = Lists.newArrayList();\n    Set<Integer> projIndxSet = new HashSet<Integer>(projIndxLst);\n    if (projIndxLst != null) {\n      for (Integer i : projIndxLst) {\n        if (hiveColStatsMap.get(i) != null) {\n          colStatsBldr.add(hiveColStatsMap.get(i));\n          projIndxSet.remove(i);\n        }\n      }\n      if (!projIndxSet.isEmpty()) {\n        updateColStats(projIndxSet, allowNullColumnForMissingStats);\n        for (Integer i : projIndxSet) {\n          colStatsBldr.add(hiveColStatsMap.get(i));\n        }\n      }\n    } else {\n      List<Integer> pILst = new ArrayList<Integer>();\n      for (Integer i = 0; i < noOfNonVirtualCols; i++) {\n        if (hiveColStatsMap.get(i) == null) {\n          pILst.add(i);\n        }\n      }\n      if (!pILst.isEmpty()) {\n        updateColStats(new HashSet<Integer>(pILst), allowNullColumnForMissingStats);\n        for (Integer pi : pILst) {\n          colStatsBldr.add(hiveColStatsMap.get(pi));\n        }\n      }\n    }\n\n    return colStatsBldr;\n  }"
        ],
        [
            "MetaStoreDirectSql::aggrColStatsForPartitions(String,String,List,List,boolean)",
            "1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191 -\n1192  \n1193  \n1194 -\n1195  \n1196  \n1197  \n1198  \n1199 -\n1200 -\n1201 -\n1202 -\n1203 -\n1204 -\n1205 -\n1206 -\n1207 -\n1208 -\n1209 -\n1210 -\n1211 -\n1212 -\n1213 -\n1214 -\n1215 -\n1216 -\n1217 -\n1218 -\n1219 -\n1220 -\n1221 -\n1222 -\n1223 -\n1224 -\n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  ",
            "  public AggrStats aggrColStatsForPartitions(String dbName, String tableName,\n      List<String> partNames, List<String> colNames, boolean useDensityFunctionForNDVEstimation)\n      throws MetaException {\n    if (colNames.isEmpty() || partNames.isEmpty()) {\n      LOG.debug(\"Columns is empty or partNames is empty : Short-circuiting stats eval\");\n      return new AggrStats(new ArrayList<ColumnStatisticsObj>(), 0); // Nothing to aggregate\n    }\n    long partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n    List<ColumnStatisticsObj> colStatsList;\n    // Try to read from the cache first\n    if (isAggregateStatsCacheEnabled) {\n      AggrColStats colStatsAggrCached;\n      List<ColumnStatisticsObj> colStatsAggrFromDB;\n      int maxPartsPerCacheNode = aggrStatsCache.getMaxPartsPerCacheNode();\n      float fpp = aggrStatsCache.getFalsePositiveProbability();\n      int partitionsRequested = partNames.size();\n      if (partitionsRequested > maxPartsPerCacheNode) {\n        colStatsList = columnStatisticsObjForPartitions(dbName, tableName, partNames, colNames,\n            partsFound, useDensityFunctionForNDVEstimation);\n      } else {\n        colStatsList = new ArrayList<ColumnStatisticsObj>();\n        // Bloom filter for the new node that we will eventually add to the cache\n        BloomFilter bloomFilter = createPartsBloomFilter(maxPartsPerCacheNode, fpp, partNames);\n        for (String colName : colNames) {\n          // Check the cache first\n          colStatsAggrCached = aggrStatsCache.get(dbName, tableName, colName, partNames);\n          if (colStatsAggrCached != null) {\n            colStatsList.add(colStatsAggrCached.getColStats());\n          } else {\n            List<String> colNamesForDB = new ArrayList<String>();\n            colNamesForDB.add(colName);\n            // Read aggregated stats for one column\n            colStatsAggrFromDB =\n                columnStatisticsObjForPartitions(dbName, tableName, partNames, colNamesForDB,\n                    partsFound, useDensityFunctionForNDVEstimation);\n            if (!colStatsAggrFromDB.isEmpty()) {\n              ColumnStatisticsObj colStatsAggr = colStatsAggrFromDB.get(0);\n              colStatsList.add(colStatsAggr);\n              // Update the cache to add this new aggregate node\n              aggrStatsCache.add(dbName, tableName, colName, partsFound, colStatsAggr, bloomFilter);\n            }\n          }\n        }\n      }\n    } else {\n      colStatsList =\n          columnStatisticsObjForPartitions(dbName, tableName, partNames, colNames, partsFound,\n              useDensityFunctionForNDVEstimation);\n    }\n    LOG.info(\"useDensityFunctionForNDVEstimation = \" + useDensityFunctionForNDVEstimation\n        + \"\\npartsFound = \" + partsFound + \"\\nColumnStatisticsObj = \"\n        + Arrays.toString(colStatsList.toArray()));\n    return new AggrStats(colStatsList, partsFound);\n  }",
            "1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191 +\n1192  \n1193  \n1194 +\n1195 +\n1196  \n1197  \n1198  \n1199  \n1200 +\n1201 +\n1202 +\n1203 +\n1204 +\n1205 +\n1206 +\n1207 +\n1208 +\n1209 +\n1210 +\n1211 +\n1212 +\n1213 +\n1214 +\n1215 +\n1216 +\n1217 +\n1218 +\n1219 +\n1220 +\n1221 +\n1222 +\n1223 +\n1224 +\n1225 +\n1226  \n1227  \n1228  \n1229  \n1230 +\n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  ",
            "  public AggrStats aggrColStatsForPartitions(String dbName, String tableName,\n      List<String> partNames, List<String> colNames, boolean useDensityFunctionForNDVEstimation)\n      throws MetaException {\n    if (colNames.isEmpty() || partNames.isEmpty()) {\n      LOG.debug(\"Columns is empty or partNames is empty : Short-circuiting stats eval\");\n      return new AggrStats(new ArrayList<ColumnStatisticsObj>(), 0); // Nothing to aggregate\n    }\n    long partsFound = 0;\n    List<ColumnStatisticsObj> colStatsList;\n    // Try to read from the cache first\n    if (isAggregateStatsCacheEnabled\n        && (partNames.size() < aggrStatsCache.getMaxPartsPerCacheNode())) {\n      AggrColStats colStatsAggrCached;\n      List<ColumnStatisticsObj> colStatsAggrFromDB;\n      int maxPartsPerCacheNode = aggrStatsCache.getMaxPartsPerCacheNode();\n      float fpp = aggrStatsCache.getFalsePositiveProbability();\n      colStatsList = new ArrayList<ColumnStatisticsObj>();\n      // Bloom filter for the new node that we will eventually add to the cache\n      BloomFilter bloomFilter = createPartsBloomFilter(maxPartsPerCacheNode, fpp, partNames);\n      boolean computePartsFound = true;\n      for (String colName : colNames) {\n        // Check the cache first\n        colStatsAggrCached = aggrStatsCache.get(dbName, tableName, colName, partNames);\n        if (colStatsAggrCached != null) {\n          colStatsList.add(colStatsAggrCached.getColStats());\n          partsFound = colStatsAggrCached.getNumPartsCached();\n        } else {\n          if (computePartsFound) {\n            partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n            computePartsFound = false;\n          }\n          List<String> colNamesForDB = new ArrayList<String>();\n          colNamesForDB.add(colName);\n          // Read aggregated stats for one column\n          colStatsAggrFromDB =\n              columnStatisticsObjForPartitions(dbName, tableName, partNames, colNamesForDB,\n                  partsFound, useDensityFunctionForNDVEstimation);\n          if (!colStatsAggrFromDB.isEmpty()) {\n            ColumnStatisticsObj colStatsAggr = colStatsAggrFromDB.get(0);\n            colStatsList.add(colStatsAggr);\n            // Update the cache to add this new aggregate node\n            aggrStatsCache.add(dbName, tableName, colName, partsFound, colStatsAggr, bloomFilter);\n          }\n        }\n      }\n    } else {\n      partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n      colStatsList =\n          columnStatisticsObjForPartitions(dbName, tableName, partNames, colNames, partsFound,\n              useDensityFunctionForNDVEstimation);\n    }\n    LOG.info(\"useDensityFunctionForNDVEstimation = \" + useDensityFunctionForNDVEstimation\n        + \"\\npartsFound = \" + partsFound + \"\\nColumnStatisticsObj = \"\n        + Arrays.toString(colStatsList.toArray()));\n    return new AggrStats(colStatsList, partsFound);\n  }"
        ]
    ],
    "cf87b0e244d141a461c6b423b198680c9e6250fc": [
        [
            "HiveHFileOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 -\n 175 -\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "  @Override\n  public RecordWriter getHiveRecordWriter(\n    final JobConf jc,\n    final Path finalOutPath,\n    Class<? extends Writable> valueClass,\n    boolean isCompressed,\n    Properties tableProperties,\n    final Progressable progressable) throws IOException {\n\n    // Read configuration for the target path, first from jobconf, then from table properties\n    String hfilePath = getFamilyPath(jc, tableProperties);\n    if (hfilePath == null) {\n      throw new RuntimeException(\n        \"Please set \" + HFILE_FAMILY_PATH + \" to target location for HFiles\");\n    }\n\n    // Target path's last component is also the column family name.\n    final Path columnFamilyPath = new Path(hfilePath);\n    final String columnFamilyName = columnFamilyPath.getName();\n    final byte [] columnFamilyNameBytes = Bytes.toBytes(columnFamilyName);\n    final Job job = new Job(jc);\n    setCompressOutput(job, isCompressed);\n    setOutputPath(job, finalOutPath);\n\n    // Create the HFile writer\n    final org.apache.hadoop.mapreduce.TaskAttemptContext tac =\n      ShimLoader.getHadoopShims().newTaskAttemptContext(\n          job.getConfiguration(), progressable);\n\n    final Path outputdir = FileOutputFormat.getOutputPath(tac);\n    final org.apache.hadoop.mapreduce.RecordWriter<\n      ImmutableBytesWritable, KeyValue> fileWriter = getFileWriter(tac);\n\n    // Individual columns are going to be pivoted to HBase cells,\n    // and for each row, they need to be written out in order\n    // of column name, so sort the column names now, creating a\n    // mapping to their column position.  However, the first\n    // column is interpreted as the row key.\n    String columnList = tableProperties.getProperty(\"columns\");\n    String [] columnArray = columnList.split(\",\");\n    final SortedMap<byte [], Integer> columnMap =\n      new TreeMap<byte [], Integer>(Bytes.BYTES_COMPARATOR);\n    int i = 0;\n    for (String columnName : columnArray) {\n      if (i != 0) {\n        columnMap.put(Bytes.toBytes(columnName), i);\n      }\n      ++i;\n    }\n\n    return new RecordWriter() {\n\n      @Override\n      public void close(boolean abort) throws IOException {\n        try {\n          fileWriter.close(null);\n          if (abort) {\n            return;\n          }\n          // Move the hfiles file(s) from the task output directory to the\n          // location specified by the user.\n          FileSystem fs = outputdir.getFileSystem(jc);\n          fs.mkdirs(columnFamilyPath);\n          Path srcDir = outputdir;\n          for (;;) {\n            FileStatus [] files = fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER);\n            if ((files == null) || (files.length == 0)) {\n              throw new IOException(\"No family directories found in \" + srcDir);\n            }\n            if (files.length != 1) {\n              throw new IOException(\"Multiple family directories found in \" + srcDir);\n            }\n            srcDir = files[0].getPath();\n            if (srcDir.getName().equals(columnFamilyName)) {\n              break;\n            }\n          }\n          for (FileStatus regionFile : fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER)) {\n            fs.rename(\n              regionFile.getPath(),\n              new Path(\n                columnFamilyPath,\n                regionFile.getPath().getName()));\n          }\n          // Hive actually wants a file as task output (not a directory), so\n          // replace the empty directory with an empty file to keep it happy.\n          fs.delete(outputdir, true);\n          fs.createNewFile(outputdir);\n        } catch (InterruptedException ex) {\n          throw new IOException(ex);\n        }\n      }\n\n      private void writeText(Text text) throws IOException {\n        // Decompose the incoming text row into fields.\n        String s = text.toString();\n        String [] fields = s.split(\"\\u0001\");\n        assert(fields.length <= (columnMap.size() + 1));\n        // First field is the row key.\n        byte [] rowKeyBytes = Bytes.toBytes(fields[0]);\n        // Remaining fields are cells addressed by column name within row.\n        for (Map.Entry<byte [], Integer> entry : columnMap.entrySet()) {\n          byte [] columnNameBytes = entry.getKey();\n          int iColumn = entry.getValue();\n          String val;\n          if (iColumn >= fields.length) {\n            // trailing blank field\n            val = \"\";\n          } else {\n            val = fields[iColumn];\n            if (\"\\\\N\".equals(val)) {\n              // omit nulls\n              continue;\n            }\n          }\n          byte [] valBytes = Bytes.toBytes(val);\n          KeyValue kv = new KeyValue(\n            rowKeyBytes,\n            columnFamilyNameBytes,\n            columnNameBytes,\n            valBytes);\n          try {\n            fileWriter.write(null, kv);\n          } catch (IOException e) {\n            LOG.error(\"Failed while writing row: \" + s);\n            throw e;\n          } catch (InterruptedException ex) {\n            throw new IOException(ex);\n          }\n        }\n      }\n\n      private void writePut(PutWritable put) throws IOException {\n        ImmutableBytesWritable row = new ImmutableBytesWritable(put.getPut().getRow());\n        SortedMap<byte[], List<Cell>> cells = put.getPut().getFamilyCellMap();\n        for (Map.Entry<byte[], List<Cell>> entry : cells.entrySet()) {\n          Collections.sort(entry.getValue(), new CellComparator());\n          for (Cell c : entry.getValue()) {\n            try {\n              fileWriter.write(row, KeyValueUtil.copyToNewKeyValue(c));\n            } catch (InterruptedException e) {\n              throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n            }\n          }\n        }\n      }\n\n      @Override\n      public void write(Writable w) throws IOException {\n        if (w instanceof Text) {\n          writeText((Text) w);\n        } else if (w instanceof PutWritable) {\n          writePut((PutWritable) w);\n        } else {\n          throw new IOException(\"Unexpected writable \" + w);\n        }\n      }\n    };\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 +\n 182 +\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  ",
            "  @Override\n  public RecordWriter getHiveRecordWriter(\n    final JobConf jc,\n    final Path finalOutPath,\n    Class<? extends Writable> valueClass,\n    boolean isCompressed,\n    Properties tableProperties,\n    final Progressable progressable) throws IOException {\n\n    // Read configuration for the target path, first from jobconf, then from table properties\n    String hfilePath = getFamilyPath(jc, tableProperties);\n    if (hfilePath == null) {\n      throw new RuntimeException(\n        \"Please set \" + HFILE_FAMILY_PATH + \" to target location for HFiles\");\n    }\n\n    // Target path's last component is also the column family name.\n    final Path columnFamilyPath = new Path(hfilePath);\n    final String columnFamilyName = columnFamilyPath.getName();\n    final byte [] columnFamilyNameBytes = Bytes.toBytes(columnFamilyName);\n    final Job job = new Job(jc);\n    setCompressOutput(job, isCompressed);\n    setOutputPath(job, finalOutPath);\n\n    // Create the HFile writer\n    final org.apache.hadoop.mapreduce.TaskAttemptContext tac =\n      ShimLoader.getHadoopShims().newTaskAttemptContext(\n          job.getConfiguration(), progressable);\n\n    final Path outputdir = FileOutputFormat.getOutputPath(tac);\n    final Path taskAttemptOutputdir = FileOutputCommitter.getTaskAttemptPath(tac, outputdir);\n    final org.apache.hadoop.mapreduce.RecordWriter<\n      ImmutableBytesWritable, KeyValue> fileWriter = getFileWriter(tac);\n\n    // Individual columns are going to be pivoted to HBase cells,\n    // and for each row, they need to be written out in order\n    // of column name, so sort the column names now, creating a\n    // mapping to their column position.  However, the first\n    // column is interpreted as the row key.\n    String columnList = tableProperties.getProperty(\"columns\");\n    String [] columnArray = columnList.split(\",\");\n    final SortedMap<byte [], Integer> columnMap =\n      new TreeMap<byte [], Integer>(Bytes.BYTES_COMPARATOR);\n    int i = 0;\n    for (String columnName : columnArray) {\n      if (i != 0) {\n        columnMap.put(Bytes.toBytes(columnName), i);\n      }\n      ++i;\n    }\n\n    return new RecordWriter() {\n\n      @Override\n      public void close(boolean abort) throws IOException {\n        try {\n          fileWriter.close(null);\n          if (abort) {\n            return;\n          }\n          // Move the hfiles file(s) from the task output directory to the\n          // location specified by the user.\n          FileSystem fs = outputdir.getFileSystem(jc);\n          fs.mkdirs(columnFamilyPath);\n          Path srcDir = taskAttemptOutputdir;\n          for (;;) {\n            FileStatus [] files = fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER);\n            if ((files == null) || (files.length == 0)) {\n              throw new IOException(\"No family directories found in \" + srcDir);\n            }\n            if (files.length != 1) {\n              throw new IOException(\"Multiple family directories found in \" + srcDir);\n            }\n            srcDir = files[0].getPath();\n            if (srcDir.getName().equals(columnFamilyName)) {\n              break;\n            }\n            if (files[0].isFile()) {\n              throw new IOException(\"No family directories found in \" + taskAttemptOutputdir + \". \"\n                  + \"The last component in hfile path should match column family name \"\n                  + columnFamilyName);\n            }\n          }\n          for (FileStatus regionFile : fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER)) {\n            fs.rename(\n              regionFile.getPath(),\n              new Path(\n                columnFamilyPath,\n                regionFile.getPath().getName()));\n          }\n          // Hive actually wants a file as task output (not a directory), so\n          // replace the empty directory with an empty file to keep it happy.\n          fs.delete(taskAttemptOutputdir, true);\n          fs.createNewFile(taskAttemptOutputdir);\n        } catch (InterruptedException ex) {\n          throw new IOException(ex);\n        }\n      }\n\n      private void writeText(Text text) throws IOException {\n        // Decompose the incoming text row into fields.\n        String s = text.toString();\n        String [] fields = s.split(\"\\u0001\");\n        assert(fields.length <= (columnMap.size() + 1));\n        // First field is the row key.\n        byte [] rowKeyBytes = Bytes.toBytes(fields[0]);\n        // Remaining fields are cells addressed by column name within row.\n        for (Map.Entry<byte [], Integer> entry : columnMap.entrySet()) {\n          byte [] columnNameBytes = entry.getKey();\n          int iColumn = entry.getValue();\n          String val;\n          if (iColumn >= fields.length) {\n            // trailing blank field\n            val = \"\";\n          } else {\n            val = fields[iColumn];\n            if (\"\\\\N\".equals(val)) {\n              // omit nulls\n              continue;\n            }\n          }\n          byte [] valBytes = Bytes.toBytes(val);\n          KeyValue kv = new KeyValue(\n            rowKeyBytes,\n            columnFamilyNameBytes,\n            columnNameBytes,\n            valBytes);\n          try {\n            fileWriter.write(null, kv);\n          } catch (IOException e) {\n            LOG.error(\"Failed while writing row: \" + s);\n            throw e;\n          } catch (InterruptedException ex) {\n            throw new IOException(ex);\n          }\n        }\n      }\n\n      private void writePut(PutWritable put) throws IOException {\n        ImmutableBytesWritable row = new ImmutableBytesWritable(put.getPut().getRow());\n        SortedMap<byte[], List<Cell>> cells = put.getPut().getFamilyCellMap();\n        for (Map.Entry<byte[], List<Cell>> entry : cells.entrySet()) {\n          Collections.sort(entry.getValue(), new CellComparator());\n          for (Cell c : entry.getValue()) {\n            try {\n              fileWriter.write(row, KeyValueUtil.copyToNewKeyValue(c));\n            } catch (InterruptedException e) {\n              throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n            }\n          }\n        }\n      }\n\n      @Override\n      public void write(Writable w) throws IOException {\n        if (w instanceof Text) {\n          writeText((Text) w);\n        } else if (w instanceof PutWritable) {\n          writePut((PutWritable) w);\n        } else {\n          throw new IOException(\"Unexpected writable \" + w);\n        }\n      }\n    };\n  }"
        ]
    ],
    "df9b2b57a05499c4848e95a24a591154640e40fb": [
        [
            "DruidQueryRecordReader::initialize(InputSplit,Configuration)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "  public void initialize(InputSplit split, Configuration conf) throws IOException {\n    HiveDruidSplit hiveDruidSplit = (HiveDruidSplit) split;\n\n    // Create query\n    query = createQuery(hiveDruidSplit.getDruidQuery());\n\n    // Execute query\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Retrieving from druid using query:\\n \" + query);\n    }\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response = DruidStorageHandlerUtils.submitRequest(client,\n            DruidStorageHandlerUtils.createRequest(hiveDruidSplit.getAddress(), query));\n\n    // Retrieve results\n    List<R> resultsList;\n    try {\n      resultsList = createResultsList(response);\n    } catch (IOException e) {\n      response.close();\n      throw e;\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      return;\n    }\n    results = resultsList.iterator();\n  }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  public void initialize(InputSplit split, Configuration conf) throws IOException {\n    HiveDruidSplit hiveDruidSplit = (HiveDruidSplit) split;\n\n    // Create query\n    query = createQuery(hiveDruidSplit.getDruidQuery());\n\n    // Execute query\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Retrieving from druid using query:\\n \" + query);\n    }\n\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withReadTimeout(readTimeout.toStandardDuration())\n                    .withNumConnections(numConnection).build(), new Lifecycle());\n    InputStream response = DruidStorageHandlerUtils.submitRequest(client,\n            DruidStorageHandlerUtils.createRequest(hiveDruidSplit.getAddress(), query));\n\n    // Retrieve results\n    List<R> resultsList;\n    try {\n      resultsList = createResultsList(response);\n    } catch (IOException e) {\n      response.close();\n      throw e;\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      return;\n    }\n    results = resultsList.iterator();\n  }"
        ],
        [
            "DruidSerDe::initialize(Configuration,Properties)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. We need to create a Druid Segment Metadata query that retrieves all\n      // columns present in the data source (dimensions and metrics).\n      // Create Segment Metadata Query\n      String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null) {\n        throw new SerDeException(\"Druid data source not specified; use \" +\n                Constants.DRUID_DATA_SOURCE + \" in table properties\");\n      }\n      SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n      builder.dataSource(dataSource);\n      builder.merge(true);\n      builder.analysisTypes();\n      SegmentMetadataQuery query = builder.build();\n\n      // Execute query in Druid\n      String address = HiveConf.getVar(configuration,\n              HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n      if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n        throw new SerDeException(\"Druid broker address not specified in configuration\");\n      }\n\n      // Infer schema\n      SegmentAnalysis schemaInfo;\n      try {\n        schemaInfo = submitMetadataRequest(address, query);\n      } catch (IOException e) {\n        throw new SerDeException(e);\n      }\n      for (Entry<String,ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n        if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          // Special handling for timestamp column\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n          continue;\n        }\n        columnNames.add(columnInfo.getKey()); // field name\n        PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                columnInfo.getValue().getType()); // field type\n        columnTypes.add(type);\n        inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n      }\n      columns = columnNames.toArray(new String[columnNames.size()]);\n      types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      switch (query.getType()) {\n        case Query.TIMESERIES:\n          inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n          break;\n        case Query.TOPN:\n          inferSchema((TopNQuery) query, columnNames, columnTypes);\n          break;\n        case Query.SELECT:\n          inferSchema((SelectQuery) query, columnNames, columnTypes);\n          break;\n        case Query.GROUP_BY:\n          inferSchema((GroupByQuery) query, columnNames, columnTypes);\n          break;\n        default:\n          throw new SerDeException(\"Not supported Druid query\");\n      }\n    \n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121 +\n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. We need to create a Druid Segment Metadata query that retrieves all\n      // columns present in the data source (dimensions and metrics).\n      // Create Segment Metadata Query\n      String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n      if (dataSource == null) {\n        throw new SerDeException(\"Druid data source not specified; use \" +\n                Constants.DRUID_DATA_SOURCE + \" in table properties\");\n      }\n      SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n      builder.dataSource(dataSource);\n      builder.merge(true);\n      builder.analysisTypes();\n      SegmentMetadataQuery query = builder.build();\n\n      // Execute query in Druid\n      String address = HiveConf.getVar(configuration,\n              HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n      if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n        throw new SerDeException(\"Druid broker address not specified in configuration\");\n      }\n\n      numConnection = HiveConf\n              .getIntVar(configuration, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n      readTimeout = new Period(\n              HiveConf.getVar(configuration, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n      // Infer schema\n      SegmentAnalysis schemaInfo;\n      try {\n        schemaInfo = submitMetadataRequest(address, query);\n      } catch (IOException e) {\n        throw new SerDeException(e);\n      }\n      for (Entry<String,ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n        if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          // Special handling for timestamp column\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n          continue;\n        }\n        columnNames.add(columnInfo.getKey()); // field name\n        PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                columnInfo.getValue().getType()); // field type\n        columnTypes.add(type);\n        inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n      }\n      columns = columnNames.toArray(new String[columnNames.size()]);\n      types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      switch (query.getType()) {\n        case Query.TIMESERIES:\n          inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n          break;\n        case Query.TOPN:\n          inferSchema((TopNQuery) query, columnNames, columnTypes);\n          break;\n        case Query.SELECT:\n          inferSchema((SelectQuery) query, columnNames, columnTypes);\n          break;\n        case Query.GROUP_BY:\n          inferSchema((GroupByQuery) query, columnNames, columnTypes);\n          break;\n        default:\n          throw new SerDeException(\"Not supported Druid query\");\n      }\n    \n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }"
        ],
        [
            "DruidSerDe::submitMetadataRequest(String,SegmentMetadataQuery)",
            " 185  \n 186  \n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "  protected SegmentAnalysis submitMetadataRequest(String address, SegmentMetadataQuery query)\n          throws SerDeException, IOException {\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, query));\n    } catch (Exception e) {\n      throw new SerDeException(StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> resultsList;\n    try {\n      resultsList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new SerDeException(StringUtils.stringifyException(e));\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      throw new SerDeException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (resultsList.size() != 1) {\n      throw new SerDeException(\"Information about segments should have been merged\");\n    }\n\n    return resultsList.get(0);\n  }",
            " 193  \n 194  \n 195 +\n 196 +\n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "  protected SegmentAnalysis submitMetadataRequest(String address, SegmentMetadataQuery query)\n          throws SerDeException, IOException {\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, query));\n    } catch (Exception e) {\n      throw new SerDeException(StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> resultsList;\n    try {\n      resultsList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new SerDeException(StringUtils.stringifyException(e));\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      throw new SerDeException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (resultsList.size() != 1) {\n      throw new SerDeException(\"Information about segments should have been merged\");\n    }\n\n    return resultsList.get(0);\n  }"
        ],
        [
            "HiveDruidQueryBasedInputFormat::splitSelectQuery(Configuration,String,String,Path)",
            " 159  \n 160  \n 161  \n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(HttpClientConfig.builder().build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }",
            " 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192 +\n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), new Lifecycle());\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery));\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n            new TypeReference<List<SegmentAnalysis>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null || metadataList.isEmpty()) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery));\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<Result<TimeBoundaryResultValue>>>() {});\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath);\n    }\n    return splits;\n  }"
        ]
    ],
    "707bca7ae5c3fb6a7a350ca2a89ebbf874f0de52": [
        [
            "VectorizationContext::getCustomUDFExpression(ExprNodeGenericFuncDesc,VectorExpressionDescriptor)",
            "2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178 -\n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185 -\n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  ",
            "  private VectorExpression getCustomUDFExpression(ExprNodeGenericFuncDesc expr, VectorExpressionDescriptor.Mode mode)\n      throws HiveException {\n\n    boolean isFilter = false;    // Assume.\n    if (mode == VectorExpressionDescriptor.Mode.FILTER) {\n\n      // Is output type a BOOLEAN?\n      TypeInfo resultTypeInfo = expr.getTypeInfo();\n      if (resultTypeInfo.getCategory() == Category.PRIMITIVE &&\n          ((PrimitiveTypeInfo) resultTypeInfo).getPrimitiveCategory() == PrimitiveCategory.BOOLEAN) {\n        isFilter = true;\n      } else {\n        return null;\n      }\n    }\n\n    //GenericUDFBridge udfBridge = (GenericUDFBridge) expr.getGenericUDF();\n    List<ExprNodeDesc> childExprList = expr.getChildren();\n\n    // argument descriptors\n    VectorUDFArgDesc[] argDescs = new VectorUDFArgDesc[expr.getChildren().size()];\n    for (int i = 0; i < argDescs.length; i++) {\n      argDescs[i] = new VectorUDFArgDesc();\n    }\n\n    // positions of variable arguments (columns or non-constant expressions)\n    List<Integer> variableArgPositions = new ArrayList<Integer>();\n\n    // Column numbers of batch corresponding to expression result arguments\n    List<Integer> exprResultColumnNums = new ArrayList<Integer>();\n\n    // Prepare children\n    List<VectorExpression> vectorExprs = new ArrayList<VectorExpression>();\n\n    for (int i = 0; i < childExprList.size(); i++) {\n      ExprNodeDesc child = childExprList.get(i);\n      if (child instanceof ExprNodeGenericFuncDesc) {\n        VectorExpression e = getVectorExpression(child, VectorExpressionDescriptor.Mode.PROJECTION);\n        vectorExprs.add(e);\n        variableArgPositions.add(i);\n        exprResultColumnNums.add(e.getOutputColumn());\n        argDescs[i].setVariable(e.getOutputColumn());\n      } else if (child instanceof ExprNodeColumnDesc) {\n        variableArgPositions.add(i);\n        argDescs[i].setVariable(getInputColumnIndex(((ExprNodeColumnDesc) child).getColumn()));\n      } else if (child instanceof ExprNodeConstantDesc) {\n        // this is a constant (or null)\n        argDescs[i].setConstant((ExprNodeConstantDesc) child);\n      } else {\n        throw new HiveException(\"Unable to vectorize custom UDF. Encountered unsupported expr desc : \"\n            + child);\n      }\n    }\n\n    // Allocate output column and get column number;\n    int outputCol = -1;\n    String resultTypeName = expr.getTypeInfo().getTypeName();\n\n    outputCol = ocm.allocateOutputColumn(resultTypeName);\n\n    // Make vectorized operator\n    String normalizedName = getNormalizedName(resultTypeName);\n\n    VectorExpression ve = new VectorUDFAdaptor(expr, outputCol, normalizedName, argDescs);\n\n    // Set child expressions\n    VectorExpression[] childVEs = null;\n    if (exprResultColumnNums.size() != 0) {\n      childVEs = new VectorExpression[exprResultColumnNums.size()];\n      for (int i = 0; i < childVEs.length; i++) {\n        childVEs[i] = vectorExprs.get(i);\n      }\n    }\n    ve.setChildExpressions(childVEs);\n\n    // Free output columns if inputs have non-leaf expression trees.\n    for (Integer i : exprResultColumnNums) {\n      ocm.freeOutputColumn(i);\n    }\n\n    if (isFilter) {\n      SelectColumnIsTrue filterVectorExpr = new SelectColumnIsTrue(outputCol);\n      filterVectorExpr.setChildExpressions(new VectorExpression[] {ve});\n      return filterVectorExpr;\n    } else {\n      return ve;\n    }\n  }",
            "2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172 +\n2173 +\n2174 +\n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181 +\n2182 +\n2183 +\n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190 +\n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  ",
            "  private VectorExpression getCustomUDFExpression(ExprNodeGenericFuncDesc expr, VectorExpressionDescriptor.Mode mode)\n      throws HiveException {\n\n    boolean isFilter = false;    // Assume.\n    if (mode == VectorExpressionDescriptor.Mode.FILTER) {\n\n      // Is output type a BOOLEAN?\n      TypeInfo resultTypeInfo = expr.getTypeInfo();\n      if (resultTypeInfo.getCategory() == Category.PRIMITIVE &&\n          ((PrimitiveTypeInfo) resultTypeInfo).getPrimitiveCategory() == PrimitiveCategory.BOOLEAN) {\n        isFilter = true;\n      } else {\n        return null;\n      }\n    }\n\n    //GenericUDFBridge udfBridge = (GenericUDFBridge) expr.getGenericUDF();\n    List<ExprNodeDesc> childExprList = expr.getChildren();\n\n    // argument descriptors\n    VectorUDFArgDesc[] argDescs = new VectorUDFArgDesc[expr.getChildren().size()];\n    for (int i = 0; i < argDescs.length; i++) {\n      argDescs[i] = new VectorUDFArgDesc();\n    }\n\n    // positions of variable arguments (columns or non-constant expressions)\n    List<Integer> variableArgPositions = new ArrayList<Integer>();\n\n    // Column numbers of batch corresponding to expression result arguments\n    List<Integer> exprResultColumnNums = new ArrayList<Integer>();\n\n    // Prepare children\n    List<VectorExpression> vectorExprs = new ArrayList<VectorExpression>();\n\n    for (int i = 0; i < childExprList.size(); i++) {\n      ExprNodeDesc child = childExprList.get(i);\n      /*\n      UNDONE: Until we fix scratch column allocation to not release after each expression, we\n      UNDONE: cannot have another other than a column or constant in the parameter list.\n      if (child instanceof ExprNodeGenericFuncDesc) {\n        VectorExpression e = getVectorExpression(child, VectorExpressionDescriptor.Mode.PROJECTION);\n        vectorExprs.add(e);\n        variableArgPositions.add(i);\n        exprResultColumnNums.add(e.getOutputColumn());\n        argDescs[i].setVariable(e.getOutputColumn());\n      } else \n      */\n      if (child instanceof ExprNodeColumnDesc) {\n        variableArgPositions.add(i);\n        argDescs[i].setVariable(getInputColumnIndex(((ExprNodeColumnDesc) child).getColumn()));\n      } else if (child instanceof ExprNodeConstantDesc) {\n        // this is a constant (or null)\n        argDescs[i].setConstant((ExprNodeConstantDesc) child);\n      } else {\n        throw new HiveException(\"Unable to use the VectorUDFAdaptor. Encountered unsupported expr desc : \"\n            + child);\n      }\n    }\n\n    // Allocate output column and get column number;\n    int outputCol = -1;\n    String resultTypeName = expr.getTypeInfo().getTypeName();\n\n    outputCol = ocm.allocateOutputColumn(resultTypeName);\n\n    // Make vectorized operator\n    String normalizedName = getNormalizedName(resultTypeName);\n\n    VectorExpression ve = new VectorUDFAdaptor(expr, outputCol, normalizedName, argDescs);\n\n    // Set child expressions\n    VectorExpression[] childVEs = null;\n    if (exprResultColumnNums.size() != 0) {\n      childVEs = new VectorExpression[exprResultColumnNums.size()];\n      for (int i = 0; i < childVEs.length; i++) {\n        childVEs[i] = vectorExprs.get(i);\n      }\n    }\n    ve.setChildExpressions(childVEs);\n\n    // Free output columns if inputs have non-leaf expression trees.\n    for (Integer i : exprResultColumnNums) {\n      ocm.freeOutputColumn(i);\n    }\n\n    if (isFilter) {\n      SelectColumnIsTrue filterVectorExpr = new SelectColumnIsTrue(outputCol);\n      filterVectorExpr.setChildExpressions(new VectorExpression[] {ve});\n      return filterVectorExpr;\n    } else {\n      return ve;\n    }\n  }"
        ]
    ],
    "0c94b11b21bb927d4c763a594143451d40d8a9ad": [
        [
            "Vectorizer::validateAggregationDesc(AggregationDesc,ProcessingMode,boolean)",
            "1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938 -\n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  ",
            "  private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, ProcessingMode processingMode,\n      boolean hasKeys) {\n\n    String udfName = aggDesc.getGenericUDAFName().toLowerCase();\n    if (!supportedAggregationUdfs.contains(udfName)) {\n      LOG.info(\"Cannot vectorize groupby aggregate expression: UDF \" + udfName + \" not supported\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n    if (aggDesc.getParameters() != null && !validateExprNodeDesc(aggDesc.getParameters())) {\n      LOG.info(\"Cannot vectorize groupby aggregate expression: UDF parameters not supported\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n\n    // See if we can vectorize the aggregation.\n    VectorizationContext vc = new ValidatorVectorizationContext(hiveConf);\n    VectorAggregateExpression vectorAggrExpr;\n    try {\n        vectorAggrExpr = vc.getAggregatorExpression(aggDesc);\n    } catch (Exception e) {\n      // We should have already attempted to vectorize in validateAggregationDesc.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Vectorization of aggreation should have succeeded \", e);\n      }\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Aggregation \" + aggDesc.getExprString() + \" --> \" +\n          \" vector expression \" + vectorAggrExpr.toString());\n    }\n\n    boolean outputIsPrimitive = validateAggregationIsPrimitive(vectorAggrExpr);\n    if (processingMode == ProcessingMode.MERGE_PARTIAL &&\n        hasKeys &&\n        !outputIsPrimitive) {\n      LOG.info(\"Vectorized Reduce MergePartial GROUP BY keys can only handle aggregate outputs that are primitive types\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n\n    return new Pair<Boolean,Boolean>(true, outputIsPrimitive);\n  }",
            "1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938 +\n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  ",
            "  private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, ProcessingMode processingMode,\n      boolean hasKeys) {\n\n    String udfName = aggDesc.getGenericUDAFName().toLowerCase();\n    if (!supportedAggregationUdfs.contains(udfName)) {\n      LOG.info(\"Cannot vectorize groupby aggregate expression: UDF \" + udfName + \" not supported\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n    if (aggDesc.getParameters() != null && !validateExprNodeDesc(aggDesc.getParameters())) {\n      LOG.info(\"Cannot vectorize groupby aggregate expression: UDF parameters not supported\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n\n    // See if we can vectorize the aggregation.\n    VectorizationContext vc = new ValidatorVectorizationContext(hiveConf);\n    VectorAggregateExpression vectorAggrExpr;\n    try {\n        vectorAggrExpr = vc.getAggregatorExpression(aggDesc);\n    } catch (Exception e) {\n      // We should have already attempted to vectorize in validateAggregationDesc.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Vectorization of aggregation should have succeeded \", e);\n      }\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Aggregation \" + aggDesc.getExprString() + \" --> \" +\n          \" vector expression \" + vectorAggrExpr.toString());\n    }\n\n    boolean outputIsPrimitive = validateAggregationIsPrimitive(vectorAggrExpr);\n    if (processingMode == ProcessingMode.MERGE_PARTIAL &&\n        hasKeys &&\n        !outputIsPrimitive) {\n      LOG.info(\"Vectorized Reduce MergePartial GROUP BY keys can only handle aggregate outputs that are primitive types\");\n      return new Pair<Boolean,Boolean>(false, false);\n    }\n\n    return new Pair<Boolean,Boolean>(true, outputIsPrimitive);\n  }"
        ],
        [
            "ConstantPropagateProcFactory::foldExprShortcut(ExprNodeDesc,Map,ConstantPropagateProcCtx,Operator,int,boolean)",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360 -\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Fold input expression desc, only performing short-cutting.\n   *\n   * Unnecessary AND/OR operations involving a constant true/false value will be eliminated.\n   *\n   * @param desc folding expression\n   * @param constants current propagated constant map\n   * @param cppCtx\n   * @param op processing operator\n   * @param propagate if true, assignment expressions will be added to constants.\n   * @return fold expression\n   * @throws UDFArgumentException\n   */\n  private static ExprNodeDesc foldExprShortcut(ExprNodeDesc desc, Map<ColumnInfo, ExprNodeDesc> constants,\n      ConstantPropagateProcCtx cppCtx, Operator<? extends Serializable> op, int tag,\n      boolean propagate) throws UDFArgumentException {\n    // Combine NOT operator with the child operator. Otherwise, the following optimization\n    // from bottom up could lead to incorrect result, such as not(x > 3 and x is not null),\n    // should not be optimized to not(x > 3), but (x <=3 or x is null).\n    desc = foldNegative(desc);\n\n    if (desc instanceof ExprNodeGenericFuncDesc) {\n      ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;\n\n      GenericUDF udf = funcDesc.getGenericUDF();\n\n      boolean propagateNext = propagate && propagatableUdfs.contains(udf.getClass());\n      List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();\n      for (ExprNodeDesc childExpr : desc.getChildren()) {\n        newExprs.add(foldExpr(childExpr, constants, cppCtx, op, tag, propagateNext));\n      }\n\n      // Don't evaluate nondeterministic function since the value can only calculate during runtime.\n      if (!isDeterministicUdf(udf, newExprs)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Function \" + udf.getClass() + \" is undeterministic. Don't evalulate immediately.\");\n        }\n        ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n        return desc;\n      }\n\n      // Check if the function can be short cut.\n      ExprNodeDesc shortcut = shortcutFunction(udf, newExprs, op);\n      if (shortcut != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Folding expression:\" + desc + \" -> \" + shortcut);\n        }\n        return shortcut;\n      }\n      ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n    }\n    return desc;\n  }",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360 +\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Fold input expression desc, only performing short-cutting.\n   *\n   * Unnecessary AND/OR operations involving a constant true/false value will be eliminated.\n   *\n   * @param desc folding expression\n   * @param constants current propagated constant map\n   * @param cppCtx\n   * @param op processing operator\n   * @param propagate if true, assignment expressions will be added to constants.\n   * @return fold expression\n   * @throws UDFArgumentException\n   */\n  private static ExprNodeDesc foldExprShortcut(ExprNodeDesc desc, Map<ColumnInfo, ExprNodeDesc> constants,\n      ConstantPropagateProcCtx cppCtx, Operator<? extends Serializable> op, int tag,\n      boolean propagate) throws UDFArgumentException {\n    // Combine NOT operator with the child operator. Otherwise, the following optimization\n    // from bottom up could lead to incorrect result, such as not(x > 3 and x is not null),\n    // should not be optimized to not(x > 3), but (x <=3 or x is null).\n    desc = foldNegative(desc);\n\n    if (desc instanceof ExprNodeGenericFuncDesc) {\n      ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;\n\n      GenericUDF udf = funcDesc.getGenericUDF();\n\n      boolean propagateNext = propagate && propagatableUdfs.contains(udf.getClass());\n      List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();\n      for (ExprNodeDesc childExpr : desc.getChildren()) {\n        newExprs.add(foldExpr(childExpr, constants, cppCtx, op, tag, propagateNext));\n      }\n\n      // Don't evaluate nondeterministic function since the value can only calculate during runtime.\n      if (!isDeterministicUdf(udf, newExprs)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Function \" + udf.getClass() + \" is undeterministic. Don't evaluate immediately.\");\n        }\n        ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n        return desc;\n      }\n\n      // Check if the function can be short cut.\n      ExprNodeDesc shortcut = shortcutFunction(udf, newExprs, op);\n      if (shortcut != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Folding expression:\" + desc + \" -> \" + shortcut);\n        }\n        return shortcut;\n      }\n      ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n    }\n    return desc;\n  }"
        ],
        [
            "CalcitePlanner::CalcitePlannerAction::genSelectLogicalPlan(QB,RelNode,RelNode)",
            "3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258  \n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307  \n3308  \n3309  \n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336 -\n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394  \n3395  \n3396  \n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  ",
            "    /**\n     * NOTE: there can only be one select caluse since we don't handle multi\n     * destination insert.\n     *\n     * @throws SemanticException\n     */\n    private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)\n        throws SemanticException {\n      // 0. Generate a Select Node for Windowing\n      // Exclude the newly-generated select columns from */etc. resolution.\n      HashSet<ColumnInfo> excludedColumns = new HashSet<ColumnInfo>();\n      RelNode selForWindow = genSelectForWindowing(qb, srcRel, excludedColumns);\n      srcRel = (selForWindow == null) ? srcRel : selForWindow;\n\n      ArrayList<ExprNodeDesc> col_list = new ArrayList<ExprNodeDesc>();\n\n      // 1. Get Select Expression List\n      QBParseInfo qbp = getQBParseInfo(qb);\n      String selClauseName = qbp.getClauseNames().iterator().next();\n      ASTNode selExprList = qbp.getSelForClause(selClauseName);\n\n      final boolean cubeRollupGrpSetPresent = (!qbp.getDestRollups().isEmpty()\n              || !qbp.getDestGroupingSets().isEmpty() || !qbp.getDestCubes().isEmpty());\n\n      // 2.Row resolvers for input, output\n      RowResolver out_rwsch = new RowResolver();\n      Integer pos = Integer.valueOf(0);\n      // TODO: will this also fix windowing? try\n      RowResolver inputRR = this.relToHiveRR.get(srcRel), starRR = inputRR;\n      if (starSrcRel != null) {\n        starRR = this.relToHiveRR.get(starSrcRel);\n      }\n\n      // 3. Query Hints\n      // TODO: Handle Query Hints; currently we ignore them\n      boolean selectStar = false;\n      int posn = 0;\n      boolean hintPresent = (selExprList.getChild(0).getType() == HiveParser.TOK_HINTLIST);\n      if (hintPresent) {\n        String hint = ctx.getTokenRewriteStream().toString(\n            selExprList.getChild(0).getTokenStartIndex(),\n            selExprList.getChild(0).getTokenStopIndex());\n        String msg = String.format(\"Hint specified for %s.\"\n            + \" Currently we don't support hints in CBO, turn off cbo to use hints.\", hint);\n        LOG.debug(msg);\n        throw new CalciteSemanticException(msg, UnsupportedFeature.Hint);\n      }\n\n      // 4. Bailout if select involves Transform\n      boolean isInTransform = (selExprList.getChild(posn).getChild(0).getType() == HiveParser.TOK_TRANSFORM);\n      if (isInTransform) {\n        String msg = String.format(\"SELECT TRANSFORM is currently not supported in CBO,\"\n            + \" turn off cbo to use TRANSFORM.\");\n        LOG.debug(msg);\n        throw new CalciteSemanticException(msg, UnsupportedFeature.Select_transform);\n      }\n\n      // 5. Check if select involves UDTF\n      String udtfTableAlias = null;\n      GenericUDTF genericUDTF = null;\n      String genericUDTFName = null;\n      ArrayList<String> udtfColAliases = new ArrayList<String>();\n      ASTNode expr = (ASTNode) selExprList.getChild(posn).getChild(0);\n      int exprType = expr.getType();\n      if (exprType == HiveParser.TOK_FUNCTION || exprType == HiveParser.TOK_FUNCTIONSTAR) {\n        String funcName = TypeCheckProcFactory.DefaultExprProcessor.getFunctionText(expr, true);\n        FunctionInfo fi = FunctionRegistry.getFunctionInfo(funcName);\n        if (fi != null && fi.getGenericUDTF() != null) {\n          LOG.debug(\"Find UDTF \" + funcName);\n          genericUDTF = fi.getGenericUDTF();\n          genericUDTFName = funcName;\n          if (genericUDTF != null && (selectStar = exprType == HiveParser.TOK_FUNCTIONSTAR)) {\n            genColListRegex(\".*\", null, (ASTNode) expr.getChild(0),\n                col_list, null, inputRR, starRR, pos, out_rwsch, qb.getAliases(), false);\n          }\n        }\n      }\n\n      if (genericUDTF != null) {\n        // Only support a single expression when it's a UDTF\n        if (selExprList.getChildCount() > 1) {\n          throw new SemanticException(generateErrorMessage(\n              (ASTNode) selExprList.getChild(1),\n              ErrorMsg.UDTF_MULTIPLE_EXPR.getMsg()));\n        }\n\n        ASTNode selExpr = (ASTNode) selExprList.getChild(posn);\n\n        // Get the column / table aliases from the expression. Start from 1 as\n        // 0 is the TOK_FUNCTION\n        // column names also can be inferred from result of UDTF\n        for (int i = 1; i < selExpr.getChildCount(); i++) {\n          ASTNode selExprChild = (ASTNode) selExpr.getChild(i);\n          switch (selExprChild.getType()) {\n          case HiveParser.Identifier:\n            udtfColAliases.add(unescapeIdentifier(selExprChild.getText().toLowerCase()));\n            break;\n          case HiveParser.TOK_TABALIAS:\n            assert (selExprChild.getChildCount() == 1);\n            udtfTableAlias = unescapeIdentifier(selExprChild.getChild(0)\n                .getText());\n            qb.addAlias(udtfTableAlias);\n            break;\n          default:\n            throw new SemanticException(\"Find invalid token type \" + selExprChild.getType()\n                + \" in UDTF.\");\n          }\n        }\n        LOG.debug(\"UDTF table alias is \" + udtfTableAlias);\n        LOG.debug(\"UDTF col aliases are \" + udtfColAliases);\n      }\n\n      // 6. Iterate over all expression (after SELECT)\n      ASTNode exprList;\n      if (genericUDTF != null) {\n        exprList = expr;\n      } else {\n        exprList = selExprList;\n      }\n      // For UDTF's, skip the function name to get the expressions\n      int startPosn = genericUDTF != null ? posn + 1 : posn;\n      for (int i = startPosn; i < exprList.getChildCount(); ++i) {\n\n        // 6.1 child can be EXPR AS ALIAS, or EXPR.\n        ASTNode child = (ASTNode) exprList.getChild(i);\n        boolean hasAsClause = (!isInTransform) && (child.getChildCount() == 2);\n\n        // 6.2 EXPR AS (ALIAS,...) parses, but is only allowed for UDTF's\n        // This check is not needed and invalid when there is a transform b/c\n        // the\n        // AST's are slightly different.\n        if (genericUDTF == null && child.getChildCount() > 2) {\n          throw new SemanticException(SemanticAnalyzer.generateErrorMessage(\n              (ASTNode) child.getChild(2), ErrorMsg.INVALID_AS.getMsg()));\n        }\n\n        String tabAlias;\n        String colAlias;\n\n        if (genericUDTF != null) {\n          tabAlias = null;\n          colAlias = getAutogenColAliasPrfxLbl() + i;\n          expr = child;\n        } else {\n          // 6.3 Get rid of TOK_SELEXPR\n          expr = (ASTNode) child.getChild(0);\n          String[] colRef = SemanticAnalyzer.getColAlias(child, getAutogenColAliasPrfxLbl(),\n              inputRR, autogenColAliasPrfxIncludeFuncName(), i);\n          tabAlias = colRef[0];\n          colAlias = colRef[1];\n        }\n\n        // 6.4 Build ExprNode corresponding to colums\n        if (expr.getType() == HiveParser.TOK_ALLCOLREF) {\n          pos = genColListRegex(\".*\", expr.getChildCount() == 0 ? null : SemanticAnalyzer\n              .getUnescapedName((ASTNode) expr.getChild(0)).toLowerCase(), expr, col_list,\n              excludedColumns, inputRR, starRR, pos, out_rwsch, qb.getAliases(), true);\n          selectStar = true;\n        } else if (expr.getType() == HiveParser.TOK_TABLE_OR_COL\n            && !hasAsClause\n            && !inputRR.getIsExprResolver()\n            && SemanticAnalyzer.isRegex(\n                SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText()), conf)) {\n          // In case the expression is a regex COL.\n          // This can only happen without AS clause\n          // We don't allow this for ExprResolver - the Group By case\n          pos = genColListRegex(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText()),\n              null, expr, col_list, excludedColumns, inputRR, starRR, pos, out_rwsch,\n              qb.getAliases(), true);\n        } else if (expr.getType() == HiveParser.DOT\n            && expr.getChild(0).getType() == HiveParser.TOK_TABLE_OR_COL\n            && inputRR.hasTableAlias(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0)\n                .getChild(0).getText().toLowerCase()))\n            && !hasAsClause\n            && !inputRR.getIsExprResolver()\n            && SemanticAnalyzer.isRegex(\n                SemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText()), conf)) {\n          // In case the expression is TABLE.COL (col can be regex).\n          // This can only happen without AS clause\n          // We don't allow this for ExprResolver - the Group By case\n          pos = genColListRegex(\n              SemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText()),\n              SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getChild(0).getText()\n                  .toLowerCase()), expr, col_list, excludedColumns, inputRR, starRR, pos,\n              out_rwsch, qb.getAliases(), true);\n        } else if (ParseUtils.containsTokenOfType(expr, HiveParser.TOK_FUNCTIONDI)\n            && !(srcRel instanceof HiveAggregate)) {\n          // Likely a malformed query eg, select hash(distinct c1) from t1;\n          throw new CalciteSemanticException(\"Distinct without an aggreggation.\",\n              UnsupportedFeature.Distinct_without_an_aggreggation);\n        } else {\n          // Case when this is an expression\n          TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR);\n          // We allow stateful functions in the SELECT list (but nowhere else)\n          tcCtx.setAllowStatefulFunctions(true);\n          if (cubeRollupGrpSetPresent) {\n            // Special handling of grouping function\n            expr = rewriteGroupingFunctionAST(getGroupByForClause(qbp, selClauseName), expr);\n          }\n          ExprNodeDesc exp = genExprNodeDesc(expr, inputRR, tcCtx);\n          String recommended = recommendName(exp, colAlias);\n          if (recommended != null && out_rwsch.get(null, recommended) == null) {\n            colAlias = recommended;\n          }\n          col_list.add(exp);\n\n          ColumnInfo colInfo = new ColumnInfo(SemanticAnalyzer.getColumnInternalName(pos),\n              exp.getWritableObjectInspector(), tabAlias, false);\n          colInfo.setSkewedCol((exp instanceof ExprNodeColumnDesc) ? ((ExprNodeColumnDesc) exp)\n              .isSkewedCol() : false);\n          if (!out_rwsch.putWithCheck(tabAlias, colAlias, null, colInfo)) {\n            throw new CalciteSemanticException(\"Cannot add column to RR: \" + tabAlias + \".\"\n                + colAlias + \" => \" + colInfo + \" due to duplication, see previous warnings\",\n                UnsupportedFeature.Duplicates_in_RR);\n          }\n\n          if (exp instanceof ExprNodeColumnDesc) {\n            ExprNodeColumnDesc colExp = (ExprNodeColumnDesc) exp;\n            String[] altMapping = inputRR.getAlternateMappings(colExp.getColumn());\n            if (altMapping != null) {\n              // TODO: this can overwrite the mapping. Should this be allowed?\n              out_rwsch.put(altMapping[0], altMapping[1], colInfo);\n            }\n          }\n\n          pos = Integer.valueOf(pos.intValue() + 1);\n        }\n      }\n      selectStar = selectStar && exprList.getChildCount() == posn + 1;\n\n      // 7. Convert Hive projections to Calcite\n      List<RexNode> calciteColLst = new ArrayList<RexNode>();\n      RexNodeConverter rexNodeConv = new RexNodeConverter(cluster, srcRel.getRowType(),\n          buildHiveColNameToInputPosMap(col_list, inputRR), 0, false);\n      for (ExprNodeDesc colExpr : col_list) {\n        calciteColLst.add(rexNodeConv.convert(colExpr));\n      }\n\n      // 8. Build Calcite Rel\n      RelNode outputRel = null;\n      if (genericUDTF != null) {\n        // The basic idea for CBO support of UDTF is to treat UDTF as a special project.\n        // In AST return path, as we just need to generate a SEL_EXPR, we just need to remember the expressions and the alias.\n        // In OP return path, we need to generate a SEL and then a UDTF following old semantic analyzer.\n        outputRel = genUDTFPlan(genericUDTF, genericUDTFName, udtfTableAlias, udtfColAliases, qb, calciteColLst, out_rwsch, srcRel);\n      }\n      else{\n        outputRel = genSelectRelNode(calciteColLst, out_rwsch, srcRel);\n      }\n\n      // 9. Handle select distinct as GBY if there exist windowing functions\n      if (selForWindow != null && selExprList.getToken().getType() == HiveParser.TOK_SELECTDI) {\n        ImmutableBitSet groupSet = ImmutableBitSet.range(outputRel.getRowType().getFieldList().size());\n        outputRel = new HiveAggregate(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION),\n              outputRel, false, groupSet, null, new ArrayList<AggregateCall>());\n        RowResolver groupByOutputRowResolver = new RowResolver();\n        for (int i = 0; i < out_rwsch.getColumnInfos().size(); i++) {\n          ColumnInfo colInfo = out_rwsch.getColumnInfos().get(i);\n          ColumnInfo newColInfo = new ColumnInfo(colInfo.getInternalName(),\n              colInfo.getType(), colInfo.getTabAlias(), colInfo.getIsVirtualCol());\n          groupByOutputRowResolver.put(colInfo.getTabAlias(), colInfo.getAlias(), newColInfo);\n        }\n        relToHiveColNameCalcitePosMap.put(outputRel,\n            buildHiveToCalciteColumnMap(groupByOutputRowResolver, outputRel));\n        this.relToHiveRR.put(outputRel, groupByOutputRowResolver);\n      }\n\n      return outputRel;\n    }",
            "3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258  \n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307  \n3308  \n3309  \n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336 +\n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394  \n3395  \n3396  \n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  ",
            "    /**\n     * NOTE: there can only be one select caluse since we don't handle multi\n     * destination insert.\n     *\n     * @throws SemanticException\n     */\n    private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)\n        throws SemanticException {\n      // 0. Generate a Select Node for Windowing\n      // Exclude the newly-generated select columns from */etc. resolution.\n      HashSet<ColumnInfo> excludedColumns = new HashSet<ColumnInfo>();\n      RelNode selForWindow = genSelectForWindowing(qb, srcRel, excludedColumns);\n      srcRel = (selForWindow == null) ? srcRel : selForWindow;\n\n      ArrayList<ExprNodeDesc> col_list = new ArrayList<ExprNodeDesc>();\n\n      // 1. Get Select Expression List\n      QBParseInfo qbp = getQBParseInfo(qb);\n      String selClauseName = qbp.getClauseNames().iterator().next();\n      ASTNode selExprList = qbp.getSelForClause(selClauseName);\n\n      final boolean cubeRollupGrpSetPresent = (!qbp.getDestRollups().isEmpty()\n              || !qbp.getDestGroupingSets().isEmpty() || !qbp.getDestCubes().isEmpty());\n\n      // 2.Row resolvers for input, output\n      RowResolver out_rwsch = new RowResolver();\n      Integer pos = Integer.valueOf(0);\n      // TODO: will this also fix windowing? try\n      RowResolver inputRR = this.relToHiveRR.get(srcRel), starRR = inputRR;\n      if (starSrcRel != null) {\n        starRR = this.relToHiveRR.get(starSrcRel);\n      }\n\n      // 3. Query Hints\n      // TODO: Handle Query Hints; currently we ignore them\n      boolean selectStar = false;\n      int posn = 0;\n      boolean hintPresent = (selExprList.getChild(0).getType() == HiveParser.TOK_HINTLIST);\n      if (hintPresent) {\n        String hint = ctx.getTokenRewriteStream().toString(\n            selExprList.getChild(0).getTokenStartIndex(),\n            selExprList.getChild(0).getTokenStopIndex());\n        String msg = String.format(\"Hint specified for %s.\"\n            + \" Currently we don't support hints in CBO, turn off cbo to use hints.\", hint);\n        LOG.debug(msg);\n        throw new CalciteSemanticException(msg, UnsupportedFeature.Hint);\n      }\n\n      // 4. Bailout if select involves Transform\n      boolean isInTransform = (selExprList.getChild(posn).getChild(0).getType() == HiveParser.TOK_TRANSFORM);\n      if (isInTransform) {\n        String msg = String.format(\"SELECT TRANSFORM is currently not supported in CBO,\"\n            + \" turn off cbo to use TRANSFORM.\");\n        LOG.debug(msg);\n        throw new CalciteSemanticException(msg, UnsupportedFeature.Select_transform);\n      }\n\n      // 5. Check if select involves UDTF\n      String udtfTableAlias = null;\n      GenericUDTF genericUDTF = null;\n      String genericUDTFName = null;\n      ArrayList<String> udtfColAliases = new ArrayList<String>();\n      ASTNode expr = (ASTNode) selExprList.getChild(posn).getChild(0);\n      int exprType = expr.getType();\n      if (exprType == HiveParser.TOK_FUNCTION || exprType == HiveParser.TOK_FUNCTIONSTAR) {\n        String funcName = TypeCheckProcFactory.DefaultExprProcessor.getFunctionText(expr, true);\n        FunctionInfo fi = FunctionRegistry.getFunctionInfo(funcName);\n        if (fi != null && fi.getGenericUDTF() != null) {\n          LOG.debug(\"Find UDTF \" + funcName);\n          genericUDTF = fi.getGenericUDTF();\n          genericUDTFName = funcName;\n          if (genericUDTF != null && (selectStar = exprType == HiveParser.TOK_FUNCTIONSTAR)) {\n            genColListRegex(\".*\", null, (ASTNode) expr.getChild(0),\n                col_list, null, inputRR, starRR, pos, out_rwsch, qb.getAliases(), false);\n          }\n        }\n      }\n\n      if (genericUDTF != null) {\n        // Only support a single expression when it's a UDTF\n        if (selExprList.getChildCount() > 1) {\n          throw new SemanticException(generateErrorMessage(\n              (ASTNode) selExprList.getChild(1),\n              ErrorMsg.UDTF_MULTIPLE_EXPR.getMsg()));\n        }\n\n        ASTNode selExpr = (ASTNode) selExprList.getChild(posn);\n\n        // Get the column / table aliases from the expression. Start from 1 as\n        // 0 is the TOK_FUNCTION\n        // column names also can be inferred from result of UDTF\n        for (int i = 1; i < selExpr.getChildCount(); i++) {\n          ASTNode selExprChild = (ASTNode) selExpr.getChild(i);\n          switch (selExprChild.getType()) {\n          case HiveParser.Identifier:\n            udtfColAliases.add(unescapeIdentifier(selExprChild.getText().toLowerCase()));\n            break;\n          case HiveParser.TOK_TABALIAS:\n            assert (selExprChild.getChildCount() == 1);\n            udtfTableAlias = unescapeIdentifier(selExprChild.getChild(0)\n                .getText());\n            qb.addAlias(udtfTableAlias);\n            break;\n          default:\n            throw new SemanticException(\"Find invalid token type \" + selExprChild.getType()\n                + \" in UDTF.\");\n          }\n        }\n        LOG.debug(\"UDTF table alias is \" + udtfTableAlias);\n        LOG.debug(\"UDTF col aliases are \" + udtfColAliases);\n      }\n\n      // 6. Iterate over all expression (after SELECT)\n      ASTNode exprList;\n      if (genericUDTF != null) {\n        exprList = expr;\n      } else {\n        exprList = selExprList;\n      }\n      // For UDTF's, skip the function name to get the expressions\n      int startPosn = genericUDTF != null ? posn + 1 : posn;\n      for (int i = startPosn; i < exprList.getChildCount(); ++i) {\n\n        // 6.1 child can be EXPR AS ALIAS, or EXPR.\n        ASTNode child = (ASTNode) exprList.getChild(i);\n        boolean hasAsClause = (!isInTransform) && (child.getChildCount() == 2);\n\n        // 6.2 EXPR AS (ALIAS,...) parses, but is only allowed for UDTF's\n        // This check is not needed and invalid when there is a transform b/c\n        // the\n        // AST's are slightly different.\n        if (genericUDTF == null && child.getChildCount() > 2) {\n          throw new SemanticException(SemanticAnalyzer.generateErrorMessage(\n              (ASTNode) child.getChild(2), ErrorMsg.INVALID_AS.getMsg()));\n        }\n\n        String tabAlias;\n        String colAlias;\n\n        if (genericUDTF != null) {\n          tabAlias = null;\n          colAlias = getAutogenColAliasPrfxLbl() + i;\n          expr = child;\n        } else {\n          // 6.3 Get rid of TOK_SELEXPR\n          expr = (ASTNode) child.getChild(0);\n          String[] colRef = SemanticAnalyzer.getColAlias(child, getAutogenColAliasPrfxLbl(),\n              inputRR, autogenColAliasPrfxIncludeFuncName(), i);\n          tabAlias = colRef[0];\n          colAlias = colRef[1];\n        }\n\n        // 6.4 Build ExprNode corresponding to colums\n        if (expr.getType() == HiveParser.TOK_ALLCOLREF) {\n          pos = genColListRegex(\".*\", expr.getChildCount() == 0 ? null : SemanticAnalyzer\n              .getUnescapedName((ASTNode) expr.getChild(0)).toLowerCase(), expr, col_list,\n              excludedColumns, inputRR, starRR, pos, out_rwsch, qb.getAliases(), true);\n          selectStar = true;\n        } else if (expr.getType() == HiveParser.TOK_TABLE_OR_COL\n            && !hasAsClause\n            && !inputRR.getIsExprResolver()\n            && SemanticAnalyzer.isRegex(\n                SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText()), conf)) {\n          // In case the expression is a regex COL.\n          // This can only happen without AS clause\n          // We don't allow this for ExprResolver - the Group By case\n          pos = genColListRegex(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText()),\n              null, expr, col_list, excludedColumns, inputRR, starRR, pos, out_rwsch,\n              qb.getAliases(), true);\n        } else if (expr.getType() == HiveParser.DOT\n            && expr.getChild(0).getType() == HiveParser.TOK_TABLE_OR_COL\n            && inputRR.hasTableAlias(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0)\n                .getChild(0).getText().toLowerCase()))\n            && !hasAsClause\n            && !inputRR.getIsExprResolver()\n            && SemanticAnalyzer.isRegex(\n                SemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText()), conf)) {\n          // In case the expression is TABLE.COL (col can be regex).\n          // This can only happen without AS clause\n          // We don't allow this for ExprResolver - the Group By case\n          pos = genColListRegex(\n              SemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText()),\n              SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getChild(0).getText()\n                  .toLowerCase()), expr, col_list, excludedColumns, inputRR, starRR, pos,\n              out_rwsch, qb.getAliases(), true);\n        } else if (ParseUtils.containsTokenOfType(expr, HiveParser.TOK_FUNCTIONDI)\n            && !(srcRel instanceof HiveAggregate)) {\n          // Likely a malformed query eg, select hash(distinct c1) from t1;\n          throw new CalciteSemanticException(\"Distinct without an aggregation.\",\n              UnsupportedFeature.Distinct_without_an_aggreggation);\n        } else {\n          // Case when this is an expression\n          TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR);\n          // We allow stateful functions in the SELECT list (but nowhere else)\n          tcCtx.setAllowStatefulFunctions(true);\n          if (cubeRollupGrpSetPresent) {\n            // Special handling of grouping function\n            expr = rewriteGroupingFunctionAST(getGroupByForClause(qbp, selClauseName), expr);\n          }\n          ExprNodeDesc exp = genExprNodeDesc(expr, inputRR, tcCtx);\n          String recommended = recommendName(exp, colAlias);\n          if (recommended != null && out_rwsch.get(null, recommended) == null) {\n            colAlias = recommended;\n          }\n          col_list.add(exp);\n\n          ColumnInfo colInfo = new ColumnInfo(SemanticAnalyzer.getColumnInternalName(pos),\n              exp.getWritableObjectInspector(), tabAlias, false);\n          colInfo.setSkewedCol((exp instanceof ExprNodeColumnDesc) ? ((ExprNodeColumnDesc) exp)\n              .isSkewedCol() : false);\n          if (!out_rwsch.putWithCheck(tabAlias, colAlias, null, colInfo)) {\n            throw new CalciteSemanticException(\"Cannot add column to RR: \" + tabAlias + \".\"\n                + colAlias + \" => \" + colInfo + \" due to duplication, see previous warnings\",\n                UnsupportedFeature.Duplicates_in_RR);\n          }\n\n          if (exp instanceof ExprNodeColumnDesc) {\n            ExprNodeColumnDesc colExp = (ExprNodeColumnDesc) exp;\n            String[] altMapping = inputRR.getAlternateMappings(colExp.getColumn());\n            if (altMapping != null) {\n              // TODO: this can overwrite the mapping. Should this be allowed?\n              out_rwsch.put(altMapping[0], altMapping[1], colInfo);\n            }\n          }\n\n          pos = Integer.valueOf(pos.intValue() + 1);\n        }\n      }\n      selectStar = selectStar && exprList.getChildCount() == posn + 1;\n\n      // 7. Convert Hive projections to Calcite\n      List<RexNode> calciteColLst = new ArrayList<RexNode>();\n      RexNodeConverter rexNodeConv = new RexNodeConverter(cluster, srcRel.getRowType(),\n          buildHiveColNameToInputPosMap(col_list, inputRR), 0, false);\n      for (ExprNodeDesc colExpr : col_list) {\n        calciteColLst.add(rexNodeConv.convert(colExpr));\n      }\n\n      // 8. Build Calcite Rel\n      RelNode outputRel = null;\n      if (genericUDTF != null) {\n        // The basic idea for CBO support of UDTF is to treat UDTF as a special project.\n        // In AST return path, as we just need to generate a SEL_EXPR, we just need to remember the expressions and the alias.\n        // In OP return path, we need to generate a SEL and then a UDTF following old semantic analyzer.\n        outputRel = genUDTFPlan(genericUDTF, genericUDTFName, udtfTableAlias, udtfColAliases, qb, calciteColLst, out_rwsch, srcRel);\n      }\n      else{\n        outputRel = genSelectRelNode(calciteColLst, out_rwsch, srcRel);\n      }\n\n      // 9. Handle select distinct as GBY if there exist windowing functions\n      if (selForWindow != null && selExprList.getToken().getType() == HiveParser.TOK_SELECTDI) {\n        ImmutableBitSet groupSet = ImmutableBitSet.range(outputRel.getRowType().getFieldList().size());\n        outputRel = new HiveAggregate(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION),\n              outputRel, false, groupSet, null, new ArrayList<AggregateCall>());\n        RowResolver groupByOutputRowResolver = new RowResolver();\n        for (int i = 0; i < out_rwsch.getColumnInfos().size(); i++) {\n          ColumnInfo colInfo = out_rwsch.getColumnInfos().get(i);\n          ColumnInfo newColInfo = new ColumnInfo(colInfo.getInternalName(),\n              colInfo.getType(), colInfo.getTabAlias(), colInfo.getIsVirtualCol());\n          groupByOutputRowResolver.put(colInfo.getTabAlias(), colInfo.getAlias(), newColInfo);\n        }\n        relToHiveColNameCalcitePosMap.put(outputRel,\n            buildHiveToCalciteColumnMap(groupByOutputRowResolver, outputRel));\n        this.relToHiveRR.put(outputRel, groupByOutputRowResolver);\n      }\n\n      return outputRel;\n    }"
        ],
        [
            "FileSinkOperator::dpSetup()",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492 -\n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  ",
            "  /**\n   * Set up for dynamic partitioning including a new ObjectInspector for the output row.\n   */\n  private void dpSetup() {\n\n    this.bDynParts = false;\n    this.numDynParts = dpCtx.getNumDPCols();\n    this.dpColNames = dpCtx.getDPColNames();\n    this.maxPartitions = dpCtx.getMaxPartitionsPerNode();\n\n    assert numDynParts == dpColNames.size()\n        : \"number of dynamic paritions should be the same as the size of DP mapping\";\n\n    if (dpColNames != null && dpColNames.size() > 0) {\n      this.bDynParts = true;\n      assert inputObjInspectors.length == 1 : \"FileSinkOperator should have 1 parent, but it has \"\n          + inputObjInspectors.length;\n      StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n      this.dpStartCol = Utilities.getDPColOffset(conf);\n      this.subSetOI = new SubStructObjectInspector(soi, 0, this.dpStartCol);\n      this.dpVals = new ArrayList<String>(numDynParts);\n      this.dpWritables = new ArrayList<Object>(numDynParts);\n    }\n  }",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492 +\n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  ",
            "  /**\n   * Set up for dynamic partitioning including a new ObjectInspector for the output row.\n   */\n  private void dpSetup() {\n\n    this.bDynParts = false;\n    this.numDynParts = dpCtx.getNumDPCols();\n    this.dpColNames = dpCtx.getDPColNames();\n    this.maxPartitions = dpCtx.getMaxPartitionsPerNode();\n\n    assert numDynParts == dpColNames.size()\n        : \"number of dynamic partitions should be the same as the size of DP mapping\";\n\n    if (dpColNames != null && dpColNames.size() > 0) {\n      this.bDynParts = true;\n      assert inputObjInspectors.length == 1 : \"FileSinkOperator should have 1 parent, but it has \"\n          + inputObjInspectors.length;\n      StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n      this.dpStartCol = Utilities.getDPColOffset(conf);\n      this.subSetOI = new SubStructObjectInspector(soi, 0, this.dpStartCol);\n      this.dpVals = new ArrayList<String>(numDynParts);\n      this.dpWritables = new ArrayList<Object>(numDynParts);\n    }\n  }"
        ],
        [
            "DbTxnManager::heartbeat()",
            " 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 -\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 -\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  ",
            "  @Override\n  public void heartbeat() throws LockException {\n    List<HiveLock> locks;\n    if(isTxnOpen()) {\n      // Create one dummy lock so we can go through the loop below, though we only\n      //really need txnId\n      DbLockManager.DbHiveLock dummyLock = new DbLockManager.DbHiveLock(0L);\n      locks = new ArrayList<>(1);\n      locks.add(dummyLock);\n    }\n    else {\n      locks = lockMgr.getLocks(false, false);\n    }\n    if(LOG.isInfoEnabled()) {\n      StringBuilder sb = new StringBuilder(\"Sending heartbeat for \")\n        .append(JavaUtils.txnIdToString(txnId)).append(\" and\");\n      for(HiveLock lock : locks) {\n        sb.append(\" \").append(lock.toString());\n      }\n      LOG.info(sb.toString());\n    }\n    if(!isTxnOpen() && locks.isEmpty()) {\n      // No locks, no txn, we outta here.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"No need to send heartbeat as there is no transaction and no locks.\");\n      }\n      return;\n    }\n    for (HiveLock lock : locks) {\n      long lockId = ((DbLockManager.DbHiveLock)lock).lockId;\n      try {\n        // Get the threadlocal metastore client for the heartbeat calls.\n        SynchronizedMetaStoreClient heartbeaterClient = getThreadLocalMSClient();\n        if (heartbeaterClient == null) {\n          Hive db;\n          try {\n            db = Hive.get(conf);\n            // Create a new threadlocal synchronized metastore client for use in hearbeater threads.\n            // This makes the concurrent use of heartbeat thread safe, and won't cause transaction\n            // abort due to a long metastore client call blocking the heartbeat call.\n            heartbeaterClient = new SynchronizedMetaStoreClient(db.getMSC());\n            threadLocalMSClient.set(heartbeaterClient);\n          } catch (HiveException e) {\n            LOG.error(\"Unable to create new metastore client for heartbeating\", e);\n            throw new LockException(e);\n          }\n          // Increment the threadlocal metastore client count\n          if (heartbeaterMSClientCount.incrementAndGet() >= heartbeaterThreadPoolSize) {\n            LOG.warn(\"The number of hearbeater metastore clients - + \"\n                + heartbeaterMSClientCount.get() + \", has exceeded the max limit - \"\n                + heartbeaterThreadPoolSize);\n          }\n        }\n        heartbeaterClient.heartbeat(txnId, lockId);\n      } catch (NoSuchLockException e) {\n        LOG.error(\"Unable to find lock \" + JavaUtils.lockIdToString(lockId));\n        throw new LockException(e, ErrorMsg.LOCK_NO_SUCH_LOCK, JavaUtils.lockIdToString(lockId));\n      } catch (NoSuchTxnException e) {\n        LOG.error(\"Unable to find transaction \" + JavaUtils.txnIdToString(txnId));\n        throw new LockException(e, ErrorMsg.TXN_NO_SUCH_TRANSACTION, JavaUtils.txnIdToString(txnId));\n      } catch (TxnAbortedException e) {\n        LockException le = new LockException(e, ErrorMsg.TXN_ABORTED, JavaUtils.txnIdToString(txnId), e.getMessage());\n        LOG.error(le.getMessage());\n        throw le;\n      } catch (TException e) {\n        throw new LockException(\n            ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg() + \"(\" + JavaUtils.txnIdToString(txnId)\n              + \",\" + lock.toString() + \")\", e);\n      }\n    }\n  }",
            " 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 +\n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 +\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  ",
            "  @Override\n  public void heartbeat() throws LockException {\n    List<HiveLock> locks;\n    if(isTxnOpen()) {\n      // Create one dummy lock so we can go through the loop below, though we only\n      //really need txnId\n      DbLockManager.DbHiveLock dummyLock = new DbLockManager.DbHiveLock(0L);\n      locks = new ArrayList<>(1);\n      locks.add(dummyLock);\n    }\n    else {\n      locks = lockMgr.getLocks(false, false);\n    }\n    if(LOG.isInfoEnabled()) {\n      StringBuilder sb = new StringBuilder(\"Sending heartbeat for \")\n        .append(JavaUtils.txnIdToString(txnId)).append(\" and\");\n      for(HiveLock lock : locks) {\n        sb.append(\" \").append(lock.toString());\n      }\n      LOG.info(sb.toString());\n    }\n    if(!isTxnOpen() && locks.isEmpty()) {\n      // No locks, no txn, we outta here.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"No need to send heartbeat as there is no transaction and no locks.\");\n      }\n      return;\n    }\n    for (HiveLock lock : locks) {\n      long lockId = ((DbLockManager.DbHiveLock)lock).lockId;\n      try {\n        // Get the threadlocal metastore client for the heartbeat calls.\n        SynchronizedMetaStoreClient heartbeaterClient = getThreadLocalMSClient();\n        if (heartbeaterClient == null) {\n          Hive db;\n          try {\n            db = Hive.get(conf);\n            // Create a new threadlocal synchronized metastore client for use in heartbeater threads.\n            // This makes the concurrent use of heartbeat thread safe, and won't cause transaction\n            // abort due to a long metastore client call blocking the heartbeat call.\n            heartbeaterClient = new SynchronizedMetaStoreClient(db.getMSC());\n            threadLocalMSClient.set(heartbeaterClient);\n          } catch (HiveException e) {\n            LOG.error(\"Unable to create new metastore client for heartbeating\", e);\n            throw new LockException(e);\n          }\n          // Increment the threadlocal metastore client count\n          if (heartbeaterMSClientCount.incrementAndGet() >= heartbeaterThreadPoolSize) {\n            LOG.warn(\"The number of heartbeater metastore clients - + \"\n                + heartbeaterMSClientCount.get() + \", has exceeded the max limit - \"\n                + heartbeaterThreadPoolSize);\n          }\n        }\n        heartbeaterClient.heartbeat(txnId, lockId);\n      } catch (NoSuchLockException e) {\n        LOG.error(\"Unable to find lock \" + JavaUtils.lockIdToString(lockId));\n        throw new LockException(e, ErrorMsg.LOCK_NO_SUCH_LOCK, JavaUtils.lockIdToString(lockId));\n      } catch (NoSuchTxnException e) {\n        LOG.error(\"Unable to find transaction \" + JavaUtils.txnIdToString(txnId));\n        throw new LockException(e, ErrorMsg.TXN_NO_SUCH_TRANSACTION, JavaUtils.txnIdToString(txnId));\n      } catch (TxnAbortedException e) {\n        LockException le = new LockException(e, ErrorMsg.TXN_ABORTED, JavaUtils.txnIdToString(txnId), e.getMessage());\n        LOG.error(le.getMessage());\n        throw le;\n      } catch (TException e) {\n        throw new LockException(\n            ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg() + \"(\" + JavaUtils.txnIdToString(txnId)\n              + \",\" + lock.toString() + \")\", e);\n      }\n    }\n  }"
        ],
        [
            "DDLTask::unarchive(Hive,AlterTableSimpleDesc)",
            "1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694 -\n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  ",
            "  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n      throws HiveException, URISyntaxException {\n\n    Table tbl = db.getTable(simpleDesc.getTableName());\n\n    // Means user specified a table, not a partition\n    if (simpleDesc.getPartSpec() == null) {\n      throw new HiveException(\"UNARCHIVE is for partitions only\");\n    }\n\n    if (tbl.getTableType() != TableType.MANAGED_TABLE) {\n      throw new HiveException(\"UNARCHIVE can only be performed on managed tables\");\n    }\n\n    Map<String, String> partSpec = simpleDesc.getPartSpec();\n    PartSpecInfo partSpecInfo = PartSpecInfo.create(tbl, partSpec);\n    List<Partition> partitions = db.getPartitions(tbl, partSpec);\n\n    int partSpecLevel = partSpec.size();\n\n    Path originalDir = null;\n\n    // when we have partial partitions specification we must assume partitions\n    // lie in standard place - if they were in custom locations putting\n    // them into one archive would involve mass amount of copying\n    // in full partition specification case we allow custom locations\n    // to keep backward compatibility\n    if (partitions.isEmpty()) {\n      throw new HiveException(\"No partition matches the specification\");\n    } else if(partSpecInfo.values.size() != tbl.getPartCols().size()) {\n      // for partial specifications we need partitions to follow the scheme\n      for(Partition p: partitions){\n        if(partitionInCustomLocation(tbl, p)) {\n          String message = String.format(\"UNARCHIVE cannot run for partition \" +\n              \"groups with custom locations like %s\", p.getLocation());\n          throw new HiveException(message);\n        }\n      }\n      originalDir = partSpecInfo.createPath(tbl);\n    } else {\n      Partition p = partitions.get(0);\n      if(ArchiveUtils.isArchived(p)) {\n        originalDir = new Path(getOriginalLocation(p));\n      } else {\n        originalDir = new Path(p.getLocation());\n      }\n    }\n\n    URI originalUri = ArchiveUtils.addSlash(originalDir.toUri());\n    Path intermediateArchivedDir = new Path(originalDir.getParent(),\n        originalDir.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n    Path intermediateExtractedDir = new Path(originalDir.getParent(),\n        originalDir.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);\n    boolean recovery = false;\n    if(pathExists(intermediateArchivedDir) || pathExists(intermediateExtractedDir)) {\n      recovery = true;\n      console.printInfo(\"Starting recovery after failed UNARCHIVE\");\n    }\n\n    for(Partition p: partitions) {\n      checkArchiveProperty(partSpecLevel, recovery, p);\n    }\n\n    String archiveName = \"data.har\";\n    FileSystem fs = null;\n    try {\n      fs = originalDir.getFileSystem(conf);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // assume the archive is in the original dir, check if it exists\n    Path archivePath = new Path(originalDir, archiveName);\n    URI archiveUri = archivePath.toUri();\n    ArchiveUtils.HarPathHelper harHelper = new ArchiveUtils.HarPathHelper(conf,\n        archiveUri, originalUri);\n    URI sourceUri = harHelper.getHarUri(originalUri);\n    Path sourceDir = new Path(sourceUri.getScheme(), sourceUri.getAuthority(), sourceUri.getPath());\n\n    if(!pathExists(intermediateArchivedDir) && !pathExists(archivePath)) {\n      throw new HiveException(\"Haven't found any archive where it should be\");\n    }\n\n    Path tmpPath = driverContext.getCtx().getExternalTmpPath(originalDir);\n\n    try {\n      fs = tmpPath.getFileSystem(conf);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // Clarification of terms:\n    // - The originalDir directory represents the original directory of the\n    //   partitions' files. They now contain an archived version of those files\n    //   eg. hdfs:/warehouse/myTable/ds=1/\n    // - The source directory is the directory containing all the files that\n    //   should be in the partitions. e.g. har:/warehouse/myTable/ds=1/myTable.har/\n    //   Note the har:/ scheme\n\n    // Steps:\n    // 1. Extract the archive in a temporary folder\n    // 2. Move the archive dir to an intermediate dir that is in at the same\n    //    dir as originalLocation. Call the new dir intermediate-extracted.\n    // 3. Rename the original partitions dir to an intermediate dir. Call the\n    //    renamed dir intermediate-archive\n    // 4. Rename intermediate-extracted to the original partitions dir\n    // 5. Change the metadata\n    // 6. Delete the archived partitions files in intermediate-archive\n\n    if (!pathExists(intermediateExtractedDir) &&\n        !pathExists(intermediateArchivedDir)) {\n      try {\n\n        // Copy the files out of the archive into the temporary directory\n        String copySource = sourceDir.toString();\n        String copyDest = tmpPath.toString();\n        List<String> args = new ArrayList<String>();\n        args.add(\"-cp\");\n        args.add(copySource);\n        args.add(copyDest);\n\n        console.printInfo(\"Copying \" + copySource + \" to \" + copyDest);\n        FileSystem srcFs = FileSystem.get(sourceDir.toUri(), conf);\n        srcFs.initialize(sourceDir.toUri(), conf);\n\n        FsShell fss = new FsShell(conf);\n        int ret = 0;\n        try {\n          ret = ToolRunner.run(fss, args.toArray(new String[0]));\n        } catch (Exception e) {\n          e.printStackTrace();\n          throw new HiveException(e);\n        }\n\n        if (ret != 0) {\n          throw new HiveException(\"Error while copying files from archive, return code=\" + ret);\n        } else {\n          console.printInfo(\"Succefully Copied \" + copySource + \" to \" + copyDest);\n        }\n\n        console.printInfo(\"Moving \" + tmpPath + \" to \" + intermediateExtractedDir);\n        if (fs.exists(intermediateExtractedDir)) {\n          throw new HiveException(\"Invalid state: the intermediate extracted \" +\n              \"directory already exists.\");\n        }\n        fs.rename(tmpPath, intermediateExtractedDir);\n      } catch (Exception e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // At this point, we know that the extracted files are in the intermediate\n    // extracted dir, or in the the original directory.\n\n    if (!pathExists(intermediateArchivedDir)) {\n      try {\n        console.printInfo(\"Moving \" + originalDir + \" to \" + intermediateArchivedDir);\n        fs.rename(originalDir, intermediateArchivedDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(intermediateArchivedDir + \" already exists. \" +\n          \"Assuming it contains the archived version of the partition\");\n    }\n\n    // If there is a failure from here to until when the metadata is changed,\n    // the partition will be empty or throw errors on read.\n\n    // If the original location exists here, then it must be the extracted files\n    // because in the previous step, we moved the previous original location\n    // (containing the archived version of the files) to intermediateArchiveDir\n    if (!pathExists(originalDir)) {\n      try {\n        console.printInfo(\"Moving \" + intermediateExtractedDir + \" to \" + originalDir);\n        fs.rename(intermediateExtractedDir, originalDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(originalDir + \" already exists. \" +\n          \"Assuming it contains the extracted files in the partition\");\n    }\n\n    for(Partition p: partitions) {\n      setUnArchived(p);\n      try {\n        db.alterPartition(simpleDesc.getTableName(), p, null);\n      } catch (InvalidOperationException e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // If a failure happens here, the intermediate archive files won't be\n    // deleted. The user will need to call unarchive again to clear those up.\n    if(pathExists(intermediateArchivedDir)) {\n      deleteDir(intermediateArchivedDir);\n    }\n\n    if(recovery) {\n      console.printInfo(\"Recovery after UNARCHIVE succeeded\");\n    }\n\n    return 0;\n  }",
            "1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694 +\n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  ",
            "  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n      throws HiveException, URISyntaxException {\n\n    Table tbl = db.getTable(simpleDesc.getTableName());\n\n    // Means user specified a table, not a partition\n    if (simpleDesc.getPartSpec() == null) {\n      throw new HiveException(\"UNARCHIVE is for partitions only\");\n    }\n\n    if (tbl.getTableType() != TableType.MANAGED_TABLE) {\n      throw new HiveException(\"UNARCHIVE can only be performed on managed tables\");\n    }\n\n    Map<String, String> partSpec = simpleDesc.getPartSpec();\n    PartSpecInfo partSpecInfo = PartSpecInfo.create(tbl, partSpec);\n    List<Partition> partitions = db.getPartitions(tbl, partSpec);\n\n    int partSpecLevel = partSpec.size();\n\n    Path originalDir = null;\n\n    // when we have partial partitions specification we must assume partitions\n    // lie in standard place - if they were in custom locations putting\n    // them into one archive would involve mass amount of copying\n    // in full partition specification case we allow custom locations\n    // to keep backward compatibility\n    if (partitions.isEmpty()) {\n      throw new HiveException(\"No partition matches the specification\");\n    } else if(partSpecInfo.values.size() != tbl.getPartCols().size()) {\n      // for partial specifications we need partitions to follow the scheme\n      for(Partition p: partitions){\n        if(partitionInCustomLocation(tbl, p)) {\n          String message = String.format(\"UNARCHIVE cannot run for partition \" +\n              \"groups with custom locations like %s\", p.getLocation());\n          throw new HiveException(message);\n        }\n      }\n      originalDir = partSpecInfo.createPath(tbl);\n    } else {\n      Partition p = partitions.get(0);\n      if(ArchiveUtils.isArchived(p)) {\n        originalDir = new Path(getOriginalLocation(p));\n      } else {\n        originalDir = new Path(p.getLocation());\n      }\n    }\n\n    URI originalUri = ArchiveUtils.addSlash(originalDir.toUri());\n    Path intermediateArchivedDir = new Path(originalDir.getParent(),\n        originalDir.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n    Path intermediateExtractedDir = new Path(originalDir.getParent(),\n        originalDir.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);\n    boolean recovery = false;\n    if(pathExists(intermediateArchivedDir) || pathExists(intermediateExtractedDir)) {\n      recovery = true;\n      console.printInfo(\"Starting recovery after failed UNARCHIVE\");\n    }\n\n    for(Partition p: partitions) {\n      checkArchiveProperty(partSpecLevel, recovery, p);\n    }\n\n    String archiveName = \"data.har\";\n    FileSystem fs = null;\n    try {\n      fs = originalDir.getFileSystem(conf);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // assume the archive is in the original dir, check if it exists\n    Path archivePath = new Path(originalDir, archiveName);\n    URI archiveUri = archivePath.toUri();\n    ArchiveUtils.HarPathHelper harHelper = new ArchiveUtils.HarPathHelper(conf,\n        archiveUri, originalUri);\n    URI sourceUri = harHelper.getHarUri(originalUri);\n    Path sourceDir = new Path(sourceUri.getScheme(), sourceUri.getAuthority(), sourceUri.getPath());\n\n    if(!pathExists(intermediateArchivedDir) && !pathExists(archivePath)) {\n      throw new HiveException(\"Haven't found any archive where it should be\");\n    }\n\n    Path tmpPath = driverContext.getCtx().getExternalTmpPath(originalDir);\n\n    try {\n      fs = tmpPath.getFileSystem(conf);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // Clarification of terms:\n    // - The originalDir directory represents the original directory of the\n    //   partitions' files. They now contain an archived version of those files\n    //   eg. hdfs:/warehouse/myTable/ds=1/\n    // - The source directory is the directory containing all the files that\n    //   should be in the partitions. e.g. har:/warehouse/myTable/ds=1/myTable.har/\n    //   Note the har:/ scheme\n\n    // Steps:\n    // 1. Extract the archive in a temporary folder\n    // 2. Move the archive dir to an intermediate dir that is in at the same\n    //    dir as originalLocation. Call the new dir intermediate-extracted.\n    // 3. Rename the original partitions dir to an intermediate dir. Call the\n    //    renamed dir intermediate-archive\n    // 4. Rename intermediate-extracted to the original partitions dir\n    // 5. Change the metadata\n    // 6. Delete the archived partitions files in intermediate-archive\n\n    if (!pathExists(intermediateExtractedDir) &&\n        !pathExists(intermediateArchivedDir)) {\n      try {\n\n        // Copy the files out of the archive into the temporary directory\n        String copySource = sourceDir.toString();\n        String copyDest = tmpPath.toString();\n        List<String> args = new ArrayList<String>();\n        args.add(\"-cp\");\n        args.add(copySource);\n        args.add(copyDest);\n\n        console.printInfo(\"Copying \" + copySource + \" to \" + copyDest);\n        FileSystem srcFs = FileSystem.get(sourceDir.toUri(), conf);\n        srcFs.initialize(sourceDir.toUri(), conf);\n\n        FsShell fss = new FsShell(conf);\n        int ret = 0;\n        try {\n          ret = ToolRunner.run(fss, args.toArray(new String[0]));\n        } catch (Exception e) {\n          e.printStackTrace();\n          throw new HiveException(e);\n        }\n\n        if (ret != 0) {\n          throw new HiveException(\"Error while copying files from archive, return code=\" + ret);\n        } else {\n          console.printInfo(\"Successfully Copied \" + copySource + \" to \" + copyDest);\n        }\n\n        console.printInfo(\"Moving \" + tmpPath + \" to \" + intermediateExtractedDir);\n        if (fs.exists(intermediateExtractedDir)) {\n          throw new HiveException(\"Invalid state: the intermediate extracted \" +\n              \"directory already exists.\");\n        }\n        fs.rename(tmpPath, intermediateExtractedDir);\n      } catch (Exception e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // At this point, we know that the extracted files are in the intermediate\n    // extracted dir, or in the the original directory.\n\n    if (!pathExists(intermediateArchivedDir)) {\n      try {\n        console.printInfo(\"Moving \" + originalDir + \" to \" + intermediateArchivedDir);\n        fs.rename(originalDir, intermediateArchivedDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(intermediateArchivedDir + \" already exists. \" +\n          \"Assuming it contains the archived version of the partition\");\n    }\n\n    // If there is a failure from here to until when the metadata is changed,\n    // the partition will be empty or throw errors on read.\n\n    // If the original location exists here, then it must be the extracted files\n    // because in the previous step, we moved the previous original location\n    // (containing the archived version of the files) to intermediateArchiveDir\n    if (!pathExists(originalDir)) {\n      try {\n        console.printInfo(\"Moving \" + intermediateExtractedDir + \" to \" + originalDir);\n        fs.rename(intermediateExtractedDir, originalDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(originalDir + \" already exists. \" +\n          \"Assuming it contains the extracted files in the partition\");\n    }\n\n    for(Partition p: partitions) {\n      setUnArchived(p);\n      try {\n        db.alterPartition(simpleDesc.getTableName(), p, null);\n      } catch (InvalidOperationException e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // If a failure happens here, the intermediate archive files won't be\n    // deleted. The user will need to call unarchive again to clear those up.\n    if(pathExists(intermediateArchivedDir)) {\n      deleteDir(intermediateArchivedDir);\n    }\n\n    if(recovery) {\n      console.printInfo(\"Recovery after UNARCHIVE succeeded\");\n    }\n\n    return 0;\n  }"
        ],
        [
            "Registry::addFunction(String,FunctionInfo)",
            " 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 -\n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  ",
            "  private void addFunction(String functionName, FunctionInfo function) {\n    lock.lock();\n    try {\n      // Built-in functions shouldn't go in the session registry,\n      // and temp functions shouldn't go in the system registry.\n      // Persistent functions can be in either registry.\n      if ((!isNative && function.isBuiltIn()) || (isNative && !function.isNative())) {\n        throw new RuntimeException(\"Function \" + functionName + \" is not for this registry\");\n      }\n      functionName = functionName.toLowerCase();\n      FunctionInfo prev = mFunctions.get(functionName);\n      if (prev != null) {\n        if (isBuiltInFunc(prev.getFunctionClass())) {\n          throw new RuntimeException(\"Function \" + functionName + \" is hive builtin function, \" +\n              \"which cannot be overriden.\");\n        }\n        prev.discarded();\n      }\n      mFunctions.put(functionName, function);\n      if (function.isBuiltIn()) {\n        builtIns.add(function.getFunctionClass());\n      } else if (function.isPersistent() && !isNative) {\n        // System registry should not be used to check persistent functions - see isPermanentFunc()\n        Class<?> functionClass = getPermanentUdfClass(function);\n        Integer refCount = persistent.get(functionClass);\n        persistent.put(functionClass, Integer.valueOf(refCount == null ? 1 : refCount + 1));\n      }\n    } finally {\n      lock.unlock();\n    }\n  }",
            " 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 +\n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  ",
            "  private void addFunction(String functionName, FunctionInfo function) {\n    lock.lock();\n    try {\n      // Built-in functions shouldn't go in the session registry,\n      // and temp functions shouldn't go in the system registry.\n      // Persistent functions can be in either registry.\n      if ((!isNative && function.isBuiltIn()) || (isNative && !function.isNative())) {\n        throw new RuntimeException(\"Function \" + functionName + \" is not for this registry\");\n      }\n      functionName = functionName.toLowerCase();\n      FunctionInfo prev = mFunctions.get(functionName);\n      if (prev != null) {\n        if (isBuiltInFunc(prev.getFunctionClass())) {\n          throw new RuntimeException(\"Function \" + functionName + \" is hive builtin function, \" +\n              \"which cannot be overridden.\");\n        }\n        prev.discarded();\n      }\n      mFunctions.put(functionName, function);\n      if (function.isBuiltIn()) {\n        builtIns.add(function.getFunctionClass());\n      } else if (function.isPersistent() && !isNative) {\n        // System registry should not be used to check persistent functions - see isPermanentFunc()\n        Class<?> functionClass = getPermanentUdfClass(function);\n        Integer refCount = persistent.get(functionClass);\n        persistent.put(functionClass, Integer.valueOf(refCount == null ? 1 : refCount + 1));\n      }\n    } finally {\n      lock.unlock();\n    }\n  }"
        ],
        [
            "TezCompiler::generateTaskTree(List,ParseContext,List,Set,Set)",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353 -\n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  ",
            "  @Override\n  protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, ParseContext pCtx,\n      List<Task<MoveWork>> mvTask, Set<ReadEntity> inputs, Set<WriteEntity> outputs)\n      throws SemanticException {\n\n\tPerfLogger perfLogger = SessionState.getPerfLogger();\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    ParseContext tempParseContext = getParseContext(pCtx, rootTasks);\n    GenTezUtils utils = new GenTezUtils();\n    GenTezWork genTezWork = new GenTezWork(utils);\n\n    GenTezProcContext procCtx = new GenTezProcContext(\n        conf, tempParseContext, mvTask, rootTasks, inputs, outputs);\n\n    // create a walker which walks the tree in a DFS manner while maintaining\n    // the operator stack.\n    // The dispatcher generates the plan from the operator tree\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(new RuleRegExp(\"Split Work - ReduceSink\",\n        ReduceSinkOperator.getOperatorName() + \"%\"),\n        genTezWork);\n\n    opRules.put(new RuleRegExp(\"No more walking on ReduceSink-MapJoin\",\n        MapJoinOperator.getOperatorName() + \"%\"), new ReduceSinkMapJoinProc());\n\n    opRules.put(new RuleRegExp(\"Recoginze a Sorted Merge Join operator to setup the right edge and\"\n        + \" stop traversing the DummyStore-MapJoin\", CommonMergeJoinOperator.getOperatorName()\n        + \"%\"), new MergeJoinProc());\n\n    opRules.put(new RuleRegExp(\"Split Work + Move/Merge - FileSink\",\n        FileSinkOperator.getOperatorName() + \"%\"),\n        new CompositeProcessor(new FileSinkProcessor(), genTezWork));\n\n    opRules.put(new RuleRegExp(\"Split work - DummyStore\", DummyStoreOperator.getOperatorName()\n        + \"%\"), genTezWork);\n\n    opRules.put(new RuleRegExp(\"Handle Potential Analyze Command\",\n        TableScanOperator.getOperatorName() + \"%\"),\n        new ProcessAnalyzeTable(utils));\n\n    opRules.put(new RuleRegExp(\"Remember union\",\n        UnionOperator.getOperatorName() + \"%\"),\n        new UnionProcessor());\n\n    opRules.put(new RuleRegExp(\"AppMasterEventOperator\",\n        AppMasterEventOperator.getOperatorName() + \"%\"),\n        new AppMasterEventProcessor());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(pCtx.getTopOps().values());\n    GraphWalker ogw = new GenTezWorkWalker(disp, procCtx);\n    ogw.startWalking(topNodes, null);\n\n    // we need to specify the reserved memory for each work that contains Map Join\n    for (List<BaseWork> baseWorkList : procCtx.mapJoinWorkMap.values()) {\n      for (BaseWork w : baseWorkList) {\n        // work should be the smallest unit for memory allocation\n        w.setReservedMemoryMB(\n            (int)(conf.getLongVar(ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD) / (1024 * 1024)));\n      }\n    }\n\n    // we need to clone some operator plans and remove union operators still\n    int indexForTezUnion = 0;\n    for (BaseWork w: procCtx.workWithUnionOperators) {\n      GenTezUtils.removeUnionOperators(procCtx, w, indexForTezUnion++);\n    }\n\n    // then we make sure the file sink operators are set up right\n    for (FileSinkOperator fileSink: procCtx.fileSinkSet) {\n      GenTezUtils.processFileSink(procCtx, fileSink);\n    }\n\n    // and finally we hook up any events that need to be sent to the tez AM\n    LOG.debug(\"There are \" + procCtx.eventOperatorSet.size() + \" app master events.\");\n    for (AppMasterEventOperator event : procCtx.eventOperatorSet) {\n      LOG.debug(\"Handling AppMasterEventOperator: \" + event);\n      GenTezUtils.processAppMasterEvent(procCtx, event);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"generateTaskTree\");\n  }",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353 +\n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  ",
            "  @Override\n  protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, ParseContext pCtx,\n      List<Task<MoveWork>> mvTask, Set<ReadEntity> inputs, Set<WriteEntity> outputs)\n      throws SemanticException {\n\n\tPerfLogger perfLogger = SessionState.getPerfLogger();\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    ParseContext tempParseContext = getParseContext(pCtx, rootTasks);\n    GenTezUtils utils = new GenTezUtils();\n    GenTezWork genTezWork = new GenTezWork(utils);\n\n    GenTezProcContext procCtx = new GenTezProcContext(\n        conf, tempParseContext, mvTask, rootTasks, inputs, outputs);\n\n    // create a walker which walks the tree in a DFS manner while maintaining\n    // the operator stack.\n    // The dispatcher generates the plan from the operator tree\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(new RuleRegExp(\"Split Work - ReduceSink\",\n        ReduceSinkOperator.getOperatorName() + \"%\"),\n        genTezWork);\n\n    opRules.put(new RuleRegExp(\"No more walking on ReduceSink-MapJoin\",\n        MapJoinOperator.getOperatorName() + \"%\"), new ReduceSinkMapJoinProc());\n\n    opRules.put(new RuleRegExp(\"Recognize a Sorted Merge Join operator to setup the right edge and\"\n        + \" stop traversing the DummyStore-MapJoin\", CommonMergeJoinOperator.getOperatorName()\n        + \"%\"), new MergeJoinProc());\n\n    opRules.put(new RuleRegExp(\"Split Work + Move/Merge - FileSink\",\n        FileSinkOperator.getOperatorName() + \"%\"),\n        new CompositeProcessor(new FileSinkProcessor(), genTezWork));\n\n    opRules.put(new RuleRegExp(\"Split work - DummyStore\", DummyStoreOperator.getOperatorName()\n        + \"%\"), genTezWork);\n\n    opRules.put(new RuleRegExp(\"Handle Potential Analyze Command\",\n        TableScanOperator.getOperatorName() + \"%\"),\n        new ProcessAnalyzeTable(utils));\n\n    opRules.put(new RuleRegExp(\"Remember union\",\n        UnionOperator.getOperatorName() + \"%\"),\n        new UnionProcessor());\n\n    opRules.put(new RuleRegExp(\"AppMasterEventOperator\",\n        AppMasterEventOperator.getOperatorName() + \"%\"),\n        new AppMasterEventProcessor());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(pCtx.getTopOps().values());\n    GraphWalker ogw = new GenTezWorkWalker(disp, procCtx);\n    ogw.startWalking(topNodes, null);\n\n    // we need to specify the reserved memory for each work that contains Map Join\n    for (List<BaseWork> baseWorkList : procCtx.mapJoinWorkMap.values()) {\n      for (BaseWork w : baseWorkList) {\n        // work should be the smallest unit for memory allocation\n        w.setReservedMemoryMB(\n            (int)(conf.getLongVar(ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD) / (1024 * 1024)));\n      }\n    }\n\n    // we need to clone some operator plans and remove union operators still\n    int indexForTezUnion = 0;\n    for (BaseWork w: procCtx.workWithUnionOperators) {\n      GenTezUtils.removeUnionOperators(procCtx, w, indexForTezUnion++);\n    }\n\n    // then we make sure the file sink operators are set up right\n    for (FileSinkOperator fileSink: procCtx.fileSinkSet) {\n      GenTezUtils.processFileSink(procCtx, fileSink);\n    }\n\n    // and finally we hook up any events that need to be sent to the tez AM\n    LOG.debug(\"There are \" + procCtx.eventOperatorSet.size() + \" app master events.\");\n    for (AppMasterEventOperator event : procCtx.eventOperatorSet) {\n      LOG.debug(\"Handling AppMasterEventOperator: \" + event);\n      GenTezUtils.processAppMasterEvent(procCtx, event);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"generateTaskTree\");\n  }"
        ],
        [
            "RexNodeConverter::getInputCtx(ExprNodeColumnDesc)",
            " 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 -\n 495  \n 496  \n 497  \n 498  ",
            "  private InputCtx getInputCtx(ExprNodeColumnDesc col) throws SemanticException {\n    InputCtx ctxLookingFor = null;\n\n    if (inputCtxs.size() == 1 && inputCtxs.get(0).hiveRR == null) {\n      ctxLookingFor = inputCtxs.get(0);\n    } else {\n      String tableAlias = col.getTabAlias();\n      String colAlias = col.getColumn();\n      int noInp = 0;\n      for (InputCtx ic : inputCtxs) {\n        if (tableAlias == null || ic.hiveRR.hasTableAlias(tableAlias)) {\n          if (ic.hiveRR.getPosition(colAlias) >= 0) {\n            ctxLookingFor = ic;\n            noInp++;\n          }\n        }\n      }\n\n      if (noInp > 1)\n        throw new RuntimeException(\"Ambigous column mapping\");\n    }\n\n    return ctxLookingFor;\n  }",
            " 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 +\n 495  \n 496  \n 497  \n 498  ",
            "  private InputCtx getInputCtx(ExprNodeColumnDesc col) throws SemanticException {\n    InputCtx ctxLookingFor = null;\n\n    if (inputCtxs.size() == 1 && inputCtxs.get(0).hiveRR == null) {\n      ctxLookingFor = inputCtxs.get(0);\n    } else {\n      String tableAlias = col.getTabAlias();\n      String colAlias = col.getColumn();\n      int noInp = 0;\n      for (InputCtx ic : inputCtxs) {\n        if (tableAlias == null || ic.hiveRR.hasTableAlias(tableAlias)) {\n          if (ic.hiveRR.getPosition(colAlias) >= 0) {\n            ctxLookingFor = ic;\n            noInp++;\n          }\n        }\n      }\n\n      if (noInp > 1)\n        throw new RuntimeException(\"Ambiguous column mapping\");\n    }\n\n    return ctxLookingFor;\n  }"
        ],
        [
            "PartitionPruner::prune(Table,ExprNodeDesc,HiveConf,String,Map)",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "  /**\n   * Get the partition list for the table that satisfies the partition pruner\n   * condition.\n   *\n   * @param tab\n   *          the table object for the alias\n   * @param prunerExpr\n   *          the pruner expression for the alias\n   * @param conf\n   *          for checking whether \"strict\" mode is on.\n   * @param alias\n   *          for generating error message only.\n   * @param prunedPartitionsMap\n   *          cached result for the table\n   * @return the partition list for the table that satisfies the partition\n   *         pruner condition.\n   * @throws SemanticException\n   */\n  public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,\n      HiveConf conf, String alias, Map<String, PrunedPartitionList> prunedPartitionsMap)\n          throws SemanticException {\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Started pruning partiton\");\n      LOG.trace(\"dbname = \" + tab.getDbName());\n      LOG.trace(\"tabname = \" + tab.getTableName());\n      LOG.trace(\"prune Expression = \" + (prunerExpr == null ? \"\" : prunerExpr));\n    }\n\n    String key = tab.getDbName() + \".\" + tab.getTableName() + \";\";\n\n    if (!tab.isPartitioned()) {\n      // If the table is not partitioned, return empty list.\n      return getAllPartsFromCacheOrServer(tab, key, false, prunedPartitionsMap);\n    }\n\n    if (!hasColumnExpr(prunerExpr)) {\n      // If the \"strict\" mode is on, we have to provide partition pruner for each table.\n      String error = StrictChecks.checkNoPartitionFilter(conf);\n      if (error != null) {\n        throw new SemanticException(error + \" No partition predicate for Alias \\\"\"\n            + alias + \"\\\" Table \\\"\" + tab.getTableName() + \"\\\"\");\n      }\n    }\n\n    if (prunerExpr == null) {\n      // In non-strict mode and there is no predicates at all - get everything.\n      return getAllPartsFromCacheOrServer(tab, key, false, prunedPartitionsMap);\n    }\n\n    Set<String> partColsUsedInFilter = new LinkedHashSet<String>();\n    // Replace virtual columns with nulls. See javadoc for details.\n    prunerExpr = removeNonPartCols(prunerExpr, extractPartColNames(tab), partColsUsedInFilter);\n    // Remove all parts that are not partition columns. See javadoc for details.\n    ExprNodeDesc compactExpr = compactExpr(prunerExpr.clone());\n    String oldFilter = prunerExpr.getExprString();\n    if (compactExpr == null || isBooleanExpr(compactExpr)) {\n      if (isFalseExpr(compactExpr)) {\n        return new PrunedPartitionList(\n            tab, new LinkedHashSet<Partition>(0), new ArrayList<String>(0), false);\n      }\n      // For null and true values, return every partition\n      return getAllPartsFromCacheOrServer(tab, key, true, prunedPartitionsMap);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Filter w/ compacting: \" + compactExpr.getExprString()\n          + \"; filter w/o compacting: \" + oldFilter);\n    }\n\n    key = key + compactExpr.getExprString();\n    PrunedPartitionList ppList = prunedPartitionsMap.get(key);\n    if (ppList != null) {\n      return ppList;\n    }\n\n    ppList = getPartitionsFromServer(tab, (ExprNodeGenericFuncDesc)compactExpr, conf, alias, partColsUsedInFilter, oldFilter.equals(compactExpr.getExprString()));\n    prunedPartitionsMap.put(key, ppList);\n    return ppList;\n  }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "  /**\n   * Get the partition list for the table that satisfies the partition pruner\n   * condition.\n   *\n   * @param tab\n   *          the table object for the alias\n   * @param prunerExpr\n   *          the pruner expression for the alias\n   * @param conf\n   *          for checking whether \"strict\" mode is on.\n   * @param alias\n   *          for generating error message only.\n   * @param prunedPartitionsMap\n   *          cached result for the table\n   * @return the partition list for the table that satisfies the partition\n   *         pruner condition.\n   * @throws SemanticException\n   */\n  public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,\n      HiveConf conf, String alias, Map<String, PrunedPartitionList> prunedPartitionsMap)\n          throws SemanticException {\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Started pruning partition\");\n      LOG.trace(\"dbname = \" + tab.getDbName());\n      LOG.trace(\"tabname = \" + tab.getTableName());\n      LOG.trace(\"prune Expression = \" + (prunerExpr == null ? \"\" : prunerExpr));\n    }\n\n    String key = tab.getDbName() + \".\" + tab.getTableName() + \";\";\n\n    if (!tab.isPartitioned()) {\n      // If the table is not partitioned, return empty list.\n      return getAllPartsFromCacheOrServer(tab, key, false, prunedPartitionsMap);\n    }\n\n    if (!hasColumnExpr(prunerExpr)) {\n      // If the \"strict\" mode is on, we have to provide partition pruner for each table.\n      String error = StrictChecks.checkNoPartitionFilter(conf);\n      if (error != null) {\n        throw new SemanticException(error + \" No partition predicate for Alias \\\"\"\n            + alias + \"\\\" Table \\\"\" + tab.getTableName() + \"\\\"\");\n      }\n    }\n\n    if (prunerExpr == null) {\n      // In non-strict mode and there is no predicates at all - get everything.\n      return getAllPartsFromCacheOrServer(tab, key, false, prunedPartitionsMap);\n    }\n\n    Set<String> partColsUsedInFilter = new LinkedHashSet<String>();\n    // Replace virtual columns with nulls. See javadoc for details.\n    prunerExpr = removeNonPartCols(prunerExpr, extractPartColNames(tab), partColsUsedInFilter);\n    // Remove all parts that are not partition columns. See javadoc for details.\n    ExprNodeDesc compactExpr = compactExpr(prunerExpr.clone());\n    String oldFilter = prunerExpr.getExprString();\n    if (compactExpr == null || isBooleanExpr(compactExpr)) {\n      if (isFalseExpr(compactExpr)) {\n        return new PrunedPartitionList(\n            tab, new LinkedHashSet<Partition>(0), new ArrayList<String>(0), false);\n      }\n      // For null and true values, return every partition\n      return getAllPartsFromCacheOrServer(tab, key, true, prunedPartitionsMap);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Filter w/ compacting: \" + compactExpr.getExprString()\n          + \"; filter w/o compacting: \" + oldFilter);\n    }\n\n    key = key + compactExpr.getExprString();\n    PrunedPartitionList ppList = prunedPartitionsMap.get(key);\n    if (ppList != null) {\n      return ppList;\n    }\n\n    ppList = getPartitionsFromServer(tab, (ExprNodeGenericFuncDesc)compactExpr, conf, alias, partColsUsedInFilter, oldFilter.equals(compactExpr.getExprString()));\n    prunedPartitionsMap.put(key, ppList);\n    return ppList;\n  }"
        ],
        [
            "SemanticAnalyzer::genAutoColumnStatsGatheringPipeline(QB,TableDesc,Map,Operator,boolean)",
            "7168  \n7169  \n7170  \n7171  \n7172  \n7173  \n7174  \n7175  \n7176  \n7177 -\n7178  \n7179  \n7180  \n7181  \n7182  \n7183  ",
            "  private void genAutoColumnStatsGatheringPipeline(QB qb, TableDesc table_desc,\n      Map<String, String> partSpec, Operator curr, boolean isInsertInto) throws SemanticException {\n    String tableName = table_desc.getTableName();\n    Table table = null;\n    try {\n      table = db.getTable(tableName);\n    } catch (HiveException e) {\n      throw new SemanticException(e.getMessage());\n    }\n    LOG.info(\"Generate an operator pipleline to autogather column stats for table \" + tableName\n        + \" in query \" + ctx.getCmd());\n    ColumnStatsAutoGatherContext columnStatsAutoGatherContext = null;\n    columnStatsAutoGatherContext = new ColumnStatsAutoGatherContext(this, conf, curr, table, partSpec, isInsertInto, ctx);\n    columnStatsAutoGatherContext.insertAnalyzePipeline();\n    columnStatsAutoGatherContexts.add(columnStatsAutoGatherContext);\n  }",
            "7168  \n7169  \n7170  \n7171  \n7172  \n7173  \n7174  \n7175  \n7176  \n7177 +\n7178  \n7179  \n7180  \n7181  \n7182  \n7183  ",
            "  private void genAutoColumnStatsGatheringPipeline(QB qb, TableDesc table_desc,\n      Map<String, String> partSpec, Operator curr, boolean isInsertInto) throws SemanticException {\n    String tableName = table_desc.getTableName();\n    Table table = null;\n    try {\n      table = db.getTable(tableName);\n    } catch (HiveException e) {\n      throw new SemanticException(e.getMessage());\n    }\n    LOG.info(\"Generate an operator pipeline to autogather column stats for table \" + tableName\n        + \" in query \" + ctx.getCmd());\n    ColumnStatsAutoGatherContext columnStatsAutoGatherContext = null;\n    columnStatsAutoGatherContext = new ColumnStatsAutoGatherContext(this, conf, curr, table, partSpec, isInsertInto, ctx);\n    columnStatsAutoGatherContext.insertAnalyzePipeline();\n    columnStatsAutoGatherContexts.add(columnStatsAutoGatherContext);\n  }"
        ],
        [
            "GenMapRedUtils::setMapWork(MapWork,ParseContext,Set,PrunedPartitionList,TableScanOperator,String,HiveConf,boolean)",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 -\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  ",
            "  /**\n   * initialize MapWork\n   *\n   * @param alias_id\n   *          current alias\n   * @param topOp\n   *          the top operator of the stack\n   * @param plan\n   *          map work to initialize\n   * @param local\n   *          whether you need to add to map-reduce or local work\n   * @param pList\n   *          pruned partition list. If it is null it will be computed on-the-fly.\n   * @param inputs\n   *          read entities for the map work\n   * @param conf\n   *          current instance of hive conf\n   */\n  public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntity> inputs,\n      PrunedPartitionList partsList, TableScanOperator tsOp, String alias_id,\n      HiveConf conf, boolean local) throws SemanticException {\n    ArrayList<Path> partDir = new ArrayList<Path>();\n    ArrayList<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();\n    boolean isAcidTable = false;\n\n    Path tblDir = null;\n    plan.setNameToSplitSample(parseCtx.getNameToSplitSample());\n\n    if (partsList == null) {\n      try {\n        partsList = PartitionPruner.prune(tsOp, parseCtx, alias_id);\n        isAcidTable = tsOp.getConf().isAcidTable();\n      } catch (SemanticException e) {\n        throw e;\n      }\n    }\n\n    // Generate the map work for this alias_id\n    // pass both confirmed and unknown partitions through the map-reduce\n    // framework\n    Set<Partition> parts = partsList.getPartitions();\n    PartitionDesc aliasPartnDesc = null;\n    try {\n      if (!parts.isEmpty()) {\n        aliasPartnDesc = Utilities.getPartitionDesc(parts.iterator().next());\n      }\n    } catch (HiveException e) {\n      LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      throw new SemanticException(e.getMessage(), e);\n    }\n\n    // The table does not have any partitions\n    if (aliasPartnDesc == null) {\n      aliasPartnDesc = new PartitionDesc(Utilities.getTableDesc(tsOp\n          .getConf().getTableMetadata()), null);\n    }\n\n    Map<String, String> props = tsOp.getConf().getOpProps();\n    if (props != null) {\n      Properties target = aliasPartnDesc.getProperties();\n      target.putAll(props);\n    }\n\n    plan.getAliasToPartnInfo().put(alias_id, aliasPartnDesc);\n\n    long sizeNeeded = Integer.MAX_VALUE;\n    int fileLimit = -1;\n    if (parseCtx.getGlobalLimitCtx().isEnable()) {\n      if (isAcidTable) {\n        LOG.info(\"Skip Global Limit optimization for ACID table\");\n        parseCtx.getGlobalLimitCtx().disableOpt();\n      } else {\n        long sizePerRow = HiveConf.getLongVar(parseCtx.getConf(),\n            HiveConf.ConfVars.HIVELIMITMAXROWSIZE);\n        sizeNeeded = (parseCtx.getGlobalLimitCtx().getGlobalOffset()\n            + parseCtx.getGlobalLimitCtx().getGlobalLimit()) * sizePerRow;\n        // for the optimization that reduce number of input file, we limit number\n        // of files allowed. If more than specific number of files have to be\n        // selected, we skip this optimization. Since having too many files as\n        // inputs can cause unpredictable latency. It's not necessarily to be\n        // cheaper.\n        fileLimit =\n            HiveConf.getIntVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITOPTLIMITFILE);\n\n        if (sizePerRow <= 0 || fileLimit <= 0) {\n          LOG.info(\"Skip optimization to reduce input size of 'limit'\");\n          parseCtx.getGlobalLimitCtx().disableOpt();\n        } else if (parts.isEmpty()) {\n          LOG.info(\"Empty input: skip limit optimiztion\");\n        } else {\n          LOG.info(\"Try to reduce input size for 'limit' \" +\n              \"sizeNeeded: \" + sizeNeeded +\n              \"  file limit : \" + fileLimit);\n        }\n      }\n    }\n    boolean isFirstPart = true;\n    boolean emptyInput = true;\n    boolean singlePartition = (parts.size() == 1);\n\n    // Track the dependencies for the view. Consider a query like: select * from V;\n    // where V is a view of the form: select * from T\n    // The dependencies should include V at depth 0, and T at depth 1 (inferred).\n    Map<String, ReadEntity> viewToInput = parseCtx.getViewAliasToInput();\n    ReadEntity parentViewInfo = PlanUtils.getParentViewInfo(alias_id, viewToInput);\n\n    // The table should also be considered a part of inputs, even if the table is a\n    // partitioned table and whether any partition is selected or not\n\n    //This read entity is a direct read entity and not an indirect read (that is when\n    // this is being read because it is a dependency of a view).\n    boolean isDirectRead = (parentViewInfo == null);\n    TableDesc tblDesc = null;\n    boolean initTableDesc = false;\n\n    PlanUtils.addPartitionInputs(parts, inputs, parentViewInfo, isDirectRead);\n\n    for (Partition part: parts) {\n      // Later the properties have to come from the partition as opposed\n      // to from the table in order to support versioning.\n      Path[] paths = null;\n      SampleDesc sampleDescr = parseCtx.getOpToSamplePruner().get(tsOp);\n\n      // Lookup list bucketing pruner\n      Map<String, ExprNodeDesc> partToPruner = parseCtx.getOpToPartToSkewedPruner().get(tsOp);\n      ExprNodeDesc listBucketingPruner = (partToPruner != null) ? partToPruner.get(part.getName())\n          : null;\n\n      if (sampleDescr != null) {\n        assert (listBucketingPruner == null) : \"Sampling and list bucketing can't coexit.\";\n        paths = SamplePruner.prune(part, sampleDescr);\n        parseCtx.getGlobalLimitCtx().disableOpt();\n      } else if (listBucketingPruner != null) {\n        assert (sampleDescr == null) : \"Sampling and list bucketing can't coexist.\";\n        /* Use list bucketing prunner's path. */\n        paths = ListBucketingPruner.prune(parseCtx, part, listBucketingPruner);\n      } else {\n        // Now we only try the first partition, if the first partition doesn't\n        // contain enough size, we change to normal mode.\n        if (parseCtx.getGlobalLimitCtx().isEnable()) {\n          if (isFirstPart) {\n            long sizeLeft = sizeNeeded;\n            ArrayList<Path> retPathList = new ArrayList<Path>();\n            SamplePruner.LimitPruneRetStatus status = SamplePruner.limitPrune(part, sizeLeft,\n                fileLimit, retPathList);\n            if (status.equals(SamplePruner.LimitPruneRetStatus.NoFile)) {\n              continue;\n            } else if (status.equals(SamplePruner.LimitPruneRetStatus.NotQualify)) {\n              LOG.info(\"Use full input -- first \" + fileLimit + \" files are more than \"\n                  + sizeNeeded\n                  + \" bytes\");\n\n              parseCtx.getGlobalLimitCtx().disableOpt();\n\n            } else {\n              emptyInput = false;\n              paths = new Path[retPathList.size()];\n              int index = 0;\n              for (Path path : retPathList) {\n                paths[index++] = path;\n              }\n              if (status.equals(SamplePruner.LimitPruneRetStatus.NeedAllFiles) && singlePartition) {\n                // if all files are needed to meet the size limit, we disable\n                // optimization. It usually happens for empty table/partition or\n                // table/partition with only one file. By disabling this\n                // optimization, we can avoid retrying the query if there is\n                // not sufficient rows.\n                parseCtx.getGlobalLimitCtx().disableOpt();\n              }\n            }\n            isFirstPart = false;\n          } else {\n            paths = new Path[0];\n          }\n        }\n        if (!parseCtx.getGlobalLimitCtx().isEnable()) {\n          paths = part.getPath();\n        }\n      }\n\n      // is it a partitioned table ?\n      if (!part.getTable().isPartitioned()) {\n        assert (tblDir == null);\n\n        tblDir = paths[0];\n        if (!initTableDesc) {\n          tblDesc = Utilities.getTableDesc(part.getTable());\n          initTableDesc = true;\n        }\n      } else if (tblDesc == null) {\n        if (!initTableDesc) {\n          tblDesc = Utilities.getTableDesc(part.getTable());\n          initTableDesc = true;\n        }\n      }\n\n      if (props != null) {\n        Properties target = tblDesc.getProperties();\n        target.putAll(props);\n      }\n\n      for (Path p : paths) {\n        if (p == null) {\n          continue;\n        }\n        String path = p.toString();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + path + \" of table\" + alias_id);\n        }\n\n        partDir.add(p);\n        try {\n          if (part.getTable().isPartitioned()) {\n            partDesc.add(Utilities.getPartitionDesc(part));\n          }\n          else {\n            partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part, false));\n          }\n        } catch (HiveException e) {\n          LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));\n          throw new SemanticException(e.getMessage(), e);\n        }\n      }\n    }\n\n    if (emptyInput) {\n      parseCtx.getGlobalLimitCtx().disableOpt();\n    }\n\n    Utilities.addSchemaEvolutionToTableScanOperator(partsList.getSourceTable(),tsOp);\n\n    Iterator<Path> iterPath = partDir.iterator();\n    Iterator<PartitionDesc> iterPartnDesc = partDesc.iterator();\n\n    if (!local) {\n      while (iterPath.hasNext()) {\n        assert iterPartnDesc.hasNext();\n        Path path = iterPath.next();\n\n        PartitionDesc prtDesc = iterPartnDesc.next();\n\n        // Add the path to alias mapping\n        plan.addPathToAlias(path,alias_id);\n        plan.addPathToPartitionInfo(path, prtDesc);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Information added for path \" + path);\n        }\n      }\n\n      assert plan.getAliasToWork().get(alias_id) == null;\n      plan.getAliasToWork().put(alias_id, tsOp);\n    } else {\n      // populate local work if needed\n      MapredLocalWork localPlan = plan.getMapRedLocalWork();\n      if (localPlan == null) {\n        localPlan = new MapredLocalWork(\n            new LinkedHashMap<String, Operator<? extends OperatorDesc>>(),\n            new LinkedHashMap<String, FetchWork>());\n      }\n\n      assert localPlan.getAliasToWork().get(alias_id) == null;\n      assert localPlan.getAliasToFetchWork().get(alias_id) == null;\n      localPlan.getAliasToWork().put(alias_id, tsOp);\n      if (tblDir == null) {\n        tblDesc = Utilities.getTableDesc(partsList.getSourceTable());\n        localPlan.getAliasToFetchWork().put(\n            alias_id,\n            new FetchWork(partDir, partDesc, tblDesc));\n      } else {\n        localPlan.getAliasToFetchWork().put(alias_id,\n            new FetchWork(tblDir, tblDesc));\n      }\n      plan.setMapRedLocalWork(localPlan);\n    }\n  }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 +\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  ",
            "  /**\n   * initialize MapWork\n   *\n   * @param alias_id\n   *          current alias\n   * @param topOp\n   *          the top operator of the stack\n   * @param plan\n   *          map work to initialize\n   * @param local\n   *          whether you need to add to map-reduce or local work\n   * @param pList\n   *          pruned partition list. If it is null it will be computed on-the-fly.\n   * @param inputs\n   *          read entities for the map work\n   * @param conf\n   *          current instance of hive conf\n   */\n  public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntity> inputs,\n      PrunedPartitionList partsList, TableScanOperator tsOp, String alias_id,\n      HiveConf conf, boolean local) throws SemanticException {\n    ArrayList<Path> partDir = new ArrayList<Path>();\n    ArrayList<PartitionDesc> partDesc = new ArrayList<PartitionDesc>();\n    boolean isAcidTable = false;\n\n    Path tblDir = null;\n    plan.setNameToSplitSample(parseCtx.getNameToSplitSample());\n\n    if (partsList == null) {\n      try {\n        partsList = PartitionPruner.prune(tsOp, parseCtx, alias_id);\n        isAcidTable = tsOp.getConf().isAcidTable();\n      } catch (SemanticException e) {\n        throw e;\n      }\n    }\n\n    // Generate the map work for this alias_id\n    // pass both confirmed and unknown partitions through the map-reduce\n    // framework\n    Set<Partition> parts = partsList.getPartitions();\n    PartitionDesc aliasPartnDesc = null;\n    try {\n      if (!parts.isEmpty()) {\n        aliasPartnDesc = Utilities.getPartitionDesc(parts.iterator().next());\n      }\n    } catch (HiveException e) {\n      LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      throw new SemanticException(e.getMessage(), e);\n    }\n\n    // The table does not have any partitions\n    if (aliasPartnDesc == null) {\n      aliasPartnDesc = new PartitionDesc(Utilities.getTableDesc(tsOp\n          .getConf().getTableMetadata()), null);\n    }\n\n    Map<String, String> props = tsOp.getConf().getOpProps();\n    if (props != null) {\n      Properties target = aliasPartnDesc.getProperties();\n      target.putAll(props);\n    }\n\n    plan.getAliasToPartnInfo().put(alias_id, aliasPartnDesc);\n\n    long sizeNeeded = Integer.MAX_VALUE;\n    int fileLimit = -1;\n    if (parseCtx.getGlobalLimitCtx().isEnable()) {\n      if (isAcidTable) {\n        LOG.info(\"Skip Global Limit optimization for ACID table\");\n        parseCtx.getGlobalLimitCtx().disableOpt();\n      } else {\n        long sizePerRow = HiveConf.getLongVar(parseCtx.getConf(),\n            HiveConf.ConfVars.HIVELIMITMAXROWSIZE);\n        sizeNeeded = (parseCtx.getGlobalLimitCtx().getGlobalOffset()\n            + parseCtx.getGlobalLimitCtx().getGlobalLimit()) * sizePerRow;\n        // for the optimization that reduce number of input file, we limit number\n        // of files allowed. If more than specific number of files have to be\n        // selected, we skip this optimization. Since having too many files as\n        // inputs can cause unpredictable latency. It's not necessarily to be\n        // cheaper.\n        fileLimit =\n            HiveConf.getIntVar(parseCtx.getConf(), HiveConf.ConfVars.HIVELIMITOPTLIMITFILE);\n\n        if (sizePerRow <= 0 || fileLimit <= 0) {\n          LOG.info(\"Skip optimization to reduce input size of 'limit'\");\n          parseCtx.getGlobalLimitCtx().disableOpt();\n        } else if (parts.isEmpty()) {\n          LOG.info(\"Empty input: skip limit optimization\");\n        } else {\n          LOG.info(\"Try to reduce input size for 'limit' \" +\n              \"sizeNeeded: \" + sizeNeeded +\n              \"  file limit : \" + fileLimit);\n        }\n      }\n    }\n    boolean isFirstPart = true;\n    boolean emptyInput = true;\n    boolean singlePartition = (parts.size() == 1);\n\n    // Track the dependencies for the view. Consider a query like: select * from V;\n    // where V is a view of the form: select * from T\n    // The dependencies should include V at depth 0, and T at depth 1 (inferred).\n    Map<String, ReadEntity> viewToInput = parseCtx.getViewAliasToInput();\n    ReadEntity parentViewInfo = PlanUtils.getParentViewInfo(alias_id, viewToInput);\n\n    // The table should also be considered a part of inputs, even if the table is a\n    // partitioned table and whether any partition is selected or not\n\n    //This read entity is a direct read entity and not an indirect read (that is when\n    // this is being read because it is a dependency of a view).\n    boolean isDirectRead = (parentViewInfo == null);\n    TableDesc tblDesc = null;\n    boolean initTableDesc = false;\n\n    PlanUtils.addPartitionInputs(parts, inputs, parentViewInfo, isDirectRead);\n\n    for (Partition part: parts) {\n      // Later the properties have to come from the partition as opposed\n      // to from the table in order to support versioning.\n      Path[] paths = null;\n      SampleDesc sampleDescr = parseCtx.getOpToSamplePruner().get(tsOp);\n\n      // Lookup list bucketing pruner\n      Map<String, ExprNodeDesc> partToPruner = parseCtx.getOpToPartToSkewedPruner().get(tsOp);\n      ExprNodeDesc listBucketingPruner = (partToPruner != null) ? partToPruner.get(part.getName())\n          : null;\n\n      if (sampleDescr != null) {\n        assert (listBucketingPruner == null) : \"Sampling and list bucketing can't coexit.\";\n        paths = SamplePruner.prune(part, sampleDescr);\n        parseCtx.getGlobalLimitCtx().disableOpt();\n      } else if (listBucketingPruner != null) {\n        assert (sampleDescr == null) : \"Sampling and list bucketing can't coexist.\";\n        /* Use list bucketing prunner's path. */\n        paths = ListBucketingPruner.prune(parseCtx, part, listBucketingPruner);\n      } else {\n        // Now we only try the first partition, if the first partition doesn't\n        // contain enough size, we change to normal mode.\n        if (parseCtx.getGlobalLimitCtx().isEnable()) {\n          if (isFirstPart) {\n            long sizeLeft = sizeNeeded;\n            ArrayList<Path> retPathList = new ArrayList<Path>();\n            SamplePruner.LimitPruneRetStatus status = SamplePruner.limitPrune(part, sizeLeft,\n                fileLimit, retPathList);\n            if (status.equals(SamplePruner.LimitPruneRetStatus.NoFile)) {\n              continue;\n            } else if (status.equals(SamplePruner.LimitPruneRetStatus.NotQualify)) {\n              LOG.info(\"Use full input -- first \" + fileLimit + \" files are more than \"\n                  + sizeNeeded\n                  + \" bytes\");\n\n              parseCtx.getGlobalLimitCtx().disableOpt();\n\n            } else {\n              emptyInput = false;\n              paths = new Path[retPathList.size()];\n              int index = 0;\n              for (Path path : retPathList) {\n                paths[index++] = path;\n              }\n              if (status.equals(SamplePruner.LimitPruneRetStatus.NeedAllFiles) && singlePartition) {\n                // if all files are needed to meet the size limit, we disable\n                // optimization. It usually happens for empty table/partition or\n                // table/partition with only one file. By disabling this\n                // optimization, we can avoid retrying the query if there is\n                // not sufficient rows.\n                parseCtx.getGlobalLimitCtx().disableOpt();\n              }\n            }\n            isFirstPart = false;\n          } else {\n            paths = new Path[0];\n          }\n        }\n        if (!parseCtx.getGlobalLimitCtx().isEnable()) {\n          paths = part.getPath();\n        }\n      }\n\n      // is it a partitioned table ?\n      if (!part.getTable().isPartitioned()) {\n        assert (tblDir == null);\n\n        tblDir = paths[0];\n        if (!initTableDesc) {\n          tblDesc = Utilities.getTableDesc(part.getTable());\n          initTableDesc = true;\n        }\n      } else if (tblDesc == null) {\n        if (!initTableDesc) {\n          tblDesc = Utilities.getTableDesc(part.getTable());\n          initTableDesc = true;\n        }\n      }\n\n      if (props != null) {\n        Properties target = tblDesc.getProperties();\n        target.putAll(props);\n      }\n\n      for (Path p : paths) {\n        if (p == null) {\n          continue;\n        }\n        String path = p.toString();\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + path + \" of table\" + alias_id);\n        }\n\n        partDir.add(p);\n        try {\n          if (part.getTable().isPartitioned()) {\n            partDesc.add(Utilities.getPartitionDesc(part));\n          }\n          else {\n            partDesc.add(Utilities.getPartitionDescFromTableDesc(tblDesc, part, false));\n          }\n        } catch (HiveException e) {\n          LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));\n          throw new SemanticException(e.getMessage(), e);\n        }\n      }\n    }\n\n    if (emptyInput) {\n      parseCtx.getGlobalLimitCtx().disableOpt();\n    }\n\n    Utilities.addSchemaEvolutionToTableScanOperator(partsList.getSourceTable(),tsOp);\n\n    Iterator<Path> iterPath = partDir.iterator();\n    Iterator<PartitionDesc> iterPartnDesc = partDesc.iterator();\n\n    if (!local) {\n      while (iterPath.hasNext()) {\n        assert iterPartnDesc.hasNext();\n        Path path = iterPath.next();\n\n        PartitionDesc prtDesc = iterPartnDesc.next();\n\n        // Add the path to alias mapping\n        plan.addPathToAlias(path,alias_id);\n        plan.addPathToPartitionInfo(path, prtDesc);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Information added for path \" + path);\n        }\n      }\n\n      assert plan.getAliasToWork().get(alias_id) == null;\n      plan.getAliasToWork().put(alias_id, tsOp);\n    } else {\n      // populate local work if needed\n      MapredLocalWork localPlan = plan.getMapRedLocalWork();\n      if (localPlan == null) {\n        localPlan = new MapredLocalWork(\n            new LinkedHashMap<String, Operator<? extends OperatorDesc>>(),\n            new LinkedHashMap<String, FetchWork>());\n      }\n\n      assert localPlan.getAliasToWork().get(alias_id) == null;\n      assert localPlan.getAliasToFetchWork().get(alias_id) == null;\n      localPlan.getAliasToWork().put(alias_id, tsOp);\n      if (tblDir == null) {\n        tblDesc = Utilities.getTableDesc(partsList.getSourceTable());\n        localPlan.getAliasToFetchWork().put(\n            alias_id,\n            new FetchWork(partDir, partDesc, tblDesc));\n      } else {\n        localPlan.getAliasToFetchWork().put(alias_id,\n            new FetchWork(tblDir, tblDesc));\n      }\n      plan.setMapRedLocalWork(localPlan);\n    }\n  }"
        ],
        [
            "ArchiveUtils::PartSpecInfo::create(Table,Map)",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "    /**\n     * Extract partial prefix specification from table and key-value map\n     *\n     * @param tbl table in which partition is\n     * @param partSpec specification of partition\n     * @return extracted specification\n     */\n    static public PartSpecInfo create(Table tbl, Map<String, String> partSpec)\n        throws HiveException {\n      // we have to check if we receive prefix of partition keys so in table\n      // scheme like table/ds=2011-01-02/hr=13/\n      // ARCHIVE PARTITION (ds='2011-01-02') will work and\n      // ARCHIVE PARTITION(hr='13') won't\n      List<FieldSchema> prefixFields = new ArrayList<FieldSchema>();\n      List<String> prefixValues = new ArrayList<String>();\n      List<FieldSchema> partCols = tbl.getPartCols();\n      Iterator<String> itrPsKeys = partSpec.keySet().iterator();\n      for (FieldSchema fs : partCols) {\n        if (!itrPsKeys.hasNext()) {\n          break;\n        }\n        if (!itrPsKeys.next().toLowerCase().equals(\n            fs.getName().toLowerCase())) {\n          throw new HiveException(\"Invalid partition specifiation: \"\n              + partSpec);\n        }\n        prefixFields.add(fs);\n        prefixValues.add(partSpec.get(fs.getName()));\n      }\n\n      return new PartSpecInfo(prefixFields, prefixValues);\n    }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "    /**\n     * Extract partial prefix specification from table and key-value map\n     *\n     * @param tbl table in which partition is\n     * @param partSpec specification of partition\n     * @return extracted specification\n     */\n    static public PartSpecInfo create(Table tbl, Map<String, String> partSpec)\n        throws HiveException {\n      // we have to check if we receive prefix of partition keys so in table\n      // scheme like table/ds=2011-01-02/hr=13/\n      // ARCHIVE PARTITION (ds='2011-01-02') will work and\n      // ARCHIVE PARTITION(hr='13') won't\n      List<FieldSchema> prefixFields = new ArrayList<FieldSchema>();\n      List<String> prefixValues = new ArrayList<String>();\n      List<FieldSchema> partCols = tbl.getPartCols();\n      Iterator<String> itrPsKeys = partSpec.keySet().iterator();\n      for (FieldSchema fs : partCols) {\n        if (!itrPsKeys.hasNext()) {\n          break;\n        }\n        if (!itrPsKeys.next().toLowerCase().equals(\n            fs.getName().toLowerCase())) {\n          throw new HiveException(\"Invalid partition specification: \"\n              + partSpec);\n        }\n        prefixFields.add(fs);\n        prefixValues.add(partSpec.get(fs.getName()));\n      }\n\n      return new PartSpecInfo(prefixFields, prefixValues);\n    }"
        ],
        [
            "SemanticAnalyzer::analyzeCreateTable(ASTNode,QB,PlannerContext)",
            "11641  \n11642  \n11643  \n11644  \n11645  \n11646  \n11647  \n11648  \n11649  \n11650  \n11651  \n11652  \n11653  \n11654  \n11655  \n11656  \n11657  \n11658  \n11659  \n11660  \n11661  \n11662  \n11663  \n11664  \n11665  \n11666  \n11667  \n11668  \n11669  \n11670  \n11671  \n11672  \n11673  \n11674  \n11675  \n11676  \n11677  \n11678  \n11679  \n11680  \n11681  \n11682  \n11683  \n11684  \n11685  \n11686  \n11687  \n11688  \n11689  \n11690  \n11691  \n11692  \n11693  \n11694  \n11695  \n11696  \n11697  \n11698  \n11699  \n11700  \n11701  \n11702  \n11703  \n11704  \n11705  \n11706  \n11707  \n11708  \n11709  \n11710  \n11711  \n11712  \n11713  \n11714  \n11715  \n11716  \n11717  \n11718  \n11719  \n11720  \n11721  \n11722  \n11723  \n11724  \n11725  \n11726  \n11727  \n11728  \n11729  \n11730  \n11731  \n11732  \n11733  \n11734  \n11735  \n11736  \n11737  \n11738  \n11739  \n11740  \n11741  \n11742  \n11743  \n11744  \n11745  \n11746  \n11747  \n11748  \n11749  \n11750  \n11751  \n11752  \n11753  \n11754  \n11755  \n11756  \n11757  \n11758  \n11759  \n11760  \n11761  \n11762  \n11763  \n11764  \n11765  \n11766  \n11767  \n11768  \n11769  \n11770  \n11771  \n11772  \n11773  \n11774  \n11775  \n11776  \n11777  \n11778  \n11779  \n11780  \n11781  \n11782  \n11783  \n11784  \n11785  \n11786  \n11787  \n11788  \n11789  \n11790  \n11791  \n11792  \n11793  \n11794  \n11795  \n11796  \n11797  \n11798  \n11799  \n11800  \n11801  \n11802  \n11803  \n11804  \n11805  \n11806  \n11807  \n11808  \n11809  \n11810  \n11811  \n11812  \n11813  \n11814  \n11815  \n11816  \n11817  \n11818  \n11819  \n11820  \n11821  \n11822  \n11823  \n11824  \n11825  \n11826  \n11827 -\n11828  \n11829  \n11830  \n11831  \n11832  \n11833  \n11834  \n11835  \n11836  \n11837  \n11838  \n11839  \n11840  \n11841  \n11842  \n11843  \n11844  \n11845  \n11846  \n11847  \n11848  \n11849  \n11850  \n11851  \n11852  \n11853  \n11854  \n11855  \n11856  \n11857  \n11858  \n11859  \n11860  \n11861  \n11862  \n11863  \n11864  \n11865  \n11866  \n11867  \n11868  \n11869  \n11870  \n11871  \n11872  \n11873  \n11874  \n11875  \n11876  \n11877  \n11878  \n11879  \n11880  \n11881  \n11882  \n11883  \n11884  \n11885  \n11886  \n11887  \n11888  \n11889  \n11890  \n11891  \n11892  \n11893  \n11894  \n11895  \n11896  \n11897  \n11898  \n11899  \n11900  \n11901  \n11902  \n11903  \n11904  \n11905  \n11906  \n11907  \n11908  \n11909  \n11910  \n11911  \n11912  \n11913  \n11914  \n11915  \n11916  \n11917  \n11918  \n11919  \n11920  \n11921  \n11922  \n11923  \n11924  \n11925  \n11926  \n11927  \n11928  \n11929  \n11930  \n11931  \n11932  \n11933  \n11934  \n11935  \n11936  \n11937  \n11938  \n11939  \n11940  \n11941  \n11942  \n11943  \n11944  \n11945  \n11946  \n11947  \n11948  \n11949  \n11950  \n11951  \n11952  \n11953  \n11954  \n11955  \n11956  \n11957  \n11958  \n11959  \n11960  \n11961  \n11962  \n11963  \n11964  \n11965  \n11966  \n11967  \n11968  \n11969  \n11970  \n11971  \n11972  \n11973  \n11974  \n11975  ",
            "  /**\n   * Analyze the create table command. If it is a regular create-table or\n   * create-table-like statements, we create a DDLWork and return true. If it is\n   * a create-table-as-select, we get the necessary info such as the SerDe and\n   * Storage Format and put it in QB, and return false, indicating the rest of\n   * the semantic analyzer need to deal with the select statement with respect\n   * to the SerDe and Storage Format.\n   */\n  ASTNode analyzeCreateTable(\n      ASTNode ast, QB qb, PlannerContext plannerCtx) throws SemanticException {\n    String[] qualifiedTabName = getQualifiedTableName((ASTNode) ast.getChild(0));\n    String dbDotTab = getDotName(qualifiedTabName);\n\n    String likeTableName = null;\n    List<FieldSchema> cols = new ArrayList<FieldSchema>();\n    List<FieldSchema> partCols = new ArrayList<FieldSchema>();\n    List<String> bucketCols = new ArrayList<String>();\n    List<SQLPrimaryKey> primaryKeys = new ArrayList<SQLPrimaryKey>();\n    List<SQLForeignKey> foreignKeys = new ArrayList<SQLForeignKey>();\n    List<Order> sortCols = new ArrayList<Order>();\n    int numBuckets = -1;\n    String comment = null;\n    String location = null;\n    Map<String, String> tblProps = null;\n    boolean ifNotExists = false;\n    boolean isExt = false;\n    boolean isTemporary = false;\n    boolean isMaterialization = false;\n    ASTNode selectStmt = null;\n    final int CREATE_TABLE = 0; // regular CREATE TABLE\n    final int CTLT = 1; // CREATE TABLE LIKE ... (CTLT)\n    final int CTAS = 2; // CREATE TABLE AS SELECT ... (CTAS)\n    int command_type = CREATE_TABLE;\n    List<String> skewedColNames = new ArrayList<String>();\n    List<List<String>> skewedValues = new ArrayList<List<String>>();\n    Map<List<String>, String> listBucketColValuesMapping = new HashMap<List<String>, String>();\n    boolean storedAsDirs = false;\n    boolean isUserStorageFormat = false;\n\n    RowFormatParams rowFormatParams = new RowFormatParams();\n    StorageFormat storageFormat = new StorageFormat(conf);\n\n    LOG.info(\"Creating table \" + dbDotTab + \" position=\" + ast.getCharPositionInLine());\n    int numCh = ast.getChildCount();\n\n    /*\n     * Check the 1st-level children and do simple semantic checks: 1) CTLT and\n     * CTAS should not coexists. 2) CTLT or CTAS should not coexists with column\n     * list (target table schema). 3) CTAS does not support partitioning (for\n     * now).\n     */\n    for (int num = 1; num < numCh; num++) {\n      ASTNode child = (ASTNode) ast.getChild(num);\n      if (storageFormat.fillStorageFormat(child)) {\n        isUserStorageFormat = true;\n        continue;\n      }\n      switch (child.getToken().getType()) {\n      case HiveParser.TOK_IFNOTEXISTS:\n        ifNotExists = true;\n        break;\n      case HiveParser.KW_EXTERNAL:\n        isExt = true;\n        break;\n      case HiveParser.KW_TEMPORARY:\n        isTemporary = true;\n        isMaterialization = MATERIALIZATION_MARKER.equals(child.getText());\n        break;\n      case HiveParser.TOK_LIKETABLE:\n        if (child.getChildCount() > 0) {\n          likeTableName = getUnescapedName((ASTNode) child.getChild(0));\n          if (likeTableName != null) {\n            if (command_type == CTAS) {\n              throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE\n                  .getMsg());\n            }\n            if (cols.size() != 0) {\n              throw new SemanticException(ErrorMsg.CTLT_COLLST_COEXISTENCE\n                  .getMsg());\n            }\n          }\n          command_type = CTLT;\n        }\n        break;\n\n      case HiveParser.TOK_QUERY: // CTAS\n        if (command_type == CTLT) {\n          throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());\n        }\n        if (cols.size() != 0) {\n          throw new SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());\n        }\n        if (partCols.size() != 0 || bucketCols.size() != 0) {\n          boolean dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);\n          if (dynPart == false) {\n            throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n          } else {\n            // TODO: support dynamic partition for CTAS\n            throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n          }\n        }\n        if (isExt) {\n          throw new SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());\n        }\n        command_type = CTAS;\n        if (plannerCtx != null) {\n          plannerCtx.setCTASOrMVToken(child);\n        }\n        selectStmt = child;\n        break;\n      case HiveParser.TOK_TABCOLLIST:\n        cols = getColumns(child, true, primaryKeys, foreignKeys);\n        break;\n      case HiveParser.TOK_TABLECOMMENT:\n        comment = unescapeSQLString(child.getChild(0).getText());\n        break;\n      case HiveParser.TOK_TABLEPARTCOLS:\n        partCols = getColumns((ASTNode) child.getChild(0), false);\n        break;\n      case HiveParser.TOK_ALTERTABLE_BUCKETS:\n        bucketCols = getColumnNames((ASTNode) child.getChild(0));\n        if (child.getChildCount() == 2) {\n          numBuckets = Integer.parseInt(child.getChild(1).getText());\n        } else {\n          sortCols = getColumnNamesOrder((ASTNode) child.getChild(1));\n          numBuckets = Integer.parseInt(child.getChild(2).getText());\n        }\n        break;\n      case HiveParser.TOK_TABLEROWFORMAT:\n        rowFormatParams.analyzeRowFormat(child);\n        break;\n      case HiveParser.TOK_TABLELOCATION:\n        location = unescapeSQLString(child.getChild(0).getText());\n        location = EximUtil.relativeToAbsolutePath(conf, location);\n        inputs.add(toReadEntity(location));\n        break;\n      case HiveParser.TOK_TABLEPROPERTIES:\n        tblProps = DDLSemanticAnalyzer.getProps((ASTNode) child.getChild(0));\n        break;\n      case HiveParser.TOK_TABLESERIALIZER:\n        child = (ASTNode) child.getChild(0);\n        storageFormat.setSerde(unescapeSQLString(child.getChild(0).getText()));\n        if (child.getChildCount() == 2) {\n          readProps((ASTNode) (child.getChild(1).getChild(0)),\n              storageFormat.getSerdeProps());\n        }\n        break;\n      case HiveParser.TOK_TABLESKEWED:\n        /**\n         * Throw an error if the user tries to use the DDL with\n         * hive.internal.ddl.list.bucketing.enable set to false.\n         */\n        HiveConf hiveConf = SessionState.get().getConf();\n\n        // skewed column names\n        skewedColNames = analyzeSkewedTablDDLColNames(skewedColNames, child);\n        // skewed value\n        analyzeDDLSkewedValues(skewedValues, child);\n        // stored as directories\n        storedAsDirs = analyzeStoredAdDirs(child);\n\n        break;\n      default:\n        throw new AssertionError(\"Unknown token: \" + child.getToken());\n      }\n    }\n\n    if (command_type == CREATE_TABLE || command_type == CTLT) {\n        queryState.setCommandType(HiveOperation.CREATETABLE);\n    } else if (command_type == CTAS) {\n        queryState.setCommandType(HiveOperation.CREATETABLE_AS_SELECT);\n    } else {\n        throw new SemanticException(\"Unrecognized command.\");\n    }\n\n    storageFormat.fillDefaultStorageFormat(isExt, false);\n\n    // check for existence of table\n    if (ifNotExists) {\n      try {\n        Table table = getTable(qualifiedTabName, false);\n        if (table != null) { // table exists\n          return null;\n        }\n      } catch (HiveException e) {\n        // should not occur since second parameter to getTableWithQN is false\n        throw new IllegalStateException(\"Unxpected Exception thrown: \" + e.getMessage(), e);\n      }\n    }\n\n    addDbAndTabToOutputs(qualifiedTabName, TableType.MANAGED_TABLE);\n\n    if (isTemporary) {\n      if (partCols.size() > 0) {\n        throw new SemanticException(\"Partition columns are not supported on temporary tables\");\n      }\n\n      if (location == null) {\n        // for temporary tables we set the location to something in the session's scratch dir\n        // it has the same life cycle as the tmp table\n        try {\n          // Generate a unique ID for temp table path.\n          // This path will be fixed for the life of the temp table.\n          Path path = new Path(SessionState.getTempTableSpace(conf), UUID.randomUUID().toString());\n          path = Warehouse.getDnsPath(path, conf);\n          location = path.toString();\n        } catch (MetaException err) {\n          throw new SemanticException(\"Error while generating temp table path:\", err);\n        }\n      }\n    }\n\n    // Handle different types of CREATE TABLE command\n    switch (command_type) {\n\n    case CREATE_TABLE: // REGULAR CREATE TABLE DDL\n      tblProps = addDefaultProperties(tblProps);\n\n      CreateTableDesc crtTblDesc = new CreateTableDesc(dbDotTab, isExt, isTemporary, cols, partCols,\n          bucketCols, sortCols, numBuckets, rowFormatParams.fieldDelim,\n          rowFormatParams.fieldEscape,\n          rowFormatParams.collItemDelim, rowFormatParams.mapKeyDelim, rowFormatParams.lineDelim,\n          comment,\n          storageFormat.getInputFormat(), storageFormat.getOutputFormat(), location, storageFormat.getSerde(),\n          storageFormat.getStorageHandler(), storageFormat.getSerdeProps(), tblProps, ifNotExists, skewedColNames,\n          skewedValues, primaryKeys, foreignKeys);\n      crtTblDesc.setStoredAsSubDirectories(storedAsDirs);\n      crtTblDesc.setNullFormat(rowFormatParams.nullFormat);\n\n      crtTblDesc.validate(conf);\n      // outputs is empty, which means this create table happens in the current\n      // database.\n      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n          crtTblDesc), conf));\n      break;\n\n    case CTLT: // create table like <tbl_name>\n      tblProps = addDefaultProperties(tblProps);\n\n      if (isTemporary) {\n        Table likeTable = getTable(likeTableName, false);\n        if (likeTable != null && likeTable.getPartCols().size() > 0) {\n          throw new SemanticException(\"Partition columns are not supported on temporary tables \"\n              + \"and source table in CREATE TABLE LIKE is partitioned.\");\n        }\n      }\n      CreateTableLikeDesc crtTblLikeDesc = new CreateTableLikeDesc(dbDotTab, isExt, isTemporary,\n          storageFormat.getInputFormat(), storageFormat.getOutputFormat(), location,\n          storageFormat.getSerde(), storageFormat.getSerdeProps(), tblProps, ifNotExists,\n          likeTableName, isUserStorageFormat);\n      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n          crtTblLikeDesc), conf));\n      break;\n\n    case CTAS: // create table as select\n\n      if (isTemporary) {\n        if (!ctx.isExplainSkipExecution() && !isMaterialization) {\n          String dbName = qualifiedTabName[0];\n          String tblName = qualifiedTabName[1];\n          SessionState ss = SessionState.get();\n          if (ss == null) {\n            throw new SemanticException(\"No current SessionState, cannot create temporary table \"\n                + dbName + \".\" + tblName);\n          }\n          Map<String, Table> tables = SessionHiveMetaStoreClient.getTempTablesForDatabase(dbName);\n          if (tables != null && tables.containsKey(tblName)) {\n            throw new SemanticException(\"Temporary table \" + dbName + \".\" + tblName\n                + \" already exists\");\n          }\n        }\n      } else {\n        // Verify that the table does not already exist\n        // dumpTable is only used to check the conflict for non-temporary tables\n        try {\n          Table dumpTable = db.newTable(dbDotTab);\n          if (null != db.getTable(dumpTable.getDbName(), dumpTable.getTableName(), false) && !ctx.isExplainSkipExecution()) {\n            throw new SemanticException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(dbDotTab));\n          }\n        } catch (HiveException e) {\n          throw new SemanticException(e);\n        }\n      }\n\n      if(location != null && location.length() != 0) {\n        Path locPath = new Path(location);\n        FileSystem curFs = null;\n        FileStatus locStats = null;\n        try {\n          curFs = locPath.getFileSystem(conf);\n          if(curFs != null) {\n            locStats = curFs.getFileStatus(locPath);\n          }\n          if(locStats != null && locStats.isDir()) {\n            FileStatus[] lStats = curFs.listStatus(locPath);\n            if(lStats != null && lStats.length != 0) {\n              // Don't throw an exception if the target location only contains the staging-dirs\n              for (FileStatus lStat : lStats) {\n                if (!lStat.getPath().getName().startsWith(HiveConf.getVar(conf, HiveConf.ConfVars.STAGINGDIR))) {\n                  throw new SemanticException(ErrorMsg.CTAS_LOCATION_NONEMPTY.getMsg(location));\n                }\n              }\n            }\n          }\n        } catch (FileNotFoundException nfe) {\n          //we will create the folder if it does not exist.\n        } catch (IOException ioE) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Exception when validate folder \",ioE);\n          }\n\n        }\n      }\n\n      tblProps = addDefaultProperties(tblProps);\n\n      tableDesc = new CreateTableDesc(qualifiedTabName[0], dbDotTab, isExt, isTemporary, cols,\n          partCols, bucketCols, sortCols, numBuckets, rowFormatParams.fieldDelim,\n          rowFormatParams.fieldEscape, rowFormatParams.collItemDelim, rowFormatParams.mapKeyDelim,\n          rowFormatParams.lineDelim, comment, storageFormat.getInputFormat(),\n          storageFormat.getOutputFormat(), location, storageFormat.getSerde(),\n          storageFormat.getStorageHandler(), storageFormat.getSerdeProps(), tblProps, ifNotExists,\n\t  skewedColNames, skewedValues, true, primaryKeys, foreignKeys);\n      tableDesc.setMaterialization(isMaterialization);\n      tableDesc.setStoredAsSubDirectories(storedAsDirs);\n      tableDesc.setNullFormat(rowFormatParams.nullFormat);\n      qb.setTableDesc(tableDesc);\n\n      return selectStmt;\n\n    default:\n      throw new SemanticException(\"Unrecognized command.\");\n    }\n    return null;\n  }",
            "11641  \n11642  \n11643  \n11644  \n11645  \n11646  \n11647  \n11648  \n11649  \n11650  \n11651  \n11652  \n11653  \n11654  \n11655  \n11656  \n11657  \n11658  \n11659  \n11660  \n11661  \n11662  \n11663  \n11664  \n11665  \n11666  \n11667  \n11668  \n11669  \n11670  \n11671  \n11672  \n11673  \n11674  \n11675  \n11676  \n11677  \n11678  \n11679  \n11680  \n11681  \n11682  \n11683  \n11684  \n11685  \n11686  \n11687  \n11688  \n11689  \n11690  \n11691  \n11692  \n11693  \n11694  \n11695  \n11696  \n11697  \n11698  \n11699  \n11700  \n11701  \n11702  \n11703  \n11704  \n11705  \n11706  \n11707  \n11708  \n11709  \n11710  \n11711  \n11712  \n11713  \n11714  \n11715  \n11716  \n11717  \n11718  \n11719  \n11720  \n11721  \n11722  \n11723  \n11724  \n11725  \n11726  \n11727  \n11728  \n11729  \n11730  \n11731  \n11732  \n11733  \n11734  \n11735  \n11736  \n11737  \n11738  \n11739  \n11740  \n11741  \n11742  \n11743  \n11744  \n11745  \n11746  \n11747  \n11748  \n11749  \n11750  \n11751  \n11752  \n11753  \n11754  \n11755  \n11756  \n11757  \n11758  \n11759  \n11760  \n11761  \n11762  \n11763  \n11764  \n11765  \n11766  \n11767  \n11768  \n11769  \n11770  \n11771  \n11772  \n11773  \n11774  \n11775  \n11776  \n11777  \n11778  \n11779  \n11780  \n11781  \n11782  \n11783  \n11784  \n11785  \n11786  \n11787  \n11788  \n11789  \n11790  \n11791  \n11792  \n11793  \n11794  \n11795  \n11796  \n11797  \n11798  \n11799  \n11800  \n11801  \n11802  \n11803  \n11804  \n11805  \n11806  \n11807  \n11808  \n11809  \n11810  \n11811  \n11812  \n11813  \n11814  \n11815  \n11816  \n11817  \n11818  \n11819  \n11820  \n11821  \n11822  \n11823  \n11824  \n11825  \n11826  \n11827 +\n11828  \n11829  \n11830  \n11831  \n11832  \n11833  \n11834  \n11835  \n11836  \n11837  \n11838  \n11839  \n11840  \n11841  \n11842  \n11843  \n11844  \n11845  \n11846  \n11847  \n11848  \n11849  \n11850  \n11851  \n11852  \n11853  \n11854  \n11855  \n11856  \n11857  \n11858  \n11859  \n11860  \n11861  \n11862  \n11863  \n11864  \n11865  \n11866  \n11867  \n11868  \n11869  \n11870  \n11871  \n11872  \n11873  \n11874  \n11875  \n11876  \n11877  \n11878  \n11879  \n11880  \n11881  \n11882  \n11883  \n11884  \n11885  \n11886  \n11887  \n11888  \n11889  \n11890  \n11891  \n11892  \n11893  \n11894  \n11895  \n11896  \n11897  \n11898  \n11899  \n11900  \n11901  \n11902  \n11903  \n11904  \n11905  \n11906  \n11907  \n11908  \n11909  \n11910  \n11911  \n11912  \n11913  \n11914  \n11915  \n11916  \n11917  \n11918  \n11919  \n11920  \n11921  \n11922  \n11923  \n11924  \n11925  \n11926  \n11927  \n11928  \n11929  \n11930  \n11931  \n11932  \n11933  \n11934  \n11935  \n11936  \n11937  \n11938  \n11939  \n11940  \n11941  \n11942  \n11943  \n11944  \n11945  \n11946  \n11947  \n11948  \n11949  \n11950  \n11951  \n11952  \n11953  \n11954  \n11955  \n11956  \n11957  \n11958  \n11959  \n11960  \n11961  \n11962  \n11963  \n11964  \n11965  \n11966  \n11967  \n11968  \n11969  \n11970  \n11971  \n11972  \n11973  \n11974  \n11975  ",
            "  /**\n   * Analyze the create table command. If it is a regular create-table or\n   * create-table-like statements, we create a DDLWork and return true. If it is\n   * a create-table-as-select, we get the necessary info such as the SerDe and\n   * Storage Format and put it in QB, and return false, indicating the rest of\n   * the semantic analyzer need to deal with the select statement with respect\n   * to the SerDe and Storage Format.\n   */\n  ASTNode analyzeCreateTable(\n      ASTNode ast, QB qb, PlannerContext plannerCtx) throws SemanticException {\n    String[] qualifiedTabName = getQualifiedTableName((ASTNode) ast.getChild(0));\n    String dbDotTab = getDotName(qualifiedTabName);\n\n    String likeTableName = null;\n    List<FieldSchema> cols = new ArrayList<FieldSchema>();\n    List<FieldSchema> partCols = new ArrayList<FieldSchema>();\n    List<String> bucketCols = new ArrayList<String>();\n    List<SQLPrimaryKey> primaryKeys = new ArrayList<SQLPrimaryKey>();\n    List<SQLForeignKey> foreignKeys = new ArrayList<SQLForeignKey>();\n    List<Order> sortCols = new ArrayList<Order>();\n    int numBuckets = -1;\n    String comment = null;\n    String location = null;\n    Map<String, String> tblProps = null;\n    boolean ifNotExists = false;\n    boolean isExt = false;\n    boolean isTemporary = false;\n    boolean isMaterialization = false;\n    ASTNode selectStmt = null;\n    final int CREATE_TABLE = 0; // regular CREATE TABLE\n    final int CTLT = 1; // CREATE TABLE LIKE ... (CTLT)\n    final int CTAS = 2; // CREATE TABLE AS SELECT ... (CTAS)\n    int command_type = CREATE_TABLE;\n    List<String> skewedColNames = new ArrayList<String>();\n    List<List<String>> skewedValues = new ArrayList<List<String>>();\n    Map<List<String>, String> listBucketColValuesMapping = new HashMap<List<String>, String>();\n    boolean storedAsDirs = false;\n    boolean isUserStorageFormat = false;\n\n    RowFormatParams rowFormatParams = new RowFormatParams();\n    StorageFormat storageFormat = new StorageFormat(conf);\n\n    LOG.info(\"Creating table \" + dbDotTab + \" position=\" + ast.getCharPositionInLine());\n    int numCh = ast.getChildCount();\n\n    /*\n     * Check the 1st-level children and do simple semantic checks: 1) CTLT and\n     * CTAS should not coexists. 2) CTLT or CTAS should not coexists with column\n     * list (target table schema). 3) CTAS does not support partitioning (for\n     * now).\n     */\n    for (int num = 1; num < numCh; num++) {\n      ASTNode child = (ASTNode) ast.getChild(num);\n      if (storageFormat.fillStorageFormat(child)) {\n        isUserStorageFormat = true;\n        continue;\n      }\n      switch (child.getToken().getType()) {\n      case HiveParser.TOK_IFNOTEXISTS:\n        ifNotExists = true;\n        break;\n      case HiveParser.KW_EXTERNAL:\n        isExt = true;\n        break;\n      case HiveParser.KW_TEMPORARY:\n        isTemporary = true;\n        isMaterialization = MATERIALIZATION_MARKER.equals(child.getText());\n        break;\n      case HiveParser.TOK_LIKETABLE:\n        if (child.getChildCount() > 0) {\n          likeTableName = getUnescapedName((ASTNode) child.getChild(0));\n          if (likeTableName != null) {\n            if (command_type == CTAS) {\n              throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE\n                  .getMsg());\n            }\n            if (cols.size() != 0) {\n              throw new SemanticException(ErrorMsg.CTLT_COLLST_COEXISTENCE\n                  .getMsg());\n            }\n          }\n          command_type = CTLT;\n        }\n        break;\n\n      case HiveParser.TOK_QUERY: // CTAS\n        if (command_type == CTLT) {\n          throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());\n        }\n        if (cols.size() != 0) {\n          throw new SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());\n        }\n        if (partCols.size() != 0 || bucketCols.size() != 0) {\n          boolean dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);\n          if (dynPart == false) {\n            throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n          } else {\n            // TODO: support dynamic partition for CTAS\n            throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n          }\n        }\n        if (isExt) {\n          throw new SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());\n        }\n        command_type = CTAS;\n        if (plannerCtx != null) {\n          plannerCtx.setCTASOrMVToken(child);\n        }\n        selectStmt = child;\n        break;\n      case HiveParser.TOK_TABCOLLIST:\n        cols = getColumns(child, true, primaryKeys, foreignKeys);\n        break;\n      case HiveParser.TOK_TABLECOMMENT:\n        comment = unescapeSQLString(child.getChild(0).getText());\n        break;\n      case HiveParser.TOK_TABLEPARTCOLS:\n        partCols = getColumns((ASTNode) child.getChild(0), false);\n        break;\n      case HiveParser.TOK_ALTERTABLE_BUCKETS:\n        bucketCols = getColumnNames((ASTNode) child.getChild(0));\n        if (child.getChildCount() == 2) {\n          numBuckets = Integer.parseInt(child.getChild(1).getText());\n        } else {\n          sortCols = getColumnNamesOrder((ASTNode) child.getChild(1));\n          numBuckets = Integer.parseInt(child.getChild(2).getText());\n        }\n        break;\n      case HiveParser.TOK_TABLEROWFORMAT:\n        rowFormatParams.analyzeRowFormat(child);\n        break;\n      case HiveParser.TOK_TABLELOCATION:\n        location = unescapeSQLString(child.getChild(0).getText());\n        location = EximUtil.relativeToAbsolutePath(conf, location);\n        inputs.add(toReadEntity(location));\n        break;\n      case HiveParser.TOK_TABLEPROPERTIES:\n        tblProps = DDLSemanticAnalyzer.getProps((ASTNode) child.getChild(0));\n        break;\n      case HiveParser.TOK_TABLESERIALIZER:\n        child = (ASTNode) child.getChild(0);\n        storageFormat.setSerde(unescapeSQLString(child.getChild(0).getText()));\n        if (child.getChildCount() == 2) {\n          readProps((ASTNode) (child.getChild(1).getChild(0)),\n              storageFormat.getSerdeProps());\n        }\n        break;\n      case HiveParser.TOK_TABLESKEWED:\n        /**\n         * Throw an error if the user tries to use the DDL with\n         * hive.internal.ddl.list.bucketing.enable set to false.\n         */\n        HiveConf hiveConf = SessionState.get().getConf();\n\n        // skewed column names\n        skewedColNames = analyzeSkewedTablDDLColNames(skewedColNames, child);\n        // skewed value\n        analyzeDDLSkewedValues(skewedValues, child);\n        // stored as directories\n        storedAsDirs = analyzeStoredAdDirs(child);\n\n        break;\n      default:\n        throw new AssertionError(\"Unknown token: \" + child.getToken());\n      }\n    }\n\n    if (command_type == CREATE_TABLE || command_type == CTLT) {\n        queryState.setCommandType(HiveOperation.CREATETABLE);\n    } else if (command_type == CTAS) {\n        queryState.setCommandType(HiveOperation.CREATETABLE_AS_SELECT);\n    } else {\n        throw new SemanticException(\"Unrecognized command.\");\n    }\n\n    storageFormat.fillDefaultStorageFormat(isExt, false);\n\n    // check for existence of table\n    if (ifNotExists) {\n      try {\n        Table table = getTable(qualifiedTabName, false);\n        if (table != null) { // table exists\n          return null;\n        }\n      } catch (HiveException e) {\n        // should not occur since second parameter to getTableWithQN is false\n        throw new IllegalStateException(\"Unexpected Exception thrown: \" + e.getMessage(), e);\n      }\n    }\n\n    addDbAndTabToOutputs(qualifiedTabName, TableType.MANAGED_TABLE);\n\n    if (isTemporary) {\n      if (partCols.size() > 0) {\n        throw new SemanticException(\"Partition columns are not supported on temporary tables\");\n      }\n\n      if (location == null) {\n        // for temporary tables we set the location to something in the session's scratch dir\n        // it has the same life cycle as the tmp table\n        try {\n          // Generate a unique ID for temp table path.\n          // This path will be fixed for the life of the temp table.\n          Path path = new Path(SessionState.getTempTableSpace(conf), UUID.randomUUID().toString());\n          path = Warehouse.getDnsPath(path, conf);\n          location = path.toString();\n        } catch (MetaException err) {\n          throw new SemanticException(\"Error while generating temp table path:\", err);\n        }\n      }\n    }\n\n    // Handle different types of CREATE TABLE command\n    switch (command_type) {\n\n    case CREATE_TABLE: // REGULAR CREATE TABLE DDL\n      tblProps = addDefaultProperties(tblProps);\n\n      CreateTableDesc crtTblDesc = new CreateTableDesc(dbDotTab, isExt, isTemporary, cols, partCols,\n          bucketCols, sortCols, numBuckets, rowFormatParams.fieldDelim,\n          rowFormatParams.fieldEscape,\n          rowFormatParams.collItemDelim, rowFormatParams.mapKeyDelim, rowFormatParams.lineDelim,\n          comment,\n          storageFormat.getInputFormat(), storageFormat.getOutputFormat(), location, storageFormat.getSerde(),\n          storageFormat.getStorageHandler(), storageFormat.getSerdeProps(), tblProps, ifNotExists, skewedColNames,\n          skewedValues, primaryKeys, foreignKeys);\n      crtTblDesc.setStoredAsSubDirectories(storedAsDirs);\n      crtTblDesc.setNullFormat(rowFormatParams.nullFormat);\n\n      crtTblDesc.validate(conf);\n      // outputs is empty, which means this create table happens in the current\n      // database.\n      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n          crtTblDesc), conf));\n      break;\n\n    case CTLT: // create table like <tbl_name>\n      tblProps = addDefaultProperties(tblProps);\n\n      if (isTemporary) {\n        Table likeTable = getTable(likeTableName, false);\n        if (likeTable != null && likeTable.getPartCols().size() > 0) {\n          throw new SemanticException(\"Partition columns are not supported on temporary tables \"\n              + \"and source table in CREATE TABLE LIKE is partitioned.\");\n        }\n      }\n      CreateTableLikeDesc crtTblLikeDesc = new CreateTableLikeDesc(dbDotTab, isExt, isTemporary,\n          storageFormat.getInputFormat(), storageFormat.getOutputFormat(), location,\n          storageFormat.getSerde(), storageFormat.getSerdeProps(), tblProps, ifNotExists,\n          likeTableName, isUserStorageFormat);\n      rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n          crtTblLikeDesc), conf));\n      break;\n\n    case CTAS: // create table as select\n\n      if (isTemporary) {\n        if (!ctx.isExplainSkipExecution() && !isMaterialization) {\n          String dbName = qualifiedTabName[0];\n          String tblName = qualifiedTabName[1];\n          SessionState ss = SessionState.get();\n          if (ss == null) {\n            throw new SemanticException(\"No current SessionState, cannot create temporary table \"\n                + dbName + \".\" + tblName);\n          }\n          Map<String, Table> tables = SessionHiveMetaStoreClient.getTempTablesForDatabase(dbName);\n          if (tables != null && tables.containsKey(tblName)) {\n            throw new SemanticException(\"Temporary table \" + dbName + \".\" + tblName\n                + \" already exists\");\n          }\n        }\n      } else {\n        // Verify that the table does not already exist\n        // dumpTable is only used to check the conflict for non-temporary tables\n        try {\n          Table dumpTable = db.newTable(dbDotTab);\n          if (null != db.getTable(dumpTable.getDbName(), dumpTable.getTableName(), false) && !ctx.isExplainSkipExecution()) {\n            throw new SemanticException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(dbDotTab));\n          }\n        } catch (HiveException e) {\n          throw new SemanticException(e);\n        }\n      }\n\n      if(location != null && location.length() != 0) {\n        Path locPath = new Path(location);\n        FileSystem curFs = null;\n        FileStatus locStats = null;\n        try {\n          curFs = locPath.getFileSystem(conf);\n          if(curFs != null) {\n            locStats = curFs.getFileStatus(locPath);\n          }\n          if(locStats != null && locStats.isDir()) {\n            FileStatus[] lStats = curFs.listStatus(locPath);\n            if(lStats != null && lStats.length != 0) {\n              // Don't throw an exception if the target location only contains the staging-dirs\n              for (FileStatus lStat : lStats) {\n                if (!lStat.getPath().getName().startsWith(HiveConf.getVar(conf, HiveConf.ConfVars.STAGINGDIR))) {\n                  throw new SemanticException(ErrorMsg.CTAS_LOCATION_NONEMPTY.getMsg(location));\n                }\n              }\n            }\n          }\n        } catch (FileNotFoundException nfe) {\n          //we will create the folder if it does not exist.\n        } catch (IOException ioE) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Exception when validate folder \",ioE);\n          }\n\n        }\n      }\n\n      tblProps = addDefaultProperties(tblProps);\n\n      tableDesc = new CreateTableDesc(qualifiedTabName[0], dbDotTab, isExt, isTemporary, cols,\n          partCols, bucketCols, sortCols, numBuckets, rowFormatParams.fieldDelim,\n          rowFormatParams.fieldEscape, rowFormatParams.collItemDelim, rowFormatParams.mapKeyDelim,\n          rowFormatParams.lineDelim, comment, storageFormat.getInputFormat(),\n          storageFormat.getOutputFormat(), location, storageFormat.getSerde(),\n          storageFormat.getStorageHandler(), storageFormat.getSerdeProps(), tblProps, ifNotExists,\n\t  skewedColNames, skewedValues, true, primaryKeys, foreignKeys);\n      tableDesc.setMaterialization(isMaterialization);\n      tableDesc.setStoredAsSubDirectories(storedAsDirs);\n      tableDesc.setNullFormat(rowFormatParams.nullFormat);\n      qb.setTableDesc(tableDesc);\n\n      return selectStmt;\n\n    default:\n      throw new SemanticException(\"Unrecognized command.\");\n    }\n    return null;\n  }"
        ],
        [
            "MapJoinEagerRowContainer::write(MapJoinObjectSerDeContext,ObjectOutputStream)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @Override\n  public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)\n  throws IOException, SerDeException {\n    AbstractSerDe serde = context.getSerDe();\n    ObjectInspector valueObjectInspector = context.getStandardOI();\n    long numRows = rowCount();\n    long numRowsWritten = 0L;\n    out.writeLong(numRows);\n    for (List<Object> row = first(); row != null; row = next()) {\n      serde.serialize(row.toArray(), valueObjectInspector).write(out);\n      ++numRowsWritten;      \n    }\n    if(numRows != rowCount()) {\n      throw new ConcurrentModificationException(\"Values was modifified while persisting\");\n    }\n    if(numRowsWritten != numRows) {\n      throw new IllegalStateException(\"Expected to write \" + numRows + \" but wrote \" + numRowsWritten);\n    }\n  }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @Override\n  public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)\n  throws IOException, SerDeException {\n    AbstractSerDe serde = context.getSerDe();\n    ObjectInspector valueObjectInspector = context.getStandardOI();\n    long numRows = rowCount();\n    long numRowsWritten = 0L;\n    out.writeLong(numRows);\n    for (List<Object> row = first(); row != null; row = next()) {\n      serde.serialize(row.toArray(), valueObjectInspector).write(out);\n      ++numRowsWritten;      \n    }\n    if(numRows != rowCount()) {\n      throw new ConcurrentModificationException(\"Values was modified while persisting\");\n    }\n    if(numRowsWritten != numRows) {\n      throw new IllegalStateException(\"Expected to write \" + numRows + \" but wrote \" + numRowsWritten);\n    }\n  }"
        ],
        [
            "FetchWork::getPartDescOrderedByPartDir()",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 -\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  ",
            "  /**\n   * Get Partition descriptors in sorted (ascending) order of partition directory\n   *\n   * @return the partDesc array list\n   */\n  @Explain(displayName = \"Partition Description\", explainLevels = { Level.EXTENDED })\n  public ArrayList<PartitionDesc> getPartDescOrderedByPartDir() {\n    ArrayList<PartitionDesc> partDescOrdered = partDesc;\n\n    if (partDir != null && partDir.size() > 1) {\n      if (partDesc == null || partDir.size() != partDesc.size()) {\n        throw new RuntimeException(\n            \"Partiton Directory list size doesn't match Partition Descriptor list size\");\n      }\n\n      // Construct a sorted Map of Partition Dir - Partition Descriptor; ordering is based on\n      // patition dir (map key)\n      // Assumption: there is a 1-1 mapping between partition dir and partition descriptor lists\n      TreeMap<Path, PartitionDesc> partDirToPartSpecMap = new TreeMap<Path, PartitionDesc>();\n      for (int i = 0; i < partDir.size(); i++) {\n        partDirToPartSpecMap.put(partDir.get(i), partDesc.get(i));\n      }\n\n      // Extract partition desc from sorted map (ascending order of part dir)\n      partDescOrdered = new ArrayList<PartitionDesc>(partDirToPartSpecMap.values());\n    }\n\n    return partDescOrdered;\n  }",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  ",
            "  /**\n   * Get Partition descriptors in sorted (ascending) order of partition directory\n   *\n   * @return the partDesc array list\n   */\n  @Explain(displayName = \"Partition Description\", explainLevels = { Level.EXTENDED })\n  public ArrayList<PartitionDesc> getPartDescOrderedByPartDir() {\n    ArrayList<PartitionDesc> partDescOrdered = partDesc;\n\n    if (partDir != null && partDir.size() > 1) {\n      if (partDesc == null || partDir.size() != partDesc.size()) {\n        throw new RuntimeException(\n            \"Partition Directory list size doesn't match Partition Descriptor list size\");\n      }\n\n      // Construct a sorted Map of Partition Dir - Partition Descriptor; ordering is based on\n      // patition dir (map key)\n      // Assumption: there is a 1-1 mapping between partition dir and partition descriptor lists\n      TreeMap<Path, PartitionDesc> partDirToPartSpecMap = new TreeMap<Path, PartitionDesc>();\n      for (int i = 0; i < partDir.size(); i++) {\n        partDirToPartSpecMap.put(partDir.get(i), partDesc.get(i));\n      }\n\n      // Extract partition desc from sorted map (ascending order of part dir)\n      partDescOrdered = new ArrayList<PartitionDesc>(partDirToPartSpecMap.values());\n    }\n\n    return partDescOrdered;\n  }"
        ],
        [
            "GenericUDFTrunc::initialize(ObjectInspector)",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length == 2) {\n      inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n      inputType2 = ((PrimitiveObjectInspector) arguments[1]).getPrimitiveCategory();\n      if ((PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.DATE_GROUP\n          || PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.STRING_GROUP)\n          && PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType2) == PrimitiveGrouping.STRING_GROUP) {\n        dateTypeArg = true;\n        return initializeDate(arguments);\n      } else if (PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.NUMERIC_GROUP\n          && PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType2) == PrimitiveGrouping.NUMERIC_GROUP) {\n        dateTypeArg = false;\n        return initializeNumber(arguments);\n      }\n      throw new UDFArgumentException(\"Got wrong argument types : first argument type : \"\n          + arguments[0].getTypeName() + \", second argument type : \" + arguments[1].getTypeName());\n    } else if (arguments.length == 1) {\n      inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n      if (PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.NUMERIC_GROUP) {\n        dateTypeArg = false;\n        return initializeNumber(arguments);\n      } else {\n        throw new UDFArgumentException(\n            \"Only primitive type arguments are accepted, when arguments lenght is one, got \"\n                + arguments[1].getTypeName());\n      }\n    }\n    throw new UDFArgumentException(\"TRUNC requires one or two argument, got \" + arguments.length);\n  }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length == 2) {\n      inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n      inputType2 = ((PrimitiveObjectInspector) arguments[1]).getPrimitiveCategory();\n      if ((PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.DATE_GROUP\n          || PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.STRING_GROUP)\n          && PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType2) == PrimitiveGrouping.STRING_GROUP) {\n        dateTypeArg = true;\n        return initializeDate(arguments);\n      } else if (PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.NUMERIC_GROUP\n          && PrimitiveObjectInspectorUtils\n              .getPrimitiveGrouping(inputType2) == PrimitiveGrouping.NUMERIC_GROUP) {\n        dateTypeArg = false;\n        return initializeNumber(arguments);\n      }\n      throw new UDFArgumentException(\"Got wrong argument types : first argument type : \"\n          + arguments[0].getTypeName() + \", second argument type : \" + arguments[1].getTypeName());\n    } else if (arguments.length == 1) {\n      inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n      if (PrimitiveObjectInspectorUtils\n          .getPrimitiveGrouping(inputType1) == PrimitiveGrouping.NUMERIC_GROUP) {\n        dateTypeArg = false;\n        return initializeNumber(arguments);\n      } else {\n        throw new UDFArgumentException(\n            \"Only primitive type arguments are accepted, when arguments length is one, got \"\n                + arguments[1].getTypeName());\n      }\n    }\n    throw new UDFArgumentException(\"TRUNC requires one or two argument, got \" + arguments.length);\n  }"
        ],
        [
            "TaskCompiler::compile(ParseContext,List,HashSet,HashSet)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 -\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  ",
            "  @SuppressWarnings({\"nls\", \"unchecked\"})\n  public void compile(final ParseContext pCtx, final List<Task<? extends Serializable>> rootTasks,\n      final HashSet<ReadEntity> inputs, final HashSet<WriteEntity> outputs) throws SemanticException {\n\n    Context ctx = pCtx.getContext();\n    GlobalLimitCtx globalLimitCtx = pCtx.getGlobalLimitCtx();\n    List<Task<MoveWork>> mvTask = new ArrayList<Task<MoveWork>>();\n\n    List<LoadTableDesc> loadTableWork = pCtx.getLoadTableWork();\n    List<LoadFileDesc> loadFileWork = pCtx.getLoadFileWork();\n\n    boolean isCStats = pCtx.getQueryProperties().isAnalyzeRewrite();\n    int outerQueryLimit = pCtx.getQueryProperties().getOuterQueryLimit();\n\n    if (pCtx.getFetchTask() != null) {\n      if (pCtx.getFetchTask().getTblDesc() == null) {\n        return;\n      }\n      pCtx.getFetchTask().getWork().setHiveServerQuery(SessionState.get().isHiveServerQuery());\n      TableDesc resultTab = pCtx.getFetchTask().getTblDesc();\n      // If the serializer is ThriftJDBCBinarySerDe, then it requires that NoOpFetchFormatter be used. But when it isn't,\n      // then either the ThriftFormatter or the DefaultFetchFormatter should be used.\n      if (!resultTab.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName())) {\n        if (SessionState.get().isHiveServerQuery()) {\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName());\n        } else {\n          String formatterName = conf.get(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER);\n          if (formatterName == null || formatterName.isEmpty()) {\n            conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName());\n          }\n        }\n      }\n\n      return;\n    }\n\n    optimizeOperatorPlan(pCtx, inputs, outputs);\n\n    /*\n     * In case of a select, use a fetch task instead of a move task.\n     * If the select is from analyze table column rewrite, don't create a fetch task. Instead create\n     * a column stats task later.\n     */\n    if (pCtx.getQueryProperties().isQuery() && !isCStats) {\n      if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1)) {\n        throw new SemanticException(ErrorMsg.INVALID_LOAD_TABLE_FILE_WORK.getMsg());\n      }\n\n      LoadFileDesc loadFileDesc = loadFileWork.get(0);\n\n      String cols = loadFileDesc.getColumns();\n      String colTypes = loadFileDesc.getColumnTypes();\n\n      String resFileFormat;\n      TableDesc resultTab = pCtx.getFetchTableDesc();\n      if (resultTab == null) {\n        resFileFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT);\n        if (SessionState.get().getIsUsingThriftJDBCBinarySerDe()\n            && (resFileFormat.equalsIgnoreCase(\"SequenceFile\"))) {\n          resultTab =\n              PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, resFileFormat,\n                  ThriftJDBCBinarySerDe.class);\n          // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n          // read formatted thrift objects from the output SequenceFile written by Tasks.\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n        } else {\n          resultTab =\n              PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, resFileFormat,\n                  LazySimpleSerDe.class);\n        }\n      } else {\n        if (resultTab.getProperties().getProperty(serdeConstants.SERIALIZATION_LIB)\n            .equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName())) {\n          // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n          // read formatted thrift objects from the output SequenceFile written by Tasks.\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n        }\n      }\n\n      FetchWork fetch = new FetchWork(loadFileDesc.getSourcePath(), resultTab, outerQueryLimit);\n      boolean isHiveServerQuery = SessionState.get().isHiveServerQuery();\n      fetch.setHiveServerQuery(isHiveServerQuery);\n      fetch.setSource(pCtx.getFetchSource());\n      fetch.setSink(pCtx.getFetchSink());\n      if (isHiveServerQuery &&\n        null != resultTab &&\n        resultTab.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) &&\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {\n          fetch.setIsUsingThriftJDBCBinarySerDe(true);\n      } else {\n          fetch.setIsUsingThriftJDBCBinarySerDe(false);\n      }\n\n      pCtx.setFetchTask((FetchTask) TaskFactory.get(fetch, conf));\n\n      // For the FetchTask, the limit optimization requires we fetch all the rows\n      // in memory and count how many rows we get. It's not practical if the\n      // limit factor is too big\n      int fetchLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVELIMITOPTMAXFETCH);\n      if (globalLimitCtx.isEnable() && globalLimitCtx.getGlobalLimit() > fetchLimit) {\n        LOG.info(\"For FetchTask, LIMIT \" + globalLimitCtx.getGlobalLimit() + \" > \" + fetchLimit\n            + \". Doesn't qualify limit optimiztion.\");\n        globalLimitCtx.disableOpt();\n\n      }\n      if (outerQueryLimit == 0) {\n        // Believe it or not, some tools do generate queries with limit 0 and than expect\n        // query to run quickly. Lets meet their requirement.\n        LOG.info(\"Limit 0. No query execution needed.\");\n        return;\n      }\n    } else if (!isCStats) {\n      for (LoadTableDesc ltd : loadTableWork) {\n        Task<MoveWork> tsk = TaskFactory.get(new MoveWork(null, null, ltd, null, false), conf);\n        mvTask.add(tsk);\n        // Check to see if we are stale'ing any indexes and auto-update them if we want\n        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEINDEXAUTOUPDATE)) {\n          IndexUpdater indexUpdater = new IndexUpdater(loadTableWork, inputs, conf);\n          try {\n            List<Task<? extends Serializable>> indexUpdateTasks = indexUpdater\n                .generateUpdateTasks();\n            for (Task<? extends Serializable> updateTask : indexUpdateTasks) {\n              tsk.addDependentTask(updateTask);\n            }\n          } catch (HiveException e) {\n            console\n                .printInfo(\"WARNING: could not auto-update stale indexes, which are not in sync\");\n          }\n        }\n      }\n\n      boolean oneLoadFile = true;\n      for (LoadFileDesc lfd : loadFileWork) {\n        if (pCtx.getQueryProperties().isCTAS() || pCtx.getQueryProperties().isMaterializedView()) {\n          assert (oneLoadFile); // should not have more than 1 load file for\n          // CTAS\n          // make the movetask's destination directory the table's destination.\n          Path location;\n          String loc = pCtx.getQueryProperties().isCTAS() ?\n                  pCtx.getCreateTable().getLocation() : pCtx.getCreateViewDesc().getLocation();\n          if (loc == null) {\n            // get the default location\n            Path targetPath;\n            try {\n              String protoName = null;\n              if (pCtx.getQueryProperties().isCTAS()) {\n                protoName = pCtx.getCreateTable().getTableName();\n              } else if (pCtx.getQueryProperties().isMaterializedView()) {\n                protoName = pCtx.getCreateViewDesc().getViewName();\n              }\n              String[] names = Utilities.getDbTableName(protoName);\n              if (!db.databaseExists(names[0])) {\n                throw new SemanticException(\"ERROR: The database \" + names[0]\n                    + \" does not exist.\");\n              }\n              Warehouse wh = new Warehouse(conf);\n              targetPath = wh.getTablePath(db.getDatabase(names[0]), names[1]);\n            } catch (HiveException e) {\n              throw new SemanticException(e);\n            } catch (MetaException e) {\n              throw new SemanticException(e);\n            }\n\n            location = targetPath;\n          } else {\n            location = new Path(loc);\n          }\n          lfd.setTargetDir(location);\n\n          oneLoadFile = false;\n        }\n        mvTask.add(TaskFactory.get(new MoveWork(null, null, null, lfd, false), conf));\n      }\n    }\n\n    generateTaskTree(rootTasks, pCtx, mvTask, inputs, outputs);\n\n    // For each task, set the key descriptor for the reducer\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      GenMapRedUtils.setKeyAndValueDescForTaskTree(rootTask);\n    }\n\n    // If a task contains an operator which instructs bucketizedhiveinputformat\n    // to be used, please do so\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      setInputFormat(rootTask);\n    }\n\n    optimizeTaskPlan(rootTasks, pCtx, ctx);\n\n    /*\n     * If the query was the result of analyze table column compute statistics rewrite, create\n     * a column stats task instead of a fetch task to persist stats to the metastore.\n     */\n    if (isCStats || !pCtx.getColumnStatsAutoGatherContexts().isEmpty()) {\n      Set<Task<? extends Serializable>> leafTasks = new LinkedHashSet<Task<? extends Serializable>>();\n      getLeafTasks(rootTasks, leafTasks);\n      if (isCStats) {\n        genColumnStatsTask(pCtx.getAnalyzeRewrite(), loadFileWork, leafTasks, outerQueryLimit, 0);\n      } else {\n        for (ColumnStatsAutoGatherContext columnStatsAutoGatherContext : pCtx\n            .getColumnStatsAutoGatherContexts()) {\n          if (!columnStatsAutoGatherContext.isInsertInto()) {\n            genColumnStatsTask(columnStatsAutoGatherContext.getAnalyzeRewrite(),\n                columnStatsAutoGatherContext.getLoadFileWork(), leafTasks, outerQueryLimit, 0);\n          } else {\n            int numBitVector;\n            try {\n              numBitVector = HiveStatsUtils.getNumBitVectorsForNDVEstimation(conf);\n            } catch (Exception e) {\n              throw new SemanticException(e.getMessage());\n            }\n            genColumnStatsTask(columnStatsAutoGatherContext.getAnalyzeRewrite(),\n                columnStatsAutoGatherContext.getLoadFileWork(), leafTasks, outerQueryLimit, numBitVector);\n          }\n        }\n      }\n    }\n\n    decideExecMode(rootTasks, ctx, globalLimitCtx);\n\n    if (pCtx.getQueryProperties().isCTAS() && !pCtx.getCreateTable().isMaterialization()) {\n      // generate a DDL task and make it a dependent task of the leaf\n      CreateTableDesc crtTblDesc = pCtx.getCreateTable();\n\n      crtTblDesc.validate(conf);\n\n      Task<? extends Serializable> crtTblTask = TaskFactory.get(new DDLWork(\n          inputs, outputs, crtTblDesc), conf);\n      patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtTblTask);\n    } else if (pCtx.getQueryProperties().isMaterializedView()) {\n      // generate a DDL task and make it a dependent task of the leaf\n      CreateViewDesc viewDesc = pCtx.getCreateViewDesc();\n      Task<? extends Serializable> crtViewTask = TaskFactory.get(new DDLWork(\n          inputs, outputs, viewDesc), conf);\n      patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtViewTask);\n    }\n\n    if (globalLimitCtx.isEnable() && pCtx.getFetchTask() != null) {\n      LOG.info(\"set least row check for FetchTask: \" + globalLimitCtx.getGlobalLimit());\n      pCtx.getFetchTask().getWork().setLeastNumRows(globalLimitCtx.getGlobalLimit());\n    }\n\n    if (globalLimitCtx.isEnable() && globalLimitCtx.getLastReduceLimitDesc() != null) {\n      LOG.info(\"set least row check for LimitDesc: \" + globalLimitCtx.getGlobalLimit());\n      globalLimitCtx.getLastReduceLimitDesc().setLeastRows(globalLimitCtx.getGlobalLimit());\n      List<ExecDriver> mrTasks = Utilities.getMRTasks(rootTasks);\n      for (ExecDriver tsk : mrTasks) {\n        tsk.setRetryCmdWhenFail(true);\n      }\n      List<SparkTask> sparkTasks = Utilities.getSparkTasks(rootTasks);\n      for (SparkTask sparkTask : sparkTasks) {\n        sparkTask.setRetryCmdWhenFail(true);\n      }\n    }\n\n    Interner<TableDesc> interner = Interners.newStrongInterner();\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      GenMapRedUtils.internTableDesc(rootTask, interner);\n      GenMapRedUtils.deriveFinalExplainAttributes(rootTask, pCtx.getConf());\n    }\n  }",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  ",
            "  @SuppressWarnings({\"nls\", \"unchecked\"})\n  public void compile(final ParseContext pCtx, final List<Task<? extends Serializable>> rootTasks,\n      final HashSet<ReadEntity> inputs, final HashSet<WriteEntity> outputs) throws SemanticException {\n\n    Context ctx = pCtx.getContext();\n    GlobalLimitCtx globalLimitCtx = pCtx.getGlobalLimitCtx();\n    List<Task<MoveWork>> mvTask = new ArrayList<Task<MoveWork>>();\n\n    List<LoadTableDesc> loadTableWork = pCtx.getLoadTableWork();\n    List<LoadFileDesc> loadFileWork = pCtx.getLoadFileWork();\n\n    boolean isCStats = pCtx.getQueryProperties().isAnalyzeRewrite();\n    int outerQueryLimit = pCtx.getQueryProperties().getOuterQueryLimit();\n\n    if (pCtx.getFetchTask() != null) {\n      if (pCtx.getFetchTask().getTblDesc() == null) {\n        return;\n      }\n      pCtx.getFetchTask().getWork().setHiveServerQuery(SessionState.get().isHiveServerQuery());\n      TableDesc resultTab = pCtx.getFetchTask().getTblDesc();\n      // If the serializer is ThriftJDBCBinarySerDe, then it requires that NoOpFetchFormatter be used. But when it isn't,\n      // then either the ThriftFormatter or the DefaultFetchFormatter should be used.\n      if (!resultTab.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName())) {\n        if (SessionState.get().isHiveServerQuery()) {\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName());\n        } else {\n          String formatterName = conf.get(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER);\n          if (formatterName == null || formatterName.isEmpty()) {\n            conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName());\n          }\n        }\n      }\n\n      return;\n    }\n\n    optimizeOperatorPlan(pCtx, inputs, outputs);\n\n    /*\n     * In case of a select, use a fetch task instead of a move task.\n     * If the select is from analyze table column rewrite, don't create a fetch task. Instead create\n     * a column stats task later.\n     */\n    if (pCtx.getQueryProperties().isQuery() && !isCStats) {\n      if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1)) {\n        throw new SemanticException(ErrorMsg.INVALID_LOAD_TABLE_FILE_WORK.getMsg());\n      }\n\n      LoadFileDesc loadFileDesc = loadFileWork.get(0);\n\n      String cols = loadFileDesc.getColumns();\n      String colTypes = loadFileDesc.getColumnTypes();\n\n      String resFileFormat;\n      TableDesc resultTab = pCtx.getFetchTableDesc();\n      if (resultTab == null) {\n        resFileFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT);\n        if (SessionState.get().getIsUsingThriftJDBCBinarySerDe()\n            && (resFileFormat.equalsIgnoreCase(\"SequenceFile\"))) {\n          resultTab =\n              PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, resFileFormat,\n                  ThriftJDBCBinarySerDe.class);\n          // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n          // read formatted thrift objects from the output SequenceFile written by Tasks.\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n        } else {\n          resultTab =\n              PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, resFileFormat,\n                  LazySimpleSerDe.class);\n        }\n      } else {\n        if (resultTab.getProperties().getProperty(serdeConstants.SERIALIZATION_LIB)\n            .equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName())) {\n          // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n          // read formatted thrift objects from the output SequenceFile written by Tasks.\n          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n        }\n      }\n\n      FetchWork fetch = new FetchWork(loadFileDesc.getSourcePath(), resultTab, outerQueryLimit);\n      boolean isHiveServerQuery = SessionState.get().isHiveServerQuery();\n      fetch.setHiveServerQuery(isHiveServerQuery);\n      fetch.setSource(pCtx.getFetchSource());\n      fetch.setSink(pCtx.getFetchSink());\n      if (isHiveServerQuery &&\n        null != resultTab &&\n        resultTab.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) &&\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {\n          fetch.setIsUsingThriftJDBCBinarySerDe(true);\n      } else {\n          fetch.setIsUsingThriftJDBCBinarySerDe(false);\n      }\n\n      pCtx.setFetchTask((FetchTask) TaskFactory.get(fetch, conf));\n\n      // For the FetchTask, the limit optimization requires we fetch all the rows\n      // in memory and count how many rows we get. It's not practical if the\n      // limit factor is too big\n      int fetchLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVELIMITOPTMAXFETCH);\n      if (globalLimitCtx.isEnable() && globalLimitCtx.getGlobalLimit() > fetchLimit) {\n        LOG.info(\"For FetchTask, LIMIT \" + globalLimitCtx.getGlobalLimit() + \" > \" + fetchLimit\n            + \". Doesn't qualify limit optimization.\");\n        globalLimitCtx.disableOpt();\n\n      }\n      if (outerQueryLimit == 0) {\n        // Believe it or not, some tools do generate queries with limit 0 and than expect\n        // query to run quickly. Lets meet their requirement.\n        LOG.info(\"Limit 0. No query execution needed.\");\n        return;\n      }\n    } else if (!isCStats) {\n      for (LoadTableDesc ltd : loadTableWork) {\n        Task<MoveWork> tsk = TaskFactory.get(new MoveWork(null, null, ltd, null, false), conf);\n        mvTask.add(tsk);\n        // Check to see if we are stale'ing any indexes and auto-update them if we want\n        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEINDEXAUTOUPDATE)) {\n          IndexUpdater indexUpdater = new IndexUpdater(loadTableWork, inputs, conf);\n          try {\n            List<Task<? extends Serializable>> indexUpdateTasks = indexUpdater\n                .generateUpdateTasks();\n            for (Task<? extends Serializable> updateTask : indexUpdateTasks) {\n              tsk.addDependentTask(updateTask);\n            }\n          } catch (HiveException e) {\n            console\n                .printInfo(\"WARNING: could not auto-update stale indexes, which are not in sync\");\n          }\n        }\n      }\n\n      boolean oneLoadFile = true;\n      for (LoadFileDesc lfd : loadFileWork) {\n        if (pCtx.getQueryProperties().isCTAS() || pCtx.getQueryProperties().isMaterializedView()) {\n          assert (oneLoadFile); // should not have more than 1 load file for\n          // CTAS\n          // make the movetask's destination directory the table's destination.\n          Path location;\n          String loc = pCtx.getQueryProperties().isCTAS() ?\n                  pCtx.getCreateTable().getLocation() : pCtx.getCreateViewDesc().getLocation();\n          if (loc == null) {\n            // get the default location\n            Path targetPath;\n            try {\n              String protoName = null;\n              if (pCtx.getQueryProperties().isCTAS()) {\n                protoName = pCtx.getCreateTable().getTableName();\n              } else if (pCtx.getQueryProperties().isMaterializedView()) {\n                protoName = pCtx.getCreateViewDesc().getViewName();\n              }\n              String[] names = Utilities.getDbTableName(protoName);\n              if (!db.databaseExists(names[0])) {\n                throw new SemanticException(\"ERROR: The database \" + names[0]\n                    + \" does not exist.\");\n              }\n              Warehouse wh = new Warehouse(conf);\n              targetPath = wh.getTablePath(db.getDatabase(names[0]), names[1]);\n            } catch (HiveException e) {\n              throw new SemanticException(e);\n            } catch (MetaException e) {\n              throw new SemanticException(e);\n            }\n\n            location = targetPath;\n          } else {\n            location = new Path(loc);\n          }\n          lfd.setTargetDir(location);\n\n          oneLoadFile = false;\n        }\n        mvTask.add(TaskFactory.get(new MoveWork(null, null, null, lfd, false), conf));\n      }\n    }\n\n    generateTaskTree(rootTasks, pCtx, mvTask, inputs, outputs);\n\n    // For each task, set the key descriptor for the reducer\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      GenMapRedUtils.setKeyAndValueDescForTaskTree(rootTask);\n    }\n\n    // If a task contains an operator which instructs bucketizedhiveinputformat\n    // to be used, please do so\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      setInputFormat(rootTask);\n    }\n\n    optimizeTaskPlan(rootTasks, pCtx, ctx);\n\n    /*\n     * If the query was the result of analyze table column compute statistics rewrite, create\n     * a column stats task instead of a fetch task to persist stats to the metastore.\n     */\n    if (isCStats || !pCtx.getColumnStatsAutoGatherContexts().isEmpty()) {\n      Set<Task<? extends Serializable>> leafTasks = new LinkedHashSet<Task<? extends Serializable>>();\n      getLeafTasks(rootTasks, leafTasks);\n      if (isCStats) {\n        genColumnStatsTask(pCtx.getAnalyzeRewrite(), loadFileWork, leafTasks, outerQueryLimit, 0);\n      } else {\n        for (ColumnStatsAutoGatherContext columnStatsAutoGatherContext : pCtx\n            .getColumnStatsAutoGatherContexts()) {\n          if (!columnStatsAutoGatherContext.isInsertInto()) {\n            genColumnStatsTask(columnStatsAutoGatherContext.getAnalyzeRewrite(),\n                columnStatsAutoGatherContext.getLoadFileWork(), leafTasks, outerQueryLimit, 0);\n          } else {\n            int numBitVector;\n            try {\n              numBitVector = HiveStatsUtils.getNumBitVectorsForNDVEstimation(conf);\n            } catch (Exception e) {\n              throw new SemanticException(e.getMessage());\n            }\n            genColumnStatsTask(columnStatsAutoGatherContext.getAnalyzeRewrite(),\n                columnStatsAutoGatherContext.getLoadFileWork(), leafTasks, outerQueryLimit, numBitVector);\n          }\n        }\n      }\n    }\n\n    decideExecMode(rootTasks, ctx, globalLimitCtx);\n\n    if (pCtx.getQueryProperties().isCTAS() && !pCtx.getCreateTable().isMaterialization()) {\n      // generate a DDL task and make it a dependent task of the leaf\n      CreateTableDesc crtTblDesc = pCtx.getCreateTable();\n\n      crtTblDesc.validate(conf);\n\n      Task<? extends Serializable> crtTblTask = TaskFactory.get(new DDLWork(\n          inputs, outputs, crtTblDesc), conf);\n      patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtTblTask);\n    } else if (pCtx.getQueryProperties().isMaterializedView()) {\n      // generate a DDL task and make it a dependent task of the leaf\n      CreateViewDesc viewDesc = pCtx.getCreateViewDesc();\n      Task<? extends Serializable> crtViewTask = TaskFactory.get(new DDLWork(\n          inputs, outputs, viewDesc), conf);\n      patchUpAfterCTASorMaterializedView(rootTasks, outputs, crtViewTask);\n    }\n\n    if (globalLimitCtx.isEnable() && pCtx.getFetchTask() != null) {\n      LOG.info(\"set least row check for FetchTask: \" + globalLimitCtx.getGlobalLimit());\n      pCtx.getFetchTask().getWork().setLeastNumRows(globalLimitCtx.getGlobalLimit());\n    }\n\n    if (globalLimitCtx.isEnable() && globalLimitCtx.getLastReduceLimitDesc() != null) {\n      LOG.info(\"set least row check for LimitDesc: \" + globalLimitCtx.getGlobalLimit());\n      globalLimitCtx.getLastReduceLimitDesc().setLeastRows(globalLimitCtx.getGlobalLimit());\n      List<ExecDriver> mrTasks = Utilities.getMRTasks(rootTasks);\n      for (ExecDriver tsk : mrTasks) {\n        tsk.setRetryCmdWhenFail(true);\n      }\n      List<SparkTask> sparkTasks = Utilities.getSparkTasks(rootTasks);\n      for (SparkTask sparkTask : sparkTasks) {\n        sparkTask.setRetryCmdWhenFail(true);\n      }\n    }\n\n    Interner<TableDesc> interner = Interners.newStrongInterner();\n    for (Task<? extends Serializable> rootTask : rootTasks) {\n      GenMapRedUtils.internTableDesc(rootTask, interner);\n      GenMapRedUtils.deriveFinalExplainAttributes(rootTask, pCtx.getConf());\n    }\n  }"
        ],
        [
            "ListBucketingPrunerUtils::startComparisonInEqualNode(List,List,List,Boolean,ExprNodeDesc,ExprNodeDesc)",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  ",
            "  /**\n   * Comparison in equal node\n   *\n   * @param skewedCols\n   * @param cell\n   * @param uniqSkewedValues\n   * @param result\n   * @param left\n   * @param right\n   * @return\n   * @throws SemanticException\n   */\n  private static Boolean startComparisonInEqualNode(final List<String> skewedCols,\n      final List<String> cell, final List<List<String>> uniqSkewedValues, Boolean result,\n      ExprNodeDesc left, ExprNodeDesc right) throws SemanticException {\n    String columnNameInFilter = ((ExprNodeColumnDesc) left).getColumn();\n    String constantValueInFilter = ((ExprNodeConstantDesc) right).getValue().toString();\n    assert (skewedCols.contains(columnNameInFilter)) : \"List bucketing pruner has a column name \"\n        + columnNameInFilter\n        + \" which is not found in the partiton's skewed column list\";\n    int index = skewedCols.indexOf(columnNameInFilter);\n    assert (index < cell.size()) : \"GenericUDFOPEqual has a ExprNodeColumnDesc (\"\n        + columnNameInFilter + \") which is \" + index + \"th\" + \"skewed column. \"\n        + \" But it can't find the matching part in cell.\" + \" Because the cell size is \"\n        + cell.size();\n    String cellValueInPosition = cell.get(index);\n    assert (index < uniqSkewedValues.size()) : \"GenericUDFOPEqual has a ExprNodeColumnDesc (\"\n        + columnNameInFilter + \") which is \" + index + \"th\" + \"skewed column. \"\n        + \" But it can't find the matching part in uniq skewed value list.\"\n        + \" Because the cell size is \"\n        + uniqSkewedValues.size();\n    List<String> uniqSkewedValuesInPosition = uniqSkewedValues.get(index);\n    result = coreComparisonInEqualNode(constantValueInFilter, cellValueInPosition,\n        uniqSkewedValuesInPosition);\n    return result;\n  }",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  ",
            "  /**\n   * Comparison in equal node\n   *\n   * @param skewedCols\n   * @param cell\n   * @param uniqSkewedValues\n   * @param result\n   * @param left\n   * @param right\n   * @return\n   * @throws SemanticException\n   */\n  private static Boolean startComparisonInEqualNode(final List<String> skewedCols,\n      final List<String> cell, final List<List<String>> uniqSkewedValues, Boolean result,\n      ExprNodeDesc left, ExprNodeDesc right) throws SemanticException {\n    String columnNameInFilter = ((ExprNodeColumnDesc) left).getColumn();\n    String constantValueInFilter = ((ExprNodeConstantDesc) right).getValue().toString();\n    assert (skewedCols.contains(columnNameInFilter)) : \"List bucketing pruner has a column name \"\n        + columnNameInFilter\n        + \" which is not found in the partition's skewed column list\";\n    int index = skewedCols.indexOf(columnNameInFilter);\n    assert (index < cell.size()) : \"GenericUDFOPEqual has a ExprNodeColumnDesc (\"\n        + columnNameInFilter + \") which is \" + index + \"th\" + \"skewed column. \"\n        + \" But it can't find the matching part in cell.\" + \" Because the cell size is \"\n        + cell.size();\n    String cellValueInPosition = cell.get(index);\n    assert (index < uniqSkewedValues.size()) : \"GenericUDFOPEqual has a ExprNodeColumnDesc (\"\n        + columnNameInFilter + \") which is \" + index + \"th\" + \"skewed column. \"\n        + \" But it can't find the matching part in uniq skewed value list.\"\n        + \" Because the cell size is \"\n        + uniqSkewedValues.size();\n    List<String> uniqSkewedValuesInPosition = uniqSkewedValues.get(index);\n    result = coreComparisonInEqualNode(constantValueInFilter, cellValueInPosition,\n        uniqSkewedValuesInPosition);\n    return result;\n  }"
        ],
        [
            "DDLTask::alterIndex(Hive,AlterIndexDesc)",
            " 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 -\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  ",
            "  private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException {\n\n    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n      throw new UnsupportedOperationException(\"Indexes unsupported for Tez execution engine\");\n    }\n\n    String baseTableName = alterIndex.getBaseTableName();\n    String indexName = alterIndex.getIndexName();\n    Index idx = db.getIndex(baseTableName, indexName);\n\n    switch(alterIndex.getOp()) {\n    case ADDPROPS:\n      idx.getParameters().putAll(alterIndex.getProps());\n      break;\n    case UPDATETIMESTAMP:\n      try {\n        Map<String, String> props = new HashMap<String, String>();\n        Map<Map<String, String>, Long> basePartTs = new HashMap<Map<String, String>, Long>();\n\n        Table baseTbl = db.getTable(baseTableName);\n\n        if (baseTbl.isPartitioned()) {\n          List<Partition> baseParts;\n          if (alterIndex.getSpec() != null) {\n            baseParts = db.getPartitions(baseTbl, alterIndex.getSpec());\n          } else {\n            baseParts = db.getPartitions(baseTbl);\n          }\n          if (baseParts != null) {\n            for (Partition p : baseParts) {\n              FileSystem fs = p.getDataLocation().getFileSystem(db.getConf());\n              FileStatus fss = fs.getFileStatus(p.getDataLocation());\n              basePartTs.put(p.getSpec(), fss.getModificationTime());\n            }\n          }\n        } else {\n          FileSystem fs = baseTbl.getPath().getFileSystem(db.getConf());\n          FileStatus fss = fs.getFileStatus(baseTbl.getPath());\n          basePartTs.put(null, fss.getModificationTime());\n        }\n        for (Map<String, String> spec : basePartTs.keySet()) {\n          if (spec != null) {\n            props.put(spec.toString(), basePartTs.get(spec).toString());\n          } else {\n            props.put(\"base_timestamp\", basePartTs.get(null).toString());\n          }\n        }\n        idx.getParameters().putAll(props);\n      } catch (HiveException e) {\n        throw new HiveException(\"ERROR: Failed to update index timestamps\");\n      } catch (IOException e) {\n        throw new HiveException(\"ERROR: Failed to look up timestamps on filesystem\");\n      }\n\n      break;\n    default:\n      console.printError(\"Unsupported Alter commnad\");\n      return 1;\n    }\n\n    // set last modified by properties\n    if (!updateModifiedParameters(idx.getParameters(), conf)) {\n      return 1;\n    }\n\n    try {\n      db.alterIndex(baseTableName, indexName, idx);\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter index: \" + stringifyException(e));\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      return 1;\n    }\n    return 0;\n  }",
            " 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995 +\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  ",
            "  private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException {\n\n    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n      throw new UnsupportedOperationException(\"Indexes unsupported for Tez execution engine\");\n    }\n\n    String baseTableName = alterIndex.getBaseTableName();\n    String indexName = alterIndex.getIndexName();\n    Index idx = db.getIndex(baseTableName, indexName);\n\n    switch(alterIndex.getOp()) {\n    case ADDPROPS:\n      idx.getParameters().putAll(alterIndex.getProps());\n      break;\n    case UPDATETIMESTAMP:\n      try {\n        Map<String, String> props = new HashMap<String, String>();\n        Map<Map<String, String>, Long> basePartTs = new HashMap<Map<String, String>, Long>();\n\n        Table baseTbl = db.getTable(baseTableName);\n\n        if (baseTbl.isPartitioned()) {\n          List<Partition> baseParts;\n          if (alterIndex.getSpec() != null) {\n            baseParts = db.getPartitions(baseTbl, alterIndex.getSpec());\n          } else {\n            baseParts = db.getPartitions(baseTbl);\n          }\n          if (baseParts != null) {\n            for (Partition p : baseParts) {\n              FileSystem fs = p.getDataLocation().getFileSystem(db.getConf());\n              FileStatus fss = fs.getFileStatus(p.getDataLocation());\n              basePartTs.put(p.getSpec(), fss.getModificationTime());\n            }\n          }\n        } else {\n          FileSystem fs = baseTbl.getPath().getFileSystem(db.getConf());\n          FileStatus fss = fs.getFileStatus(baseTbl.getPath());\n          basePartTs.put(null, fss.getModificationTime());\n        }\n        for (Map<String, String> spec : basePartTs.keySet()) {\n          if (spec != null) {\n            props.put(spec.toString(), basePartTs.get(spec).toString());\n          } else {\n            props.put(\"base_timestamp\", basePartTs.get(null).toString());\n          }\n        }\n        idx.getParameters().putAll(props);\n      } catch (HiveException e) {\n        throw new HiveException(\"ERROR: Failed to update index timestamps\");\n      } catch (IOException e) {\n        throw new HiveException(\"ERROR: Failed to look up timestamps on filesystem\");\n      }\n\n      break;\n    default:\n      console.printError(\"Unsupported Alter command\");\n      return 1;\n    }\n\n    // set last modified by properties\n    if (!updateModifiedParameters(idx.getParameters(), conf)) {\n      return 1;\n    }\n\n    try {\n      db.alterIndex(baseTableName, indexName, idx);\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter index: \" + stringifyException(e));\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      return 1;\n    }\n    return 0;\n  }"
        ],
        [
            "ConstantPropagateProcCtx::getPropagatedConstants(Operator)",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 -\n 240  \n 241  ",
            "  /**\n   * Get propagated constant map from parents.\n   *\n   * Traverse all parents of current operator, if there is propagated constant (determined by\n   * assignment expression like column=constant value), resolve the column using RowResolver and add\n   * it to current constant map.\n   *\n   * @param op\n   *        operator getting the propagated constants.\n   * @return map of ColumnInfo to ExprNodeDesc. The values of that map must be either\n   *         ExprNodeConstantDesc or ExprNodeNullDesc.\n   */\n  public Map<ColumnInfo, ExprNodeDesc> getPropagatedConstants(Operator<? extends Serializable> op) {\n    // this map should map columnInfo to ExprConstantNodeDesc\n    Map<ColumnInfo, ExprNodeDesc> constants = new HashMap<ColumnInfo, ExprNodeDesc>();\n    if (op.getSchema() == null) {\n      return constants;\n    }\n    RowSchema rs = op.getSchema();\n    LOG.debug(\"Getting constants of op:\" + op + \" with rs:\" + rs);\n\n    if (op.getParentOperators() == null) {\n      return constants;\n    }\n\n    // A previous solution is based on tableAlias and colAlias, which is\n    // unsafe, esp. when CBO generates derived table names. see HIVE-13602.\n    // For correctness purpose, we only trust colExpMap.\n    // We assume that CBO can do the constantPropagation before this function is\n    // called to help improve the performance.\n    // UnionOperator, LimitOperator and FilterOperator are special, they should already be\n    // column-position aligned.\n\n    List<Map<Integer, ExprNodeDesc>> parentsToConstant = new ArrayList<>();\n    boolean areAllParentsContainConstant = true;\n    boolean noParentsContainConstant = true;\n    for (Operator<?> parent : op.getParentOperators()) {\n      Map<ColumnInfo, ExprNodeDesc> constMap = opToConstantExprs.get(parent);\n      if (constMap == null) {\n        LOG.debug(\"Constant of Op \" + parent.getOperatorId() + \" is not found\");\n        areAllParentsContainConstant = false;\n      } else {\n        noParentsContainConstant = false;\n        Map<Integer, ExprNodeDesc> map = new HashMap<>();\n        for (Entry<ColumnInfo, ExprNodeDesc> entry : constMap.entrySet()) {\n          map.put(parent.getSchema().getPosition(entry.getKey().getInternalName()),\n              entry.getValue());\n        }\n        parentsToConstant.add(map);\n        LOG.debug(\"Constant of Op \" + parent.getOperatorId() + \" \" + constMap);\n      }\n    }\n    if (noParentsContainConstant) {\n      return constants;\n    }\n\n    ArrayList<ColumnInfo> signature = op.getSchema().getSignature();\n    if (op instanceof LimitOperator || op instanceof FilterOperator) {\n      // there should be only one parent.\n      if (op.getParentOperators().size() == 1) {\n        Map<Integer, ExprNodeDesc> parentToConstant = parentsToConstant.get(0);\n        for (int index = 0; index < signature.size(); index++) {\n          if (parentToConstant.containsKey(index)) {\n            constants.put(signature.get(index), parentToConstant.get(index));\n          }\n        }\n      }\n    } else if (op instanceof UnionOperator && areAllParentsContainConstant) {\n      for (int index = 0; index < signature.size(); index++) {\n        ExprNodeDesc constant = null;\n        for (Map<Integer, ExprNodeDesc> parentToConstant : parentsToConstant) {\n          if (!parentToConstant.containsKey(index)) {\n            // if this parent does not contain a constant at this position, we\n            // continue to look at other positions.\n            constant = null;\n            break;\n          } else {\n            if (constant == null) {\n              constant = parentToConstant.get(index);\n            } else {\n              // compare if they are the same constant.\n              ExprNodeDesc nextConstant = parentToConstant.get(index);\n              if (!nextConstant.isSame(constant)) {\n                // they are not the same constant. for example, union all of 1\n                // and 2.\n                constant = null;\n                break;\n              }\n            }\n          }\n        }\n        // we have checked all the parents for the \"index\" position.\n        if (constant != null) {\n          constants.put(signature.get(index), constant);\n        }\n      }\n    } else if (op instanceof JoinOperator) {\n      JoinOperator joinOp = (JoinOperator) op;\n      Iterator<Entry<Byte, List<ExprNodeDesc>>> itr = joinOp.getConf().getExprs().entrySet()\n          .iterator();\n      while (itr.hasNext()) {\n        Entry<Byte, List<ExprNodeDesc>> e = itr.next();\n        int tag = e.getKey();\n        Operator<?> parent = op.getParentOperators().get(tag);\n        List<ExprNodeDesc> exprs = e.getValue();\n        if (exprs == null) {\n          continue;\n        }\n        for (ExprNodeDesc expr : exprs) {\n          // we are only interested in ExprNodeColumnDesc\n          if (expr instanceof ExprNodeColumnDesc) {\n            String parentColName = ((ExprNodeColumnDesc) expr).getColumn();\n            // find this parentColName in its parent's rs\n            int parentPos = parent.getSchema().getPosition(parentColName);\n            if (parentsToConstant.get(tag).containsKey(parentPos)) {\n              // this position in parent is a constant\n              // reverse look up colExprMap to find the childColName\n              if (op.getColumnExprMap() != null && op.getColumnExprMap().entrySet() != null) {\n                for (Entry<String, ExprNodeDesc> entry : op.getColumnExprMap().entrySet()) {\n                  if (entry.getValue().isSame(expr)) {\n                    // now propagate the constant from the parent to the child\n                    constants.put(signature.get(op.getSchema().getPosition(entry.getKey())),\n                        parentsToConstant.get(tag).get(parentPos));\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    } else {\n      // there should be only one parent.\n      if (op.getParentOperators().size() == 1) {\n        Operator<?> parent = op.getParentOperators().get(0);\n        if (op.getColumnExprMap() != null && op.getColumnExprMap().entrySet() != null) {\n          for (Entry<String, ExprNodeDesc> entry : op.getColumnExprMap().entrySet()) {\n            if (op.getSchema().getPosition(entry.getKey()) == -1) {\n              // Not present\n              continue;\n            }\n            ExprNodeDesc expr = entry.getValue();\n            if (expr instanceof ExprNodeColumnDesc) {\n              String parentColName = ((ExprNodeColumnDesc) expr).getColumn();\n              // find this parentColName in its parent's rs\n              int parentPos = parent.getSchema().getPosition(parentColName);\n              if (parentsToConstant.get(0).containsKey(parentPos)) {\n                // this position in parent is a constant\n                // now propagate the constant from the parent to the child\n                constants.put(signature.get(op.getSchema().getPosition(entry.getKey())),\n                    parentsToConstant.get(0).get(parentPos));\n              }\n            }\n          }\n        }\n      }\n    }\n    LOG.debug(\"Offerring constants \" + constants.keySet() + \" to operator \" + op.toString());\n    return constants;\n  }",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 +\n 240  \n 241  ",
            "  /**\n   * Get propagated constant map from parents.\n   *\n   * Traverse all parents of current operator, if there is propagated constant (determined by\n   * assignment expression like column=constant value), resolve the column using RowResolver and add\n   * it to current constant map.\n   *\n   * @param op\n   *        operator getting the propagated constants.\n   * @return map of ColumnInfo to ExprNodeDesc. The values of that map must be either\n   *         ExprNodeConstantDesc or ExprNodeNullDesc.\n   */\n  public Map<ColumnInfo, ExprNodeDesc> getPropagatedConstants(Operator<? extends Serializable> op) {\n    // this map should map columnInfo to ExprConstantNodeDesc\n    Map<ColumnInfo, ExprNodeDesc> constants = new HashMap<ColumnInfo, ExprNodeDesc>();\n    if (op.getSchema() == null) {\n      return constants;\n    }\n    RowSchema rs = op.getSchema();\n    LOG.debug(\"Getting constants of op:\" + op + \" with rs:\" + rs);\n\n    if (op.getParentOperators() == null) {\n      return constants;\n    }\n\n    // A previous solution is based on tableAlias and colAlias, which is\n    // unsafe, esp. when CBO generates derived table names. see HIVE-13602.\n    // For correctness purpose, we only trust colExpMap.\n    // We assume that CBO can do the constantPropagation before this function is\n    // called to help improve the performance.\n    // UnionOperator, LimitOperator and FilterOperator are special, they should already be\n    // column-position aligned.\n\n    List<Map<Integer, ExprNodeDesc>> parentsToConstant = new ArrayList<>();\n    boolean areAllParentsContainConstant = true;\n    boolean noParentsContainConstant = true;\n    for (Operator<?> parent : op.getParentOperators()) {\n      Map<ColumnInfo, ExprNodeDesc> constMap = opToConstantExprs.get(parent);\n      if (constMap == null) {\n        LOG.debug(\"Constant of Op \" + parent.getOperatorId() + \" is not found\");\n        areAllParentsContainConstant = false;\n      } else {\n        noParentsContainConstant = false;\n        Map<Integer, ExprNodeDesc> map = new HashMap<>();\n        for (Entry<ColumnInfo, ExprNodeDesc> entry : constMap.entrySet()) {\n          map.put(parent.getSchema().getPosition(entry.getKey().getInternalName()),\n              entry.getValue());\n        }\n        parentsToConstant.add(map);\n        LOG.debug(\"Constant of Op \" + parent.getOperatorId() + \" \" + constMap);\n      }\n    }\n    if (noParentsContainConstant) {\n      return constants;\n    }\n\n    ArrayList<ColumnInfo> signature = op.getSchema().getSignature();\n    if (op instanceof LimitOperator || op instanceof FilterOperator) {\n      // there should be only one parent.\n      if (op.getParentOperators().size() == 1) {\n        Map<Integer, ExprNodeDesc> parentToConstant = parentsToConstant.get(0);\n        for (int index = 0; index < signature.size(); index++) {\n          if (parentToConstant.containsKey(index)) {\n            constants.put(signature.get(index), parentToConstant.get(index));\n          }\n        }\n      }\n    } else if (op instanceof UnionOperator && areAllParentsContainConstant) {\n      for (int index = 0; index < signature.size(); index++) {\n        ExprNodeDesc constant = null;\n        for (Map<Integer, ExprNodeDesc> parentToConstant : parentsToConstant) {\n          if (!parentToConstant.containsKey(index)) {\n            // if this parent does not contain a constant at this position, we\n            // continue to look at other positions.\n            constant = null;\n            break;\n          } else {\n            if (constant == null) {\n              constant = parentToConstant.get(index);\n            } else {\n              // compare if they are the same constant.\n              ExprNodeDesc nextConstant = parentToConstant.get(index);\n              if (!nextConstant.isSame(constant)) {\n                // they are not the same constant. for example, union all of 1\n                // and 2.\n                constant = null;\n                break;\n              }\n            }\n          }\n        }\n        // we have checked all the parents for the \"index\" position.\n        if (constant != null) {\n          constants.put(signature.get(index), constant);\n        }\n      }\n    } else if (op instanceof JoinOperator) {\n      JoinOperator joinOp = (JoinOperator) op;\n      Iterator<Entry<Byte, List<ExprNodeDesc>>> itr = joinOp.getConf().getExprs().entrySet()\n          .iterator();\n      while (itr.hasNext()) {\n        Entry<Byte, List<ExprNodeDesc>> e = itr.next();\n        int tag = e.getKey();\n        Operator<?> parent = op.getParentOperators().get(tag);\n        List<ExprNodeDesc> exprs = e.getValue();\n        if (exprs == null) {\n          continue;\n        }\n        for (ExprNodeDesc expr : exprs) {\n          // we are only interested in ExprNodeColumnDesc\n          if (expr instanceof ExprNodeColumnDesc) {\n            String parentColName = ((ExprNodeColumnDesc) expr).getColumn();\n            // find this parentColName in its parent's rs\n            int parentPos = parent.getSchema().getPosition(parentColName);\n            if (parentsToConstant.get(tag).containsKey(parentPos)) {\n              // this position in parent is a constant\n              // reverse look up colExprMap to find the childColName\n              if (op.getColumnExprMap() != null && op.getColumnExprMap().entrySet() != null) {\n                for (Entry<String, ExprNodeDesc> entry : op.getColumnExprMap().entrySet()) {\n                  if (entry.getValue().isSame(expr)) {\n                    // now propagate the constant from the parent to the child\n                    constants.put(signature.get(op.getSchema().getPosition(entry.getKey())),\n                        parentsToConstant.get(tag).get(parentPos));\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    } else {\n      // there should be only one parent.\n      if (op.getParentOperators().size() == 1) {\n        Operator<?> parent = op.getParentOperators().get(0);\n        if (op.getColumnExprMap() != null && op.getColumnExprMap().entrySet() != null) {\n          for (Entry<String, ExprNodeDesc> entry : op.getColumnExprMap().entrySet()) {\n            if (op.getSchema().getPosition(entry.getKey()) == -1) {\n              // Not present\n              continue;\n            }\n            ExprNodeDesc expr = entry.getValue();\n            if (expr instanceof ExprNodeColumnDesc) {\n              String parentColName = ((ExprNodeColumnDesc) expr).getColumn();\n              // find this parentColName in its parent's rs\n              int parentPos = parent.getSchema().getPosition(parentColName);\n              if (parentsToConstant.get(0).containsKey(parentPos)) {\n                // this position in parent is a constant\n                // now propagate the constant from the parent to the child\n                constants.put(signature.get(op.getSchema().getPosition(entry.getKey())),\n                    parentsToConstant.get(0).get(parentPos));\n              }\n            }\n          }\n        }\n      }\n    }\n    LOG.debug(\"Offering constants \" + constants.keySet() + \" to operator \" + op.toString());\n    return constants;\n  }"
        ],
        [
            "GenMapRedUtils::addStatsTask(FileSinkOperator,MoveTask,Task,HiveConf)",
            "1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482 -\n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  ",
            "  /**\n   * Add the StatsTask as a dependent task of the MoveTask\n   * because StatsTask will change the Table/Partition metadata. For atomicity, we\n   * should not change it before the data is actually there done by MoveTask.\n   *\n   * @param nd\n   *          the FileSinkOperator whose results are taken care of by the MoveTask.\n   * @param mvTask\n   *          The MoveTask that moves the FileSinkOperator's results.\n   * @param currTask\n   *          The MapRedTask that the FileSinkOperator belongs to.\n   * @param hconf\n   *          HiveConf\n   */\n  public static void addStatsTask(FileSinkOperator nd, MoveTask mvTask,\n      Task<? extends Serializable> currTask, HiveConf hconf) {\n\n    MoveWork mvWork = mvTask.getWork();\n    StatsWork statsWork = null;\n    if (mvWork.getLoadTableWork() != null) {\n      statsWork = new StatsWork(mvWork.getLoadTableWork());\n    } else if (mvWork.getLoadFileWork() != null) {\n      statsWork = new StatsWork(mvWork.getLoadFileWork());\n    }\n    assert statsWork != null : \"Error when genereting StatsTask\";\n\n    statsWork.setSourceTask(currTask);\n    statsWork.setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));\n    statsWork.setStatsTmpDir(nd.getConf().getStatsTmpDir());\n    if (currTask.getWork() instanceof MapredWork) {\n      MapredWork mrWork = (MapredWork) currTask.getWork();\n      mrWork.getMapWork().setGatheringStats(true);\n      if (mrWork.getReduceWork() != null) {\n        mrWork.getReduceWork().setGatheringStats(true);\n      }\n    } else if (currTask.getWork() instanceof SparkWork) {\n      SparkWork work = (SparkWork) currTask.getWork();\n      for (BaseWork w: work.getAllWork()) {\n        w.setGatheringStats(true);\n      }\n    } else { // must be TezWork\n      TezWork work = (TezWork) currTask.getWork();\n      for (BaseWork w: work.getAllWork()) {\n        w.setGatheringStats(true);\n      }\n    }\n\n    // AggKey in StatsWork is used for stats aggregation while StatsAggPrefix\n    // in FileSinkDesc is used for stats publishing. They should be consistent.\n    statsWork.setAggKey(nd.getConf().getStatsAggPrefix());\n    Task<? extends Serializable> statsTask = TaskFactory.get(statsWork, hconf);\n\n    // subscribe feeds from the MoveTask so that MoveTask can forward the list\n    // of dynamic partition list to the StatsTask\n    mvTask.addDependentTask(statsTask);\n    statsTask.subscribeFeed(mvTask);\n  }",
            "1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482 +\n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  ",
            "  /**\n   * Add the StatsTask as a dependent task of the MoveTask\n   * because StatsTask will change the Table/Partition metadata. For atomicity, we\n   * should not change it before the data is actually there done by MoveTask.\n   *\n   * @param nd\n   *          the FileSinkOperator whose results are taken care of by the MoveTask.\n   * @param mvTask\n   *          The MoveTask that moves the FileSinkOperator's results.\n   * @param currTask\n   *          The MapRedTask that the FileSinkOperator belongs to.\n   * @param hconf\n   *          HiveConf\n   */\n  public static void addStatsTask(FileSinkOperator nd, MoveTask mvTask,\n      Task<? extends Serializable> currTask, HiveConf hconf) {\n\n    MoveWork mvWork = mvTask.getWork();\n    StatsWork statsWork = null;\n    if (mvWork.getLoadTableWork() != null) {\n      statsWork = new StatsWork(mvWork.getLoadTableWork());\n    } else if (mvWork.getLoadFileWork() != null) {\n      statsWork = new StatsWork(mvWork.getLoadFileWork());\n    }\n    assert statsWork != null : \"Error when generating StatsTask\";\n\n    statsWork.setSourceTask(currTask);\n    statsWork.setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));\n    statsWork.setStatsTmpDir(nd.getConf().getStatsTmpDir());\n    if (currTask.getWork() instanceof MapredWork) {\n      MapredWork mrWork = (MapredWork) currTask.getWork();\n      mrWork.getMapWork().setGatheringStats(true);\n      if (mrWork.getReduceWork() != null) {\n        mrWork.getReduceWork().setGatheringStats(true);\n      }\n    } else if (currTask.getWork() instanceof SparkWork) {\n      SparkWork work = (SparkWork) currTask.getWork();\n      for (BaseWork w: work.getAllWork()) {\n        w.setGatheringStats(true);\n      }\n    } else { // must be TezWork\n      TezWork work = (TezWork) currTask.getWork();\n      for (BaseWork w: work.getAllWork()) {\n        w.setGatheringStats(true);\n      }\n    }\n\n    // AggKey in StatsWork is used for stats aggregation while StatsAggPrefix\n    // in FileSinkDesc is used for stats publishing. They should be consistent.\n    statsWork.setAggKey(nd.getConf().getStatsAggPrefix());\n    Task<? extends Serializable> statsTask = TaskFactory.get(statsWork, hconf);\n\n    // subscribe feeds from the MoveTask so that MoveTask can forward the list\n    // of dynamic partition list to the StatsTask\n    mvTask.addDependentTask(statsTask);\n    statsTask.subscribeFeed(mvTask);\n  }"
        ],
        [
            "DDLTask::dumpLockInfo(DataOutputStream,ShowLocksResponse)",
            "2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636 -\n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  ",
            "  public static void dumpLockInfo(DataOutputStream os, ShowLocksResponse rsp) throws IOException {\n    // Write a header\n    os.writeBytes(\"Lock ID\");\n    os.write(separator);\n    os.writeBytes(\"Database\");\n    os.write(separator);\n    os.writeBytes(\"Table\");\n    os.write(separator);\n    os.writeBytes(\"Partition\");\n    os.write(separator);\n    os.writeBytes(\"State\");\n    os.write(separator);\n    os.writeBytes(\"Blocked By\");\n    os.write(separator);\n    os.writeBytes(\"Type\");\n    os.write(separator);\n    os.writeBytes(\"Transaction ID\");\n    os.write(separator);\n    os.writeBytes(\"Last Hearbeat\");\n    os.write(separator);\n    os.writeBytes(\"Acquired At\");\n    os.write(separator);\n    os.writeBytes(\"User\");\n    os.write(separator);\n    os.writeBytes(\"Hostname\");\n    os.write(separator);\n    os.writeBytes(\"Agent Info\");\n    os.write(terminator);\n\n    List<ShowLocksResponseElement> locks = rsp.getLocks();\n    if (locks != null) {\n      for (ShowLocksResponseElement lock : locks) {\n        if(lock.isSetLockIdInternal()) {\n          os.writeBytes(Long.toString(lock.getLockid()) + \".\" + Long.toString(lock.getLockIdInternal()));\n        }\n        else {\n          os.writeBytes(Long.toString(lock.getLockid()));\n        }\n        os.write(separator);\n        os.writeBytes(lock.getDbname());\n        os.write(separator);\n        os.writeBytes((lock.getTablename() == null) ? \"NULL\" : lock.getTablename());\n        os.write(separator);\n        os.writeBytes((lock.getPartname() == null) ? \"NULL\" : lock.getPartname());\n        os.write(separator);\n        os.writeBytes(lock.getState().toString());\n        os.write(separator);\n        if(lock.isSetBlockedByExtId()) {//both \"blockedby\" are either there or not\n          os.writeBytes(Long.toString(lock.getBlockedByExtId()) + \".\" + Long.toString(lock.getBlockedByIntId()));\n        }\n        else {\n          os.writeBytes(\"            \");//12 chars - try to keep cols aligned\n        }\n        os.write(separator);\n        os.writeBytes(lock.getType().toString());\n        os.write(separator);\n        os.writeBytes((lock.getTxnid() == 0) ? \"NULL\" : Long.toString(lock.getTxnid()));\n        os.write(separator);\n        os.writeBytes(Long.toString(lock.getLastheartbeat()));\n        os.write(separator);\n        os.writeBytes((lock.getAcquiredat() == 0) ? \"NULL\" : Long.toString(lock.getAcquiredat()));\n        os.write(separator);\n        os.writeBytes(lock.getUser());\n        os.write(separator);\n        os.writeBytes(lock.getHostname());\n        os.write(separator);\n        os.writeBytes(lock.getAgentInfo() == null ? \"NULL\" : lock.getAgentInfo());\n        os.write(separator);\n        os.write(terminator);\n      }\n    }\n  }",
            "2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636 +\n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  ",
            "  public static void dumpLockInfo(DataOutputStream os, ShowLocksResponse rsp) throws IOException {\n    // Write a header\n    os.writeBytes(\"Lock ID\");\n    os.write(separator);\n    os.writeBytes(\"Database\");\n    os.write(separator);\n    os.writeBytes(\"Table\");\n    os.write(separator);\n    os.writeBytes(\"Partition\");\n    os.write(separator);\n    os.writeBytes(\"State\");\n    os.write(separator);\n    os.writeBytes(\"Blocked By\");\n    os.write(separator);\n    os.writeBytes(\"Type\");\n    os.write(separator);\n    os.writeBytes(\"Transaction ID\");\n    os.write(separator);\n    os.writeBytes(\"Last Heartbeat\");\n    os.write(separator);\n    os.writeBytes(\"Acquired At\");\n    os.write(separator);\n    os.writeBytes(\"User\");\n    os.write(separator);\n    os.writeBytes(\"Hostname\");\n    os.write(separator);\n    os.writeBytes(\"Agent Info\");\n    os.write(terminator);\n\n    List<ShowLocksResponseElement> locks = rsp.getLocks();\n    if (locks != null) {\n      for (ShowLocksResponseElement lock : locks) {\n        if(lock.isSetLockIdInternal()) {\n          os.writeBytes(Long.toString(lock.getLockid()) + \".\" + Long.toString(lock.getLockIdInternal()));\n        }\n        else {\n          os.writeBytes(Long.toString(lock.getLockid()));\n        }\n        os.write(separator);\n        os.writeBytes(lock.getDbname());\n        os.write(separator);\n        os.writeBytes((lock.getTablename() == null) ? \"NULL\" : lock.getTablename());\n        os.write(separator);\n        os.writeBytes((lock.getPartname() == null) ? \"NULL\" : lock.getPartname());\n        os.write(separator);\n        os.writeBytes(lock.getState().toString());\n        os.write(separator);\n        if(lock.isSetBlockedByExtId()) {//both \"blockedby\" are either there or not\n          os.writeBytes(Long.toString(lock.getBlockedByExtId()) + \".\" + Long.toString(lock.getBlockedByIntId()));\n        }\n        else {\n          os.writeBytes(\"            \");//12 chars - try to keep cols aligned\n        }\n        os.write(separator);\n        os.writeBytes(lock.getType().toString());\n        os.write(separator);\n        os.writeBytes((lock.getTxnid() == 0) ? \"NULL\" : Long.toString(lock.getTxnid()));\n        os.write(separator);\n        os.writeBytes(Long.toString(lock.getLastheartbeat()));\n        os.write(separator);\n        os.writeBytes((lock.getAcquiredat() == 0) ? \"NULL\" : Long.toString(lock.getAcquiredat()));\n        os.write(separator);\n        os.writeBytes(lock.getUser());\n        os.write(separator);\n        os.writeBytes(lock.getHostname());\n        os.write(separator);\n        os.writeBytes(lock.getAgentInfo() == null ? \"NULL\" : lock.getAgentInfo());\n        os.write(separator);\n        os.write(terminator);\n      }\n    }\n  }"
        ],
        [
            "DDLSemanticAnalyzer::analyzeAlterTablePartMergeFiles(ASTNode,String,HashMap)",
            "1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640 -\n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  ",
            "  private void analyzeAlterTablePartMergeFiles(ASTNode ast,\n      String tableName, HashMap<String, String> partSpec)\n      throws SemanticException {\n    AlterTablePartMergeFilesDesc mergeDesc = new AlterTablePartMergeFilesDesc(\n        tableName, partSpec);\n\n    List<Path> inputDir = new ArrayList<Path>();\n    Path oldTblPartLoc = null;\n    Path newTblPartLoc = null;\n    Table tblObj = null;\n    ListBucketingCtx lbCtx = null;\n\n    try {\n      tblObj = getTable(tableName);\n\n      List<String> bucketCols = null;\n      Class<? extends InputFormat> inputFormatClass = null;\n      boolean isArchived = false;\n      boolean checkIndex = HiveConf.getBoolVar(conf,\n          HiveConf.ConfVars.HIVE_CONCATENATE_CHECK_INDEX);\n      if (checkIndex) {\n        List<Index> indexes = db.getIndexes(tblObj.getDbName(), tblObj.getTableName(),\n            Short.MAX_VALUE);\n        if (indexes != null && indexes.size() > 0) {\n          throw new SemanticException(\"can not do merge because source table \"\n              + tableName + \" is indexed.\");\n        }\n      }\n\n      if (tblObj.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(\"source table \" + tableName\n              + \" is partitioned but no partition desc found.\");\n        } else {\n          Partition part = getPartition(tblObj, partSpec, false);\n          if (part == null) {\n            throw new SemanticException(\"source table \" + tableName\n                + \" is partitioned but partition not found.\");\n          }\n          bucketCols = part.getBucketCols();\n          inputFormatClass = part.getInputFormatClass();\n          isArchived = ArchiveUtils.isArchived(part);\n\n          Path tabPath = tblObj.getPath();\n          Path partPath = part.getDataLocation();\n\n          // if the table is in a different dfs than the partition,\n          // replace the partition's dfs with the table's dfs.\n          newTblPartLoc = new Path(tabPath.toUri().getScheme(), tabPath.toUri()\n              .getAuthority(), partPath.toUri().getPath());\n\n          oldTblPartLoc = partPath;\n\n          lbCtx = constructListBucketingCtx(part.getSkewedColNames(), part.getSkewedColValues(),\n              part.getSkewedColValueLocationMaps(), part.isStoredAsSubDirectories(), conf);\n        }\n      } else {\n        inputFormatClass = tblObj.getInputFormatClass();\n        bucketCols = tblObj.getBucketCols();\n\n        // input and output are the same\n        oldTblPartLoc = tblObj.getPath();\n        newTblPartLoc = tblObj.getPath();\n\n        lbCtx = constructListBucketingCtx(tblObj.getSkewedColNames(), tblObj.getSkewedColValues(),\n            tblObj.getSkewedColValueLocationMaps(), tblObj.isStoredAsSubDirectories(), conf);\n      }\n\n      // throw a HiveException for other than rcfile and orcfile.\n      if (!((inputFormatClass.equals(RCFileInputFormat.class) ||\n          (inputFormatClass.equals(OrcInputFormat.class))))) {\n        throw new SemanticException(\n            \"Only RCFile and ORCFile Formats are supportted right now.\");\n      }\n      mergeDesc.setInputFormatClass(inputFormatClass);\n\n      // throw a HiveException if the table/partition is bucketized\n      if (bucketCols != null && bucketCols.size() > 0) {\n        throw new SemanticException(\n            \"Merge can not perform on bucketized partition/table.\");\n      }\n\n      // throw a HiveException if the table/partition is archived\n      if (isArchived) {\n        throw new SemanticException(\n            \"Merge can not perform on archived partitions.\");\n      }\n\n      inputDir.add(oldTblPartLoc);\n\n      mergeDesc.setInputDir(inputDir);\n\n      mergeDesc.setLbCtx(lbCtx);\n\n      addInputsOutputsAlterTable(tableName, partSpec, AlterTableTypes.MERGEFILES);\n      DDLWork ddlWork = new DDLWork(getInputs(), getOutputs(), mergeDesc);\n      ddlWork.setNeedLock(true);\n      Task<? extends Serializable> mergeTask = TaskFactory.get(ddlWork, conf);\n      TableDesc tblDesc = Utilities.getTableDesc(tblObj);\n      Path queryTmpdir = ctx.getExternalTmpPath(newTblPartLoc);\n      mergeDesc.setOutputDir(queryTmpdir);\n      LoadTableDesc ltd = new LoadTableDesc(queryTmpdir, tblDesc,\n          partSpec == null ? new HashMap<String, String>() : partSpec);\n      ltd.setLbCtx(lbCtx);\n      Task<MoveWork> moveTsk = TaskFactory.get(new MoveWork(null, null, ltd, null, false),\n          conf);\n      mergeTask.addDependentTask(moveTsk);\n\n      if (conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsWork statDesc;\n        if (oldTblPartLoc.equals(newTblPartLoc)) {\n          // If we're merging to the same location, we can avoid some metastore calls\n          TableSpec tablepart = new TableSpec(db, conf, tableName, partSpec);\n          statDesc = new StatsWork(tablepart);\n        } else {\n          statDesc = new StatsWork(ltd);\n        }\n        statDesc.setNoStatsAggregator(true);\n        statDesc.setClearAggregatorStats(true);\n        statDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n        Task<? extends Serializable> statTask = TaskFactory.get(statDesc, conf);\n        moveTsk.addDependentTask(statTask);\n      }\n\n      rootTasks.add(mergeTask);\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }",
            "1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640 +\n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  ",
            "  private void analyzeAlterTablePartMergeFiles(ASTNode ast,\n      String tableName, HashMap<String, String> partSpec)\n      throws SemanticException {\n    AlterTablePartMergeFilesDesc mergeDesc = new AlterTablePartMergeFilesDesc(\n        tableName, partSpec);\n\n    List<Path> inputDir = new ArrayList<Path>();\n    Path oldTblPartLoc = null;\n    Path newTblPartLoc = null;\n    Table tblObj = null;\n    ListBucketingCtx lbCtx = null;\n\n    try {\n      tblObj = getTable(tableName);\n\n      List<String> bucketCols = null;\n      Class<? extends InputFormat> inputFormatClass = null;\n      boolean isArchived = false;\n      boolean checkIndex = HiveConf.getBoolVar(conf,\n          HiveConf.ConfVars.HIVE_CONCATENATE_CHECK_INDEX);\n      if (checkIndex) {\n        List<Index> indexes = db.getIndexes(tblObj.getDbName(), tblObj.getTableName(),\n            Short.MAX_VALUE);\n        if (indexes != null && indexes.size() > 0) {\n          throw new SemanticException(\"can not do merge because source table \"\n              + tableName + \" is indexed.\");\n        }\n      }\n\n      if (tblObj.isPartitioned()) {\n        if (partSpec == null) {\n          throw new SemanticException(\"source table \" + tableName\n              + \" is partitioned but no partition desc found.\");\n        } else {\n          Partition part = getPartition(tblObj, partSpec, false);\n          if (part == null) {\n            throw new SemanticException(\"source table \" + tableName\n                + \" is partitioned but partition not found.\");\n          }\n          bucketCols = part.getBucketCols();\n          inputFormatClass = part.getInputFormatClass();\n          isArchived = ArchiveUtils.isArchived(part);\n\n          Path tabPath = tblObj.getPath();\n          Path partPath = part.getDataLocation();\n\n          // if the table is in a different dfs than the partition,\n          // replace the partition's dfs with the table's dfs.\n          newTblPartLoc = new Path(tabPath.toUri().getScheme(), tabPath.toUri()\n              .getAuthority(), partPath.toUri().getPath());\n\n          oldTblPartLoc = partPath;\n\n          lbCtx = constructListBucketingCtx(part.getSkewedColNames(), part.getSkewedColValues(),\n              part.getSkewedColValueLocationMaps(), part.isStoredAsSubDirectories(), conf);\n        }\n      } else {\n        inputFormatClass = tblObj.getInputFormatClass();\n        bucketCols = tblObj.getBucketCols();\n\n        // input and output are the same\n        oldTblPartLoc = tblObj.getPath();\n        newTblPartLoc = tblObj.getPath();\n\n        lbCtx = constructListBucketingCtx(tblObj.getSkewedColNames(), tblObj.getSkewedColValues(),\n            tblObj.getSkewedColValueLocationMaps(), tblObj.isStoredAsSubDirectories(), conf);\n      }\n\n      // throw a HiveException for other than rcfile and orcfile.\n      if (!((inputFormatClass.equals(RCFileInputFormat.class) ||\n          (inputFormatClass.equals(OrcInputFormat.class))))) {\n        throw new SemanticException(\n            \"Only RCFile and ORCFile Formats are supported right now.\");\n      }\n      mergeDesc.setInputFormatClass(inputFormatClass);\n\n      // throw a HiveException if the table/partition is bucketized\n      if (bucketCols != null && bucketCols.size() > 0) {\n        throw new SemanticException(\n            \"Merge can not perform on bucketized partition/table.\");\n      }\n\n      // throw a HiveException if the table/partition is archived\n      if (isArchived) {\n        throw new SemanticException(\n            \"Merge can not perform on archived partitions.\");\n      }\n\n      inputDir.add(oldTblPartLoc);\n\n      mergeDesc.setInputDir(inputDir);\n\n      mergeDesc.setLbCtx(lbCtx);\n\n      addInputsOutputsAlterTable(tableName, partSpec, AlterTableTypes.MERGEFILES);\n      DDLWork ddlWork = new DDLWork(getInputs(), getOutputs(), mergeDesc);\n      ddlWork.setNeedLock(true);\n      Task<? extends Serializable> mergeTask = TaskFactory.get(ddlWork, conf);\n      TableDesc tblDesc = Utilities.getTableDesc(tblObj);\n      Path queryTmpdir = ctx.getExternalTmpPath(newTblPartLoc);\n      mergeDesc.setOutputDir(queryTmpdir);\n      LoadTableDesc ltd = new LoadTableDesc(queryTmpdir, tblDesc,\n          partSpec == null ? new HashMap<String, String>() : partSpec);\n      ltd.setLbCtx(lbCtx);\n      Task<MoveWork> moveTsk = TaskFactory.get(new MoveWork(null, null, ltd, null, false),\n          conf);\n      mergeTask.addDependentTask(moveTsk);\n\n      if (conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n        StatsWork statDesc;\n        if (oldTblPartLoc.equals(newTblPartLoc)) {\n          // If we're merging to the same location, we can avoid some metastore calls\n          TableSpec tablepart = new TableSpec(db, conf, tableName, partSpec);\n          statDesc = new StatsWork(tablepart);\n        } else {\n          statDesc = new StatsWork(ltd);\n        }\n        statDesc.setNoStatsAggregator(true);\n        statDesc.setClearAggregatorStats(true);\n        statDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n        Task<? extends Serializable> statTask = TaskFactory.get(statDesc, conf);\n        moveTsk.addDependentTask(statTask);\n      }\n\n      rootTasks.add(mergeTask);\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedStream(long,DiskRangeList,long,long,ColumnStreamData,long,long)",
            " 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646 -\n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset)\n          throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    boolean isCompressed = codec != null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<ByteBuffer> toRelease = null;\n    List<IncompleteCb> badEstimates = null;\n    if (isCompressed) {\n      toRelease = !dataReader.isTrackingDiskRanges() ? null : new ArrayList<ByteBuffer>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset,\n              unlockUntilCOffset, current, csd, toRelease, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \" compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepate \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) return lastUncompressed; // Nothing to do.\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize);\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        decompressChunk(chunk.originalData, codec, dest);\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      chunk.originalData = null;\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      cacheWrapper.reuseBuffer(chunk.getBuffer());\n    }\n\n    // 5. Release original compressed buffers to zero-copy reader if needed.\n    if (toRelease != null) {\n      assert dataReader.isTrackingDiskRanges();\n      for (ByteBuffer buffer : toRelease) {\n        dataReader.releaseBuffer(buffer);\n      }\n    }\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some compression buffers anymore.\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }",
            " 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646 +\n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset)\n          throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    boolean isCompressed = codec != null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<ByteBuffer> toRelease = null;\n    List<IncompleteCb> badEstimates = null;\n    if (isCompressed) {\n      toRelease = !dataReader.isTrackingDiskRanges() ? null : new ArrayList<ByteBuffer>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset,\n              unlockUntilCOffset, current, csd, toRelease, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \" compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) return lastUncompressed; // Nothing to do.\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize);\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        decompressChunk(chunk.originalData, codec, dest);\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      chunk.originalData = null;\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      cacheWrapper.reuseBuffer(chunk.getBuffer());\n    }\n\n    // 5. Release original compressed buffers to zero-copy reader if needed.\n    if (toRelease != null) {\n      assert dataReader.isTrackingDiskRanges();\n      for (ByteBuffer buffer : toRelease) {\n        dataReader.releaseBuffer(buffer);\n      }\n    }\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some compression buffers anymore.\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }"
        ],
        [
            "SettableConfigUpdater::setHiveConfWhiteList(HiveConf)",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 -\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  ",
            "  public static void setHiveConfWhiteList(HiveConf hiveConf) throws HiveAuthzPluginException {\n\n    String whiteListParamsStr = hiveConf\n        .getVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST);\n\n    if(whiteListParamsStr == null || whiteListParamsStr.trim().isEmpty()) {\n      throw new HiveAuthzPluginException(\"Configuration parameter \"\n          + ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST.varname\n          + \" is not iniatialized.\");\n    }\n\n    // append regexes that user wanted to add\n    String whiteListAppend = hiveConf\n        .getVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND);\n    if (whiteListAppend != null && !whiteListAppend.trim().equals(\"\")) {\n      whiteListParamsStr = whiteListParamsStr + \"|\" + whiteListAppend;\n    }\n\n    hiveConf.setModifiableWhiteListRegex(whiteListParamsStr);\n\n    // disallow udfs that can potentially allow untrusted code execution\n    // if admin has already customized this list, honor that\n    String curBlackList = hiveConf.getVar(ConfVars.HIVE_SERVER2_BUILTIN_UDF_BLACKLIST);\n    if (curBlackList == null || curBlackList.trim().isEmpty()) {\n      hiveConf.setVar(ConfVars.HIVE_SERVER2_BUILTIN_UDF_BLACKLIST, \"reflect,reflect2,java_method\");\n    }\n  }",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  ",
            "  public static void setHiveConfWhiteList(HiveConf hiveConf) throws HiveAuthzPluginException {\n\n    String whiteListParamsStr = hiveConf\n        .getVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST);\n\n    if(whiteListParamsStr == null || whiteListParamsStr.trim().isEmpty()) {\n      throw new HiveAuthzPluginException(\"Configuration parameter \"\n          + ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST.varname\n          + \" is not initialized.\");\n    }\n\n    // append regexes that user wanted to add\n    String whiteListAppend = hiveConf\n        .getVar(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND);\n    if (whiteListAppend != null && !whiteListAppend.trim().equals(\"\")) {\n      whiteListParamsStr = whiteListParamsStr + \"|\" + whiteListAppend;\n    }\n\n    hiveConf.setModifiableWhiteListRegex(whiteListParamsStr);\n\n    // disallow udfs that can potentially allow untrusted code execution\n    // if admin has already customized this list, honor that\n    String curBlackList = hiveConf.getVar(ConfVars.HIVE_SERVER2_BUILTIN_UDF_BLACKLIST);\n    if (curBlackList == null || curBlackList.trim().isEmpty()) {\n      hiveConf.setVar(ConfVars.HIVE_SERVER2_BUILTIN_UDF_BLACKLIST, \"reflect,reflect2,java_method\");\n    }\n  }"
        ],
        [
            "MapJoinTableContainerSerDe::create(String,Map)",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316  \n 317  \n 318  \n 319  ",
            "  private MapJoinPersistableTableContainer create(\n      String name, Map<String, String> metaData) throws HiveException {\n    try {\n      @SuppressWarnings(\"unchecked\")\n      Class<? extends MapJoinPersistableTableContainer> clazz =\n          (Class<? extends MapJoinPersistableTableContainer>) JavaUtils.loadClass(name);\n      Constructor<? extends MapJoinPersistableTableContainer> constructor =\n          clazz.getDeclaredConstructor(Map.class);\n      return constructor.newInstance(metaData);\n    } catch (Exception e) {\n      String msg = \"Error while attemping to create table container\" +\n          \" of type: \" + name + \", with metaData: \" + metaData;\n      throw new HiveException(msg, e);\n    }\n  }",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 +\n 316  \n 317  \n 318  \n 319  ",
            "  private MapJoinPersistableTableContainer create(\n      String name, Map<String, String> metaData) throws HiveException {\n    try {\n      @SuppressWarnings(\"unchecked\")\n      Class<? extends MapJoinPersistableTableContainer> clazz =\n          (Class<? extends MapJoinPersistableTableContainer>) JavaUtils.loadClass(name);\n      Constructor<? extends MapJoinPersistableTableContainer> constructor =\n          clazz.getDeclaredConstructor(Map.class);\n      return constructor.newInstance(metaData);\n    } catch (Exception e) {\n      String msg = \"Error while attempting to create table container\" +\n          \" of type: \" + name + \", with metaData: \" + metaData;\n      throw new HiveException(msg, e);\n    }\n  }"
        ],
        [
            "ConstantPropagateProcFactory::evaluateFunction(GenericUDF,List,List)",
            " 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993 -\n 994  \n 995  \n 996  ",
            "  /**\n   * Evaluate UDF\n   *\n   * @param udf UDF object\n   * @param exprs\n   * @param oldExprs\n   * @return null if expression cannot be evaluated (not all parameters are constants). Or evaluated\n   *         ExprNodeConstantDesc if possible.\n   * @throws HiveException\n   */\n  private static ExprNodeDesc evaluateFunction(GenericUDF udf, List<ExprNodeDesc> exprs,\n      List<ExprNodeDesc> oldExprs) {\n    DeferredJavaObject[] arguments = new DeferredJavaObject[exprs.size()];\n    ObjectInspector[] argois = new ObjectInspector[exprs.size()];\n    for (int i = 0; i < exprs.size(); i++) {\n      ExprNodeDesc desc = exprs.get(i);\n      if (desc instanceof ExprNodeConstantDesc) {\n        ExprNodeConstantDesc constant = (ExprNodeConstantDesc) exprs.get(i);\n        if (!constant.getTypeInfo().equals(oldExprs.get(i).getTypeInfo())) {\n          constant = typeCast(constant, oldExprs.get(i).getTypeInfo());\n          if (constant == null) {\n            return null;\n          }\n        }\n        if (constant.getTypeInfo().getCategory() != Category.PRIMITIVE) {\n          // nested complex types cannot be folded cleanly\n          return null;\n        }\n        Object value = constant.getValue();\n        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) constant.getTypeInfo();\n        Object writableValue = null == value ? value :\n            PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti)\n                .getPrimitiveWritableObject(value);\n        arguments[i] = new DeferredJavaObject(writableValue);\n        argois[i] =\n            ObjectInspectorUtils.getConstantObjectInspector(constant.getWritableObjectInspector(),\n                writableValue);\n\n      } else if (desc instanceof ExprNodeGenericFuncDesc) {\n        ExprNodeDesc evaluatedFn = foldExpr((ExprNodeGenericFuncDesc)desc);\n        if (null == evaluatedFn || !(evaluatedFn instanceof ExprNodeConstantDesc)) {\n          return null;\n        }\n        ExprNodeConstantDesc constant = (ExprNodeConstantDesc) evaluatedFn;\n        if (constant.getTypeInfo().getCategory() != Category.PRIMITIVE) {\n          // nested complex types cannot be folded cleanly\n          return null;\n        }\n        Object writableValue = PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(\n          (PrimitiveTypeInfo) constant.getTypeInfo()).getPrimitiveWritableObject(constant.getValue());\n        arguments[i] = new DeferredJavaObject(writableValue);\n        argois[i] = ObjectInspectorUtils.getConstantObjectInspector(constant.getWritableObjectInspector(), writableValue);\n      } else {\n        return null;\n      }\n    }\n\n    try {\n      ObjectInspector oi = udf.initialize(argois);\n      Object o = udf.evaluate(arguments);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(udf.getClass().getName() + \"(\" + exprs + \")=\" + o);\n      }\n      if (o == null) {\n        return new ExprNodeConstantDesc(\n            TypeInfoUtils.getTypeInfoFromObjectInspector(oi), o);\n      }\n      Class<?> clz = o.getClass();\n      if (PrimitiveObjectInspectorUtils.isPrimitiveWritableClass(clz)) {\n        PrimitiveObjectInspector poi = (PrimitiveObjectInspector) oi;\n        TypeInfo typeInfo = poi.getTypeInfo();\n        o = poi.getPrimitiveJavaObject(o);\n        if (typeInfo.getTypeName().contains(serdeConstants.DECIMAL_TYPE_NAME)\n            || typeInfo.getTypeName()\n                .contains(serdeConstants.VARCHAR_TYPE_NAME)\n            || typeInfo.getTypeName().contains(serdeConstants.CHAR_TYPE_NAME)) {\n          return new ExprNodeConstantDesc(typeInfo, o);\n        }\n      } else if (udf instanceof GenericUDFStruct\n          && oi instanceof StandardConstantStructObjectInspector) {\n        // do not fold named_struct, only struct()\n        ConstantObjectInspector coi = (ConstantObjectInspector) oi;\n        TypeInfo structType = TypeInfoUtils.getTypeInfoFromObjectInspector(coi);\n        return new ExprNodeConstantDesc(structType,\n            ObjectInspectorUtils.copyToStandardJavaObject(o, coi));\n      } else if (!PrimitiveObjectInspectorUtils.isPrimitiveJavaClass(clz)) {\n        if (LOG.isErrorEnabled()) {\n          LOG.error(\"Unable to evaluate \" + udf\n              + \". Return value unrecoginizable.\");\n        }\n        return null;\n      } else {\n        // fall through\n      }\n      String constStr = null;\n      if (arguments.length == 1 && FunctionRegistry.isOpCast(udf)) {\n        // remember original string representation of constant.\n        constStr = arguments[0].get().toString();\n      }\n      return new ExprNodeConstantDesc(o).setFoldedFromVal(constStr);\n    } catch (HiveException e) {\n      LOG.error(\"Evaluation function \" + udf.getClass()\n          + \" failed in Constant Propagatation Optimizer.\");\n      throw new RuntimeException(e);\n    }\n  }",
            " 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993 +\n 994  \n 995  \n 996  ",
            "  /**\n   * Evaluate UDF\n   *\n   * @param udf UDF object\n   * @param exprs\n   * @param oldExprs\n   * @return null if expression cannot be evaluated (not all parameters are constants). Or evaluated\n   *         ExprNodeConstantDesc if possible.\n   * @throws HiveException\n   */\n  private static ExprNodeDesc evaluateFunction(GenericUDF udf, List<ExprNodeDesc> exprs,\n      List<ExprNodeDesc> oldExprs) {\n    DeferredJavaObject[] arguments = new DeferredJavaObject[exprs.size()];\n    ObjectInspector[] argois = new ObjectInspector[exprs.size()];\n    for (int i = 0; i < exprs.size(); i++) {\n      ExprNodeDesc desc = exprs.get(i);\n      if (desc instanceof ExprNodeConstantDesc) {\n        ExprNodeConstantDesc constant = (ExprNodeConstantDesc) exprs.get(i);\n        if (!constant.getTypeInfo().equals(oldExprs.get(i).getTypeInfo())) {\n          constant = typeCast(constant, oldExprs.get(i).getTypeInfo());\n          if (constant == null) {\n            return null;\n          }\n        }\n        if (constant.getTypeInfo().getCategory() != Category.PRIMITIVE) {\n          // nested complex types cannot be folded cleanly\n          return null;\n        }\n        Object value = constant.getValue();\n        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) constant.getTypeInfo();\n        Object writableValue = null == value ? value :\n            PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti)\n                .getPrimitiveWritableObject(value);\n        arguments[i] = new DeferredJavaObject(writableValue);\n        argois[i] =\n            ObjectInspectorUtils.getConstantObjectInspector(constant.getWritableObjectInspector(),\n                writableValue);\n\n      } else if (desc instanceof ExprNodeGenericFuncDesc) {\n        ExprNodeDesc evaluatedFn = foldExpr((ExprNodeGenericFuncDesc)desc);\n        if (null == evaluatedFn || !(evaluatedFn instanceof ExprNodeConstantDesc)) {\n          return null;\n        }\n        ExprNodeConstantDesc constant = (ExprNodeConstantDesc) evaluatedFn;\n        if (constant.getTypeInfo().getCategory() != Category.PRIMITIVE) {\n          // nested complex types cannot be folded cleanly\n          return null;\n        }\n        Object writableValue = PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(\n          (PrimitiveTypeInfo) constant.getTypeInfo()).getPrimitiveWritableObject(constant.getValue());\n        arguments[i] = new DeferredJavaObject(writableValue);\n        argois[i] = ObjectInspectorUtils.getConstantObjectInspector(constant.getWritableObjectInspector(), writableValue);\n      } else {\n        return null;\n      }\n    }\n\n    try {\n      ObjectInspector oi = udf.initialize(argois);\n      Object o = udf.evaluate(arguments);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(udf.getClass().getName() + \"(\" + exprs + \")=\" + o);\n      }\n      if (o == null) {\n        return new ExprNodeConstantDesc(\n            TypeInfoUtils.getTypeInfoFromObjectInspector(oi), o);\n      }\n      Class<?> clz = o.getClass();\n      if (PrimitiveObjectInspectorUtils.isPrimitiveWritableClass(clz)) {\n        PrimitiveObjectInspector poi = (PrimitiveObjectInspector) oi;\n        TypeInfo typeInfo = poi.getTypeInfo();\n        o = poi.getPrimitiveJavaObject(o);\n        if (typeInfo.getTypeName().contains(serdeConstants.DECIMAL_TYPE_NAME)\n            || typeInfo.getTypeName()\n                .contains(serdeConstants.VARCHAR_TYPE_NAME)\n            || typeInfo.getTypeName().contains(serdeConstants.CHAR_TYPE_NAME)) {\n          return new ExprNodeConstantDesc(typeInfo, o);\n        }\n      } else if (udf instanceof GenericUDFStruct\n          && oi instanceof StandardConstantStructObjectInspector) {\n        // do not fold named_struct, only struct()\n        ConstantObjectInspector coi = (ConstantObjectInspector) oi;\n        TypeInfo structType = TypeInfoUtils.getTypeInfoFromObjectInspector(coi);\n        return new ExprNodeConstantDesc(structType,\n            ObjectInspectorUtils.copyToStandardJavaObject(o, coi));\n      } else if (!PrimitiveObjectInspectorUtils.isPrimitiveJavaClass(clz)) {\n        if (LOG.isErrorEnabled()) {\n          LOG.error(\"Unable to evaluate \" + udf\n              + \". Return value unrecoginizable.\");\n        }\n        return null;\n      } else {\n        // fall through\n      }\n      String constStr = null;\n      if (arguments.length == 1 && FunctionRegistry.isOpCast(udf)) {\n        // remember original string representation of constant.\n        constStr = arguments[0].get().toString();\n      }\n      return new ExprNodeConstantDesc(o).setFoldedFromVal(constStr);\n    } catch (HiveException e) {\n      LOG.error(\"Evaluation function \" + udf.getClass()\n          + \" failed in Constant Propagation Optimizer.\");\n      throw new RuntimeException(e);\n    }\n  }"
        ],
        [
            "Utilities::createTempDir(String)",
            "3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382 -\n3383  \n3384  ",
            "  /**\n   * Create a temp dir in specified baseDir\n   * This can go away once hive moves to support only JDK 7\n   *  and can use Files.createTempDirectory\n   *  Guava Files.createTempDir() does not take a base dir\n   * @param baseDir - directory under which new temp dir will be created\n   * @return File object for new temp dir\n   */\n  public static File createTempDir(String baseDir){\n    //try creating the temp dir MAX_ATTEMPTS times\n    final int MAX_ATTEMPS = 30;\n    for(int i = 0; i < MAX_ATTEMPS; i++){\n      //pick a random file name\n      String tempDirName = \"tmp_\" + ((int)(100000 * Math.random()));\n\n      //return if dir could successfully be created with that file name\n      File tempDir = new File(baseDir, tempDirName);\n      if(tempDir.mkdir()){\n        return tempDir;\n      }\n    }\n    throw new IllegalStateException(\"Failed to create a temp dir under \"\n    + baseDir + \" Giving up after \" + MAX_ATTEMPS + \" attemps\");\n\n  }",
            "3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382 +\n3383  \n3384  ",
            "  /**\n   * Create a temp dir in specified baseDir\n   * This can go away once hive moves to support only JDK 7\n   *  and can use Files.createTempDirectory\n   *  Guava Files.createTempDir() does not take a base dir\n   * @param baseDir - directory under which new temp dir will be created\n   * @return File object for new temp dir\n   */\n  public static File createTempDir(String baseDir){\n    //try creating the temp dir MAX_ATTEMPTS times\n    final int MAX_ATTEMPS = 30;\n    for(int i = 0; i < MAX_ATTEMPS; i++){\n      //pick a random file name\n      String tempDirName = \"tmp_\" + ((int)(100000 * Math.random()));\n\n      //return if dir could successfully be created with that file name\n      File tempDir = new File(baseDir, tempDirName);\n      if(tempDir.mkdir()){\n        return tempDir;\n      }\n    }\n    throw new IllegalStateException(\"Failed to create a temp dir under \"\n    + baseDir + \" Giving up after \" + MAX_ATTEMPS + \" attempts\");\n\n  }"
        ],
        [
            "ReduceSinkOperator::process(Object,int)",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 -\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Exptected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        if (buckColIdxInKeyForAcid != -1) {\n          cachedKeys[0][buckColIdxInKeyForAcid] = new Text(String.valueOf(bucketNumber));\n        }\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void process(Object row, int tag) throws HiveException {\n    try {\n      ObjectInspector rowInspector = inputObjInspectors[tag];\n      if (firstRow) {\n        firstRow = false;\n        // TODO: this is fishy - we init object inspectors based on first tag. We\n        //       should either init for each tag, or if rowInspector doesn't really\n        //       matter, then we can create this in ctor and get rid of firstRow.\n        if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n            conf.getWriteType() == AcidUtils.Operation.DELETE) {\n          assert rowInspector instanceof StructObjectInspector :\n              \"Expected rowInspector to be instance of StructObjectInspector but it is a \" +\n                  rowInspector.getClass().getName();\n          acidRowInspector = (StructObjectInspector)rowInspector;\n          // The record identifier is always in the first column\n          recIdField = acidRowInspector.getAllStructFieldRefs().get(0);\n          recIdInspector = (StructObjectInspector)recIdField.getFieldObjectInspector();\n          // The bucket field is in the second position\n          bucketField = recIdInspector.getAllStructFieldRefs().get(1);\n          bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();\n        }\n\n        if (isLogInfoEnabled) {\n          LOG.info(\"keys are \" + conf.getOutputKeyColumnNames() + \" num distributions: \" +\n              conf.getNumDistributionKeys());\n        }\n        keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,\n            distinctColIndices,\n            conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);\n        valueObjectInspector = initEvaluatorsAndReturnStruct(valueEval,\n            conf.getOutputValueColumnNames(), rowInspector);\n        partitionObjectInspectors = initEvaluators(partitionEval, rowInspector);\n        if (bucketEval != null) {\n          bucketObjectInspectors = initEvaluators(bucketEval, rowInspector);\n        }\n        int numKeys = numDistinctExprs > 0 ? numDistinctExprs : 1;\n        int keyLen = numDistinctExprs > 0 ? numDistributionKeys + 1 : numDistributionKeys;\n        cachedKeys = new Object[numKeys][keyLen];\n        cachedValues = new Object[valueEval.length];\n      }\n\n      // Determine distKeyLength (w/o distincts), and then add the first if present.\n      populateCachedDistributionKeys(row, 0);\n\n      // replace bucketing columns with hashcode % numBuckets\n      int bucketNumber = -1;\n      if (bucketEval != null) {\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));\n      } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||\n          conf.getWriteType() == AcidUtils.Operation.DELETE) {\n        // In the non-partitioned case we still want to compute the bucket number for updates and\n        // deletes.\n        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());\n        if (buckColIdxInKeyForAcid != -1) {\n          cachedKeys[0][buckColIdxInKeyForAcid] = new Text(String.valueOf(bucketNumber));\n        }\n      }\n\n      HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);\n      int distKeyLength = firstKey.getDistKeyLength();\n      if (numDistinctExprs > 0) {\n        populateCachedDistinctKeys(row, 0);\n        firstKey = toHiveKey(cachedKeys[0], tag, distKeyLength);\n      }\n\n      final int hashCode;\n\n      // distKeyLength doesn't include tag, but includes buckNum in cachedKeys[0]\n      if (useUniformHash && partitionEval.length > 0) {\n        hashCode = computeMurmurHash(firstKey);\n      } else {\n        hashCode = computeHashCode(row, bucketNumber);\n      }\n\n      firstKey.setHashCode(hashCode);\n\n      /*\n       * in case of TopN for windowing, we need to distinguish between rows with\n       * null partition keys and rows with value 0 for partition keys.\n       */\n      boolean partKeyNull = conf.isPTFReduceSink() && partitionKeysAreNull(row);\n\n      // Try to store the first key.\n      // if TopNHashes aren't active, always forward\n      // if TopNHashes are active, proceed if not already excluded (i.e order by limit)\n      final int firstIndex =\n          (reducerHash != null) ? reducerHash.tryStoreKey(firstKey, partKeyNull) : TopNHash.FORWARD;\n      if (firstIndex == TopNHash.EXCLUDE) return; // Nothing to do.\n      // Compute value and hashcode - we'd either store or forward them.\n      BytesWritable value = makeValueWritable(row);\n\n      if (firstIndex == TopNHash.FORWARD) {\n        collect(firstKey, value);\n      } else {\n        // invariant: reducerHash != null\n        assert firstIndex >= 0;\n        reducerHash.storeValue(firstIndex, firstKey.hashCode(), value, false);\n      }\n\n      // All other distinct keys will just be forwarded. This could be optimized...\n      for (int i = 1; i < numDistinctExprs; i++) {\n        System.arraycopy(cachedKeys[0], 0, cachedKeys[i], 0, numDistributionKeys);\n        populateCachedDistinctKeys(row, i);\n        HiveKey hiveKey = toHiveKey(cachedKeys[i], tag, distKeyLength);\n        hiveKey.setHashCode(hashCode);\n        collect(hiveKey, value);\n      }\n    } catch (HiveException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "SkewJoinHandler::handleSkew(int)",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 -\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "  public void handleSkew(int tag) throws HiveException {\n\n    if (joinOp.newGroupStarted || tag != currTag) {\n      rowNumber = 0;\n      currTag = tag;\n    }\n\n    if (joinOp.newGroupStarted) {\n      currBigKeyTag = -1;\n      joinOp.newGroupStarted = false;\n      dummyKey = (List<Object>) joinOp.getGroupKeyObject();\n      skewKeyInCurrentGroup = false;\n\n      for (int i = 0; i < numAliases; i++) {\n        RowContainer<ArrayList<Object>> rc = (RowContainer)joinOp.storage[i];\n        if (rc != null) {\n          rc.setKeyObject(dummyKey);\n        }\n      }\n    }\n\n    rowNumber++;\n    if (currBigKeyTag == -1 && (tag < numAliases - 1)\n        && rowNumber >= skewKeyDefinition) {\n      // the first time we see a big key. If this key is not in the last\n      // table (the last table can always be streamed), we define that we get\n      // a skew key now.\n      currBigKeyTag = tag;\n      updateSkewJoinJobCounter(tag);\n      // right now we assume that the group by is an ArrayList object. It may\n      // change in future.\n      if (!(dummyKey instanceof List)) {\n        throw new RuntimeException(\"Bug in handle skew key in a seperate job.\");\n      }\n\n      skewKeyInCurrentGroup = true;\n      bigKeysExistingMap.put(Byte.valueOf((byte) currBigKeyTag), Boolean.TRUE);\n    }\n  }",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "  public void handleSkew(int tag) throws HiveException {\n\n    if (joinOp.newGroupStarted || tag != currTag) {\n      rowNumber = 0;\n      currTag = tag;\n    }\n\n    if (joinOp.newGroupStarted) {\n      currBigKeyTag = -1;\n      joinOp.newGroupStarted = false;\n      dummyKey = (List<Object>) joinOp.getGroupKeyObject();\n      skewKeyInCurrentGroup = false;\n\n      for (int i = 0; i < numAliases; i++) {\n        RowContainer<ArrayList<Object>> rc = (RowContainer)joinOp.storage[i];\n        if (rc != null) {\n          rc.setKeyObject(dummyKey);\n        }\n      }\n    }\n\n    rowNumber++;\n    if (currBigKeyTag == -1 && (tag < numAliases - 1)\n        && rowNumber >= skewKeyDefinition) {\n      // the first time we see a big key. If this key is not in the last\n      // table (the last table can always be streamed), we define that we get\n      // a skew key now.\n      currBigKeyTag = tag;\n      updateSkewJoinJobCounter(tag);\n      // right now we assume that the group by is an ArrayList object. It may\n      // change in future.\n      if (!(dummyKey instanceof List)) {\n        throw new RuntimeException(\"Bug in handle skew key in a separate job.\");\n      }\n\n      skewKeyInCurrentGroup = true;\n      bigKeysExistingMap.put(Byte.valueOf((byte) currBigKeyTag), Boolean.TRUE);\n    }\n  }"
        ],
        [
            "QueryPlanTreeTransformation::applyCorrelation(ParseContext,CorrelationNodeProcCtx,IntraQueryCorrelation)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232 -\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  ",
            "  /**\n   * Based on the correlation, we transform the query plan tree (operator tree).\n   * In here, we first create DemuxOperator and all bottom ReduceSinkOperators\n   * (bottom means near TableScanOperaotr) in the correlation will be be\n   * the parents of the DemuxOperaotr. We also reassign tags to those\n   * ReduceSinkOperators. Then, we use MuxOperators to replace ReduceSinkOperators\n   * which are not bottom ones in this correlation.\n   * Example: The original operator tree is ...\n   *      JOIN2\n   *      /    \\\n   *     RS4   RS5\n   *    /        \\\n   *   GBY1     JOIN1\n   *    |       /    \\\n   *   RS1     RS2   RS3\n   * If GBY1, JOIN1, and JOIN2 can be executed in the same reducer\n   * (optimized by Correlation Optimizer).\n   * The new operator tree will be ...\n   *      JOIN2\n   *        |\n   *       MUX\n   *      /   \\\n   *    GBY1  JOIN1\n   *      \\    /\n   *       DEMUX\n   *      /  |  \\\n   *     /   |   \\\n   *    /    |    \\\n   *   RS1   RS2   RS3\n   * @param pCtx\n   * @param corrCtx\n   * @param correlation\n   * @throws SemanticException\n   */\n  protected static void applyCorrelation(\n      ParseContext pCtx,\n      CorrelationNodeProcCtx corrCtx,\n      IntraQueryCorrelation correlation)\n      throws SemanticException {\n\n    final List<ReduceSinkOperator> bottomReduceSinkOperators =\n        correlation.getBottomReduceSinkOperators();\n    final int numReducers = correlation.getNumReducers();\n    List<Operator<? extends OperatorDesc>> childrenOfDemux =\n        new ArrayList<Operator<? extends OperatorDesc>>();\n    List<Operator<? extends OperatorDesc>> parentRSsOfDemux =\n        new ArrayList<Operator<? extends OperatorDesc>>();\n    Map<Integer, Integer> childIndexToOriginalNumParents =\n        new HashMap<Integer, Integer>();\n    List<TableDesc> keysSerializeInfos = new ArrayList<TableDesc>();\n    List<TableDesc> valuessSerializeInfos = new ArrayList<TableDesc>();\n    Map<ReduceSinkOperator, Integer> bottomRSToNewTag =\n        new HashMap<ReduceSinkOperator, Integer>();\n    int newTag = 0;\n    CompilationOpContext opCtx = null;\n    for (ReduceSinkOperator rsop: bottomReduceSinkOperators) {\n      if (opCtx == null) {\n        opCtx = rsop.getCompilationOpContext();\n      }\n      rsop.getConf().setNumReducers(numReducers);\n      bottomRSToNewTag.put(rsop, newTag);\n      parentRSsOfDemux.add(rsop);\n      keysSerializeInfos.add(rsop.getConf().getKeySerializeInfo());\n      valuessSerializeInfos.add(rsop.getConf().getValueSerializeInfo());\n      Operator<? extends OperatorDesc> child = CorrelationUtilities.getSingleChild(rsop, true);\n      if (!childrenOfDemux.contains(child)) {\n        childrenOfDemux.add(child);\n        int childIndex = childrenOfDemux.size() - 1;\n        childIndexToOriginalNumParents.put(childIndex, child.getNumParent());\n      }\n      newTag++;\n    }\n\n    for (ReduceSinkOperator rsop: bottomReduceSinkOperators) {\n      setNewTag(correlation, childrenOfDemux, rsop, bottomRSToNewTag);\n    }\n\n    // Create the DemuxOperaotr\n    DemuxDesc demuxDesc =\n        new DemuxDesc(\n            correlation.getNewTagToOldTag(),\n            correlation.getNewTagToChildIndex(),\n            childIndexToOriginalNumParents,\n            keysSerializeInfos,\n            valuessSerializeInfos);\n    Operator<? extends OperatorDesc> demuxOp = OperatorFactory.get(opCtx, demuxDesc);\n    demuxOp.setChildOperators(childrenOfDemux);\n    demuxOp.setParentOperators(parentRSsOfDemux);\n    for (Operator<? extends OperatorDesc> child: childrenOfDemux) {\n      List<Operator<? extends OperatorDesc>> parentsWithMultipleDemux =\n          new ArrayList<Operator<? extends OperatorDesc>>();\n      boolean hasBottomReduceSinkOperators = false;\n      boolean hasNonBottomReduceSinkOperators = false;\n      for (int i = 0; i < child.getParentOperators().size(); i++) {\n        Operator<? extends OperatorDesc> p = child.getParentOperators().get(i);\n        assert p instanceof ReduceSinkOperator;\n        ReduceSinkOperator rsop = (ReduceSinkOperator)p;\n        if (bottomReduceSinkOperators.contains(rsop)) {\n          hasBottomReduceSinkOperators = true;\n          parentsWithMultipleDemux.add(demuxOp);\n        } else {\n          hasNonBottomReduceSinkOperators = true;\n          parentsWithMultipleDemux.add(rsop);\n        }\n      }\n      if (hasBottomReduceSinkOperators && hasNonBottomReduceSinkOperators) {\n        child.setParentOperators(parentsWithMultipleDemux);\n      } else {\n        child.setParentOperators(Utilities.makeList(demuxOp));\n      }\n    }\n    for (Operator<? extends OperatorDesc> parent: parentRSsOfDemux) {\n      parent.setChildOperators(Utilities.makeList(demuxOp));\n    }\n\n    // replace all ReduceSinkOperators which are not at the bottom of\n    // this correlation to MuxOperators\n    Set<ReduceSinkOperator> handledRSs = new HashSet<ReduceSinkOperator>();\n    for (ReduceSinkOperator rsop : correlation.getAllReduceSinkOperators()) {\n      if (!bottomReduceSinkOperators.contains(rsop)) {\n        if (handledRSs.contains(rsop)) {\n          continue;\n        }\n        Operator<? extends OperatorDesc> childOP =\n            CorrelationUtilities.getSingleChild(rsop, true);\n        if (childOP instanceof GroupByOperator) {\n          CorrelationUtilities.removeReduceSinkForGroupBy(\n              rsop, (GroupByOperator)childOP, pCtx, corrCtx);\n          List<Operator<? extends OperatorDesc>> parentsOfMux =\n              new ArrayList<Operator<? extends OperatorDesc>>();\n          Operator<? extends OperatorDesc> parentOp =\n              CorrelationUtilities.getSingleParent(childOP, true);\n          parentsOfMux.add(parentOp);\n          Operator<? extends OperatorDesc> mux = OperatorFactory.get(\n              childOP.getCompilationOpContext(), new MuxDesc(parentsOfMux));\n          mux.setChildOperators(Utilities.makeList(childOP));\n          mux.setParentOperators(parentsOfMux);\n          childOP.setParentOperators(Utilities.makeList(mux));\n          parentOp.setChildOperators(Utilities.makeList(mux));\n        } else {\n          List<Operator<? extends OperatorDesc>> parentsOfMux =\n              new ArrayList<Operator<? extends OperatorDesc>>();\n          List<Operator<? extends OperatorDesc>> siblingOPs =\n              CorrelationUtilities.findSiblingOperators(rsop);\n          for (Operator<? extends OperatorDesc> op: siblingOPs) {\n            if (op instanceof DemuxOperator) {\n              parentsOfMux.add(op);\n            } else if (op instanceof ReduceSinkOperator){\n              GroupByOperator pGBYm =\n                  CorrelationUtilities.getSingleParent(op, GroupByOperator.class);\n              if (pGBYm != null && pGBYm.getConf().getMode() == GroupByDesc.Mode.HASH) {\n                // We get a semi join at here.\n                // This map-side GroupByOperator needs to be removed\n                CorrelationUtilities.removeOperator(\n                    pGBYm, op, CorrelationUtilities.getSingleParent(pGBYm, true), pCtx);\n              }\n              handledRSs.add((ReduceSinkOperator)op);\n              parentsOfMux.add(CorrelationUtilities.getSingleParent(op, true));\n            } else {\n              throw new SemanticException(\"An slibing of ReduceSinkOperator is nethier a \" +\n                  \"DemuxOperator nor a ReduceSinkOperator\");\n            }\n          }\n          MuxDesc muxDesc = new MuxDesc(siblingOPs);\n          Operator<? extends OperatorDesc> mux = OperatorFactory.get(\n              rsop.getCompilationOpContext(), muxDesc);\n          mux.setChildOperators(Utilities.makeList(childOP));\n          mux.setParentOperators(parentsOfMux);\n\n          for (Operator<? extends OperatorDesc> op: parentsOfMux) {\n            if (op instanceof DemuxOperator) {\n              // op is a DemuxOperator and it directly connects to childOP.\n              // We will add this MuxOperator between DemuxOperator\n              // and childOP.\n              if (op.getChildOperators().contains(childOP)) {\n                op.replaceChild(childOP, mux);\n              }\n            } else {\n              // op is not a DemuxOperator, so it should have\n              // a single child.\n              op.setChildOperators(Utilities.makeList(mux));\n            }\n          }\n          childOP.setParentOperators(Utilities.makeList(mux));\n        }\n      }\n    }\n    for (ReduceSinkOperator rsop: handledRSs) {\n      rsop.setChildOperators(null);\n      rsop.setParentOperators(null);\n    }\n  }",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232 +\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  ",
            "  /**\n   * Based on the correlation, we transform the query plan tree (operator tree).\n   * In here, we first create DemuxOperator and all bottom ReduceSinkOperators\n   * (bottom means near TableScanOperaotr) in the correlation will be be\n   * the parents of the DemuxOperaotr. We also reassign tags to those\n   * ReduceSinkOperators. Then, we use MuxOperators to replace ReduceSinkOperators\n   * which are not bottom ones in this correlation.\n   * Example: The original operator tree is ...\n   *      JOIN2\n   *      /    \\\n   *     RS4   RS5\n   *    /        \\\n   *   GBY1     JOIN1\n   *    |       /    \\\n   *   RS1     RS2   RS3\n   * If GBY1, JOIN1, and JOIN2 can be executed in the same reducer\n   * (optimized by Correlation Optimizer).\n   * The new operator tree will be ...\n   *      JOIN2\n   *        |\n   *       MUX\n   *      /   \\\n   *    GBY1  JOIN1\n   *      \\    /\n   *       DEMUX\n   *      /  |  \\\n   *     /   |   \\\n   *    /    |    \\\n   *   RS1   RS2   RS3\n   * @param pCtx\n   * @param corrCtx\n   * @param correlation\n   * @throws SemanticException\n   */\n  protected static void applyCorrelation(\n      ParseContext pCtx,\n      CorrelationNodeProcCtx corrCtx,\n      IntraQueryCorrelation correlation)\n      throws SemanticException {\n\n    final List<ReduceSinkOperator> bottomReduceSinkOperators =\n        correlation.getBottomReduceSinkOperators();\n    final int numReducers = correlation.getNumReducers();\n    List<Operator<? extends OperatorDesc>> childrenOfDemux =\n        new ArrayList<Operator<? extends OperatorDesc>>();\n    List<Operator<? extends OperatorDesc>> parentRSsOfDemux =\n        new ArrayList<Operator<? extends OperatorDesc>>();\n    Map<Integer, Integer> childIndexToOriginalNumParents =\n        new HashMap<Integer, Integer>();\n    List<TableDesc> keysSerializeInfos = new ArrayList<TableDesc>();\n    List<TableDesc> valuessSerializeInfos = new ArrayList<TableDesc>();\n    Map<ReduceSinkOperator, Integer> bottomRSToNewTag =\n        new HashMap<ReduceSinkOperator, Integer>();\n    int newTag = 0;\n    CompilationOpContext opCtx = null;\n    for (ReduceSinkOperator rsop: bottomReduceSinkOperators) {\n      if (opCtx == null) {\n        opCtx = rsop.getCompilationOpContext();\n      }\n      rsop.getConf().setNumReducers(numReducers);\n      bottomRSToNewTag.put(rsop, newTag);\n      parentRSsOfDemux.add(rsop);\n      keysSerializeInfos.add(rsop.getConf().getKeySerializeInfo());\n      valuessSerializeInfos.add(rsop.getConf().getValueSerializeInfo());\n      Operator<? extends OperatorDesc> child = CorrelationUtilities.getSingleChild(rsop, true);\n      if (!childrenOfDemux.contains(child)) {\n        childrenOfDemux.add(child);\n        int childIndex = childrenOfDemux.size() - 1;\n        childIndexToOriginalNumParents.put(childIndex, child.getNumParent());\n      }\n      newTag++;\n    }\n\n    for (ReduceSinkOperator rsop: bottomReduceSinkOperators) {\n      setNewTag(correlation, childrenOfDemux, rsop, bottomRSToNewTag);\n    }\n\n    // Create the DemuxOperaotr\n    DemuxDesc demuxDesc =\n        new DemuxDesc(\n            correlation.getNewTagToOldTag(),\n            correlation.getNewTagToChildIndex(),\n            childIndexToOriginalNumParents,\n            keysSerializeInfos,\n            valuessSerializeInfos);\n    Operator<? extends OperatorDesc> demuxOp = OperatorFactory.get(opCtx, demuxDesc);\n    demuxOp.setChildOperators(childrenOfDemux);\n    demuxOp.setParentOperators(parentRSsOfDemux);\n    for (Operator<? extends OperatorDesc> child: childrenOfDemux) {\n      List<Operator<? extends OperatorDesc>> parentsWithMultipleDemux =\n          new ArrayList<Operator<? extends OperatorDesc>>();\n      boolean hasBottomReduceSinkOperators = false;\n      boolean hasNonBottomReduceSinkOperators = false;\n      for (int i = 0; i < child.getParentOperators().size(); i++) {\n        Operator<? extends OperatorDesc> p = child.getParentOperators().get(i);\n        assert p instanceof ReduceSinkOperator;\n        ReduceSinkOperator rsop = (ReduceSinkOperator)p;\n        if (bottomReduceSinkOperators.contains(rsop)) {\n          hasBottomReduceSinkOperators = true;\n          parentsWithMultipleDemux.add(demuxOp);\n        } else {\n          hasNonBottomReduceSinkOperators = true;\n          parentsWithMultipleDemux.add(rsop);\n        }\n      }\n      if (hasBottomReduceSinkOperators && hasNonBottomReduceSinkOperators) {\n        child.setParentOperators(parentsWithMultipleDemux);\n      } else {\n        child.setParentOperators(Utilities.makeList(demuxOp));\n      }\n    }\n    for (Operator<? extends OperatorDesc> parent: parentRSsOfDemux) {\n      parent.setChildOperators(Utilities.makeList(demuxOp));\n    }\n\n    // replace all ReduceSinkOperators which are not at the bottom of\n    // this correlation to MuxOperators\n    Set<ReduceSinkOperator> handledRSs = new HashSet<ReduceSinkOperator>();\n    for (ReduceSinkOperator rsop : correlation.getAllReduceSinkOperators()) {\n      if (!bottomReduceSinkOperators.contains(rsop)) {\n        if (handledRSs.contains(rsop)) {\n          continue;\n        }\n        Operator<? extends OperatorDesc> childOP =\n            CorrelationUtilities.getSingleChild(rsop, true);\n        if (childOP instanceof GroupByOperator) {\n          CorrelationUtilities.removeReduceSinkForGroupBy(\n              rsop, (GroupByOperator)childOP, pCtx, corrCtx);\n          List<Operator<? extends OperatorDesc>> parentsOfMux =\n              new ArrayList<Operator<? extends OperatorDesc>>();\n          Operator<? extends OperatorDesc> parentOp =\n              CorrelationUtilities.getSingleParent(childOP, true);\n          parentsOfMux.add(parentOp);\n          Operator<? extends OperatorDesc> mux = OperatorFactory.get(\n              childOP.getCompilationOpContext(), new MuxDesc(parentsOfMux));\n          mux.setChildOperators(Utilities.makeList(childOP));\n          mux.setParentOperators(parentsOfMux);\n          childOP.setParentOperators(Utilities.makeList(mux));\n          parentOp.setChildOperators(Utilities.makeList(mux));\n        } else {\n          List<Operator<? extends OperatorDesc>> parentsOfMux =\n              new ArrayList<Operator<? extends OperatorDesc>>();\n          List<Operator<? extends OperatorDesc>> siblingOPs =\n              CorrelationUtilities.findSiblingOperators(rsop);\n          for (Operator<? extends OperatorDesc> op: siblingOPs) {\n            if (op instanceof DemuxOperator) {\n              parentsOfMux.add(op);\n            } else if (op instanceof ReduceSinkOperator){\n              GroupByOperator pGBYm =\n                  CorrelationUtilities.getSingleParent(op, GroupByOperator.class);\n              if (pGBYm != null && pGBYm.getConf().getMode() == GroupByDesc.Mode.HASH) {\n                // We get a semi join at here.\n                // This map-side GroupByOperator needs to be removed\n                CorrelationUtilities.removeOperator(\n                    pGBYm, op, CorrelationUtilities.getSingleParent(pGBYm, true), pCtx);\n              }\n              handledRSs.add((ReduceSinkOperator)op);\n              parentsOfMux.add(CorrelationUtilities.getSingleParent(op, true));\n            } else {\n              throw new SemanticException(\"A sibling of ReduceSinkOperator is neither a \" +\n                  \"DemuxOperator nor a ReduceSinkOperator\");\n            }\n          }\n          MuxDesc muxDesc = new MuxDesc(siblingOPs);\n          Operator<? extends OperatorDesc> mux = OperatorFactory.get(\n              rsop.getCompilationOpContext(), muxDesc);\n          mux.setChildOperators(Utilities.makeList(childOP));\n          mux.setParentOperators(parentsOfMux);\n\n          for (Operator<? extends OperatorDesc> op: parentsOfMux) {\n            if (op instanceof DemuxOperator) {\n              // op is a DemuxOperator and it directly connects to childOP.\n              // We will add this MuxOperator between DemuxOperator\n              // and childOP.\n              if (op.getChildOperators().contains(childOP)) {\n                op.replaceChild(childOP, mux);\n              }\n            } else {\n              // op is not a DemuxOperator, so it should have\n              // a single child.\n              op.setChildOperators(Utilities.makeList(mux));\n            }\n          }\n          childOP.setParentOperators(Utilities.makeList(mux));\n        }\n      }\n    }\n    for (ReduceSinkOperator rsop: handledRSs) {\n      rsop.setChildOperators(null);\n      rsop.setParentOperators(null);\n    }\n  }"
        ]
    ],
    "7299c080f3619a858e56b3826b4f91c0bcf18c6b": [
        [
            "QBSubQuery::ConjunctAnalyzer::resolveDot(ASTNode)",
            " 326  \n 327  \n 328  \n 329  \n 330 -\n 331 -\n 332 -\n 333 -\n 334 -\n 335 -\n 336 -\n 337 -\n 338  \n 339  \n 340  \n 341  ",
            "    protected ColumnInfo resolveDot(ASTNode node) {\n      try {\n        TypeCheckCtx tcCtx = new TypeCheckCtx(parentQueryRR);\n        String str = BaseSemanticAnalyzer.unescapeIdentifier(node.getChild(1).getText());\n        ExprNodeDesc idDesc = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo,\n                str.toLowerCase());\n         ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc)\n             defaultExprProcessor.process(node, stack, tcCtx, (Object) null, idDesc);\n         if ( colDesc != null ) {\n           String[] qualName = parentQueryRR.reverseLookup(colDesc.getColumn());\n           return parentQueryRR.get(qualName[0], qualName[1]);\n         }\n      } catch(SemanticException se) {\n      }\n      return null;\n    }",
            " 326  \n 327  \n 328  \n 329  \n 330 +\n 331 +\n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337  \n 338  \n 339  \n 340  ",
            "    protected ColumnInfo resolveDot(ASTNode node) {\n      try {\n        TypeCheckCtx tcCtx = new TypeCheckCtx(parentQueryRR);\n        String str = BaseSemanticAnalyzer.unescapeIdentifier(node.getChild(1).getText());\n        ExprNodeDesc idDesc = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, str.toLowerCase());\n        Object desc = defaultExprProcessor.process(node, stack, tcCtx, (Object) null, idDesc);\n        if (desc != null && desc instanceof ExprNodeColumnDesc) {\n          ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc) desc;\n          String[] qualName = parentQueryRR.reverseLookup(colDesc.getColumn());\n          return parentQueryRR.get(qualName[0], qualName[1]);\n        }\n      } catch(SemanticException se) {\n      }\n      return null;\n    }"
        ]
    ],
    "b4ac43db41e5af0fb867a3a5139c4dff2e36e865": [
        [
            "VectorExtractRow::extractRowColumn(VectorizedRowBatch,int,int)",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 -\n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  ",
            "  /**\n   * Extract a row's column object from the ColumnVector at batchIndex in the VectorizedRowBatch.\n   *\n   * @param batch\n   * @param batchIndex\n   * @param logicalColumnIndex\n   * @return\n   */\n  public Object extractRowColumn(VectorizedRowBatch batch, int batchIndex, int logicalColumnIndex) {\n    final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];\n    ColumnVector colVector = batch.cols[projectionColumnNum];\n    if (colVector == null) {\n      // The planner will not include unneeded columns for reading but other parts of execution\n      // may ask for them..\n      return null;\n    }\n    int adjustedIndex = (colVector.isRepeating ? 0 : batchIndex);\n    if (!colVector.noNulls && colVector.isNull[adjustedIndex]) {\n      return null;\n    }\n\n    Category category = categories[logicalColumnIndex];\n    switch (category) {\n    case PRIMITIVE:\n      {\n        Writable primitiveWritable =\n            primitiveWritables[logicalColumnIndex];\n        PrimitiveCategory primitiveCategory = primitiveCategories[logicalColumnIndex];\n        switch (primitiveCategory) {\n        case VOID:\n          return null;\n        case BOOLEAN:\n          ((BooleanWritable) primitiveWritable).set(\n              ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex] == 0 ?\n                  false : true);\n          return primitiveWritable;\n        case BYTE:\n          ((ByteWritable) primitiveWritable).set(\n              (byte) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case SHORT:\n          ((ShortWritable) primitiveWritable).set(\n              (short) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INT:\n          ((IntWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case LONG:\n          ((LongWritable) primitiveWritable).set(\n              ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case TIMESTAMP:\n          ((TimestampWritable) primitiveWritable).set(\n              ((TimestampColumnVector) batch.cols[projectionColumnNum]).asScratchTimestamp(adjustedIndex));\n          return primitiveWritable;\n        case DATE:\n          ((DateWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case FLOAT:\n          ((FloatWritable) primitiveWritable).set(\n              (float) ((DoubleColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case DOUBLE:\n          ((DoubleWritable) primitiveWritable).set(\n              ((DoubleColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case BINARY:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              LOG.info(\"null binary entry: batchIndex \" + batchIndex + \" projection column num \" + projectionColumnNum);\n            }\n\n            BytesWritable bytesWritable = (BytesWritable) primitiveWritable;\n            bytesWritable.set(bytes, start, length);\n            return primitiveWritable;\n          }\n        case STRING:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            // Use org.apache.hadoop.io.Text as our helper to go from byte[] to String.\n            ((Text) primitiveWritable).set(bytes, start, length);\n            return primitiveWritable;\n          }\n        case VARCHAR:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            int adjustedLength = StringExpr.truncate(bytes, start, length,\n                maxLengths[logicalColumnIndex]);\n\n            HiveVarcharWritable hiveVarcharWritable = (HiveVarcharWritable) primitiveWritable;\n            hiveVarcharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8), -1);\n            return primitiveWritable;\n          }\n        case CHAR:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            int adjustedLength = StringExpr.rightTrimAndTruncate(bytes, start, length,\n                maxLengths[logicalColumnIndex]);\n\n            HiveCharWritable hiveCharWritable = (HiveCharWritable) primitiveWritable;\n            hiveCharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8), -1);\n            return primitiveWritable;\n          }\n        case DECIMAL:\n          // The HiveDecimalWritable set method will quickly copy the deserialized decimal writable fields.\n          ((HiveDecimalWritable) primitiveWritable).set(\n              ((DecimalColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INTERVAL_YEAR_MONTH:\n          ((HiveIntervalYearMonthWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INTERVAL_DAY_TIME:\n          ((HiveIntervalDayTimeWritable) primitiveWritable).set(\n              ((IntervalDayTimeColumnVector) batch.cols[projectionColumnNum]).asScratchIntervalDayTime(adjustedIndex));\n          return primitiveWritable;\n        default:\n          throw new RuntimeException(\"Primitive category \" + primitiveCategory.name() +\n              \" not supported\");\n        }\n      }\n    default:\n      throw new RuntimeException(\"Category \" + category.name() + \" not supported\");\n    }\n  }",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 +\n 310 +\n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "  /**\n   * Extract a row's column object from the ColumnVector at batchIndex in the VectorizedRowBatch.\n   *\n   * @param batch\n   * @param batchIndex\n   * @param logicalColumnIndex\n   * @return\n   */\n  public Object extractRowColumn(VectorizedRowBatch batch, int batchIndex, int logicalColumnIndex) {\n    final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];\n    ColumnVector colVector = batch.cols[projectionColumnNum];\n    if (colVector == null) {\n      // The planner will not include unneeded columns for reading but other parts of execution\n      // may ask for them..\n      return null;\n    }\n    int adjustedIndex = (colVector.isRepeating ? 0 : batchIndex);\n    if (!colVector.noNulls && colVector.isNull[adjustedIndex]) {\n      return null;\n    }\n\n    Category category = categories[logicalColumnIndex];\n    switch (category) {\n    case PRIMITIVE:\n      {\n        Writable primitiveWritable =\n            primitiveWritables[logicalColumnIndex];\n        PrimitiveCategory primitiveCategory = primitiveCategories[logicalColumnIndex];\n        switch (primitiveCategory) {\n        case VOID:\n          return null;\n        case BOOLEAN:\n          ((BooleanWritable) primitiveWritable).set(\n              ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex] == 0 ?\n                  false : true);\n          return primitiveWritable;\n        case BYTE:\n          ((ByteWritable) primitiveWritable).set(\n              (byte) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case SHORT:\n          ((ShortWritable) primitiveWritable).set(\n              (short) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INT:\n          ((IntWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case LONG:\n          ((LongWritable) primitiveWritable).set(\n              ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case TIMESTAMP:\n          ((TimestampWritable) primitiveWritable).set(\n              ((TimestampColumnVector) batch.cols[projectionColumnNum]).asScratchTimestamp(adjustedIndex));\n          return primitiveWritable;\n        case DATE:\n          ((DateWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case FLOAT:\n          ((FloatWritable) primitiveWritable).set(\n              (float) ((DoubleColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case DOUBLE:\n          ((DoubleWritable) primitiveWritable).set(\n              ((DoubleColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case BINARY:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              LOG.info(\"null binary entry: batchIndex \" + batchIndex + \" projection column num \" + projectionColumnNum);\n            }\n\n            BytesWritable bytesWritable = (BytesWritable) primitiveWritable;\n            bytesWritable.set(bytes, start, length);\n            return primitiveWritable;\n          }\n        case STRING:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            // Use org.apache.hadoop.io.Text as our helper to go from byte[] to String.\n            ((Text) primitiveWritable).set(bytes, start, length);\n            return primitiveWritable;\n          }\n        case VARCHAR:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            int adjustedLength = StringExpr.truncate(bytes, start, length,\n                maxLengths[logicalColumnIndex]);\n\n            HiveVarcharWritable hiveVarcharWritable = (HiveVarcharWritable) primitiveWritable;\n            hiveVarcharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8), -1);\n            return primitiveWritable;\n          }\n        case CHAR:\n          {\n            BytesColumnVector bytesColVector =\n                ((BytesColumnVector) batch.cols[projectionColumnNum]);\n            byte[] bytes = bytesColVector.vector[adjustedIndex];\n            int start = bytesColVector.start[adjustedIndex];\n            int length = bytesColVector.length[adjustedIndex];\n\n            if (bytes == null) {\n              nullBytesReadError(primitiveCategory, batchIndex, projectionColumnNum);\n            }\n\n            int adjustedLength = StringExpr.rightTrimAndTruncate(bytes, start, length,\n                maxLengths[logicalColumnIndex]);\n\n            HiveCharWritable hiveCharWritable = (HiveCharWritable) primitiveWritable;\n            hiveCharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8),\n                maxLengths[logicalColumnIndex]);\n            return primitiveWritable;\n          }\n        case DECIMAL:\n          // The HiveDecimalWritable set method will quickly copy the deserialized decimal writable fields.\n          ((HiveDecimalWritable) primitiveWritable).set(\n              ((DecimalColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INTERVAL_YEAR_MONTH:\n          ((HiveIntervalYearMonthWritable) primitiveWritable).set(\n              (int) ((LongColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex]);\n          return primitiveWritable;\n        case INTERVAL_DAY_TIME:\n          ((HiveIntervalDayTimeWritable) primitiveWritable).set(\n              ((IntervalDayTimeColumnVector) batch.cols[projectionColumnNum]).asScratchIntervalDayTime(adjustedIndex));\n          return primitiveWritable;\n        default:\n          throw new RuntimeException(\"Primitive category \" + primitiveCategory.name() +\n              \" not supported\");\n        }\n      }\n    default:\n      throw new RuntimeException(\"Category \" + category.name() + \" not supported\");\n    }\n  }"
        ]
    ],
    "566788696d2b3f71a9381da1fa7b29e547162175": [
        [
            "TestJdbcDriver2::testResultSetMetaDataDuplicateColumnNames()",
            "1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807 -\n1808  \n1809  ",
            "  @Test\n  public void testResultSetMetaDataDuplicateColumnNames() throws SQLException {\n    Statement stmt = con.createStatement();\n    ResultSet res =\n        stmt.executeQuery(\"select c1 as c2_1, c2, c1*2 from \" + dataTypeTableName + \" limit 1\");\n    ResultSetMetaData meta = res.getMetaData();\n    ResultSet colRS =\n        con.getMetaData().getColumns(null, null, dataTypeTableName.toLowerCase(), null);\n    assertEquals(3, meta.getColumnCount());\n    assertTrue(colRS.next());\n    assertEquals(\"c2_1\", meta.getColumnName(1));\n    assertTrue(colRS.next());\n    assertEquals(\"c2\", meta.getColumnName(2));\n    assertTrue(colRS.next());\n    assertEquals(\"c2_2\", meta.getColumnName(3));\n    stmt.close();\n  }",
            "1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807 +\n1808  \n1809  ",
            "  @Test\n  public void testResultSetMetaDataDuplicateColumnNames() throws SQLException {\n    Statement stmt = con.createStatement();\n    ResultSet res =\n        stmt.executeQuery(\"select c1 as c2_1, c2, c1*2 from \" + dataTypeTableName + \" limit 1\");\n    ResultSetMetaData meta = res.getMetaData();\n    ResultSet colRS =\n        con.getMetaData().getColumns(null, null, dataTypeTableName.toLowerCase(), null);\n    assertEquals(3, meta.getColumnCount());\n    assertTrue(colRS.next());\n    assertEquals(\"c2_1\", meta.getColumnName(1));\n    assertTrue(colRS.next());\n    assertEquals(\"c2\", meta.getColumnName(2));\n    assertTrue(colRS.next());\n    assertEquals(\"_c2\", meta.getColumnName(3));\n    stmt.close();\n  }"
        ],
        [
            "TestJdbcDriver2::testBuiltInUDFCol()",
            "1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979 -\n1980  \n1981  \n1982  \n1983  \n1984  \n1985  ",
            "  /**\n   * Verify selecting using builtin UDFs\n   * @throws SQLException\n   */\n  @Test\n  public void testBuiltInUDFCol() throws SQLException {\n    Statement stmt = con.createStatement();\n    ResultSet res =\n        stmt.executeQuery(\"select c12, bin(c12) from \" + dataTypeTableName + \" where c1=1\");\n    ResultSetMetaData md = res.getMetaData();\n    assertEquals(md.getColumnCount(), 2); // only one result column\n    assertEquals(md.getColumnLabel(2), \"c1\"); // verify the system generated column name\n    assertTrue(res.next());\n    assertEquals(res.getLong(1), 1);\n    assertEquals(res.getString(2), \"1\");\n    res.close();\n    stmt.close();\n  }",
            "1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979 +\n1980  \n1981  \n1982  \n1983  \n1984  \n1985  ",
            "  /**\n   * Verify selecting using builtin UDFs\n   * @throws SQLException\n   */\n  @Test\n  public void testBuiltInUDFCol() throws SQLException {\n    Statement stmt = con.createStatement();\n    ResultSet res =\n        stmt.executeQuery(\"select c12, bin(c12) from \" + dataTypeTableName + \" where c1=1\");\n    ResultSetMetaData md = res.getMetaData();\n    assertEquals(md.getColumnCount(), 2); // only one result column\n    assertEquals(md.getColumnLabel(2), \"_c1\"); // verify the system generated column name\n    assertTrue(res.next());\n    assertEquals(res.getLong(1), 1);\n    assertEquals(res.getString(2), \"1\");\n    res.close();\n    stmt.close();\n  }"
        ],
        [
            "PlanModifierForASTConv::renameTopLevelSelectInResultSchema(RelNode,Pair,List)",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210 -\n 211 -\n 212 -\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  ",
            "  public static RelNode renameTopLevelSelectInResultSchema(final RelNode rootRel,\n      Pair<RelNode, RelNode> topSelparentPair, List<FieldSchema> resultSchema)\n      throws CalciteSemanticException {\n    RelNode parentOforiginalProjRel = topSelparentPair.getKey();\n    HiveProject originalProjRel = (HiveProject) topSelparentPair.getValue();\n\n    // Assumption: top portion of tree could only be\n    // (limit)?(OB)?(Project)....\n    List<RexNode> rootChildExps = originalProjRel.getChildExps();\n    if (resultSchema.size() != rootChildExps.size()) {\n      // Safeguard against potential issues in CBO RowResolver construction. Disable CBO for now.\n      LOG.error(PlanModifierUtil.generateInvalidSchemaMessage(originalProjRel, resultSchema, 0));\n      throw new CalciteSemanticException(\"Result Schema didn't match Optimized Op Tree Schema\");\n    }\n\n    List<String> newSelAliases = new ArrayList<String>();\n    String colAlias;\n    for (int i = 0; i < rootChildExps.size(); i++) {\n      colAlias = resultSchema.get(i).getName();\n      if (colAlias.startsWith(\"_\")) {\n        colAlias = colAlias.substring(1);\n        colAlias = getNewColAlias(newSelAliases, colAlias);\n      }\n      newSelAliases.add(colAlias);\n    }\n\n    HiveProject replacementProjectRel = HiveProject.create(originalProjRel.getInput(),\n        originalProjRel.getChildExps(), newSelAliases);\n\n    if (rootRel == originalProjRel) {\n      return replacementProjectRel;\n    } else {\n      parentOforiginalProjRel.replaceInput(0, replacementProjectRel);\n      return rootRel;\n    }\n  }",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "  public static RelNode renameTopLevelSelectInResultSchema(final RelNode rootRel,\n      Pair<RelNode, RelNode> topSelparentPair, List<FieldSchema> resultSchema)\n      throws CalciteSemanticException {\n    RelNode parentOforiginalProjRel = topSelparentPair.getKey();\n    HiveProject originalProjRel = (HiveProject) topSelparentPair.getValue();\n\n    // Assumption: top portion of tree could only be\n    // (limit)?(OB)?(Project)....\n    List<RexNode> rootChildExps = originalProjRel.getChildExps();\n    if (resultSchema.size() != rootChildExps.size()) {\n      // Safeguard against potential issues in CBO RowResolver construction. Disable CBO for now.\n      LOG.error(PlanModifierUtil.generateInvalidSchemaMessage(originalProjRel, resultSchema, 0));\n      throw new CalciteSemanticException(\"Result Schema didn't match Optimized Op Tree Schema\");\n    }\n\n    List<String> newSelAliases = new ArrayList<String>();\n    String colAlias;\n    for (int i = 0; i < rootChildExps.size(); i++) {\n      colAlias = resultSchema.get(i).getName();\n      colAlias = getNewColAlias(newSelAliases, colAlias);\n      newSelAliases.add(colAlias);\n    }\n\n    HiveProject replacementProjectRel = HiveProject.create(originalProjRel.getInput(),\n        originalProjRel.getChildExps(), newSelAliases);\n\n    if (rootRel == originalProjRel) {\n      return replacementProjectRel;\n    } else {\n      parentOforiginalProjRel.replaceInput(0, replacementProjectRel);\n      return rootRel;\n    }\n  }"
        ],
        [
            "TestJdbcDriver2::testResultSetMetaData()",
            "1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670 -\n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  ",
            "  @Test\n  public void testResultSetMetaData() throws SQLException {\n    Statement stmt = con.createStatement();\n\n    ResultSet res =\n        stmt.executeQuery(\"select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, \"\n            + \"c1*2, sentences(null, null, null) as b, c17, c18, c20, c21, c22, c23, null as null_val from \"\n            + dataTypeTableName + \" limit 1\");\n    ResultSetMetaData meta = res.getMetaData();\n\n    ResultSet colRS =\n        con.getMetaData().getColumns(null, null, dataTypeTableName.toLowerCase(), null);\n\n    assertEquals(21, meta.getColumnCount());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c1\", meta.getColumnName(1));\n    assertEquals(Types.INTEGER, meta.getColumnType(1));\n    assertEquals(\"int\", meta.getColumnTypeName(1));\n    assertEquals(11, meta.getColumnDisplaySize(1));\n    assertEquals(10, meta.getPrecision(1));\n    assertEquals(0, meta.getScale(1));\n\n    assertEquals(\"c1\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.INTEGER, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"int\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(1), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(1), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c2\", meta.getColumnName(2));\n    assertEquals(\"boolean\", meta.getColumnTypeName(2));\n    assertEquals(Types.BOOLEAN, meta.getColumnType(2));\n    assertEquals(1, meta.getColumnDisplaySize(2));\n    assertEquals(1, meta.getPrecision(2));\n    assertEquals(0, meta.getScale(2));\n\n    assertEquals(\"c2\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.BOOLEAN, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"boolean\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getScale(2), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c3\", meta.getColumnName(3));\n    assertEquals(Types.DOUBLE, meta.getColumnType(3));\n    assertEquals(\"double\", meta.getColumnTypeName(3));\n    assertEquals(25, meta.getColumnDisplaySize(3));\n    assertEquals(15, meta.getPrecision(3));\n    assertEquals(15, meta.getScale(3));\n\n    assertEquals(\"c3\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DOUBLE, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"double\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(3), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(3), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c4\", meta.getColumnName(4));\n    assertEquals(Types.VARCHAR, meta.getColumnType(4));\n    assertEquals(\"string\", meta.getColumnTypeName(4));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));\n    assertEquals(0, meta.getScale(4));\n\n    assertEquals(\"c4\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.VARCHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"string\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(4), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(4), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"a\", meta.getColumnName(5));\n    assertEquals(Types.ARRAY, meta.getColumnType(5));\n    assertEquals(\"array\", meta.getColumnTypeName(5));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));\n    assertEquals(0, meta.getScale(5));\n\n    assertEquals(\"c5\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.ARRAY, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"array<int>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c6\", meta.getColumnName(6));\n    assertEquals(Types.JAVA_OBJECT, meta.getColumnType(6));\n    assertEquals(\"map\", meta.getColumnTypeName(6));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));\n    assertEquals(0, meta.getScale(6));\n\n    assertEquals(\"c6\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.JAVA_OBJECT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"map<int,string>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c7\", meta.getColumnName(7));\n    assertEquals(Types.JAVA_OBJECT, meta.getColumnType(7));\n    assertEquals(\"map\", meta.getColumnTypeName(7));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));\n    assertEquals(0, meta.getScale(7));\n\n    assertEquals(\"c7\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.JAVA_OBJECT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"map<string,string>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c8\", meta.getColumnName(8));\n    assertEquals(Types.STRUCT, meta.getColumnType(8));\n    assertEquals(\"struct\", meta.getColumnTypeName(8));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));\n    assertEquals(0, meta.getScale(8));\n\n    assertEquals(\"c8\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.STRUCT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"struct<r:string,s:int,t:double>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c9\", meta.getColumnName(9));\n    assertEquals(Types.TINYINT, meta.getColumnType(9));\n    assertEquals(\"tinyint\", meta.getColumnTypeName(9));\n    assertEquals(4, meta.getColumnDisplaySize(9));\n    assertEquals(3, meta.getPrecision(9));\n    assertEquals(0, meta.getScale(9));\n\n    assertEquals(\"c9\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.TINYINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"tinyint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(9), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(9), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c10\", meta.getColumnName(10));\n    assertEquals(Types.SMALLINT, meta.getColumnType(10));\n    assertEquals(\"smallint\", meta.getColumnTypeName(10));\n    assertEquals(6, meta.getColumnDisplaySize(10));\n    assertEquals(5, meta.getPrecision(10));\n    assertEquals(0, meta.getScale(10));\n\n    assertEquals(\"c10\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.SMALLINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"smallint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(10), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(10), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c11\", meta.getColumnName(11));\n    assertEquals(Types.FLOAT, meta.getColumnType(11));\n    assertEquals(\"float\", meta.getColumnTypeName(11));\n    assertEquals(24, meta.getColumnDisplaySize(11));\n    assertEquals(7, meta.getPrecision(11));\n    assertEquals(7, meta.getScale(11));\n\n    assertEquals(\"c11\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.FLOAT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"float\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(11), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(11), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c12\", meta.getColumnName(12));\n    assertEquals(Types.BIGINT, meta.getColumnType(12));\n    assertEquals(\"bigint\", meta.getColumnTypeName(12));\n    assertEquals(20, meta.getColumnDisplaySize(12));\n    assertEquals(19, meta.getPrecision(12));\n    assertEquals(0, meta.getScale(12));\n\n    assertEquals(\"c12\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.BIGINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"bigint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(12), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(12), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertEquals(\"c12_1\", meta.getColumnName(13));\n    assertEquals(Types.INTEGER, meta.getColumnType(13));\n    assertEquals(\"int\", meta.getColumnTypeName(13));\n    assertEquals(11, meta.getColumnDisplaySize(13));\n    assertEquals(10, meta.getPrecision(13));\n    assertEquals(0, meta.getScale(13));\n\n    assertEquals(\"b\", meta.getColumnName(14));\n    assertEquals(Types.ARRAY, meta.getColumnType(14));\n    assertEquals(\"array\", meta.getColumnTypeName(14));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));\n    assertEquals(0, meta.getScale(14));\n\n    // Move the result of getColumns() forward to match the columns of the query\n    assertTrue(colRS.next()); // c13\n    assertTrue(colRS.next()); // c14\n    assertTrue(colRS.next()); // c15\n    assertTrue(colRS.next()); // c16\n    assertTrue(colRS.next()); // c17\n\n    assertEquals(\"c17\", meta.getColumnName(15));\n    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));\n    assertEquals(\"timestamp\", meta.getColumnTypeName(15));\n    assertEquals(29, meta.getColumnDisplaySize(15));\n    assertEquals(29, meta.getPrecision(15));\n    assertEquals(9, meta.getScale(15));\n\n    assertEquals(\"c17\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.TIMESTAMP, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"timestamp\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(15), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(15), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c18\", meta.getColumnName(16));\n    assertEquals(Types.DECIMAL, meta.getColumnType(16));\n    assertEquals(\"decimal\", meta.getColumnTypeName(16));\n    assertEquals(18, meta.getColumnDisplaySize(16));\n    assertEquals(16, meta.getPrecision(16));\n    assertEquals(7, meta.getScale(16));\n\n    assertEquals(\"c18\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DECIMAL, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"decimal\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(16), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(16), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next()); // skip c19, since not selected by query\n    assertTrue(colRS.next());\n\n    assertEquals(\"c20\", meta.getColumnName(17));\n    assertEquals(Types.DATE, meta.getColumnType(17));\n    assertEquals(\"date\", meta.getColumnTypeName(17));\n    assertEquals(10, meta.getColumnDisplaySize(17));\n    assertEquals(10, meta.getPrecision(17));\n    assertEquals(0, meta.getScale(17));\n\n    assertEquals(\"c20\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DATE, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"date\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(17), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(17), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c21\", meta.getColumnName(18));\n    assertEquals(Types.VARCHAR, meta.getColumnType(18));\n    assertEquals(\"varchar\", meta.getColumnTypeName(18));\n    // varchar columns should have correct display size/precision\n    assertEquals(20, meta.getColumnDisplaySize(18));\n    assertEquals(20, meta.getPrecision(18));\n    assertEquals(0, meta.getScale(18));\n\n    assertEquals(\"c21\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.VARCHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"varchar\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(18), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(18), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c22\", meta.getColumnName(19));\n    assertEquals(Types.CHAR, meta.getColumnType(19));\n    assertEquals(\"char\", meta.getColumnTypeName(19));\n    // char columns should have correct display size/precision\n    assertEquals(15, meta.getColumnDisplaySize(19));\n    assertEquals(15, meta.getPrecision(19));\n    assertEquals(0, meta.getScale(19));\n\n    assertEquals(\"c22\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.CHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"char\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(19), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(19), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c23\", meta.getColumnName(20));\n    assertEquals(Types.BINARY, meta.getColumnType(20));\n    assertEquals(\"binary\", meta.getColumnTypeName(20));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(20));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(20));\n    assertEquals(0, meta.getScale(20));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"null_val\", meta.getColumnName(21));\n    assertEquals(Types.NULL, meta.getColumnType(21));\n    assertEquals(\"void\", meta.getColumnTypeName(21));\n    assertEquals(4, meta.getColumnDisplaySize(21));\n    assertEquals(0, meta.getPrecision(21));\n    assertEquals(0, meta.getScale(21));\n\n    for (int i = 1; i <= meta.getColumnCount(); i++) {\n      assertFalse(meta.isAutoIncrement(i));\n      assertFalse(meta.isCurrency(i));\n      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));\n    }\n    stmt.close();\n  }",
            "1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670 +\n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  ",
            "  @Test\n  public void testResultSetMetaData() throws SQLException {\n    Statement stmt = con.createStatement();\n\n    ResultSet res =\n        stmt.executeQuery(\"select c1, c2, c3, c4, c5 as a, c6, c7, c8, c9, c10, c11, c12, \"\n            + \"c1*2, sentences(null, null, null) as b, c17, c18, c20, c21, c22, c23, null as null_val from \"\n            + dataTypeTableName + \" limit 1\");\n    ResultSetMetaData meta = res.getMetaData();\n\n    ResultSet colRS =\n        con.getMetaData().getColumns(null, null, dataTypeTableName.toLowerCase(), null);\n\n    assertEquals(21, meta.getColumnCount());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c1\", meta.getColumnName(1));\n    assertEquals(Types.INTEGER, meta.getColumnType(1));\n    assertEquals(\"int\", meta.getColumnTypeName(1));\n    assertEquals(11, meta.getColumnDisplaySize(1));\n    assertEquals(10, meta.getPrecision(1));\n    assertEquals(0, meta.getScale(1));\n\n    assertEquals(\"c1\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.INTEGER, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"int\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(1), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(1), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c2\", meta.getColumnName(2));\n    assertEquals(\"boolean\", meta.getColumnTypeName(2));\n    assertEquals(Types.BOOLEAN, meta.getColumnType(2));\n    assertEquals(1, meta.getColumnDisplaySize(2));\n    assertEquals(1, meta.getPrecision(2));\n    assertEquals(0, meta.getScale(2));\n\n    assertEquals(\"c2\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.BOOLEAN, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"boolean\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getScale(2), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c3\", meta.getColumnName(3));\n    assertEquals(Types.DOUBLE, meta.getColumnType(3));\n    assertEquals(\"double\", meta.getColumnTypeName(3));\n    assertEquals(25, meta.getColumnDisplaySize(3));\n    assertEquals(15, meta.getPrecision(3));\n    assertEquals(15, meta.getScale(3));\n\n    assertEquals(\"c3\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DOUBLE, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"double\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(3), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(3), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c4\", meta.getColumnName(4));\n    assertEquals(Types.VARCHAR, meta.getColumnType(4));\n    assertEquals(\"string\", meta.getColumnTypeName(4));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(4));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(4));\n    assertEquals(0, meta.getScale(4));\n\n    assertEquals(\"c4\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.VARCHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"string\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(4), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(4), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"a\", meta.getColumnName(5));\n    assertEquals(Types.ARRAY, meta.getColumnType(5));\n    assertEquals(\"array\", meta.getColumnTypeName(5));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(5));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(5));\n    assertEquals(0, meta.getScale(5));\n\n    assertEquals(\"c5\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.ARRAY, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"array<int>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c6\", meta.getColumnName(6));\n    assertEquals(Types.JAVA_OBJECT, meta.getColumnType(6));\n    assertEquals(\"map\", meta.getColumnTypeName(6));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(6));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(6));\n    assertEquals(0, meta.getScale(6));\n\n    assertEquals(\"c6\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.JAVA_OBJECT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"map<int,string>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c7\", meta.getColumnName(7));\n    assertEquals(Types.JAVA_OBJECT, meta.getColumnType(7));\n    assertEquals(\"map\", meta.getColumnTypeName(7));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(7));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(7));\n    assertEquals(0, meta.getScale(7));\n\n    assertEquals(\"c7\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.JAVA_OBJECT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"map<string,string>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c8\", meta.getColumnName(8));\n    assertEquals(Types.STRUCT, meta.getColumnType(8));\n    assertEquals(\"struct\", meta.getColumnTypeName(8));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(8));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(8));\n    assertEquals(0, meta.getScale(8));\n\n    assertEquals(\"c8\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.STRUCT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"struct<r:string,s:int,t:double>\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c9\", meta.getColumnName(9));\n    assertEquals(Types.TINYINT, meta.getColumnType(9));\n    assertEquals(\"tinyint\", meta.getColumnTypeName(9));\n    assertEquals(4, meta.getColumnDisplaySize(9));\n    assertEquals(3, meta.getPrecision(9));\n    assertEquals(0, meta.getScale(9));\n\n    assertEquals(\"c9\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.TINYINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"tinyint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(9), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(9), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c10\", meta.getColumnName(10));\n    assertEquals(Types.SMALLINT, meta.getColumnType(10));\n    assertEquals(\"smallint\", meta.getColumnTypeName(10));\n    assertEquals(6, meta.getColumnDisplaySize(10));\n    assertEquals(5, meta.getPrecision(10));\n    assertEquals(0, meta.getScale(10));\n\n    assertEquals(\"c10\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.SMALLINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"smallint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(10), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(10), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c11\", meta.getColumnName(11));\n    assertEquals(Types.FLOAT, meta.getColumnType(11));\n    assertEquals(\"float\", meta.getColumnTypeName(11));\n    assertEquals(24, meta.getColumnDisplaySize(11));\n    assertEquals(7, meta.getPrecision(11));\n    assertEquals(7, meta.getScale(11));\n\n    assertEquals(\"c11\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.FLOAT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"float\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(11), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(11), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c12\", meta.getColumnName(12));\n    assertEquals(Types.BIGINT, meta.getColumnType(12));\n    assertEquals(\"bigint\", meta.getColumnTypeName(12));\n    assertEquals(20, meta.getColumnDisplaySize(12));\n    assertEquals(19, meta.getPrecision(12));\n    assertEquals(0, meta.getScale(12));\n\n    assertEquals(\"c12\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.BIGINT, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"bigint\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(12), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(12), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertEquals(\"_c12\", meta.getColumnName(13));\n    assertEquals(Types.INTEGER, meta.getColumnType(13));\n    assertEquals(\"int\", meta.getColumnTypeName(13));\n    assertEquals(11, meta.getColumnDisplaySize(13));\n    assertEquals(10, meta.getPrecision(13));\n    assertEquals(0, meta.getScale(13));\n\n    assertEquals(\"b\", meta.getColumnName(14));\n    assertEquals(Types.ARRAY, meta.getColumnType(14));\n    assertEquals(\"array\", meta.getColumnTypeName(14));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(14));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(14));\n    assertEquals(0, meta.getScale(14));\n\n    // Move the result of getColumns() forward to match the columns of the query\n    assertTrue(colRS.next()); // c13\n    assertTrue(colRS.next()); // c14\n    assertTrue(colRS.next()); // c15\n    assertTrue(colRS.next()); // c16\n    assertTrue(colRS.next()); // c17\n\n    assertEquals(\"c17\", meta.getColumnName(15));\n    assertEquals(Types.TIMESTAMP, meta.getColumnType(15));\n    assertEquals(\"timestamp\", meta.getColumnTypeName(15));\n    assertEquals(29, meta.getColumnDisplaySize(15));\n    assertEquals(29, meta.getPrecision(15));\n    assertEquals(9, meta.getScale(15));\n\n    assertEquals(\"c17\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.TIMESTAMP, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"timestamp\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(15), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(15), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c18\", meta.getColumnName(16));\n    assertEquals(Types.DECIMAL, meta.getColumnType(16));\n    assertEquals(\"decimal\", meta.getColumnTypeName(16));\n    assertEquals(18, meta.getColumnDisplaySize(16));\n    assertEquals(16, meta.getPrecision(16));\n    assertEquals(7, meta.getScale(16));\n\n    assertEquals(\"c18\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DECIMAL, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"decimal\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(16), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(16), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next()); // skip c19, since not selected by query\n    assertTrue(colRS.next());\n\n    assertEquals(\"c20\", meta.getColumnName(17));\n    assertEquals(Types.DATE, meta.getColumnType(17));\n    assertEquals(\"date\", meta.getColumnTypeName(17));\n    assertEquals(10, meta.getColumnDisplaySize(17));\n    assertEquals(10, meta.getPrecision(17));\n    assertEquals(0, meta.getScale(17));\n\n    assertEquals(\"c20\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.DATE, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"date\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(17), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(17), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c21\", meta.getColumnName(18));\n    assertEquals(Types.VARCHAR, meta.getColumnType(18));\n    assertEquals(\"varchar\", meta.getColumnTypeName(18));\n    // varchar columns should have correct display size/precision\n    assertEquals(20, meta.getColumnDisplaySize(18));\n    assertEquals(20, meta.getPrecision(18));\n    assertEquals(0, meta.getScale(18));\n\n    assertEquals(\"c21\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.VARCHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"varchar\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(18), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(18), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c22\", meta.getColumnName(19));\n    assertEquals(Types.CHAR, meta.getColumnType(19));\n    assertEquals(\"char\", meta.getColumnTypeName(19));\n    // char columns should have correct display size/precision\n    assertEquals(15, meta.getColumnDisplaySize(19));\n    assertEquals(15, meta.getPrecision(19));\n    assertEquals(0, meta.getScale(19));\n\n    assertEquals(\"c22\", colRS.getString(\"COLUMN_NAME\"));\n    assertEquals(Types.CHAR, colRS.getInt(\"DATA_TYPE\"));\n    assertEquals(\"char\", colRS.getString(\"TYPE_NAME\").toLowerCase());\n    assertEquals(meta.getPrecision(19), colRS.getInt(\"COLUMN_SIZE\"));\n    assertEquals(meta.getScale(19), colRS.getInt(\"DECIMAL_DIGITS\"));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"c23\", meta.getColumnName(20));\n    assertEquals(Types.BINARY, meta.getColumnType(20));\n    assertEquals(\"binary\", meta.getColumnTypeName(20));\n    assertEquals(Integer.MAX_VALUE, meta.getColumnDisplaySize(20));\n    assertEquals(Integer.MAX_VALUE, meta.getPrecision(20));\n    assertEquals(0, meta.getScale(20));\n\n    assertTrue(colRS.next());\n\n    assertEquals(\"null_val\", meta.getColumnName(21));\n    assertEquals(Types.NULL, meta.getColumnType(21));\n    assertEquals(\"void\", meta.getColumnTypeName(21));\n    assertEquals(4, meta.getColumnDisplaySize(21));\n    assertEquals(0, meta.getPrecision(21));\n    assertEquals(0, meta.getScale(21));\n\n    for (int i = 1; i <= meta.getColumnCount(); i++) {\n      assertFalse(meta.isAutoIncrement(i));\n      assertFalse(meta.isCurrency(i));\n      assertEquals(ResultSetMetaData.columnNullable, meta.isNullable(i));\n    }\n    stmt.close();\n  }"
        ]
    ],
    "b5763019af557d7bf553cba66de0f12de130dc7b": [
        [
            "DruidQueryRecordReader::initialize(InputSplit,Configuration)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100 -\n 101 -\n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  ",
            "  public void initialize(InputSplit split, Configuration conf) throws IOException {\n    HiveDruidSplit hiveDruidSplit = (HiveDruidSplit) split;\n\n    // Create query\n    query = createQuery(hiveDruidSplit.getDruidQuery());\n\n    // Execute query\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Retrieving from druid using query:\\n \" + query);\n    }\n\n    final Lifecycle lifecycle = new Lifecycle();\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withReadTimeout(readTimeout.toStandardDuration())\n                    .withNumConnections(numConnection).build(), lifecycle);\n\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Issues with lifecycle start\", e);\n    }\n    InputStream response = DruidStorageHandlerUtils.submitRequest(client,\n            DruidStorageHandlerUtils.createRequest(hiveDruidSplit.getAddress(), query)\n    );\n    lifecycle.stop();\n    // Retrieve results\n    List<R> resultsList;\n    try {\n      resultsList = createResultsList(response);\n    } catch (IOException e) {\n      response.close();\n      throw e;\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      return;\n    }\n    results = resultsList.iterator();\n  }",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 +\n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106 +\n 107 +\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 +\n 116 +\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  ",
            "  public void initialize(InputSplit split, Configuration conf) throws IOException {\n    HiveDruidSplit hiveDruidSplit = (HiveDruidSplit) split;\n\n    // Create query\n    query = createQuery(hiveDruidSplit.getDruidQuery());\n\n    // Execute query\n    if (LOG.isInfoEnabled()) {\n      LOG.info(\"Retrieving from druid using query:\\n \" + query);\n    }\n\n    final Lifecycle lifecycle = new Lifecycle();\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withReadTimeout(readTimeout.toStandardDuration())\n                    .withNumConnections(numConnection).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Issues with lifecycle start\", e);\n    }\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(hiveDruidSplit.getAddress(), query)\n      );\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<R> resultsList;\n    try {\n      resultsList = createResultsList(response);\n    } catch (IOException e) {\n      response.close();\n      throw e;\n    } finally {\n      lifecycle.stop();\n    }\n    if (resultsList == null || resultsList.isEmpty()) {\n      return;\n    }\n    results = resultsList.iterator();\n  }"
        ],
        [
            "DruidQueryBasedInputFormat::splitSelectQuery(Configuration,String,String,Path)",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath\n  ) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n    final Lifecycle lifecycle = new Lifecycle();\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\", e);\n    }\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery)\n      );\n    } catch (Exception e) {\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<SegmentAnalysis>>() {\n              }\n      );\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    if (metadataList == null) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.isEmpty()) {\n      // There are no rows for that time range, we can submit query as it is\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath\n      ) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery)\n        );\n      } catch (Exception e) {\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n                new TypeReference<List<Result<TimeBoundaryResultValue>>>() {\n                }\n        );\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\n                \"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()\n      ));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath\n      );\n    }\n    return splits;\n  }",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271  \n 272  \n 273  \n 274  \n 275  \n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 +\n 291 +\n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  ",
            "  private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String address,\n          String druidQuery, Path dummyPath\n  ) throws IOException {\n    final int selectThreshold = (int) HiveConf.getIntVar(\n            conf, HiveConf.ConfVars.HIVE_DRUID_SELECT_THRESHOLD);\n    final int numConnection = HiveConf\n            .getIntVar(conf, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n    SelectQuery query;\n    try {\n      query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, SelectQuery.class);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      // If it has a limit, we use it and we do not split the query\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n\n    // We do not have the number of rows, thus we need to execute a\n    // Segment Metadata query to obtain number of rows\n    SegmentMetadataQueryBuilder metadataBuilder = new Druids.SegmentMetadataQueryBuilder();\n    metadataBuilder.dataSource(query.getDataSource());\n    metadataBuilder.intervals(query.getIntervals());\n    metadataBuilder.merge(true);\n    metadataBuilder.analysisTypes();\n    SegmentMetadataQuery metadataQuery = metadataBuilder.build();\n    Lifecycle lifecycle = new Lifecycle();\n    HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\", e);\n    }\n    InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client,\n              DruidStorageHandlerUtils.createRequest(address, metadataQuery)\n      );\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    List<SegmentAnalysis> metadataList;\n    try {\n      metadataList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n              new TypeReference<List<SegmentAnalysis>>() {\n              }\n      );\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n    if (metadataList == null) {\n      throw new IOException(\"Connected to Druid but could not retrieve datasource information\");\n    }\n    if (metadataList.isEmpty()) {\n      // There are no rows for that time range, we can submit query as it is\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              address, DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath) };\n    }\n    if (metadataList.size() != 1) {\n      throw new IOException(\"Information about segments should have been merged\");\n    }\n\n    final long numRows = metadataList.get(0).getNumRows();\n\n    query = query.withPagingSpec(PagingSpec.newSpec(Integer.MAX_VALUE));\n    if (numRows <= selectThreshold) {\n      // We are not going to split it\n      return new HiveDruidSplit[] { new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath\n      ) };\n    }\n\n    // If the query does not specify a timestamp, we obtain the total time using\n    // a Time Boundary query. Then, we use the information to split the query\n    // following the Select threshold configuration property\n    final List<Interval> intervals = new ArrayList<>();\n    if (query.getIntervals().size() == 1 && query.getIntervals().get(0).withChronology(\n            ISOChronology.getInstanceUTC()).equals(DruidTable.DEFAULT_INTERVAL)) {\n      // Default max and min, we should execute a time boundary query to get a\n      // more precise range\n      TimeBoundaryQueryBuilder timeBuilder = new Druids.TimeBoundaryQueryBuilder();\n      timeBuilder.dataSource(query.getDataSource());\n      TimeBoundaryQuery timeQuery = timeBuilder.build();\n\n      lifecycle = new Lifecycle();\n      client = HttpClientInit.createClient(\n              HttpClientConfig.builder().withNumConnections(numConnection)\n                      .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n      try {\n        lifecycle.start();\n      } catch (Exception e) {\n        LOG.error(\"Lifecycle start issue\", e);\n      }\n      try {\n        lifecycle.start();\n      } catch (Exception e) {\n        LOG.error(\"Lifecycle start issue\", e);\n      }\n      try {\n        response = DruidStorageHandlerUtils.submitRequest(client,\n                DruidStorageHandlerUtils.createRequest(address, timeQuery)\n        );\n      } catch (Exception e) {\n        lifecycle.stop();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n\n      // Retrieve results\n      List<Result<TimeBoundaryResultValue>> timeList;\n      try {\n        timeList = DruidStorageHandlerUtils.SMILE_MAPPER.readValue(response,\n                new TypeReference<List<Result<TimeBoundaryResultValue>>>() {\n                }\n        );\n      } catch (Exception e) {\n        response.close();\n        throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n      } finally {\n        lifecycle.stop();\n      }\n      if (timeList == null || timeList.isEmpty()) {\n        throw new IOException(\n                \"Connected to Druid but could not retrieve time boundary information\");\n      }\n      if (timeList.size() != 1) {\n        throw new IOException(\"We should obtain a single time boundary\");\n      }\n\n      intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()\n      ));\n    } else {\n      intervals.addAll(query.getIntervals());\n    }\n\n    // Create (numRows/default threshold) input splits\n    int numSplits = (int) Math.ceil((double) numRows / selectThreshold);\n    List<List<Interval>> newIntervals = createSplitsIntervals(intervals, numSplits);\n    HiveDruidSplit[] splits = new HiveDruidSplit[numSplits];\n    for (int i = 0; i < numSplits; i++) {\n      // Create partial Select query\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleIntervalSegmentSpec(newIntervals.get(i)));\n      splits[i] = new HiveDruidSplit(address,\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery), dummyPath\n      );\n    }\n    return splits;\n  }"
        ]
    ],
    "f8b6ebb20517079c988a4fa043b6f3d33f1b973b": [
        [
            "HiveExpandDistinctAggregatesRule::onMatch(RelOptRuleCall)",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    int numCountDistinct = getNumCountDistinctCall(aggregate);\n    if (numCountDistinct == 0) {\n      return;\n    }\n\n    // Find all of the agg expressions. We use a List (for all count(distinct))\n    // as well as a Set (for all others) to ensure determinism.\n    int nonDistinctCount = 0;\n    List<List<Integer>> argListList = new ArrayList<List<Integer>>();\n    Set<List<Integer>> argListSets = new LinkedHashSet<List<Integer>>();\n    Set<Integer> positions = new HashSet<>();\n    for (AggregateCall aggCall : aggregate.getAggCallList()) {\n      if (!aggCall.isDistinct()) {\n        ++nonDistinctCount;\n        continue;\n      }\n      ArrayList<Integer> argList = new ArrayList<Integer>();\n      for (Integer arg : aggCall.getArgList()) {\n        argList.add(arg);\n        positions.add(arg);\n      }\n      // Aggr checks for sorted argList.\n      argListList.add(argList);\n      argListSets.add(argList);\n    }\n    Util.permAssert(argListSets.size() > 0, \"containsDistinctCall lied\");\n\n    if (numCountDistinct > 1 && numCountDistinct == aggregate.getAggCallList().size()\n        && aggregate.getGroupSet().isEmpty()) {\n      // now positions contains all the distinct positions, i.e., $5, $4, $6\n      // we need to first sort them as group by set\n      // and then get their position later, i.e., $4->1, $5->2, $6->3\n      cluster = aggregate.getCluster();\n      rexBuilder = cluster.getRexBuilder();\n      RelNode converted = null;\n      List<Integer> sourceOfForCountDistinct = new ArrayList<>();\n      sourceOfForCountDistinct.addAll(positions);\n      Collections.sort(sourceOfForCountDistinct);\n      try {\n        converted = convert(aggregate, argListList, sourceOfForCountDistinct);\n      } catch (CalciteSemanticException e) {\n        LOG.debug(e.toString());\n        throw new RuntimeException(e);\n      }\n      call.transformTo(converted);\n      return;\n    }\n\n    // If all of the agg expressions are distinct and have the same\n    // arguments then we can use a more efficient form.\n    if ((nonDistinctCount == 0) && (argListSets.size() == 1)) {\n      for (Integer arg : argListSets.iterator().next()) {\n        Set<RelColumnOrigin> colOrigs = RelMetadataQuery.instance().getColumnOrigins(aggregate, arg);\n        if (null != colOrigs) {\n          for (RelColumnOrigin colOrig : colOrigs) {\n            RelOptHiveTable hiveTbl = (RelOptHiveTable)colOrig.getOriginTable();\n            if(hiveTbl.getPartColInfoMap().containsKey(colOrig.getOriginColumnOrdinal())) {\n              // Encountered partitioning column, this will be better handled by MetadataOnly optimizer.\n              return;\n            }\n          }\n        }\n      }\n      RelNode converted =\n          convertMonopole(\n              aggregate,\n              argListSets.iterator().next());\n      call.transformTo(converted);\n      return;\n    }\n  }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    int numCountDistinct = getNumCountDistinctCall(aggregate);\n    if (numCountDistinct == 0) {\n      return;\n    }\n\n    // Find all of the agg expressions. We use a List (for all count(distinct))\n    // as well as a Set (for all others) to ensure determinism.\n    int nonDistinctCount = 0;\n    List<List<Integer>> argListList = new ArrayList<List<Integer>>();\n    Set<List<Integer>> argListSets = new LinkedHashSet<List<Integer>>();\n    Set<Integer> positions = new HashSet<>();\n    for (AggregateCall aggCall : aggregate.getAggCallList()) {\n      if (!aggCall.isDistinct()) {\n        ++nonDistinctCount;\n        continue;\n      }\n      ArrayList<Integer> argList = new ArrayList<Integer>();\n      for (Integer arg : aggCall.getArgList()) {\n        argList.add(arg);\n        positions.add(arg);\n      }\n      // Aggr checks for sorted argList.\n      argListList.add(argList);\n      argListSets.add(argList);\n    }\n    Util.permAssert(argListSets.size() > 0, \"containsDistinctCall lied\");\n\n    if (numCountDistinct > 1 && numCountDistinct == aggregate.getAggCallList().size()\n        && aggregate.getGroupSet().isEmpty()) {\n      LOG.debug(\"Trigger countDistinct rewrite. numCountDistinct is \" + numCountDistinct);\n      // now positions contains all the distinct positions, i.e., $5, $4, $6\n      // we need to first sort them as group by set\n      // and then get their position later, i.e., $4->1, $5->2, $6->3\n      cluster = aggregate.getCluster();\n      rexBuilder = cluster.getRexBuilder();\n      RelNode converted = null;\n      List<Integer> sourceOfForCountDistinct = new ArrayList<>();\n      sourceOfForCountDistinct.addAll(positions);\n      Collections.sort(sourceOfForCountDistinct);\n      try {\n        converted = convert(aggregate, argListList, sourceOfForCountDistinct);\n      } catch (CalciteSemanticException e) {\n        LOG.debug(e.toString());\n        throw new RuntimeException(e);\n      }\n      call.transformTo(converted);\n      return;\n    }\n\n    // If all of the agg expressions are distinct and have the same\n    // arguments then we can use a more efficient form.\n    if ((nonDistinctCount == 0) && (argListSets.size() == 1)) {\n      for (Integer arg : argListSets.iterator().next()) {\n        Set<RelColumnOrigin> colOrigs = RelMetadataQuery.instance().getColumnOrigins(aggregate, arg);\n        if (null != colOrigs) {\n          for (RelColumnOrigin colOrig : colOrigs) {\n            RelOptHiveTable hiveTbl = (RelOptHiveTable)colOrig.getOriginTable();\n            if(hiveTbl.getPartColInfoMap().containsKey(colOrig.getOriginColumnOrdinal())) {\n              // Encountered partitioning column, this will be better handled by MetadataOnly optimizer.\n              return;\n            }\n          }\n        }\n      }\n      RelNode converted =\n          convertMonopole(\n              aggregate,\n              argListSets.iterator().next());\n      call.transformTo(converted);\n      return;\n    }\n  }"
        ],
        [
            "HiveExpandDistinctAggregatesRule::createCount(Aggregate,List,List,Map,List)",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234 -\n 235 -\n 236 -\n 237 -\n 238  \n 239 -\n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  /**\n   * @param aggr: the original aggregate\n   * @param argList: the original argList in aggregate\n   * @param cleanArgList: the new argList without duplicates\n   * @param map: the mapping from the original argList to the new argList\n   * @param sourceOfForCountDistinct: the sorted positions of groupset\n   * @return\n   * @throws CalciteSemanticException\n   */\n  private RelNode createCount(Aggregate aggr, List<List<Integer>> argList,\n      List<List<Integer>> cleanArgList, Map<Integer, Integer> map,\n      List<Integer> sourceOfForCountDistinct) throws CalciteSemanticException {\n    List<RexNode> originalInputRefs = Lists.transform(aggr.getRowType().getFieldList(),\n        new Function<RelDataTypeField, RexNode>() {\n          @Override\n          public RexNode apply(RelDataTypeField input) {\n            return new RexInputRef(input.getIndex(), input.getType());\n          }\n        });\n    final List<RexNode> gbChildProjLst = Lists.newArrayList();\n    for (List<Integer> list : cleanArgList) {\n      RexNode equal = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS,\n          originalInputRefs.get(originalInputRefs.size() - 1),\n          rexBuilder.makeExactLiteral(new BigDecimal(getGroupingIdValue(list, sourceOfForCountDistinct))));\n      RexNode condition = rexBuilder.makeCall(SqlStdOperatorTable.CASE, equal,\n          rexBuilder.makeExactLiteral(BigDecimal.ONE), rexBuilder.constantNull());\n      gbChildProjLst.add(condition);\n    }\n\n    // create the project before GB\n    RelNode gbInputRel = HiveProject.create(aggr, gbChildProjLst, null);\n\n    // create the aggregate\n    List<AggregateCall> aggregateCalls = Lists.newArrayList();\n    RelDataType aggFnRetType = TypeConverter.convert(TypeInfoFactory.longTypeInfo,\n        cluster.getTypeFactory());\n    for (int i = 0; i < cleanArgList.size(); i++) {\n      AggregateCall aggregateCall = HiveCalciteUtil.createSingleArgAggCall(\"count\", cluster,\n          TypeInfoFactory.longTypeInfo, i, aggFnRetType);\n      aggregateCalls.add(aggregateCall);\n    }\n    Aggregate aggregate = new HiveAggregate(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION), gbInputRel,\n        false, ImmutableBitSet.of(), null, aggregateCalls);\n\n    // create the project after GB. For those repeated values, e.g., select\n    // count(distinct x, y), count(distinct y, x), we find the correct mapping.\n    if (map.isEmpty()) {\n      return aggregate;\n    } else {\n      List<RexNode> originalAggrRefs = Lists.transform(aggregate.getRowType().getFieldList(),\n          new Function<RelDataTypeField, RexNode>() {\n            @Override\n            public RexNode apply(RelDataTypeField input) {\n              return new RexInputRef(input.getIndex(), input.getType());\n            }\n          });\n      final List<RexNode> projLst = Lists.newArrayList();\n      int index = 0;\n      for (int i = 0; i < argList.size(); i++) {\n        if (map.containsKey(i)) {\n          projLst.add(originalAggrRefs.get(map.get(i)));\n        } else {\n          projLst.add(originalAggrRefs.get(index++));\n        }\n      }\n      return HiveProject.create(aggregate, projLst, null);\n    }\n  }",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 +\n 238 +\n 239 +\n 240  \n 241 +\n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248 +\n 249 +\n 250 +\n 251  \n 252 +\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "  /**\n   * @param aggr: the original aggregate\n   * @param argList: the original argList in aggregate\n   * @param cleanArgList: the new argList without duplicates\n   * @param map: the mapping from the original argList to the new argList\n   * @param sourceOfForCountDistinct: the sorted positions of groupset\n   * @return\n   * @throws CalciteSemanticException\n   */\n  private RelNode createCount(Aggregate aggr, List<List<Integer>> argList,\n      List<List<Integer>> cleanArgList, Map<Integer, Integer> map,\n      List<Integer> sourceOfForCountDistinct) throws CalciteSemanticException {\n    List<RexNode> originalInputRefs = Lists.transform(aggr.getRowType().getFieldList(),\n        new Function<RelDataTypeField, RexNode>() {\n          @Override\n          public RexNode apply(RelDataTypeField input) {\n            return new RexInputRef(input.getIndex(), input.getType());\n          }\n        });\n    final List<RexNode> gbChildProjLst = Lists.newArrayList();\n    // for singular arg, count should not include null\n    // e.g., count(case when i=1 and department_id is not null then 1 else null end) as c0, \n    // for non-singular args, count can include null, i.e. (,) is counted as 1\n    for (List<Integer> list : cleanArgList) {\n      RexNode condition = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS, originalInputRefs\n          .get(originalInputRefs.size() - 1), rexBuilder.makeExactLiteral(new BigDecimal(\n          getGroupingIdValue(list, sourceOfForCountDistinct))));\n      if (list.size() == 1) {\n        int pos = list.get(0);\n        RexNode notNull = rexBuilder.makeCall(SqlStdOperatorTable.IS_NOT_NULL,\n            originalInputRefs.get(pos));\n        condition = rexBuilder.makeCall(SqlStdOperatorTable.AND, condition, notNull);\n      }\n      RexNode when = rexBuilder.makeCall(SqlStdOperatorTable.CASE, condition,\n          rexBuilder.makeExactLiteral(BigDecimal.ONE), rexBuilder.constantNull());\n      gbChildProjLst.add(when);\n    }\n\n    // create the project before GB\n    RelNode gbInputRel = HiveProject.create(aggr, gbChildProjLst, null);\n\n    // create the aggregate\n    List<AggregateCall> aggregateCalls = Lists.newArrayList();\n    RelDataType aggFnRetType = TypeConverter.convert(TypeInfoFactory.longTypeInfo,\n        cluster.getTypeFactory());\n    for (int i = 0; i < cleanArgList.size(); i++) {\n      AggregateCall aggregateCall = HiveCalciteUtil.createSingleArgAggCall(\"count\", cluster,\n          TypeInfoFactory.longTypeInfo, i, aggFnRetType);\n      aggregateCalls.add(aggregateCall);\n    }\n    Aggregate aggregate = new HiveAggregate(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION), gbInputRel,\n        false, ImmutableBitSet.of(), null, aggregateCalls);\n\n    // create the project after GB. For those repeated values, e.g., select\n    // count(distinct x, y), count(distinct y, x), we find the correct mapping.\n    if (map.isEmpty()) {\n      return aggregate;\n    } else {\n      List<RexNode> originalAggrRefs = Lists.transform(aggregate.getRowType().getFieldList(),\n          new Function<RelDataTypeField, RexNode>() {\n            @Override\n            public RexNode apply(RelDataTypeField input) {\n              return new RexInputRef(input.getIndex(), input.getType());\n            }\n          });\n      final List<RexNode> projLst = Lists.newArrayList();\n      int index = 0;\n      for (int i = 0; i < argList.size(); i++) {\n        if (map.containsKey(i)) {\n          projLst.add(originalAggrRefs.get(map.get(i)));\n        } else {\n          projLst.add(originalAggrRefs.get(index++));\n        }\n      }\n      return HiveProject.create(aggregate, projLst, null);\n    }\n  }"
        ]
    ],
    "c31c2963d6db464c482a56c532011f2d00b527aa": [
        [
            "ExprWalkerProcFactory::extractFinalCandidates(ExprNodeDesc,ExprWalkerInfo,HiveConf)",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370 -\n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "  /**\n   * Walks through the top AND nodes and determine which of them are final\n   * candidates.\n   */\n  private static void extractFinalCandidates(ExprNodeDesc expr,\n      ExprWalkerInfo ctx, HiveConf conf) {\n    // We decompose an AND expression into its parts before checking if the\n    // entire expression is a candidate because each part may be a candidate\n    // for replicating transitively over an equijoin condition.\n    if (FunctionRegistry.isOpAnd(expr)) {\n      // If the operator is AND, we need to determine if any of the children are\n      // final candidates.\n\n      // For the children, we populate the NewToOldExprMap to keep track of\n      // the original condition before rewriting it for this operator\n      assert ctx.getNewToOldExprMap().containsKey(expr);\n      for (int i = 0; i < expr.getChildren().size(); i++) {\n        ctx.getNewToOldExprMap().put(\n            expr.getChildren().get(i),\n            ctx.getNewToOldExprMap().get(expr).getChildren().get(i));\n        extractFinalCandidates(expr.getChildren().get(i),\n            ctx, conf);\n      }\n      return;\n    }\n\n    ExprInfo exprInfo = ctx.getExprInfo(expr);\n    if (exprInfo != null && exprInfo.isCandidate) {\n      ctx.addFinalCandidate(exprInfo.alias, exprInfo.convertedExpr != null ?\n              exprInfo.convertedExpr : expr);\n      return;\n    } else if (!FunctionRegistry.isOpAnd(expr) &&\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEPPDREMOVEDUPLICATEFILTERS)) {\n      ctx.addNonFinalCandidate(exprInfo != null ? exprInfo.alias : null, expr);\n    }\n  }",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370 +\n 371 +\n 372 +\n 373 +\n 374 +\n 375 +\n 376 +\n 377 +\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  ",
            "  /**\n   * Walks through the top AND nodes and determine which of them are final\n   * candidates.\n   */\n  private static void extractFinalCandidates(ExprNodeDesc expr,\n      ExprWalkerInfo ctx, HiveConf conf) {\n    // We decompose an AND expression into its parts before checking if the\n    // entire expression is a candidate because each part may be a candidate\n    // for replicating transitively over an equijoin condition.\n    if (FunctionRegistry.isOpAnd(expr)) {\n      // If the operator is AND, we need to determine if any of the children are\n      // final candidates.\n\n      // For the children, we populate the NewToOldExprMap to keep track of\n      // the original condition before rewriting it for this operator\n      assert ctx.getNewToOldExprMap().containsKey(expr);\n      for (int i = 0; i < expr.getChildren().size(); i++) {\n        ctx.getNewToOldExprMap().put(\n            expr.getChildren().get(i),\n            ctx.getNewToOldExprMap().get(expr).getChildren().get(i));\n        extractFinalCandidates(expr.getChildren().get(i),\n            ctx, conf);\n      }\n      return;\n    }\n\n    ExprInfo exprInfo = ctx.getExprInfo(expr);\n    if (exprInfo != null && exprInfo.isCandidate) {\n      String alias = exprInfo.alias;\n      if ((alias == null) && (exprInfo.convertedExpr != null)) {\n    \tExprInfo convertedExprInfo = ctx.getExprInfo(exprInfo.convertedExpr);\n    \tif (convertedExprInfo != null) {\n    \t\talias = convertedExprInfo.alias;\n    \t}\n      }\n      ctx.addFinalCandidate(alias, exprInfo.convertedExpr != null ?\n              exprInfo.convertedExpr : expr);\n      return;\n    } else if (!FunctionRegistry.isOpAnd(expr) &&\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEPPDREMOVEDUPLICATEFILTERS)) {\n      ctx.addNonFinalCandidate(exprInfo != null ? exprInfo.alias : null, expr);\n    }\n  }"
        ]
    ],
    "2d501879c38f755273dfd97e4f66ac9217cd20b3": [
        [
            "SubQueryUtils::checkAggOrWindowing(ASTNode)",
            " 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 -\n 266  \n 267  \n 268  \n 269  \n 270 -\n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  ",
            "  static int checkAggOrWindowing(ASTNode expressionTree) throws SemanticException {\n    int exprTokenType = expressionTree.getToken().getType();\n    if (exprTokenType == HiveParser.TOK_FUNCTION\n        || exprTokenType == HiveParser.TOK_FUNCTIONDI\n        || exprTokenType == HiveParser.TOK_FUNCTIONSTAR) {\n      assert (expressionTree.getChildCount() != 0);\n      if (expressionTree.getChild(expressionTree.getChildCount()-1).getType()\n          == HiveParser.TOK_WINDOWSPEC) {\n        return 2;\n      }\n      if (expressionTree.getChild(0).getType() == HiveParser.Identifier) {\n        String functionName = SemanticAnalyzer.unescapeIdentifier(expressionTree.getChild(0)\n            .getText());\n        if (FunctionRegistry.getGenericUDAFResolver(functionName) != null) {\n          return 1;\n        }\n      }\n    }\n    int r = 0;\n    for (int i = 0; i < expressionTree.getChildCount(); i++) {\n      int c = checkAggOrWindowing((ASTNode) expressionTree.getChild(i));\n      r = Math.max(r, c);\n    }\n    return r;\n  }",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273 +\n 274 +\n 275 +\n 276 +\n 277 +\n 278 +\n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  ",
            "  static int checkAggOrWindowing(ASTNode expressionTree) throws SemanticException {\n    int exprTokenType = expressionTree.getToken().getType();\n    if (exprTokenType == HiveParser.TOK_FUNCTION\n        || exprTokenType == HiveParser.TOK_FUNCTIONDI\n        || exprTokenType == HiveParser.TOK_FUNCTIONSTAR) {\n      assert (expressionTree.getChildCount() != 0);\n      if (expressionTree.getChild(expressionTree.getChildCount()-1).getType()\n          == HiveParser.TOK_WINDOWSPEC) {\n        return 3;\n      }\n      if (expressionTree.getChild(0).getType() == HiveParser.Identifier) {\n        String functionName = SemanticAnalyzer.unescapeIdentifier(expressionTree.getChild(0)\n            .getText());\n        GenericUDAFResolver udafResolver = FunctionRegistry.getGenericUDAFResolver(functionName);\n        if (udafResolver != null) {\n            // we need to know if it is COUNT since this is specialized for IN/NOT IN\n            // corr subqueries.\n          if(udafResolver instanceof GenericUDAFCount) {\n            return 2;\n          }\n          return 1;\n        }\n      }\n    }\n    int r = 0;\n    for (int i = 0; i < expressionTree.getChildCount(); i++) {\n      int c = checkAggOrWindowing((ASTNode) expressionTree.getChild(i));\n      r = Math.max(r, c);\n    }\n    return r;\n  }"
        ],
        [
            "HiveSubQueryRemoveRule::apply(RexSubQuery,Set,RelOptUtil,HiveSubQRemoveRelBuilder,int,int,boolean)",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  ",
            "    protected RexNode apply(RexSubQuery e, Set<CorrelationId> variablesSet,\n                            RelOptUtil.Logic logic,\n                            HiveSubQRemoveRelBuilder builder, int inputCount, int offset,\n                            boolean isCorrScalarAgg) {\n        switch (e.getKind()) {\n            case SCALAR_QUERY:\n                if(isCorrScalarAgg) {\n                    // Transformation :\n                    // Outer Query Left Join (inner query) on correlated predicate and preserve rows only from left side.\n                    builder.push(e.rel);\n                    final List<RexNode> parentQueryFields = new ArrayList<>();\n                    parentQueryFields.addAll(builder.fields());\n\n                    // id is appended since there could be multiple scalar subqueries and FILTER\n                    // is created using field name\n                    String indicator = \"alwaysTrue\" + e.rel.getId();\n                    parentQueryFields.add(builder.alias(builder.literal(true), indicator));\n                    builder.project(parentQueryFields);\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n\n                    final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                    RexNode literal;\n                    if(isAggZeroOnEmpty(e)) {\n                        literal = builder.literal(0);\n                    }\n                    else {\n                        literal = e.rel.getCluster().getRexBuilder().makeNullLiteral(getAggTypeForScalarSub(e));\n                    }\n                    operands.add((builder.isNull(builder.field(indicator))), literal);\n                    operands.add(field(builder, 1, builder.fields().size()-2));\n                    return builder.call(SqlStdOperatorTable.CASE, operands.build());\n                }\n\n                //Transformation is to left join for correlated predicates and inner join otherwise,\n                // but do a count on inner side before that to make sure it generates atmost 1 row.\n                builder.push(e.rel);\n                // returns single row/column\n                builder.aggregate(builder.groupKey(),\n                        builder.count(false, \"cnt\"));\n\n                SqlFunction countCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                        InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                // we create FILTER (sq_count_check(count()) <= 1) instead of PROJECT because RelFieldTrimmer\n                //  ends up getting rid of Project since it is not used further up the tree\n                builder.filter(builder.call(SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n                        builder.call(countCheck, builder.field(\"cnt\")),\n                        builder.literal(1)));\n\n                if( !variablesSet.isEmpty())\n                {\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                }\n                else\n                    builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                builder.push(e.rel);\n                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                offset++;\n                return field(builder, inputCount, offset);\n\n            case IN:\n            case EXISTS:\n                // Most general case, where the left and right keys might have nulls, and\n                // caller requires 3-valued logic return.\n                //\n                // select e.deptno, e.deptno in (select deptno from emp)\n                //\n                // becomes\n                //\n                // select e.deptno,\n                //   case\n                //   when ct.c = 0 then false\n                //   when dt.i is not null then true\n                //   when e.deptno is null then null\n                //   when ct.ck < ct.c then null\n                //   else false\n                //   end\n                // from e\n                // left join (\n                //   (select count(*) as c, count(deptno) as ck from emp) as ct\n                //   cross join (select distinct deptno, true as i from emp)) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // If keys are not null we can remove \"ct\" and simplify to\n                //\n                // select e.deptno,\n                //   case\n                //   when dt.i is not null then true\n                //   else false\n                //   end\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // We could further simplify to\n                //\n                // select e.deptno,\n                //   dt.i is not null\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // but have not yet.\n                //\n                // If the logic is TRUE we can just kill the record if the condition\n                // evaluates to FALSE or UNKNOWN. Thus the query simplifies to an inner\n                // join:\n                //\n                // select e.deptno,\n                //   true\n                // from e\n                // inner join (select distinct deptno from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n\n                builder.push(e.rel);\n                final List<RexNode> fields = new ArrayList<>();\n                switch (e.getKind()) {\n                    case IN:\n                        fields.addAll(builder.fields());\n                }\n\n                // First, the cross join\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        // Since EXISTS/NOT EXISTS are not affected by presence of\n                        // null keys we do not need to generate count(*), count(c)\n                        if (e.getKind() == SqlKind.EXISTS) {\n                            logic = RelOptUtil.Logic.TRUE_FALSE;\n                            break;\n                        }\n                        builder.aggregate(builder.groupKey(),\n                                builder.count(false, \"c\"),\n                                builder.aggregateCall(SqlStdOperatorTable.COUNT, false, null, \"ck\",\n                                        builder.fields()));\n                        builder.as(\"ct\");\n                        if( !variablesSet.isEmpty())\n                        {\n                            //builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                        }\n                        else\n                            builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n\n                        offset += 2;\n                        builder.push(e.rel);\n                        break;\n                }\n\n                // Now the left join\n                switch (logic) {\n                    case TRUE:\n                        if (fields.isEmpty()) {\n                            builder.project(builder.alias(builder.literal(true), \"i\"));\n                            builder.aggregate(builder.groupKey(0));\n                        } else {\n                            builder.aggregate(builder.groupKey(fields));\n                        }\n                        break;\n                    default:\n                        fields.add(builder.alias(builder.literal(true), \"i\"));\n                        builder.project(fields);\n                        builder.distinct();\n                }\n                builder.as(\"dt\");\n                final List<RexNode> conditions = new ArrayList<>();\n                for (Pair<RexNode, RexNode> pair\n                        : Pair.zip(e.getOperands(), builder.fields())) {\n                    conditions.add(\n                            builder.equals(pair.left, RexUtil.shift(pair.right, offset)));\n                }\n                switch (logic) {\n                    case TRUE:\n                        builder.join(JoinRelType.INNER, builder.and(conditions), variablesSet);\n                        return builder.literal(true);\n                }\n                builder.join(JoinRelType.LEFT, builder.and(conditions), variablesSet);\n\n                final List<RexNode> keyIsNulls = new ArrayList<>();\n                for (RexNode operand : e.getOperands()) {\n                    if (operand.getType().isNullable()) {\n                        keyIsNulls.add(builder.isNull(operand));\n                    }\n                }\n                final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.equals(builder.field(\"ct\", \"c\"), builder.literal(0)),\n                                builder.literal(false));\n                        //now that we are using LEFT OUTER JOIN to join inner count, count(*)\n                        // with outer table, we wouldn't be able to tell if count is zero\n                        // for inner table since inner join with correlated values will get rid\n                        // of all values where join cond is not true (i.e where actual inner table\n                        // will produce zero result). To  handle this case we need to check both\n                        // count is zero or count is null\n                        operands.add((builder.isNull(builder.field(\"ct\", \"c\"))), builder.literal(false));\n                        break;\n                }\n                operands.add(builder.isNotNull(builder.field(\"dt\", \"i\")),\n                        builder.literal(true));\n                if (!keyIsNulls.isEmpty()) {\n                    //Calcite creates null literal with Null type here but because HIVE doesn't support null type\n                    // it is appropriately typed boolean\n                    operands.add(builder.or(keyIsNulls), e.rel.getCluster().getRexBuilder().makeNullLiteral(SqlTypeName.BOOLEAN));\n                    // we are creating filter here so should not be returning NULL. Not sure why Calcite return NULL\n                    //operands.add(builder.or(keyIsNulls), builder.literal(false));\n                }\n                Boolean b = true;\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                        b = null;\n                        // fall through\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.call(SqlStdOperatorTable.LESS_THAN,\n                                        builder.field(\"ct\", \"ck\"), builder.field(\"ct\", \"c\")),\n                                builder.literal(b));\n                        break;\n                }\n                operands.add(builder.literal(false));\n                return builder.call(SqlStdOperatorTable.CASE, operands.build());\n\n            default:\n                throw new AssertionError(e.getKind());\n        }\n    }",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 +\n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273 +\n 274 +\n 275 +\n 276 +\n 277 +\n 278 +\n 279 +\n 280 +\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "    protected RexNode apply(RexSubQuery e, Set<CorrelationId> variablesSet,\n                            RelOptUtil.Logic logic,\n                            HiveSubQRemoveRelBuilder builder, int inputCount, int offset,\n                            boolean isCorrScalarAgg) {\n        switch (e.getKind()) {\n            case SCALAR_QUERY:\n                if(isCorrScalarAgg) {\n                    // Transformation :\n                    // Outer Query Left Join (inner query) on correlated predicate and preserve rows only from left side.\n                    builder.push(e.rel);\n                    final List<RexNode> parentQueryFields = new ArrayList<>();\n                    parentQueryFields.addAll(builder.fields());\n\n                    // id is appended since there could be multiple scalar subqueries and FILTER\n                    // is created using field name\n                    String indicator = \"alwaysTrue\" + e.rel.getId();\n                    parentQueryFields.add(builder.alias(builder.literal(true), indicator));\n                    builder.project(parentQueryFields);\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n\n                    final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                    RexNode literal;\n                    if(isAggZeroOnEmpty(e)) {\n                        literal = builder.literal(0);\n                    }\n                    else {\n                        literal = e.rel.getCluster().getRexBuilder().makeNullLiteral(getAggTypeForScalarSub(e));\n                    }\n                    operands.add((builder.isNull(builder.field(indicator))), literal);\n                    operands.add(field(builder, 1, builder.fields().size()-2));\n                    return builder.call(SqlStdOperatorTable.CASE, operands.build());\n                }\n\n                //Transformation is to left join for correlated predicates and inner join otherwise,\n                // but do a count on inner side before that to make sure it generates atmost 1 row.\n                builder.push(e.rel);\n                // returns single row/column\n                builder.aggregate(builder.groupKey(),\n                        builder.count(false, \"cnt\"));\n\n                SqlFunction countCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                        InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                // we create FILTER (sq_count_check(count()) <= 1) instead of PROJECT because RelFieldTrimmer\n                //  ends up getting rid of Project since it is not used further up the tree\n                builder.filter(builder.call(SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n                        builder.call(countCheck, builder.field(\"cnt\")),\n                        builder.literal(1)));\n\n                if( !variablesSet.isEmpty())\n                {\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                }\n                else\n                    builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                builder.push(e.rel);\n                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                offset++;\n                return field(builder, inputCount, offset);\n\n            case IN:\n            case EXISTS:\n                // Most general case, where the left and right keys might have nulls, and\n                // caller requires 3-valued logic return.\n                //\n                // select e.deptno, e.deptno in (select deptno from emp)\n                //\n                // becomes\n                //\n                // select e.deptno,\n                //   case\n                //   when ct.c = 0 then false\n                //   when dt.i is not null then true\n                //   when e.deptno is null then null\n                //   when ct.ck < ct.c then null\n                //   else false\n                //   end\n                // from e\n                // left join (\n                //   (select count(*) as c, count(deptno) as ck from emp) as ct\n                //   cross join (select distinct deptno, true as i from emp)) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // If keys are not null we can remove \"ct\" and simplify to\n                //\n                // select e.deptno,\n                //   case\n                //   when dt.i is not null then true\n                //   else false\n                //   end\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // We could further simplify to\n                //\n                // select e.deptno,\n                //   dt.i is not null\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // but have not yet.\n                //\n                // If the logic is TRUE we can just kill the record if the condition\n                // evaluates to FALSE or UNKNOWN. Thus the query simplifies to an inner\n                // join:\n                //\n                // select e.deptno,\n                //   true\n                // from e\n                // inner join (select distinct deptno from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n\n                builder.push(e.rel);\n                final List<RexNode> fields = new ArrayList<>();\n                switch (e.getKind()) {\n                    case IN:\n                        fields.addAll(builder.fields());\n                        // Transformation: sq_count_check(count(*), true) FILTER is generated on top\n                        //  of subquery which is then joined (LEFT or INNER) with outer query\n                        //  This transformation is done to add run time check using sq_count_check to\n                        //  throw an error if subquery is producing zero row, since with aggregate this\n                        //  will produce wrong results (because we further rewrite such queries into JOIN)\n                        if(isCorrScalarAgg) {\n                            // returns single row/column\n                            builder.aggregate(builder.groupKey(),\n                                    builder.count(false, \"cnt_in\"));\n\n                            if (!variablesSet.isEmpty()) {\n                                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                            } else {\n                                builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            }\n\n                            SqlFunction inCountCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                                    InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                            // we create FILTER (sq_count_check(count()) > 0) instead of PROJECT because RelFieldTrimmer\n                            //  ends up getting rid of Project since it is not used further up the tree\n                            builder.filter(builder.call(SqlStdOperatorTable.GREATER_THAN,\n                                    //true here indicates that sq_count_check is for IN/NOT IN subqueries\n                                    builder.call(inCountCheck, builder.field(\"cnt_in\"), builder.literal(true)),\n                                    builder.literal(0)));\n                            offset =  offset + 1;\n                            builder.push(e.rel);\n                        }\n                }\n\n                // First, the cross join\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        // Since EXISTS/NOT EXISTS are not affected by presence of\n                        // null keys we do not need to generate count(*), count(c)\n                        if (e.getKind() == SqlKind.EXISTS) {\n                            logic = RelOptUtil.Logic.TRUE_FALSE;\n                            break;\n                        }\n                        builder.aggregate(builder.groupKey(),\n                                builder.count(false, \"c\"),\n                                builder.aggregateCall(SqlStdOperatorTable.COUNT, false, null, \"ck\",\n                                        builder.fields()));\n                        builder.as(\"ct\");\n                        if( !variablesSet.isEmpty())\n                        {\n                            //builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                        }\n                        else\n                            builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n\n                        offset += 2;\n                        builder.push(e.rel);\n                        break;\n                }\n\n                // Now the left join\n                switch (logic) {\n                    case TRUE:\n                        if (fields.isEmpty()) {\n                            builder.project(builder.alias(builder.literal(true), \"i\"));\n                            builder.aggregate(builder.groupKey(0));\n                        } else {\n                            builder.aggregate(builder.groupKey(fields));\n                        }\n                        break;\n                    default:\n                        fields.add(builder.alias(builder.literal(true), \"i\"));\n                        builder.project(fields);\n                        builder.distinct();\n                }\n                builder.as(\"dt\");\n                final List<RexNode> conditions = new ArrayList<>();\n                for (Pair<RexNode, RexNode> pair\n                        : Pair.zip(e.getOperands(), builder.fields())) {\n                    conditions.add(\n                            builder.equals(pair.left, RexUtil.shift(pair.right, offset)));\n                }\n                switch (logic) {\n                    case TRUE:\n                        builder.join(JoinRelType.INNER, builder.and(conditions), variablesSet);\n                        return builder.literal(true);\n                }\n                builder.join(JoinRelType.LEFT, builder.and(conditions), variablesSet);\n\n                final List<RexNode> keyIsNulls = new ArrayList<>();\n                for (RexNode operand : e.getOperands()) {\n                    if (operand.getType().isNullable()) {\n                        keyIsNulls.add(builder.isNull(operand));\n                    }\n                }\n                final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.equals(builder.field(\"ct\", \"c\"), builder.literal(0)),\n                                builder.literal(false));\n                        //now that we are using LEFT OUTER JOIN to join inner count, count(*)\n                        // with outer table, we wouldn't be able to tell if count is zero\n                        // for inner table since inner join with correlated values will get rid\n                        // of all values where join cond is not true (i.e where actual inner table\n                        // will produce zero result). To  handle this case we need to check both\n                        // count is zero or count is null\n                        operands.add((builder.isNull(builder.field(\"ct\", \"c\"))), builder.literal(false));\n                        break;\n                }\n                operands.add(builder.isNotNull(builder.field(\"dt\", \"i\")),\n                        builder.literal(true));\n                if (!keyIsNulls.isEmpty()) {\n                    //Calcite creates null literal with Null type here but because HIVE doesn't support null type\n                    // it is appropriately typed boolean\n                    operands.add(builder.or(keyIsNulls), e.rel.getCluster().getRexBuilder().makeNullLiteral(SqlTypeName.BOOLEAN));\n                    // we are creating filter here so should not be returning NULL. Not sure why Calcite return NULL\n                    //operands.add(builder.or(keyIsNulls), builder.literal(false));\n                }\n                Boolean b = true;\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                        b = null;\n                        // fall through\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.call(SqlStdOperatorTable.LESS_THAN,\n                                        builder.field(\"ct\", \"ck\"), builder.field(\"ct\", \"c\")),\n                                builder.literal(b));\n                        break;\n                }\n                operands.add(builder.literal(false));\n                return builder.call(SqlStdOperatorTable.CASE, operands.build());\n\n            default:\n                throw new AssertionError(e.getKind());\n        }\n    }"
        ],
        [
            "QBSubQuery::validateAndRewriteAST(RowResolver,boolean,String,Set)",
            " 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687 -\n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  ",
            "  void validateAndRewriteAST(RowResolver outerQueryRR,\n      boolean forHavingClause,\n      String outerQueryAlias,\n      Set<String> outerQryAliases) throws SemanticException {\n\n    ASTNode fromClause = getChildFromSubqueryAST(\"From\", HiveParser.TOK_FROM);\n    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n\n    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n\n    int selectExprStart = 0;\n    if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n      selectExprStart = 1;\n    }\n\n    /*\n     * Restriction.16.s :: Correlated Expression in Outer Query must not contain\n     * unqualified column references.\n     * disabled : if it's obvious, we allow unqualified refs\n     */\n\n    /*\n     * Restriction 17.s :: SubQuery cannot use the same table alias as one used in\n     * the Outer Query.\n     */\n    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(fromClause);\n    String sharedAlias = null;\n    for(String s : sqAliases ) {\n      if ( outerQryAliases.contains(s) ) {\n        sharedAlias = s;\n      }\n    }\n    if ( sharedAlias != null) {\n      ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n    }\n\n    /*\n     * Check.5.h :: For In and Not In the SubQuery must implicitly or\n     * explicitly only contain one select item.\n     */\n    if ( operator.getType() != SubQueryType.EXISTS &&\n        operator.getType() != SubQueryType.NOT_EXISTS &&\n        selectClause.getChildCount() - selectExprStart > 1 ) {\n      subQueryAST.setOrigin(originalSQASTOrigin);\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"SubQuery can contain only 1 item in Select List.\"));\n    }\n\n    containsAggregationExprs = false;\n    boolean containsWindowing = false;\n    for(int i= selectExprStart; i < selectClause.getChildCount(); i++ ) {\n\n      ASTNode selectItem = (ASTNode) selectClause.getChild(i);\n      int r = SubQueryUtils.checkAggOrWindowing(selectItem);\n\n      containsWindowing = containsWindowing | ( r == 2);\n      containsAggregationExprs = containsAggregationExprs | ( r == 1 );\n    }\n\n    rewrite(outerQueryRR, forHavingClause, outerQueryAlias, insertClause, selectClause);\n\n    /*\n     * Restriction.13.m :: In the case of an implied Group By on a\n     * correlated SubQuery, the SubQuery always returns 1 row.\n     * An exists on a SubQuery with an implied GBy will always return true.\n     * Whereas Algebraically transforming to a Join may not return true. See\n     * Specification doc for details.\n     * Similarly a not exists on a SubQuery with a implied GBY will always return false.\n     */\n    if ( operator.getType() == SubQueryType.EXISTS  &&\n        containsAggregationExprs &&\n        groupbyAddedToSQ ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST,\n          \"An Exists predicate on SubQuery with implicit Aggregation(no Group By clause) \" +\n          \"cannot be rewritten. (predicate will always return true).\"));\n    }\n    if ( operator.getType() == SubQueryType.NOT_EXISTS  &&\n        containsAggregationExprs &&\n        groupbyAddedToSQ ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST,\n          \"A Not Exists predicate on SubQuery with implicit Aggregation(no Group By clause) \" +\n          \"cannot be rewritten. (predicate will always return false).\"));\n    }\n\n    /*\n     * Restriction.14.h :: Correlated Sub Queries cannot contain Windowing clauses.\n     */\n    if ( containsWindowing && hasCorrelation ) {\n      throw new SemanticException(ErrorMsg.UNSUPPORTED_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"Correlated Sub Queries cannot contain Windowing clauses.\"));\n    }\n\n    /*\n     * Check.4.h :: For Exists and Not Exists, the Sub Query must\n     * have 1 or more correlated predicates.\n     */\n    if ( ( operator.getType() == SubQueryType.EXISTS ||\n        operator.getType() == SubQueryType.NOT_EXISTS ) &&\n        !hasCorrelation ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"For Exists/Not Exists operator SubQuery must be Correlated.\"));\n    }\n\n  }",
            " 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719 +\n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  ",
            "  void validateAndRewriteAST(RowResolver outerQueryRR,\n      boolean forHavingClause,\n      String outerQueryAlias,\n      Set<String> outerQryAliases) throws SemanticException {\n\n    ASTNode fromClause = getChildFromSubqueryAST(\"From\", HiveParser.TOK_FROM);\n    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n\n    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n\n    int selectExprStart = 0;\n    if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n      selectExprStart = 1;\n    }\n\n    /*\n     * Restriction.16.s :: Correlated Expression in Outer Query must not contain\n     * unqualified column references.\n     * disabled : if it's obvious, we allow unqualified refs\n     */\n\n    /*\n     * Restriction 17.s :: SubQuery cannot use the same table alias as one used in\n     * the Outer Query.\n     */\n    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(fromClause);\n    String sharedAlias = null;\n    for(String s : sqAliases ) {\n      if ( outerQryAliases.contains(s) ) {\n        sharedAlias = s;\n      }\n    }\n    if ( sharedAlias != null) {\n      ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n    }\n\n    /*\n     * Check.5.h :: For In and Not In the SubQuery must implicitly or\n     * explicitly only contain one select item.\n     */\n    if ( operator.getType() != SubQueryType.EXISTS &&\n        operator.getType() != SubQueryType.NOT_EXISTS &&\n        selectClause.getChildCount() - selectExprStart > 1 ) {\n      subQueryAST.setOrigin(originalSQASTOrigin);\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"SubQuery can contain only 1 item in Select List.\"));\n    }\n\n    containsAggregationExprs = false;\n    boolean containsWindowing = false;\n    for(int i= selectExprStart; i < selectClause.getChildCount(); i++ ) {\n\n      ASTNode selectItem = (ASTNode) selectClause.getChild(i);\n      int r = SubQueryUtils.checkAggOrWindowing(selectItem);\n\n      containsWindowing = containsWindowing | ( r == 3);\n      containsAggregationExprs = containsAggregationExprs | ( r == 1 );\n    }\n\n    rewrite(outerQueryRR, forHavingClause, outerQueryAlias, insertClause, selectClause);\n\n    /*\n     * Restriction.13.m :: In the case of an implied Group By on a\n     * correlated SubQuery, the SubQuery always returns 1 row.\n     * An exists on a SubQuery with an implied GBy will always return true.\n     * Whereas Algebraically transforming to a Join may not return true. See\n     * Specification doc for details.\n     * Similarly a not exists on a SubQuery with a implied GBY will always return false.\n     */\n    if ( operator.getType() == SubQueryType.EXISTS  &&\n        containsAggregationExprs &&\n        groupbyAddedToSQ ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST,\n          \"An Exists predicate on SubQuery with implicit Aggregation(no Group By clause) \" +\n          \"cannot be rewritten. (predicate will always return true).\"));\n    }\n    if ( operator.getType() == SubQueryType.NOT_EXISTS  &&\n        containsAggregationExprs &&\n        groupbyAddedToSQ ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST,\n          \"A Not Exists predicate on SubQuery with implicit Aggregation(no Group By clause) \" +\n          \"cannot be rewritten. (predicate will always return false).\"));\n    }\n\n    /*\n     * Restriction.14.h :: Correlated Sub Queries cannot contain Windowing clauses.\n     */\n    if ( containsWindowing && hasCorrelation ) {\n      throw new SemanticException(ErrorMsg.UNSUPPORTED_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"Correlated Sub Queries cannot contain Windowing clauses.\"));\n    }\n\n    /*\n     * Check.4.h :: For Exists and Not Exists, the Sub Query must\n     * have 1 or more correlated predicates.\n     */\n    if ( ( operator.getType() == SubQueryType.EXISTS ||\n        operator.getType() == SubQueryType.NOT_EXISTS ) &&\n        !hasCorrelation ) {\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n          subQueryAST, \"For Exists/Not Exists operator SubQuery must be Correlated.\"));\n    }\n\n  }"
        ],
        [
            "TestQBSubQuery::testCheckAggOrWindowing()",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126  ",
            "  @Test\n  public void testCheckAggOrWindowing() throws Exception {\n    ASTNode ast = parse(QUERY3);\n    ASTNode select = select(ast);\n\n    Assert.assertEquals(0, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(0)));\n    Assert.assertEquals(1, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(1)));\n    Assert.assertEquals(2, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(2)));\n  }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  ",
            "  @Test\n  public void testCheckAggOrWindowing() throws Exception {\n    ASTNode ast = parse(QUERY3);\n    ASTNode select = select(ast);\n\n    Assert.assertEquals(0, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(0)));\n    Assert.assertEquals(1, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(1)));\n    Assert.assertEquals(3, SubQueryUtils.checkAggOrWindowing((ASTNode) select.getChild(2)));\n  }"
        ],
        [
            "GenericUDFSQCountCheck::initialize(ObjectInspector)",
            "  47  \n  48  \n  49 -\n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length != 1) {\n      throw new UDFArgumentLengthException(\n          \"Invalid scalar subquery expression. Subquery count check expected one argument but received: \" + arguments.length);\n    }\n\n    converters[0] = ObjectInspectorConverters.getConverter(arguments[0],\n            PrimitiveObjectInspectorFactory.writableLongObjectInspector);\n\n    ObjectInspector outputOI = null;\n    outputOI = PrimitiveObjectInspectorFactory.writableLongObjectInspector;\n    return outputOI;\n  }",
            "  47  \n  48  \n  49 +\n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length > 2) {\n      throw new UDFArgumentLengthException(\n          \"Invalid scalar subquery expression. Subquery count check expected two argument but received: \" + arguments.length);\n    }\n\n    converters[0] = ObjectInspectorConverters.getConverter(arguments[0],\n            PrimitiveObjectInspectorFactory.writableLongObjectInspector);\n\n    ObjectInspector outputOI = null;\n    outputOI = PrimitiveObjectInspectorFactory.writableLongObjectInspector;\n    return outputOI;\n  }"
        ],
        [
            "GenericUDFSQCountCheck::evaluate(DeferredObject)",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69 -\n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  ",
            "  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    Object valObject = arguments[0].get();\n    assert(valObject != null);\n    Long val = getLongValue(arguments, 0, converters);\n    assert(val >= 0);\n    if(val > 1) {\n      throw new UDFArgumentException(\n              \" Scalar subquery expression returns more than one row.\");\n    }\n\n    resultLong.set(val);\n    return resultLong;\n  }",
            "  62  \n  63  \n  64  \n  65  \n  66 +\n  67  \n  68  \n  69 +\n  70 +\n  71 +\n  72 +\n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  ",
            "  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    Object valObject = arguments[0].get();\n    assert(valObject != null);\n\n    Long val = getLongValue(arguments, 0, converters);\n    assert(val >= 0);\n\n    switch (arguments.length){\n      case 1: //Scalar queries, should expect value/count less than 1\n        if (val > 1) {\n          throw new UDFArgumentException(\n                  \" Scalar subquery expression returns more than one row.\");\n        }\n        break;\n      case 2:\n        if (val == 0) { // IN/NOT IN subqueries with aggregate\n          throw new UDFArgumentException(\n                  \" IN/NOT IN subquery with aggregate returning zero result. Currently this is not supported.\");\n        }\n        break;\n    }\n\n    resultLong.set(val);\n    return resultLong;\n  }"
        ],
        [
            "QBSubQuery::get(ASTNode)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  ",
            "    public static SubQueryType get(ASTNode opNode) throws SemanticException {\n      if(opNode == null) {\n        return SCALAR;\n      }\n\n      switch(opNode.getType()) {\n      case HiveParser.KW_EXISTS:\n        return EXISTS;\n      case HiveParser.TOK_SUBQUERY_OP_NOTEXISTS:\n        return NOT_EXISTS;\n      case HiveParser.KW_IN:\n        return IN;\n      case HiveParser.TOK_SUBQUERY_OP_NOTIN:\n        return NOT_IN;\n      default:\n        throw new SemanticException(SemanticAnalyzer.generateErrorMessage(opNode,\n            \"Operator not supported in SubQuery use.\"));\n      }\n    }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57 +\n  58  \n  59 +\n  60 +\n  61 +\n  62 +\n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69 +\n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "    public static SubQueryType get(ASTNode opNode) throws SemanticException {\n      if(opNode == null) {\n        return SCALAR;\n      }\n\n      switch(opNode.getType()) {\n        // opNode's type is always either KW_EXISTS or KW_IN never NOTEXISTS or NOTIN\n        //  to figure this out we need to check it's grand parent's parent\n      case HiveParser.KW_EXISTS:\n        if(opNode.getParent().getParent().getParent() != null\n                && opNode.getParent().getParent().getParent().getType() == HiveParser.KW_NOT) {\n          return NOT_EXISTS;\n        }\n        return EXISTS;\n      case HiveParser.TOK_SUBQUERY_OP_NOTEXISTS:\n        return NOT_EXISTS;\n      case HiveParser.KW_IN:\n        if(opNode.getParent().getParent().getParent() != null\n                && opNode.getParent().getParent().getParent().getType() == HiveParser.KW_NOT) {\n          return NOT_IN;\n        }\n        return IN;\n      case HiveParser.TOK_SUBQUERY_OP_NOTIN:\n        return NOT_IN;\n      default:\n        throw new SemanticException(SemanticAnalyzer.generateErrorMessage(opNode,\n            \"Operator not supported in SubQuery use.\"));\n      }\n    }"
        ],
        [
            "QBSubQuery::subqueryRestrictionsCheck(RowResolver,boolean,String)",
            " 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 -\n 553 -\n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606 -\n 607 -\n 608 -\n 609 -\n 610 -\n 611 -\n 612 -\n 613 -\n 614 -\n 615 -\n 616 -\n 617 -\n 618 -\n 619  \n 620  \n 621 -\n 622  \n 623 -\n 624  \n 625  \n 626 -\n 627  \n 628 -\n 629  \n 630  ",
            "  /**\n   * @param parentQueryRR\n   * @param forHavingClause\n   * @param outerQueryAlias\n   * @return true if it is correlated scalar subquery with an aggregate\n   * @throws SemanticException\n   */\n  boolean subqueryRestrictionsCheck(RowResolver parentQueryRR,\n                                 boolean forHavingClause,\n                                 String outerQueryAlias)\n          throws SemanticException {\n    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n\n    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n\n\n    int selectExprStart = 0;\n    if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n      selectExprStart = 1;\n    }\n\n    /*\n     * Check.5.h :: For In and Not In the SubQuery must implicitly or\n     * explicitly only contain one select item.\n     */\n    if ( operator.getType() != SubQueryType.EXISTS &&\n            operator.getType() != SubQueryType.NOT_EXISTS &&\n            selectClause.getChildCount() - selectExprStart > 1 ) {\n      subQueryAST.setOrigin(originalSQASTOrigin);\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n              subQueryAST, \"SubQuery can contain only 1 item in Select List.\"));\n    }\n\n    boolean hasAggreateExprs = false;\n    boolean hasWindowing = false;\n    for(int i= selectExprStart; i < selectClause.getChildCount(); i++ ) {\n\n      ASTNode selectItem = (ASTNode) selectClause.getChild(i);\n      int r = SubQueryUtils.checkAggOrWindowing(selectItem);\n\n      hasWindowing = hasWindowing | ( r == 2);\n      hasAggreateExprs = hasAggreateExprs | ( r == 1 );\n    }\n\n\n\n    ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n\n    if ( whereClause == null ) {\n      return false;\n    }\n    ASTNode searchCond = (ASTNode) whereClause.getChild(0);\n    List<ASTNode> conjuncts = new ArrayList<ASTNode>();\n    SubQueryUtils.extractConjuncts(searchCond, conjuncts);\n\n    ConjunctAnalyzer conjunctAnalyzer = new ConjunctAnalyzer(parentQueryRR,\n            forHavingClause, outerQueryAlias);\n\n    boolean hasCorrelation = false;\n    boolean hasNonEquiJoinPred = false;\n    for(ASTNode conjunctAST : conjuncts) {\n      Conjunct conjunct = conjunctAnalyzer.analyzeConjunct(conjunctAST);\n      if(conjunct.isCorrelated()){\n       hasCorrelation = true;\n      }\n      if ( conjunct.eitherSideRefersBoth() && conjunctAST.getType() != HiveParser.EQUAL) {\n        hasNonEquiJoinPred = true;\n      }\n    }\n    boolean noImplicityGby = true;\n    if ( insertClause.getChild(1).getChildCount() > 3 &&\n            insertClause.getChild(1).getChild(3).getType() == HiveParser.TOK_GROUPBY ) {\n      if((ASTNode) insertClause.getChild(1).getChild(3) != null){\n        noImplicityGby = false;\n      }\n    }\n\n    /*\n     * Restriction.14.h :: Correlated Sub Queries cannot contain Windowing clauses.\n     */\n    if (  hasWindowing && hasCorrelation) {\n      throw new CalciteSubquerySemanticException(ErrorMsg.UNSUPPORTED_SUBQUERY_EXPRESSION.getMsg(\n              subQueryAST, \"Correlated Sub Queries cannot contain Windowing clauses.\"));\n    }\n\n    /*\n     * Restriction.13.m :: In the case of an implied Group By on a\n     * correlated SubQuery, the SubQuery always returns 1 row.\n     * An exists on a SubQuery with an implied GBy will always return true.\n     * Whereas Algebraically transforming to a Join may not return true. See\n     * Specification doc for details.\n     * Similarly a not exists on a SubQuery with a implied GBY will always return false.\n     */\n      if (hasAggreateExprs &&\n              noImplicityGby ) {\n\n        if( hasCorrelation && (operator.getType() == SubQueryType.EXISTS\n                || operator.getType() == SubQueryType.NOT_EXISTS\n                || operator.getType() == SubQueryType.IN\n                || operator.getType() == SubQueryType.NOT_IN)) {\n          throw new CalciteSubquerySemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n                subQueryAST,\n                \"A predicate on EXISTS/NOT EXISTS/IN/NOT IN SubQuery with implicit Aggregation(no Group By clause) \" +\n                        \"cannot be rewritten.\"));\n        }\n        else if(operator.getType() == SubQueryType.SCALAR && hasNonEquiJoinPred) {\n          // throw an error if predicates are not equal\n            throw new CalciteSubquerySemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n                    subQueryAST,\n                    \"Scalar subqueries with aggregate cannot have non-equi join predicate\"));\n        }\n        else if(operator.getType() == SubQueryType.SCALAR && hasCorrelation) {\n            return true;\n        }\n\n      }\n\n    return false;\n  }",
            " 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 +\n 558 +\n 559 +\n 560 +\n 561  \n 562  \n 563  \n 564  \n 565  \n 566 +\n 567 +\n 568 +\n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620 +\n 621 +\n 622 +\n 623 +\n 624 +\n 625 +\n 626 +\n 627 +\n 628  \n 629 +\n 630 +\n 631 +\n 632 +\n 633 +\n 634  \n 635  \n 636 +\n 637 +\n 638 +\n 639 +\n 640 +\n 641 +\n 642 +\n 643 +\n 644 +\n 645 +\n 646 +\n 647 +\n 648 +\n 649  \n 650 +\n 651 +\n 652  \n 653 +\n 654 +\n 655 +\n 656 +\n 657 +\n 658 +\n 659  \n 660  \n 661  \n 662  ",
            "  /**\n   * @param parentQueryRR\n   * @param forHavingClause\n   * @param outerQueryAlias\n   * @return true if it is correlated scalar subquery with an aggregate\n   * @throws SemanticException\n   */\n  boolean subqueryRestrictionsCheck(RowResolver parentQueryRR,\n                                 boolean forHavingClause,\n                                 String outerQueryAlias)\n          throws SemanticException {\n    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n\n    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n\n\n    int selectExprStart = 0;\n    if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n      selectExprStart = 1;\n    }\n\n    /*\n     * Check.5.h :: For In and Not In the SubQuery must implicitly or\n     * explicitly only contain one select item.\n     */\n    if ( operator.getType() != SubQueryType.EXISTS &&\n            operator.getType() != SubQueryType.NOT_EXISTS &&\n            selectClause.getChildCount() - selectExprStart > 1 ) {\n      subQueryAST.setOrigin(originalSQASTOrigin);\n      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n              subQueryAST, \"SubQuery can contain only 1 item in Select List.\"));\n    }\n\n    boolean hasAggreateExprs = false;\n    boolean hasWindowing = false;\n\n    // we need to know if aggregate is COUNT since IN corr subq with count aggregate\n    // is not special cased later in subquery remove rule\n    boolean hasCount = false;\n    for(int i= selectExprStart; i < selectClause.getChildCount(); i++ ) {\n\n      ASTNode selectItem = (ASTNode) selectClause.getChild(i);\n      int r = SubQueryUtils.checkAggOrWindowing(selectItem);\n\n      hasWindowing = hasWindowing | ( r == 3);\n      hasAggreateExprs = hasAggreateExprs | ( r == 1 | r== 2 );\n      hasCount = hasCount | ( r == 2 );\n    }\n\n\n\n    ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n\n    if ( whereClause == null ) {\n      return false;\n    }\n    ASTNode searchCond = (ASTNode) whereClause.getChild(0);\n    List<ASTNode> conjuncts = new ArrayList<ASTNode>();\n    SubQueryUtils.extractConjuncts(searchCond, conjuncts);\n\n    ConjunctAnalyzer conjunctAnalyzer = new ConjunctAnalyzer(parentQueryRR,\n            forHavingClause, outerQueryAlias);\n\n    boolean hasCorrelation = false;\n    boolean hasNonEquiJoinPred = false;\n    for(ASTNode conjunctAST : conjuncts) {\n      Conjunct conjunct = conjunctAnalyzer.analyzeConjunct(conjunctAST);\n      if(conjunct.isCorrelated()){\n       hasCorrelation = true;\n      }\n      if ( conjunct.eitherSideRefersBoth() && conjunctAST.getType() != HiveParser.EQUAL) {\n        hasNonEquiJoinPred = true;\n      }\n    }\n    boolean noImplicityGby = true;\n    if ( insertClause.getChild(1).getChildCount() > 3 &&\n            insertClause.getChild(1).getChild(3).getType() == HiveParser.TOK_GROUPBY ) {\n      if((ASTNode) insertClause.getChild(1).getChild(3) != null){\n        noImplicityGby = false;\n      }\n    }\n\n    /*\n     * Restriction.14.h :: Correlated Sub Queries cannot contain Windowing clauses.\n     */\n    if (  hasWindowing && hasCorrelation) {\n      throw new CalciteSubquerySemanticException(ErrorMsg.UNSUPPORTED_SUBQUERY_EXPRESSION.getMsg(\n              subQueryAST, \"Correlated Sub Queries cannot contain Windowing clauses.\"));\n    }\n\n    /*\n     * Restriction.13.m :: In the case of an implied Group By on a\n     * correlated SubQuery, the SubQuery always returns 1 row.\n     * An exists on a SubQuery with an implied GBy will always return true.\n     * Whereas Algebraically transforming to a Join may not return true. See\n     * Specification doc for details.\n     * Similarly a not exists on a SubQuery with a implied GBY will always return false.\n     */\n      // Following is special cases for different type of subqueries which have aggregate and no implicit group by\n      // and are correlatd\n      // * EXISTS/NOT EXISTS - NOT allowed, throw an error for now. We plan to allow this later\n      // * SCALAR - only allow if it has non equi join predicate. This should return true since later in subquery remove\n      //              rule we need to know about this case.\n      // * IN - always allowed, BUT returns true for cases with aggregate other than COUNT since later in subquery remove\n      //        rule we need to know about this case.\n      // * NOT IN - always allow, but always return true because later subq remove rule will generate diff plan for this case\n      if (hasAggreateExprs &&\n              noImplicityGby) {\n\n        if(operator.getType() == SubQueryType.EXISTS\n                || operator.getType() == SubQueryType.NOT_EXISTS) {\n          if(hasCorrelation) {\n            throw new CalciteSubquerySemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n                    subQueryAST,\n                    \"A predicate on EXISTS/NOT EXISTS SubQuery with implicit Aggregation(no Group By clause) \" +\n                            \"cannot be rewritten.\"));\n          }\n        }\n        else if(operator.getType() == SubQueryType.SCALAR) {\n            if(hasNonEquiJoinPred) {\n              throw new CalciteSubquerySemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n                      subQueryAST,\n                      \"Scalar subqueries with aggregate cannot have non-equi join predicate\"));\n            }\n            if(hasCorrelation) {\n              return true;\n            }\n        }\n        else if(operator.getType() == SubQueryType.IN) {\n          if(hasCount && hasCorrelation) {\n            return true;\n          }\n        }\n        else if (operator.getType() == SubQueryType.NOT_IN) {\n            if(hasCorrelation) {\n              return true;\n            }\n        }\n      }\n    return false;\n  }"
        ]
    ],
    "4becd689d59ee3f75a36119fbb950c44e16c65df": [
        [
            "HiveSubQueryRemoveRule::apply(RexSubQuery,Set,RelOptUtil,HiveSubQRemoveRelBuilder,int,int,boolean)",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 -\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 -\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "    protected RexNode apply(RexSubQuery e, Set<CorrelationId> variablesSet,\n                            RelOptUtil.Logic logic,\n                            HiveSubQRemoveRelBuilder builder, int inputCount, int offset,\n                            boolean isCorrScalarAgg) {\n        switch (e.getKind()) {\n            case SCALAR_QUERY:\n                if(isCorrScalarAgg) {\n                    // Transformation :\n                    // Outer Query Left Join (inner query) on correlated predicate and preserve rows only from left side.\n                    builder.push(e.rel);\n                    final List<RexNode> parentQueryFields = new ArrayList<>();\n                    parentQueryFields.addAll(builder.fields());\n\n                    // id is appended since there could be multiple scalar subqueries and FILTER\n                    // is created using field name\n                    String indicator = \"alwaysTrue\" + e.rel.getId();\n                    parentQueryFields.add(builder.alias(builder.literal(true), indicator));\n                    builder.project(parentQueryFields);\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n\n                    final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                    RexNode literal;\n                    if(isAggZeroOnEmpty(e)) {\n                        literal = builder.literal(0);\n                    }\n                    else {\n                        literal = e.rel.getCluster().getRexBuilder().makeNullLiteral(getAggTypeForScalarSub(e));\n                    }\n                    operands.add((builder.isNull(builder.field(indicator))), literal);\n                    operands.add(field(builder, 1, builder.fields().size()-2));\n                    return builder.call(SqlStdOperatorTable.CASE, operands.build());\n                }\n\n                //Transformation is to left join for correlated predicates and inner join otherwise,\n                // but do a count on inner side before that to make sure it generates atmost 1 row.\n                builder.push(e.rel);\n                // returns single row/column\n                builder.aggregate(builder.groupKey(),\n                        builder.count(false, \"cnt\"));\n\n                SqlFunction countCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                        InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                // we create FILTER (sq_count_check(count()) <= 1) instead of PROJECT because RelFieldTrimmer\n                //  ends up getting rid of Project since it is not used further up the tree\n                builder.filter(builder.call(SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n                        builder.call(countCheck, builder.field(\"cnt\")),\n                        builder.literal(1)));\n\n                if( !variablesSet.isEmpty())\n                {\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                }\n                else\n                    builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                builder.push(e.rel);\n                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                offset++;\n                return field(builder, inputCount, offset);\n\n            case IN:\n            case EXISTS:\n                // Most general case, where the left and right keys might have nulls, and\n                // caller requires 3-valued logic return.\n                //\n                // select e.deptno, e.deptno in (select deptno from emp)\n                //\n                // becomes\n                //\n                // select e.deptno,\n                //   case\n                //   when ct.c = 0 then false\n                //   when dt.i is not null then true\n                //   when e.deptno is null then null\n                //   when ct.ck < ct.c then null\n                //   else false\n                //   end\n                // from e\n                // left join (\n                //   (select count(*) as c, count(deptno) as ck from emp) as ct\n                //   cross join (select distinct deptno, true as i from emp)) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // If keys are not null we can remove \"ct\" and simplify to\n                //\n                // select e.deptno,\n                //   case\n                //   when dt.i is not null then true\n                //   else false\n                //   end\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // We could further simplify to\n                //\n                // select e.deptno,\n                //   dt.i is not null\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // but have not yet.\n                //\n                // If the logic is TRUE we can just kill the record if the condition\n                // evaluates to FALSE or UNKNOWN. Thus the query simplifies to an inner\n                // join:\n                //\n                // select e.deptno,\n                //   true\n                // from e\n                // inner join (select distinct deptno from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n\n                builder.push(e.rel);\n                final List<RexNode> fields = new ArrayList<>();\n                switch (e.getKind()) {\n                    case IN:\n                        fields.addAll(builder.fields());\n                        // Transformation: sq_count_check(count(*), true) FILTER is generated on top\n                        //  of subquery which is then joined (LEFT or INNER) with outer query\n                        //  This transformation is done to add run time check using sq_count_check to\n                        //  throw an error if subquery is producing zero row, since with aggregate this\n                        //  will produce wrong results (because we further rewrite such queries into JOIN)\n                        if(isCorrScalarAgg) {\n                            // returns single row/column\n                            builder.aggregate(builder.groupKey(),\n                                    builder.count(false, \"cnt_in\"));\n\n                            if (!variablesSet.isEmpty()) {\n                                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                            } else {\n                                builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            }\n\n                            SqlFunction inCountCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                                    InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                            // we create FILTER (sq_count_check(count()) > 0) instead of PROJECT because RelFieldTrimmer\n                            //  ends up getting rid of Project since it is not used further up the tree\n                            builder.filter(builder.call(SqlStdOperatorTable.GREATER_THAN,\n                                    //true here indicates that sq_count_check is for IN/NOT IN subqueries\n                                    builder.call(inCountCheck, builder.field(\"cnt_in\"), builder.literal(true)),\n                                    builder.literal(0)));\n                            offset =  offset + 1;\n                            builder.push(e.rel);\n                        }\n                }\n\n                // First, the cross join\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        // Since EXISTS/NOT EXISTS are not affected by presence of\n                        // null keys we do not need to generate count(*), count(c)\n                        if (e.getKind() == SqlKind.EXISTS) {\n                            logic = RelOptUtil.Logic.TRUE_FALSE;\n                            break;\n                        }\n                        builder.aggregate(builder.groupKey(),\n                                builder.count(false, \"c\"),\n                                builder.aggregateCall(SqlStdOperatorTable.COUNT, false, null, \"ck\",\n                                        builder.fields()));\n                        builder.as(\"ct\");\n                        if( !variablesSet.isEmpty())\n                        {\n                            //builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                        }\n                        else\n                            builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n\n                        offset += 2;\n                        builder.push(e.rel);\n                        break;\n                }\n\n                // Now the left join\n                switch (logic) {\n                    case TRUE:\n                        if (fields.isEmpty()) {\n                            builder.project(builder.alias(builder.literal(true), \"i\"));\n                            builder.aggregate(builder.groupKey(0));\n                        } else {\n                            builder.aggregate(builder.groupKey(fields));\n                        }\n                        break;\n                    default:\n                        fields.add(builder.alias(builder.literal(true), \"i\"));\n                        builder.project(fields);\n                        builder.distinct();\n                }\n                builder.as(\"dt\");\n                final List<RexNode> conditions = new ArrayList<>();\n                for (Pair<RexNode, RexNode> pair\n                        : Pair.zip(e.getOperands(), builder.fields())) {\n                    conditions.add(\n                            builder.equals(pair.left, RexUtil.shift(pair.right, offset)));\n                }\n                switch (logic) {\n                    case TRUE:\n                        builder.join(JoinRelType.INNER, builder.and(conditions), variablesSet);\n                        return builder.literal(true);\n                }\n                builder.join(JoinRelType.LEFT, builder.and(conditions), variablesSet);\n\n                final List<RexNode> keyIsNulls = new ArrayList<>();\n                for (RexNode operand : e.getOperands()) {\n                    if (operand.getType().isNullable()) {\n                        keyIsNulls.add(builder.isNull(operand));\n                    }\n                }\n                final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.equals(builder.field(\"ct\", \"c\"), builder.literal(0)),\n                                builder.literal(false));\n                        //now that we are using LEFT OUTER JOIN to join inner count, count(*)\n                        // with outer table, we wouldn't be able to tell if count is zero\n                        // for inner table since inner join with correlated values will get rid\n                        // of all values where join cond is not true (i.e where actual inner table\n                        // will produce zero result). To  handle this case we need to check both\n                        // count is zero or count is null\n                        operands.add((builder.isNull(builder.field(\"ct\", \"c\"))), builder.literal(false));\n                        break;\n                }\n                operands.add(builder.isNotNull(builder.field(\"dt\", \"i\")),\n                        builder.literal(true));\n                if (!keyIsNulls.isEmpty()) {\n                    //Calcite creates null literal with Null type here but because HIVE doesn't support null type\n                    // it is appropriately typed boolean\n                    operands.add(builder.or(keyIsNulls), e.rel.getCluster().getRexBuilder().makeNullLiteral(SqlTypeName.BOOLEAN));\n                    // we are creating filter here so should not be returning NULL. Not sure why Calcite return NULL\n                    //operands.add(builder.or(keyIsNulls), builder.literal(false));\n                }\n                Boolean b = true;\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                        b = null;\n                        // fall through\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.call(SqlStdOperatorTable.LESS_THAN,\n                                        builder.field(\"ct\", \"ck\"), builder.field(\"ct\", \"c\")),\n                                builder.literal(b));\n                        break;\n                }\n                operands.add(builder.literal(false));\n                return builder.call(SqlStdOperatorTable.CASE, operands.build());\n\n            default:\n                throw new AssertionError(e.getKind());\n        }\n    }",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 +\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "    protected RexNode apply(RexSubQuery e, Set<CorrelationId> variablesSet,\n                            RelOptUtil.Logic logic,\n                            HiveSubQRemoveRelBuilder builder, int inputCount, int offset,\n                            boolean isCorrScalarAgg) {\n        switch (e.getKind()) {\n            case SCALAR_QUERY:\n                if(isCorrScalarAgg) {\n                    // Transformation :\n                    // Outer Query Left Join (inner query) on correlated predicate and preserve rows only from left side.\n                    builder.push(e.rel);\n                    final List<RexNode> parentQueryFields = new ArrayList<>();\n                    parentQueryFields.addAll(builder.fields());\n\n                    // id is appended since there could be multiple scalar subqueries and FILTER\n                    // is created using field name\n                    String indicator = \"alwaysTrue\" + e.rel.getId();\n                    parentQueryFields.add(builder.alias(builder.literal(true), indicator));\n                    builder.project(parentQueryFields);\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n\n                    final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                    RexNode literal;\n                    if(isAggZeroOnEmpty(e)) {\n                        literal = builder.literal(0);\n                    }\n                    else {\n                        literal = e.rel.getCluster().getRexBuilder().makeNullLiteral(getAggTypeForScalarSub(e));\n                    }\n                    operands.add((builder.isNull(builder.field(indicator))), literal);\n                    operands.add(field(builder, 1, builder.fields().size()-2));\n                    return builder.call(SqlStdOperatorTable.CASE, operands.build());\n                }\n\n                //Transformation is to left join for correlated predicates and inner join otherwise,\n                // but do a count on inner side before that to make sure it generates atmost 1 row.\n                builder.push(e.rel);\n                // returns single row/column\n                builder.aggregate(builder.groupKey(),\n                        builder.count(false, \"cnt\"));\n\n                SqlFunction countCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                        InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                // we create FILTER (sq_count_check(count()) <= 1) instead of PROJECT because RelFieldTrimmer\n                //  ends up getting rid of Project since it is not used further up the tree\n                builder.filter(builder.call(SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n                        builder.call(countCheck, builder.field(\"cnt\")),\n                        builder.literal(1)));\n\n                if( !variablesSet.isEmpty())\n                {\n                    builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                }\n                else\n                    builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                builder.push(e.rel);\n                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                offset++;\n                return field(builder, inputCount, offset);\n\n            case IN:\n            case EXISTS:\n                // Most general case, where the left and right keys might have nulls, and\n                // caller requires 3-valued logic return.\n                //\n                // select e.deptno, e.deptno in (select deptno from emp)\n                //\n                // becomes\n                //\n                // select e.deptno,\n                //   case\n                //   when ct.c = 0 then false\n                //   when dt.i is not null then true\n                //   when e.deptno is null then null\n                //   when ct.ck < ct.c then null\n                //   else false\n                //   end\n                // from e\n                // left join (\n                //   (select count(*) as c, count(deptno) as ck from emp) as ct\n                //   cross join (select distinct deptno, true as i from emp)) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // If keys are not null we can remove \"ct\" and simplify to\n                //\n                // select e.deptno,\n                //   case\n                //   when dt.i is not null then true\n                //   else false\n                //   end\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // We could further simplify to\n                //\n                // select e.deptno,\n                //   dt.i is not null\n                // from e\n                // left join (select distinct deptno, true as i from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n                // but have not yet.\n                //\n                // If the logic is TRUE we can just kill the record if the condition\n                // evaluates to FALSE or UNKNOWN. Thus the query simplifies to an inner\n                // join:\n                //\n                // select e.deptno,\n                //   true\n                // from e\n                // inner join (select distinct deptno from emp) as dt\n                //   on e.deptno = dt.deptno\n                //\n\n                builder.push(e.rel);\n                final List<RexNode> fields = new ArrayList<>();\n                switch (e.getKind()) {\n                    case IN:\n                        fields.addAll(builder.fields());\n                        // Transformation: sq_count_check(count(*), true) FILTER is generated on top\n                        //  of subquery which is then joined (LEFT or INNER) with outer query\n                        //  This transformation is done to add run time check using sq_count_check to\n                        //  throw an error if subquery is producing zero row, since with aggregate this\n                        //  will produce wrong results (because we further rewrite such queries into JOIN)\n                        if(isCorrScalarAgg) {\n                            // returns single row/column\n                            builder.aggregate(builder.groupKey(),\n                                    builder.count(false, \"cnt_in\"));\n\n                            if (!variablesSet.isEmpty()) {\n                                builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                            } else {\n                                builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            }\n\n                            SqlFunction inCountCheck = new SqlFunction(\"sq_count_check\", SqlKind.OTHER_FUNCTION, ReturnTypes.BIGINT,\n                                    InferTypes.RETURN_TYPE, OperandTypes.NUMERIC, SqlFunctionCategory.USER_DEFINED_FUNCTION);\n\n                            // we create FILTER (sq_count_check(count()) > 0) instead of PROJECT because RelFieldTrimmer\n                            //  ends up getting rid of Project since it is not used further up the tree\n                            builder.filter(builder.call(SqlStdOperatorTable.GREATER_THAN,\n                                    //true here indicates that sq_count_check is for IN/NOT IN subqueries\n                                    builder.call(inCountCheck, builder.field(\"cnt_in\"), builder.literal(true)),\n                                    builder.literal(0)));\n                            offset =  offset + 1;\n                            builder.push(e.rel);\n                        }\n                }\n\n                // First, the cross join\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        // Since EXISTS/NOT EXISTS are not affected by presence of\n                        // null keys we do not need to generate count(*), count(c)\n                        if (e.getKind() == SqlKind.EXISTS) {\n                            logic = RelOptUtil.Logic.TRUE_FALSE;\n                            break;\n                        }\n                        builder.aggregate(builder.groupKey(),\n                                builder.count(false, \"c\"),\n                                builder.aggregateCall(SqlStdOperatorTable.COUNT, false, null, \"ck\",\n                                        builder.fields()));\n                        builder.as(\"ct\");\n                        if( !variablesSet.isEmpty())\n                        {\n                            //builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n                            builder.join(JoinRelType.LEFT, builder.literal(true), variablesSet);\n                        }\n                        else\n                            builder.join(JoinRelType.INNER, builder.literal(true), variablesSet);\n\n                        offset += 2;\n                        builder.push(e.rel);\n                        break;\n                }\n\n                // Now the left join\n                switch (logic) {\n                    case TRUE:\n                        if (fields.isEmpty()) {\n                            builder.project(builder.alias(builder.literal(true), \"i\" + e.rel.getId()));\n                            builder.aggregate(builder.groupKey(0));\n                        } else {\n                            builder.aggregate(builder.groupKey(fields));\n                        }\n                        break;\n                    default:\n                        fields.add(builder.alias(builder.literal(true), \"i\" + e.rel.getId()));\n                        builder.project(fields);\n                        builder.distinct();\n                }\n                builder.as(\"dt\");\n                final List<RexNode> conditions = new ArrayList<>();\n                for (Pair<RexNode, RexNode> pair\n                        : Pair.zip(e.getOperands(), builder.fields())) {\n                    conditions.add(\n                            builder.equals(pair.left, RexUtil.shift(pair.right, offset)));\n                }\n                switch (logic) {\n                    case TRUE:\n                        builder.join(JoinRelType.INNER, builder.and(conditions), variablesSet);\n                        return builder.literal(true);\n                }\n                builder.join(JoinRelType.LEFT, builder.and(conditions), variablesSet);\n\n                final List<RexNode> keyIsNulls = new ArrayList<>();\n                for (RexNode operand : e.getOperands()) {\n                    if (operand.getType().isNullable()) {\n                        keyIsNulls.add(builder.isNull(operand));\n                    }\n                }\n                final ImmutableList.Builder<RexNode> operands = ImmutableList.builder();\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.equals(builder.field(\"ct\", \"c\"), builder.literal(0)),\n                                builder.literal(false));\n                        //now that we are using LEFT OUTER JOIN to join inner count, count(*)\n                        // with outer table, we wouldn't be able to tell if count is zero\n                        // for inner table since inner join with correlated values will get rid\n                        // of all values where join cond is not true (i.e where actual inner table\n                        // will produce zero result). To  handle this case we need to check both\n                        // count is zero or count is null\n                        operands.add((builder.isNull(builder.field(\"ct\", \"c\"))), builder.literal(false));\n                        break;\n                }\n                operands.add(builder.isNotNull(builder.field(\"dt\", \"i\" + e.rel.getId())),\n                        builder.literal(true));\n                if (!keyIsNulls.isEmpty()) {\n                    //Calcite creates null literal with Null type here but because HIVE doesn't support null type\n                    // it is appropriately typed boolean\n                    operands.add(builder.or(keyIsNulls), e.rel.getCluster().getRexBuilder().makeNullLiteral(SqlTypeName.BOOLEAN));\n                    // we are creating filter here so should not be returning NULL. Not sure why Calcite return NULL\n                    //operands.add(builder.or(keyIsNulls), builder.literal(false));\n                }\n                Boolean b = true;\n                switch (logic) {\n                    case TRUE_FALSE_UNKNOWN:\n                        b = null;\n                        // fall through\n                    case UNKNOWN_AS_TRUE:\n                        operands.add(\n                                builder.call(SqlStdOperatorTable.LESS_THAN,\n                                        builder.field(\"ct\", \"ck\"), builder.field(\"ct\", \"c\")),\n                                builder.literal(b));\n                        break;\n                }\n                operands.add(builder.literal(false));\n                return builder.call(SqlStdOperatorTable.CASE, operands.build());\n\n            default:\n                throw new AssertionError(e.getKind());\n        }\n    }"
        ],
        [
            "HiveReduceExpressionsRule::FilterReduceExpressionsRule::onMatch(RelOptRuleCall)",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "    @Override public void onMatch(RelOptRuleCall call) {\n      final Filter filter = call.rel(0);\n      final List<RexNode> expList =\n          Lists.newArrayList(filter.getCondition());\n      RexNode newConditionExp;\n      boolean reduced;\n      final RelMetadataQuery mq = RelMetadataQuery.instance();\n      final RelOptPredicateList predicates =\n          mq.getPulledUpPredicates(filter.getInput());\n      if (reduceExpressions(filter, expList, predicates, true)) {\n        assert expList.size() == 1;\n        newConditionExp = expList.get(0);\n        reduced = true;\n      } else {\n        // No reduction, but let's still test the original\n        // predicate to see if it was already a constant,\n        // in which case we don't need any runtime decision\n        // about filtering.\n        newConditionExp = filter.getCondition();\n        reduced = false;\n      }\n\n      // Even if no reduction, let's still test the original\n      // predicate to see if it was already a constant,\n      // in which case we don't need any runtime decision\n      // about filtering.\n      if (newConditionExp.isAlwaysTrue()) {\n        call.transformTo(\n            filter.getInput());\n      } else if (reduced) {\n        if (RexUtil.isNullabilityCast(filter.getCluster().getTypeFactory(),\n            newConditionExp)) {\n          newConditionExp = ((RexCall) newConditionExp).getOperands().get(0);\n        }\n        call.transformTo(call.builder().\n            push(filter.getInput()).filter(newConditionExp).build());\n      } else {\n        if (newConditionExp instanceof RexCall) {\n          RexCall rexCall = (RexCall) newConditionExp;\n          boolean reverse = rexCall.getKind() == SqlKind.NOT;\n          if (reverse) {\n            if (!(rexCall.getOperands().get(0) instanceof RexCall)) {\n              // If child is not a RexCall instance, we can bail out\n              return;\n            }\n            rexCall = (RexCall) rexCall.getOperands().get(0);\n          }\n          reduceNotNullableFilter(call, filter, rexCall, reverse);\n        }\n        return;\n      }\n\n      // New plan is absolutely better than old plan.\n      call.getPlanner().setImportance(filter, 0.0);\n    }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    @Override public void onMatch(RelOptRuleCall call) {\n      final Filter filter = call.rel(0);\n      final List<RexNode> expList =\n          Lists.newArrayList(filter.getCondition());\n      RexNode newConditionExp;\n      boolean reduced;\n      final RelMetadataQuery mq = RelMetadataQuery.instance();\n      final RelOptPredicateList predicates =\n          mq.getPulledUpPredicates(filter.getInput());\n      if (reduceExpressions(filter, expList, predicates, true)) {\n        assert expList.size() == 1;\n        newConditionExp = expList.get(0);\n        reduced = true;\n      } else {\n        // No reduction, but let's still test the original\n        // predicate to see if it was already a constant,\n        // in which case we don't need any runtime decision\n        // about filtering.\n        newConditionExp = filter.getCondition();\n        reduced = false;\n      }\n\n      // Even if no reduction, let's still test the original\n      // predicate to see if it was already a constant,\n      // in which case we don't need any runtime decision\n      // about filtering.\n      if (newConditionExp.isAlwaysTrue()) {\n        call.transformTo(\n            filter.getInput());\n      } else if (reduced) {\n        if (RexUtil.isNullabilityCast(filter.getCluster().getTypeFactory(),\n            newConditionExp)) {\n          newConditionExp = ((RexCall) newConditionExp).getOperands().get(0);\n        }\n        // reduce might end up creating an expression with null type\n        // e.g condition(null = null) is reduced to condition (null) with null type\n        // since this is a condition which will always be boolean type we cast it to\n        // boolean type\n        if(newConditionExp.getType().getSqlTypeName() == SqlTypeName.NULL) {\n          newConditionExp = call.builder().cast(newConditionExp, SqlTypeName.BOOLEAN);\n        }\n        call.transformTo(call.builder().\n            push(filter.getInput()).filter(newConditionExp).build());\n      } else {\n        if (newConditionExp instanceof RexCall) {\n          RexCall rexCall = (RexCall) newConditionExp;\n          boolean reverse = rexCall.getKind() == SqlKind.NOT;\n          if (reverse) {\n            if (!(rexCall.getOperands().get(0) instanceof RexCall)) {\n              // If child is not a RexCall instance, we can bail out\n              return;\n            }\n            rexCall = (RexCall) rexCall.getOperands().get(0);\n          }\n          reduceNotNullableFilter(call, filter, rexCall, reverse);\n        }\n        return;\n      }\n\n      // New plan is absolutely better than old plan.\n      call.getPlanner().setImportance(filter, 0.0);\n    }"
        ],
        [
            "HiveSubQRemoveRelBuilder::HiveSubQRemoveRelBuilder(Context,RelOptCluster,RelOptSchema)",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155  \n 156  \n 157 -\n 158  \n 159  \n 160 -\n 161  \n 162  \n 163 -\n 164  \n 165  \n 166 -\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  public HiveSubQRemoveRelBuilder(Context context, RelOptCluster cluster,\n                       RelOptSchema relOptSchema) {\n    this.cluster = cluster;\n    this.relOptSchema = relOptSchema;\n    if (context == null) {\n      context = Contexts.EMPTY_CONTEXT;\n    }\n    this.aggregateFactory =\n            Util.first(context.unwrap(RelFactories.AggregateFactory.class),\n                    RelFactories.DEFAULT_AGGREGATE_FACTORY);\n    this.filterFactory =\n            Util.first(context.unwrap(RelFactories.FilterFactory.class),\n                    HiveRelFactories.HIVE_FILTER_FACTORY);\n    this.projectFactory =\n            Util.first(context.unwrap(RelFactories.ProjectFactory.class),\n                    RelFactories.DEFAULT_PROJECT_FACTORY);\n    this.sortFactory =\n            Util.first(context.unwrap(RelFactories.SortFactory.class),\n                    RelFactories.DEFAULT_SORT_FACTORY);\n    this.setOpFactory =\n            Util.first(context.unwrap(RelFactories.SetOpFactory.class),\n                    RelFactories.DEFAULT_SET_OP_FACTORY);\n    this.joinFactory =\n            Util.first(context.unwrap(RelFactories.JoinFactory.class),\n                    RelFactories.DEFAULT_JOIN_FACTORY);\n    this.semiJoinFactory =\n            Util.first(context.unwrap(RelFactories.SemiJoinFactory.class),\n                    RelFactories.DEFAULT_SEMI_JOIN_FACTORY);\n    this.correlateFactory =\n            Util.first(context.unwrap(RelFactories.CorrelateFactory.class),\n                    RelFactories.DEFAULT_CORRELATE_FACTORY);\n    this.valuesFactory =\n            Util.first(context.unwrap(RelFactories.ValuesFactory.class),\n                    RelFactories.DEFAULT_VALUES_FACTORY);\n    this.scanFactory =\n            Util.first(context.unwrap(RelFactories.TableScanFactory.class),\n                    RelFactories.DEFAULT_TABLE_SCAN_FACTORY);\n  }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154 +\n 155  \n 156  \n 157 +\n 158  \n 159  \n 160 +\n 161  \n 162  \n 163 +\n 164  \n 165  \n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  public HiveSubQRemoveRelBuilder(Context context, RelOptCluster cluster,\n                       RelOptSchema relOptSchema) {\n    this.cluster = cluster;\n    this.relOptSchema = relOptSchema;\n    if (context == null) {\n      context = Contexts.EMPTY_CONTEXT;\n    }\n    this.aggregateFactory =\n            Util.first(context.unwrap(RelFactories.AggregateFactory.class),\n                    HiveRelFactories.HIVE_AGGREGATE_FACTORY);\n    this.filterFactory =\n            Util.first(context.unwrap(RelFactories.FilterFactory.class),\n                    HiveRelFactories.HIVE_FILTER_FACTORY);\n    this.projectFactory =\n            Util.first(context.unwrap(RelFactories.ProjectFactory.class),\n                    HiveRelFactories.HIVE_PROJECT_FACTORY);\n    this.sortFactory =\n            Util.first(context.unwrap(RelFactories.SortFactory.class),\n                    HiveRelFactories.HIVE_SORT_FACTORY);\n    this.setOpFactory =\n            Util.first(context.unwrap(RelFactories.SetOpFactory.class),\n                    HiveRelFactories.HIVE_SET_OP_FACTORY);\n    this.joinFactory =\n            Util.first(context.unwrap(RelFactories.JoinFactory.class),\n                    HiveRelFactories.HIVE_JOIN_FACTORY);\n    this.semiJoinFactory =\n            Util.first(context.unwrap(RelFactories.SemiJoinFactory.class),\n                    HiveRelFactories.HIVE_SEMI_JOIN_FACTORY);\n    this.correlateFactory =\n            Util.first(context.unwrap(RelFactories.CorrelateFactory.class),\n                    RelFactories.DEFAULT_CORRELATE_FACTORY);\n    this.valuesFactory =\n            Util.first(context.unwrap(RelFactories.ValuesFactory.class),\n                    RelFactories.DEFAULT_VALUES_FACTORY);\n    this.scanFactory =\n            Util.first(context.unwrap(RelFactories.TableScanFactory.class),\n                    RelFactories.DEFAULT_TABLE_SCAN_FACTORY);\n  }"
        ]
    ],
    "06b2b0e998183be6ed102ca57de1e00d39db3e95": [
        [
            "DruidOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      TypeInfo f = columnTypes.get(i);\n      assert f.getCategory() == ObjectInspector.Category.PRIMITIVE;\n      AggregatorFactory af;\n      switch (f.getTypeName()) {\n        case serdeConstants.TINYINT_TYPE_NAME:\n        case serdeConstants.SMALLINT_TYPE_NAME:\n        case serdeConstants.INT_TYPE_NAME:\n        case serdeConstants.BIGINT_TYPE_NAME:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case serdeConstants.FLOAT_TYPE_NAME:\n        case serdeConstants.DOUBLE_TYPE_NAME:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        default:\n          // Dimension or timestamp\n          String columnName = columnNames.get(i);\n          if (!columnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !columnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            dimensions.add(new StringDimensionSchema(columnName));\n          }\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 +\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      TypeInfo f = columnTypes.get(i);\n      assert f.getCategory() == ObjectInspector.Category.PRIMITIVE;\n      AggregatorFactory af;\n      switch (f.getTypeName()) {\n        case serdeConstants.TINYINT_TYPE_NAME:\n        case serdeConstants.SMALLINT_TYPE_NAME:\n        case serdeConstants.INT_TYPE_NAME:\n        case serdeConstants.BIGINT_TYPE_NAME:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case serdeConstants.FLOAT_TYPE_NAME:\n        case serdeConstants.DOUBLE_TYPE_NAME:\n        case serdeConstants.DECIMAL_TYPE_NAME:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        default:\n          // Dimension or timestamp\n          String columnName = columnNames.get(i);\n          if (!columnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !columnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            dimensions.add(new StringDimensionSchema(columnName));\n          }\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)"
        ],
        [
            "DruidSerDe::serialize(Object,ObjectInspector)",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "  @Override\n  public Writable serialize(Object o, ObjectInspector objectInspector) throws SerDeException {\n    if (objectInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n              + \" can only serialize struct types, but we got: \"\n              + objectInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objectInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(o);\n    // We deserialize the result\n    Map<String, Object> value = new HashMap<>();\n    for (int i = 0; i < columns.length; i++) {\n      if (values.get(i) == null) {\n        // null, we just add it\n        value.put(columns[i], null);\n        continue;\n      }\n      final Object res;\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          res = ((TimestampObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(\n                          values.get(i)).getTime();\n          break;\n        case BYTE:\n          res = ((ByteObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case SHORT:\n          res = ((ShortObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case INT:\n          res = ((IntObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case LONG:\n          res = ((LongObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case FLOAT:\n          res = ((FloatObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case DOUBLE:\n          res = ((DoubleObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        case STRING:\n          res = ((StringObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(\n                          values.get(i));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n      value.put(columns[i], res);\n    }\n    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n            ((TimestampObjectInspector) fields.get(columns.length).getFieldObjectInspector())\n                    .getPrimitiveJavaObject(values.get(columns.length)).getTime()\n    );\n    return new DruidWritable(value);\n  }",
            " 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422 +\n 423 +\n 424 +\n 425 +\n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  ",
            "  @Override\n  public Writable serialize(Object o, ObjectInspector objectInspector) throws SerDeException {\n    if (objectInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n              + \" can only serialize struct types, but we got: \"\n              + objectInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objectInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(o);\n    // We deserialize the result\n    Map<String, Object> value = new HashMap<>();\n    for (int i = 0; i < columns.length; i++) {\n      if (values.get(i) == null) {\n        // null, we just add it\n        value.put(columns[i], null);\n        continue;\n      }\n      final Object res;\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMP:\n          res = ((TimestampObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(\n                          values.get(i)).getTime();\n          break;\n        case BYTE:\n          res = ((ByteObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case SHORT:\n          res = ((ShortObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case INT:\n          res = ((IntObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case LONG:\n          res = ((LongObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case FLOAT:\n          res = ((FloatObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case DOUBLE:\n          res = ((DoubleObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        case DECIMAL:\n          res = ((HiveDecimalObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).doubleValue();\n          break;\n        case STRING:\n          res = ((StringObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(\n                          values.get(i));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n      value.put(columns[i], res);\n    }\n    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n            ((TimestampObjectInspector) fields.get(columns.length).getFieldObjectInspector())\n                    .getPrimitiveJavaObject(values.get(columns.length)).getTime()\n    );\n    return new DruidWritable(value);\n  }"
        ]
    ],
    "f63dc2d4fbbf09a04af98c4a9ba047a355a2da0a": [
        [
            "ASTConverter::convertSource(RelNode)",
            " 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join) {\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }",
            " 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 +\n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join && !semiJoin) {\n          // should not be done for semijoin since it will change the semantics\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }"
        ],
        [
            "HiveSemiJoinRule::onMatch(RelOptRuleCall)",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  ",
            "  @Override public void onMatch(RelOptRuleCall call) {\n    LOG.debug(\"Matched HiveSemiJoinRule\");\n    final Project project = call.rel(0);\n    final Join join = call.rel(1);\n    final RelNode left = call.rel(2);\n    final Aggregate aggregate = call.rel(3);\n    final RelOptCluster cluster = join.getCluster();\n    final RexBuilder rexBuilder = cluster.getRexBuilder();\n    final ImmutableBitSet bits =\n        RelOptUtil.InputFinder.bits(project.getProjects(), null);\n    final ImmutableBitSet rightBits =\n        ImmutableBitSet.range(left.getRowType().getFieldCount(),\n            join.getRowType().getFieldCount());\n    if (bits.intersects(rightBits)) {\n      return;\n    }\n    final JoinInfo joinInfo = join.analyzeCondition();\n    if (!joinInfo.rightSet().equals(\n        ImmutableBitSet.range(aggregate.getGroupCount()))) {\n      // Rule requires that aggregate key to be the same as the join key.\n      // By the way, neither a super-set nor a sub-set would work.\n      return;\n    }\n    if (join.getJoinType() != JoinRelType.INNER) {\n      return;\n    }\n    if (!joinInfo.isEqui()) {\n      return;\n    }\n    LOG.debug(\"All conditions matched for HiveSemiJoinRule. Going to apply transformation.\");\n    final List<Integer> newRightKeyBuilder = Lists.newArrayList();\n    final List<Integer> aggregateKeys = aggregate.getGroupSet().asList();\n    for (int key : joinInfo.rightKeys) {\n      newRightKeyBuilder.add(aggregateKeys.get(key));\n    }\n    final ImmutableIntList newRightKeys =\n        ImmutableIntList.copyOf(newRightKeyBuilder);\n    final RelNode newRight = aggregate.getInput();\n    final RexNode newCondition =\n        RelOptUtil.createEquiJoinCondition(left, joinInfo.leftKeys, newRight,\n            newRightKeys, rexBuilder);\n    RelNode semi = call.builder().push(left).push(aggregate.getInput()).semiJoin(newCondition).build();\n    call.transformTo(call.builder().push(semi).project(project.getProjects(), project.getRowType().getFieldNames()).build());\n  }",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129  \n 130  ",
            "  @Override public void onMatch(RelOptRuleCall call) {\n    LOG.debug(\"Matched HiveSemiJoinRule\");\n    final Project project = call.rel(0);\n    final Join join = call.rel(1);\n    final RelNode left = call.rel(2);\n    final Aggregate aggregate = call.rel(3);\n    final RelOptCluster cluster = join.getCluster();\n    final RexBuilder rexBuilder = cluster.getRexBuilder();\n    final ImmutableBitSet bits =\n        RelOptUtil.InputFinder.bits(project.getProjects(), null);\n    final ImmutableBitSet rightBits =\n        ImmutableBitSet.range(left.getRowType().getFieldCount(),\n            join.getRowType().getFieldCount());\n    if (bits.intersects(rightBits)) {\n      return;\n    }\n    final JoinInfo joinInfo = join.analyzeCondition();\n    if (!joinInfo.rightSet().equals(\n        ImmutableBitSet.range(aggregate.getGroupCount()))) {\n      // Rule requires that aggregate key to be the same as the join key.\n      // By the way, neither a super-set nor a sub-set would work.\n      return;\n    }\n    if(join.getJoinType() == JoinRelType.LEFT) {\n      // since for LEFT join we are only interested in rows from LEFT we can get rid of right side\n      call.transformTo(call.builder().push(left).project(project.getProjects(), project.getRowType().getFieldNames()).build());\n      return;\n    }\n    if (join.getJoinType() != JoinRelType.INNER) {\n      return;\n    }\n    if (!joinInfo.isEqui()) {\n      return;\n    }\n    LOG.debug(\"All conditions matched for HiveSemiJoinRule. Going to apply transformation.\");\n    final List<Integer> newRightKeyBuilder = Lists.newArrayList();\n    final List<Integer> aggregateKeys = aggregate.getGroupSet().asList();\n    for (int key : joinInfo.rightKeys) {\n      newRightKeyBuilder.add(aggregateKeys.get(key));\n    }\n    final ImmutableIntList newRightKeys =\n        ImmutableIntList.copyOf(newRightKeyBuilder);\n    final RelNode newRight = aggregate.getInput();\n    final RexNode newCondition =\n        RelOptUtil.createEquiJoinCondition(left, joinInfo.leftKeys, newRight,\n            newRightKeys, rexBuilder);\n\n    RelNode semi = null;\n    //HIVE-15458: we need to add a Project on top of Join since SemiJoin with Join as it's right input\n    // is not expected further down the pipeline. see jira for more details\n    if(aggregate.getInput() instanceof HepRelVertex\n          && ((HepRelVertex)aggregate.getInput()).getCurrentRel() instanceof  Join) {\n        Join rightJoin = (Join)(((HepRelVertex)aggregate.getInput()).getCurrentRel());\n        List<RexNode> projects = new ArrayList<>();\n        for(int i=0; i<rightJoin.getRowType().getFieldCount(); i++){\n          projects.add(rexBuilder.makeInputRef(rightJoin, i));\n        }\n       RelNode topProject =  call.builder().push(rightJoin).project(projects, rightJoin.getRowType().getFieldNames(), true).build();\n      semi = call.builder().push(left).push(topProject).semiJoin(newCondition).build();\n    }\n    else {\n      semi = call.builder().push(left).push(aggregate.getInput()).semiJoin(newCondition).build();\n    }\n    call.transformTo(call.builder().push(semi).project(project.getProjects(), project.getRowType().getFieldNames()).build());\n  }"
        ],
        [
            "CalcitePlanner::genOPTree(ASTNode,PlannerContext)",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340 -\n 341 -\n 342 -\n 343 -\n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 -\n 450 -\n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS_OR_MV) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        // currently semi-join optimization doesn't work with subqueries\n        // so this will be turned off for if we find subqueries and will later be\n        // restored to its original state\n        boolean originalSemiOptVal = this.conf.getBoolVar(ConfVars.SEMIJOIN_CONVERSION);\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            sinkOp = getOptimizedHiveOPDag();\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 1. Gen Optimized AST\n            ASTNode newAST = getOptimizedAST();\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 2. Regen OP plan from optimized AST\n            init(false);\n            if (cboCtx.type == PreCboCtx.Type.CTAS_OR_MV) {\n              // Redo create-table/view analysis, because it's not part of doPhase1.\n              if (materializedView) {\n                // Use the REWRITTEN AST\n                setAST(newAST);\n                newAST = reAnalyzeMaterializedViewAfterCbo(newAST);\n                // Store text of the ORIGINAL QUERY\n                String originalText = ctx.getTokenRewriteStream().toString(\n                    cboCtx.nodeOfInterest.getTokenStartIndex(),\n                    cboCtx.nodeOfInterest.getTokenStopIndex());\n                createVwDesc.setViewOriginalText(originalText);\n                viewSelect = newAST;\n                viewsExpanded = new ArrayList<>();\n                viewsExpanded.add(createVwDesc.getViewName());\n              } else {\n                // CTAS\n                setAST(newAST);\n                newAST = reAnalyzeCTASAfterCbo(newAST);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n          // restore semi-join opt flag\n          this.conf.setBoolVar(ConfVars.SEMIJOIN_CONVERSION, originalSemiOptVal);\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }",
            " 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS_OR_MV) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            sinkOp = getOptimizedHiveOPDag();\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 1. Gen Optimized AST\n            ASTNode newAST = getOptimizedAST();\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 2. Regen OP plan from optimized AST\n            init(false);\n            if (cboCtx.type == PreCboCtx.Type.CTAS_OR_MV) {\n              // Redo create-table/view analysis, because it's not part of doPhase1.\n              if (materializedView) {\n                // Use the REWRITTEN AST\n                setAST(newAST);\n                newAST = reAnalyzeMaterializedViewAfterCbo(newAST);\n                // Store text of the ORIGINAL QUERY\n                String originalText = ctx.getTokenRewriteStream().toString(\n                    cboCtx.nodeOfInterest.getTokenStartIndex(),\n                    cboCtx.nodeOfInterest.getTokenStopIndex());\n                createVwDesc.setViewOriginalText(originalText);\n                viewSelect = newAST;\n                viewsExpanded = new ArrayList<>();\n                viewsExpanded.add(createVwDesc.getViewName());\n              } else {\n                // CTAS\n                setAST(newAST);\n                newAST = reAnalyzeCTASAfterCbo(newAST);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }"
        ],
        [
            "CalcitePlanner::CalcitePlannerAction::genFilterRelNode(QB,ASTNode,RelNode,Map,ImmutableMap,RowResolver,boolean)",
            "2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419 -\n2420 -\n2421 -\n2422  \n2423  \n2424  \n2425  \n2426  ",
            "    private RelNode genFilterRelNode(QB qb, ASTNode searchCond, RelNode srcRel,\n        Map<String, RelNode> aliasToRel, ImmutableMap<String, Integer> outerNameToPosMap,\n        RowResolver outerRR, boolean forHavingClause) throws SemanticException {\n\n      Map<ASTNode, RelNode> subQueryToRelNode = new HashMap<>();\n      boolean isSubQuery = genSubQueryRelNode(qb, searchCond, srcRel, forHavingClause,\n                                                subQueryToRelNode, aliasToRel);\n      if(isSubQuery) {\n        ExprNodeDesc subQueryExpr = genExprNodeDesc(searchCond, relToHiveRR.get(srcRel),\n                outerRR, subQueryToRelNode, forHavingClause);\n\n        ImmutableMap<String, Integer> hiveColNameCalcitePosMap = this.relToHiveColNameCalcitePosMap\n                .get(srcRel);\n        RexNode convertedFilterLHS = new RexNodeConverter(cluster, srcRel.getRowType(),\n                outerNameToPosMap, hiveColNameCalcitePosMap, relToHiveRR.get(srcRel),\n                outerRR, 0, true, subqueryId).convert(subQueryExpr);\n\n        RelNode filterRel = new HiveFilter(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION),\n                srcRel, convertedFilterLHS);\n\n        this.relToHiveColNameCalcitePosMap.put(filterRel, this.relToHiveColNameCalcitePosMap\n                .get(srcRel));\n        relToHiveRR.put(filterRel, relToHiveRR.get(srcRel));\n        this.subqueryId++;\n\n        // semi-join opt doesn't work with subqueries\n        conf.setBoolVar(ConfVars.SEMIJOIN_CONVERSION, false);\n        return filterRel;\n      } else {\n        return genFilterRelNode(searchCond, srcRel, outerNameToPosMap, outerRR, forHavingClause);\n      }\n    }",
            "2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424  \n2425  \n2426  ",
            "    private RelNode genFilterRelNode(QB qb, ASTNode searchCond, RelNode srcRel,\n        Map<String, RelNode> aliasToRel, ImmutableMap<String, Integer> outerNameToPosMap,\n        RowResolver outerRR, boolean forHavingClause) throws SemanticException {\n\n      Map<ASTNode, RelNode> subQueryToRelNode = new HashMap<>();\n      boolean isSubQuery = genSubQueryRelNode(qb, searchCond, srcRel, forHavingClause,\n                                                subQueryToRelNode, aliasToRel);\n      if(isSubQuery) {\n        ExprNodeDesc subQueryExpr = genExprNodeDesc(searchCond, relToHiveRR.get(srcRel),\n                outerRR, subQueryToRelNode, forHavingClause);\n\n        ImmutableMap<String, Integer> hiveColNameCalcitePosMap = this.relToHiveColNameCalcitePosMap\n                .get(srcRel);\n        RexNode convertedFilterLHS = new RexNodeConverter(cluster, srcRel.getRowType(),\n                outerNameToPosMap, hiveColNameCalcitePosMap, relToHiveRR.get(srcRel),\n                outerRR, 0, true, subqueryId).convert(subQueryExpr);\n\n        RelNode filterRel = new HiveFilter(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION),\n                srcRel, convertedFilterLHS);\n\n        this.relToHiveColNameCalcitePosMap.put(filterRel, this.relToHiveColNameCalcitePosMap\n                .get(srcRel));\n        relToHiveRR.put(filterRel, relToHiveRR.get(srcRel));\n        this.subqueryId++;\n        return filterRel;\n      } else {\n        return genFilterRelNode(searchCond, srcRel, outerNameToPosMap, outerRR, forHavingClause);\n      }\n    }"
        ]
    ],
    "f6cdbc87955aa5cdb83f174a73db9a7d8071f78b": [
        [
            "ConvertJoinMapJoin::removeCycleCreatingSemiJoinOps(MapJoinOperator,Operator,ParseContext)",
            " 780  \n 781  \n 782  \n 783 -\n 784 -\n 785 -\n 786  \n 787  \n 788  \n 789  \n 790  \n 791 -\n 792  \n 793 -\n 794 -\n 795 -\n 796 -\n 797 -\n 798 -\n 799  \n 800  \n 801 -\n 802 -\n 803 -\n 804 -\n 805  \n 806 -\n 807 -\n 808 -\n 809 -\n 810 -\n 811 -\n 812 -\n 813 -\n 814  \n 815  \n 816  \n 817  \n 818 -\n 819 -\n 820 -\n 821 -\n 822 -\n 823  \n 824  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    boolean semiJoinCycle = false;\n    ReduceSinkOperator rs = null;\n    TableScanOperator ts = null;\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0 ) {\n        op = op.getChildOperators().get(0);\n        if (!(op instanceof ReduceSinkOperator)) {\n          continue;\n        }\n        rs = (ReduceSinkOperator) op;\n        ts = parseContext.getRsOpToTsOpMap().get(rs);\n        if (ts == null) {\n          continue;\n        }\n        for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n          if (!(parent instanceof ReduceSinkOperator)) {\n            continue;\n          }\n\n          Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                  TableScanOperator.class);\n          for (TableScanOperator parentTS : tsOps) {\n            // If the parent is same as the ts, then we have a cycle.\n            if (ts == parentTS) {\n              semiJoinCycle = true;\n              break;\n            }\n          }\n        }\n      }\n    }\n\n    // By design there can be atmost 1 such cycle.\n    if (semiJoinCycle) {\n      GenTezUtils.removeBranch(rs);\n      GenTezUtils.removeSemiJoinOperator(parseContext, rs, ts);\n    }\n  }",
            " 780  \n 781  \n 782  \n 783 +\n 784 +\n 785  \n 786  \n 787  \n 788  \n 789  \n 790 +\n 791  \n 792 +\n 793 +\n 794 +\n 795 +\n 796 +\n 797 +\n 798 +\n 799 +\n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805 +\n 806 +\n 807 +\n 808 +\n 809  \n 810  \n 811  \n 812 +\n 813 +\n 814 +\n 815 +\n 816 +\n 817 +\n 818 +\n 819  \n 820  \n 821  \n 822  \n 823 +\n 824 +\n 825 +\n 826 +\n 827 +\n 828 +\n 829  \n 830  ",
            "  private void removeCycleCreatingSemiJoinOps(MapJoinOperator mapjoinOp,\n                                              Operator<?> parentSelectOpOfBigTable,\n                                              ParseContext parseContext) throws SemanticException {\n    Map<ReduceSinkOperator, TableScanOperator> semiJoinMap =\n            new HashMap<ReduceSinkOperator, TableScanOperator>();\n    for (Operator<?> op : parentSelectOpOfBigTable.getChildOperators()) {\n      if (!(op instanceof SelectOperator)) {\n        continue;\n      }\n\n      while (op.getChildOperators().size() > 0) {\n        op = op.getChildOperators().get(0);\n      }\n\n      // If not ReduceSink Op, skip\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      ReduceSinkOperator rs = (ReduceSinkOperator) op;\n      TableScanOperator ts = parseContext.getRsOpToTsOpMap().get(rs);\n      if (ts == null) {\n        // skip, no semijoin branch\n        continue;\n      }\n\n      // Found a semijoin branch.\n      for (Operator<?> parent : mapjoinOp.getParentOperators()) {\n        if (!(parent instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        Set<TableScanOperator> tsOps = OperatorUtils.findOperatorsUpstream(parent,\n                TableScanOperator.class);\n        for (TableScanOperator parentTS : tsOps) {\n          // If the parent is same as the ts, then we have a cycle.\n          if (ts == parentTS) {\n            semiJoinMap.put(rs, ts);\n            break;\n          }\n        }\n      }\n    }\n    if (semiJoinMap.size() > 0) {\n      for (ReduceSinkOperator rs : semiJoinMap.keySet()) {\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(parseContext, rs,\n                semiJoinMap.get(rs));\n      }\n    }\n  }"
        ],
        [
            "GenTezUtils::removeUnionOperators(GenTezProcContext,BaseWork,int)",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 -\n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  ",
            "  public static void removeUnionOperators(GenTezProcContext context, BaseWork work, int indexForTezUnion)\n    throws SemanticException {\n\n    List<Operator<?>> roots = new ArrayList<Operator<?>>();\n    roots.addAll(work.getAllRootOperators());\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n    roots.addAll(context.eventOperatorSet);\n\n    // need to clone the plan.\n    List<Operator<?>> newRoots = SerializationUtilities.cloneOperatorTree(roots, indexForTezUnion);\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    BiMap<Operator<?>, Operator<?>> replacementMap = HashBiMap.create();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Set<FileSinkOperator> fsOpSet = OperatorUtils.findOperators(orig, FileSinkOperator.class);\n      for (FileSinkOperator fsOp : fsOpSet) {\n        context.fileSinkSet.remove(fsOp);\n      }\n\n      Operator<?> newRoot = it.next();\n\n      replacementMap.put(orig, newRoot);\n\n      if (newRoot instanceof HashTableDummyOperator) {\n        // dummy ops need to be updated to the cloned ones.\n        dummyOps.add((HashTableDummyOperator) newRoot);\n        it.remove();\n      } else if (newRoot instanceof AppMasterEventOperator) {\n        // event operators point to table scan operators. When cloning these we\n        // need to restore the original scan.\n        if (newRoot.getConf() instanceof DynamicPruningEventDesc) {\n          TableScanOperator ts = ((DynamicPruningEventDesc) orig.getConf()).getTableScan();\n          if (ts == null) {\n            throw new AssertionError(\"No table scan associated with dynamic event pruning. \" + orig);\n          }\n          ((DynamicPruningEventDesc) newRoot.getConf()).setTableScan(ts);\n        }\n        it.remove();\n      } else {\n        if (newRoot instanceof TableScanOperator) {\n          if (context.tsToEventMap.containsKey(orig)) {\n            // we need to update event operators with the cloned table scan\n            for (AppMasterEventOperator event : context.tsToEventMap.get(orig)) {\n              ((DynamicPruningEventDesc) event.getConf()).setTableScan((TableScanOperator) newRoot);\n            }\n          }\n          // This TableScanOperator could be part of semijoin optimization.\n          Map<ReduceSinkOperator, TableScanOperator> rsOpToTsOpMap =\n                  context.parseContext.getRsOpToTsOpMap();\n          for (ReduceSinkOperator rs : rsOpToTsOpMap.keySet()) {\n            if (rsOpToTsOpMap.get(rs) == orig) {\n              rsOpToTsOpMap.put(rs, (TableScanOperator) newRoot);\n              break;\n            }\n          }\n        }\n        context.rootToWorkMap.remove(orig);\n        context.rootToWorkMap.put(newRoot, work);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while(!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof FileSinkOperator) {\n        FileSinkOperator fileSink = (FileSinkOperator)current;\n\n        // remember it for additional processing later\n        context.fileSinkSet.add(fileSink);\n\n        FileSinkDesc desc = fileSink.getConf();\n        Path path = desc.getDirName();\n        List<FileSinkDesc> linked;\n\n        if (!context.linkedFileSinks.containsKey(path)) {\n          linked = new ArrayList<FileSinkDesc>();\n          context.linkedFileSinks.put(path, linked);\n        }\n        linked = context.linkedFileSinks.get(path);\n        linked.add(desc);\n\n        desc.setDirName(new Path(path, \"\" + linked.size()));\n        desc.setLinkedFileSink(true);\n        desc.setParentDir(path);\n        desc.setLinkedFileSinkDesc(linked);\n      }\n\n      if (current instanceof AppMasterEventOperator) {\n        // remember for additional processing later\n        context.eventOperatorSet.add((AppMasterEventOperator) current);\n\n        // mark the original as abandoned. Don't need it anymore.\n        context.abandonedEventOperatorSet.add((AppMasterEventOperator) replacementMap.inverse()\n            .get(current));\n      }\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        assert count <= 1;\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    LOG.debug(\"Setting dummy ops for work \" + work.getName() + \": \" + dummyOps);\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  ",
            "  public static void removeUnionOperators(GenTezProcContext context, BaseWork work, int indexForTezUnion)\n    throws SemanticException {\n\n    List<Operator<?>> roots = new ArrayList<Operator<?>>();\n    roots.addAll(work.getAllRootOperators());\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n    roots.addAll(context.eventOperatorSet);\n\n    // need to clone the plan.\n    List<Operator<?>> newRoots = SerializationUtilities.cloneOperatorTree(roots, indexForTezUnion);\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    BiMap<Operator<?>, Operator<?>> replacementMap = HashBiMap.create();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Set<FileSinkOperator> fsOpSet = OperatorUtils.findOperators(orig, FileSinkOperator.class);\n      for (FileSinkOperator fsOp : fsOpSet) {\n        context.fileSinkSet.remove(fsOp);\n      }\n\n      Operator<?> newRoot = it.next();\n\n      replacementMap.put(orig, newRoot);\n\n      if (newRoot instanceof HashTableDummyOperator) {\n        // dummy ops need to be updated to the cloned ones.\n        dummyOps.add((HashTableDummyOperator) newRoot);\n        it.remove();\n      } else if (newRoot instanceof AppMasterEventOperator) {\n        // event operators point to table scan operators. When cloning these we\n        // need to restore the original scan.\n        if (newRoot.getConf() instanceof DynamicPruningEventDesc) {\n          TableScanOperator ts = ((DynamicPruningEventDesc) orig.getConf()).getTableScan();\n          if (ts == null) {\n            throw new AssertionError(\"No table scan associated with dynamic event pruning. \" + orig);\n          }\n          ((DynamicPruningEventDesc) newRoot.getConf()).setTableScan(ts);\n        }\n        it.remove();\n      } else {\n        if (newRoot instanceof TableScanOperator) {\n          if (context.tsToEventMap.containsKey(orig)) {\n            // we need to update event operators with the cloned table scan\n            for (AppMasterEventOperator event : context.tsToEventMap.get(orig)) {\n              ((DynamicPruningEventDesc) event.getConf()).setTableScan((TableScanOperator) newRoot);\n            }\n          }\n          // This TableScanOperator could be part of semijoin optimization.\n          Map<ReduceSinkOperator, TableScanOperator> rsOpToTsOpMap =\n                  context.parseContext.getRsOpToTsOpMap();\n          for (ReduceSinkOperator rs : rsOpToTsOpMap.keySet()) {\n            if (rsOpToTsOpMap.get(rs) == orig) {\n              rsOpToTsOpMap.put(rs, (TableScanOperator) newRoot);\n            }\n          }\n        }\n        context.rootToWorkMap.remove(orig);\n        context.rootToWorkMap.put(newRoot, work);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while(!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof FileSinkOperator) {\n        FileSinkOperator fileSink = (FileSinkOperator)current;\n\n        // remember it for additional processing later\n        context.fileSinkSet.add(fileSink);\n\n        FileSinkDesc desc = fileSink.getConf();\n        Path path = desc.getDirName();\n        List<FileSinkDesc> linked;\n\n        if (!context.linkedFileSinks.containsKey(path)) {\n          linked = new ArrayList<FileSinkDesc>();\n          context.linkedFileSinks.put(path, linked);\n        }\n        linked = context.linkedFileSinks.get(path);\n        linked.add(desc);\n\n        desc.setDirName(new Path(path, \"\" + linked.size()));\n        desc.setLinkedFileSink(true);\n        desc.setParentDir(path);\n        desc.setLinkedFileSinkDesc(linked);\n      }\n\n      if (current instanceof AppMasterEventOperator) {\n        // remember for additional processing later\n        context.eventOperatorSet.add((AppMasterEventOperator) current);\n\n        // mark the original as abandoned. Don't need it anymore.\n        context.abandonedEventOperatorSet.add((AppMasterEventOperator) replacementMap.inverse()\n            .get(current));\n      }\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        assert count <= 1;\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    LOG.debug(\"Setting dummy ops for work \" + work.getName() + \": \" + dummyOps);\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }"
        ],
        [
            "GenTezUtils::removeSemiJoinOperator(ParseContext,ReduceSinkOperator,TableScanOperator)",
            " 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572 -\n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597 -\n 598 -\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  ",
            "  public static void removeSemiJoinOperator(ParseContext context,\n                                     ReduceSinkOperator rs,\n                                     TableScanOperator ts) throws SemanticException{\n    // Cleanup the synthetic predicate in the tablescan operator by\n    // replacing it with \"true\"\n    LOG.debug(\"Removing ReduceSink \" + rs + \" and TableScan \" + ts);\n    ExprNodeDesc constNode = new ExprNodeConstantDesc(\n            TypeInfoFactory.booleanTypeInfo, Boolean.TRUE);\n    DynamicValuePredicateContext filterDynamicValuePredicatesCollection =\n            new DynamicValuePredicateContext();\n    collectDynamicValuePredicates(((FilterOperator)(ts.getChildOperators().get(0))).getConf().getPredicate(),\n            filterDynamicValuePredicatesCollection);\n    for (ExprNodeDesc nodeToRemove : filterDynamicValuePredicatesCollection\n            .childParentMapping.keySet()) {\n      // Find out if this synthetic predicate belongs to the current cycle\n      boolean skip = true;\n      for (ExprNodeDesc expr : nodeToRemove.getChildren()) {\n        if (expr instanceof ExprNodeDynamicValueDesc ) {\n          String dynamicValueIdFromExpr = ((ExprNodeDynamicValueDesc) expr)\n                  .getDynamicValue().getId();\n          List<String> dynamicValueIdsFromMap = context.\n                  getRsToRuntimeValuesInfoMap().get(rs).getDynamicValueIDs();\n          for (String dynamicValueIdFromMap : dynamicValueIdsFromMap) {\n            if (dynamicValueIdFromExpr.equals(dynamicValueIdFromMap)) {\n              // Intended predicate to be removed\n              skip = false;\n              break;\n            }\n          }\n        }\n      }\n      if (!skip) {\n        ExprNodeDesc nodeParent = filterDynamicValuePredicatesCollection\n                .childParentMapping.get(nodeToRemove);\n        if (nodeParent == null) {\n          // This was the only predicate, set filter expression to null\n          ts.getConf().setFilterExpr(null);\n        } else {\n          int i = nodeParent.getChildren().indexOf(nodeToRemove);\n          nodeParent.getChildren().remove(i);\n          nodeParent.getChildren().add(i, constNode);\n        }\n        // skip the rest of the predicates\n        skip = true;\n      }\n    }\n    context.getRsOpToTsOpMap().remove(rs);\n  }",
            " 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571 +\n 572 +\n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597 +\n 598 +\n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  ",
            "  public static void removeSemiJoinOperator(ParseContext context,\n                                     ReduceSinkOperator rs,\n                                     TableScanOperator ts) throws SemanticException{\n    // Cleanup the synthetic predicate in the tablescan operator by\n    // replacing it with \"true\"\n    LOG.debug(\"Removing ReduceSink \" + rs + \" and TableScan \" + ts);\n    ExprNodeDesc constNode = new ExprNodeConstantDesc(\n            TypeInfoFactory.booleanTypeInfo, Boolean.TRUE);\n    DynamicValuePredicateContext filterDynamicValuePredicatesCollection =\n            new DynamicValuePredicateContext();\n    FilterDesc filterDesc = ((FilterOperator)(ts.getChildOperators().get(0))).getConf();\n    collectDynamicValuePredicates(filterDesc.getPredicate(),\n            filterDynamicValuePredicatesCollection);\n    for (ExprNodeDesc nodeToRemove : filterDynamicValuePredicatesCollection\n            .childParentMapping.keySet()) {\n      // Find out if this synthetic predicate belongs to the current cycle\n      boolean skip = true;\n      for (ExprNodeDesc expr : nodeToRemove.getChildren()) {\n        if (expr instanceof ExprNodeDynamicValueDesc ) {\n          String dynamicValueIdFromExpr = ((ExprNodeDynamicValueDesc) expr)\n                  .getDynamicValue().getId();\n          List<String> dynamicValueIdsFromMap = context.\n                  getRsToRuntimeValuesInfoMap().get(rs).getDynamicValueIDs();\n          for (String dynamicValueIdFromMap : dynamicValueIdsFromMap) {\n            if (dynamicValueIdFromExpr.equals(dynamicValueIdFromMap)) {\n              // Intended predicate to be removed\n              skip = false;\n              break;\n            }\n          }\n        }\n      }\n      if (!skip) {\n        ExprNodeDesc nodeParent = filterDynamicValuePredicatesCollection\n                .childParentMapping.get(nodeToRemove);\n        if (nodeParent == null) {\n          // This was the only predicate, set filter expression to const\n          filterDesc.setPredicate(constNode);\n        } else {\n          int i = nodeParent.getChildren().indexOf(nodeToRemove);\n          nodeParent.getChildren().remove(i);\n          nodeParent.getChildren().add(i, constNode);\n        }\n        // skip the rest of the predicates\n        skip = true;\n      }\n    }\n    context.getRsOpToTsOpMap().remove(rs);\n  }"
        ]
    ],
    "21ef70bdd82fc423991391ba901ce8c9d2d3f31c": [
        [
            "LlapTaskCommunicator::getCompletedLogsUrl(TezTaskAttemptID,NodeId)",
            " 516  \n 517  \n 518 -\n 519 -\n 520 -\n 521  ",
            "  @Override\n  public String getCompletedLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n    // Not supported yet.\n    // Need support from YARN to link to an already aggregated log, or at least list them.\n    return null;\n  }",
            " 545  \n 546  \n 547 +\n 548 +\n 549 +\n 550 +\n 551 +\n 552 +\n 553 +\n 554 +\n 555 +\n 556 +\n 557 +\n 558 +\n 559 +\n 560 +\n 561  ",
            "  @Override\n  public String getCompletedLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n    String url = \"\";\n    if (timelineServerUri != null) {\n      LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n      BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n      ContainerId containerId = biMap.inverse().get(attemptID);\n      if (containerId != null) {\n        String dagId = attemptID.getTaskID().getVertexID().getDAGId().toString();\n        String filename = currentHiveQueryId + \"-\" + dagId + \".log.done\";\n        // YARN-6011 provides a webservice to get the logs\n        url = PATH_JOINER.join(timelineServerUri.toString(), \"containers\", containerId.toString(), \"logs\",\n          filename);\n      }\n    }\n    return url;\n  }"
        ],
        [
            "LlapTaskCommunicator::getInProgressLogsUrl(TezTaskAttemptID,NodeId)",
            " 509  \n 510  \n 511 -\n 512 -\n 513 -\n 514  ",
            "  @Override\n  public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n    // Not supported yet.\n    // Need support from YARN to link to an already aggregated log, or at least list them.\n    return null;\n  }",
            " 527  \n 528  \n 529 +\n 530 +\n 531 +\n 532 +\n 533 +\n 534 +\n 535 +\n 536 +\n 537 +\n 538 +\n 539 +\n 540 +\n 541 +\n 542 +\n 543  ",
            "  @Override\n  public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n    String url = \"\";\n    if (timelineServerUri != null) {\n      LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n      BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n      ContainerId containerId = biMap.inverse().get(attemptID);\n      if (containerId != null) {\n        String dagId = attemptID.getTaskID().getVertexID().getDAGId().toString();\n        String filename = currentHiveQueryId + \"-\" + dagId + \".log\";\n        // YARN-6011 provides a webservice to get the logs\n        url = PATH_JOINER.join(timelineServerUri.toString(), \"containers\", containerId.toString(), \"logs\",\n          filename);\n      }\n    }\n    return url;\n  }"
        ],
        [
            "LlapTaskCommunicator::initialize()",
            " 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "  @Override\n  public void initialize() throws Exception {\n    super.initialize();\n    Configuration conf = getConf();\n    int numThreads = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_COMMUNICATOR_NUM_THREADS);\n    this.communicator = createLlapProtocolClientProxy(numThreads, conf);\n    this.deleteDelayOnDagComplete = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_FILE_CLEANUP_DELAY_SECONDS, TimeUnit.SECONDS);\n    LOG.info(\"Running LlapTaskCommunicator with \"\n        + \"fileCleanupDelay=\" + deleteDelayOnDagComplete\n        + \", numCommunicatorThreads=\" + numThreads);\n    this.communicator.init(conf);\n  }",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206  ",
            "  @Override\n  public void initialize() throws Exception {\n    super.initialize();\n    Configuration conf = getConf();\n    int numThreads = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_COMMUNICATOR_NUM_THREADS);\n    this.communicator = createLlapProtocolClientProxy(numThreads, conf);\n    this.deleteDelayOnDagComplete = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_FILE_CLEANUP_DELAY_SECONDS, TimeUnit.SECONDS);\n    LOG.info(\"Running LlapTaskCommunicator with \"\n        + \"fileCleanupDelay=\" + deleteDelayOnDagComplete\n        + \", numCommunicatorThreads=\" + numThreads);\n    this.communicator.init(conf);\n    if (YarnConfiguration.useHttps(conf)) {\n      timelineServerUri = URI\n        .create(JOINER.join(\"https://\", conf.get(\n          YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n          YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS),\n          RESOURCE_URI_STR));\n    } else {\n      timelineServerUri = URI.create(JOINER.join(\"http://\", conf.get(\n        YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS),\n        RESOURCE_URI_STR));\n    }\n  }"
        ]
    ],
    "0debf9f2916b2ed115e1cdb392a595ae7cf0c761": [
        [
            "DynamicPartitionPruningOptimization::generateSemiJoinOperatorPlan(DynamicListContext,ParseContext,TableScanOperator,String)",
            " 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 -\n 402  \n 403  \n 404 -\n 405  \n 406 -\n 407 -\n 408 -\n 409  \n 410 -\n 411 -\n 412 -\n 413 -\n 414 -\n 415 -\n 416  \n 417 -\n 418 -\n 419 -\n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on parition column.\n      String internalColName = null;\n      ExprNodeDesc exprNodeDesc = key;\n      // Find the ExprNodeColumnDesc\n      while (!(exprNodeDesc instanceof ExprNodeColumnDesc)) {\n        exprNodeDesc = exprNodeDesc.getChildren().get(0);\n      }\n      internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n\n      ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc)(parentOfRS.\n              getColumnExprMap().get(internalColName)));\n      String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n      // Fetch the TableScan Operator.\n      Operator<?> op = parentOfRS.getParentOperators().get(0);\n      while (op != null && !(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      assert op != null;\n\n      Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n      if (table.isPartitionKey(colName)) {\n        // The column is partition column, skip the optimization.\n        return false;\n      }\n    }\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    new RowSchema(parentOfRS.getSchema()), parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }",
            " 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 +\n 402 +\n 403  \n 404  \n 405  \n 406 +\n 407 +\n 408  \n 409 +\n 410 +\n 411 +\n 412 +\n 413 +\n 414 +\n 415 +\n 416 +\n 417 +\n 418 +\n 419  \n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on parition column.\n      String internalColName = null;\n      ExprNodeDesc exprNodeDesc = key;\n      // Find the ExprNodeColumnDesc\n      while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n              (exprNodeDesc.getChildren() != null)) {\n        exprNodeDesc = exprNodeDesc.getChildren().get(0);\n      }\n\n      if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n        internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      } else {\n        // No column found!\n        // Bail out\n        return false;\n      }\n    }\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    new RowSchema(parentOfRS.getSchema()), parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }"
        ]
    ],
    "016afe0d69f3a90290e3a127149430ad6d4c603f": [
        [
            "GenVectorCode::generateFilterColumnBetweenDynamicValue(String)",
            "1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421 -\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433 -\n1434  \n1435  \n1436  \n1437  \n1438  \n1439 -\n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453 -\n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  ",
            "  private void generateFilterColumnBetweenDynamicValue(String[] tdesc) throws Exception {\n    String operandType = tdesc[1];\n    String optionalNot = tdesc[2];\n\n    String className = \"Filter\" + getCamelCaseType(operandType) + \"Column\" +\n      (optionalNot.equals(\"!\") ? \"Not\" : \"\") + \"BetweenDynamicValue\";\n\n    String typeName = getCamelCaseType(operandType);\n    String defaultValue;\n    String vectorType;\n    String getPrimitiveMethod;\n    String getValueMethod;\n    String conversionMethod;\n\n    if (operandType.equals(\"long\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getLong\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"double\")) {\n      defaultValue = \"0\";\n      vectorType = \"double\";\n      getPrimitiveMethod = \"getDouble\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"decimal\")) {\n      defaultValue = \"null\";\n      vectorType = \"HiveDecimal\";\n      getPrimitiveMethod = \"getHiveDecimal\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"string\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getString\";\n      getValueMethod = \".getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"char\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveChar\";\n      getValueMethod = \".getStrippedValue().getBytes()\";  // Does vectorization use stripped char values?\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"varchar\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveVarchar\";\n      getValueMethod = \".getValue().getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"date\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getDate\";\n      getValueMethod = \"\";\n      conversionMethod = \"DateWritable.dateToDays\";\n      // Special case - Date requires its own specific BetweenDynamicValue class, but derives from FilterLongColumnBetween\n      typeName = \"Long\";\n    } else if (operandType.equals(\"timestamp\")) {\n      defaultValue = \"null\";\n      vectorType = \"Timestamp\";\n      getPrimitiveMethod = \"getTimestamp\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else {\n      throw new IllegalArgumentException(\"Type \" + operandType + \" not supported\");\n    }\n\n    // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<TypeName>\", typeName);\n    templateString = templateString.replaceAll(\"<DefaultValue>\", defaultValue);\n    templateString = templateString.replaceAll(\"<VectorType>\", vectorType);\n    templateString = templateString.replaceAll(\"<GetPrimitiveMethod>\", getPrimitiveMethod);\n    templateString = templateString.replaceAll(\"<GetValueMethod>\", getValueMethod);\n    templateString = templateString.replaceAll(\"<ConversionMethod>\", conversionMethod);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }",
            "1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421 +\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433 +\n1434  \n1435  \n1436  \n1437  \n1438  \n1439 +\n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453 +\n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  ",
            "  private void generateFilterColumnBetweenDynamicValue(String[] tdesc) throws Exception {\n    String operandType = tdesc[1];\n    String optionalNot = tdesc[2];\n\n    String className = \"Filter\" + getCamelCaseType(operandType) + \"Column\" +\n      (optionalNot.equals(\"!\") ? \"Not\" : \"\") + \"BetweenDynamicValue\";\n\n    String typeName = getCamelCaseType(operandType);\n    String defaultValue;\n    String vectorType;\n    String getPrimitiveMethod;\n    String getValueMethod;\n    String conversionMethod;\n\n    if (operandType.equals(\"long\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getLong\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"double\")) {\n      defaultValue = \"0\";\n      vectorType = \"double\";\n      getPrimitiveMethod = \"getDouble\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"decimal\")) {\n      defaultValue = \"HiveDecimal.ZERO\";\n      vectorType = \"HiveDecimal\";\n      getPrimitiveMethod = \"getHiveDecimal\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"string\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getString\";\n      getValueMethod = \".getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"char\")) {\n      defaultValue = \"new HiveChar(\\\"\\\", 1)\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveChar\";\n      getValueMethod = \".getStrippedValue().getBytes()\";  // Does vectorization use stripped char values?\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"varchar\")) {\n      defaultValue = \"new HiveVarchar(\\\"\\\", 1)\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveVarchar\";\n      getValueMethod = \".getValue().getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"date\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getDate\";\n      getValueMethod = \"\";\n      conversionMethod = \"DateWritable.dateToDays\";\n      // Special case - Date requires its own specific BetweenDynamicValue class, but derives from FilterLongColumnBetween\n      typeName = \"Long\";\n    } else if (operandType.equals(\"timestamp\")) {\n      defaultValue = \"new Timestamp(0)\";\n      vectorType = \"Timestamp\";\n      getPrimitiveMethod = \"getTimestamp\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else {\n      throw new IllegalArgumentException(\"Type \" + operandType + \" not supported\");\n    }\n\n    // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<TypeName>\", typeName);\n    templateString = templateString.replaceAll(\"<DefaultValue>\", defaultValue);\n    templateString = templateString.replaceAll(\"<VectorType>\", vectorType);\n    templateString = templateString.replaceAll(\"<GetPrimitiveMethod>\", getPrimitiveMethod);\n    templateString = templateString.replaceAll(\"<GetValueMethod>\", getValueMethod);\n    templateString = templateString.replaceAll(\"<ConversionMethod>\", conversionMethod);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }"
        ]
    ],
    "8973d2c66394ed25b1baa20df3920870ae9b053c": [
        [
            "DruidSerDe::initialize(Configuration,Properties)",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177 -\n 178 -\n 179 -\n 180 -\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. Either it is a CTAS, or we need to create a Druid\n      // Segment Metadata query that retrieves all columns present in\n      // the data source (dimensions and metrics).\n      if (!org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMNS))\n              && !org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES))) {\n        columnNames.addAll(Utilities.getColumnNames(properties));\n        if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          throw new SerDeException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n                  \"') not specified in create table; list of columns is : \" +\n                  properties.getProperty(serdeConstants.LIST_COLUMNS));\n        }\n        columnTypes.addAll(Lists.transform(Utilities.getColumnTypes(properties),\n                new Function<String, PrimitiveTypeInfo>() {\n                  @Override\n                  public PrimitiveTypeInfo apply(String type) {\n                    return TypeInfoFactory.getPrimitiveTypeInfo(type);\n                  }\n                }\n        ));\n        inspectors.addAll(Lists.transform(columnTypes,\n                new Function<PrimitiveTypeInfo, ObjectInspector>() {\n                  @Override\n                  public ObjectInspector apply(PrimitiveTypeInfo type) {\n                    return PrimitiveObjectInspectorFactory\n                            .getPrimitiveWritableObjectInspector(type);\n                  }\n                }\n        ));\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      } else {\n        String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n        if (dataSource == null) {\n          throw new SerDeException(\"Druid data source not specified; use \" +\n                  Constants.DRUID_DATA_SOURCE + \" in table properties\");\n        }\n        SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n        builder.dataSource(dataSource);\n        builder.merge(true);\n        builder.analysisTypes();\n        SegmentMetadataQuery query = builder.build();\n\n        // Execute query in Druid\n        String address = HiveConf.getVar(configuration,\n                HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n        );\n        if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n          throw new SerDeException(\"Druid broker address not specified in configuration\");\n        }\n\n        numConnection = HiveConf\n              .getIntVar(configuration, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n        readTimeout = new Period(\n              HiveConf.getVar(configuration, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n        // Infer schema\n        SegmentAnalysis schemaInfo;\n        try {\n          schemaInfo = submitMetadataRequest(address, query);\n        } catch (IOException e) {\n          throw new SerDeException(e);\n        }\n        for (Entry<String, ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n          if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n            // Special handling for timestamp column\n            columnNames.add(columnInfo.getKey()); // field name\n            PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n            columnTypes.add(type);\n            inspectors\n                    .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n            continue;\n          }\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                  columnInfo.getValue().getType()); // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n        }\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      }\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n\n        switch (query.getType()) {\n          case Query.TIMESERIES:\n            inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n            break;\n          case Query.TOPN:\n            inferSchema((TopNQuery) query, columnNames, columnTypes);\n            break;\n          case Query.SELECT:\n            String address = HiveConf.getVar(configuration,\n                    HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n            if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n              throw new SerDeException(\"Druid broker address not specified in configuration\");\n            }\n            inferSchema((SelectQuery) query, columnNames, columnTypes, address);\n            break;\n          case Query.GROUP_BY:\n            inferSchema((GroupByQuery) query, columnNames, columnTypes);\n            break;\n          default:\n            throw new SerDeException(\"Not supported Druid query\");\n        }\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors\n                .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }",
            " 110  \n 111  \n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Init connection properties\n    numConnection = HiveConf\n          .getIntVar(configuration, HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    readTimeout = new Period(\n          HiveConf.getVar(configuration, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    final List<String> columnNames = new ArrayList<>();\n    final List<PrimitiveTypeInfo> columnTypes = new ArrayList<>();\n    List<ObjectInspector> inspectors = new ArrayList<>();\n\n    // Druid query\n    String druidQuery = properties.getProperty(Constants.DRUID_QUERY_JSON);\n    if (druidQuery == null) {\n      // No query. Either it is a CTAS, or we need to create a Druid\n      // Segment Metadata query that retrieves all columns present in\n      // the data source (dimensions and metrics).\n      if (!org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMNS))\n              && !org.apache.commons.lang3.StringUtils\n              .isEmpty(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES))) {\n        columnNames.addAll(Utilities.getColumnNames(properties));\n        if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n          throw new SerDeException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n                  \"') not specified in create table; list of columns is : \" +\n                  properties.getProperty(serdeConstants.LIST_COLUMNS));\n        }\n        columnTypes.addAll(Lists.transform(Utilities.getColumnTypes(properties),\n                new Function<String, PrimitiveTypeInfo>() {\n                  @Override\n                  public PrimitiveTypeInfo apply(String type) {\n                    return TypeInfoFactory.getPrimitiveTypeInfo(type);\n                  }\n                }\n        ));\n        inspectors.addAll(Lists.transform(columnTypes,\n                new Function<PrimitiveTypeInfo, ObjectInspector>() {\n                  @Override\n                  public ObjectInspector apply(PrimitiveTypeInfo type) {\n                    return PrimitiveObjectInspectorFactory\n                            .getPrimitiveWritableObjectInspector(type);\n                  }\n                }\n        ));\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      } else {\n        String dataSource = properties.getProperty(Constants.DRUID_DATA_SOURCE);\n        if (dataSource == null) {\n          throw new SerDeException(\"Druid data source not specified; use \" +\n                  Constants.DRUID_DATA_SOURCE + \" in table properties\");\n        }\n        SegmentMetadataQueryBuilder builder = new Druids.SegmentMetadataQueryBuilder();\n        builder.dataSource(dataSource);\n        builder.merge(true);\n        builder.analysisTypes();\n        SegmentMetadataQuery query = builder.build();\n\n        // Execute query in Druid\n        String address = HiveConf.getVar(configuration,\n                HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS\n        );\n        if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n          throw new SerDeException(\"Druid broker address not specified in configuration\");\n        }\n\n        // Infer schema\n        SegmentAnalysis schemaInfo;\n        try {\n          schemaInfo = submitMetadataRequest(address, query);\n        } catch (IOException e) {\n          throw new SerDeException(e);\n        }\n        for (Entry<String, ColumnAnalysis> columnInfo : schemaInfo.getColumns().entrySet()) {\n          if (columnInfo.getKey().equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n            // Special handling for timestamp column\n            columnNames.add(columnInfo.getKey()); // field name\n            PrimitiveTypeInfo type = TypeInfoFactory.timestampTypeInfo; // field type\n            columnTypes.add(type);\n            inspectors\n                    .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n            continue;\n          }\n          columnNames.add(columnInfo.getKey()); // field name\n          PrimitiveTypeInfo type = DruidSerDeUtils.convertDruidToHiveType(\n                  columnInfo.getValue().getType()); // field type\n          columnTypes.add(type);\n          inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(type));\n        }\n        columns = columnNames.toArray(new String[columnNames.size()]);\n        types = columnTypes.toArray(new PrimitiveTypeInfo[columnTypes.size()]);\n        inspector = ObjectInspectorFactory\n                .getStandardStructObjectInspector(columnNames, inspectors);\n      }\n    } else {\n      // Query is specified, we can extract the results schema from the query\n      Query<?> query;\n      try {\n        query = DruidStorageHandlerUtils.JSON_MAPPER.readValue(druidQuery, Query.class);\n\n        switch (query.getType()) {\n          case Query.TIMESERIES:\n            inferSchema((TimeseriesQuery) query, columnNames, columnTypes);\n            break;\n          case Query.TOPN:\n            inferSchema((TopNQuery) query, columnNames, columnTypes);\n            break;\n          case Query.SELECT:\n            String address = HiveConf.getVar(configuration,\n                    HiveConf.ConfVars.HIVE_DRUID_BROKER_DEFAULT_ADDRESS);\n            if (org.apache.commons.lang3.StringUtils.isEmpty(address)) {\n              throw new SerDeException(\"Druid broker address not specified in configuration\");\n            }\n            inferSchema((SelectQuery) query, columnNames, columnTypes, address);\n            break;\n          case Query.GROUP_BY:\n            inferSchema((GroupByQuery) query, columnNames, columnTypes);\n            break;\n          default:\n            throw new SerDeException(\"Not supported Druid query\");\n        }\n      } catch (Exception e) {\n        throw new SerDeException(e);\n      }\n\n      columns = new String[columnNames.size()];\n      types = new PrimitiveTypeInfo[columnNames.size()];\n      for (int i = 0; i < columnTypes.size(); ++i) {\n        columns[i] = columnNames.get(i);\n        types[i] = columnTypes.get(i);\n        inspectors\n                .add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(types[i]));\n      }\n      inspector = ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, inspectors);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DruidSerDe initialized with\\n\"\n              + \"\\t columns: \" + columnNames\n              + \"\\n\\t types: \" + columnTypes);\n    }\n  }"
        ]
    ],
    "8449304eb241f153f3f11ae93d76b8114f793486": [
        [
            "SQLStdHiveAuthorizationValidatorForTest::applyRowFilterAndColumnMasking(HiveAuthzContext,List)",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  public List<HivePrivilegeObject> applyRowFilterAndColumnMasking(HiveAuthzContext context,\n      List<HivePrivilegeObject> privObjs) throws SemanticException {\n    List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n    for (HivePrivilegeObject privObj : privObjs) {\n      if (privObj.getObjectName().equals(\"masking_test\")) {\n        privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n        List<String> cellValueTransformers = new ArrayList<>();\n        for (String columnName : privObj.getColumns()) {\n          if (columnName.equals(\"value\")) {\n            cellValueTransformers.add(\"reverse(value)\");\n          } else {\n            cellValueTransformers.add(columnName);\n          }\n        }\n        privObj.setCellValueTransformers(cellValueTransformers);\n        needRewritePrivObjs.add(privObj);\n      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n        privObj\n            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n        needRewritePrivObjs.add(privObj);\n      }\n    }\n    return needRewritePrivObjs;\n  }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140 +\n 141  \n 142  \n 143  \n 144  ",
            "  public List<HivePrivilegeObject> applyRowFilterAndColumnMasking(HiveAuthzContext context,\n      List<HivePrivilegeObject> privObjs) throws SemanticException {\n    List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n    for (HivePrivilegeObject privObj : privObjs) {\n      if (privObj.getObjectName().equals(\"masking_test\")) {\n        privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n        List<String> cellValueTransformers = new ArrayList<>();\n        for (String columnName : privObj.getColumns()) {\n          if (columnName.equals(\"value\")) {\n            cellValueTransformers.add(\"reverse(value)\");\n          } else {\n            cellValueTransformers.add(columnName);\n          }\n        }\n        privObj.setCellValueTransformers(cellValueTransformers);\n        needRewritePrivObjs.add(privObj);\n      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n        privObj\n            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n        needRewritePrivObjs.add(privObj);\n      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n        // testing acid usage when no masking/filtering is present\n        needRewritePrivObjs.add(privObj);\n      }\n    }\n    return needRewritePrivObjs;\n  }"
        ],
        [
            "SemanticAnalyzer::walkASTMarkTABREF(ASTNode,Set)",
            "10747  \n10748  \n10749  \n10750  \n10751  \n10752  \n10753  \n10754  \n10755  \n10756  \n10757  \n10758  \n10759  \n10760  \n10761  \n10762  \n10763  \n10764  \n10765  \n10766  \n10767  \n10768  \n10769  \n10770  \n10771  \n10772  \n10773  \n10774  \n10775  \n10776  \n10777  \n10778  \n10779  \n10780  \n10781  \n10782  \n10783  \n10784  \n10785  \n10786  \n10787  \n10788  \n10789  \n10790  \n10791  \n10792  \n10793  \n10794  \n10795  \n10796  \n10797  \n10798  \n10799  \n10800  \n10801  \n10802  \n10803  \n10804  \n10805  \n10806  \n10807  \n10808  \n10809  \n10810  \n10811  \n10812  \n10813  \n10814  \n10815  \n10816  \n10817  \n10818  \n10819  \n10820  \n10821 -\n10822 -\n10823 -\n10824 -\n10825 -\n10826  \n10827  \n10828  \n10829  \n10830  \n10831  \n10832  \n10833  \n10834  ",
            "  private void walkASTMarkTABREF(ASTNode ast, Set<String> cteAlias)\n      throws SemanticException {\n    Queue<Node> queue = new LinkedList<>();\n    queue.add(ast);\n    Map<HivePrivilegeObject, MaskAndFilterInfo> basicInfos = new LinkedHashMap<>();\n    while (!queue.isEmpty()) {\n      ASTNode astNode = (ASTNode) queue.poll();\n      if (astNode.getToken().getType() == HiveParser.TOK_TABREF) {\n        int aliasIndex = 0;\n        StringBuffer additionalTabInfo = new StringBuffer();\n        for (int index = 1; index < astNode.getChildCount(); index++) {\n          ASTNode ct = (ASTNode) astNode.getChild(index);\n          if (ct.getToken().getType() == HiveParser.TOK_TABLEBUCKETSAMPLE\n              || ct.getToken().getType() == HiveParser.TOK_TABLESPLITSAMPLE\n              || ct.getToken().getType() == HiveParser.TOK_TABLEPROPERTIES) {\n            additionalTabInfo.append(ctx.getTokenRewriteStream().toString(ct.getTokenStartIndex(),\n                ct.getTokenStopIndex()));\n          } else {\n            aliasIndex = index;\n          }\n        }\n\n        ASTNode tableTree = (ASTNode) (astNode.getChild(0));\n\n        String tabIdName = getUnescapedName(tableTree);\n\n        String alias;\n        if (aliasIndex != 0) {\n          alias = unescapeIdentifier(astNode.getChild(aliasIndex).getText());\n        } else {\n          alias = getUnescapedUnqualifiedTableName(tableTree);\n        }\n\n        // We need to know if it is CTE or not.\n        // A CTE may have the same name as a table.\n        // For example,\n        // with select TAB1 [masking] as TAB2\n        // select * from TAB2 [no masking]\n        if (cteAlias.contains(tabIdName)) {\n          continue;\n        }\n\n        String replacementText = null;\n        Table table = null;\n        try {\n          table = getTableObjectByName(tabIdName);\n        } catch (HiveException e) {\n          // Table may not be found when materialization of CTE is on.\n          LOG.info(\"Table \" + tabIdName + \" is not found in walkASTMarkTABREF.\");\n          continue;\n        }\n\n        List<String> colNames = new ArrayList<>();\n        List<String> colTypes = new ArrayList<>();\n        for (FieldSchema col : table.getAllCols()) {\n          colNames.add(col.getName());\n          colTypes.add(col.getType());\n        }\n\n        basicInfos.put(new HivePrivilegeObject(table.getDbName(), table.getTableName(), colNames),\n            new MaskAndFilterInfo(colTypes, additionalTabInfo.toString(), alias, astNode, table.isView()));\n      }\n      if (astNode.getChildCount() > 0 && !ignoredTokens.contains(astNode.getToken().getType())) {\n        for (Node child : astNode.getChildren()) {\n          queue.offer(child);\n        }\n      }\n    }\n    List<HivePrivilegeObject> basicPrivObjs = new ArrayList<>();\n    basicPrivObjs.addAll(basicInfos.keySet());\n    List<HivePrivilegeObject> needRewritePrivObjs = tableMask\n        .applyRowFilterAndColumnMasking(basicPrivObjs);\n    if (needRewritePrivObjs != null && !needRewritePrivObjs.isEmpty()) {\n      for (HivePrivilegeObject privObj : needRewritePrivObjs) {\n        // We don't support masking/filtering against ACID query at the moment\n        if (ctx.getIsUpdateDeleteMerge()) {\n          throw new SemanticException(ErrorMsg.MASKING_FILTERING_ON_ACID_NOT_SUPPORTED,\n              privObj.getDbname(), privObj.getObjectName());\n        }\n        MaskAndFilterInfo info = basicInfos.get(privObj);\n        String replacementText = tableMask.create(privObj, info);\n        if (replacementText != null) {\n          tableMask.setNeedsRewrite(true);\n          tableMask.addTranslation(info.astNode, replacementText);\n        }\n      }\n    }\n  }",
            "10747  \n10748  \n10749  \n10750  \n10751  \n10752  \n10753  \n10754  \n10755  \n10756  \n10757  \n10758  \n10759  \n10760  \n10761  \n10762  \n10763  \n10764  \n10765  \n10766  \n10767  \n10768  \n10769  \n10770  \n10771  \n10772  \n10773  \n10774  \n10775  \n10776  \n10777  \n10778  \n10779  \n10780  \n10781  \n10782  \n10783  \n10784  \n10785  \n10786  \n10787  \n10788  \n10789  \n10790  \n10791  \n10792  \n10793  \n10794  \n10795  \n10796  \n10797  \n10798  \n10799  \n10800  \n10801  \n10802  \n10803  \n10804  \n10805  \n10806  \n10807  \n10808  \n10809  \n10810  \n10811  \n10812  \n10813  \n10814  \n10815  \n10816  \n10817  \n10818  \n10819  \n10820  \n10821  \n10822  \n10823  \n10824 +\n10825 +\n10826 +\n10827 +\n10828 +\n10829  \n10830  \n10831  \n10832  \n10833  \n10834  ",
            "  private void walkASTMarkTABREF(ASTNode ast, Set<String> cteAlias)\n      throws SemanticException {\n    Queue<Node> queue = new LinkedList<>();\n    queue.add(ast);\n    Map<HivePrivilegeObject, MaskAndFilterInfo> basicInfos = new LinkedHashMap<>();\n    while (!queue.isEmpty()) {\n      ASTNode astNode = (ASTNode) queue.poll();\n      if (astNode.getToken().getType() == HiveParser.TOK_TABREF) {\n        int aliasIndex = 0;\n        StringBuffer additionalTabInfo = new StringBuffer();\n        for (int index = 1; index < astNode.getChildCount(); index++) {\n          ASTNode ct = (ASTNode) astNode.getChild(index);\n          if (ct.getToken().getType() == HiveParser.TOK_TABLEBUCKETSAMPLE\n              || ct.getToken().getType() == HiveParser.TOK_TABLESPLITSAMPLE\n              || ct.getToken().getType() == HiveParser.TOK_TABLEPROPERTIES) {\n            additionalTabInfo.append(ctx.getTokenRewriteStream().toString(ct.getTokenStartIndex(),\n                ct.getTokenStopIndex()));\n          } else {\n            aliasIndex = index;\n          }\n        }\n\n        ASTNode tableTree = (ASTNode) (astNode.getChild(0));\n\n        String tabIdName = getUnescapedName(tableTree);\n\n        String alias;\n        if (aliasIndex != 0) {\n          alias = unescapeIdentifier(astNode.getChild(aliasIndex).getText());\n        } else {\n          alias = getUnescapedUnqualifiedTableName(tableTree);\n        }\n\n        // We need to know if it is CTE or not.\n        // A CTE may have the same name as a table.\n        // For example,\n        // with select TAB1 [masking] as TAB2\n        // select * from TAB2 [no masking]\n        if (cteAlias.contains(tabIdName)) {\n          continue;\n        }\n\n        String replacementText = null;\n        Table table = null;\n        try {\n          table = getTableObjectByName(tabIdName);\n        } catch (HiveException e) {\n          // Table may not be found when materialization of CTE is on.\n          LOG.info(\"Table \" + tabIdName + \" is not found in walkASTMarkTABREF.\");\n          continue;\n        }\n\n        List<String> colNames = new ArrayList<>();\n        List<String> colTypes = new ArrayList<>();\n        for (FieldSchema col : table.getAllCols()) {\n          colNames.add(col.getName());\n          colTypes.add(col.getType());\n        }\n\n        basicInfos.put(new HivePrivilegeObject(table.getDbName(), table.getTableName(), colNames),\n            new MaskAndFilterInfo(colTypes, additionalTabInfo.toString(), alias, astNode, table.isView()));\n      }\n      if (astNode.getChildCount() > 0 && !ignoredTokens.contains(astNode.getToken().getType())) {\n        for (Node child : astNode.getChildren()) {\n          queue.offer(child);\n        }\n      }\n    }\n    List<HivePrivilegeObject> basicPrivObjs = new ArrayList<>();\n    basicPrivObjs.addAll(basicInfos.keySet());\n    List<HivePrivilegeObject> needRewritePrivObjs = tableMask\n        .applyRowFilterAndColumnMasking(basicPrivObjs);\n    if (needRewritePrivObjs != null && !needRewritePrivObjs.isEmpty()) {\n      for (HivePrivilegeObject privObj : needRewritePrivObjs) {\n        MaskAndFilterInfo info = basicInfos.get(privObj);\n        String replacementText = tableMask.create(privObj, info);\n        if (replacementText != null) {\n          // We don't support masking/filtering against ACID query at the moment\n          if (ctx.getIsUpdateDeleteMerge()) {\n            throw new SemanticException(ErrorMsg.MASKING_FILTERING_ON_ACID_NOT_SUPPORTED,\n                privObj.getDbname(), privObj.getObjectName());\n          }\n          tableMask.setNeedsRewrite(true);\n          tableMask.addTranslation(info.astNode, replacementText);\n        }\n      }\n    }\n  }"
        ]
    ],
    "928b4c01531470601a68950625bbe64faea95674": [
        [
            "ParseUtils::processSetColsNode(ASTNode,ASTSearcher)",
            " 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 -\n 404 -\n 405 -\n 406 -\n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  ",
            "    /**\n     * Replaces a spurious TOK_SETCOLREF added by parser with column names referring to the query\n     * in e.g. a union. This is to maintain the expectations that some code, like order by position\n     * alias, might have about not having ALLCOLREF. If it cannot find the columns with confidence\n     * it will just replace SETCOLREF with ALLCOLREF. Most of the cases where that happens are\n     * easy to work around in the query (e.g. by adding column aliases in the union).\n     * @param setCols TOK_SETCOLREF ASTNode.\n     * @param searcher AST searcher to reuse.\n     */\n    private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {\n      searcher.reset();\n      CommonTree rootNode = setCols;\n      while (rootNode != null && rootNode.getType() != HiveParser.TOK_INSERT) {\n        rootNode = rootNode.parent;\n      }\n      if (rootNode == null || rootNode.parent == null) {\n        // Couldn't find the parent insert; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the root INSERT\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n      rootNode = rootNode.parent; // TOK_QUERY above insert\n      Tree fromNode = null;\n      for (int j = 0; j < rootNode.getChildCount(); ++j) {\n        Tree child = rootNode.getChild(j);\n        if (child.getType() == HiveParser.TOK_FROM) {\n          fromNode = child;\n          break;\n        }\n      }\n      if (!(fromNode instanceof ASTNode)) {\n        // Couldn't find the from that contains subquery; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the FROM\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n      // We are making what we are trying to do more explicit if there's a union alias; so\n      // that if we do something we didn't expect to do, it'd be more likely to fail.\n      String alias = null;\n      if (fromNode.getChildCount() > 0) {\n        Tree fromWhat = fromNode.getChild(0);\n        if (fromWhat.getType() == HiveParser.TOK_SUBQUERY && fromWhat.getChildCount() > 1) {\n          Tree child = fromWhat.getChild(fromWhat.getChildCount() - 1);\n          if (child.getType() == HiveParser.Identifier) {\n            alias = child.getText();\n          }\n        }\n      }\n      // We find the SELECT closest to the top. This assumes there's only one FROM or FROM-s\n      // are all equivalent (union case). Also, this assumption could be false for an already\n      // malformed query; we don't check for that here - it will fail later anyway.\n      // TODO: Maybe we should find ALL the SELECT-s not nested in another from, and compare.\n      ASTNode select = searcher.simpleBreadthFirstSearchAny((ASTNode)fromNode,\n          HiveParser.TOK_SELECT, HiveParser.TOK_SELECTDI);\n      if (select == null) {\n        // Couldn't find the from that contains subquery; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the SELECT\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n      // Found the proper columns.\n      List<ASTNode> newChildren = new ArrayList<>(select.getChildCount());\n      HashSet<String> aliases = new HashSet<>();\n      for (int i = 0; i < select.getChildCount(); ++i) {\n        Tree selExpr = select.getChild(i);\n        assert selExpr.getType() == HiveParser.TOK_SELEXPR;\n        assert selExpr.getChildCount() > 0;\n        // Examine the last child. It could be an alias.\n        Tree child = selExpr.getChild(selExpr.getChildCount() - 1);\n        switch (child.getType()) {\n        case HiveParser.TOK_SETCOLREF:\n          // We have a nested setcolref. Process that and start from scratch TODO: use stack?\n          processSetColsNode((ASTNode)child, searcher);\n          processSetColsNode(setCols, searcher);\n          return;\n        case HiveParser.TOK_ALLCOLREF:\n          // We should find an alias of this insert and do (alias).*. This however won't fix e.g.\n          // positional order by alias case, cause we'd still have a star on the top level. Bail.\n          LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because of nested ALLCOLREF\");\n          setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n          return;\n        case HiveParser.TOK_TABLE_OR_COL:\n          Tree idChild = child.getChild(0);\n          assert idChild.getType() == HiveParser.Identifier : idChild;\n          if (!createChildColumnRef(idChild, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        case HiveParser.Identifier:\n          if (!createChildColumnRef(child, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        case HiveParser.DOT: {\n          Tree colChild = child.getChild(child.getChildCount() - 1);\n          assert colChild.getType() == HiveParser.Identifier : colChild;\n          if (!createChildColumnRef(colChild, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        }\n        default:\n          // Not really sure how to refer to this (or if we can).\n          // TODO: We could find a different from branch for the union, that might have an alias?\n          //       Or we could add an alias here to refer to, but that might break other branches.\n          LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because of the nested node \"\n              + child.getType() + \" \" + child.getText());\n          setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n          return;\n        }\n      }\n      // Insert search in the beginning would have failed if these parents didn't exist.\n      ASTNode parent = (ASTNode)setCols.parent.parent;\n      int t = parent.getType();\n      assert t == HiveParser.TOK_SELECT || t == HiveParser.TOK_SELECTDI : t;\n      int ix = setCols.parent.childIndex;\n      parent.deleteChild(ix);\n      for (ASTNode node : newChildren) {\n        parent.insertChild(ix++, node);\n      }\n    }",
            " 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405 +\n 406 +\n 407 +\n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 +\n 417 +\n 418 +\n 419 +\n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428 +\n 429 +\n 430 +\n 431 +\n 432 +\n 433 +\n 434 +\n 435 +\n 436 +\n 437 +\n 438 +\n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "    /**\n     * Replaces a spurious TOK_SETCOLREF added by parser with column names referring to the query\n     * in e.g. a union. This is to maintain the expectations that some code, like order by position\n     * alias, might have about not having ALLCOLREF. If it cannot find the columns with confidence\n     * it will just replace SETCOLREF with ALLCOLREF. Most of the cases where that happens are\n     * easy to work around in the query (e.g. by adding column aliases in the union).\n     * @param setCols TOK_SETCOLREF ASTNode.\n     * @param searcher AST searcher to reuse.\n     */\n    private static void processSetColsNode(ASTNode setCols, ASTSearcher searcher) {\n      searcher.reset();\n      CommonTree rootNode = setCols;\n      while (rootNode != null && rootNode.getType() != HiveParser.TOK_INSERT) {\n        rootNode = rootNode.parent;\n      }\n      if (rootNode == null || rootNode.parent == null) {\n        // Couldn't find the parent insert; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the root INSERT\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n      rootNode = rootNode.parent; // TOK_QUERY above insert\n      Tree fromNode = null;\n      for (int j = 0; j < rootNode.getChildCount(); ++j) {\n        Tree child = rootNode.getChild(j);\n        if (child.getType() == HiveParser.TOK_FROM) {\n          fromNode = child;\n          break;\n        }\n      }\n      if (!(fromNode instanceof ASTNode)) {\n        // Couldn't find the from that contains subquery; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the FROM\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n      // We are making what we are trying to do more explicit if there's a union alias; so\n      // that if we do something we didn't expect to do, it'd be more likely to fail.\n      String alias = null;\n      if (fromNode.getChildCount() > 0) {\n        Tree fromWhat = fromNode.getChild(0);\n        if (fromWhat.getType() == HiveParser.TOK_SUBQUERY && fromWhat.getChildCount() > 1) {\n          Tree child = fromWhat.getChild(fromWhat.getChildCount() - 1);\n          if (child.getType() == HiveParser.Identifier) {\n            alias = child.getText();\n          }\n        }\n      }\n      // Note: we assume that this isn't an already malformed query;\n      //       we don't check for that here - it will fail later anyway.\n      // First, we find the SELECT closest to the top.\n      ASTNode select = searcher.simpleBreadthFirstSearchAny((ASTNode)fromNode,\n          HiveParser.TOK_SELECT, HiveParser.TOK_SELECTDI);\n      if (select == null) {\n        // Couldn't find the from that contains subquery; replace with ALLCOLREF.\n        LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the SELECT\");\n        setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n        return;\n      }\n\n      // Then, find the leftmost logical sibling select, because that's what Hive uses for aliases. \n      while (true) {\n        CommonTree queryOfSelect = select.parent;\n        while (queryOfSelect != null && queryOfSelect.getType() != HiveParser.TOK_QUERY) {\n          queryOfSelect = queryOfSelect.parent;\n        }\n        // We should have some QUERY; and also its parent because by supposition we are in subq.\n        if (queryOfSelect == null || queryOfSelect.parent == null) {\n          LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because we couldn't find the QUERY\");\n          setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n          return;\n        }\n        if (queryOfSelect.childIndex == 0) break; // We are the left-most child.\n        Tree moreToTheLeft = queryOfSelect.parent.getChild(0);\n        Preconditions.checkState(moreToTheLeft != queryOfSelect);\n        ASTNode newSelect = searcher.simpleBreadthFirstSearchAny((ASTNode)moreToTheLeft,\n          HiveParser.TOK_SELECT, HiveParser.TOK_SELECTDI);\n        Preconditions.checkState(newSelect != select);\n        select = newSelect;\n        // Repeat the procedure for the new select.\n      }\n\n      // Found the proper columns.\n      List<ASTNode> newChildren = new ArrayList<>(select.getChildCount());\n      HashSet<String> aliases = new HashSet<>();\n      for (int i = 0; i < select.getChildCount(); ++i) {\n        Tree selExpr = select.getChild(i);\n        assert selExpr.getType() == HiveParser.TOK_SELEXPR;\n        assert selExpr.getChildCount() > 0;\n        // Examine the last child. It could be an alias.\n        Tree child = selExpr.getChild(selExpr.getChildCount() - 1);\n        switch (child.getType()) {\n        case HiveParser.TOK_SETCOLREF:\n          // We have a nested setcolref. Process that and start from scratch TODO: use stack?\n          processSetColsNode((ASTNode)child, searcher);\n          processSetColsNode(setCols, searcher);\n          return;\n        case HiveParser.TOK_ALLCOLREF:\n          // We should find an alias of this insert and do (alias).*. This however won't fix e.g.\n          // positional order by alias case, cause we'd still have a star on the top level. Bail.\n          LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because of nested ALLCOLREF\");\n          setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n          return;\n        case HiveParser.TOK_TABLE_OR_COL:\n          Tree idChild = child.getChild(0);\n          assert idChild.getType() == HiveParser.Identifier : idChild;\n          if (!createChildColumnRef(idChild, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        case HiveParser.Identifier:\n          if (!createChildColumnRef(child, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        case HiveParser.DOT: {\n          Tree colChild = child.getChild(child.getChildCount() - 1);\n          assert colChild.getType() == HiveParser.Identifier : colChild;\n          if (!createChildColumnRef(colChild, alias, newChildren, aliases)) {\n            setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n            return;\n          }\n          break;\n        }\n        default:\n          // Not really sure how to refer to this (or if we can).\n          // TODO: We could find a different from branch for the union, that might have an alias?\n          //       Or we could add an alias here to refer to, but that might break other branches.\n          LOG.debug(\"Replacing SETCOLREF with ALLCOLREF because of the nested node \"\n              + child.getType() + \" \" + child.getText());\n          setCols.token.setType(HiveParser.TOK_ALLCOLREF);\n          return;\n        }\n      }\n      // Insert search in the beginning would have failed if these parents didn't exist.\n      ASTNode parent = (ASTNode)setCols.parent.parent;\n      int t = parent.getType();\n      assert t == HiveParser.TOK_SELECT || t == HiveParser.TOK_SELECTDI : t;\n      int ix = setCols.parent.childIndex;\n      parent.deleteChild(ix);\n      for (ASTNode node : newChildren) {\n        parent.insertChild(ix++, node);\n      }\n    }"
        ]
    ],
    "4904ab78639f11b295d869cd9ea08072c2218a21": [
        [
            "DruidOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132 -\n 133  \n 134 -\n 135 -\n 136 -\n 137 -\n 138 -\n 139  \n 140  \n 141 -\n 142 -\n 143 -\n 144  \n 145  \n 146 -\n 147 -\n 148 -\n 149 -\n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      TypeInfo f = columnTypes.get(i);\n      assert f.getCategory() == ObjectInspector.Category.PRIMITIVE;\n      AggregatorFactory af;\n      switch (f.getTypeName()) {\n        case serdeConstants.TINYINT_TYPE_NAME:\n        case serdeConstants.SMALLINT_TYPE_NAME:\n        case serdeConstants.INT_TYPE_NAME:\n        case serdeConstants.BIGINT_TYPE_NAME:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case serdeConstants.FLOAT_TYPE_NAME:\n        case serdeConstants.DOUBLE_TYPE_NAME:\n        case serdeConstants.DECIMAL_TYPE_NAME:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        default:\n          // Dimension or timestamp\n          String columnName = columnNames.get(i);\n          if (!columnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !columnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            dimensions.add(new StringDimensionSchema(columnName));\n          }\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133 +\n 134  \n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140  \n 141  \n 142 +\n 143 +\n 144 +\n 145  \n 146  \n 147 +\n 148 +\n 149 +\n 150  \n 151 +\n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158 +\n 159 +\n 160 +\n 161 +\n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);\n      AggregatorFactory af;\n      switch (f.getPrimitiveCategory()) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n        case DECIMAL:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case TIMESTAMP:\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=\n                  PrimitiveGrouping.STRING_GROUP) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)"
        ]
    ],
    "4a041428b14cd195c5d89c93e07ef518652fdcd3": [
        [
            "DruidOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 -\n 194 -\n 195 -\n 196 -\n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            null,\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);\n      AggregatorFactory af;\n      switch (f.getPrimitiveCategory()) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n        case DECIMAL:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case TIMESTAMP:\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=\n                  PrimitiveGrouping.STRING_GROUP) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    final RealtimeTuningConfig realtimeTuningConfig = RealtimeTuningConfig\n            .makeDefaultTuningConfig(new File(\n                    basePersistDirectory))\n            .withVersioningPolicy(new CustomVersioningPolicy(version));\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107 +\n 108 +\n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY) != null\n                    ? tableProperties.getProperty(Constants.DRUID_SEGMENT_DIRECTORY)\n                    : HiveConf.getVar(jc, HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY);\n\n    final HdfsDataSegmentPusherConfig hdfsDataSegmentPusherConfig = new HdfsDataSegmentPusherConfig();\n    hdfsDataSegmentPusherConfig.setStorageDirectory(segmentDirectory);\n    final DataSegmentPusher hdfsDataSegmentPusher = new HdfsDataSegmentPusher(\n            hdfsDataSegmentPusherConfig, jc, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.valueOf(segmentGranularity),\n            QueryGranularity.fromString(\n                    tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY) == null\n                            ? \"NONE\"\n                            : tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY)),\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidTable.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidTable.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      PrimitiveTypeInfo f = (PrimitiveTypeInfo) columnTypes.get(i);\n      AggregatorFactory af;\n      switch (f.getPrimitiveCategory()) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n        case DECIMAL:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case TIMESTAMP:\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(DruidTable.DEFAULT_TIMESTAMP_COLUMN) && !tColumnName\n                  .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(f.getPrimitiveCategory()) !=\n                  PrimitiveGrouping.STRING_GROUP) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    f.getPrimitiveCategory());\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    Integer maxRowInMemory = HiveConf.getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_ROW_IN_MEMORY);\n\n    RealtimeTuningConfig realtimeTuningConfig = new RealtimeTuningConfig(maxRowInMemory,\n            null,\n            null,\n            new File(basePersistDirectory),\n            new CustomVersioningPolicy(version),\n            null,\n            null,\n            null,\n            null,\n            true,\n            0,\n            0,\n            true,\n            null\n    );\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig, hdfsDataSegmentPusher,\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)"
        ]
    ],
    "35d707950ddd210c37533be3da51cea730bac881": [
        [
            "LocalHiveSparkClient::execute(DriverContext,SparkWork)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "  @Override\n  public SparkJobRef execute(DriverContext driverContext, SparkWork sparkWork) throws Exception {\n    Context ctx = driverContext.getCtx();\n    HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    JobConf jobConf = new JobConf(hiveConf);\n\n    // Create temporary scratch dir\n    Path emptyScratchDir;\n    emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    // Update credential provider location\n    // the password to the credential provider in already set in the sparkConf\n    // in HiveSparkClientFactory\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    SparkCounters sparkCounters = new SparkCounters(sc);\n    Map<String, List<String>> prefixes = sparkWork.getRequiredCounterPrefix();\n    if (prefixes != null) {\n      for (String group : prefixes.keySet()) {\n        for (String counterName : prefixes.get(group)) {\n          sparkCounters.createCounter(group, counterName);\n        }\n      }\n    }\n    SparkReporter sparkReporter = new SparkReporter(sparkCounters);\n\n    // Generate Spark plan\n    SparkPlanGenerator gen =\n      new SparkPlanGenerator(sc, ctx, jobConf, emptyScratchDir, sparkReporter);\n    SparkPlan plan = gen.generate(sparkWork);\n\n    // Execute generated plan.\n    JavaPairRDD<HiveKey, BytesWritable> finalRDD = plan.generateGraph();\n    // We use Spark RDD async action to submit job as it's the only way to get jobId now.\n    JavaFutureAction<Void> future = finalRDD.foreachAsync(HiveVoidFunction.getInstance());\n    // As we always use foreach action to submit RDD graph, it would only trigger one job.\n    int jobId = future.jobIds().get(0);\n    LocalSparkJobStatus sparkJobStatus = new LocalSparkJobStatus(\n      sc, jobId, jobMetricsListener, sparkCounters, plan.getCachedRDDIds(), future);\n    return new LocalSparkJobRef(Integer.toString(jobId), hiveConf,  sparkJobStatus, sc);\n  }",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140 +\n 141 +\n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @Override\n  public SparkJobRef execute(DriverContext driverContext, SparkWork sparkWork) throws Exception {\n    Context ctx = driverContext.getCtx();\n    HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    JobConf jobConf = new JobConf(hiveConf);\n\n    // Create temporary scratch dir\n    Path emptyScratchDir;\n    emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    // Update credential provider location\n    // the password to the credential provider in already set in the sparkConf\n    // in HiveSparkClientFactory\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    SparkCounters sparkCounters = new SparkCounters(sc);\n    Map<String, List<String>> prefixes = sparkWork.getRequiredCounterPrefix();\n    if (prefixes != null) {\n      for (String group : prefixes.keySet()) {\n        for (String counterName : prefixes.get(group)) {\n          sparkCounters.createCounter(group, counterName);\n        }\n      }\n    }\n    SparkReporter sparkReporter = new SparkReporter(sparkCounters);\n\n    // Generate Spark plan\n    SparkPlanGenerator gen =\n      new SparkPlanGenerator(sc, ctx, jobConf, emptyScratchDir, sparkReporter);\n    SparkPlan plan = gen.generate(sparkWork);\n\n    if (driverContext.isShutdown()) {\n      throw new HiveException(\"Operation is cancelled.\");\n    }\n\n    // Execute generated plan.\n    JavaPairRDD<HiveKey, BytesWritable> finalRDD = plan.generateGraph();\n    // We use Spark RDD async action to submit job as it's the only way to get jobId now.\n    JavaFutureAction<Void> future = finalRDD.foreachAsync(HiveVoidFunction.getInstance());\n    // As we always use foreach action to submit RDD graph, it would only trigger one job.\n    int jobId = future.jobIds().get(0);\n    LocalSparkJobStatus sparkJobStatus = new LocalSparkJobStatus(\n      sc, jobId, jobMetricsListener, sparkCounters, plan.getCachedRDDIds(), future);\n    return new LocalSparkJobRef(Integer.toString(jobId), hiveConf,  sparkJobStatus, sc);\n  }"
        ],
        [
            "ZooKeeperHiveLockManager::unlockPrimitive(HiveLock,String,CuratorFramework)",
            " 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489 -\n 490  \n 491  \n 492 -\n 493  \n 494 -\n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  ",
            "  @VisibleForTesting\n  static void unlockPrimitive(HiveLock hiveLock, String parent, CuratorFramework curatorFramework) throws LockException {\n    ZooKeeperHiveLock zLock = (ZooKeeperHiveLock)hiveLock;\n    HiveLockMode lMode = hiveLock.getHiveLockMode();\n    HiveLockObject obj = zLock.getHiveLockObject();\n    String name  = getLastObjectName(parent, obj);\n    try {\n      curatorFramework.delete().forPath(zLock.getPath());\n\n      // Delete the parent node if all the children have been deleted\n      List<String> children = curatorFramework.getChildren().forPath(name);\n      if (children == null || children.isEmpty()) {\n        curatorFramework.delete().forPath(name);\n      }\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          switch(lMode) {\n          case EXCLUSIVE:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);\n            break;\n          case SEMI_SHARED:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);\n            break;\n          default:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);\n            break;\n          }\n        } catch (Exception e) {\n          LOG.warn(\"Error Reporting hive client zookeeper unlock operation to Metrics system\", e);\n        }\n      }\n    } catch (KeeperException.NoNodeException nne) {\n      //can happen in retrying deleting the zLock after exceptions like InterruptedException\n      //or in a race condition where parent has already been deleted by other process when it\n      //is to be deleted. Both cases should not raise error\n      LOG.debug(\"Node \" + zLock.getPath() + \" or its parent has already been deleted.\");\n    } catch (KeeperException.NotEmptyException nee) {\n      //can happen in a race condition where another process adds a zLock under this parent\n      //just before it is about to be deleted. It should not be a problem since this parent\n      //can eventually be deleted by the process which hold its last child zLock\n      LOG.debug(\"Node \" + name + \" to be deleted is not empty.\");\n    } catch (Exception e) {\n      //exceptions including InterruptException and other KeeperException\n      LOG.error(\"Failed to release ZooKeeper lock: \", e);\n      throw new LockException(e);\n    }\n  }",
            " 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489 +\n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495  \n 496  \n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502 +\n 503  \n 504 +\n 505 +\n 506 +\n 507 +\n 508 +\n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  ",
            "  @VisibleForTesting\n  static void unlockPrimitive(HiveLock hiveLock, String parent, CuratorFramework curatorFramework) throws LockException {\n    ZooKeeperHiveLock zLock = (ZooKeeperHiveLock)hiveLock;\n    HiveLockMode lMode = hiveLock.getHiveLockMode();\n    HiveLockObject obj = zLock.getHiveLockObject();\n    String name  = getLastObjectName(parent, obj);\n    try {\n      //catch InterruptedException to make sure locks can be released when the query is cancelled.\n      try {\n        curatorFramework.delete().forPath(zLock.getPath());\n      } catch (InterruptedException ie) {\n        curatorFramework.delete().forPath(zLock.getPath());\n      }\n\n      // Delete the parent node if all the children have been deleted\n      List<String> children = null;\n      try {\n        children = curatorFramework.getChildren().forPath(name);\n      } catch (InterruptedException ie) {\n        children = curatorFramework.getChildren().forPath(name);\n      }\n      if (children == null || children.isEmpty()) {\n        try {\n          curatorFramework.delete().forPath(name);\n        } catch (InterruptedException ie) {\n          curatorFramework.delete().forPath(name);\n        }\n      }\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          switch(lMode) {\n          case EXCLUSIVE:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);\n            break;\n          case SEMI_SHARED:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);\n            break;\n          default:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);\n            break;\n          }\n        } catch (Exception e) {\n          LOG.warn(\"Error Reporting hive client zookeeper unlock operation to Metrics system\", e);\n        }\n      }\n    } catch (KeeperException.NoNodeException nne) {\n      //can happen in retrying deleting the zLock after exceptions like InterruptedException\n      //or in a race condition where parent has already been deleted by other process when it\n      //is to be deleted. Both cases should not raise error\n      LOG.debug(\"Node \" + zLock.getPath() + \" or its parent has already been deleted.\");\n    } catch (KeeperException.NotEmptyException nee) {\n      //can happen in a race condition where another process adds a zLock under this parent\n      //just before it is about to be deleted. It should not be a problem since this parent\n      //can eventually be deleted by the process which hold its last child zLock\n      LOG.debug(\"Node \" + name + \" to be deleted is not empty.\");\n    } catch (Exception e) {\n      //exceptions including InterruptException and other KeeperException\n      LOG.error(\"Failed to release ZooKeeper lock: \", e);\n      throw new LockException(e);\n    }\n  }"
        ],
        [
            "ExecDriver::execute(DriverContext)",
            " 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "   /**\n   * Execute a query plan using Hadoop.\n   */\n  @SuppressWarnings({\"deprecation\", \"unchecked\"})\n  @Override\n  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n    JobClient jc = null;\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass(JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    propagateSplitSettings(job, mWork);\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat(JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      job.set(MRJobConfig.JOB_NAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(ctx, mWork, job);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          List<String> statsTmpDir = Utilities.getStatsTmpDirs(mWork, job);\n          if (rWork != null) {\n            statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork, job));\n          }\n          StatsCollectionContext sc = new StatsCollectionContext(job);\n          sc.setStatsTmpDirs(statsTmpDir);\n          if (!statsPublisher.init(sc)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().closeIfNotDefault(session, true);\n      }\n\n      HiveConfUtil.updateJobCredentialProviders(job);\n      // Finally SUBMIT the JOB!\n      rj = jc.submitJob(job);\n      this.jobID = rj.getJobID();\n      updateStatusInQueryDisplay();\n      returnVal = jobExecHelper.progress(rj, jc, ctx);\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      setException(e);\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          jobID = rj.getID().toString();\n        }\n        if (jc!=null) {\n          jc.close();\n        }\n      } catch (Exception e) {\n\tLOG.warn(\"Failed while cleaning up \", e);\n      } finally {\n\tHadoopJobExecHelper.runningJobs.remove(rj);\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        setException(e);\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }",
            " 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406 +\n 407 +\n 408 +\n 409 +\n 410 +\n 411  \n 412 +\n 413 +\n 414 +\n 415 +\n 416 +\n 417 +\n 418 +\n 419 +\n 420 +\n 421 +\n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  ",
            "   /**\n   * Execute a query plan using Hadoop.\n   */\n  @SuppressWarnings({\"deprecation\", \"unchecked\"})\n  @Override\n  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n    JobClient jc = null;\n\n    if (driverContext.isShutdown()) {\n      LOG.warn(\"Task was cancelled\");\n      return 5;\n    }\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass(JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    propagateSplitSettings(job, mWork);\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat(JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      job.set(MRJobConfig.JOB_NAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(ctx, mWork, job);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          List<String> statsTmpDir = Utilities.getStatsTmpDirs(mWork, job);\n          if (rWork != null) {\n            statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork, job));\n          }\n          StatsCollectionContext sc = new StatsCollectionContext(job);\n          sc.setStatsTmpDirs(statsTmpDir);\n          if (!statsPublisher.init(sc)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().closeIfNotDefault(session, true);\n      }\n\n      HiveConfUtil.updateJobCredentialProviders(job);\n      // Finally SUBMIT the JOB!\n      if (driverContext.isShutdown()) {\n        LOG.warn(\"Task was cancelled\");\n        return 5;\n      }\n\n      rj = jc.submitJob(job);\n\n      if (driverContext.isShutdown()) {\n        LOG.warn(\"Task was cancelled\");\n        if (rj != null) {\n          rj.killJob();\n          rj = null;\n        }\n        return 5;\n      }\n\n      this.jobID = rj.getJobID();\n      updateStatusInQueryDisplay();\n      returnVal = jobExecHelper.progress(rj, jc, ctx);\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      setException(e);\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          jobID = rj.getID().toString();\n        }\n        if (jc!=null) {\n          jc.close();\n        }\n      } catch (Exception e) {\n\tLOG.warn(\"Failed while cleaning up \", e);\n      } finally {\n\tHadoopJobExecHelper.runningJobs.remove(rj);\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        setException(e);\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }"
        ],
        [
            "RemoteHiveSparkClient::submit(DriverContext,SparkWork)",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "  private SparkJobRef submit(final DriverContext driverContext, final SparkWork sparkWork) throws Exception {\n    final Context ctx = driverContext.getCtx();\n    final HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    final JobConf jobConf = new JobConf(hiveConf);\n\n    //update the credential provider location in the jobConf\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    // Create temporary scratch dir\n    final Path emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    byte[] jobConfBytes = KryoSerializer.serializeJobConf(jobConf);\n    byte[] scratchDirBytes = KryoSerializer.serialize(emptyScratchDir);\n    byte[] sparkWorkBytes = KryoSerializer.serialize(sparkWork);\n\n    JobStatusJob job = new JobStatusJob(jobConfBytes, scratchDirBytes, sparkWorkBytes);\n    JobHandle<Serializable> jobHandle = remoteClient.submit(job);\n    RemoteSparkJobStatus sparkJobStatus = new RemoteSparkJobStatus(remoteClient, jobHandle, sparkClientTimtout);\n    return new RemoteSparkJobRef(hiveConf, jobHandle, sparkJobStatus);\n  }",
            " 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212 +\n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  ",
            "  private SparkJobRef submit(final DriverContext driverContext, final SparkWork sparkWork) throws Exception {\n    final Context ctx = driverContext.getCtx();\n    final HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    final JobConf jobConf = new JobConf(hiveConf);\n\n    //update the credential provider location in the jobConf\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    // Create temporary scratch dir\n    final Path emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    byte[] jobConfBytes = KryoSerializer.serializeJobConf(jobConf);\n    byte[] scratchDirBytes = KryoSerializer.serialize(emptyScratchDir);\n    byte[] sparkWorkBytes = KryoSerializer.serialize(sparkWork);\n\n    JobStatusJob job = new JobStatusJob(jobConfBytes, scratchDirBytes, sparkWorkBytes);\n    if (driverContext.isShutdown()) {\n      throw new HiveException(\"Operation is cancelled.\");\n    }\n\n    JobHandle<Serializable> jobHandle = remoteClient.submit(job);\n    RemoteSparkJobStatus sparkJobStatus = new RemoteSparkJobStatus(remoteClient, jobHandle, sparkClientTimtout);\n    return new RemoteSparkJobRef(hiveConf, jobHandle, sparkJobStatus);\n  }"
        ],
        [
            "Driver::isInterrupted()",
            " 654  \n 655  \n 656  \n 657  \n 658 -\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  ",
            "  private boolean isInterrupted() {\n    lDrvState.stateLock.lock();\n    try {\n      if (lDrvState.driverState == DriverState.INTERRUPT) {\n        Thread.currentThread().interrupt();\n        return true;\n      } else {\n        return false;\n      }\n    } finally {\n      lDrvState.stateLock.unlock();\n    }\n  }",
            " 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "  private boolean isInterrupted() {\n    lDrvState.stateLock.lock();\n    try {\n      if (lDrvState.driverState == DriverState.INTERRUPT) {\n        return true;\n      } else {\n        return false;\n      }\n    } finally {\n      lDrvState.stateLock.unlock();\n    }\n  }"
        ]
    ],
    "be47d9e3fae437c7644e47679119d20b86f8a332": [
        [
            "DynamicPartitionPruningOptimization::generateSemiJoinOperatorPlan(DynamicListContext,ParseContext,TableScanOperator,String)",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 -\n 398 -\n 399 -\n 400 -\n 401 -\n 402 -\n 403 -\n 404 -\n 405 -\n 406 -\n 407 -\n 408 -\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 -\n 427 -\n 428 -\n 429 -\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on parition column.\n      String internalColName = null;\n      ExprNodeDesc exprNodeDesc = key;\n      // Find the ExprNodeColumnDesc\n      while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n              (exprNodeDesc.getChildren() != null)) {\n        exprNodeDesc = exprNodeDesc.getChildren().get(0);\n      }\n\n      if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n        internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      } else {\n        // No column found!\n        // Bail out\n        return false;\n      }\n    }\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    new RowSchema(parentOfRS.getSchema()), parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(groupByOpFinal.getOperatorId());\n      rsOp.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(ts.getOperatorId());\n      rsOpFinal.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 +\n 398 +\n 399 +\n 400 +\n 401 +\n 402 +\n 403 +\n 404  \n 405 +\n 406 +\n 407 +\n 408 +\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426 +\n 427 +\n 428 +\n 429 +\n 430  \n 431 +\n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 +\n 442 +\n 443 +\n 444 +\n 445 +\n 446 +\n 447 +\n 448 +\n 449 +\n 450 +\n 451 +\n 452 +\n 453 +\n 454 +\n 455 +\n 456 +\n 457 +\n 458 +\n 459 +\n 460 +\n 461 +\n 462 +\n 463 +\n 464  \n 465  \n 466 +\n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    String internalColName = null;\n    ExprNodeDesc exprNodeDesc = key;\n    // Find the ExprNodeColumnDesc\n    while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n            (exprNodeDesc.getChildren() != null)) {\n      exprNodeDesc = exprNodeDesc.getChildren().get(0);\n    }\n\n    if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n      internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n      if (parentOfRS instanceof SelectOperator) {\n        // Make sure the semijoin branch is not on parition column.\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      }\n    } else {\n      // No column found!\n      // Bail out\n      return false;\n    }\n\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n\n    // Create the new RowSchema for the projected column\n    ColumnInfo columnInfo = parentOfRS.getSchema().getColumnInfo(internalColName);\n    ArrayList<ColumnInfo> signature = new ArrayList<ColumnInfo>();\n    signature.add(columnInfo);\n    RowSchema rowSchema = new RowSchema(signature);\n\n    // Create the column expr map\n    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n    ExprNodeDesc exprNode = null;\n    if ( parentOfRS.getColumnExprMap() != null) {\n      exprNode = parentOfRS.getColumnExprMap().get(internalColName).clone();\n    } else {\n      exprNode = new ExprNodeColumnDesc(columnInfo);\n    }\n\n    if (exprNode instanceof ExprNodeColumnDesc) {\n      ExprNodeColumnDesc encd = (ExprNodeColumnDesc) exprNode;\n      encd.setColumn(internalColName);\n    }\n    colExprMap.put(internalColName, exprNode);\n\n    // Create the Select Operator\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    rowSchema, colExprMap, parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(groupByOpFinal.getOperatorId());\n      rsOp.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(ts.getOperatorId());\n      rsOpFinal.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }"
        ]
    ],
    "62130b6c35242f91f34df0e449a91fa65e0dda60": [
        [
            "DynamicPartitionPruningOptimization::generateSemiJoinOperatorPlan(DynamicListContext,ParseContext,TableScanOperator,String)",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405 -\n 406 -\n 407 -\n 408 -\n 409 -\n 410 -\n 411 -\n 412 -\n 413 -\n 414 -\n 415 -\n 416 -\n 417 -\n 418 -\n 419 -\n 420 -\n 421 -\n 422 -\n 423 -\n 424 -\n 425 -\n 426 -\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    String internalColName = null;\n    ExprNodeDesc exprNodeDesc = key;\n    // Find the ExprNodeColumnDesc\n    while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n            (exprNodeDesc.getChildren() != null)) {\n      exprNodeDesc = exprNodeDesc.getChildren().get(0);\n    }\n\n    if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n      internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n      if (parentOfRS instanceof SelectOperator) {\n        // Make sure the semijoin branch is not on parition column.\n        ExprNodeColumnDesc colExpr = ((ExprNodeColumnDesc) (parentOfRS.\n                getColumnExprMap().get(internalColName)));\n        String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n        // Fetch the TableScan Operator.\n        Operator<?> op = parentOfRS.getParentOperators().get(0);\n        while (op != null && !(op instanceof TableScanOperator)) {\n          op = op.getParentOperators().get(0);\n        }\n        assert op != null;\n\n        Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n        if (table.isPartitionKey(colName)) {\n          // The column is partition column, skip the optimization.\n          return false;\n        }\n      }\n    } else {\n      // No column found!\n      // Bail out\n      return false;\n    }\n\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n\n    // Create the new RowSchema for the projected column\n    ColumnInfo columnInfo = parentOfRS.getSchema().getColumnInfo(internalColName);\n    ArrayList<ColumnInfo> signature = new ArrayList<ColumnInfo>();\n    signature.add(columnInfo);\n    RowSchema rowSchema = new RowSchema(signature);\n\n    // Create the column expr map\n    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n    ExprNodeDesc exprNode = null;\n    if ( parentOfRS.getColumnExprMap() != null) {\n      exprNode = parentOfRS.getColumnExprMap().get(internalColName).clone();\n    } else {\n      exprNode = new ExprNodeColumnDesc(columnInfo);\n    }\n\n    if (exprNode instanceof ExprNodeColumnDesc) {\n      ExprNodeColumnDesc encd = (ExprNodeColumnDesc) exprNode;\n      encd.setColumn(internalColName);\n    }\n    colExprMap.put(internalColName, exprNode);\n\n    // Create the Select Operator\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    rowSchema, colExprMap, parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(groupByOpFinal.getOperatorId());\n      rsOp.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(ts.getOperatorId());\n      rsOpFinal.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }",
            " 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405 +\n 406  \n 407  \n 408  \n 409  \n 410  \n 411 +\n 412 +\n 413 +\n 414 +\n 415 +\n 416 +\n 417 +\n 418 +\n 419 +\n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428 +\n 429 +\n 430 +\n 431 +\n 432 +\n 433 +\n 434 +\n 435 +\n 436 +\n 437 +\n 438 +\n 439 +\n 440 +\n 441 +\n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "  private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContext parseContext,\n      TableScanOperator ts, String keyBaseAlias) throws SemanticException {\n\n    // we will put a fork in the plan at the source of the reduce sink\n    Operator<? extends OperatorDesc> parentOfRS = ctx.generator.getParentOperators().get(0);\n\n    // we need the expr that generated the key of the reduce sink\n    ExprNodeDesc key = ctx.generator.getConf().getKeyCols().get(ctx.desc.getKeyIndex());\n\n    String internalColName = null;\n    ExprNodeDesc exprNodeDesc = key;\n    // Find the ExprNodeColumnDesc\n    while (!(exprNodeDesc instanceof ExprNodeColumnDesc) &&\n            (exprNodeDesc.getChildren() != null)) {\n      exprNodeDesc = exprNodeDesc.getChildren().get(0);\n    }\n\n    if (!(exprNodeDesc instanceof ExprNodeColumnDesc)) {\n      // No column found!\n      // Bail out\n      return false;\n    }\n\n    internalColName = ((ExprNodeColumnDesc) exprNodeDesc).getColumn();\n    if (parentOfRS instanceof SelectOperator) {\n      // Make sure the semijoin branch is not on partition column.\n      ExprNodeDesc expr = parentOfRS.getColumnExprMap().get(internalColName);\n      while (!(expr instanceof ExprNodeColumnDesc) &&\n              (expr.getChildren() != null)) {\n        expr = expr.getChildren().get(0);\n      }\n\n      if (!(expr instanceof ExprNodeColumnDesc)) {\n        // No column found!\n        // Bail out\n        return false;\n      }\n\n      ExprNodeColumnDesc colExpr = (ExprNodeColumnDesc) expr;\n      String colName = ExprNodeDescUtils.extractColName(colExpr);\n\n      // Fetch the TableScan Operator.\n      Operator<?> op = parentOfRS.getParentOperators().get(0);\n      while (op != null && !(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      assert op != null;\n\n      Table table = ((TableScanOperator) op).getConf().getTableMetadata();\n      if (table.isPartitionKey(colName)) {\n        // The column is partition column, skip the optimization.\n        return false;\n      }\n    }\n\n    List<ExprNodeDesc> keyExprs = new ArrayList<ExprNodeDesc>();\n    keyExprs.add(key);\n\n    // group by requires \"ArrayList\", don't ask.\n    ArrayList<String> outputNames = new ArrayList<String>();\n    outputNames.add(HiveConf.getColumnInternalName(0));\n\n    // project the relevant key column\n    SelectDesc select = new SelectDesc(keyExprs, outputNames);\n\n    // Create the new RowSchema for the projected column\n    ColumnInfo columnInfo = parentOfRS.getSchema().getColumnInfo(internalColName);\n    ArrayList<ColumnInfo> signature = new ArrayList<ColumnInfo>();\n    signature.add(columnInfo);\n    RowSchema rowSchema = new RowSchema(signature);\n\n    // Create the column expr map\n    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n    ExprNodeDesc exprNode = null;\n    if ( parentOfRS.getColumnExprMap() != null) {\n      exprNode = parentOfRS.getColumnExprMap().get(internalColName).clone();\n    } else {\n      exprNode = new ExprNodeColumnDesc(columnInfo);\n    }\n\n    if (exprNode instanceof ExprNodeColumnDesc) {\n      ExprNodeColumnDesc encd = (ExprNodeColumnDesc) exprNode;\n      encd.setColumn(internalColName);\n    }\n    colExprMap.put(internalColName, exprNode);\n\n    // Create the Select Operator\n    SelectOperator selectOp =\n            (SelectOperator) OperatorFactory.getAndMakeChild(select,\n                    rowSchema, colExprMap, parentOfRS);\n\n    // do a group by to aggregate min,max and bloom filter.\n    float groupByMemoryUsage =\n            HiveConf.getFloatVar(parseContext.getConf(), HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n    float memoryThreshold =\n            HiveConf.getFloatVar(parseContext.getConf(),\n                    HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n\n    ArrayList<ExprNodeDesc> groupByExprs = new ArrayList<ExprNodeDesc>();\n\n    // Add min/max and bloom filter aggregations\n    List<ObjectInspector> aggFnOIs = new ArrayList<ObjectInspector>();\n    aggFnOIs.add(key.getWritableObjectInspector());\n    ArrayList<ExprNodeDesc> params = new ArrayList<ExprNodeDesc>();\n    params.add(\n            new ExprNodeColumnDesc(key.getTypeInfo(), outputNames.get(0),\n                    \"\", false));\n\n    ArrayList<AggregationDesc> aggs = new ArrayList<AggregationDesc>();\n    try {\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", aggFnOIs, false, false),\n              params, false, Mode.PARTIAL1);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n      aggs.add(min);\n      aggs.add(max);\n      aggs.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    // Create the Group by Operator\n    ArrayList<String> gbOutputNames = new ArrayList<String>();\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(0));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(1));\n    gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));\n    GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n\n    ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(1), key.getTypeInfo(), \"\", false));\n    groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(2), key.getTypeInfo(), \"\", false));\n\n    GroupByOperator groupByOp = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupBy, new RowSchema(groupbyColInfos), selectOp);\n\n    groupByOp.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // Get the column names of the aggregations for reduce sink\n    int colPos = 0;\n    ArrayList<ExprNodeDesc> rsValueCols = new ArrayList<ExprNodeDesc>();\n    for (int i = 0; i < aggs.size() - 1; i++) {\n      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(key.getTypeInfo(),\n              gbOutputNames.get(colPos++), \"\", false);\n      rsValueCols.add(colExpr);\n    }\n\n    // Bloom Filter uses binary\n    ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(TypeInfoFactory.binaryTypeInfo,\n            gbOutputNames.get(colPos++), \"\", false);\n    rsValueCols.add(colExpr);\n\n    // Create the reduce sink operator\n    ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOp = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDesc, new RowSchema(groupByOp.getSchema()), groupByOp);\n    Map<String, ExprNodeDesc> columnExprMap = new HashMap<String, ExprNodeDesc>();\n    rsOp.setColumnExprMap(columnExprMap);\n\n    // Create the final Group By Operator\n    ArrayList<AggregationDesc> aggsFinal = new ArrayList<AggregationDesc>();\n    try {\n      List<ObjectInspector> minFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> maxFinalFnOIs = new ArrayList<ObjectInspector>();\n      List<ObjectInspector> bloomFilterFinalFnOIs = new ArrayList<ObjectInspector>();\n      ArrayList<ExprNodeDesc> minFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> maxFinalParams = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> bloomFilterFinalParams = new ArrayList<ExprNodeDesc>();\n      // Use the expressions from Reduce Sink.\n      minFinalFnOIs.add(rsValueCols.get(0).getWritableObjectInspector());\n      maxFinalFnOIs.add(rsValueCols.get(1).getWritableObjectInspector());\n      bloomFilterFinalFnOIs.add(rsValueCols.get(2).getWritableObjectInspector());\n      // Coming from a ReduceSink the aggregations would be in the form VALUE._col0, VALUE._col1\n      minFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(0).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(0), \"\", false));\n      maxFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(1).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(1), \"\", false));\n      bloomFilterFinalParams.add(\n              new ExprNodeColumnDesc(\n                      rsValueCols.get(2).getTypeInfo(),\n                      Utilities.ReduceField.VALUE + \".\" +\n                              gbOutputNames.get(2), \"\", false));\n\n      AggregationDesc min = new AggregationDesc(\"min\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"min\", minFinalFnOIs,\n                      false, false),\n              minFinalParams, false, Mode.FINAL);\n      AggregationDesc max = new AggregationDesc(\"max\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"max\", maxFinalFnOIs,\n                      false, false),\n              maxFinalParams, false, Mode.FINAL);\n      AggregationDesc bloomFilter = new AggregationDesc(\"bloom_filter\",\n              FunctionRegistry.getGenericUDAFEvaluator(\"bloom_filter\", bloomFilterFinalFnOIs,\n                      false, false),\n              bloomFilterFinalParams, false, Mode.FINAL);\n      GenericUDAFBloomFilterEvaluator bloomFilterEval = (GenericUDAFBloomFilterEvaluator) bloomFilter.getGenericUDAFEvaluator();\n      bloomFilterEval.setSourceOperator(selectOp);\n      bloomFilterEval.setMaxEntries(parseContext.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n      bloomFilter.setGenericUDAFWritableEvaluator(bloomFilterEval);\n\n      aggsFinal.add(min);\n      aggsFinal.add(max);\n      aggsFinal.add(bloomFilter);\n    } catch (SemanticException e) {\n      LOG.error(\"Error creating min/max aggregations on key\", e);\n      throw new IllegalStateException(\"Error creating min/max aggregations on key\", e);\n    }\n\n    GroupByDesc groupByDescFinal = new GroupByDesc(GroupByDesc.Mode.FINAL,\n            gbOutputNames, new ArrayList<ExprNodeDesc>(), aggsFinal, false,\n            groupByMemoryUsage, memoryThreshold, null, false, 0, false);\n    GroupByOperator groupByOpFinal = (GroupByOperator)OperatorFactory.getAndMakeChild(\n            groupByDescFinal, new RowSchema(rsOp.getSchema()), rsOp);\n    groupByOpFinal.setColumnExprMap(new HashMap<String, ExprNodeDesc>());\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(groupByOpFinal.getOperatorId());\n      rsOp.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Create the final Reduce Sink Operator\n    ReduceSinkDesc rsDescFinal = PlanUtils.getReduceSinkDesc(\n            new ArrayList<ExprNodeDesc>(), rsValueCols, gbOutputNames, false,\n            -1, 0, 1, Operation.NOT_ACID);\n    ReduceSinkOperator rsOpFinal = (ReduceSinkOperator)OperatorFactory.getAndMakeChild(\n            rsDescFinal, new RowSchema(groupByOpFinal.getSchema()), groupByOpFinal);\n    rsOpFinal.setColumnExprMap(columnExprMap);\n\n    LOG.debug(\"DynamicMinMaxPushdown: Saving RS to TS mapping: \" + rsOpFinal + \": \" + ts);\n    parseContext.getRsOpToTsOpMap().put(rsOpFinal, ts);\n\n    // for explain purpose\n    if (parseContext.getContext().getExplainConfig() != null\n        && parseContext.getContext().getExplainConfig().isFormatted()) {\n      List<String> outputOperators = new ArrayList<>();\n      outputOperators.add(ts.getOperatorId());\n      rsOpFinal.getConf().setOutputOperators(outputOperators);\n    }\n\n    // Save the info that is required at query time to resolve dynamic/runtime values.\n    RuntimeValuesInfo runtimeValuesInfo = new RuntimeValuesInfo();\n    TableDesc rsFinalTableDesc = PlanUtils.getReduceValueTableDesc(\n            PlanUtils.getFieldSchemasFromColumnList(rsValueCols, \"_col\"));\n    List<String> dynamicValueIDs = new ArrayList<String>();\n    dynamicValueIDs.add(keyBaseAlias + \"_min\");\n    dynamicValueIDs.add(keyBaseAlias + \"_max\");\n    dynamicValueIDs.add(keyBaseAlias + \"_bloom_filter\");\n\n    runtimeValuesInfo.setTableDesc(rsFinalTableDesc);\n    runtimeValuesInfo.setDynamicValueIDs(dynamicValueIDs);\n    runtimeValuesInfo.setColExprs(rsValueCols);\n    parseContext.getRsToRuntimeValuesInfoMap().put(rsOpFinal, runtimeValuesInfo);\n\n    return true;\n  }"
        ]
    ],
    "ce695b5d4ae07d0bfc79fb88fcb09cb99e9e4706": [
        [
            "Vectorizer::VectorizationDispatcher::verifyAndSetVectorPartDesc(PartitionDesc,boolean,HashSet,HashSet,ArrayList)",
            " 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  ",
            "    private boolean verifyAndSetVectorPartDesc(PartitionDesc pd, boolean isAcidTable,\n        HashSet<String> inputFileFormatClassNameSet, HashSet<String> enabledConditionsMetSet,\n        ArrayList<String> enabledConditionsNotMetList) {\n\n      String inputFileFormatClassName = pd.getInputFileFormatClassName();\n\n      // Always collect input file formats.\n      inputFileFormatClassNameSet.add(inputFileFormatClassName);\n\n      boolean isInputFileFormatVectorized = Utilities.isInputFileFormatVectorized(pd);\n\n      if (isAcidTable) {\n\n        // Today, ACID tables are only ORC and that format is vectorizable.  Verify these\n        // assumptions.\n        Preconditions.checkState(isInputFileFormatVectorized);\n        Preconditions.checkState(inputFileFormatClassName.equals(OrcInputFormat.class.getName()));\n\n        if (!useVectorizedInputFileFormat) {\n          enabledConditionsNotMetList.add(\n              \"Vectorizing ACID tables requires \" + HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n          return false;\n        }\n\n        pd.setVectorPartitionDesc(\n            VectorPartitionDesc.createVectorizedInputFileFormat(\n                inputFileFormatClassName, Utilities.isInputFileFormatSelfDescribing(pd)));\n\n        enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n        return true;\n      }\n\n      // Look for Pass-Thru case where InputFileFormat has VectorizedInputFormatInterface\n      // and reads VectorizedRowBatch as a \"row\".\n\n      if (useVectorizedInputFileFormat) {\n\n        if (isInputFileFormatVectorized) {\n\n          pd.setVectorPartitionDesc(\n              VectorPartitionDesc.createVectorizedInputFileFormat(\n                  inputFileFormatClassName, Utilities.isInputFileFormatSelfDescribing(pd)));\n\n          enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n          return true;\n        }\n        // Fall through and look for other options...\n      }\n\n      if (!isSchemaEvolution) {\n        enabledConditionsNotMetList.add(\n            \"Vectorizing tables without Schema Evolution requires \" + HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n      }\n\n      String deserializerClassName = pd.getDeserializerClassName();\n\n      // Look for InputFileFormat / Serde combinations we can deserialize more efficiently\n      // using VectorDeserializeRow and a deserialize class with the DeserializeRead interface.\n      //\n      // Do the \"vectorized\" row-by-row deserialization into a VectorizedRowBatch in the\n      // VectorMapOperator.\n      boolean isTextFormat = inputFileFormatClassName.equals(TextInputFormat.class.getName()) &&\n          deserializerClassName.equals(LazySimpleSerDe.class.getName());\n      boolean isSequenceFormat =\n          inputFileFormatClassName.equals(SequenceFileInputFormat.class.getName()) &&\n          deserializerClassName.equals(LazyBinarySerDe.class.getName());\n      boolean isVectorDeserializeEligable = isTextFormat || isSequenceFormat;\n\n      if (useVectorDeserialize) {\n\n        // Currently, we support LazySimple deserialization:\n        //\n        //    org.apache.hadoop.mapred.TextInputFormat\n        //    org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n        //\n        // AND\n        //\n        //    org.apache.hadoop.mapred.SequenceFileInputFormat\n        //    org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n\n        if (isTextFormat) {\n\n          Properties properties = pd.getTableDesc().getProperties();\n          String lastColumnTakesRestString =\n              properties.getProperty(serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST);\n          boolean lastColumnTakesRest =\n              (lastColumnTakesRestString != null &&\n              lastColumnTakesRestString.equalsIgnoreCase(\"true\"));\n          if (lastColumnTakesRest) {\n\n            // If row mode will not catch this input file format, then not enabled.\n            if (useRowDeserialize) {\n              enabledConditionsNotMetList.add(\n                  inputFileFormatClassName + \" \" +\n                  serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST + \" must be disabled \");\n              return false;\n            }\n          } else {\n            pd.setVectorPartitionDesc(\n                VectorPartitionDesc.createVectorDeserialize(\n                    inputFileFormatClassName, VectorDeserializeType.LAZY_SIMPLE));\n\n            enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n            return true;\n          }\n        } else if (isSequenceFormat) {\n\n          pd.setVectorPartitionDesc(\n              VectorPartitionDesc.createVectorDeserialize(\n                  inputFileFormatClassName, VectorDeserializeType.LAZY_BINARY));\n\n          enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n          return true;\n        }\n        // Fall through and look for other options...\n      }\n\n      // Otherwise, if enabled, deserialize rows using regular Serde and add the object\n      // inspect-able Object[] row to a VectorizedRowBatch in the VectorMapOperator.\n\n      if (useRowDeserialize) {\n\n        pd.setVectorPartitionDesc(\n            VectorPartitionDesc.createRowDeserialize(\n                inputFileFormatClassName,\n                Utilities.isInputFileFormatSelfDescribing(pd),\n                deserializerClassName));\n\n        enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_ROW_DESERIALIZE.varname);\n        return true;\n\n      }\n\n      if (isInputFileFormatVectorized) {\n        Preconditions.checkState(!useVectorizedInputFileFormat);\n        enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n      } else {\n        // Only offer these when the input file format is not the fast vectorized formats.\n        if (isVectorDeserializeEligable) {\n          Preconditions.checkState(!useVectorDeserialize);\n          enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n        } else {\n          // Since row mode takes everyone.\n          enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_ROW_DESERIALIZE.varname);\n        }\n      }\n \n      return false;\n    }",
            " 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752 +\n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  ",
            "    private boolean verifyAndSetVectorPartDesc(PartitionDesc pd, boolean isAcidTable,\n        HashSet<String> inputFileFormatClassNameSet, HashSet<String> enabledConditionsMetSet,\n        ArrayList<String> enabledConditionsNotMetList) {\n\n      String inputFileFormatClassName = pd.getInputFileFormatClassName();\n\n      // Always collect input file formats.\n      inputFileFormatClassNameSet.add(inputFileFormatClassName);\n\n      boolean isInputFileFormatVectorized = Utilities.isInputFileFormatVectorized(pd);\n\n      if (isAcidTable) {\n\n        // Today, ACID tables are only ORC and that format is vectorizable.  Verify these\n        // assumptions.\n        Preconditions.checkState(isInputFileFormatVectorized);\n        Preconditions.checkState(inputFileFormatClassName.equals(OrcInputFormat.class.getName()));\n\n        if (!useVectorizedInputFileFormat) {\n          enabledConditionsNotMetList.add(\n              \"Vectorizing ACID tables requires \" + HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n          return false;\n        }\n\n        pd.setVectorPartitionDesc(\n            VectorPartitionDesc.createVectorizedInputFileFormat(\n                inputFileFormatClassName, Utilities.isInputFileFormatSelfDescribing(pd)));\n\n        enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n        return true;\n      }\n\n      // Look for Pass-Thru case where InputFileFormat has VectorizedInputFormatInterface\n      // and reads VectorizedRowBatch as a \"row\".\n\n      if (useVectorizedInputFileFormat) {\n\n        if (isInputFileFormatVectorized) {\n\n          pd.setVectorPartitionDesc(\n              VectorPartitionDesc.createVectorizedInputFileFormat(\n                  inputFileFormatClassName, Utilities.isInputFileFormatSelfDescribing(pd)));\n\n          enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n          return true;\n        }\n        // Fall through and look for other options...\n      }\n\n      if (!isSchemaEvolution) {\n        enabledConditionsNotMetList.add(\n            \"Vectorizing tables without Schema Evolution requires \" + HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n        return false;\n      }\n\n      String deserializerClassName = pd.getDeserializerClassName();\n\n      // Look for InputFileFormat / Serde combinations we can deserialize more efficiently\n      // using VectorDeserializeRow and a deserialize class with the DeserializeRead interface.\n      //\n      // Do the \"vectorized\" row-by-row deserialization into a VectorizedRowBatch in the\n      // VectorMapOperator.\n      boolean isTextFormat = inputFileFormatClassName.equals(TextInputFormat.class.getName()) &&\n          deserializerClassName.equals(LazySimpleSerDe.class.getName());\n      boolean isSequenceFormat =\n          inputFileFormatClassName.equals(SequenceFileInputFormat.class.getName()) &&\n          deserializerClassName.equals(LazyBinarySerDe.class.getName());\n      boolean isVectorDeserializeEligable = isTextFormat || isSequenceFormat;\n\n      if (useVectorDeserialize) {\n\n        // Currently, we support LazySimple deserialization:\n        //\n        //    org.apache.hadoop.mapred.TextInputFormat\n        //    org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n        //\n        // AND\n        //\n        //    org.apache.hadoop.mapred.SequenceFileInputFormat\n        //    org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n\n        if (isTextFormat) {\n\n          Properties properties = pd.getTableDesc().getProperties();\n          String lastColumnTakesRestString =\n              properties.getProperty(serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST);\n          boolean lastColumnTakesRest =\n              (lastColumnTakesRestString != null &&\n              lastColumnTakesRestString.equalsIgnoreCase(\"true\"));\n          if (lastColumnTakesRest) {\n\n            // If row mode will not catch this input file format, then not enabled.\n            if (useRowDeserialize) {\n              enabledConditionsNotMetList.add(\n                  inputFileFormatClassName + \" \" +\n                  serdeConstants.SERIALIZATION_LAST_COLUMN_TAKES_REST + \" must be disabled \");\n              return false;\n            }\n          } else {\n            pd.setVectorPartitionDesc(\n                VectorPartitionDesc.createVectorDeserialize(\n                    inputFileFormatClassName, VectorDeserializeType.LAZY_SIMPLE));\n\n            enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n            return true;\n          }\n        } else if (isSequenceFormat) {\n\n          pd.setVectorPartitionDesc(\n              VectorPartitionDesc.createVectorDeserialize(\n                  inputFileFormatClassName, VectorDeserializeType.LAZY_BINARY));\n\n          enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n          return true;\n        }\n        // Fall through and look for other options...\n      }\n\n      // Otherwise, if enabled, deserialize rows using regular Serde and add the object\n      // inspect-able Object[] row to a VectorizedRowBatch in the VectorMapOperator.\n\n      if (useRowDeserialize) {\n\n        pd.setVectorPartitionDesc(\n            VectorPartitionDesc.createRowDeserialize(\n                inputFileFormatClassName,\n                Utilities.isInputFileFormatSelfDescribing(pd),\n                deserializerClassName));\n\n        enabledConditionsMetSet.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_ROW_DESERIALIZE.varname);\n        return true;\n\n      }\n\n      if (isInputFileFormatVectorized) {\n        Preconditions.checkState(!useVectorizedInputFileFormat);\n        enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);\n      } else {\n        // Only offer these when the input file format is not the fast vectorized formats.\n        if (isVectorDeserializeEligable) {\n          Preconditions.checkState(!useVectorDeserialize);\n          enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE.varname);\n        } else {\n          // Since row mode takes everyone.\n          enabledConditionsNotMetList.add(HiveConf.ConfVars.HIVE_VECTORIZATION_USE_ROW_DESERIALIZE.varname);\n        }\n      }\n \n      return false;\n    }"
        ]
    ],
    "cd4a3d15c1abb8f6302b621c9d991115884f8b03": [
        [
            "VectorGroupKeyHelper::copyGroupKey(VectorizedRowBatch,VectorizedRowBatch,DataOutputBuffer)",
            "  61  \n  62  \n  63  \n  64 -\n  65 -\n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102 -\n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121 -\n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135 -\n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147 -\n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "  public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outputBatch,\n          DataOutputBuffer buffer) throws HiveException {\n    for(int i = 0; i< longIndices.length; ++i) {\n      int keyIndex = longIndices[i];\n      LongColumnVector inputColumnVector = (LongColumnVector) inputBatch.cols[keyIndex];\n      LongColumnVector outputColumnVector = (LongColumnVector) outputBatch.cols[keyIndex];\n\n      // This vectorized code pattern says: \n      //    If the input batch has no nulls at all (noNulls is true) OR\n      //    the input row is NOT NULL, copy the value.\n      //\n      //    Otherwise, we have a NULL input value.  The standard way to mark a NULL in the\n      //    output batch is: turn off noNulls indicating there is at least one NULL in the batch\n      //    and mark that row as NULL.\n      //\n      //    When a vectorized row batch is reset, noNulls is set to true and the isNull array\n      //    is zeroed.\n      //\n      // We grab the key at index 0.  We don't care about selected or repeating since all keys\n      // in the input batch are suppose to be the same.\n      //\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        outputColumnVector.vector[outputBatch.size] = inputColumnVector.vector[0];\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<doubleIndices.length; ++i) {\n      int keyIndex = doubleIndices[i];\n      DoubleColumnVector inputColumnVector = (DoubleColumnVector) inputBatch.cols[keyIndex];\n      DoubleColumnVector outputColumnVector = (DoubleColumnVector) outputBatch.cols[keyIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        outputColumnVector.vector[outputBatch.size] = inputColumnVector.vector[0];\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<stringIndices.length; ++i) {\n      int keyIndex = stringIndices[i];\n      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[keyIndex];\n      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[keyIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        // Copy bytes into scratch buffer.\n        int start = buffer.getLength();\n        int length = inputColumnVector.length[0];\n        try {\n          buffer.write(inputColumnVector.vector[0], inputColumnVector.start[0], length);\n        } catch (IOException ioe) {\n          throw new IllegalStateException(\"bad write\", ioe);\n        }\n        outputColumnVector.setRef(outputBatch.size, buffer.getData(), start, length);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<decimalIndices.length; ++i) {\n      int keyIndex = decimalIndices[i];\n      DecimalColumnVector inputColumnVector = (DecimalColumnVector) inputBatch.cols[keyIndex];\n      DecimalColumnVector outputColumnVector = (DecimalColumnVector) outputBatch.cols[keyIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        // Since we store references to HiveDecimalWritable instances, we must use the update method instead\n        // of plain assignment.\n        outputColumnVector.set(outputBatch.size, inputColumnVector.vector[0]);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<timestampIndices.length; ++i) {\n      int keyIndex = timestampIndices[i];\n      TimestampColumnVector inputColumnVector = (TimestampColumnVector) inputBatch.cols[keyIndex];\n      TimestampColumnVector outputColumnVector = (TimestampColumnVector) outputBatch.cols[keyIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<intervalDayTimeIndices.length; ++i) {\n      int keyIndex = intervalDayTimeIndices[i];\n      IntervalDayTimeColumnVector inputColumnVector = (IntervalDayTimeColumnVector) inputBatch.cols[keyIndex];\n      IntervalDayTimeColumnVector outputColumnVector = (IntervalDayTimeColumnVector) outputBatch.cols[keyIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n  }",
            "  65  \n  66  \n  67  \n  68 +\n  69 +\n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95 +\n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106 +\n 107 +\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125 +\n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140 +\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "  public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outputBatch,\n          DataOutputBuffer buffer) throws HiveException {\n    for(int i = 0; i< longIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[longIndices[i]];\n      LongColumnVector inputColumnVector = (LongColumnVector) inputBatch.cols[columnIndex];\n      LongColumnVector outputColumnVector = (LongColumnVector) outputBatch.cols[columnIndex];\n\n      // This vectorized code pattern says: \n      //    If the input batch has no nulls at all (noNulls is true) OR\n      //    the input row is NOT NULL, copy the value.\n      //\n      //    Otherwise, we have a NULL input value.  The standard way to mark a NULL in the\n      //    output batch is: turn off noNulls indicating there is at least one NULL in the batch\n      //    and mark that row as NULL.\n      //\n      //    When a vectorized row batch is reset, noNulls is set to true and the isNull array\n      //    is zeroed.\n      //\n      // We grab the key at index 0.  We don't care about selected or repeating since all keys\n      // in the input batch are suppose to be the same.\n      //\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        outputColumnVector.vector[outputBatch.size] = inputColumnVector.vector[0];\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<doubleIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[doubleIndices[i]];\n      DoubleColumnVector inputColumnVector = (DoubleColumnVector) inputBatch.cols[columnIndex];\n      DoubleColumnVector outputColumnVector = (DoubleColumnVector) outputBatch.cols[columnIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        outputColumnVector.vector[outputBatch.size] = inputColumnVector.vector[0];\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<stringIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[stringIndices[i]];\n      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[columnIndex];\n      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[columnIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n        // Copy bytes into scratch buffer.\n        int start = buffer.getLength();\n        int length = inputColumnVector.length[0];\n        try {\n          buffer.write(inputColumnVector.vector[0], inputColumnVector.start[0], length);\n        } catch (IOException ioe) {\n          throw new IllegalStateException(\"bad write\", ioe);\n        }\n        outputColumnVector.setRef(outputBatch.size, buffer.getData(), start, length);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<decimalIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[decimalIndices[i]];\n      DecimalColumnVector inputColumnVector = (DecimalColumnVector) inputBatch.cols[columnIndex];\n      DecimalColumnVector outputColumnVector = (DecimalColumnVector) outputBatch.cols[columnIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        // Since we store references to HiveDecimalWritable instances, we must use the update method instead\n        // of plain assignment.\n        outputColumnVector.set(outputBatch.size, inputColumnVector.vector[0]);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<timestampIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[timestampIndices[i]];\n      TimestampColumnVector inputColumnVector = (TimestampColumnVector) inputBatch.cols[columnIndex];\n      TimestampColumnVector outputColumnVector = (TimestampColumnVector) outputBatch.cols[columnIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n    for(int i=0;i<intervalDayTimeIndices.length; ++i) {\n      final int columnIndex = outputColumnNums[intervalDayTimeIndices[i]];\n      IntervalDayTimeColumnVector inputColumnVector = (IntervalDayTimeColumnVector) inputBatch.cols[columnIndex];\n      IntervalDayTimeColumnVector outputColumnVector = (IntervalDayTimeColumnVector) outputBatch.cols[columnIndex];\n      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {\n\n        outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);\n      } else {\n        outputColumnVector.noNulls = false;\n        outputColumnVector.isNull[outputBatch.size] = true;\n      }\n    }\n  }"
        ],
        [
            "VectorGroupByOperator::ProcessingModeReduceMergePartial::doProcessBatch(VectorizedRowBatch,boolean,boolean)",
            " 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  ",
            "    @Override\n    public void doProcessBatch(VectorizedRowBatch batch, boolean isFirstGroupingSet,\n        boolean[] currentGroupingSetsOverrideIsNulls) throws HiveException {\n      assert(inGroup);\n      if (first) {\n        // Copy the group key to output batch now.  We'll copy in the aggregates at the end of the group.\n        first = false;\n        groupKeyHelper.copyGroupKey(batch, outputBatch, buffer);\n      }\n\n      // Aggregate this batch.\n      for (int i = 0; i < aggregators.length; ++i) {\n        aggregators[i].aggregateInput(groupAggregators.getAggregationBuffer(i), batch);\n      }\n    }",
            " 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 +\n 819 +\n 820 +\n 821 +\n 822 +\n 823 +\n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  ",
            "    @Override\n    public void doProcessBatch(VectorizedRowBatch batch, boolean isFirstGroupingSet,\n        boolean[] currentGroupingSetsOverrideIsNulls) throws HiveException {\n      assert(inGroup);\n      if (first) {\n        // Copy the group key to output batch now.  We'll copy in the aggregates at the end of the group.\n        first = false;\n\n        // Evaluate the key expressions of just this first batch to get the correct key.\n        for (int i = 0; i < outputKeyLength; i++) {\n          keyExpressions[i].evaluate(batch);\n        }\n\n        groupKeyHelper.copyGroupKey(batch, outputBatch, buffer);\n      }\n\n      // Aggregate this batch.\n      for (int i = 0; i < aggregators.length; ++i) {\n        aggregators[i].aggregateInput(groupAggregators.getAggregationBuffer(i), batch);\n      }\n    }"
        ],
        [
            "VectorGroupKeyHelper::init(VectorExpression)",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44 -\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  ",
            "  void init(VectorExpression[] keyExpressions) throws HiveException {\n\n    // NOTE: To support pruning the grouping set id dummy key by VectorGroupbyOpeator MERGE_PARTIAL\n    // case, we use the keyCount passed to the constructor and not keyExpressions.length.\n\n    // Inspect the output type of each key expression.\n    for(int i=0; i < keyCount; ++i) {\n      String typeName = VectorizationContext.mapTypeNameSynonyms(keyExpressions[i].getOutputType());\n      TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeName);\n      Type columnVectorType = VectorizationContext.getColumnVectorTypeFromTypeInfo(typeInfo);\n      addKey(columnVectorType);\n    }\n    finishAdding();\n  }",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48  \n  49  \n  50  \n  51  \n  52  \n  53 +\n  54  \n  55  \n  56  ",
            "  void init(VectorExpression[] keyExpressions) throws HiveException {\n\n    // NOTE: To support pruning the grouping set id dummy key by VectorGroupbyOpeator MERGE_PARTIAL\n    // case, we use the keyCount passed to the constructor and not keyExpressions.length.\n\n    // Inspect the output type of each key expression.  And, remember the output columns.\n    outputColumnNums = new int[keyCount];\n    for(int i=0; i < keyCount; ++i) {\n      String typeName = VectorizationContext.mapTypeNameSynonyms(keyExpressions[i].getOutputType());\n      TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeName);\n      Type columnVectorType = VectorizationContext.getColumnVectorTypeFromTypeInfo(typeInfo);\n      addKey(columnVectorType);\n      outputColumnNums[i] = keyExpressions[i].getOutputColumn();\n    }\n    finishAdding();\n  }"
        ],
        [
            "Vectorizer::validateGroupByOperator(GroupByOperator,boolean,boolean)",
            "1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025 -\n2026 -\n2027 -\n2028 -\n2029 -\n2030 -\n2031 -\n2032 -\n2033 -\n2034 -\n2035 -\n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  ",
            "  private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, boolean isTezOrSpark) {\n    GroupByDesc desc = op.getConf();\n\n    if (desc.getMode() != GroupByDesc.Mode.HASH && desc.isDistinct()) {\n      setOperatorIssue(\"DISTINCT not supported\");\n      return false;\n    }\n    boolean ret = validateExprNodeDesc(desc.getKeys(), \"Key\");\n    if (!ret) {\n      return false;\n    }\n\n    /**\n     *\n     * GROUP BY DEFINITIONS:\n     *\n     * GroupByDesc.Mode enumeration:\n     *\n     *    The different modes of a GROUP BY operator.\n     *\n     *    These descriptions are hopefully less cryptic than the comments for GroupByDesc.Mode.\n     *\n     *        COMPLETE       Aggregates original rows into full aggregation row(s).\n     *\n     *                       If the key length is 0, this is also called Global aggregation and\n     *                       1 output row is produced.\n     *\n     *                       When the key length is > 0, the original rows come in ALREADY GROUPED.\n     *\n     *                       An example for key length > 0 is a GROUP BY being applied to the\n     *                       ALREADY GROUPED rows coming from an upstream JOIN operator.  Or,\n     *                       ALREADY GROUPED rows coming from upstream MERGEPARTIAL GROUP BY\n     *                       operator.\n     *\n     *        PARTIAL1       The first of 2 (or more) phases that aggregates ALREADY GROUPED\n     *                       original rows into partial aggregations.\n     *\n     *                       Subsequent phases PARTIAL2 (optional) and MERGEPARTIAL will merge\n     *                       the partial aggregations and output full aggregations.\n     *\n     *        PARTIAL2       Accept ALREADY GROUPED partial aggregations and merge them into another\n     *                       partial aggregation.  Output the merged partial aggregations.\n     *\n     *                       (Haven't seen this one used)\n     *\n     *        PARTIALS       (Behaves for non-distinct the same as PARTIAL2; and behaves for\n     *                       distinct the same as PARTIAL1.)\n     *\n     *        FINAL          Accept ALREADY GROUPED original rows and aggregate them into\n     *                       full aggregations.\n     *\n     *                       Example is a GROUP BY being applied to rows from a sorted table, where\n     *                       the group key is the table sort key (or a prefix).\n     *\n     *        HASH           Accept UNORDERED original rows and aggregate them into a memory table.\n     *                       Output the partial aggregations on closeOp (or low memory).\n     *\n     *                       Similar to PARTIAL1 except original rows are UNORDERED.\n     *\n     *                       Commonly used in both Mapper and Reducer nodes.  Always followed by\n     *                       a Reducer with MERGEPARTIAL GROUP BY.\n     *\n     *        MERGEPARTIAL   Always first operator of a Reducer.  Data is grouped by reduce-shuffle.\n     *\n     *                       (Behaves for non-distinct aggregations the same as FINAL; and behaves\n     *                       for distinct aggregations the same as COMPLETE.)\n     *\n     *                       The output is full aggregation(s).\n     *\n     *                       Used in Reducers after a stage with a HASH GROUP BY operator.\n     *\n     *\n     *  VectorGroupByDesc.ProcessingMode for VectorGroupByOperator:\n     *\n     *     GLOBAL         No key.  All rows --> 1 full aggregation on end of input\n     *\n     *     HASH           Rows aggregated in to hash table on group key -->\n     *                        1 partial aggregation per key (normally, unless there is spilling)\n     *\n     *     MERGE_PARTIAL  As first operator in a REDUCER, partial aggregations come grouped from\n     *                    reduce-shuffle -->\n     *                        aggregate the partial aggregations and emit full aggregation on\n     *                        endGroup / closeOp\n     *\n     *     STREAMING      Rows come from PARENT operator ALREADY GROUPED -->\n     *                        aggregate the rows and emit full aggregation on key change / closeOp\n     *\n     *     NOTE: Hash can spill partial result rows prematurely if it runs low on memory.\n     *     NOTE: Streaming has to compare keys where MergePartial gets an endGroup call.\n     *\n     *\n     *  DECIDER: Which VectorGroupByDesc.ProcessingMode for VectorGroupByOperator?\n     *\n     *     Decides using GroupByDesc.Mode and whether there are keys with the\n     *     VectorGroupByDesc.groupByDescModeToVectorProcessingMode method.\n     *\n     *         Mode.COMPLETE      --> (numKeys == 0 ? ProcessingMode.GLOBAL : ProcessingMode.STREAMING)\n     *\n     *         Mode.HASH          --> ProcessingMode.HASH\n     *\n     *         Mode.MERGEPARTIAL  --> (numKeys == 0 ? ProcessingMode.GLOBAL : ProcessingMode.MERGE_PARTIAL)\n     *\n     *         Mode.PARTIAL1,\n     *         Mode.PARTIAL2,\n     *         Mode.PARTIALS,\n     *         Mode.FINAL        --> ProcessingMode.STREAMING\n     *\n     */\n    boolean hasKeys = (desc.getKeys().size() > 0);\n\n    ProcessingMode processingMode =\n        VectorGroupByDesc.groupByDescModeToVectorProcessingMode(desc.getMode(), hasKeys);\n    if (desc.isGroupingSetsPresent() &&\n        (processingMode != ProcessingMode.HASH && processingMode != ProcessingMode.STREAMING)) {\n      LOG.info(\"Vectorized GROUPING SETS only expected for HASH and STREAMING processing modes\");\n      return false;\n    }\n\n    if (processingMode == ProcessingMode.MERGE_PARTIAL) {\n      // For now, VectorGroupByOperator ProcessingModeReduceMergePartial cannot handle key\n      // expressions.\n      for (ExprNodeDesc keyExpr : desc.getKeys()) {\n        if (!(keyExpr instanceof ExprNodeColumnDesc)) {\n          setExpressionIssue(\"Key\", \"Non-column key expressions not supported for MERGEPARTIAL\");\n          return false;\n        }\n      }\n    }\n\n    Pair<Boolean,Boolean> retPair =\n        validateAggregationDescs(desc.getAggregators(), processingMode, hasKeys);\n    if (!retPair.left) {\n      return false;\n    }\n\n    // If all the aggregation outputs are primitive, we can output VectorizedRowBatch.\n    // Otherwise, we the rest of the operator tree will be row mode.\n    VectorGroupByDesc vectorDesc = new VectorGroupByDesc();\n    desc.setVectorDesc(vectorDesc);\n\n    vectorDesc.setVectorOutput(retPair.right);\n\n    vectorDesc.setProcessingMode(processingMode);\n\n    LOG.info(\"Vector GROUP BY operator will use processing mode \" + processingMode.name() +\n        \", isVectorOutput \" + vectorDesc.isVectorOutput());\n\n    return true;\n  }",
            "1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  ",
            "  private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, boolean isTezOrSpark) {\n    GroupByDesc desc = op.getConf();\n\n    if (desc.getMode() != GroupByDesc.Mode.HASH && desc.isDistinct()) {\n      setOperatorIssue(\"DISTINCT not supported\");\n      return false;\n    }\n    boolean ret = validateExprNodeDesc(desc.getKeys(), \"Key\");\n    if (!ret) {\n      return false;\n    }\n\n    /**\n     *\n     * GROUP BY DEFINITIONS:\n     *\n     * GroupByDesc.Mode enumeration:\n     *\n     *    The different modes of a GROUP BY operator.\n     *\n     *    These descriptions are hopefully less cryptic than the comments for GroupByDesc.Mode.\n     *\n     *        COMPLETE       Aggregates original rows into full aggregation row(s).\n     *\n     *                       If the key length is 0, this is also called Global aggregation and\n     *                       1 output row is produced.\n     *\n     *                       When the key length is > 0, the original rows come in ALREADY GROUPED.\n     *\n     *                       An example for key length > 0 is a GROUP BY being applied to the\n     *                       ALREADY GROUPED rows coming from an upstream JOIN operator.  Or,\n     *                       ALREADY GROUPED rows coming from upstream MERGEPARTIAL GROUP BY\n     *                       operator.\n     *\n     *        PARTIAL1       The first of 2 (or more) phases that aggregates ALREADY GROUPED\n     *                       original rows into partial aggregations.\n     *\n     *                       Subsequent phases PARTIAL2 (optional) and MERGEPARTIAL will merge\n     *                       the partial aggregations and output full aggregations.\n     *\n     *        PARTIAL2       Accept ALREADY GROUPED partial aggregations and merge them into another\n     *                       partial aggregation.  Output the merged partial aggregations.\n     *\n     *                       (Haven't seen this one used)\n     *\n     *        PARTIALS       (Behaves for non-distinct the same as PARTIAL2; and behaves for\n     *                       distinct the same as PARTIAL1.)\n     *\n     *        FINAL          Accept ALREADY GROUPED original rows and aggregate them into\n     *                       full aggregations.\n     *\n     *                       Example is a GROUP BY being applied to rows from a sorted table, where\n     *                       the group key is the table sort key (or a prefix).\n     *\n     *        HASH           Accept UNORDERED original rows and aggregate them into a memory table.\n     *                       Output the partial aggregations on closeOp (or low memory).\n     *\n     *                       Similar to PARTIAL1 except original rows are UNORDERED.\n     *\n     *                       Commonly used in both Mapper and Reducer nodes.  Always followed by\n     *                       a Reducer with MERGEPARTIAL GROUP BY.\n     *\n     *        MERGEPARTIAL   Always first operator of a Reducer.  Data is grouped by reduce-shuffle.\n     *\n     *                       (Behaves for non-distinct aggregations the same as FINAL; and behaves\n     *                       for distinct aggregations the same as COMPLETE.)\n     *\n     *                       The output is full aggregation(s).\n     *\n     *                       Used in Reducers after a stage with a HASH GROUP BY operator.\n     *\n     *\n     *  VectorGroupByDesc.ProcessingMode for VectorGroupByOperator:\n     *\n     *     GLOBAL         No key.  All rows --> 1 full aggregation on end of input\n     *\n     *     HASH           Rows aggregated in to hash table on group key -->\n     *                        1 partial aggregation per key (normally, unless there is spilling)\n     *\n     *     MERGE_PARTIAL  As first operator in a REDUCER, partial aggregations come grouped from\n     *                    reduce-shuffle -->\n     *                        aggregate the partial aggregations and emit full aggregation on\n     *                        endGroup / closeOp\n     *\n     *     STREAMING      Rows come from PARENT operator ALREADY GROUPED -->\n     *                        aggregate the rows and emit full aggregation on key change / closeOp\n     *\n     *     NOTE: Hash can spill partial result rows prematurely if it runs low on memory.\n     *     NOTE: Streaming has to compare keys where MergePartial gets an endGroup call.\n     *\n     *\n     *  DECIDER: Which VectorGroupByDesc.ProcessingMode for VectorGroupByOperator?\n     *\n     *     Decides using GroupByDesc.Mode and whether there are keys with the\n     *     VectorGroupByDesc.groupByDescModeToVectorProcessingMode method.\n     *\n     *         Mode.COMPLETE      --> (numKeys == 0 ? ProcessingMode.GLOBAL : ProcessingMode.STREAMING)\n     *\n     *         Mode.HASH          --> ProcessingMode.HASH\n     *\n     *         Mode.MERGEPARTIAL  --> (numKeys == 0 ? ProcessingMode.GLOBAL : ProcessingMode.MERGE_PARTIAL)\n     *\n     *         Mode.PARTIAL1,\n     *         Mode.PARTIAL2,\n     *         Mode.PARTIALS,\n     *         Mode.FINAL        --> ProcessingMode.STREAMING\n     *\n     */\n    boolean hasKeys = (desc.getKeys().size() > 0);\n\n    ProcessingMode processingMode =\n        VectorGroupByDesc.groupByDescModeToVectorProcessingMode(desc.getMode(), hasKeys);\n    if (desc.isGroupingSetsPresent() &&\n        (processingMode != ProcessingMode.HASH && processingMode != ProcessingMode.STREAMING)) {\n      LOG.info(\"Vectorized GROUPING SETS only expected for HASH and STREAMING processing modes\");\n      return false;\n    }\n\n    Pair<Boolean,Boolean> retPair =\n        validateAggregationDescs(desc.getAggregators(), processingMode, hasKeys);\n    if (!retPair.left) {\n      return false;\n    }\n\n    // If all the aggregation outputs are primitive, we can output VectorizedRowBatch.\n    // Otherwise, we the rest of the operator tree will be row mode.\n    VectorGroupByDesc vectorDesc = new VectorGroupByDesc();\n    desc.setVectorDesc(vectorDesc);\n\n    vectorDesc.setVectorOutput(retPair.right);\n\n    vectorDesc.setProcessingMode(processingMode);\n\n    LOG.info(\"Vector GROUP BY operator will use processing mode \" + processingMode.name() +\n        \", isVectorOutput \" + vectorDesc.isVectorOutput());\n\n    return true;\n  }"
        ]
    ],
    "1a1e8357bcb09ab7b775f26b83f00d6f687bbc23": [
        [
            "GenericUDFGrouping::getDisplayString(String)",
            "  85  \n  86  \n  87 -\n  88  \n  89  ",
            "  @Override\n  public String getDisplayString(String[] children) {\n    assert (children.length == 2);\n    return getStandardDisplayString(\"grouping\", children);\n  }",
            "  96  \n  97  \n  98 +\n  99  \n 100  ",
            "  @Override\n  public String getDisplayString(String[] children) {\n    assert (children.length > 1);\n    return getStandardDisplayString(\"grouping\", children);\n  }"
        ],
        [
            "SemanticAnalyzer::rewriteGroupingFunctionAST(List,ASTNode,boolean)",
            "3063  \n3064  \n3065 -\n3066 -\n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075  \n3076  \n3077  \n3078 -\n3079  \n3080 -\n3081 -\n3082 -\n3083 -\n3084 -\n3085 -\n3086 -\n3087 -\n3088 -\n3089 -\n3090 -\n3091 -\n3092 -\n3093 -\n3094 -\n3095 -\n3096 -\n3097 -\n3098  \n3099 -\n3100 -\n3101 -\n3102 -\n3103 -\n3104 -\n3105  \n3106  \n3107  \n3108  \n3109  \n3110  \n3111  \n3112 -\n3113 -\n3114 -\n3115 -\n3116 -\n3117  ",
            "  protected static ASTNode rewriteGroupingFunctionAST(final List<ASTNode> grpByAstExprs, ASTNode targetNode,\n          final boolean noneSet) throws SemanticException {\n    final MutableBoolean visited = new MutableBoolean(false);\n    final MutableBoolean found = new MutableBoolean(false);\n\n    TreeVisitorAction action = new TreeVisitorAction() {\n\n      @Override\n      public Object pre(Object t) {\n        return t;\n      }\n\n      @Override\n      public Object post(Object t) {\n        ASTNode root = (ASTNode) t;\n        if (root.getType() == HiveParser.TOK_FUNCTION && root.getChildCount() == 2) {\n          ASTNode func = (ASTNode) ParseDriver.adaptor.getChild(root, 0);\n          if (func.getText().equals(\"grouping\")) {\n            ASTNode c = (ASTNode) ParseDriver.adaptor.getChild(root, 1);\n            visited.setValue(true);\n            for (int i = 0; i < grpByAstExprs.size(); i++) {\n              ASTNode grpByExpr = grpByAstExprs.get(i);\n              if (grpByExpr.toStringTree().equals(c.toStringTree())) {\n                ASTNode child1;\n                if (noneSet) {\n                  // Query does not contain CUBE, ROLLUP, or GROUPING SETS, and thus,\n                  // grouping should return 0\n                  child1 = (ASTNode) ParseDriver.adaptor.create(HiveParser.IntegralLiteral,\n                        String.valueOf(0));\n                } else {\n                  // We refer to grouping_id column\n                  child1 = (ASTNode) ParseDriver.adaptor.create(\n                          HiveParser.TOK_TABLE_OR_COL, \"TOK_TABLE_OR_COL\");\n                  ParseDriver.adaptor.addChild(child1, ParseDriver.adaptor.create(\n                          HiveParser.Identifier, VirtualColumn.GROUPINGID.getName()));\n                }\n                ASTNode child2 = (ASTNode) ParseDriver.adaptor.create(HiveParser.IntegralLiteral,\n                        String.valueOf(IntMath.mod(-i-1, grpByAstExprs.size())));\n                root.setChild(1, child1);\n                root.addChild(child2);\n                found.setValue(true);\n                break;\n              }\n            }\n          }\n        }\n        return t;\n      }\n    };\n    ASTNode newTargetNode = (ASTNode) new TreeVisitor(ParseDriver.adaptor).visit(targetNode, action);\n    if (visited.booleanValue() && !found.booleanValue()) {\n      throw new SemanticException(ErrorMsg.HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY.getMsg());\n    }\n    return newTargetNode;\n  }",
            "3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075 +\n3076  \n3077 +\n3078 +\n3079 +\n3080 +\n3081 +\n3082 +\n3083 +\n3084 +\n3085 +\n3086 +\n3087 +\n3088 +\n3089 +\n3090 +\n3091 +\n3092 +\n3093 +\n3094 +\n3095 +\n3096 +\n3097 +\n3098 +\n3099 +\n3100 +\n3101 +\n3102 +\n3103 +\n3104 +\n3105 +\n3106 +\n3107 +\n3108 +\n3109 +\n3110 +\n3111 +\n3112 +\n3113 +\n3114 +\n3115 +\n3116 +\n3117  \n3118  \n3119  \n3120 +\n3121 +\n3122 +\n3123 +\n3124 +\n3125  \n3126  \n3127  \n3128  \n3129  \n3130 +\n3131  ",
            "  protected static ASTNode rewriteGroupingFunctionAST(final List<ASTNode> grpByAstExprs, ASTNode targetNode,\n          final boolean noneSet) throws SemanticException {\n\n    TreeVisitorAction action = new TreeVisitorAction() {\n\n      @Override\n      public Object pre(Object t) {\n        return t;\n      }\n\n      @Override\n      public Object post(Object t) {\n        ASTNode root = (ASTNode) t;\n        if (root.getType() == HiveParser.TOK_FUNCTION) {\n          ASTNode func = (ASTNode) ParseDriver.adaptor.getChild(root, 0);\n          if (func.getText().equals(\"grouping\") && func.getChildCount() == 0) {\n            int numberOperands = ParseDriver.adaptor.getChildCount(root);\n            // We implement this logic using replaceChildren instead of replacing\n            // the root node itself because windowing logic stores multiple\n            // pointers to the AST, and replacing root might lead to some pointers\n            // leading to non-rewritten version\n            ASTNode newRoot = new ASTNode();\n            // Rewritten grouping function\n            ASTNode groupingFunc = (ASTNode) ParseDriver.adaptor.create(\n                    HiveParser.Identifier, \"grouping\");\n            ParseDriver.adaptor.addChild(groupingFunc, ParseDriver.adaptor.create(\n                    HiveParser.Identifier, \"rewritten\"));\n            newRoot.addChild(groupingFunc);\n            // Grouping ID reference\n            ASTNode childGroupingID;\n            if (noneSet) {\n              // Query does not contain CUBE, ROLLUP, or GROUPING SETS, and thus,\n              // grouping should return 0\n              childGroupingID = (ASTNode) ParseDriver.adaptor.create(HiveParser.IntegralLiteral,\n                    String.valueOf(0));\n            } else {\n              // We refer to grouping_id column\n              childGroupingID = (ASTNode) ParseDriver.adaptor.create(\n                      HiveParser.TOK_TABLE_OR_COL, \"TOK_TABLE_OR_COL\");\n              ParseDriver.adaptor.addChild(childGroupingID, ParseDriver.adaptor.create(\n                      HiveParser.Identifier, VirtualColumn.GROUPINGID.getName()));\n            }\n            newRoot.addChild(childGroupingID);\n            // Indices\n            for (int i = 1; i < numberOperands; i++) {\n              ASTNode c = (ASTNode) ParseDriver.adaptor.getChild(root, i);\n              for (int j = 0; j < grpByAstExprs.size(); j++) {\n                ASTNode grpByExpr = grpByAstExprs.get(j);\n                if (grpByExpr.toStringTree().equals(c.toStringTree())) {\n                  // Create and add AST node with position of grouping function input\n                  // in group by clause\n                  ASTNode childN = (ASTNode) ParseDriver.adaptor.create(HiveParser.IntegralLiteral,\n                          String.valueOf(IntMath.mod(-j-1, grpByAstExprs.size())));\n                  newRoot.addChild(childN);\n                  break;\n                }\n              }\n            }\n            if (numberOperands + 1 != ParseDriver.adaptor.getChildCount(newRoot)) {\n              throw new RuntimeException(ErrorMsg.HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY.getMsg());\n            }\n            // Replace expression\n            root.replaceChildren(0, numberOperands - 1, newRoot);\n          }\n        }\n        return t;\n      }\n    };\n    return (ASTNode) new TreeVisitor(ParseDriver.adaptor).visit(targetNode, action);\n  }"
        ],
        [
            "GenericUDFGrouping::evaluate(DeferredObject)",
            "  76  \n  77  \n  78  \n  79  \n  80 -\n  81 -\n  82 -\n  83  ",
            "  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    // groupingId = PrimitiveObjectInspectorUtils.getInt(arguments[0].get(), groupingIdOI);\n    // Check that the bit at the given index is '1' or '0'\n    byteWritable.set((byte)\n            ((PrimitiveObjectInspectorUtils.getInt(arguments[0].get(), groupingIdOI) >> index) & 1));\n    return byteWritable;\n  }",
            "  80  \n  81  \n  82  \n  83  \n  84 +\n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94  ",
            "  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    // groupingId = PrimitiveObjectInspectorUtils.getInt(arguments[0].get(), groupingIdOI);\n    // Check that the bit at the given index is '1' or '0'\n    int result = 0;\n    // grouping(c1, c2, c3)\n    // is equivalent to\n    // 4 * grouping(c1) + 2 * grouping(c2) + grouping(c3)\n    for (int a = 1; a < arguments.length; a++) {\n      result += IntMath.pow(2, indices.length - a) *\n              ((PrimitiveObjectInspectorUtils.getInt(arguments[0].get(), groupingIdOI) >> indices[a - 1]) & 1);\n    }\n    intWritable.set(result);\n    return intWritable;\n  }"
        ],
        [
            "GenericUDFGrouping::initialize(ObjectInspector)",
            "  51  \n  52  \n  53 -\n  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 -\n  68 -\n  69 -\n  70  \n  71 -\n  72  \n  73 -\n  74  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length != 2) {\n      throw new UDFArgumentLengthException(\n        \"grouping() requires 2 argument, got \" + arguments.length);\n    }\n\n    if (arguments[0].getCategory() != Category.PRIMITIVE) {\n      throw new UDFArgumentTypeException(0, \"The first argument to grouping() must be primitive\");\n    }\n    PrimitiveObjectInspector arg1OI = (PrimitiveObjectInspector) arguments[0];\n    if (arg1OI.getPrimitiveCategory() != PrimitiveCategory.INT) {\n      throw new UDFArgumentTypeException(0, \"The first argument to grouping() must be an integer\");\n    }\n    groupingIdOI = (IntObjectInspector) arguments[0];\n\n    PrimitiveObjectInspector arg2OI = (PrimitiveObjectInspector) arguments[1];\n    if (!(arg2OI instanceof WritableConstantIntObjectInspector)) {\n      throw new UDFArgumentTypeException(1, \"The second argument to grouping() must be a constant\");\n    }\n    index = ((WritableConstantIntObjectInspector)arg2OI).getWritableConstantValue().get();\n\n    return PrimitiveObjectInspectorFactory.writableByteObjectInspector;\n  }",
            "  52  \n  53  \n  54 +\n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73 +\n  74 +\n  75  \n  76  \n  77 +\n  78  ",
            "  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    if (arguments.length < 2) {\n      throw new UDFArgumentLengthException(\n        \"grouping() requires at least 2 argument, got \" + arguments.length);\n    }\n\n    if (arguments[0].getCategory() != Category.PRIMITIVE) {\n      throw new UDFArgumentTypeException(0, \"The first argument to grouping() must be primitive\");\n    }\n    PrimitiveObjectInspector arg1OI = (PrimitiveObjectInspector) arguments[0];\n    if (arg1OI.getPrimitiveCategory() != PrimitiveCategory.INT) {\n      throw new UDFArgumentTypeException(0, \"The first argument to grouping() must be an integer\");\n    }\n    groupingIdOI = (IntObjectInspector) arguments[0];\n\n    indices = new int[arguments.length - 1];\n    for (int i = 1; i < arguments.length; i++) {\n      PrimitiveObjectInspector arg2OI = (PrimitiveObjectInspector) arguments[i];\n      if (!(arg2OI instanceof WritableConstantIntObjectInspector)) {\n        throw new UDFArgumentTypeException(i, \"Must be a constant\");\n      }\n      indices[i - 1] = ((WritableConstantIntObjectInspector)arg2OI).getWritableConstantValue().get();\n    }\n\n    return PrimitiveObjectInspectorFactory.writableIntObjectInspector;\n  }"
        ]
    ],
    "13967d8f284a4c7af8f2cc1f6c0256e507769357": [
        [
            "ObjectStore::getNextNotification(NotificationEventRequest)",
            "8262  \n8263  \n8264  \n8265  \n8266  \n8267  \n8268  \n8269  \n8270  \n8271  \n8272  \n8273  \n8274  \n8275 -\n8276  \n8277  \n8278 -\n8279 -\n8280  \n8281  \n8282  \n8283  \n8284  \n8285  \n8286  \n8287  \n8288  \n8289  \n8290  \n8291  \n8292  \n8293  \n8294  \n8295  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return null;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      NotificationEventResponse result = new NotificationEventResponse();\n      result.setEvents(new ArrayList<NotificationEvent>());\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      int numEvents = 0;\n      while (i.hasNext() && numEvents++ < maxEvents) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      if (query != null) {\n        query.closeAll();\n      }\n      if (!commited) {\n        rollbackTransaction();\n        return null;\n      }\n    }\n  }",
            "8262  \n8263  \n8264  \n8265  \n8266 +\n8267 +\n8268 +\n8269  \n8270  \n8271  \n8272  \n8273  \n8274  \n8275  \n8276  \n8277  \n8278 +\n8279  \n8280  \n8281  \n8282  \n8283  \n8284  \n8285  \n8286  \n8287  \n8288  \n8289  \n8290  \n8291  \n8292  \n8293  \n8294  \n8295  \n8296  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n\n    NotificationEventResponse result = new NotificationEventResponse();\n    result.setEvents(new ArrayList<NotificationEvent>());\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return result;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      int numEvents = 0;\n      while (i.hasNext() && numEvents++ < maxEvents) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      if (query != null) {\n        query.closeAll();\n      }\n      if (!commited) {\n        rollbackTransaction();\n        return null;\n      }\n    }\n  }"
        ],
        [
            "TestObjectStore::testNotificationOps()",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  /**\n   * Test notification operations\n   */\n  @Test\n  public void testNotificationOps() throws InterruptedException {\n    final int NO_EVENT_ID = 0;\n    final int FIRST_EVENT_ID = 1;\n    final int SECOND_EVENT_ID = 2;\n\n    NotificationEvent event =\n        new NotificationEvent(0, 0, EventMessage.EventType.CREATE_DATABASE.toString(), \"\");\n    NotificationEventResponse eventResponse;\n    CurrentNotificationEventId eventId;\n\n    // Verify that there is no notifications available yet\n    eventId = objectStore.getCurrentNotificationEventId();\n    Assert.assertEquals(NO_EVENT_ID, eventId.getEventId());\n\n    // Verify that addNotificationEvent() updates the NotificationEvent with the new event ID\n    objectStore.addNotificationEvent(event);\n    Assert.assertEquals(FIRST_EVENT_ID, event.getEventId());\n    objectStore.addNotificationEvent(event);\n    Assert.assertEquals(SECOND_EVENT_ID, event.getEventId());\n\n    // Verify that objectStore fetches the latest notification event ID\n    eventId = objectStore.getCurrentNotificationEventId();\n    Assert.assertEquals(SECOND_EVENT_ID, eventId.getEventId());\n\n    // Verify that getNextNotification() returns all events\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest());\n    Assert.assertEquals(2, eventResponse.getEventsSize());\n    Assert.assertEquals(FIRST_EVENT_ID, eventResponse.getEvents().get(0).getEventId());\n    Assert.assertEquals(SECOND_EVENT_ID, eventResponse.getEvents().get(1).getEventId());\n    // Verify that getNextNotification(last) returns events after a specified event\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest(FIRST_EVENT_ID));\n    Assert.assertEquals(1, eventResponse.getEventsSize());\n    Assert.assertEquals(SECOND_EVENT_ID, eventResponse.getEvents().get(0).getEventId());\n\n    // Verify that cleanNotificationEvents() cleans up all old notifications\n    Thread.sleep(1);\n    objectStore.cleanNotificationEvents(1);\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest());\n    Assert.assertEquals(0, eventResponse.getEventsSize());\n  }",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 +\n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163 +\n 164 +\n 165 +\n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "  /**\n   * Test notification operations\n   */\n  @Test\n  public void testNotificationOps() throws InterruptedException {\n    final int NO_EVENT_ID = 0;\n    final int FIRST_EVENT_ID = 1;\n    final int SECOND_EVENT_ID = 2;\n\n    NotificationEvent event =\n        new NotificationEvent(0, 0, EventMessage.EventType.CREATE_DATABASE.toString(), \"\");\n    NotificationEventResponse eventResponse;\n    CurrentNotificationEventId eventId;\n\n    // Verify that there is no notifications available yet\n    eventId = objectStore.getCurrentNotificationEventId();\n    Assert.assertEquals(NO_EVENT_ID, eventId.getEventId());\n\n    // Verify that addNotificationEvent() updates the NotificationEvent with the new event ID\n    objectStore.addNotificationEvent(event);\n    Assert.assertEquals(FIRST_EVENT_ID, event.getEventId());\n    objectStore.addNotificationEvent(event);\n    Assert.assertEquals(SECOND_EVENT_ID, event.getEventId());\n\n    // Verify that objectStore fetches the latest notification event ID\n    eventId = objectStore.getCurrentNotificationEventId();\n    Assert.assertEquals(SECOND_EVENT_ID, eventId.getEventId());\n\n    // Verify that getNextNotification() returns all events\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest());\n    Assert.assertEquals(2, eventResponse.getEventsSize());\n    Assert.assertEquals(FIRST_EVENT_ID, eventResponse.getEvents().get(0).getEventId());\n    Assert.assertEquals(SECOND_EVENT_ID, eventResponse.getEvents().get(1).getEventId());\n\n    // Verify that getNextNotification(last) returns events after a specified event\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest(FIRST_EVENT_ID));\n    Assert.assertEquals(1, eventResponse.getEventsSize());\n    Assert.assertEquals(SECOND_EVENT_ID, eventResponse.getEvents().get(0).getEventId());\n\n    // Verify that getNextNotification(last) returns zero events if there are no more notifications available\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest(SECOND_EVENT_ID));\n    Assert.assertEquals(0, eventResponse.getEventsSize());\n\n    // Verify that cleanNotificationEvents() cleans up all old notifications\n    Thread.sleep(1);\n    objectStore.cleanNotificationEvents(1);\n    eventResponse = objectStore.getNextNotification(new NotificationEventRequest());\n    Assert.assertEquals(0, eventResponse.getEventsSize());\n  }"
        ]
    ],
    "8a946ccb31b64f246b0c80d202e098aa46363a8f": [
        [
            "SemanticAnalyzer::genFileSinkPlan(String,QB,Operator)",
            "6760  \n6761  \n6762  \n6763  \n6764  \n6765  \n6766  \n6767  \n6768  \n6769  \n6770  \n6771  \n6772  \n6773  \n6774  \n6775  \n6776  \n6777  \n6778  \n6779  \n6780  \n6781  \n6782  \n6783  \n6784  \n6785  \n6786  \n6787  \n6788  \n6789  \n6790  \n6791  \n6792  \n6793  \n6794  \n6795  \n6796  \n6797  \n6798  \n6799  \n6800  \n6801  \n6802  \n6803  \n6804  \n6805  \n6806  \n6807  \n6808  \n6809  \n6810  \n6811  \n6812  \n6813  \n6814  \n6815  \n6816  \n6817  \n6818  \n6819  \n6820  \n6821  \n6822  \n6823  \n6824  \n6825  \n6826  \n6827  \n6828  \n6829  \n6830  \n6831  \n6832  \n6833  \n6834  \n6835  \n6836  \n6837  \n6838  \n6839  \n6840  \n6841  \n6842  \n6843  \n6844  \n6845  \n6846  \n6847  \n6848  \n6849  \n6850  \n6851  \n6852  \n6853  \n6854  \n6855  \n6856  \n6857  \n6858  \n6859  \n6860  \n6861  \n6862  \n6863  \n6864  \n6865  \n6866  \n6867  \n6868  \n6869  \n6870  \n6871  \n6872  \n6873  \n6874  \n6875  \n6876  \n6877  \n6878  \n6879  \n6880  \n6881  \n6882  \n6883  \n6884  \n6885  \n6886  \n6887  \n6888  \n6889  \n6890  \n6891  \n6892  \n6893  \n6894  \n6895  \n6896  \n6897  \n6898  \n6899  \n6900  \n6901  \n6902  \n6903  \n6904  \n6905  \n6906  \n6907  \n6908  \n6909  \n6910  \n6911  \n6912  \n6913  \n6914  \n6915  \n6916  \n6917  \n6918  \n6919  \n6920  \n6921  \n6922  \n6923  \n6924  \n6925  \n6926  \n6927  \n6928  \n6929  \n6930  \n6931  \n6932  \n6933  \n6934  \n6935  \n6936  \n6937  \n6938  \n6939  \n6940  \n6941  \n6942  \n6943  \n6944  \n6945  \n6946  \n6947  \n6948  \n6949  \n6950  \n6951  \n6952  \n6953  \n6954  \n6955  \n6956  \n6957  \n6958  \n6959  \n6960  \n6961  \n6962  \n6963  \n6964  \n6965  \n6966  \n6967  \n6968  \n6969  \n6970  \n6971  \n6972  \n6973  \n6974  \n6975  \n6976  \n6977  \n6978  \n6979  \n6980  \n6981  \n6982  \n6983  \n6984  \n6985  \n6986  \n6987  \n6988  \n6989  \n6990  \n6991  \n6992  \n6993  \n6994  \n6995  \n6996  \n6997  \n6998  \n6999  \n7000  \n7001  \n7002  \n7003  \n7004  \n7005  \n7006  \n7007  \n7008  \n7009  \n7010  \n7011  \n7012  \n7013  \n7014  \n7015  \n7016  \n7017  \n7018  \n7019  \n7020  \n7021  \n7022  \n7023  \n7024  \n7025  \n7026  \n7027  \n7028  \n7029  \n7030  \n7031  \n7032  \n7033  \n7034  \n7035  \n7036  \n7037  \n7038  \n7039  \n7040  \n7041  \n7042  \n7043  \n7044  \n7045  \n7046  \n7047  \n7048  \n7049  \n7050  \n7051  \n7052  \n7053  \n7054  \n7055  \n7056  \n7057  \n7058  \n7059  \n7060  \n7061  \n7062  \n7063  \n7064  \n7065  \n7066  \n7067  \n7068  \n7069  \n7070  \n7071  \n7072  \n7073  \n7074  \n7075  \n7076  \n7077  \n7078  \n7079  \n7080  \n7081  \n7082  \n7083  \n7084  \n7085  \n7086  \n7087  \n7088  \n7089  \n7090  \n7091  \n7092  \n7093  \n7094  \n7095  \n7096  \n7097  \n7098  \n7099  \n7100  \n7101  \n7102  \n7103  \n7104  \n7105  \n7106  \n7107  \n7108  \n7109  \n7110  \n7111  \n7112  \n7113  \n7114  \n7115  \n7116  \n7117  \n7118  \n7119  \n7120  \n7121  \n7122  \n7123  \n7124  \n7125  \n7126  \n7127  \n7128  \n7129  \n7130  \n7131  \n7132  \n7133  \n7134  \n7135  \n7136  \n7137  \n7138  \n7139  \n7140  \n7141  \n7142  \n7143  \n7144  \n7145  \n7146  \n7147  \n7148  \n7149  \n7150  \n7151  \n7152  \n7153  \n7154  \n7155  \n7156  \n7157  \n7158  \n7159  \n7160  \n7161  \n7162  \n7163  \n7164  \n7165  \n7166  \n7167  \n7168  \n7169  \n7170  \n7171  \n7172  \n7173  \n7174  \n7175  \n7176  \n7177  \n7178  \n7179  \n7180  \n7181  \n7182  \n7183  \n7184  \n7185  \n7186  \n7187  \n7188  \n7189  \n7190  \n7191  \n7192  \n7193  \n7194  \n7195  \n7196  \n7197  \n7198  \n7199  \n7200  \n7201  \n7202  \n7203  \n7204  \n7205  \n7206  \n7207  \n7208  \n7209  \n7210  \n7211  \n7212  \n7213  \n7214  \n7215  \n7216  \n7217  \n7218  \n7219  \n7220  \n7221  \n7222  \n7223  \n7224  \n7225  \n7226  \n7227  \n7228  \n7229  \n7230  \n7231  \n7232  \n7233  \n7234  \n7235  \n7236  \n7237  \n7238  \n7239  \n7240  \n7241  \n7242  \n7243  \n7244  \n7245  \n7246  \n7247  \n7248  \n7249  \n7250  \n7251  \n7252  \n7253  \n7254  \n7255  \n7256  \n7257  \n7258  \n7259  \n7260  \n7261  \n7262  \n7263  \n7264  \n7265  \n7266  \n7267  \n7268  \n7269  \n7270  \n7271  \n7272  \n7273 -\n7274  \n7275  \n7276  \n7277  \n7278  \n7279  \n7280  \n7281  \n7282  \n7283  \n7284  \n7285  \n7286 -\n7287  \n7288  \n7289  \n7290  \n7291  \n7292  \n7293  \n7294  \n7295  \n7296  \n7297  \n7298  \n7299  \n7300  \n7301  \n7302  \n7303  \n7304  \n7305  \n7306  \n7307  \n7308  \n7309  \n7310  \n7311  \n7312  \n7313  ",
            "  @SuppressWarnings(\"nls\")\n  protected Operator genFileSinkPlan(String dest, QB qb, Operator input)\n      throws SemanticException {\n\n    RowResolver inputRR = opParseCtx.get(input).getRowResolver();\n    QBMetaData qbm = qb.getMetaData();\n    Integer dest_type = qbm.getDestTypeForAlias(dest);\n\n    Table dest_tab = null; // destination table if any\n    boolean destTableIsAcid = false; // should the destination table be written to using ACID\n    boolean destTableIsTemporary = false;\n    boolean destTableIsMaterialization = false;\n    Partition dest_part = null;// destination partition if any\n    Path queryTmpdir = null; // the intermediate destination directory\n    Path dest_path = null; // the final destination directory\n    TableDesc table_desc = null;\n    int currentTableId = 0;\n    boolean isLocal = false;\n    SortBucketRSCtx rsCtx = new SortBucketRSCtx();\n    DynamicPartitionCtx dpCtx = null;\n    LoadTableDesc ltd = null;\n    ListBucketingCtx lbCtx = null;\n    Map<String, String> partSpec = null;\n\n    switch (dest_type.intValue()) {\n    case QBMetaData.DEST_TABLE: {\n\n      dest_tab = qbm.getDestTableForAlias(dest);\n      destTableIsAcid = AcidUtils.isAcidTable(dest_tab);\n      destTableIsTemporary = dest_tab.isTemporary();\n\n      // Is the user trying to insert into a external tables\n      if ((!conf.getBoolVar(HiveConf.ConfVars.HIVE_INSERT_INTO_EXTERNAL_TABLES)) &&\n          (dest_tab.getTableType().equals(TableType.EXTERNAL_TABLE))) {\n        throw new SemanticException(\n            ErrorMsg.INSERT_EXTERNAL_TABLE.getMsg(dest_tab.getTableName()));\n      }\n\n      partSpec = qbm.getPartSpecForAlias(dest);\n      dest_path = dest_tab.getPath();\n\n      // If the query here is an INSERT_INTO and the target is an immutable table,\n      // verify that our destination is empty before proceeding\n      if (dest_tab.isImmutable() &&\n          qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),dest_tab.getTableName())){\n        try {\n          FileSystem fs = dest_path.getFileSystem(conf);\n          if (! MetaStoreUtils.isDirEmpty(fs,dest_path)){\n            LOG.warn(\"Attempted write into an immutable table : \"\n                + dest_tab.getTableName() + \" : \" + dest_path);\n            throw new SemanticException(\n                ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(dest_tab.getTableName()));\n          }\n        } catch (IOException ioe) {\n            LOG.warn(\"Error while trying to determine if immutable table has any data : \"\n                + dest_tab.getTableName() + \" : \" + dest_path);\n          throw new SemanticException(ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(ioe.getMessage()));\n        }\n      }\n\n      // check for partition\n      List<FieldSchema> parts = dest_tab.getPartitionKeys();\n      if (parts != null && parts.size() > 0) { // table is partitioned\n        if (partSpec == null || partSpec.size() == 0) { // user did NOT specify partition\n          throw new SemanticException(generateErrorMessage(\n              qb.getParseInfo().getDestForClause(dest),\n              ErrorMsg.NEED_PARTITION_ERROR.getMsg()));\n        }\n        dpCtx = qbm.getDPCtx(dest);\n        if (dpCtx == null) {\n          dest_tab.validatePartColumnNames(partSpec, false);\n          dpCtx = new DynamicPartitionCtx(dest_tab, partSpec,\n              conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME),\n              conf.getIntVar(HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTSPERNODE));\n          qbm.setDPCtx(dest, dpCtx);\n        }\n\n        if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING)) { // allow DP\n          throw new SemanticException(generateErrorMessage(\n              qb.getParseInfo().getDestForClause(dest),\n              ErrorMsg.DYNAMIC_PARTITION_DISABLED.getMsg()));\n        }\n        if (dpCtx.getSPPath() != null) {\n          dest_path = new Path(dest_tab.getPath(), dpCtx.getSPPath());\n        }\n        if ((dest_tab.getNumBuckets() > 0)) {\n          dpCtx.setNumBuckets(dest_tab.getNumBuckets());\n        }\n      }\n\n      boolean isNonNativeTable = dest_tab.isNonNative();\n      if (isNonNativeTable) {\n        queryTmpdir = dest_path;\n      } else {\n        queryTmpdir = ctx.getTempDirForPath(dest_path, true);\n      }\n      if (dpCtx != null) {\n        // set the root of the temporary path where dynamic partition columns will populate\n        dpCtx.setRootPath(queryTmpdir);\n      }\n      // this table_desc does not contain the partitioning columns\n      table_desc = Utilities.getTableDesc(dest_tab);\n\n      // Add sorting/bucketing if needed\n      input = genBucketingSortingDest(dest, input, qb, table_desc, dest_tab, rsCtx);\n\n      idToTableNameMap.put(String.valueOf(destTableId), dest_tab.getTableName());\n      currentTableId = destTableId;\n      destTableId++;\n\n      lbCtx = constructListBucketingCtx(dest_tab.getSkewedColNames(),\n          dest_tab.getSkewedColValues(), dest_tab.getSkewedColValueLocationMaps(),\n          dest_tab.isStoredAsSubDirectories(), conf);\n\n      // Create the work for moving the table\n      // NOTE: specify Dynamic partitions in dest_tab for WriteEntity\n      if (!isNonNativeTable) {\n        AcidUtils.Operation acidOp = AcidUtils.Operation.NOT_ACID;\n        if (destTableIsAcid) {\n          acidOp = getAcidType(table_desc.getOutputFileFormatClass(), dest);\n          checkAcidConstraints(qb, table_desc, dest_tab);\n        }\n        ltd = new LoadTableDesc(queryTmpdir, table_desc, dpCtx, acidOp);\n        ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n            dest_tab.getTableName()));\n        ltd.setLbCtx(lbCtx);\n        loadTableWork.add(ltd);\n      } else {\n        // This is a non-native table.\n        // We need to set stats as inaccurate.\n        setStatsForNonNativeTable(dest_tab);\n        // true if it is insert overwrite.\n        boolean overwrite = !qb.getParseInfo().isInsertIntoTable(\n                String.format(\"%s.%s\", dest_tab.getDbName(), dest_tab.getTableName()));\n        createInsertDesc(dest_tab, overwrite);\n      }\n\n      WriteEntity output = null;\n\n      // Here only register the whole table for post-exec hook if no DP present\n      // in the case of DP, we will register WriteEntity in MoveTask when the\n      // list of dynamically created partitions are known.\n      if ((dpCtx == null || dpCtx.getNumDPCols() == 0)) {\n        output = new WriteEntity(dest_tab,  determineWriteType(ltd, isNonNativeTable, dest));\n        if (!outputs.add(output)) {\n          throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n              .getMsg(dest_tab.getTableName()));\n        }\n      }\n      if ((dpCtx != null) && (dpCtx.getNumDPCols() >= 0)) {\n        // No static partition specified\n        if (dpCtx.getNumSPCols() == 0) {\n          output = new WriteEntity(dest_tab, determineWriteType(ltd, isNonNativeTable, dest), false);\n          outputs.add(output);\n          output.setDynamicPartitionWrite(true);\n        }\n        // part of the partition specified\n        // Create a DummyPartition in this case. Since, the metastore does not store partial\n        // partitions currently, we need to store dummy partitions\n        else {\n          try {\n            String ppath = dpCtx.getSPPath();\n            ppath = ppath.substring(0, ppath.length() - 1);\n            DummyPartition p =\n                new DummyPartition(dest_tab, dest_tab.getDbName()\n                    + \"@\" + dest_tab.getTableName() + \"@\" + ppath,\n                    partSpec);\n            output = new WriteEntity(p, getWriteType(dest), false);\n            output.setDynamicPartitionWrite(true);\n            outputs.add(output);\n          } catch (HiveException e) {\n            throw new SemanticException(e.getMessage(), e);\n          }\n        }\n      }\n\n      ctx.getLoadTableOutputMap().put(ltd, output);\n      break;\n    }\n    case QBMetaData.DEST_PARTITION: {\n\n      dest_part = qbm.getDestPartitionForAlias(dest);\n      dest_tab = dest_part.getTable();\n      destTableIsAcid = AcidUtils.isAcidTable(dest_tab);\n      if ((!conf.getBoolVar(HiveConf.ConfVars.HIVE_INSERT_INTO_EXTERNAL_TABLES)) &&\n          dest_tab.getTableType().equals(TableType.EXTERNAL_TABLE)) {\n        throw new SemanticException(\n            ErrorMsg.INSERT_EXTERNAL_TABLE.getMsg(dest_tab.getTableName()));\n      }\n\n      Path tabPath = dest_tab.getPath();\n      Path partPath = dest_part.getDataLocation();\n\n      // If the query here is an INSERT_INTO and the target is an immutable table,\n      // verify that our destination is empty before proceeding\n      if (dest_tab.isImmutable() &&\n          qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),dest_tab.getTableName())){\n        try {\n          FileSystem fs = partPath.getFileSystem(conf);\n          if (! MetaStoreUtils.isDirEmpty(fs,partPath)){\n            LOG.warn(\"Attempted write into an immutable table partition : \"\n                + dest_tab.getTableName() + \" : \" + partPath);\n            throw new SemanticException(\n                ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(dest_tab.getTableName()));\n          }\n        } catch (IOException ioe) {\n            LOG.warn(\"Error while trying to determine if immutable table partition has any data : \"\n                + dest_tab.getTableName() + \" : \" + partPath);\n          throw new SemanticException(ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(ioe.getMessage()));\n        }\n      }\n\n      // if the table is in a different dfs than the partition,\n      // replace the partition's dfs with the table's dfs.\n      dest_path = new Path(tabPath.toUri().getScheme(), tabPath.toUri()\n          .getAuthority(), partPath.toUri().getPath());\n\n      queryTmpdir = ctx.getTempDirForPath(dest_path, true);\n      table_desc = Utilities.getTableDesc(dest_tab);\n\n      // Add sorting/bucketing if needed\n      input = genBucketingSortingDest(dest, input, qb, table_desc, dest_tab, rsCtx);\n\n      idToTableNameMap.put(String.valueOf(destTableId), dest_tab.getTableName());\n      currentTableId = destTableId;\n      destTableId++;\n\n      lbCtx = constructListBucketingCtx(dest_part.getSkewedColNames(),\n          dest_part.getSkewedColValues(), dest_part.getSkewedColValueLocationMaps(),\n          dest_part.isStoredAsSubDirectories(), conf);\n      AcidUtils.Operation acidOp = AcidUtils.Operation.NOT_ACID;\n      if (destTableIsAcid) {\n        acidOp = getAcidType(table_desc.getOutputFileFormatClass(), dest);\n        checkAcidConstraints(qb, table_desc, dest_tab);\n      }\n      ltd = new LoadTableDesc(queryTmpdir, table_desc, dest_part.getSpec(), acidOp);\n      ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n          dest_tab.getTableName()));\n      ltd.setLbCtx(lbCtx);\n\n      loadTableWork.add(ltd);\n      if (!outputs.add(new WriteEntity(dest_part,\n        determineWriteType(ltd, dest_tab.isNonNative(), dest)))) {\n\n        throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n            .getMsg(dest_tab.getTableName() + \"@\" + dest_part.getName()));\n      }\n      break;\n    }\n    case QBMetaData.DEST_LOCAL_FILE:\n      isLocal = true;\n      // fall through\n    case QBMetaData.DEST_DFS_FILE: {\n      dest_path = new Path(qbm.getDestFileForAlias(dest));\n\n      if (isLocal) {\n        // for local directory - we always write to map-red intermediate\n        // store and then copy to local fs\n        queryTmpdir = ctx.getMRTmpPath();\n      } else {\n        // otherwise write to the file system implied by the directory\n        // no copy is required. we may want to revisit this policy in future\n\n        try {\n          Path qPath = FileUtils.makeQualified(dest_path, conf);\n          queryTmpdir = ctx.getTempDirForPath(qPath, true);\n        } catch (Exception e) {\n          throw new SemanticException(\"Error creating temporary folder on: \"\n              + dest_path, e);\n        }\n      }\n      String cols = \"\";\n      String colTypes = \"\";\n      ArrayList<ColumnInfo> colInfos = inputRR.getColumnInfos();\n\n      // CTAS case: the file output format and serde are defined by the create\n      // table command rather than taking the default value\n      List<FieldSchema> field_schemas = null;\n      CreateTableDesc tblDesc = qb.getTableDesc();\n      CreateViewDesc viewDesc = qb.getViewDesc();\n      if (tblDesc != null) {\n        field_schemas = new ArrayList<FieldSchema>();\n        destTableIsTemporary = tblDesc.isTemporary();\n        destTableIsMaterialization = tblDesc.isMaterialization();\n      } else if (viewDesc != null) {\n        field_schemas = new ArrayList<FieldSchema>();\n        destTableIsTemporary = false;\n      }\n\n      boolean first = true;\n      for (ColumnInfo colInfo : colInfos) {\n        String[] nm = inputRR.reverseLookup(colInfo.getInternalName());\n\n        if (nm[1] != null) { // non-null column alias\n          colInfo.setAlias(nm[1]);\n        }\n\n        String colName = colInfo.getInternalName();  //default column name\n        if (field_schemas != null) {\n          FieldSchema col = new FieldSchema();\n          if (!(\"\".equals(nm[0])) && nm[1] != null) {\n            colName = unescapeIdentifier(colInfo.getAlias()).toLowerCase(); // remove ``\n          }\n          colName = fixCtasColumnName(colName);\n          col.setName(colName);\n          String typeName = colInfo.getType().getTypeName();\n          // CTAS should NOT create a VOID type\n          if (typeName.equals(serdeConstants.VOID_TYPE_NAME)) {\n              throw new SemanticException(ErrorMsg.CTAS_CREATES_VOID_TYPE\n              .getMsg(colName));\n          }\n          col.setType(typeName);\n          field_schemas.add(col);\n        }\n\n        if (!first) {\n          cols = cols.concat(\",\");\n          colTypes = colTypes.concat(\":\");\n        }\n\n        first = false;\n        cols = cols.concat(colName);\n\n        // Replace VOID type with string when the output is a temp table or\n        // local files.\n        // A VOID type can be generated under the query:\n        //\n        // select NULL from tt;\n        // or\n        // insert overwrite local directory \"abc\" select NULL from tt;\n        //\n        // where there is no column type to which the NULL value should be\n        // converted.\n        //\n        String tName = colInfo.getType().getTypeName();\n        if (tName.equals(serdeConstants.VOID_TYPE_NAME)) {\n          colTypes = colTypes.concat(serdeConstants.STRING_TYPE_NAME);\n        } else {\n          colTypes = colTypes.concat(tName);\n        }\n      }\n\n      // update the create table descriptor with the resulting schema.\n      if (tblDesc != null) {\n        tblDesc.setCols(new ArrayList<FieldSchema>(field_schemas));\n      } else if (viewDesc != null) {\n        viewDesc.setSchema(new ArrayList<FieldSchema>(field_schemas));\n      }\n\n      boolean isDestTempFile = true;\n      if (!ctx.isMRTmpFileURI(dest_path.toUri().toString())) {\n        idToTableNameMap.put(String.valueOf(destTableId), dest_path.toUri().toString());\n        currentTableId = destTableId;\n        destTableId++;\n        isDestTempFile = false;\n      }\n\n      boolean isDfsDir = (dest_type.intValue() == QBMetaData.DEST_DFS_FILE);\n      loadFileWork.add(new LoadFileDesc(tblDesc, viewDesc, queryTmpdir, dest_path, isDfsDir, cols,\n          colTypes));\n\n      if (tblDesc == null) {\n        if (viewDesc != null) {\n          table_desc = PlanUtils.getTableDesc(viewDesc, cols, colTypes);\n        } else if (qb.getIsQuery()) {\n          String fileFormat;\n          if (SessionState.get().getIsUsingThriftJDBCBinarySerDe()) {\n              fileFormat = \"SequenceFile\";\n              HiveConf.setVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT, fileFormat);\n              table_desc=\n                         PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, fileFormat,\n                           ThriftJDBCBinarySerDe.class);\n              // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n              // write out formatted thrift objects to SequenceFile\n              conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n          } else {\n              fileFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT);\n              table_desc =\n                         PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, fileFormat,\n                           LazySimpleSerDe.class);\n          }\n        } else {\n          table_desc = PlanUtils.getDefaultTableDesc(qb.getDirectoryDesc(), cols, colTypes);\n        }\n      } else {\n        table_desc = PlanUtils.getTableDesc(tblDesc, cols, colTypes);\n      }\n\n      if (!outputs.add(new WriteEntity(dest_path, !isDfsDir, isDestTempFile))) {\n        throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n            .getMsg(dest_path.toUri().toString()));\n      }\n      break;\n    }\n    default:\n      throw new SemanticException(\"Unknown destination type: \" + dest_type);\n    }\n\n    if (!(dest_type.intValue() == QBMetaData.DEST_DFS_FILE && qb.getIsQuery())) {\n      input = genConversionSelectOperator(dest, qb, input, table_desc, dpCtx);\n    }\n\n    inputRR = opParseCtx.get(input).getRowResolver();\n\n    ArrayList<ColumnInfo> vecCol = new ArrayList<ColumnInfo>();\n\n    if (updating(dest) || deleting(dest)) {\n      vecCol.add(new ColumnInfo(VirtualColumn.ROWID.getName(), VirtualColumn.ROWID.getTypeInfo(),\n          \"\", true));\n    } else {\n      try {\n        StructObjectInspector rowObjectInspector = (StructObjectInspector) table_desc\n            .getDeserializer(conf).getObjectInspector();\n        List<? extends StructField> fields = rowObjectInspector\n            .getAllStructFieldRefs();\n        for (int i = 0; i < fields.size(); i++) {\n          vecCol.add(new ColumnInfo(fields.get(i).getFieldName(), TypeInfoUtils\n              .getTypeInfoFromObjectInspector(fields.get(i)\n                  .getFieldObjectInspector()), \"\", false));\n        }\n      } catch (Exception e) {\n        throw new SemanticException(e.getMessage(), e);\n      }\n    }\n\n    RowSchema fsRS = new RowSchema(vecCol);\n\n    // The output files of a FileSink can be merged if they are either not being written to a table\n    // or are being written to a table which is not bucketed\n    // and table the table is not sorted\n    boolean canBeMerged = (dest_tab == null || !((dest_tab.getNumBuckets() > 0) ||\n        (dest_tab.getSortCols() != null && dest_tab.getSortCols().size() > 0)));\n\n    // If this table is working with ACID semantics, turn off merging\n    canBeMerged &= !destTableIsAcid;\n\n    // Generate the partition columns from the parent input\n    if (dest_type.intValue() == QBMetaData.DEST_TABLE\n        || dest_type.intValue() == QBMetaData.DEST_PARTITION) {\n      genPartnCols(dest, input, qb, table_desc, dest_tab, rsCtx);\n    }\n\n    FileSinkDesc fileSinkDesc = new FileSinkDesc(\n      queryTmpdir,\n      table_desc,\n      conf.getBoolVar(HiveConf.ConfVars.COMPRESSRESULT),\n      currentTableId,\n      rsCtx.isMultiFileSpray(),\n      canBeMerged,\n      rsCtx.getNumFiles(),\n      rsCtx.getTotalFiles(),\n      rsCtx.getPartnCols(),\n      dpCtx,\n      dest_path);\n\n    boolean isHiveServerQuery = SessionState.get().isHiveServerQuery();\n    fileSinkDesc.setHiveServerQuery(isHiveServerQuery);\n    // If this is an insert, update, or delete on an ACID table then mark that so the\n    // FileSinkOperator knows how to properly write to it.\n    if (destTableIsAcid) {\n      AcidUtils.Operation wt = updating(dest) ? AcidUtils.Operation.UPDATE :\n          (deleting(dest) ? AcidUtils.Operation.DELETE : AcidUtils.Operation.INSERT);\n      fileSinkDesc.setWriteType(wt);\n      acidFileSinks.add(fileSinkDesc);\n    }\n\n    fileSinkDesc.setTemporary(destTableIsTemporary);\n    fileSinkDesc.setMaterialization(destTableIsMaterialization);\n\n    /* Set List Bucketing context. */\n    if (lbCtx != null) {\n      lbCtx.processRowSkewedIndex(fsRS);\n      lbCtx.calculateSkewedValueSubDirList();\n    }\n    fileSinkDesc.setLbCtx(lbCtx);\n\n    // set the stats publishing/aggregating key prefix\n    // the same as directory name. The directory name\n    // can be changed in the optimizer but the key should not be changed\n    // it should be the same as the MoveWork's sourceDir.\n    fileSinkDesc.setStatsAggPrefix(fileSinkDesc.getDirName().toString());\n    if (!destTableIsMaterialization &&\n            HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n      String statsTmpLoc = ctx.getTempDirForPath(dest_path).toString();\n      fileSinkDesc.setStatsTmpDir(statsTmpLoc);\n      LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n    }\n\n    if (dest_part != null) {\n      try {\n        String staticSpec = Warehouse.makePartPath(dest_part.getSpec());\n        fileSinkDesc.setStaticSpec(staticSpec);\n      } catch (MetaException e) {\n        throw new SemanticException(e);\n      }\n    } else if (dpCtx != null) {\n      fileSinkDesc.setStaticSpec(dpCtx.getSPPath());\n    }\n\n    if (isHiveServerQuery &&\n      null != table_desc &&\n      table_desc.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) &&\n      HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {\n        fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(true);\n    } else {\n        fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(false);\n    }\n\n    Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(\n        fileSinkDesc, fsRS, input), inputRR);\n\n    if (ltd != null && SessionState.get() != null) {\n      SessionState.get().getLineageState()\n          .mapDirToOp(ltd.getSourcePath(), (FileSinkOperator) output);\n    } else if ( queryState.getCommandType().equals(HiveOperation.CREATETABLE_AS_SELECT.getOperationName())) {\n\n      Path tlocation = null;\n      String tName = Utilities.getDbTableName(tableDesc.getTableName())[1];\n      try {\n        Warehouse wh = new Warehouse(conf);\n        tlocation = wh.getDefaultTablePath(db.getDatabase(tableDesc.getDatabaseName()), tName);\n      } catch (MetaException|HiveException e) {\n        throw new SemanticException(e);\n      }\n\n      SessionState.get().getLineageState()\n              .mapDirToOp(tlocation, (FileSinkOperator) output);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created FileSink Plan for clause: \" + dest + \"dest_path: \"\n          + dest_path + \" row schema: \" + inputRR.toString());\n    }\n\n    FileSinkOperator fso = (FileSinkOperator) output;\n    fso.getConf().setTable(dest_tab);\n    fsopToTable.put(fso, dest_tab);\n    // the following code is used to collect column stats when\n    // hive.stats.autogather=true\n    // and it is an insert overwrite or insert into table\n    if (dest_tab != null && conf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)\n        && conf.getBoolVar(ConfVars.HIVESTATSCOLAUTOGATHER)\n        && ColumnStatsAutoGatherContext.canRunAutogatherStats(fso)) {\n      if (dest_type.intValue() == QBMetaData.DEST_TABLE) {\n        genAutoColumnStatsGatheringPipeline(qb, table_desc, partSpec, input, qb.getParseInfo()\n            .isInsertIntoTable(dest_tab.getDbName(), dest_tab.getTableName()));\n      } else if (dest_type.intValue() == QBMetaData.DEST_PARTITION) {\n        genAutoColumnStatsGatheringPipeline(qb, table_desc, dest_part.getSpec(), input, qb\n            .getParseInfo().isInsertIntoTable(dest_tab.getDbName(), dest_tab.getTableName()));\n\n      }\n    }\n    return output;\n  }",
            "6760  \n6761  \n6762  \n6763  \n6764  \n6765  \n6766  \n6767  \n6768  \n6769  \n6770  \n6771  \n6772  \n6773  \n6774  \n6775  \n6776  \n6777  \n6778  \n6779  \n6780  \n6781  \n6782  \n6783  \n6784  \n6785  \n6786  \n6787  \n6788  \n6789  \n6790  \n6791  \n6792  \n6793  \n6794  \n6795  \n6796  \n6797  \n6798  \n6799  \n6800  \n6801  \n6802  \n6803  \n6804  \n6805  \n6806  \n6807  \n6808  \n6809  \n6810  \n6811  \n6812  \n6813  \n6814  \n6815  \n6816  \n6817  \n6818  \n6819  \n6820  \n6821  \n6822  \n6823  \n6824  \n6825  \n6826  \n6827  \n6828  \n6829  \n6830  \n6831  \n6832  \n6833  \n6834  \n6835  \n6836  \n6837  \n6838  \n6839  \n6840  \n6841  \n6842  \n6843  \n6844  \n6845  \n6846  \n6847  \n6848  \n6849  \n6850  \n6851  \n6852  \n6853  \n6854  \n6855  \n6856  \n6857  \n6858  \n6859  \n6860  \n6861  \n6862  \n6863  \n6864  \n6865  \n6866  \n6867  \n6868  \n6869  \n6870  \n6871  \n6872  \n6873  \n6874  \n6875  \n6876  \n6877  \n6878  \n6879  \n6880  \n6881  \n6882  \n6883  \n6884  \n6885  \n6886  \n6887  \n6888  \n6889  \n6890  \n6891  \n6892  \n6893  \n6894  \n6895  \n6896  \n6897  \n6898  \n6899  \n6900  \n6901  \n6902  \n6903  \n6904  \n6905  \n6906  \n6907  \n6908  \n6909  \n6910  \n6911  \n6912  \n6913  \n6914  \n6915  \n6916  \n6917  \n6918  \n6919  \n6920  \n6921  \n6922  \n6923  \n6924  \n6925  \n6926  \n6927  \n6928  \n6929  \n6930  \n6931  \n6932  \n6933  \n6934  \n6935  \n6936  \n6937  \n6938  \n6939  \n6940  \n6941  \n6942  \n6943  \n6944  \n6945  \n6946  \n6947  \n6948  \n6949  \n6950  \n6951  \n6952  \n6953  \n6954  \n6955  \n6956  \n6957  \n6958  \n6959  \n6960  \n6961  \n6962  \n6963  \n6964  \n6965  \n6966  \n6967  \n6968  \n6969  \n6970  \n6971  \n6972  \n6973  \n6974  \n6975  \n6976  \n6977  \n6978  \n6979  \n6980  \n6981  \n6982  \n6983  \n6984  \n6985  \n6986  \n6987  \n6988  \n6989  \n6990  \n6991  \n6992  \n6993  \n6994  \n6995  \n6996  \n6997  \n6998  \n6999  \n7000  \n7001  \n7002  \n7003  \n7004  \n7005  \n7006  \n7007  \n7008  \n7009  \n7010  \n7011  \n7012  \n7013  \n7014  \n7015  \n7016  \n7017  \n7018  \n7019  \n7020  \n7021  \n7022  \n7023  \n7024  \n7025  \n7026  \n7027  \n7028  \n7029  \n7030  \n7031  \n7032  \n7033  \n7034  \n7035  \n7036  \n7037  \n7038  \n7039  \n7040  \n7041  \n7042  \n7043  \n7044  \n7045  \n7046  \n7047  \n7048  \n7049  \n7050  \n7051  \n7052  \n7053  \n7054  \n7055  \n7056  \n7057  \n7058  \n7059  \n7060  \n7061  \n7062  \n7063  \n7064  \n7065  \n7066  \n7067  \n7068  \n7069  \n7070  \n7071  \n7072  \n7073  \n7074  \n7075  \n7076  \n7077  \n7078  \n7079  \n7080  \n7081  \n7082  \n7083  \n7084  \n7085  \n7086  \n7087  \n7088  \n7089  \n7090  \n7091  \n7092  \n7093  \n7094  \n7095  \n7096  \n7097  \n7098  \n7099  \n7100  \n7101  \n7102  \n7103  \n7104  \n7105  \n7106  \n7107  \n7108  \n7109  \n7110  \n7111  \n7112  \n7113  \n7114  \n7115  \n7116  \n7117  \n7118  \n7119  \n7120  \n7121  \n7122  \n7123  \n7124  \n7125  \n7126  \n7127  \n7128  \n7129  \n7130  \n7131  \n7132  \n7133  \n7134  \n7135  \n7136  \n7137  \n7138  \n7139  \n7140  \n7141  \n7142  \n7143  \n7144  \n7145  \n7146  \n7147  \n7148  \n7149  \n7150  \n7151  \n7152  \n7153  \n7154  \n7155  \n7156  \n7157  \n7158  \n7159  \n7160  \n7161  \n7162  \n7163  \n7164  \n7165  \n7166  \n7167  \n7168  \n7169  \n7170  \n7171  \n7172  \n7173  \n7174  \n7175  \n7176  \n7177  \n7178  \n7179  \n7180  \n7181  \n7182  \n7183  \n7184  \n7185  \n7186  \n7187  \n7188  \n7189  \n7190  \n7191  \n7192  \n7193  \n7194  \n7195  \n7196  \n7197  \n7198  \n7199  \n7200  \n7201  \n7202  \n7203  \n7204  \n7205  \n7206  \n7207  \n7208  \n7209  \n7210  \n7211  \n7212  \n7213  \n7214  \n7215  \n7216  \n7217  \n7218  \n7219  \n7220  \n7221  \n7222  \n7223  \n7224  \n7225  \n7226  \n7227  \n7228  \n7229  \n7230  \n7231  \n7232  \n7233  \n7234  \n7235  \n7236  \n7237  \n7238  \n7239  \n7240  \n7241  \n7242  \n7243  \n7244  \n7245  \n7246  \n7247  \n7248  \n7249  \n7250  \n7251  \n7252  \n7253  \n7254  \n7255  \n7256  \n7257  \n7258  \n7259  \n7260  \n7261  \n7262  \n7263  \n7264  \n7265  \n7266  \n7267  \n7268  \n7269  \n7270  \n7271  \n7272  \n7273 +\n7274  \n7275  \n7276  \n7277  \n7278  \n7279  \n7280  \n7281  \n7282  \n7283  \n7284  \n7285  \n7286 +\n7287  \n7288  \n7289  \n7290  \n7291  \n7292  \n7293  \n7294  \n7295  \n7296  \n7297  \n7298  \n7299  \n7300  \n7301  \n7302  \n7303  \n7304  \n7305  \n7306  \n7307  \n7308  \n7309  \n7310  \n7311  \n7312  \n7313  ",
            "  @SuppressWarnings(\"nls\")\n  protected Operator genFileSinkPlan(String dest, QB qb, Operator input)\n      throws SemanticException {\n\n    RowResolver inputRR = opParseCtx.get(input).getRowResolver();\n    QBMetaData qbm = qb.getMetaData();\n    Integer dest_type = qbm.getDestTypeForAlias(dest);\n\n    Table dest_tab = null; // destination table if any\n    boolean destTableIsAcid = false; // should the destination table be written to using ACID\n    boolean destTableIsTemporary = false;\n    boolean destTableIsMaterialization = false;\n    Partition dest_part = null;// destination partition if any\n    Path queryTmpdir = null; // the intermediate destination directory\n    Path dest_path = null; // the final destination directory\n    TableDesc table_desc = null;\n    int currentTableId = 0;\n    boolean isLocal = false;\n    SortBucketRSCtx rsCtx = new SortBucketRSCtx();\n    DynamicPartitionCtx dpCtx = null;\n    LoadTableDesc ltd = null;\n    ListBucketingCtx lbCtx = null;\n    Map<String, String> partSpec = null;\n\n    switch (dest_type.intValue()) {\n    case QBMetaData.DEST_TABLE: {\n\n      dest_tab = qbm.getDestTableForAlias(dest);\n      destTableIsAcid = AcidUtils.isAcidTable(dest_tab);\n      destTableIsTemporary = dest_tab.isTemporary();\n\n      // Is the user trying to insert into a external tables\n      if ((!conf.getBoolVar(HiveConf.ConfVars.HIVE_INSERT_INTO_EXTERNAL_TABLES)) &&\n          (dest_tab.getTableType().equals(TableType.EXTERNAL_TABLE))) {\n        throw new SemanticException(\n            ErrorMsg.INSERT_EXTERNAL_TABLE.getMsg(dest_tab.getTableName()));\n      }\n\n      partSpec = qbm.getPartSpecForAlias(dest);\n      dest_path = dest_tab.getPath();\n\n      // If the query here is an INSERT_INTO and the target is an immutable table,\n      // verify that our destination is empty before proceeding\n      if (dest_tab.isImmutable() &&\n          qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),dest_tab.getTableName())){\n        try {\n          FileSystem fs = dest_path.getFileSystem(conf);\n          if (! MetaStoreUtils.isDirEmpty(fs,dest_path)){\n            LOG.warn(\"Attempted write into an immutable table : \"\n                + dest_tab.getTableName() + \" : \" + dest_path);\n            throw new SemanticException(\n                ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(dest_tab.getTableName()));\n          }\n        } catch (IOException ioe) {\n            LOG.warn(\"Error while trying to determine if immutable table has any data : \"\n                + dest_tab.getTableName() + \" : \" + dest_path);\n          throw new SemanticException(ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(ioe.getMessage()));\n        }\n      }\n\n      // check for partition\n      List<FieldSchema> parts = dest_tab.getPartitionKeys();\n      if (parts != null && parts.size() > 0) { // table is partitioned\n        if (partSpec == null || partSpec.size() == 0) { // user did NOT specify partition\n          throw new SemanticException(generateErrorMessage(\n              qb.getParseInfo().getDestForClause(dest),\n              ErrorMsg.NEED_PARTITION_ERROR.getMsg()));\n        }\n        dpCtx = qbm.getDPCtx(dest);\n        if (dpCtx == null) {\n          dest_tab.validatePartColumnNames(partSpec, false);\n          dpCtx = new DynamicPartitionCtx(dest_tab, partSpec,\n              conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME),\n              conf.getIntVar(HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTSPERNODE));\n          qbm.setDPCtx(dest, dpCtx);\n        }\n\n        if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING)) { // allow DP\n          throw new SemanticException(generateErrorMessage(\n              qb.getParseInfo().getDestForClause(dest),\n              ErrorMsg.DYNAMIC_PARTITION_DISABLED.getMsg()));\n        }\n        if (dpCtx.getSPPath() != null) {\n          dest_path = new Path(dest_tab.getPath(), dpCtx.getSPPath());\n        }\n        if ((dest_tab.getNumBuckets() > 0)) {\n          dpCtx.setNumBuckets(dest_tab.getNumBuckets());\n        }\n      }\n\n      boolean isNonNativeTable = dest_tab.isNonNative();\n      if (isNonNativeTable) {\n        queryTmpdir = dest_path;\n      } else {\n        queryTmpdir = ctx.getTempDirForPath(dest_path, true);\n      }\n      if (dpCtx != null) {\n        // set the root of the temporary path where dynamic partition columns will populate\n        dpCtx.setRootPath(queryTmpdir);\n      }\n      // this table_desc does not contain the partitioning columns\n      table_desc = Utilities.getTableDesc(dest_tab);\n\n      // Add sorting/bucketing if needed\n      input = genBucketingSortingDest(dest, input, qb, table_desc, dest_tab, rsCtx);\n\n      idToTableNameMap.put(String.valueOf(destTableId), dest_tab.getTableName());\n      currentTableId = destTableId;\n      destTableId++;\n\n      lbCtx = constructListBucketingCtx(dest_tab.getSkewedColNames(),\n          dest_tab.getSkewedColValues(), dest_tab.getSkewedColValueLocationMaps(),\n          dest_tab.isStoredAsSubDirectories(), conf);\n\n      // Create the work for moving the table\n      // NOTE: specify Dynamic partitions in dest_tab for WriteEntity\n      if (!isNonNativeTable) {\n        AcidUtils.Operation acidOp = AcidUtils.Operation.NOT_ACID;\n        if (destTableIsAcid) {\n          acidOp = getAcidType(table_desc.getOutputFileFormatClass(), dest);\n          checkAcidConstraints(qb, table_desc, dest_tab);\n        }\n        ltd = new LoadTableDesc(queryTmpdir, table_desc, dpCtx, acidOp);\n        ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n            dest_tab.getTableName()));\n        ltd.setLbCtx(lbCtx);\n        loadTableWork.add(ltd);\n      } else {\n        // This is a non-native table.\n        // We need to set stats as inaccurate.\n        setStatsForNonNativeTable(dest_tab);\n        // true if it is insert overwrite.\n        boolean overwrite = !qb.getParseInfo().isInsertIntoTable(\n                String.format(\"%s.%s\", dest_tab.getDbName(), dest_tab.getTableName()));\n        createInsertDesc(dest_tab, overwrite);\n      }\n\n      WriteEntity output = null;\n\n      // Here only register the whole table for post-exec hook if no DP present\n      // in the case of DP, we will register WriteEntity in MoveTask when the\n      // list of dynamically created partitions are known.\n      if ((dpCtx == null || dpCtx.getNumDPCols() == 0)) {\n        output = new WriteEntity(dest_tab,  determineWriteType(ltd, isNonNativeTable, dest));\n        if (!outputs.add(output)) {\n          throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n              .getMsg(dest_tab.getTableName()));\n        }\n      }\n      if ((dpCtx != null) && (dpCtx.getNumDPCols() >= 0)) {\n        // No static partition specified\n        if (dpCtx.getNumSPCols() == 0) {\n          output = new WriteEntity(dest_tab, determineWriteType(ltd, isNonNativeTable, dest), false);\n          outputs.add(output);\n          output.setDynamicPartitionWrite(true);\n        }\n        // part of the partition specified\n        // Create a DummyPartition in this case. Since, the metastore does not store partial\n        // partitions currently, we need to store dummy partitions\n        else {\n          try {\n            String ppath = dpCtx.getSPPath();\n            ppath = ppath.substring(0, ppath.length() - 1);\n            DummyPartition p =\n                new DummyPartition(dest_tab, dest_tab.getDbName()\n                    + \"@\" + dest_tab.getTableName() + \"@\" + ppath,\n                    partSpec);\n            output = new WriteEntity(p, getWriteType(dest), false);\n            output.setDynamicPartitionWrite(true);\n            outputs.add(output);\n          } catch (HiveException e) {\n            throw new SemanticException(e.getMessage(), e);\n          }\n        }\n      }\n\n      ctx.getLoadTableOutputMap().put(ltd, output);\n      break;\n    }\n    case QBMetaData.DEST_PARTITION: {\n\n      dest_part = qbm.getDestPartitionForAlias(dest);\n      dest_tab = dest_part.getTable();\n      destTableIsAcid = AcidUtils.isAcidTable(dest_tab);\n      if ((!conf.getBoolVar(HiveConf.ConfVars.HIVE_INSERT_INTO_EXTERNAL_TABLES)) &&\n          dest_tab.getTableType().equals(TableType.EXTERNAL_TABLE)) {\n        throw new SemanticException(\n            ErrorMsg.INSERT_EXTERNAL_TABLE.getMsg(dest_tab.getTableName()));\n      }\n\n      Path tabPath = dest_tab.getPath();\n      Path partPath = dest_part.getDataLocation();\n\n      // If the query here is an INSERT_INTO and the target is an immutable table,\n      // verify that our destination is empty before proceeding\n      if (dest_tab.isImmutable() &&\n          qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),dest_tab.getTableName())){\n        try {\n          FileSystem fs = partPath.getFileSystem(conf);\n          if (! MetaStoreUtils.isDirEmpty(fs,partPath)){\n            LOG.warn(\"Attempted write into an immutable table partition : \"\n                + dest_tab.getTableName() + \" : \" + partPath);\n            throw new SemanticException(\n                ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(dest_tab.getTableName()));\n          }\n        } catch (IOException ioe) {\n            LOG.warn(\"Error while trying to determine if immutable table partition has any data : \"\n                + dest_tab.getTableName() + \" : \" + partPath);\n          throw new SemanticException(ErrorMsg.INSERT_INTO_IMMUTABLE_TABLE.getMsg(ioe.getMessage()));\n        }\n      }\n\n      // if the table is in a different dfs than the partition,\n      // replace the partition's dfs with the table's dfs.\n      dest_path = new Path(tabPath.toUri().getScheme(), tabPath.toUri()\n          .getAuthority(), partPath.toUri().getPath());\n\n      queryTmpdir = ctx.getTempDirForPath(dest_path, true);\n      table_desc = Utilities.getTableDesc(dest_tab);\n\n      // Add sorting/bucketing if needed\n      input = genBucketingSortingDest(dest, input, qb, table_desc, dest_tab, rsCtx);\n\n      idToTableNameMap.put(String.valueOf(destTableId), dest_tab.getTableName());\n      currentTableId = destTableId;\n      destTableId++;\n\n      lbCtx = constructListBucketingCtx(dest_part.getSkewedColNames(),\n          dest_part.getSkewedColValues(), dest_part.getSkewedColValueLocationMaps(),\n          dest_part.isStoredAsSubDirectories(), conf);\n      AcidUtils.Operation acidOp = AcidUtils.Operation.NOT_ACID;\n      if (destTableIsAcid) {\n        acidOp = getAcidType(table_desc.getOutputFileFormatClass(), dest);\n        checkAcidConstraints(qb, table_desc, dest_tab);\n      }\n      ltd = new LoadTableDesc(queryTmpdir, table_desc, dest_part.getSpec(), acidOp);\n      ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n          dest_tab.getTableName()));\n      ltd.setLbCtx(lbCtx);\n\n      loadTableWork.add(ltd);\n      if (!outputs.add(new WriteEntity(dest_part,\n        determineWriteType(ltd, dest_tab.isNonNative(), dest)))) {\n\n        throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n            .getMsg(dest_tab.getTableName() + \"@\" + dest_part.getName()));\n      }\n      break;\n    }\n    case QBMetaData.DEST_LOCAL_FILE:\n      isLocal = true;\n      // fall through\n    case QBMetaData.DEST_DFS_FILE: {\n      dest_path = new Path(qbm.getDestFileForAlias(dest));\n\n      if (isLocal) {\n        // for local directory - we always write to map-red intermediate\n        // store and then copy to local fs\n        queryTmpdir = ctx.getMRTmpPath();\n      } else {\n        // otherwise write to the file system implied by the directory\n        // no copy is required. we may want to revisit this policy in future\n\n        try {\n          Path qPath = FileUtils.makeQualified(dest_path, conf);\n          queryTmpdir = ctx.getTempDirForPath(qPath, true);\n        } catch (Exception e) {\n          throw new SemanticException(\"Error creating temporary folder on: \"\n              + dest_path, e);\n        }\n      }\n      String cols = \"\";\n      String colTypes = \"\";\n      ArrayList<ColumnInfo> colInfos = inputRR.getColumnInfos();\n\n      // CTAS case: the file output format and serde are defined by the create\n      // table command rather than taking the default value\n      List<FieldSchema> field_schemas = null;\n      CreateTableDesc tblDesc = qb.getTableDesc();\n      CreateViewDesc viewDesc = qb.getViewDesc();\n      if (tblDesc != null) {\n        field_schemas = new ArrayList<FieldSchema>();\n        destTableIsTemporary = tblDesc.isTemporary();\n        destTableIsMaterialization = tblDesc.isMaterialization();\n      } else if (viewDesc != null) {\n        field_schemas = new ArrayList<FieldSchema>();\n        destTableIsTemporary = false;\n      }\n\n      boolean first = true;\n      for (ColumnInfo colInfo : colInfos) {\n        String[] nm = inputRR.reverseLookup(colInfo.getInternalName());\n\n        if (nm[1] != null) { // non-null column alias\n          colInfo.setAlias(nm[1]);\n        }\n\n        String colName = colInfo.getInternalName();  //default column name\n        if (field_schemas != null) {\n          FieldSchema col = new FieldSchema();\n          if (!(\"\".equals(nm[0])) && nm[1] != null) {\n            colName = unescapeIdentifier(colInfo.getAlias()).toLowerCase(); // remove ``\n          }\n          colName = fixCtasColumnName(colName);\n          col.setName(colName);\n          String typeName = colInfo.getType().getTypeName();\n          // CTAS should NOT create a VOID type\n          if (typeName.equals(serdeConstants.VOID_TYPE_NAME)) {\n              throw new SemanticException(ErrorMsg.CTAS_CREATES_VOID_TYPE\n              .getMsg(colName));\n          }\n          col.setType(typeName);\n          field_schemas.add(col);\n        }\n\n        if (!first) {\n          cols = cols.concat(\",\");\n          colTypes = colTypes.concat(\":\");\n        }\n\n        first = false;\n        cols = cols.concat(colName);\n\n        // Replace VOID type with string when the output is a temp table or\n        // local files.\n        // A VOID type can be generated under the query:\n        //\n        // select NULL from tt;\n        // or\n        // insert overwrite local directory \"abc\" select NULL from tt;\n        //\n        // where there is no column type to which the NULL value should be\n        // converted.\n        //\n        String tName = colInfo.getType().getTypeName();\n        if (tName.equals(serdeConstants.VOID_TYPE_NAME)) {\n          colTypes = colTypes.concat(serdeConstants.STRING_TYPE_NAME);\n        } else {\n          colTypes = colTypes.concat(tName);\n        }\n      }\n\n      // update the create table descriptor with the resulting schema.\n      if (tblDesc != null) {\n        tblDesc.setCols(new ArrayList<FieldSchema>(field_schemas));\n      } else if (viewDesc != null) {\n        viewDesc.setSchema(new ArrayList<FieldSchema>(field_schemas));\n      }\n\n      boolean isDestTempFile = true;\n      if (!ctx.isMRTmpFileURI(dest_path.toUri().toString())) {\n        idToTableNameMap.put(String.valueOf(destTableId), dest_path.toUri().toString());\n        currentTableId = destTableId;\n        destTableId++;\n        isDestTempFile = false;\n      }\n\n      boolean isDfsDir = (dest_type.intValue() == QBMetaData.DEST_DFS_FILE);\n      loadFileWork.add(new LoadFileDesc(tblDesc, viewDesc, queryTmpdir, dest_path, isDfsDir, cols,\n          colTypes));\n\n      if (tblDesc == null) {\n        if (viewDesc != null) {\n          table_desc = PlanUtils.getTableDesc(viewDesc, cols, colTypes);\n        } else if (qb.getIsQuery()) {\n          String fileFormat;\n          if (SessionState.get().getIsUsingThriftJDBCBinarySerDe()) {\n              fileFormat = \"SequenceFile\";\n              HiveConf.setVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT, fileFormat);\n              table_desc=\n                         PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, fileFormat,\n                           ThriftJDBCBinarySerDe.class);\n              // Set the fetch formatter to be a no-op for the ListSinkOperator, since we'll\n              // write out formatted thrift objects to SequenceFile\n              conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, NoOpFetchFormatter.class.getName());\n          } else {\n              fileFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT);\n              table_desc =\n                         PlanUtils.getDefaultQueryOutputTableDesc(cols, colTypes, fileFormat,\n                           LazySimpleSerDe.class);\n          }\n        } else {\n          table_desc = PlanUtils.getDefaultTableDesc(qb.getDirectoryDesc(), cols, colTypes);\n        }\n      } else {\n        table_desc = PlanUtils.getTableDesc(tblDesc, cols, colTypes);\n      }\n\n      if (!outputs.add(new WriteEntity(dest_path, !isDfsDir, isDestTempFile))) {\n        throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES\n            .getMsg(dest_path.toUri().toString()));\n      }\n      break;\n    }\n    default:\n      throw new SemanticException(\"Unknown destination type: \" + dest_type);\n    }\n\n    if (!(dest_type.intValue() == QBMetaData.DEST_DFS_FILE && qb.getIsQuery())) {\n      input = genConversionSelectOperator(dest, qb, input, table_desc, dpCtx);\n    }\n\n    inputRR = opParseCtx.get(input).getRowResolver();\n\n    ArrayList<ColumnInfo> vecCol = new ArrayList<ColumnInfo>();\n\n    if (updating(dest) || deleting(dest)) {\n      vecCol.add(new ColumnInfo(VirtualColumn.ROWID.getName(), VirtualColumn.ROWID.getTypeInfo(),\n          \"\", true));\n    } else {\n      try {\n        StructObjectInspector rowObjectInspector = (StructObjectInspector) table_desc\n            .getDeserializer(conf).getObjectInspector();\n        List<? extends StructField> fields = rowObjectInspector\n            .getAllStructFieldRefs();\n        for (int i = 0; i < fields.size(); i++) {\n          vecCol.add(new ColumnInfo(fields.get(i).getFieldName(), TypeInfoUtils\n              .getTypeInfoFromObjectInspector(fields.get(i)\n                  .getFieldObjectInspector()), \"\", false));\n        }\n      } catch (Exception e) {\n        throw new SemanticException(e.getMessage(), e);\n      }\n    }\n\n    RowSchema fsRS = new RowSchema(vecCol);\n\n    // The output files of a FileSink can be merged if they are either not being written to a table\n    // or are being written to a table which is not bucketed\n    // and table the table is not sorted\n    boolean canBeMerged = (dest_tab == null || !((dest_tab.getNumBuckets() > 0) ||\n        (dest_tab.getSortCols() != null && dest_tab.getSortCols().size() > 0)));\n\n    // If this table is working with ACID semantics, turn off merging\n    canBeMerged &= !destTableIsAcid;\n\n    // Generate the partition columns from the parent input\n    if (dest_type.intValue() == QBMetaData.DEST_TABLE\n        || dest_type.intValue() == QBMetaData.DEST_PARTITION) {\n      genPartnCols(dest, input, qb, table_desc, dest_tab, rsCtx);\n    }\n\n    FileSinkDesc fileSinkDesc = new FileSinkDesc(\n      queryTmpdir,\n      table_desc,\n      conf.getBoolVar(HiveConf.ConfVars.COMPRESSRESULT),\n      currentTableId,\n      rsCtx.isMultiFileSpray(),\n      canBeMerged,\n      rsCtx.getNumFiles(),\n      rsCtx.getTotalFiles(),\n      rsCtx.getPartnCols(),\n      dpCtx,\n      dest_path);\n\n    boolean isHiveServerQuery = SessionState.get().isHiveServerQuery();\n    fileSinkDesc.setHiveServerQuery(isHiveServerQuery);\n    // If this is an insert, update, or delete on an ACID table then mark that so the\n    // FileSinkOperator knows how to properly write to it.\n    if (destTableIsAcid) {\n      AcidUtils.Operation wt = updating(dest) ? AcidUtils.Operation.UPDATE :\n          (deleting(dest) ? AcidUtils.Operation.DELETE : AcidUtils.Operation.INSERT);\n      fileSinkDesc.setWriteType(wt);\n      acidFileSinks.add(fileSinkDesc);\n    }\n\n    fileSinkDesc.setTemporary(destTableIsTemporary);\n    fileSinkDesc.setMaterialization(destTableIsMaterialization);\n\n    /* Set List Bucketing context. */\n    if (lbCtx != null) {\n      lbCtx.processRowSkewedIndex(fsRS);\n      lbCtx.calculateSkewedValueSubDirList();\n    }\n    fileSinkDesc.setLbCtx(lbCtx);\n\n    // set the stats publishing/aggregating key prefix\n    // the same as directory name. The directory name\n    // can be changed in the optimizer but the key should not be changed\n    // it should be the same as the MoveWork's sourceDir.\n    fileSinkDesc.setStatsAggPrefix(fileSinkDesc.getDirName().toString());\n    if (!destTableIsMaterialization &&\n            HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {\n      String statsTmpLoc = ctx.getTempDirForPath(dest_path).toString();\n      fileSinkDesc.setStatsTmpDir(statsTmpLoc);\n      LOG.debug(\"Set stats collection dir : \" + statsTmpLoc);\n    }\n\n    if (dest_part != null) {\n      try {\n        String staticSpec = Warehouse.makePartPath(dest_part.getSpec());\n        fileSinkDesc.setStaticSpec(staticSpec);\n      } catch (MetaException e) {\n        throw new SemanticException(e);\n      }\n    } else if (dpCtx != null) {\n      fileSinkDesc.setStaticSpec(dpCtx.getSPPath());\n    }\n\n    if (isHiveServerQuery &&\n      null != table_desc &&\n      table_desc.getSerdeClassName().equalsIgnoreCase(ThriftJDBCBinarySerDe.class.getName()) &&\n      HiveConf.getBoolVar(conf,HiveConf.ConfVars.HIVE_SERVER2_THRIFT_RESULTSET_SERIALIZE_IN_TASKS)) {\n        fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(true);\n    } else {\n        fileSinkDesc.setIsUsingThriftJDBCBinarySerDe(false);\n    }\n\n    Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(\n        fileSinkDesc, fsRS, input), inputRR);\n\n    if (ltd != null && SessionState.get() != null) {\n      SessionState.get().getLineageState()\n          .mapDirToOp(ltd.getSourcePath(), output);\n    } else if ( queryState.getCommandType().equals(HiveOperation.CREATETABLE_AS_SELECT.getOperationName())) {\n\n      Path tlocation = null;\n      String tName = Utilities.getDbTableName(tableDesc.getTableName())[1];\n      try {\n        Warehouse wh = new Warehouse(conf);\n        tlocation = wh.getDefaultTablePath(db.getDatabase(tableDesc.getDatabaseName()), tName);\n      } catch (MetaException|HiveException e) {\n        throw new SemanticException(e);\n      }\n\n      SessionState.get().getLineageState()\n              .mapDirToOp(tlocation, output);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Created FileSink Plan for clause: \" + dest + \"dest_path: \"\n          + dest_path + \" row schema: \" + inputRR.toString());\n    }\n\n    FileSinkOperator fso = (FileSinkOperator) output;\n    fso.getConf().setTable(dest_tab);\n    fsopToTable.put(fso, dest_tab);\n    // the following code is used to collect column stats when\n    // hive.stats.autogather=true\n    // and it is an insert overwrite or insert into table\n    if (dest_tab != null && conf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)\n        && conf.getBoolVar(ConfVars.HIVESTATSCOLAUTOGATHER)\n        && ColumnStatsAutoGatherContext.canRunAutogatherStats(fso)) {\n      if (dest_type.intValue() == QBMetaData.DEST_TABLE) {\n        genAutoColumnStatsGatheringPipeline(qb, table_desc, partSpec, input, qb.getParseInfo()\n            .isInsertIntoTable(dest_tab.getDbName(), dest_tab.getTableName()));\n      } else if (dest_type.intValue() == QBMetaData.DEST_PARTITION) {\n        genAutoColumnStatsGatheringPipeline(qb, table_desc, dest_part.getSpec(), input, qb\n            .getParseInfo().isInsertIntoTable(dest_tab.getDbName(), dest_tab.getTableName()));\n\n      }\n    }\n    return output;\n  }"
        ],
        [
            "SemanticAnalyzer::doPhase1(ASTNode,QB,Phase1Ctx,PlannerContext)",
            "1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452 -\n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  ",
            "  /**\n   * Phase 1: (including, but not limited to):\n   *\n   * 1. Gets all the aliases for all the tables / subqueries and makes the\n   * appropriate mapping in aliasToTabs, aliasToSubq 2. Gets the location of the\n   * destination and names the clause \"inclause\" + i 3. Creates a map from a\n   * string representation of an aggregation tree to the actual aggregation AST\n   * 4. Creates a mapping from the clause name to the select expression AST in\n   * destToSelExpr 5. Creates a mapping from a table alias to the lateral view\n   * AST's in aliasToLateralViews\n   *\n   * @param ast\n   * @param qb\n   * @param ctx_1\n   * @throws SemanticException\n   */\n  @SuppressWarnings({\"fallthrough\", \"nls\"})\n  public boolean doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1, PlannerContext plannerCtx)\n      throws SemanticException {\n\n    boolean phase1Result = true;\n    QBParseInfo qbp = qb.getParseInfo();\n    boolean skipRecursion = false;\n\n    if (ast.getToken() != null) {\n      skipRecursion = true;\n      switch (ast.getToken().getType()) {\n      case HiveParser.TOK_SELECTDI:\n        qb.countSelDi();\n        // fall through\n      case HiveParser.TOK_SELECT:\n        qb.countSel();\n        qbp.setSelExprForClause(ctx_1.dest, ast);\n\n        int posn = 0;\n        if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.QUERY_HINT) {\n          ParseDriver pd = new ParseDriver();\n          String queryHintStr = ast.getChild(0).getText();\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"QUERY HINT: \"+queryHintStr);\n          }\n          try {\n            ASTNode hintNode = pd.parseHint(queryHintStr);\n            qbp.setHints((ASTNode) hintNode);\n            posn++;\n          } catch (ParseException e) {\n            throw new SemanticException(\"failed to parse query hint: \"+e.getMessage(), e);\n          }\n        }\n\n        if ((ast.getChild(posn).getChild(0).getType() == HiveParser.TOK_TRANSFORM))\n          queryProperties.setUsesScript(true);\n\n        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast,\n            qb, ctx_1.dest);\n        doPhase1GetColumnAliasesFromSelect(ast, qbp);\n        qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n        qbp.setDistinctFuncExprsForClause(ctx_1.dest,\n          doPhase1GetDistinctFuncExprs(aggregations));\n        break;\n\n      case HiveParser.TOK_WHERE:\n        qbp.setWhrExprForClause(ctx_1.dest, ast);\n        if (!SubQueryUtils.findSubQueries((ASTNode) ast.getChild(0)).isEmpty())\n            queryProperties.setFilterWithSubQuery(true);\n        break;\n\n      case HiveParser.TOK_INSERT_INTO:\n        String currentDatabase = SessionState.get().getCurrentDatabase();\n        String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);\n        qbp.addInsertIntoTable(tab_name, ast);\n\n      case HiveParser.TOK_DESTINATION:\n        ctx_1.dest = this.ctx.getDestNamePrefix(ast).toString() + ctx_1.nextNum;\n        ctx_1.nextNum++;\n        boolean isTmpFileDest = false;\n        if (ast.getChildCount() > 0 && ast.getChild(0) instanceof ASTNode) {\n          ASTNode ch = (ASTNode) ast.getChild(0);\n          if (ch.getToken().getType() == HiveParser.TOK_DIR && ch.getChildCount() > 0\n              && ch.getChild(0) instanceof ASTNode) {\n            ch = (ASTNode) ch.getChild(0);\n            isTmpFileDest = ch.getToken().getType() == HiveParser.TOK_TMP_FILE;\n          } else {\n            if (ast.getToken().getType() == HiveParser.TOK_DESTINATION\n                && ast.getChild(0).getType() == HiveParser.TOK_TAB) {\n              String fullTableName = getUnescapedName((ASTNode) ast.getChild(0).getChild(0),\n                  SessionState.get().getCurrentDatabase());\n              qbp.getInsertOverwriteTables().put(fullTableName, ast);\n            }\n          }\n        }\n\n        // is there a insert in the subquery\n        if (qbp.getIsSubQ() && !isTmpFileDest) {\n          throw new SemanticException(ErrorMsg.NO_INSERT_INSUBQUERY.getMsg(ast));\n        }\n\n        qbp.setDestForClause(ctx_1.dest, (ASTNode) ast.getChild(0));\n        handleInsertStatementSpecPhase1(ast, qbp, ctx_1);\n\n        if (qbp.getClauseNamesForDest().size() == 2) {\n          // From the moment that we have two destination clauses,\n          // we know that this is a multi-insert query.\n          // Thus, set property to right value.\n          // Using qbp.getClauseNamesForDest().size() >= 2 would be\n          // equivalent, but we use == to avoid setting the property\n          // multiple times\n          queryProperties.setMultiDestQuery(true);\n        }\n\n        if (plannerCtx != null && !queryProperties.hasMultiDestQuery()) {\n          plannerCtx.setInsertToken(ast, isTmpFileDest);\n        } else if (plannerCtx != null && qbp.getClauseNamesForDest().size() == 2) {\n          // For multi-insert query, currently we only optimize the FROM clause.\n          // Hence, introduce multi-insert token on top of it.\n          // However, first we need to reset existing token (insert).\n          // Using qbp.getClauseNamesForDest().size() >= 2 would be\n          // equivalent, but we use == to avoid setting the property\n          // multiple times\n          plannerCtx.resetToken();\n          plannerCtx.setMultiInsertToken((ASTNode) qbp.getQueryFrom().getChild(0));\n        }\n        break;\n\n      case HiveParser.TOK_FROM:\n        int child_count = ast.getChildCount();\n        if (child_count != 1) {\n          throw new SemanticException(generateErrorMessage(ast,\n              \"Multiple Children \" + child_count));\n        }\n\n        if (!qbp.getIsSubQ()) {\n          qbp.setQueryFromExpr(ast);\n        }\n\n        // Check if this is a subquery / lateral view\n        ASTNode frm = (ASTNode) ast.getChild(0);\n        if (frm.getToken().getType() == HiveParser.TOK_TABREF) {\n          processTable(qb, frm);\n        } else if (frm.getToken().getType() == HiveParser.TOK_VIRTUAL_TABLE) {\n          // Create a temp table with the passed values in it then rewrite this portion of the\n          // tree to be from that table.\n          ASTNode newFrom = genValuesTempTable(frm, qb);\n          ast.setChild(0, newFrom);\n          processTable(qb, newFrom);\n        } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {\n          processSubQuery(qb, frm);\n        } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||\n            frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {\n          queryProperties.setHasLateralViews(true);\n          processLateralView(qb, frm);\n        } else if (isJoinToken(frm)) {\n          processJoin(qb, frm);\n          qbp.setJoinExpr(frm);\n        }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){\n          queryProperties.setHasPTF(true);\n          processPTF(qb, frm);\n        }\n        break;\n\n      case HiveParser.TOK_CLUSTERBY:\n        // Get the clusterby aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasClusterBy(true);\n        qbp.setClusterByExprForClause(ctx_1.dest, ast);\n        break;\n\n      case HiveParser.TOK_DISTRIBUTEBY:\n        // Get the distribute by aliases - these are aliased to the entries in\n        // the\n        // select list\n        queryProperties.setHasDistributeBy(true);\n        qbp.setDistributeByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.ORDERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n        }\n        break;\n\n      case HiveParser.TOK_SORTBY:\n     // Get the sort by aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasSortBy(true);\n        qbp.setSortByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));\n        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));\n        }\n\n        break;\n\n      case HiveParser.TOK_ORDERBY:\n        // Get the order by aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasOrderBy(true);\n        qbp.setOrderByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));\n        }\n        break;\n\n      case HiveParser.TOK_GROUPBY:\n      case HiveParser.TOK_ROLLUP_GROUPBY:\n      case HiveParser.TOK_CUBE_GROUPBY:\n      case HiveParser.TOK_GROUPING_SETS:\n        // Get the groupby aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasGroupBy(true);\n        if (qbp.getJoinExpr() != null) {\n          queryProperties.setHasJoinFollowedByGroupBy(true);\n        }\n        if (qbp.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));\n        }\n        qbp.setGroupByExprForClause(ctx_1.dest, ast);\n        skipRecursion = true;\n\n        // Rollup and Cubes are syntactic sugar on top of grouping sets\n        if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {\n          qbp.getDestRollups().add(ctx_1.dest);\n        } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {\n          qbp.getDestCubes().add(ctx_1.dest);\n        } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {\n          qbp.getDestGroupingSets().add(ctx_1.dest);\n        }\n        break;\n\n      case HiveParser.TOK_HAVING:\n        qbp.setHavingExprForClause(ctx_1.dest, ast);\n        qbp.addAggregationExprsForClause(ctx_1.dest,\n            doPhase1GetAggregationsFromSelect(ast, qb, ctx_1.dest));\n        break;\n\n      case HiveParser.KW_WINDOW:\n        if (!qb.hasWindowingSpec(ctx_1.dest) ) {\n          throw new SemanticException(generateErrorMessage(ast,\n              \"Query has no Cluster/Distribute By; but has a Window definition\"));\n        }\n        handleQueryWindowClauses(qb, ctx_1, ast);\n        break;\n\n      case HiveParser.TOK_LIMIT:\n        if (ast.getChildCount() == 2) {\n          qbp.setDestLimit(ctx_1.dest,\n              new Integer(ast.getChild(0).getText()),\n              new Integer(ast.getChild(1).getText()));\n        } else {\n          qbp.setDestLimit(ctx_1.dest, new Integer(0),\n              new Integer(ast.getChild(0).getText()));\n        }\n        break;\n\n      case HiveParser.TOK_ANALYZE:\n        // Case of analyze command\n\n        String table_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0)).toLowerCase();\n\n\n        qb.setTabAlias(table_name, table_name);\n        qb.addAlias(table_name);\n        qb.getParseInfo().setIsAnalyzeCommand(true);\n        qb.getParseInfo().setNoScanAnalyzeCommand(this.noscan);\n        qb.getParseInfo().setPartialScanAnalyzeCommand(this.partialscan);\n        // Allow analyze the whole table and dynamic partitions\n        HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n        HiveConf.setVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE, \"nonstrict\");\n\n        break;\n\n      case HiveParser.TOK_UNIONALL:\n        if (!qbp.getIsSubQ()) {\n          // this shouldn't happen. The parser should have converted the union to be\n          // contained in a subquery. Just in case, we keep the error as a fallback.\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.UNION_NOTIN_SUBQ.getMsg()));\n        }\n        skipRecursion = false;\n        break;\n\n      case HiveParser.TOK_INSERT:\n        ASTNode destination = (ASTNode) ast.getChild(0);\n        Tree tab = destination.getChild(0);\n\n        // Proceed if AST contains partition & If Not Exists\n        if (destination.getChildCount() == 2 &&\n            tab.getChildCount() == 2 &&\n            destination.getChild(1).getType() == HiveParser.TOK_IFNOTEXISTS) {\n          String tableName = tab.getChild(0).getChild(0).getText();\n\n          Tree partitions = tab.getChild(1);\n          int childCount = partitions.getChildCount();\n          HashMap<String, String> partition = new HashMap<String, String>();\n          for (int i = 0; i < childCount; i++) {\n            String partitionName = partitions.getChild(i).getChild(0).getText();\n            Tree pvalue = partitions.getChild(i).getChild(1);\n            if (pvalue == null) {\n              break;\n            }\n            String partitionVal = stripQuotes(pvalue.getText());\n            partition.put(partitionName, partitionVal);\n          }\n          // if it is a dynamic partition throw the exception\n          if (childCount != partition.size()) {\n            throw new SemanticException(ErrorMsg.INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS\n                .getMsg(partition.toString()));\n          }\n          Table table = null;\n          try {\n            table = this.getTableObjectByName(tableName);\n          } catch (HiveException ex) {\n            throw new SemanticException(ex);\n          }\n          try {\n            Partition parMetaData = db.getPartition(table, partition, false);\n            // Check partition exists if it exists skip the overwrite\n            if (parMetaData != null) {\n              phase1Result = false;\n              skipRecursion = true;\n              LOG.info(\"Partition already exists so insert into overwrite \" +\n                  \"skipped for partition : \" + parMetaData.toString());\n              break;\n            }\n          } catch (HiveException e) {\n            LOG.info(\"Error while getting metadata : \", e);\n          }\n          validatePartSpec(table, partition, (ASTNode)tab, conf, false);\n        }\n        skipRecursion = false;\n        break;\n      case HiveParser.TOK_LATERAL_VIEW:\n      case HiveParser.TOK_LATERAL_VIEW_OUTER:\n        // todo: nested LV\n        assert ast.getChildCount() == 1;\n        qb.getParseInfo().getDestToLateralView().put(ctx_1.dest, ast);\n        break;\n      case HiveParser.TOK_CTE:\n        processCTE(qb, ast);\n        break;\n      default:\n        skipRecursion = false;\n        break;\n      }\n    }\n\n    if (!skipRecursion) {\n      // Iterate over the rest of the children\n      int child_count = ast.getChildCount();\n      for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {\n        // Recurse\n        phase1Result = phase1Result && doPhase1(\n            (ASTNode)ast.getChild(child_pos), qb, ctx_1, plannerCtx);\n      }\n    }\n    return phase1Result;\n  }",
            "1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452 +\n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  ",
            "  /**\n   * Phase 1: (including, but not limited to):\n   *\n   * 1. Gets all the aliases for all the tables / subqueries and makes the\n   * appropriate mapping in aliasToTabs, aliasToSubq 2. Gets the location of the\n   * destination and names the clause \"inclause\" + i 3. Creates a map from a\n   * string representation of an aggregation tree to the actual aggregation AST\n   * 4. Creates a mapping from the clause name to the select expression AST in\n   * destToSelExpr 5. Creates a mapping from a table alias to the lateral view\n   * AST's in aliasToLateralViews\n   *\n   * @param ast\n   * @param qb\n   * @param ctx_1\n   * @throws SemanticException\n   */\n  @SuppressWarnings({\"fallthrough\", \"nls\"})\n  public boolean doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1, PlannerContext plannerCtx)\n      throws SemanticException {\n\n    boolean phase1Result = true;\n    QBParseInfo qbp = qb.getParseInfo();\n    boolean skipRecursion = false;\n\n    if (ast.getToken() != null) {\n      skipRecursion = true;\n      switch (ast.getToken().getType()) {\n      case HiveParser.TOK_SELECTDI:\n        qb.countSelDi();\n        // fall through\n      case HiveParser.TOK_SELECT:\n        qb.countSel();\n        qbp.setSelExprForClause(ctx_1.dest, ast);\n\n        int posn = 0;\n        if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.QUERY_HINT) {\n          ParseDriver pd = new ParseDriver();\n          String queryHintStr = ast.getChild(0).getText();\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"QUERY HINT: \"+queryHintStr);\n          }\n          try {\n            ASTNode hintNode = pd.parseHint(queryHintStr);\n            qbp.setHints(hintNode);\n            posn++;\n          } catch (ParseException e) {\n            throw new SemanticException(\"failed to parse query hint: \"+e.getMessage(), e);\n          }\n        }\n\n        if ((ast.getChild(posn).getChild(0).getType() == HiveParser.TOK_TRANSFORM))\n          queryProperties.setUsesScript(true);\n\n        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast,\n            qb, ctx_1.dest);\n        doPhase1GetColumnAliasesFromSelect(ast, qbp);\n        qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n        qbp.setDistinctFuncExprsForClause(ctx_1.dest,\n          doPhase1GetDistinctFuncExprs(aggregations));\n        break;\n\n      case HiveParser.TOK_WHERE:\n        qbp.setWhrExprForClause(ctx_1.dest, ast);\n        if (!SubQueryUtils.findSubQueries((ASTNode) ast.getChild(0)).isEmpty())\n            queryProperties.setFilterWithSubQuery(true);\n        break;\n\n      case HiveParser.TOK_INSERT_INTO:\n        String currentDatabase = SessionState.get().getCurrentDatabase();\n        String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);\n        qbp.addInsertIntoTable(tab_name, ast);\n\n      case HiveParser.TOK_DESTINATION:\n        ctx_1.dest = this.ctx.getDestNamePrefix(ast).toString() + ctx_1.nextNum;\n        ctx_1.nextNum++;\n        boolean isTmpFileDest = false;\n        if (ast.getChildCount() > 0 && ast.getChild(0) instanceof ASTNode) {\n          ASTNode ch = (ASTNode) ast.getChild(0);\n          if (ch.getToken().getType() == HiveParser.TOK_DIR && ch.getChildCount() > 0\n              && ch.getChild(0) instanceof ASTNode) {\n            ch = (ASTNode) ch.getChild(0);\n            isTmpFileDest = ch.getToken().getType() == HiveParser.TOK_TMP_FILE;\n          } else {\n            if (ast.getToken().getType() == HiveParser.TOK_DESTINATION\n                && ast.getChild(0).getType() == HiveParser.TOK_TAB) {\n              String fullTableName = getUnescapedName((ASTNode) ast.getChild(0).getChild(0),\n                  SessionState.get().getCurrentDatabase());\n              qbp.getInsertOverwriteTables().put(fullTableName, ast);\n            }\n          }\n        }\n\n        // is there a insert in the subquery\n        if (qbp.getIsSubQ() && !isTmpFileDest) {\n          throw new SemanticException(ErrorMsg.NO_INSERT_INSUBQUERY.getMsg(ast));\n        }\n\n        qbp.setDestForClause(ctx_1.dest, (ASTNode) ast.getChild(0));\n        handleInsertStatementSpecPhase1(ast, qbp, ctx_1);\n\n        if (qbp.getClauseNamesForDest().size() == 2) {\n          // From the moment that we have two destination clauses,\n          // we know that this is a multi-insert query.\n          // Thus, set property to right value.\n          // Using qbp.getClauseNamesForDest().size() >= 2 would be\n          // equivalent, but we use == to avoid setting the property\n          // multiple times\n          queryProperties.setMultiDestQuery(true);\n        }\n\n        if (plannerCtx != null && !queryProperties.hasMultiDestQuery()) {\n          plannerCtx.setInsertToken(ast, isTmpFileDest);\n        } else if (plannerCtx != null && qbp.getClauseNamesForDest().size() == 2) {\n          // For multi-insert query, currently we only optimize the FROM clause.\n          // Hence, introduce multi-insert token on top of it.\n          // However, first we need to reset existing token (insert).\n          // Using qbp.getClauseNamesForDest().size() >= 2 would be\n          // equivalent, but we use == to avoid setting the property\n          // multiple times\n          plannerCtx.resetToken();\n          plannerCtx.setMultiInsertToken((ASTNode) qbp.getQueryFrom().getChild(0));\n        }\n        break;\n\n      case HiveParser.TOK_FROM:\n        int child_count = ast.getChildCount();\n        if (child_count != 1) {\n          throw new SemanticException(generateErrorMessage(ast,\n              \"Multiple Children \" + child_count));\n        }\n\n        if (!qbp.getIsSubQ()) {\n          qbp.setQueryFromExpr(ast);\n        }\n\n        // Check if this is a subquery / lateral view\n        ASTNode frm = (ASTNode) ast.getChild(0);\n        if (frm.getToken().getType() == HiveParser.TOK_TABREF) {\n          processTable(qb, frm);\n        } else if (frm.getToken().getType() == HiveParser.TOK_VIRTUAL_TABLE) {\n          // Create a temp table with the passed values in it then rewrite this portion of the\n          // tree to be from that table.\n          ASTNode newFrom = genValuesTempTable(frm, qb);\n          ast.setChild(0, newFrom);\n          processTable(qb, newFrom);\n        } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {\n          processSubQuery(qb, frm);\n        } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||\n            frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {\n          queryProperties.setHasLateralViews(true);\n          processLateralView(qb, frm);\n        } else if (isJoinToken(frm)) {\n          processJoin(qb, frm);\n          qbp.setJoinExpr(frm);\n        }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){\n          queryProperties.setHasPTF(true);\n          processPTF(qb, frm);\n        }\n        break;\n\n      case HiveParser.TOK_CLUSTERBY:\n        // Get the clusterby aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasClusterBy(true);\n        qbp.setClusterByExprForClause(ctx_1.dest, ast);\n        break;\n\n      case HiveParser.TOK_DISTRIBUTEBY:\n        // Get the distribute by aliases - these are aliased to the entries in\n        // the\n        // select list\n        queryProperties.setHasDistributeBy(true);\n        qbp.setDistributeByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.ORDERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n        }\n        break;\n\n      case HiveParser.TOK_SORTBY:\n     // Get the sort by aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasSortBy(true);\n        qbp.setSortByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));\n        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));\n        }\n\n        break;\n\n      case HiveParser.TOK_ORDERBY:\n        // Get the order by aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasOrderBy(true);\n        qbp.setOrderByExprForClause(ctx_1.dest, ast);\n        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));\n        }\n        break;\n\n      case HiveParser.TOK_GROUPBY:\n      case HiveParser.TOK_ROLLUP_GROUPBY:\n      case HiveParser.TOK_CUBE_GROUPBY:\n      case HiveParser.TOK_GROUPING_SETS:\n        // Get the groupby aliases - these are aliased to the entries in the\n        // select list\n        queryProperties.setHasGroupBy(true);\n        if (qbp.getJoinExpr() != null) {\n          queryProperties.setHasJoinFollowedByGroupBy(true);\n        }\n        if (qbp.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));\n        }\n        qbp.setGroupByExprForClause(ctx_1.dest, ast);\n        skipRecursion = true;\n\n        // Rollup and Cubes are syntactic sugar on top of grouping sets\n        if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {\n          qbp.getDestRollups().add(ctx_1.dest);\n        } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {\n          qbp.getDestCubes().add(ctx_1.dest);\n        } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {\n          qbp.getDestGroupingSets().add(ctx_1.dest);\n        }\n        break;\n\n      case HiveParser.TOK_HAVING:\n        qbp.setHavingExprForClause(ctx_1.dest, ast);\n        qbp.addAggregationExprsForClause(ctx_1.dest,\n            doPhase1GetAggregationsFromSelect(ast, qb, ctx_1.dest));\n        break;\n\n      case HiveParser.KW_WINDOW:\n        if (!qb.hasWindowingSpec(ctx_1.dest) ) {\n          throw new SemanticException(generateErrorMessage(ast,\n              \"Query has no Cluster/Distribute By; but has a Window definition\"));\n        }\n        handleQueryWindowClauses(qb, ctx_1, ast);\n        break;\n\n      case HiveParser.TOK_LIMIT:\n        if (ast.getChildCount() == 2) {\n          qbp.setDestLimit(ctx_1.dest,\n              new Integer(ast.getChild(0).getText()),\n              new Integer(ast.getChild(1).getText()));\n        } else {\n          qbp.setDestLimit(ctx_1.dest, new Integer(0),\n              new Integer(ast.getChild(0).getText()));\n        }\n        break;\n\n      case HiveParser.TOK_ANALYZE:\n        // Case of analyze command\n\n        String table_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0)).toLowerCase();\n\n\n        qb.setTabAlias(table_name, table_name);\n        qb.addAlias(table_name);\n        qb.getParseInfo().setIsAnalyzeCommand(true);\n        qb.getParseInfo().setNoScanAnalyzeCommand(this.noscan);\n        qb.getParseInfo().setPartialScanAnalyzeCommand(this.partialscan);\n        // Allow analyze the whole table and dynamic partitions\n        HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n        HiveConf.setVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE, \"nonstrict\");\n\n        break;\n\n      case HiveParser.TOK_UNIONALL:\n        if (!qbp.getIsSubQ()) {\n          // this shouldn't happen. The parser should have converted the union to be\n          // contained in a subquery. Just in case, we keep the error as a fallback.\n          throw new SemanticException(generateErrorMessage(ast,\n              ErrorMsg.UNION_NOTIN_SUBQ.getMsg()));\n        }\n        skipRecursion = false;\n        break;\n\n      case HiveParser.TOK_INSERT:\n        ASTNode destination = (ASTNode) ast.getChild(0);\n        Tree tab = destination.getChild(0);\n\n        // Proceed if AST contains partition & If Not Exists\n        if (destination.getChildCount() == 2 &&\n            tab.getChildCount() == 2 &&\n            destination.getChild(1).getType() == HiveParser.TOK_IFNOTEXISTS) {\n          String tableName = tab.getChild(0).getChild(0).getText();\n\n          Tree partitions = tab.getChild(1);\n          int childCount = partitions.getChildCount();\n          HashMap<String, String> partition = new HashMap<String, String>();\n          for (int i = 0; i < childCount; i++) {\n            String partitionName = partitions.getChild(i).getChild(0).getText();\n            Tree pvalue = partitions.getChild(i).getChild(1);\n            if (pvalue == null) {\n              break;\n            }\n            String partitionVal = stripQuotes(pvalue.getText());\n            partition.put(partitionName, partitionVal);\n          }\n          // if it is a dynamic partition throw the exception\n          if (childCount != partition.size()) {\n            throw new SemanticException(ErrorMsg.INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS\n                .getMsg(partition.toString()));\n          }\n          Table table = null;\n          try {\n            table = this.getTableObjectByName(tableName);\n          } catch (HiveException ex) {\n            throw new SemanticException(ex);\n          }\n          try {\n            Partition parMetaData = db.getPartition(table, partition, false);\n            // Check partition exists if it exists skip the overwrite\n            if (parMetaData != null) {\n              phase1Result = false;\n              skipRecursion = true;\n              LOG.info(\"Partition already exists so insert into overwrite \" +\n                  \"skipped for partition : \" + parMetaData.toString());\n              break;\n            }\n          } catch (HiveException e) {\n            LOG.info(\"Error while getting metadata : \", e);\n          }\n          validatePartSpec(table, partition, (ASTNode)tab, conf, false);\n        }\n        skipRecursion = false;\n        break;\n      case HiveParser.TOK_LATERAL_VIEW:\n      case HiveParser.TOK_LATERAL_VIEW_OUTER:\n        // todo: nested LV\n        assert ast.getChildCount() == 1;\n        qb.getParseInfo().getDestToLateralView().put(ctx_1.dest, ast);\n        break;\n      case HiveParser.TOK_CTE:\n        processCTE(qb, ast);\n        break;\n      default:\n        skipRecursion = false;\n        break;\n      }\n    }\n\n    if (!skipRecursion) {\n      // Iterate over the rest of the children\n      int child_count = ast.getChildCount();\n      for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {\n        // Recurse\n        phase1Result = phase1Result && doPhase1(\n            (ASTNode)ast.getChild(child_pos), qb, ctx_1, plannerCtx);\n      }\n    }\n    return phase1Result;\n  }"
        ],
        [
            "GenericUDFInFile::isTypeCompatible(ObjectInspector)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  ",
            "  private boolean isTypeCompatible(ObjectInspector argument) {\n    PrimitiveObjectInspector poi = ((PrimitiveObjectInspector) argument);\n    return\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.STRING ||\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.CHAR ||\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.VARCHAR;\n  }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91  ",
            "  private boolean isTypeCompatible(ObjectInspector argument) {\n    PrimitiveObjectInspector poi = ((PrimitiveObjectInspector) argument);\n    return\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.STRING ||\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.CHAR ||\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.VARCHAR ||\n      poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.VOID;\n  }"
        ],
        [
            "TestDbTxnManager2::testDummyTxnManagerOnAcidTable()",
            " 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 -\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "  @Test\n  public void testDummyTxnManagerOnAcidTable() throws Exception {\n    dropTable(new String[] {\"T10\", \"T11\"});\n    // Create an ACID table with DbTxnManager\n    CommandProcessorResponse cpr = driver.run(\"create table T10 (a int, b int) clustered by(b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')\");\n    checkCmdOnDriver(cpr);\n    cpr = driver.run(\"create table T11 (a int, b int) clustered by(b) into 2 buckets stored as orc\");\n    checkCmdOnDriver(cpr);\n\n    // All DML should fail with DummyTxnManager on ACID table\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"select * from T10\");\n    Assert.assertEquals(ErrorMsg.TXNMGR_NOT_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"This command is not allowed on an ACID table\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"insert into table T10 values (1, 2)\");\n    Assert.assertEquals(ErrorMsg.TXNMGR_NOT_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"This command is not allowed on an ACID table\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"insert overwrite table T10 select a, b from T11\");\n    Assert.assertEquals(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"INSERT OVERWRITE not allowed on table with OutputFormat\" +\n        \" that implements AcidOutputFormat while transaction manager that supports ACID is in use\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"update T10 set a=0 where b=1\");\n    Assert.assertEquals(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"Attempt to do update or delete using transaction manager that does not support these operations.\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"delete from T10\");\n    Assert.assertEquals(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"Attempt to do update or delete using transaction manager that does not support these operations.\"));\n\n    conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, \"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\");\n  }",
            " 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "  @Test\n  public void testDummyTxnManagerOnAcidTable() throws Exception {\n    dropTable(new String[] {\"T10\", \"T11\"});\n    // Create an ACID table with DbTxnManager\n    CommandProcessorResponse cpr = driver.run(\"create table T10 (a int, b int) clustered by(b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')\");\n    checkCmdOnDriver(cpr);\n    cpr = driver.run(\"create table T11 (a int, b int) clustered by(b) into 2 buckets stored as orc\");\n    checkCmdOnDriver(cpr);\n\n    // All DML should fail with DummyTxnManager on ACID table\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"select * from T10\");\n    Assert.assertEquals(ErrorMsg.TXNMGR_NOT_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"This command is not allowed on an ACID table\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"insert into table T10 values (1, 2)\");\n    Assert.assertEquals(ErrorMsg.TXNMGR_NOT_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"This command is not allowed on an ACID table\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"insert overwrite table T10 select a, b from T11\");\n    Assert.assertEquals(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"INSERT OVERWRITE not allowed on table default.t10 with OutputFormat\" +\n        \" that implements AcidOutputFormat while transaction manager that supports ACID is in use\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"update T10 set a=0 where b=1\");\n    Assert.assertEquals(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"Attempt to do update or delete using transaction manager that does not support these operations.\"));\n\n    useDummyTxnManagerTemporarily(conf);\n    cpr = driver.compileAndRespond(\"delete from T10\");\n    Assert.assertEquals(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getErrorCode(), cpr.getResponseCode());\n    Assert.assertTrue(cpr.getErrorMessage().contains(\"Attempt to do update or delete using transaction manager that does not support these operations.\"));\n\n    conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, \"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\");\n  }"
        ],
        [
            "SemanticAnalyzer::checkAcidConstraints(QB,TableDesc,Table)",
            "7355  \n7356  \n7357  \n7358  \n7359  \n7360 -\n7361  \n7362  \n7363  \n7364  \n7365  \n7366  \n7367  \n7368  \n7369  \n7370  \n7371  \n7372  \n7373  \n7374  \n7375  \n7376  \n7377  \n7378  \n7379  \n7380  \n7381  \n7382  ",
            "  private void checkAcidConstraints(QB qb, TableDesc tableDesc,\n                                    Table table) throws SemanticException {\n    String tableName = tableDesc.getTableName();\n    if (!qb.getParseInfo().isInsertIntoTable(tableName)) {\n      LOG.debug(\"Couldn't find table \" + tableName + \" in insertIntoTable\");\n      throw new SemanticException(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID.getMsg());\n    }\n    /*\n    LOG.info(\"Modifying config values for ACID write\");\n    conf.setBoolVar(ConfVars.HIVEOPTREDUCEDEDUPLICATION, true);\n    conf.setIntVar(ConfVars.HIVEOPTREDUCEDEDUPLICATIONMINREDUCER, 1);\n    These props are now enabled elsewhere (see commit diffs).  It would be better instead to throw\n    if they are not set.  For exmaple, if user has set hive.optimize.reducededuplication=false for\n    some reason, we'll run a query contrary to what they wanted...  But throwing now would be\n    backwards incompatible.\n    */\n    conf.set(AcidUtils.CONF_ACID_KEY, \"true\");\n\n    if (table.getNumBuckets() < 1) {\n      throw new SemanticException(ErrorMsg.ACID_OP_ON_NONACID_TABLE, table.getTableName());\n    }\n    if (table.getSortCols() != null && table.getSortCols().size() > 0) {\n      throw new SemanticException(ErrorMsg.ACID_NO_SORTED_BUCKETS, table.getTableName());\n    }\n\n\n\n  }",
            "7355  \n7356  \n7357  \n7358  \n7359  \n7360 +\n7361  \n7362  \n7363  \n7364  \n7365  \n7366  \n7367  \n7368  \n7369  \n7370  \n7371  \n7372  \n7373  \n7374  \n7375  \n7376  \n7377  \n7378  \n7379  \n7380  \n7381  \n7382  ",
            "  private void checkAcidConstraints(QB qb, TableDesc tableDesc,\n                                    Table table) throws SemanticException {\n    String tableName = tableDesc.getTableName();\n    if (!qb.getParseInfo().isInsertIntoTable(tableName)) {\n      LOG.debug(\"Couldn't find table \" + tableName + \" in insertIntoTable\");\n      throw new SemanticException(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID, tableName);\n    }\n    /*\n    LOG.info(\"Modifying config values for ACID write\");\n    conf.setBoolVar(ConfVars.HIVEOPTREDUCEDEDUPLICATION, true);\n    conf.setIntVar(ConfVars.HIVEOPTREDUCEDEDUPLICATIONMINREDUCER, 1);\n    These props are now enabled elsewhere (see commit diffs).  It would be better instead to throw\n    if they are not set.  For exmaple, if user has set hive.optimize.reducededuplication=false for\n    some reason, we'll run a query contrary to what they wanted...  But throwing now would be\n    backwards incompatible.\n    */\n    conf.set(AcidUtils.CONF_ACID_KEY, \"true\");\n\n    if (table.getNumBuckets() < 1) {\n      throw new SemanticException(ErrorMsg.ACID_OP_ON_NONACID_TABLE, table.getTableName());\n    }\n    if (table.getSortCols() != null && table.getSortCols().size() > 0) {\n      throw new SemanticException(ErrorMsg.ACID_NO_SORTED_BUCKETS, table.getTableName());\n    }\n\n\n\n  }"
        ],
        [
            "CalcitePlanner::genOPTree(ASTNode,PlannerContext)",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 -\n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n      ASTNode oldHints = getQB().getParseInfo().getHints();\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS || cboCtx.type == PreCboCtx.Type.VIEW) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              throw new SemanticException(\"Create view is not supported in cbo return path.\");\n            }\n            sinkOp = getOptimizedHiveOPDag();\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 1. Gen Optimized AST\n            ASTNode newAST = getOptimizedAST();\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 2. Regen OP plan from optimized AST\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              try {\n                handleCreateViewDDL(newAST);\n              } catch (SemanticException e) {\n                throw new CalciteViewSemanticException(e.getMessage());\n              }\n            } else {\n              init(false);\n              if (cboCtx.type == PreCboCtx.Type.VIEW && materializedView) {\n                // Redo create-table/view analysis, because it's not part of\n                // doPhase1.\n                // Use the REWRITTEN AST\n                setAST(newAST);\n                newAST = reAnalyzeViewAfterCbo(newAST);\n                // Store text of the ORIGINAL QUERY\n                String originalText = ctx.getTokenRewriteStream().toString(\n                    cboCtx.nodeOfInterest.getTokenStartIndex(),\n                    cboCtx.nodeOfInterest.getTokenStopIndex());\n                createVwDesc.setViewOriginalText(originalText);\n                viewSelect = newAST;\n                viewsExpanded = new ArrayList<>();\n                viewsExpanded.add(createVwDesc.getViewName());\n              } else if (cboCtx.type == PreCboCtx.Type.CTAS) {\n                // CTAS\n                setAST(newAST);\n                newAST = reAnalyzeCTASAfterCbo(newAST);\n              }\n            }\n            if (oldHints != null) {\n              if (getQB().getParseInfo().getHints() != null) {\n                LOG.warn(\"Hints are not null in the optimized tree; before CBO \" + oldHints.dump()\n                    + \"; after CBO \" + getQB().getParseInfo().getHints().dump());\n              } else {\n                LOG.debug(\"Propagating hints to QB: \" + oldHints);\n                getQB().getParseInfo().setHints(oldHints);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if( e instanceof CalciteViewSemanticException) {\n            // non-cbo path retries to execute create view and\n            // we believe it will throw the same error message\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 +\n 470  \n 471  \n 472 +\n 473  \n 474  \n 475  \n 476 +\n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n      ASTNode oldHints = getQB().getParseInfo().getHints();\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS || cboCtx.type == PreCboCtx.Type.VIEW) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              throw new SemanticException(\"Create view is not supported in cbo return path.\");\n            }\n            sinkOp = getOptimizedHiveOPDag();\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 1. Gen Optimized AST\n            ASTNode newAST = getOptimizedAST();\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 2. Regen OP plan from optimized AST\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              try {\n                handleCreateViewDDL(newAST);\n              } catch (SemanticException e) {\n                throw new CalciteViewSemanticException(e.getMessage());\n              }\n            } else {\n              init(false);\n              if (cboCtx.type == PreCboCtx.Type.VIEW && materializedView) {\n                // Redo create-table/view analysis, because it's not part of\n                // doPhase1.\n                // Use the REWRITTEN AST\n                setAST(newAST);\n                newAST = reAnalyzeViewAfterCbo(newAST);\n                // Store text of the ORIGINAL QUERY\n                String originalText = ctx.getTokenRewriteStream().toString(\n                    cboCtx.nodeOfInterest.getTokenStartIndex(),\n                    cboCtx.nodeOfInterest.getTokenStopIndex());\n                createVwDesc.setViewOriginalText(originalText);\n                viewSelect = newAST;\n                viewsExpanded = new ArrayList<>();\n                viewsExpanded.add(createVwDesc.getViewName());\n              } else if (cboCtx.type == PreCboCtx.Type.CTAS) {\n                // CTAS\n                setAST(newAST);\n                newAST = reAnalyzeCTASAfterCbo(newAST);\n              }\n            }\n            if (oldHints != null) {\n              if (getQB().getParseInfo().getHints() != null) {\n                LOG.warn(\"Hints are not null in the optimized tree; before CBO \" + oldHints.dump()\n                    + \"; after CBO \" + getQB().getParseInfo().getHints().dump());\n              } else {\n                LOG.debug(\"Propagating hints to QB: \" + oldHints);\n                getQB().getParseInfo().setHints(oldHints);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if( e instanceof CalciteViewSemanticException) {\n            // non-cbo path retries to execute create view and\n            // we believe it will throw the same error message\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n            // for tests we would like to avoid retrying to catch cbo failures\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else if (e instanceof SemanticException) {\n            throw e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }"
        ],
        [
            "CalcitePlanner::CalcitePlannerAction::setQueryHints(QB)",
            "3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500 -\n3501  \n3502  \n3503  \n3504  ",
            "    private void setQueryHints(QB qb) throws SemanticException {\n      QBParseInfo qbp = getQBParseInfo(qb);\n      String selClauseName = qbp.getClauseNames().iterator().next();\n      Tree selExpr0 = qbp.getSelForClause(selClauseName).getChild(0);\n\n      if (selExpr0.getType() != HiveParser.QUERY_HINT) return;\n      String hint = ctx.getTokenRewriteStream().toString(\n          selExpr0.getTokenStartIndex(), selExpr0.getTokenStopIndex());\n      LOG.debug(\"Handling query hints: \" + hint);\n      ParseDriver pd = new ParseDriver();\n      try {\n        ASTNode hintNode = pd.parseHint(hint);\n        qbp.setHints((ASTNode) hintNode);\n      } catch (ParseException e) {\n        throw new SemanticException(\"failed to parse query hint: \"+e.getMessage(), e);\n      }\n    }",
            "3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503 +\n3504  \n3505  \n3506  \n3507  ",
            "    private void setQueryHints(QB qb) throws SemanticException {\n      QBParseInfo qbp = getQBParseInfo(qb);\n      String selClauseName = qbp.getClauseNames().iterator().next();\n      Tree selExpr0 = qbp.getSelForClause(selClauseName).getChild(0);\n\n      if (selExpr0.getType() != HiveParser.QUERY_HINT) return;\n      String hint = ctx.getTokenRewriteStream().toString(\n          selExpr0.getTokenStartIndex(), selExpr0.getTokenStopIndex());\n      LOG.debug(\"Handling query hints: \" + hint);\n      ParseDriver pd = new ParseDriver();\n      try {\n        ASTNode hintNode = pd.parseHint(hint);\n        qbp.setHints(hintNode);\n      } catch (ParseException e) {\n        throw new SemanticException(\"failed to parse query hint: \"+e.getMessage(), e);\n      }\n    }"
        ],
        [
            "TestDbTxnManager2::testShowTablesLock()",
            "2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090 -\n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  ",
            "  @Test\n  public void testShowTablesLock() throws Exception {\n    dropTable(new String[] {\"T, T2\"});\n    CommandProcessorResponse cpr = driver.run(\n      \"create table if not exists T (a int, b int)\");\n    checkCmdOnDriver(cpr);\n\n    long txnid1 = txnMgr.openTxn(ctx, \"Fifer\");\n    checkCmdOnDriver(driver.compileAndRespond(\"insert into T values(1,3)\"));\n    txnMgr.acquireLocks(driver.getPlan(), ctx, \"Fifer\");\n    List<ShowLocksResponseElement> locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 1, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t\", null, locks);\n\n    DbTxnManager txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);\n    checkCmdOnDriver(driver.compileAndRespond(\"show tables\"));\n    txnMgr2.acquireLocks(driver.getPlan(), ctx, \"Fidler\");\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 2, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t\", null, locks);\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", null, null, locks);\n    txnMgr.commitTxn();\n    txnMgr2.releaseLocks(txnMgr2.getLockManager().getLocks(false, false));\n    Assert.assertEquals(\"Lock remained\", 0, getLocks().size());\n    Assert.assertEquals(\"Lock remained\", 0, getLocks(txnMgr2).size());\n\n\n    cpr = driver.run(\n      \"create table if not exists T2 (a int, b int) partitioned by (p int) clustered by (a) \" +\n        \"into 2  buckets stored as orc TBLPROPERTIES ('transactional'='false')\");\n    checkCmdOnDriver(cpr);\n\n    txnid1 = txnMgr.openTxn(ctx, \"Fifer\");\n    checkCmdOnDriver(driver.compileAndRespond(\"insert into T2 partition(p=1) values(1,3)\"));\n    txnMgr.acquireLocks(driver.getPlan(), ctx, \"Fifer\");\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 1, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t2\", \"p=1\", locks);\n\n    txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);\n    checkCmdOnDriver(driver.compileAndRespond(\"show tables\"));\n    ((DbTxnManager)txnMgr2).acquireLocks(driver.getPlan(), ctx, \"Fidler\", false);\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 2, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t2\", \"p=1\", locks);\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", null, null, locks);\n    txnMgr.commitTxn();\n    txnMgr2.releaseLocks(txnMgr2.getLockManager().getLocks(false, false));\n    Assert.assertEquals(\"Lock remained\", 0, getLocks().size());\n    Assert.assertEquals(\"Lock remained\", 0, getLocks(txnMgr2).size());\n  }",
            "2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090 +\n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  ",
            "  @Test\n  public void testShowTablesLock() throws Exception {\n    dropTable(new String[] {\"T, T2\"});\n    CommandProcessorResponse cpr = driver.run(\n      \"create table if not exists T (a int, b int)\");\n    checkCmdOnDriver(cpr);\n\n    long txnid1 = txnMgr.openTxn(ctx, \"Fifer\");\n    checkCmdOnDriver(driver.compileAndRespond(\"insert into T values(1,3)\"));\n    txnMgr.acquireLocks(driver.getPlan(), ctx, \"Fifer\");\n    List<ShowLocksResponseElement> locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 1, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t\", null, locks);\n\n    DbTxnManager txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);\n    checkCmdOnDriver(driver.compileAndRespond(\"show tables\"));\n    txnMgr2.acquireLocks(driver.getPlan(), ctx, \"Fidler\");\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 2, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t\", null, locks);\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", null, null, locks);\n    txnMgr.commitTxn();\n    txnMgr2.releaseLocks(txnMgr2.getLockManager().getLocks(false, false));\n    Assert.assertEquals(\"Lock remained\", 0, getLocks().size());\n    Assert.assertEquals(\"Lock remained\", 0, getLocks(txnMgr2).size());\n\n\n    cpr = driver.run(\n      \"create table if not exists T2 (a int, b int) partitioned by (p int) clustered by (a) \" +\n        \"into 2  buckets stored as orc TBLPROPERTIES ('transactional'='false')\");\n    checkCmdOnDriver(cpr);\n\n    txnid1 = txnMgr.openTxn(ctx, \"Fifer\");\n    checkCmdOnDriver(driver.compileAndRespond(\"insert into T2 partition(p=1) values(1,3)\"));\n    txnMgr.acquireLocks(driver.getPlan(), ctx, \"Fifer\");\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 1, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t2\", \"p=1\", locks);\n\n    txnMgr2 = (DbTxnManager) TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);\n    checkCmdOnDriver(driver.compileAndRespond(\"show tables\"));\n    txnMgr2.acquireLocks(driver.getPlan(), ctx, \"Fidler\", false);\n    locks = getLocks();\n    Assert.assertEquals(\"Unexpected lock count\", 2, locks.size());\n    checkLock(LockType.EXCLUSIVE, LockState.ACQUIRED, \"default\", \"t2\", \"p=1\", locks);\n    checkLock(LockType.SHARED_READ, LockState.ACQUIRED, \"default\", null, null, locks);\n    txnMgr.commitTxn();\n    txnMgr2.releaseLocks(txnMgr2.getLockManager().getLocks(false, false));\n    Assert.assertEquals(\"Lock remained\", 0, getLocks().size());\n    Assert.assertEquals(\"Lock remained\", 0, getLocks(txnMgr2).size());\n  }"
        ]
    ],
    "ed3c3edcf175276a1b061680651fa1753e354c0e": [
        [
            "SparkMapJoinOptimizer::getMapJoinConversionInfo(JoinOperator,OptimizeSparkProcContext)",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "  /**\n   *   This method returns the big table position in a map-join. If the given join\n   *   cannot be converted to a map-join (This could happen for several reasons - one\n   *   of them being presence of 2 or more big tables that cannot fit in-memory), it returns -1.\n   *\n   *   Otherwise, it returns an int value that is the index of the big table in the set\n   *   MapJoinProcessor.bigTableCandidateSet\n   *\n   * @param joinOp\n   * @param context\n   * @return an array of 3 long values, first value is the position,\n   *   second value is the connected map join size, and the third is big table data size.\n   */\n  private long[] getMapJoinConversionInfo(\n      JoinOperator joinOp, OptimizeSparkProcContext context) {\n    Set<Integer> bigTableCandidateSet =\n        MapJoinProcessor.getBigTableCandidates(joinOp.getConf().getConds());\n\n    long maxSize = context.getConf().getLongVar(\n        HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n\n    int bigTablePosition = -1;\n\n    Statistics bigInputStat = null;\n    long totalSize = 0;\n    int pos = 0;\n\n    // bigTableFound means we've encountered a table that's bigger than the\n    // max. This table is either the big table or we cannot convert.\n    boolean bigTableFound = false;\n    boolean useTsStats = context.getConf().getBoolean(HiveConf.ConfVars.SPARK_USE_TS_STATS_FOR_MAPJOIN.varname, false);\n\n    // If we're using TS's stats for mapjoin optimization, check each branch and see if there's any\n    // upstream operator (e.g., JOIN, LATERAL_VIEW) that can increase output data size.\n    // If so, mark that branch as the big table branch.\n    if (useTsStats) {\n      LOG.debug(\"Checking map join optimization for operator {} using TS stats\", joinOp);\n      for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n        if (isBigTableBranch(parentOp)) {\n          if (bigTablePosition < 0 && bigTableCandidateSet.contains(pos)) {\n            LOG.debug(\"Found a big table branch with parent operator {} and position {}\", parentOp, pos);\n            bigTablePosition = pos;\n            bigTableFound = true;\n            bigInputStat = new Statistics();\n            bigInputStat.setDataSize(Long.MAX_VALUE);\n          } else {\n            // Either we've found multiple big table branches, or the current branch cannot\n            // be a big table branch. Disable mapjoin for these cases.\n            LOG.debug(\"Cannot enable map join optimization for operator {}\", joinOp);\n            return new long[]{-1, 0, 0};\n          }\n        }\n        pos++;\n      }\n    }\n\n    pos = 0;\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      // Skip the potential big table identified above\n      if (pos == bigTablePosition) {\n        pos++;\n        continue;\n      }\n\n      Statistics currInputStat;\n      if (useTsStats) {\n        currInputStat = new Statistics();\n        // Find all root TSs and add up all data sizes\n        // Not adding other stats (e.g., # of rows, col stats) since only data size is used here\n        for (TableScanOperator root : OperatorUtils.findOperatorsUpstream(parentOp, TableScanOperator.class)) {\n          currInputStat.addToDataSize(root.getStatistics().getDataSize());\n        }\n      } else {\n         currInputStat = parentOp.getStatistics();\n      }\n\n      if (currInputStat == null) {\n        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n        return new long[]{-1, 0, 0};\n      }\n\n      // Union is hard to handle. For instance, the following case:\n      //  TS    TS\n      //  |      |\n      //  FIL   FIL\n      //  |      |\n      //  SEL   SEL\n      //    \\   /\n      //    UNION\n      //      |\n      //      RS\n      //      |\n      //     JOIN\n      // If we treat this as a MJ case, then after the RS is removed, we would\n      // create two MapWorks, for each of the TS. Each of these MapWork will contain\n      // a MJ operator, which is wrong.\n      // Otherwise, we could try to break the op tree at the UNION, and create two MapWorks\n      // for the branches above. Then, MJ will be in the following ReduceWork.\n      // But, this is tricky to implement, and we'll leave it as a future work for now.\n      if (containUnionWithoutRS(parentOp.getParentOperators().get(0))) {\n        return new long[]{-1, 0, 0};\n      }\n\n      long inputSize = currInputStat.getDataSize();\n\n      if (bigInputStat == null || inputSize > bigInputStat.getDataSize()) {\n\n        if (bigTableFound) {\n          // cannot convert to map join; we've already chosen a big table\n          // on size and there's another one that's bigger.\n          return new long[]{-1, 0, 0};\n        }\n\n        if (inputSize > maxSize) {\n          if (!bigTableCandidateSet.contains(pos)) {\n            // can't use the current table as the big table, but it's too\n            // big for the map side.\n            return new long[]{-1, 0, 0};\n          }\n\n          bigTableFound = true;\n        }\n\n        if (bigInputStat != null) {\n          // we're replacing the current big table with a new one. Need\n          // to count the current one as a map table then.\n          totalSize += bigInputStat.getDataSize();\n        }\n\n        if (totalSize > maxSize) {\n          // sum of small tables size in this join exceeds configured limit\n          // hence cannot convert.\n          return new long[]{-1, 0, 0};\n        }\n\n        if (bigTableCandidateSet.contains(pos)) {\n          bigTablePosition = pos;\n          bigInputStat = currInputStat;\n        }\n      } else {\n        totalSize += currInputStat.getDataSize();\n        if (totalSize > maxSize) {\n          // cannot hold all map tables in memory. Cannot convert.\n          return new long[]{-1, 0, 0};\n        }\n      }\n      pos++;\n    }\n\n    if (bigTablePosition == -1) {\n      //No big table candidates.\n      return new long[]{-1, 0, 0};\n    }\n\n    //Final check, find size of already-calculated Mapjoin Operators in same work (spark-stage).\n    //We need to factor this in to prevent overwhelming Spark executor-memory.\n    long connectedMapJoinSize = getConnectedMapJoinSize(joinOp.getParentOperators().\n      get(bigTablePosition), joinOp, context);\n    if ((connectedMapJoinSize + totalSize) > maxSize) {\n      return new long[]{-1, 0, 0};\n    }\n\n    return new long[]{bigTablePosition, connectedMapJoinSize, totalSize};\n  }",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210 +\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "  /**\n   *   This method returns the big table position in a map-join. If the given join\n   *   cannot be converted to a map-join (This could happen for several reasons - one\n   *   of them being presence of 2 or more big tables that cannot fit in-memory), it returns -1.\n   *\n   *   Otherwise, it returns an int value that is the index of the big table in the set\n   *   MapJoinProcessor.bigTableCandidateSet\n   *\n   * @param joinOp\n   * @param context\n   * @return an array of 3 long values, first value is the position,\n   *   second value is the connected map join size, and the third is big table data size.\n   */\n  private long[] getMapJoinConversionInfo(\n      JoinOperator joinOp, OptimizeSparkProcContext context) {\n    Set<Integer> bigTableCandidateSet =\n        MapJoinProcessor.getBigTableCandidates(joinOp.getConf().getConds());\n\n    long maxSize = context.getConf().getLongVar(\n        HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n\n    int bigTablePosition = -1;\n\n    Statistics bigInputStat = null;\n    long totalSize = 0;\n    int pos = 0;\n\n    // bigTableFound means we've encountered a table that's bigger than the\n    // max. This table is either the big table or we cannot convert.\n    boolean bigTableFound = false;\n    boolean useTsStats = context.getConf().getBoolean(HiveConf.ConfVars.SPARK_USE_TS_STATS_FOR_MAPJOIN.varname, false);\n\n    // If we're using TS's stats for mapjoin optimization, check each branch and see if there's any\n    // upstream operator (e.g., JOIN, LATERAL_VIEW) that can increase output data size.\n    // If so, mark that branch as the big table branch.\n    if (useTsStats) {\n      LOG.debug(\"Checking map join optimization for operator {} using TS stats\", joinOp);\n      for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n        if (isBigTableBranch(parentOp)) {\n          if (bigTablePosition < 0 && bigTableCandidateSet.contains(pos)\n              && !containUnionWithoutRS(parentOp.getParentOperators().get(0))) {\n            LOG.debug(\"Found a big table branch with parent operator {} and position {}\", parentOp, pos);\n            bigTablePosition = pos;\n            bigTableFound = true;\n            bigInputStat = new Statistics();\n            bigInputStat.setDataSize(Long.MAX_VALUE);\n          } else {\n            // Either we've found multiple big table branches, or the current branch cannot\n            // be a big table branch. Disable mapjoin for these cases.\n            LOG.debug(\"Cannot enable map join optimization for operator {}\", joinOp);\n            return new long[]{-1, 0, 0};\n          }\n        }\n        pos++;\n      }\n    }\n\n    pos = 0;\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      // Skip the potential big table identified above\n      if (pos == bigTablePosition) {\n        pos++;\n        continue;\n      }\n\n      Statistics currInputStat;\n      if (useTsStats) {\n        currInputStat = new Statistics();\n        // Find all root TSs and add up all data sizes\n        // Not adding other stats (e.g., # of rows, col stats) since only data size is used here\n        for (TableScanOperator root : OperatorUtils.findOperatorsUpstream(parentOp, TableScanOperator.class)) {\n          currInputStat.addToDataSize(root.getStatistics().getDataSize());\n        }\n      } else {\n         currInputStat = parentOp.getStatistics();\n      }\n\n      if (currInputStat == null) {\n        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n        return new long[]{-1, 0, 0};\n      }\n\n      // Union is hard to handle. For instance, the following case:\n      //  TS    TS\n      //  |      |\n      //  FIL   FIL\n      //  |      |\n      //  SEL   SEL\n      //    \\   /\n      //    UNION\n      //      |\n      //      RS\n      //      |\n      //     JOIN\n      // If we treat this as a MJ case, then after the RS is removed, we would\n      // create two MapWorks, for each of the TS. Each of these MapWork will contain\n      // a MJ operator, which is wrong.\n      // Otherwise, we could try to break the op tree at the UNION, and create two MapWorks\n      // for the branches above. Then, MJ will be in the following ReduceWork.\n      // But, this is tricky to implement, and we'll leave it as a future work for now.\n      if (containUnionWithoutRS(parentOp.getParentOperators().get(0))) {\n        return new long[]{-1, 0, 0};\n      }\n\n      long inputSize = currInputStat.getDataSize();\n\n      if (bigInputStat == null || inputSize > bigInputStat.getDataSize()) {\n\n        if (bigTableFound) {\n          // cannot convert to map join; we've already chosen a big table\n          // on size and there's another one that's bigger.\n          return new long[]{-1, 0, 0};\n        }\n\n        if (inputSize > maxSize) {\n          if (!bigTableCandidateSet.contains(pos)) {\n            // can't use the current table as the big table, but it's too\n            // big for the map side.\n            return new long[]{-1, 0, 0};\n          }\n\n          bigTableFound = true;\n        }\n\n        if (bigInputStat != null) {\n          // we're replacing the current big table with a new one. Need\n          // to count the current one as a map table then.\n          totalSize += bigInputStat.getDataSize();\n        }\n\n        if (totalSize > maxSize) {\n          // sum of small tables size in this join exceeds configured limit\n          // hence cannot convert.\n          return new long[]{-1, 0, 0};\n        }\n\n        if (bigTableCandidateSet.contains(pos)) {\n          bigTablePosition = pos;\n          bigInputStat = currInputStat;\n        }\n      } else {\n        totalSize += currInputStat.getDataSize();\n        if (totalSize > maxSize) {\n          // cannot hold all map tables in memory. Cannot convert.\n          return new long[]{-1, 0, 0};\n        }\n      }\n      pos++;\n    }\n\n    if (bigTablePosition == -1) {\n      //No big table candidates.\n      return new long[]{-1, 0, 0};\n    }\n\n    //Final check, find size of already-calculated Mapjoin Operators in same work (spark-stage).\n    //We need to factor this in to prevent overwhelming Spark executor-memory.\n    long connectedMapJoinSize = getConnectedMapJoinSize(joinOp.getParentOperators().\n      get(bigTablePosition), joinOp, context);\n    if ((connectedMapJoinSize + totalSize) > maxSize) {\n      return new long[]{-1, 0, 0};\n    }\n\n    return new long[]{bigTablePosition, connectedMapJoinSize, totalSize};\n  }"
        ]
    ],
    "509308f642f4af8eb44a9fb7f0f105198df9fac6": [
        [
            "ObjectStore::getForeignKeysInternal(String,String,String,String,boolean,boolean)",
            "8535  \n8536  \n8537  \n8538  \n8539  \n8540  \n8541  \n8542 -\n8543  \n8544  \n8545  \n8546  \n8547  \n8548  \n8549  \n8550  \n8551  \n8552  \n8553  \n8554  \n8555  \n8556  \n8557  ",
            "  protected List<SQLForeignKey> getForeignKeysInternal(final String parent_db_name_input,\n    final String parent_tbl_name_input, final String foreign_db_name_input,\n    final String foreign_tbl_name_input, boolean allowSql, boolean allowJdo) throws MetaException, NoSuchObjectException {\n    final String parent_db_name = parent_db_name_input;\n    final String parent_tbl_name = parent_tbl_name_input;\n    final String foreign_db_name = foreign_db_name_input;\n    final String foreign_tbl_name = foreign_tbl_name_input;\n    return new GetListHelper<SQLForeignKey>(foreign_db_name, foreign_tbl_name, allowSql, allowJdo) {\n\n      @Override\n      protected List<SQLForeignKey> getSqlResult(GetHelper<List<SQLForeignKey>> ctx) throws MetaException {\n        return directSql.getForeignKeys(parent_db_name,\n          parent_tbl_name, foreign_db_name, foreign_tbl_name);\n      }\n\n      @Override\n      protected List<SQLForeignKey> getJdoResult(\n        GetHelper<List<SQLForeignKey>> ctx) throws MetaException, NoSuchObjectException {\n        return getForeignKeysViaJdo(parent_db_name,\n          parent_tbl_name, foreign_db_name, foreign_tbl_name);\n      }\n    }.run(false);\n  }",
            "8535  \n8536  \n8537  \n8538  \n8539  \n8540  \n8541  \n8542 +\n8543 +\n8544 +\n8545 +\n8546 +\n8547 +\n8548 +\n8549 +\n8550 +\n8551 +\n8552 +\n8553  \n8554  \n8555  \n8556  \n8557  \n8558  \n8559  \n8560  \n8561  \n8562  \n8563  \n8564  \n8565  \n8566  \n8567  ",
            "  protected List<SQLForeignKey> getForeignKeysInternal(final String parent_db_name_input,\n    final String parent_tbl_name_input, final String foreign_db_name_input,\n    final String foreign_tbl_name_input, boolean allowSql, boolean allowJdo) throws MetaException, NoSuchObjectException {\n    final String parent_db_name = parent_db_name_input;\n    final String parent_tbl_name = parent_tbl_name_input;\n    final String foreign_db_name = foreign_db_name_input;\n    final String foreign_tbl_name = foreign_tbl_name_input;\n    final String db_name;\n    final String tbl_name;\n    if (foreign_tbl_name == null) {\n      // The FK table name might be null if we are retrieving the constraint from the PK side\n      db_name = parent_db_name_input;\n      tbl_name = parent_tbl_name_input;\n    } else {\n      db_name = foreign_db_name_input;\n      tbl_name = foreign_tbl_name_input;\n    }\n    return new GetListHelper<SQLForeignKey>(db_name, tbl_name, allowSql, allowJdo) {\n\n      @Override\n      protected List<SQLForeignKey> getSqlResult(GetHelper<List<SQLForeignKey>> ctx) throws MetaException {\n        return directSql.getForeignKeys(parent_db_name,\n          parent_tbl_name, foreign_db_name, foreign_tbl_name);\n      }\n\n      @Override\n      protected List<SQLForeignKey> getJdoResult(\n        GetHelper<List<SQLForeignKey>> ctx) throws MetaException, NoSuchObjectException {\n        return getForeignKeysViaJdo(parent_db_name,\n          parent_tbl_name, foreign_db_name, foreign_tbl_name);\n      }\n    }.run(false);\n  }"
        ],
        [
            "TestObjectStore::testTableOps()",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210 -\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 -\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "  /**\n   * Test table operations\n   */\n  @Test\n  public void testTableOps() throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n    Database db1 = new Database(DB1, \"description\", \"locationurl\", null);\n    objectStore.createDatabase(db1);\n    StorageDescriptor sd = new StorageDescriptor(null, \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null), null, null, null);\n    HashMap<String,String> params = new HashMap<String,String>();\n    params.put(\"EXTERNAL\", \"false\");\n    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n    objectStore.createTable(tbl1);\n\n    List<String> tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(1, tables.size());\n    Assert.assertEquals(TABLE1, tables.get(0));\n\n    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n    objectStore.alterTable(DB1, TABLE1, newTbl1);\n    tables = objectStore.getTables(DB1, \"new*\");\n    Assert.assertEquals(1, tables.size());\n    Assert.assertEquals(\"new\" + TABLE1, tables.get(0));\n\n    objectStore.dropTable(DB1, \"new\" + TABLE1);\n    tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(0, tables.size());\n\n    objectStore.dropDatabase(DB1);\n  }",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210 +\n 211 +\n 212  \n 213  \n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222 +\n 223 +\n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248 +\n 249 +\n 250 +\n 251 +\n 252 +\n 253 +\n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "  /**\n   * Test table operations\n   */\n  @Test\n  public void testTableOps() throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n    Database db1 = new Database(DB1, \"description\", \"locationurl\", null);\n    objectStore.createDatabase(db1);\n    StorageDescriptor sd1 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"pk_col\", \"double\", null)),\n            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n            null, null, null);\n    HashMap<String,String> params = new HashMap<String,String>();\n    params.put(\"EXTERNAL\", \"false\");\n    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd1, null, params, null, null, \"MANAGED_TABLE\");\n    objectStore.createTable(tbl1);\n\n    List<String> tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(1, tables.size());\n    Assert.assertEquals(TABLE1, tables.get(0));\n\n    StorageDescriptor sd2 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"fk_col\", \"double\", null)),\n            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n            null, null, null);\n    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd2, null, params, null, null, \"MANAGED_TABLE\");\n    objectStore.alterTable(DB1, TABLE1, newTbl1);\n    tables = objectStore.getTables(DB1, \"new*\");\n    Assert.assertEquals(1, tables.size());\n    Assert.assertEquals(\"new\" + TABLE1, tables.get(0));\n\n    objectStore.createTable(tbl1);\n    tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(2, tables.size());\n\n    List<SQLForeignKey> foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n    Assert.assertEquals(0, foreignKeys.size());\n\n    SQLPrimaryKey pk = new SQLPrimaryKey(DB1, TABLE1, \"pk_col\", 1,\n            \"pk_const_1\", false, false, false);\n    objectStore.addPrimaryKeys(ImmutableList.of(pk));\n    SQLForeignKey fk = new SQLForeignKey(DB1, TABLE1, \"pk_col\",\n            DB1, \"new\" + TABLE1, \"fk_col\", 1,\n            0, 0, \"fk_const_1\", \"pk_const_1\", false, false, false);\n    objectStore.addForeignKeys(ImmutableList.of(fk));\n\n    // Retrieve from PK side\n    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n    Assert.assertEquals(1, foreignKeys.size());\n\n    List<SQLForeignKey> fks = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n    if (fks != null) {\n      for (SQLForeignKey fkcol : fks) {\n        objectStore.dropConstraint(fkcol.getFktable_db(), fkcol.getFktable_name(), fkcol.getFk_name());\n      }\n    }\n    // Retrieve from FK side\n    foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n    Assert.assertEquals(0, foreignKeys.size());\n    // Retrieve from PK side\n    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n    Assert.assertEquals(0, foreignKeys.size());\n\n    objectStore.dropTable(DB1, TABLE1);\n    tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(1, tables.size());\n\n    objectStore.dropTable(DB1, \"new\" + TABLE1);\n    tables = objectStore.getAllTables(DB1);\n    Assert.assertEquals(0, tables.size());\n\n    objectStore.dropDatabase(DB1);\n  }"
        ]
    ],
    "cea9ea7d4fe843b1655da8f4191b6b71195db0f4": [
        [
            "MergeJoinProc::process(Node,Stack,NodeProcessorCtx,Object)",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49  \n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56 -\n  57 -\n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "  @Override\n  public Object\n      process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n          throws SemanticException {\n    GenTezProcContext context = (GenTezProcContext) procCtx;\n    CommonMergeJoinOperator mergeJoinOp = (CommonMergeJoinOperator) nd;\n    if (stack.size() < 2 || !(stack.get(stack.size() - 2) instanceof DummyStoreOperator)) {\n      context.currentMergeJoinOperator = mergeJoinOp;\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n    @SuppressWarnings(\"unchecked\")\n    Operator<? extends OperatorDesc> parentOp =\n        (Operator<? extends OperatorDesc>) ((stack.get(stack.size() - 2)));\n    // Guaranteed to be just 1 because each DummyStoreOperator can be part of only one work.\n    BaseWork parentWork = context.childToWorkMap.get(parentOp).get(0);\n\n\n    // we need to set the merge work that has been created as part of the dummy store walk. If a\n    // merge work already exists for this merge join operator, add the dummy store work to the\n    // merge work. Else create a merge work, add above work to the merge work\n    MergeJoinWork mergeWork = null;\n    if (context.opMergeJoinWorkMap.containsKey(mergeJoinOp)) {\n      // we already have the merge work corresponding to this merge join operator\n      mergeWork = context.opMergeJoinWorkMap.get(mergeJoinOp);\n    } else {\n      mergeWork = new MergeJoinWork();\n      tezWork.add(mergeWork);\n      context.opMergeJoinWorkMap.put(mergeJoinOp, mergeWork);\n    }\n\n    mergeWork.addMergedWork(null, parentWork, context.leafOperatorToFollowingWork);\n    mergeWork.setMergeJoinOperator(mergeJoinOp);\n    tezWork.setVertexType(mergeWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n\n    for (BaseWork grandParentWork : tezWork.getParents(parentWork)) {\n      TezEdgeProperty edgeProp = tezWork.getEdgeProperty(grandParentWork, parentWork);\n      tezWork.disconnect(grandParentWork, parentWork);\n      tezWork.connect(grandParentWork, mergeWork, edgeProp);\n    }\n\n    for (BaseWork childWork : tezWork.getChildren(parentWork)) {\n      TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, childWork);\n      tezWork.disconnect(parentWork, childWork);\n      tezWork.connect(mergeWork, childWork, edgeProp);\n    }\n\n    tezWork.remove(parentWork);\n\n    DummyStoreOperator dummyOp = (DummyStoreOperator) (stack.get(stack.size() - 2));\n\n    parentWork.setTag(mergeJoinOp.getTagForOperator(dummyOp));\n\n    mergeJoinOp.getParentOperators().remove(dummyOp);\n    dummyOp.getChildOperators().clear();\n\n    return true;\n  }",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45 +\n  46 +\n  47 +\n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70 +\n  71 +\n  72 +\n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  @Override\n  public Object\n      process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n          throws SemanticException {\n    GenTezProcContext context = (GenTezProcContext) procCtx;\n    CommonMergeJoinOperator mergeJoinOp = (CommonMergeJoinOperator) nd;\n    if (stack.size() < 2) {\n      // safety check for L53 to get parentOp, although it is very unlikely that\n      // stack size is less than 2, i.e., there is only one MergeJoinOperator in the stack.\n      context.currentMergeJoinOperator = mergeJoinOp;\n      return null;\n    }\n    TezWork tezWork = context.currentTask.getWork();\n    @SuppressWarnings(\"unchecked\")\n    Operator<? extends OperatorDesc> parentOp =\n        (Operator<? extends OperatorDesc>) ((stack.get(stack.size() - 2)));\n\n    // we need to set the merge work that has been created as part of the dummy store walk. If a\n    // merge work already exists for this merge join operator, add the dummy store work to the\n    // merge work. Else create a merge work, add above work to the merge work\n    MergeJoinWork mergeWork = null;\n    if (context.opMergeJoinWorkMap.containsKey(mergeJoinOp)) {\n      // we already have the merge work corresponding to this merge join operator\n      mergeWork = context.opMergeJoinWorkMap.get(mergeJoinOp);\n    } else {\n      mergeWork = new MergeJoinWork();\n      tezWork.add(mergeWork);\n      context.opMergeJoinWorkMap.put(mergeJoinOp, mergeWork);\n    }\n\n    if (!(stack.get(stack.size() - 2) instanceof DummyStoreOperator)) {\n      /* this may happen in one of the following case:\n      TS[0], FIL[26], SEL[2], DUMMY_STORE[30], MERGEJOIN[29]]\n                                              /                              \n      TS[3], FIL[27], SEL[5], ---------------\n      */\n      context.currentMergeJoinOperator = mergeJoinOp;\n      mergeWork.setTag(mergeJoinOp.getTagForOperator(parentOp));\n      return null;\n    }\n\n    // Guaranteed to be just 1 because each DummyStoreOperator can be part of only one work.\n    BaseWork parentWork = context.childToWorkMap.get(parentOp).get(0);\n    mergeWork.addMergedWork(null, parentWork, context.leafOperatorToFollowingWork);\n    mergeWork.setMergeJoinOperator(mergeJoinOp);\n    tezWork.setVertexType(mergeWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n\n    for (BaseWork grandParentWork : tezWork.getParents(parentWork)) {\n      TezEdgeProperty edgeProp = tezWork.getEdgeProperty(grandParentWork, parentWork);\n      tezWork.disconnect(grandParentWork, parentWork);\n      tezWork.connect(grandParentWork, mergeWork, edgeProp);\n    }\n\n    for (BaseWork childWork : tezWork.getChildren(parentWork)) {\n      TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, childWork);\n      tezWork.disconnect(parentWork, childWork);\n      tezWork.connect(mergeWork, childWork, edgeProp);\n    }\n\n    tezWork.remove(parentWork);\n\n    DummyStoreOperator dummyOp = (DummyStoreOperator) (stack.get(stack.size() - 2));\n\n    parentWork.setTag(mergeJoinOp.getTagForOperator(dummyOp));\n\n    mergeJoinOp.getParentOperators().remove(dummyOp);\n    dummyOp.getChildOperators().clear();\n\n    return true;\n  }"
        ]
    ],
    "7580de9a49df1107ae9e709c4e2615a0982f5ed4": [
        [
            "ExplainTask::outputPlanVectorization(PrintStream,boolean)",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242 -\n 243  \n 244 -\n 245  \n 246  \n 247  \n 248  \n 249  ",
            "  @VisibleForTesting\n  ImmutablePair<Boolean, JSONObject> outputPlanVectorization(PrintStream out, boolean jsonOutput)\n      throws Exception {\n\n    if (out != null) {\n      out.println(\"PLAN VECTORIZATION:\");\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n\n    HiveConf hiveConf = queryState.getConf();\n\n    boolean isVectorizationEnabled = HiveConf.getBoolVar(hiveConf,\n        HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED);\n    String isVectorizationEnabledCondName =\n        (isVectorizationEnabled ?\n            trueCondNameVectorizationEnabled :\n              falseCondNameVectorizationEnabled);\n    List<String> isVectorizationEnabledCondList = Arrays.asList(isVectorizationEnabledCondName);\n\n    if (out != null) {\n      out.print(indentString(2));\n      out.print(\"enabled: \");\n      out.println(isVectorizationEnabled);\n      out.print(indentString(2));\n      if (!isVectorizationEnabled) {\n        out.print(\"enabledConditionsNotMet: \");\n      } else {\n        out.print(\"enabledConditionsMet: \");\n      }\n      out.println(isVectorizationEnabledCondList);\n    }\n    if (jsonOutput) {\n      json.put(\"enabled\", isVectorizationEnabled);\n      if (!isVectorizationEnabled) {\n        json.put(\"enabledConditionsNotMet\", isVectorizationEnabledCondList);\n      } else {\n        json.put(\"enabledConditionsMet\", isVectorizationEnabledCondList);\n      }\n    }\n\n    return new ImmutablePair<Boolean, JSONObject>(isVectorizationEnabled, jsonOutput ? json : null);\n  }",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242  \n 243 +\n 244  \n 245 +\n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  @VisibleForTesting\n  ImmutablePair<Boolean, JSONObject> outputPlanVectorization(PrintStream out, boolean jsonOutput)\n      throws Exception {\n\n    if (out != null) {\n      out.println(\"PLAN VECTORIZATION:\");\n    }\n\n    JSONObject json = jsonOutput ? new JSONObject(new LinkedHashMap<>()) : null;\n\n    HiveConf hiveConf = queryState.getConf();\n\n    boolean isVectorizationEnabled = HiveConf.getBoolVar(hiveConf,\n        HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED);\n    String isVectorizationEnabledCondName =\n        (isVectorizationEnabled ?\n            trueCondNameVectorizationEnabled :\n              falseCondNameVectorizationEnabled);\n    List<String> isVectorizationEnabledCondList = Arrays.asList(isVectorizationEnabledCondName);\n\n    if (out != null) {\n      out.print(indentString(2));\n      out.print(\"enabled: \");\n      out.println(isVectorizationEnabled);\n      out.print(indentString(2));\n      if (!isVectorizationEnabled) {\n        out.print(\"enabledConditionsNotMet: \");\n      } else {\n        out.print(\"enabledConditionsMet: \");\n      }\n      out.println(isVectorizationEnabledCondList);\n    }\n    if (jsonOutput) {\n      json.put(\"enabled\", isVectorizationEnabled);\n      JSONArray jsonArray = new JSONArray(Arrays.asList(isVectorizationEnabledCondName));\n      if (!isVectorizationEnabled) {\n        json.put(\"enabledConditionsNotMet\", jsonArray);\n      } else {\n        json.put(\"enabledConditionsMet\", jsonArray);\n      }\n    }\n\n    return new ImmutablePair<Boolean, JSONObject>(isVectorizationEnabled, jsonOutput ? json : null);\n  }"
        ],
        [
            "ExplainTask::getJSONDependencies(ExplainWork)",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143 -\n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "  @VisibleForTesting\n  static JSONObject getJSONDependencies(ExplainWork work)\n      throws Exception {\n    assert(work.getDependency());\n\n    JSONObject outJSONObject = new JSONObject(new LinkedHashMap<>());\n    List<Map<String, String>> inputTableInfo = new ArrayList<Map<String, String>>();\n    List<Map<String, String>> inputPartitionInfo = new ArrayList<Map<String, String>>();\n    for (ReadEntity input: work.getInputs()) {\n      switch (input.getType()) {\n        case TABLE:\n          Table table = input.getTable();\n          Map<String, String> tableInfo = new LinkedHashMap<String, String>();\n          tableInfo.put(\"tablename\", table.getCompleteName());\n          tableInfo.put(\"tabletype\", table.getTableType().toString());\n          if ((input.getParents() != null) && (!input.getParents().isEmpty())) {\n            tableInfo.put(\"tableParents\", input.getParents().toString());\n          }\n          inputTableInfo.add(tableInfo);\n          break;\n        case PARTITION:\n          Map<String, String> partitionInfo = new HashMap<String, String>();\n          partitionInfo.put(\"partitionName\", input.getPartition().getCompleteName());\n          if ((input.getParents() != null) && (!input.getParents().isEmpty())) {\n            partitionInfo.put(\"partitionParents\", input.getParents().toString());\n          }\n          inputPartitionInfo.add(partitionInfo);\n          break;\n        default:\n          break;\n      }\n    }\n\n    outJSONObject.put(\"input_tables\", inputTableInfo);\n    outJSONObject.put(\"input_partitions\", inputPartitionInfo);\n    return outJSONObject;\n  }",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 +\n 143 +\n 144  \n 145  \n 146  \n 147  \n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154 +\n 155  \n 156  \n 157 +\n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "  @VisibleForTesting\n  static JSONObject getJSONDependencies(ExplainWork work)\n      throws Exception {\n    assert(work.getDependency());\n\n    JSONObject outJSONObject = new JSONObject(new LinkedHashMap<>());\n    JSONArray inputTableInfo = new JSONArray();\n    JSONArray inputPartitionInfo = new JSONArray();\n    for (ReadEntity input: work.getInputs()) {\n      switch (input.getType()) {\n        case TABLE:\n          Table table = input.getTable();\n          JSONObject tableInfo = new JSONObject();\n          tableInfo.put(\"tablename\", table.getCompleteName());\n          tableInfo.put(\"tabletype\", table.getTableType().toString());\n          if ((input.getParents() != null) && (!input.getParents().isEmpty())) {\n            tableInfo.put(\"tableParents\", input.getParents().toString());\n          }\n          inputTableInfo.put(tableInfo);\n          break;\n        case PARTITION:\n          JSONObject partitionInfo = new JSONObject();\n          partitionInfo.put(\"partitionName\", input.getPartition().getCompleteName());\n          if ((input.getParents() != null) && (!input.getParents().isEmpty())) {\n            partitionInfo.put(\"partitionParents\", input.getParents().toString());\n          }\n          inputPartitionInfo.put(partitionInfo);\n          break;\n        default:\n          break;\n      }\n    }\n\n    outJSONObject.put(\"input_tables\", inputTableInfo);\n    outJSONObject.put(\"input_partitions\", inputPartitionInfo);\n    return outJSONObject;\n  }"
        ]
    ],
    "e7081035bb9768bc014f0aba11417418ececbaf0": [
        [
            "HiveExpandDistinctAggregatesRule::onMatch(RelOptRuleCall)",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    int numCountDistinct = getNumCountDistinctCall(aggregate);\n    if (numCountDistinct == 0) {\n      return;\n    }\n\n    // Find all of the agg expressions. We use a List (for all count(distinct))\n    // as well as a Set (for all others) to ensure determinism.\n    int nonDistinctCount = 0;\n    List<List<Integer>> argListList = new ArrayList<List<Integer>>();\n    Set<List<Integer>> argListSets = new LinkedHashSet<List<Integer>>();\n    Set<Integer> positions = new HashSet<>();\n    for (AggregateCall aggCall : aggregate.getAggCallList()) {\n      if (!aggCall.isDistinct()) {\n        ++nonDistinctCount;\n        continue;\n      }\n      ArrayList<Integer> argList = new ArrayList<Integer>();\n      for (Integer arg : aggCall.getArgList()) {\n        argList.add(arg);\n        positions.add(arg);\n      }\n      // Aggr checks for sorted argList.\n      argListList.add(argList);\n      argListSets.add(argList);\n    }\n    Util.permAssert(argListSets.size() > 0, \"containsDistinctCall lied\");\n\n    if (numCountDistinct > 1 && numCountDistinct == aggregate.getAggCallList().size()\n        && aggregate.getGroupSet().isEmpty()) {\n      LOG.debug(\"Trigger countDistinct rewrite. numCountDistinct is \" + numCountDistinct);\n      // now positions contains all the distinct positions, i.e., $5, $4, $6\n      // we need to first sort them as group by set\n      // and then get their position later, i.e., $4->1, $5->2, $6->3\n      cluster = aggregate.getCluster();\n      rexBuilder = cluster.getRexBuilder();\n      RelNode converted = null;\n      List<Integer> sourceOfForCountDistinct = new ArrayList<>();\n      sourceOfForCountDistinct.addAll(positions);\n      Collections.sort(sourceOfForCountDistinct);\n      try {\n        converted = convert(aggregate, argListList, sourceOfForCountDistinct);\n      } catch (CalciteSemanticException e) {\n        LOG.debug(e.toString());\n        throw new RuntimeException(e);\n      }\n      call.transformTo(converted);\n      return;\n    }\n\n    // If all of the agg expressions are distinct and have the same\n    // arguments then we can use a more efficient form.\n    if ((nonDistinctCount == 0) && (argListSets.size() == 1)) {\n      for (Integer arg : argListSets.iterator().next()) {\n        Set<RelColumnOrigin> colOrigs = RelMetadataQuery.instance().getColumnOrigins(aggregate, arg);\n        if (null != colOrigs) {\n          for (RelColumnOrigin colOrig : colOrigs) {\n            RelOptHiveTable hiveTbl = (RelOptHiveTable)colOrig.getOriginTable();\n            if(hiveTbl.getPartColInfoMap().containsKey(colOrig.getOriginColumnOrdinal())) {\n              // Encountered partitioning column, this will be better handled by MetadataOnly optimizer.\n              return;\n            }\n          }\n        }\n      }\n      RelNode converted =\n          convertMonopole(\n              aggregate,\n              argListSets.iterator().next());\n      call.transformTo(converted);\n      return;\n    }\n  }",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165 +\n 166  \n 167  \n 168 +\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    int numCountDistinct = getNumCountDistinctCall(aggregate);\n    if (numCountDistinct == 0) {\n      return;\n    }\n\n    // Find all of the agg expressions. We use a List (for all count(distinct))\n    // as well as a Set (for all others) to ensure determinism.\n    int nonDistinctCount = 0;\n    List<List<Integer>> argListList = new ArrayList<List<Integer>>();\n    Set<List<Integer>> argListSets = new LinkedHashSet<List<Integer>>();\n    Set<Integer> positions = new HashSet<>();\n    for (AggregateCall aggCall : aggregate.getAggCallList()) {\n      if (!aggCall.isDistinct()) {\n        ++nonDistinctCount;\n        continue;\n      }\n      ArrayList<Integer> argList = new ArrayList<Integer>();\n      for (Integer arg : aggCall.getArgList()) {\n        argList.add(arg);\n        positions.add(arg);\n      }\n      // Aggr checks for sorted argList.\n      argListList.add(argList);\n      argListSets.add(argList);\n    }\n    Util.permAssert(argListSets.size() > 0, \"containsDistinctCall lied\");\n\n    if (numCountDistinct > 1 && numCountDistinct == aggregate.getAggCallList().size()\n        && aggregate.getGroupSet().isEmpty()) {\n      LOG.debug(\"Trigger countDistinct rewrite. numCountDistinct is \" + numCountDistinct);\n      // now positions contains all the distinct positions, i.e., $5, $4, $6\n      // we need to first sort them as group by set\n      // and then get their position later, i.e., $4->1, $5->2, $6->3\n      cluster = aggregate.getCluster();\n      rexBuilder = cluster.getRexBuilder();\n      RelNode converted = null;\n      List<Integer> sourceOfForCountDistinct = new ArrayList<>();\n      sourceOfForCountDistinct.addAll(positions);\n      Collections.sort(sourceOfForCountDistinct);\n      try {\n        converted = convert(aggregate, argListList, sourceOfForCountDistinct);\n      } catch (CalciteSemanticException e) {\n        LOG.debug(e.toString());\n        throw new RuntimeException(e);\n      }\n      call.transformTo(converted);\n      return;\n    }\n\n    // If all of the agg expressions are distinct and have the same\n    // arguments then we can use a more efficient form.\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    if ((nonDistinctCount == 0) && (argListSets.size() == 1)) {\n      for (Integer arg : argListSets.iterator().next()) {\n        Set<RelColumnOrigin> colOrigs = mq.getColumnOrigins(aggregate, arg);\n        if (null != colOrigs) {\n          for (RelColumnOrigin colOrig : colOrigs) {\n            RelOptHiveTable hiveTbl = (RelOptHiveTable)colOrig.getOriginTable();\n            if(hiveTbl.getPartColInfoMap().containsKey(colOrig.getOriginColumnOrdinal())) {\n              // Encountered partitioning column, this will be better handled by MetadataOnly optimizer.\n              return;\n            }\n          }\n        }\n      }\n      RelNode converted =\n          convertMonopole(\n              aggregate,\n              argListSets.iterator().next());\n      call.transformTo(converted);\n      return;\n    }\n  }"
        ],
        [
            "HiveOnTezCostModel::TezMapJoinAlgorithm::getCumulativeMemoryWithinPhaseSplit(HiveJoin)",
            " 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 -\n 336  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      // Check streaming side\n      RelNode inMemoryInput;\n      if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n        inMemoryInput = join.getRight();\n      } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n        inMemoryInput = join.getLeft();\n      } else {\n        return null;\n      }\n      // If simple map join, the whole relation goes in memory\n      return RelMetadataQuery.instance().cumulativeMemoryWithinPhase(inMemoryInput);\n    }",
            " 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336 +\n 337  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      // Check streaming side\n      RelNode inMemoryInput;\n      if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n        inMemoryInput = join.getRight();\n      } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n        inMemoryInput = join.getLeft();\n      } else {\n        return null;\n      }\n      // If simple map join, the whole relation goes in memory\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      return mq.cumulativeMemoryWithinPhase(inMemoryInput);\n    }"
        ],
        [
            "HiveFilterSetOpTransposeRule::onMatch(RelOptRuleCall)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  ",
            "  public void onMatch(RelOptRuleCall call) {\n    Filter filterRel = call.rel(0);\n    SetOp setOp = call.rel(1);\n\n    RexNode condition = filterRel.getCondition();\n\n    // create filters on top of each setop child, modifying the filter\n    // condition to reference each setop child\n    RexBuilder rexBuilder = filterRel.getCluster().getRexBuilder();\n    final RelBuilder relBuilder = call.builder();\n    List<RelDataTypeField> origFields = setOp.getRowType().getFieldList();\n    int[] adjustments = new int[origFields.size()];\n    final List<RelNode> newSetOpInputs = new ArrayList<>();\n    RelNode lastInput = null;\n    for (int index = 0; index < setOp.getInputs().size(); index++) {\n      RelNode input = setOp.getInput(index);\n      RexNode newCondition = condition.accept(new RelOptUtil.RexInputConverter(rexBuilder,\n          origFields, input.getRowType().getFieldList(), adjustments));\n      if (setOp instanceof Union && setOp.all) {\n        final RelMetadataQuery mq = RelMetadataQuery.instance();\n        final RelOptPredicateList predicates = mq.getPulledUpPredicates(input);\n        if (predicates != null) {\n          ImmutableList.Builder<RexNode> listBuilder = ImmutableList.builder();\n          listBuilder.addAll(predicates.pulledUpPredicates);\n          listBuilder.add(newCondition);\n          RexExecutor executor =\n              Util.first(filterRel.getCluster().getPlanner().getExecutor(), RexUtil.EXECUTOR);\n          final RexSimplify simplify =\n              new RexSimplify(rexBuilder, true, executor);\n          final RexNode x = simplify.simplifyAnds(listBuilder.build());\n          if (x.isAlwaysFalse()) {\n            // this is the last branch, and it is always false\n            // We assume alwaysFalse filter will get pushed down to TS so this\n            // branch so it won't read any data.\n            if (index == setOp.getInputs().size() - 1) {\n              lastInput = relBuilder.push(input).filter(newCondition).build();\n            }\n            // remove this branch\n            continue;\n          }\n        }\n      }\n      newSetOpInputs.add(relBuilder.push(input).filter(newCondition).build());\n    }\n    if (newSetOpInputs.size() > 1) {\n      // create a new setop whose children are the filters created above\n      SetOp newSetOp = setOp.copy(setOp.getTraitSet(), newSetOpInputs);\n      call.transformTo(newSetOp);\n    } else if (newSetOpInputs.size() == 1) {\n      call.transformTo(newSetOpInputs.get(0));\n    } else {\n      // we have to keep at least a branch before we support empty values() in\n      // hive\n      call.transformTo(lastInput);\n    }\n  }",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  ",
            "  public void onMatch(RelOptRuleCall call) {\n    Filter filterRel = call.rel(0);\n    SetOp setOp = call.rel(1);\n\n    RexNode condition = filterRel.getCondition();\n\n    // create filters on top of each setop child, modifying the filter\n    // condition to reference each setop child\n    RexBuilder rexBuilder = filterRel.getCluster().getRexBuilder();\n    final RelBuilder relBuilder = call.builder();\n    List<RelDataTypeField> origFields = setOp.getRowType().getFieldList();\n    int[] adjustments = new int[origFields.size()];\n    final List<RelNode> newSetOpInputs = new ArrayList<>();\n    RelNode lastInput = null;\n    for (int index = 0; index < setOp.getInputs().size(); index++) {\n      RelNode input = setOp.getInput(index);\n      RexNode newCondition = condition.accept(new RelOptUtil.RexInputConverter(rexBuilder,\n          origFields, input.getRowType().getFieldList(), adjustments));\n      if (setOp instanceof Union && setOp.all) {\n        final RelMetadataQuery mq = call.getMetadataQuery();\n        final RelOptPredicateList predicates = mq.getPulledUpPredicates(input);\n        if (predicates != null) {\n          ImmutableList.Builder<RexNode> listBuilder = ImmutableList.builder();\n          listBuilder.addAll(predicates.pulledUpPredicates);\n          listBuilder.add(newCondition);\n          RexExecutor executor =\n              Util.first(filterRel.getCluster().getPlanner().getExecutor(), RexUtil.EXECUTOR);\n          final RexSimplify simplify =\n              new RexSimplify(rexBuilder, true, executor);\n          final RexNode x = simplify.simplifyAnds(listBuilder.build());\n          if (x.isAlwaysFalse()) {\n            // this is the last branch, and it is always false\n            // We assume alwaysFalse filter will get pushed down to TS so this\n            // branch so it won't read any data.\n            if (index == setOp.getInputs().size() - 1) {\n              lastInput = relBuilder.push(input).filter(newCondition).build();\n            }\n            // remove this branch\n            continue;\n          }\n        }\n      }\n      newSetOpInputs.add(relBuilder.push(input).filter(newCondition).build());\n    }\n    if (newSetOpInputs.size() > 1) {\n      // create a new setop whose children are the filters created above\n      SetOp newSetOp = setOp.copy(setOp.getTraitSet(), newSetOpInputs);\n      call.transformTo(newSetOp);\n    } else if (newSetOpInputs.size() == 1) {\n      call.transformTo(newSetOpInputs.get(0));\n    } else {\n      // we have to keep at least a branch before we support empty values() in\n      // hive\n      call.transformTo(lastInput);\n    }\n  }"
        ],
        [
            "HiveOnTezCostModel::TezBucketJoinAlgorithm::getCost(HiveJoin)",
            " 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = algoUtils.computeBucketMapJoinCPUCost(cardinalities, streaming);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      //TODO: No Of buckets is not same as no of splits\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezBucketJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null\n              ? 1 : mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      final double ioCost = algoUtils.computeBucketMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }",
            " 414  \n 415  \n 416 +\n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = algoUtils.computeBucketMapJoinCPUCost(cardinalities, streaming);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      //TODO: No Of buckets is not same as no of splits\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezBucketJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null\n              ? 1 : mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      final double ioCost = algoUtils.computeBucketMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }"
        ],
        [
            "HiveOnTezCostModel::TezBucketJoinAlgorithm::isExecutable(HiveJoin)",
            " 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 -\n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401 -\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  ",
            "    @Override\n    public boolean isExecutable(HiveJoin join) {\n      final Double maxMemory = join.getCluster().getPlanner().getContext().\n              unwrap(HiveAlgorithmsConf.class).getMaxMemory();\n      // Check streaming side\n      RelNode smallInput = join.getStreamingInput();\n      if (smallInput == null) {\n        return false;\n      }\n      // Get key columns\n      JoinPredicateInfo joinPredInfo = join.getJoinPredicateInfo();\n      List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n      // Requirements: for Bucket, bucketed by their keys on both sides and fitting in memory\n      // Obtain number of buckets\n      //TODO: Incase of non bucketed splits would be computed based on data size/max part size\n      // What we need is a way to get buckets not splits\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezBucketJoinAlgorithm.INSTANCE);\n      Integer buckets = RelMetadataQuery.instance().splitCount(smallInput);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (buckets == null) {\n        return false;\n      }\n      if (!HiveAlgorithmsUtil.isFittingIntoMemory(maxMemory, smallInput, buckets)) {\n        return false;\n      }\n      for (int i=0; i<join.getInputs().size(); i++) {\n        RelNode input = join.getInputs().get(i);\n        // Is bucketJoin possible? We need correct bucketing\n        RelDistribution distribution = RelMetadataQuery.instance().distribution(input);\n        if (distribution.getType() != Type.HASH_DISTRIBUTED) {\n          return false;\n        }\n        if (!distribution.getKeys().containsAll(joinKeysInChildren.get(i))) {\n          return false;\n        }\n      }\n      return true;\n    }",
            " 365  \n 366  \n 367 +\n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391 +\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 +\n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  ",
            "    @Override\n    public boolean isExecutable(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      final Double maxMemory = join.getCluster().getPlanner().getContext().\n              unwrap(HiveAlgorithmsConf.class).getMaxMemory();\n      // Check streaming side\n      RelNode smallInput = join.getStreamingInput();\n      if (smallInput == null) {\n        return false;\n      }\n      // Get key columns\n      JoinPredicateInfo joinPredInfo = join.getJoinPredicateInfo();\n      List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n      // Requirements: for Bucket, bucketed by their keys on both sides and fitting in memory\n      // Obtain number of buckets\n      //TODO: Incase of non bucketed splits would be computed based on data size/max part size\n      // What we need is a way to get buckets not splits\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezBucketJoinAlgorithm.INSTANCE);\n      Integer buckets = mq.splitCount(smallInput);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (buckets == null) {\n        return false;\n      }\n      if (!HiveAlgorithmsUtil.isFittingIntoMemory(maxMemory, smallInput, buckets)) {\n        return false;\n      }\n      for (int i=0; i<join.getInputs().size(); i++) {\n        RelNode input = join.getInputs().get(i);\n        // Is bucketJoin possible? We need correct bucketing\n        RelDistribution distribution = mq.distribution(input);\n        if (distribution.getType() != Type.HASH_DISTRIBUTED) {\n          return false;\n        }\n        if (!distribution.getKeys().containsAll(joinKeysInChildren.get(i))) {\n          return false;\n        }\n      }\n      return true;\n    }"
        ],
        [
            "HiveAggregate::isBucketedInput()",
            "  72  \n  73 -\n  74  \n  75  ",
            "  public boolean isBucketedInput() {\n    return RelMetadataQuery.instance().distribution(this.getInput()).getKeys().\n            containsAll(groupSet.asList());\n  }",
            "  72  \n  73 +\n  74 +\n  75  \n  76  ",
            "  public boolean isBucketedInput() {\n    final RelMetadataQuery mq = this.getInput().getCluster().getMetadataQuery();\n    return mq.distribution(this.getInput()).getKeys().\n            containsAll(groupSet.asList());\n  }"
        ],
        [
            "HiveDefaultCostModel::DefaultJoinAlgorithm::getCost(HiveJoin)",
            "  86  \n  87  \n  88 -\n  89  \n  90  \n  91  \n  92  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      double leftRCount = mq.getRowCount(join.getLeft());\n      double rightRCount = mq.getRowCount(join.getRight());\n      return HiveCost.FACTORY.makeCost(leftRCount + rightRCount, 0.0, 0.0);\n    }",
            "  86  \n  87  \n  88 +\n  89  \n  90  \n  91  \n  92  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      double leftRCount = mq.getRowCount(join.getLeft());\n      double rightRCount = mq.getRowCount(join.getRight());\n      return HiveCost.FACTORY.makeCost(leftRCount + rightRCount, 0.0, 0.0);\n    }"
        ],
        [
            "HiveOnTezCostModel::TezSMBJoinAlgorithm::getCost(HiveJoin)",
            " 571  \n 572  \n 573 -\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = HiveAlgorithmsUtil.computeSMBMapJoinCPUCost(cardinalities);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n\n      // TODO: Split count is not the same as no of buckets\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezSMBJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null ? 1 : mq\n          .splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      final double ioCost = algoUtils.computeSMBMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }",
            " 574  \n 575  \n 576 +\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = HiveAlgorithmsUtil.computeSMBMapJoinCPUCost(cardinalities);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n\n      // TODO: Split count is not the same as no of buckets\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezSMBJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null ? 1 : mq\n          .splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      final double ioCost = algoUtils.computeSMBMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }"
        ],
        [
            "HiveOnTezCostModel::TezBucketJoinAlgorithm::getCumulativeMemoryWithinPhaseSplit(HiveJoin)",
            " 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499 -\n 500 -\n 501 -\n 502  \n 503  \n 504  \n 505  \n 506  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      // Check streaming side\n      RelNode inMemoryInput;\n      if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n        inMemoryInput = join.getRight();\n      } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n        inMemoryInput = join.getLeft();\n      } else {\n        return null;\n      }\n      // If bucket map join, only a split goes in memory\n      final Double memoryInput =\n              RelMetadataQuery.instance().cumulativeMemoryWithinPhase(inMemoryInput);\n      final Integer splitCount = RelMetadataQuery.instance().splitCount(inMemoryInput);\n      if (memoryInput == null || splitCount == null) {\n        return null;\n      }\n      return memoryInput / splitCount;\n    }",
            " 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 +\n 502 +\n 503 +\n 504  \n 505  \n 506  \n 507  \n 508  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      // Check streaming side\n      RelNode inMemoryInput;\n      if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n        inMemoryInput = join.getRight();\n      } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n        inMemoryInput = join.getLeft();\n      } else {\n        return null;\n      }\n      // If bucket map join, only a split goes in memory\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      final Double memoryInput = mq.cumulativeMemoryWithinPhase(inMemoryInput);\n      final Integer splitCount = mq.splitCount(inMemoryInput);\n      if (memoryInput == null || splitCount == null) {\n        return null;\n      }\n      return memoryInput / splitCount;\n    }"
        ],
        [
            "HiveOnTezCostModel::getAggregateCost(HiveAggregate)",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @Override\n  public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n    if (aggregate.isBucketedInput()) {\n      return HiveCost.FACTORY.makeZeroCost();\n    } else {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // 1. Sum of input cardinalities\n      final Double rCount = mq.getRowCount(aggregate.getInput());\n      if (rCount == null) {\n        return null;\n      }\n      // 2. CPU cost = sorting cost\n      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n      // 3. IO cost = cost of writing intermediary results to local FS +\n      //              cost of reading from local FS for transferring to GBy +\n      //              cost of transferring map outputs to GBy operator\n      final Double rAverageSize = mq.getAverageRowSize(aggregate.getInput());\n      if (rAverageSize == null) {\n        return null;\n      }\n      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double,Double>(rCount,rAverageSize));\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }\n  }",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  @Override\n  public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n    if (aggregate.isBucketedInput()) {\n      return HiveCost.FACTORY.makeZeroCost();\n    } else {\n      final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n      // 1. Sum of input cardinalities\n      final Double rCount = mq.getRowCount(aggregate.getInput());\n      if (rCount == null) {\n        return null;\n      }\n      // 2. CPU cost = sorting cost\n      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n      // 3. IO cost = cost of writing intermediary results to local FS +\n      //              cost of reading from local FS for transferring to GBy +\n      //              cost of transferring map outputs to GBy operator\n      final Double rAverageSize = mq.getAverageRowSize(aggregate.getInput());\n      if (rAverageSize == null) {\n        return null;\n      }\n      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double,Double>(rCount,rAverageSize));\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }\n  }"
        ],
        [
            "HiveAlgorithmsUtil::getSplitCountWithRepartition(HiveJoin)",
            " 337  \n 338  \n 339  \n 340  \n 341 -\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  ",
            "  public static Integer getSplitCountWithRepartition(HiveJoin join) {\n    final Double maxSplitSize = join.getCluster().getPlanner().getContext().\n            unwrap(HiveAlgorithmsConf.class).getMaxSplitSize();\n    // We repartition: new number of splits\n    RelMetadataQuery mq = RelMetadataQuery.instance();\n    final Double averageRowSize = mq.getAverageRowSize(join);\n    final Double rowCount = mq.getRowCount(join);\n    if (averageRowSize == null || rowCount == null) {\n      return null;\n    }\n    final Double totalSize = averageRowSize * rowCount;\n    final Double splitCount = totalSize / maxSplitSize;\n    return splitCount.intValue();\n  }",
            " 338  \n 339  \n 340  \n 341  \n 342 +\n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  ",
            "  public static Integer getSplitCountWithRepartition(HiveJoin join) {\n    final Double maxSplitSize = join.getCluster().getPlanner().getContext().\n            unwrap(HiveAlgorithmsConf.class).getMaxSplitSize();\n    // We repartition: new number of splits\n    final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n    final Double averageRowSize = mq.getAverageRowSize(join);\n    final Double rowCount = mq.getRowCount(join);\n    if (averageRowSize == null || rowCount == null) {\n      return null;\n    }\n    final Double totalSize = averageRowSize * rowCount;\n    final Double splitCount = totalSize / maxSplitSize;\n    return splitCount.intValue();\n  }"
        ],
        [
            "HiveSortLimitPullUpConstantsRule::onMatch(RelOptRuleCall)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final RelNode parent = call.rel(0);\n    final Sort sort = call.rel(1);\n\n    final int count = sort.getInput().getRowType().getFieldCount();\n    if (count == 1) {\n      // No room for optimization since we cannot convert to an empty\n      // Project operator.\n      return;\n    }\n\n    final RexBuilder rexBuilder = sort.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = RelMetadataQuery.instance();\n    final RelOptPredicateList predicates = mq.getPulledUpPredicates(sort.getInput());\n    if (predicates == null) {\n      return;\n    }\n\n    Map<RexNode, RexNode> conditionsExtracted = HiveReduceExpressionsRule.predicateConstants(\n            RexNode.class, rexBuilder, predicates);\n    Map<RexNode, RexNode> constants = new HashMap<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(sort.getInput(), i);\n      if (conditionsExtracted.containsKey(expr)) {\n        constants.put(expr, conditionsExtracted.get(expr));\n      }\n    }\n\n    // None of the expressions are constant. Nothing to do.\n    if (constants.isEmpty()) {\n      return;\n    }\n\n    if (count == constants.size()) {\n      // At least a single item in project is required.\n      constants.remove(constants.keySet().iterator().next());\n    }\n\n    // Create expressions for Project operators before and after the Sort\n    List<RelDataTypeField> fields = sort.getInput().getRowType().getFieldList();\n    List<Pair<RexNode, String>> newChildExprs = new ArrayList<>();\n    List<RexNode> topChildExprs = new ArrayList<>();\n    List<String> topChildExprsFields = new ArrayList<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(sort.getInput(), i);\n      RelDataTypeField field = fields.get(i);\n      if (constants.containsKey(expr)) {\n        topChildExprs.add(constants.get(expr));\n        topChildExprsFields.add(field.getName());\n      } else {\n        newChildExprs.add(Pair.<RexNode,String>of(expr, field.getName()));\n        topChildExprs.add(expr);\n        topChildExprsFields.add(field.getName());\n      }\n    }\n\n    // Update field collations\n    final Mappings.TargetMapping mapping =\n            RelOptUtil.permutation(Pair.left(newChildExprs), sort.getInput().getRowType()).inverse();\n    List<RelFieldCollation> fieldCollations = new ArrayList<>();\n    for (RelFieldCollation fc : sort.getCollation().getFieldCollations()) {\n      final int target = mapping.getTargetOpt(fc.getFieldIndex());\n      if (target < 0) {\n        // It is a constant, we can ignore it\n        continue;\n      }\n      fieldCollations.add(fc.copy(target));\n    }\n\n    // Update top Project positions\n    topChildExprs = ImmutableList.copyOf(RexUtil.apply(mapping, topChildExprs));\n\n    // Create new Project-Sort-Project sequence\n    final RelBuilder relBuilder = call.builder();\n    relBuilder.push(sort.getInput());\n    relBuilder.project(Pair.left(newChildExprs), Pair.right(newChildExprs));\n    final ImmutableList<RexNode> sortFields =\n            relBuilder.fields(RelCollations.of(fieldCollations));\n    relBuilder.sortLimit(sort.offset == null ? -1 : RexLiteral.intValue(sort.offset),\n            sort.fetch == null ? -1 : RexLiteral.intValue(sort.fetch), sortFields);\n    // Create top Project fixing nullability of fields\n    relBuilder.project(topChildExprs, topChildExprsFields);\n    relBuilder.convert(sort.getRowType(), false);\n\n    List<RelNode> inputs = new ArrayList<>();\n    for (RelNode child : parent.getInputs()) {\n      if (!((HepRelVertex) child).getCurrentRel().equals(sort)) {\n        inputs.add(child);\n      } else {\n        inputs.add(relBuilder.build());\n      }\n    }\n    call.transformTo(parent.copy(parent.getTraitSet(), inputs));\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final RelNode parent = call.rel(0);\n    final Sort sort = call.rel(1);\n\n    final int count = sort.getInput().getRowType().getFieldCount();\n    if (count == 1) {\n      // No room for optimization since we cannot convert to an empty\n      // Project operator.\n      return;\n    }\n\n    final RexBuilder rexBuilder = sort.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    final RelOptPredicateList predicates = mq.getPulledUpPredicates(sort.getInput());\n    if (predicates == null) {\n      return;\n    }\n\n    Map<RexNode, RexNode> conditionsExtracted = HiveReduceExpressionsRule.predicateConstants(\n            RexNode.class, rexBuilder, predicates);\n    Map<RexNode, RexNode> constants = new HashMap<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(sort.getInput(), i);\n      if (conditionsExtracted.containsKey(expr)) {\n        constants.put(expr, conditionsExtracted.get(expr));\n      }\n    }\n\n    // None of the expressions are constant. Nothing to do.\n    if (constants.isEmpty()) {\n      return;\n    }\n\n    if (count == constants.size()) {\n      // At least a single item in project is required.\n      constants.remove(constants.keySet().iterator().next());\n    }\n\n    // Create expressions for Project operators before and after the Sort\n    List<RelDataTypeField> fields = sort.getInput().getRowType().getFieldList();\n    List<Pair<RexNode, String>> newChildExprs = new ArrayList<>();\n    List<RexNode> topChildExprs = new ArrayList<>();\n    List<String> topChildExprsFields = new ArrayList<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(sort.getInput(), i);\n      RelDataTypeField field = fields.get(i);\n      if (constants.containsKey(expr)) {\n        topChildExprs.add(constants.get(expr));\n        topChildExprsFields.add(field.getName());\n      } else {\n        newChildExprs.add(Pair.<RexNode,String>of(expr, field.getName()));\n        topChildExprs.add(expr);\n        topChildExprsFields.add(field.getName());\n      }\n    }\n\n    // Update field collations\n    final Mappings.TargetMapping mapping =\n            RelOptUtil.permutation(Pair.left(newChildExprs), sort.getInput().getRowType()).inverse();\n    List<RelFieldCollation> fieldCollations = new ArrayList<>();\n    for (RelFieldCollation fc : sort.getCollation().getFieldCollations()) {\n      final int target = mapping.getTargetOpt(fc.getFieldIndex());\n      if (target < 0) {\n        // It is a constant, we can ignore it\n        continue;\n      }\n      fieldCollations.add(fc.copy(target));\n    }\n\n    // Update top Project positions\n    topChildExprs = ImmutableList.copyOf(RexUtil.apply(mapping, topChildExprs));\n\n    // Create new Project-Sort-Project sequence\n    final RelBuilder relBuilder = call.builder();\n    relBuilder.push(sort.getInput());\n    relBuilder.project(Pair.left(newChildExprs), Pair.right(newChildExprs));\n    final ImmutableList<RexNode> sortFields =\n            relBuilder.fields(RelCollations.of(fieldCollations));\n    relBuilder.sortLimit(sort.offset == null ? -1 : RexLiteral.intValue(sort.offset),\n            sort.fetch == null ? -1 : RexLiteral.intValue(sort.fetch), sortFields);\n    // Create top Project fixing nullability of fields\n    relBuilder.project(topChildExprs, topChildExprsFields);\n    relBuilder.convert(sort.getRowType(), false);\n\n    List<RelNode> inputs = new ArrayList<>();\n    for (RelNode child : parent.getInputs()) {\n      if (!((HepRelVertex) child).getCurrentRel().equals(sort)) {\n        inputs.add(child);\n      } else {\n        inputs.add(relBuilder.build());\n      }\n    }\n    call.transformTo(parent.copy(parent.getTraitSet(), inputs));\n  }"
        ],
        [
            "HiveSortJoinReduceRule::matches(RelOptRuleCall)",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  ",
            "  @Override\n  public boolean matches(RelOptRuleCall call) {\n    final HiveSortLimit sortLimit = call.rel(0);\n    final HiveJoin join = call.rel(1);\n\n    // If sort does not contain a limit operation or limit is 0, we bail out\n    if (!HiveCalciteUtil.limitRelNode(sortLimit) ||\n            RexLiteral.intValue(sortLimit.fetch) == 0) {\n      return false;\n    }\n\n    // 1) If join is not a left or right outer, we bail out\n    // 2) If any sort column is not part of the input where the\n    // sort is pushed, we bail out\n    RelNode reducedInput;\n    if (join.getJoinType() == JoinRelType.LEFT) {\n      reducedInput = join.getLeft();\n      if (sortLimit.getCollation() != RelCollations.EMPTY) {\n        for (RelFieldCollation relFieldCollation\n            : sortLimit.getCollation().getFieldCollations()) {\n          if (relFieldCollation.getFieldIndex()\n              >= join.getLeft().getRowType().getFieldCount()) {\n            return false;\n          }\n        }\n      }\n    } else if (join.getJoinType() == JoinRelType.RIGHT) {\n      reducedInput = join.getRight();\n      if (sortLimit.getCollation() != RelCollations.EMPTY) {\n        for (RelFieldCollation relFieldCollation\n            : sortLimit.getCollation().getFieldCollations()) {\n          if (relFieldCollation.getFieldIndex()\n              < join.getLeft().getRowType().getFieldCount()) {\n            return false;\n          }\n        }\n      }\n    } else {\n      return false;\n    }\n\n    // Finally, if we do not reduce the input size, we bail out\n    final int offset = sortLimit.offset == null ? 0 : RexLiteral.intValue(sortLimit.offset);\n    if (offset + RexLiteral.intValue(sortLimit.fetch)\n            >= RelMetadataQuery.instance().getRowCount(reducedInput)) {\n      return false;\n    }\n\n    return true;\n  }",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 +\n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  @Override\n  public boolean matches(RelOptRuleCall call) {\n    final HiveSortLimit sortLimit = call.rel(0);\n    final HiveJoin join = call.rel(1);\n\n    // If sort does not contain a limit operation or limit is 0, we bail out\n    if (!HiveCalciteUtil.limitRelNode(sortLimit) ||\n            RexLiteral.intValue(sortLimit.fetch) == 0) {\n      return false;\n    }\n\n    // 1) If join is not a left or right outer, we bail out\n    // 2) If any sort column is not part of the input where the\n    // sort is pushed, we bail out\n    RelNode reducedInput;\n    if (join.getJoinType() == JoinRelType.LEFT) {\n      reducedInput = join.getLeft();\n      if (sortLimit.getCollation() != RelCollations.EMPTY) {\n        for (RelFieldCollation relFieldCollation\n            : sortLimit.getCollation().getFieldCollations()) {\n          if (relFieldCollation.getFieldIndex()\n              >= join.getLeft().getRowType().getFieldCount()) {\n            return false;\n          }\n        }\n      }\n    } else if (join.getJoinType() == JoinRelType.RIGHT) {\n      reducedInput = join.getRight();\n      if (sortLimit.getCollation() != RelCollations.EMPTY) {\n        for (RelFieldCollation relFieldCollation\n            : sortLimit.getCollation().getFieldCollations()) {\n          if (relFieldCollation.getFieldIndex()\n              < join.getLeft().getRowType().getFieldCount()) {\n            return false;\n          }\n        }\n      }\n    } else {\n      return false;\n    }\n\n    // Finally, if we do not reduce the input size, we bail out\n    final int offset = sortLimit.offset == null ? 0 : RexLiteral.intValue(sortLimit.offset);\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    if (offset + RexLiteral.intValue(sortLimit.fetch)\n            >= mq.getRowCount(reducedInput)) {\n      return false;\n    }\n\n    return true;\n  }"
        ],
        [
            "HiveSortRemoveRule::matches(RelOptRuleCall)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "  @Override\n  public boolean matches(RelOptRuleCall call) {\n    final HiveSortLimit sortLimit = call.rel(0);\n\n    // If it is not created by HiveSortJoinReduceRule, we cannot remove it\n    if (!sortLimit.isRuleCreated()) {\n      return false;\n    }\n\n    // Finally, if we do not reduce the size input enough, we bail out\n    int limit = RexLiteral.intValue(sortLimit.fetch);\n    Double rowCount = RelMetadataQuery.instance().getRowCount(sortLimit.getInput());\n    if (rowCount != null && limit <= reductionProportion * rowCount &&\n            rowCount - limit >= reductionTuples) {\n      return false;\n    }\n\n    return true;\n  }",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "  @Override\n  public boolean matches(RelOptRuleCall call) {\n    final HiveSortLimit sortLimit = call.rel(0);\n\n    // If it is not created by HiveSortJoinReduceRule, we cannot remove it\n    if (!sortLimit.isRuleCreated()) {\n      return false;\n    }\n\n    // Finally, if we do not reduce the size input enough, we bail out\n    int limit = RexLiteral.intValue(sortLimit.fetch);\n    Double rowCount = call.getMetadataQuery().getRowCount(sortLimit.getInput());\n    if (rowCount != null && limit <= reductionProportion * rowCount &&\n            rowCount - limit >= reductionTuples) {\n      return false;\n    }\n\n    return true;\n  }"
        ],
        [
            "HiveReduceExpressionsWithStatsRule::onMatch(RelOptRuleCall)",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Filter filter = call.rel(0);\n\n    final RexBuilder rexBuilder = filter.getCluster().getRexBuilder();\n    final RelMetadataQuery metadataProvider = RelMetadataQuery.instance();\n\n    // 1. Recompose filter possibly by pulling out common elements from DNF\n    // expressions\n    RexNode newFilterCondition = RexUtil.pullFactors(rexBuilder, filter.getCondition());\n\n    // 2. Reduce filter with stats information\n    RexReplacer replacer = new RexReplacer(filter, rexBuilder, metadataProvider);\n    newFilterCondition = replacer.apply(newFilterCondition);\n\n    // 3. Transform if we have created a new filter operator\n    if (!filter.getCondition().toString().equals(newFilterCondition.toString())) {\n      Filter newFilter = filter.copy(filter.getTraitSet(), filter.getInput(), newFilterCondition);\n      call.transformTo(newFilter);\n    }\n\n  }",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Filter filter = call.rel(0);\n\n    final RexBuilder rexBuilder = filter.getCluster().getRexBuilder();\n    final RelMetadataQuery metadataProvider = call.getMetadataQuery();\n\n    // 1. Recompose filter possibly by pulling out common elements from DNF\n    // expressions\n    RexNode newFilterCondition = RexUtil.pullFactors(rexBuilder, filter.getCondition());\n\n    // 2. Reduce filter with stats information\n    RexReplacer replacer = new RexReplacer(filter, rexBuilder, metadataProvider);\n    newFilterCondition = replacer.apply(newFilterCondition);\n\n    // 3. Transform if we have created a new filter operator\n    if (!filter.getCondition().toString().equals(newFilterCondition.toString())) {\n      Filter newFilter = filter.copy(filter.getTraitSet(), filter.getInput(), newFilterCondition);\n      call.transformTo(newFilter);\n    }\n\n  }"
        ],
        [
            "HiveReduceExpressionsRule::FilterReduceExpressionsRule::onMatch(RelOptRuleCall)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    @Override public void onMatch(RelOptRuleCall call) {\n      final Filter filter = call.rel(0);\n      final List<RexNode> expList =\n          Lists.newArrayList(filter.getCondition());\n      RexNode newConditionExp;\n      boolean reduced;\n      final RelMetadataQuery mq = RelMetadataQuery.instance();\n      final RelOptPredicateList predicates =\n          mq.getPulledUpPredicates(filter.getInput());\n      if (reduceExpressions(filter, expList, predicates, true)) {\n        assert expList.size() == 1;\n        newConditionExp = expList.get(0);\n        reduced = true;\n      } else {\n        // No reduction, but let's still test the original\n        // predicate to see if it was already a constant,\n        // in which case we don't need any runtime decision\n        // about filtering.\n        newConditionExp = filter.getCondition();\n        reduced = false;\n      }\n\n      // Even if no reduction, let's still test the original\n      // predicate to see if it was already a constant,\n      // in which case we don't need any runtime decision\n      // about filtering.\n      if (newConditionExp.isAlwaysTrue()) {\n        call.transformTo(\n            filter.getInput());\n      } else if (reduced) {\n        if (RexUtil.isNullabilityCast(filter.getCluster().getTypeFactory(),\n            newConditionExp)) {\n          newConditionExp = ((RexCall) newConditionExp).getOperands().get(0);\n        }\n        // reduce might end up creating an expression with null type\n        // e.g condition(null = null) is reduced to condition (null) with null type\n        // since this is a condition which will always be boolean type we cast it to\n        // boolean type\n        if(newConditionExp.getType().getSqlTypeName() == SqlTypeName.NULL) {\n          newConditionExp = call.builder().cast(newConditionExp, SqlTypeName.BOOLEAN);\n        }\n        call.transformTo(call.builder().\n            push(filter.getInput()).filter(newConditionExp).build());\n      } else {\n        if (newConditionExp instanceof RexCall) {\n          RexCall rexCall = (RexCall) newConditionExp;\n          boolean reverse = rexCall.getKind() == SqlKind.NOT;\n          if (reverse) {\n            if (!(rexCall.getOperands().get(0) instanceof RexCall)) {\n              // If child is not a RexCall instance, we can bail out\n              return;\n            }\n            rexCall = (RexCall) rexCall.getOperands().get(0);\n          }\n          reduceNotNullableFilter(call, filter, rexCall, reverse);\n        }\n        return;\n      }\n\n      // New plan is absolutely better than old plan.\n      call.getPlanner().setImportance(filter, 0.0);\n    }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "    @Override public void onMatch(RelOptRuleCall call) {\n      final Filter filter = call.rel(0);\n      final List<RexNode> expList =\n          Lists.newArrayList(filter.getCondition());\n      RexNode newConditionExp;\n      boolean reduced;\n      final RelMetadataQuery mq = call.getMetadataQuery();\n      final RelOptPredicateList predicates =\n          mq.getPulledUpPredicates(filter.getInput());\n      if (reduceExpressions(filter, expList, predicates, true)) {\n        assert expList.size() == 1;\n        newConditionExp = expList.get(0);\n        reduced = true;\n      } else {\n        // No reduction, but let's still test the original\n        // predicate to see if it was already a constant,\n        // in which case we don't need any runtime decision\n        // about filtering.\n        newConditionExp = filter.getCondition();\n        reduced = false;\n      }\n\n      // Even if no reduction, let's still test the original\n      // predicate to see if it was already a constant,\n      // in which case we don't need any runtime decision\n      // about filtering.\n      if (newConditionExp.isAlwaysTrue()) {\n        call.transformTo(\n            filter.getInput());\n      } else if (reduced) {\n        if (RexUtil.isNullabilityCast(filter.getCluster().getTypeFactory(),\n            newConditionExp)) {\n          newConditionExp = ((RexCall) newConditionExp).getOperands().get(0);\n        }\n        // reduce might end up creating an expression with null type\n        // e.g condition(null = null) is reduced to condition (null) with null type\n        // since this is a condition which will always be boolean type we cast it to\n        // boolean type\n        if(newConditionExp.getType().getSqlTypeName() == SqlTypeName.NULL) {\n          newConditionExp = call.builder().cast(newConditionExp, SqlTypeName.BOOLEAN);\n        }\n        call.transformTo(call.builder().\n            push(filter.getInput()).filter(newConditionExp).build());\n      } else {\n        if (newConditionExp instanceof RexCall) {\n          RexCall rexCall = (RexCall) newConditionExp;\n          boolean reverse = rexCall.getKind() == SqlKind.NOT;\n          if (reverse) {\n            if (!(rexCall.getOperands().get(0) instanceof RexCall)) {\n              // If child is not a RexCall instance, we can bail out\n              return;\n            }\n            rexCall = (RexCall) rexCall.getOperands().get(0);\n          }\n          reduceNotNullableFilter(call, filter, rexCall, reverse);\n        }\n        return;\n      }\n\n      // New plan is absolutely better than old plan.\n      call.getPlanner().setImportance(filter, 0.0);\n    }"
        ],
        [
            "HiveRelDecorrelator::RemoveCorrelationForScalarAggregateRule::onMatch(RelOptRuleCall)",
            "2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512 -\n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591 -\n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  ",
            "    public void onMatch(RelOptRuleCall call) {\n      final LogicalCorrelate correlate = call.rel(0);\n      final RelNode left = call.rel(1);\n      final LogicalProject aggOutputProject = call.rel(2);\n      final LogicalAggregate aggregate = call.rel(3);\n      final LogicalProject aggInputProject = call.rel(4);\n      RelNode right = call.rel(5);\n      final RelOptCluster cluster = correlate.getCluster();\n\n      setCurrent(call.getPlanner().getRoot(), correlate);\n\n      // check for this pattern\n      // The pattern matching could be simplified if rules can be applied\n      // during decorrelation,\n      //\n      // CorrelateRel(left correlation, condition = true)\n      //   LeftInputRel\n      //   LogicalProject-A (a RexNode)\n      //     LogicalAggregate (groupby (0), agg0(), agg1()...)\n      //       LogicalProject-B (references coVar)\n      //         rightInputRel\n\n      // check aggOutputProject projects only one expression\n      final List<RexNode> aggOutputProjects = aggOutputProject.getProjects();\n      if (aggOutputProjects.size() != 1) {\n        return;\n      }\n\n      final JoinRelType joinType = correlate.getJoinType().toJoinType();\n      // corRel.getCondition was here, however Correlate was updated so it\n      // never includes a join condition. The code was not modified for brevity.\n      RexNode joinCond = rexBuilder.makeLiteral(true);\n      if ((joinType != JoinRelType.LEFT)\n              || (joinCond != rexBuilder.makeLiteral(true))) {\n        return;\n      }\n\n      // check that the agg is on the entire input\n      if (!aggregate.getGroupSet().isEmpty()) {\n        return;\n      }\n\n      final List<RexNode> aggInputProjects = aggInputProject.getProjects();\n\n      final List<AggregateCall> aggCalls = aggregate.getAggCallList();\n      final Set<Integer> isCountStar = Sets.newHashSet();\n\n      // mark if agg produces count(*) which needs to reference the\n      // nullIndicator after the transformation.\n      int k = -1;\n      for (AggregateCall aggCall : aggCalls) {\n        ++k;\n        if ((aggCall.getAggregation() instanceof SqlCountAggFunction)\n                && (aggCall.getArgList().size() == 0)) {\n          isCountStar.add(k);\n        }\n      }\n\n      if ((right instanceof LogicalFilter)\n              && cm.mapRefRelToCorRef.containsKey(right)) {\n        // rightInputRel has this shape:\n        //\n        //       LogicalFilter (references corvar)\n        //         FilterInputRel\n        LogicalFilter filter = (LogicalFilter) right;\n        right = filter.getInput();\n\n        assert right instanceof HepRelVertex;\n        right = ((HepRelVertex) right).getCurrentRel();\n\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // check filter condition type First extract the correlation out\n        // of the filter\n\n        // First breaking up the filter conditions into equality\n        // comparisons between rightJoinKeys(from the original\n        // filterInputRel) and correlatedJoinKeys. correlatedJoinKeys\n        // can only be RexFieldAccess, while rightJoinKeys can be\n        // expressions. These comparisons are AND'ed together.\n        List<RexNode> rightJoinKeys = Lists.newArrayList();\n        List<RexNode> tmpCorrelatedJoinKeys = Lists.newArrayList();\n        RelOptUtil.splitCorrelatedFilterCondition(\n                filter,\n                rightJoinKeys,\n                tmpCorrelatedJoinKeys,\n                true);\n\n        // make sure the correlated reference forms a unique key check\n        // that the columns referenced in these comparisons form an\n        // unique key of the leftInputRel\n        List<RexFieldAccess> correlatedJoinKeys = Lists.newArrayList();\n        List<RexInputRef> correlatedInputRefJoinKeys = Lists.newArrayList();\n        for (RexNode joinKey : tmpCorrelatedJoinKeys) {\n          assert joinKey instanceof RexFieldAccess;\n          correlatedJoinKeys.add((RexFieldAccess) joinKey);\n          RexNode correlatedInputRef =\n                  removeCorrelationExpr(joinKey, false);\n          assert correlatedInputRef instanceof RexInputRef;\n          correlatedInputRefJoinKeys.add(\n                  (RexInputRef) correlatedInputRef);\n        }\n\n        // check that the columns referenced in rightJoinKeys form an\n        // unique key of the filterInputRel\n        if (correlatedInputRefJoinKeys.isEmpty()) {\n          return;\n        }\n\n        // The join filters out the nulls.  So, it's ok if there are\n        // nulls in the join keys.\n        final RelMetadataQuery mq = RelMetadataQuery.instance();\n        if (!RelMdUtil.areColumnsDefinitelyUniqueWhenNullsFiltered(mq, left,\n                correlatedInputRefJoinKeys)) {\n          //SQL2REL_LOGGER.fine(correlatedJoinKeys.toString()\n           //       + \"are not unique keys for \"\n            //      + left.toString());\n          return;\n        }\n\n        // check cor var references are valid\n        if (!checkCorVars(correlate,\n                aggInputProject,\n                filter,\n                correlatedJoinKeys)) {\n          return;\n        }\n\n        // Rewrite the above plan:\n        //\n        // CorrelateRel(left correlation, condition = true)\n        //   LeftInputRel\n        //   LogicalProject-A (a RexNode)\n        //     LogicalAggregate (groupby(0), agg0(),agg1()...)\n        //       LogicalProject-B (may reference coVar)\n        //         LogicalFilter (references corVar)\n        //           RightInputRel (no correlated reference)\n        //\n\n        // to this plan:\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs)\n        //                 agg0(rewritten expression),\n        //                 agg1()...)\n        //     LogicalProject-B' (rewriten original projected exprs)\n        //       LogicalJoin(replace corvar w/ input ref from LeftInputRel)\n        //         LeftInputRel\n        //         RightInputRel\n        //\n\n        // In the case where agg is count(*) or count($corVar), it is\n        // changed to count(nullIndicator).\n        // Note:  any non-nullable field from the RHS can be used as\n        // the indicator however a \"true\" field is added to the\n        // projection list from the RHS for simplicity to avoid\n        // searching for non-null fields.\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs),\n        //                 count(nullIndicator), other aggs...)\n        //     LogicalProject-B' (all left input refs plus\n        //                    the rewritten original projected exprs)\n        //       LogicalJoin(replace corvar to input ref from LeftInputRel)\n        //         LeftInputRel\n        //         LogicalProject (everything from RightInputRel plus\n        //                     the nullIndicator \"true\")\n        //           RightInputRel\n        //\n\n        // first change the filter condition into a join condition\n        joinCond =\n                removeCorrelationExpr(filter.getCondition(), false);\n      } else if (cm.mapRefRelToCorRef.containsKey(aggInputProject)) {\n        // check rightInputRel contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // check cor var references are valid\n        if (!checkCorVars(correlate, aggInputProject, null, null)) {\n          return;\n        }\n\n        int nFields = left.getRowType().getFieldCount();\n        ImmutableBitSet allCols = ImmutableBitSet.range(nFields);\n\n        // leftInputRel contains unique keys\n        // i.e. each row is distinct and can group by on all the left\n        // fields\n        final RelMetadataQuery mq = RelMetadataQuery.instance();\n        if (!RelMdUtil.areColumnsDefinitelyUnique(mq, left, allCols)) {\n          //SQL2REL_LOGGER.fine(\"There are no unique keys for \" + left);\n          return;\n        }\n        //\n        // Rewrite the above plan:\n        //\n        // CorrelateRel(left correlation, condition = true)\n        //   LeftInputRel\n        //   LogicalProject-A (a RexNode)\n        //     LogicalAggregate (groupby(0), agg0(), agg1()...)\n        //       LogicalProject-B (references coVar)\n        //         RightInputRel (no correlated reference)\n        //\n\n        // to this plan:\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs)\n        //                 agg0(rewritten expression),\n        //                 agg1()...)\n        //     LogicalProject-B' (rewriten original projected exprs)\n        //       LogicalJoin (LOJ cond = true)\n        //         LeftInputRel\n        //         RightInputRel\n        //\n\n        // In the case where agg is count($corVar), it is changed to\n        // count(nullIndicator).\n        // Note:  any non-nullable field from the RHS can be used as\n        // the indicator however a \"true\" field is added to the\n        // projection list from the RHS for simplicity to avoid\n        // searching for non-null fields.\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs),\n        //                 count(nullIndicator), other aggs...)\n        //     LogicalProject-B' (all left input refs plus\n        //                    the rewritten original projected exprs)\n        //       LogicalJoin(replace corvar to input ref from LeftInputRel)\n        //         LeftInputRel\n        //         LogicalProject (everything from RightInputRel plus\n        //                     the nullIndicator \"true\")\n        //           RightInputRel\n      } else {\n        return;\n      }\n\n      RelDataType leftInputFieldType = left.getRowType();\n      int leftInputFieldCount = leftInputFieldType.getFieldCount();\n      int joinOutputProjExprCount =\n              leftInputFieldCount + aggInputProjects.size() + 1;\n\n      right =\n              createProjectWithAdditionalExprs(right,\n                      ImmutableList.of(\n                              Pair.<RexNode, String>of(rexBuilder.makeLiteral(true),\n                                      \"nullIndicator\")));\n\n      LogicalJoin join =\n              LogicalJoin.create(left, right, joinCond,\n                      ImmutableSet.<CorrelationId>of(), joinType);\n\n      // To the consumer of joinOutputProjRel, nullIndicator is located\n      // at the end\n      int nullIndicatorPos = join.getRowType().getFieldCount() - 1;\n\n      RexInputRef nullIndicator =\n              new RexInputRef(\n                      nullIndicatorPos,\n                      cluster.getTypeFactory().createTypeWithNullability(\n                              join.getRowType().getFieldList()\n                                      .get(nullIndicatorPos).getType(),\n                              true));\n\n      // first project all group-by keys plus the transformed agg input\n      List<RexNode> joinOutputProjects = Lists.newArrayList();\n\n      // LOJ Join preserves LHS types\n      for (int i = 0; i < leftInputFieldCount; i++) {\n        joinOutputProjects.add(\n                rexBuilder.makeInputRef(\n                        leftInputFieldType.getFieldList().get(i).getType(), i));\n      }\n\n      for (RexNode aggInputProjExpr : aggInputProjects) {\n        joinOutputProjects.add(\n                removeCorrelationExpr(aggInputProjExpr,\n                        joinType.generatesNullsOnRight(),\n                        nullIndicator));\n      }\n\n      joinOutputProjects.add(\n              rexBuilder.makeInputRef(join, nullIndicatorPos));\n\n      RelNode joinOutputProject =\n              RelOptUtil.createProject(\n                      join,\n                      joinOutputProjects,\n                      null);\n\n      // nullIndicator is now at a different location in the output of\n      // the join\n      nullIndicatorPos = joinOutputProjExprCount - 1;\n\n      final int groupCount = leftInputFieldCount;\n\n      List<AggregateCall> newAggCalls = Lists.newArrayList();\n      k = -1;\n      for (AggregateCall aggCall : aggCalls) {\n        ++k;\n        final List<Integer> argList;\n\n        if (isCountStar.contains(k)) {\n          // this is a count(*), transform it to count(nullIndicator)\n          // the null indicator is located at the end\n          argList = Collections.singletonList(nullIndicatorPos);\n        } else {\n          argList = Lists.newArrayList();\n\n          for (int aggArg : aggCall.getArgList()) {\n            argList.add(aggArg + groupCount);\n          }\n        }\n\n        int filterArg = aggCall.filterArg < 0 ? aggCall.filterArg\n                : aggCall.filterArg + groupCount;\n        newAggCalls.add(\n                aggCall.adaptTo(joinOutputProject, argList, filterArg,\n                        aggregate.getGroupCount(), groupCount));\n      }\n\n      ImmutableBitSet groupSet =\n              ImmutableBitSet.range(groupCount);\n      LogicalAggregate newAggregate =\n              LogicalAggregate.create(joinOutputProject,\n                      false,\n                      groupSet,\n                      null,\n                      newAggCalls);\n\n      List<RexNode> newAggOutputProjectList = Lists.newArrayList();\n      for (int i : groupSet) {\n        newAggOutputProjectList.add(\n                rexBuilder.makeInputRef(newAggregate, i));\n      }\n\n      RexNode newAggOutputProjects =\n              removeCorrelationExpr(aggOutputProjects.get(0), false);\n      newAggOutputProjectList.add(\n              rexBuilder.makeCast(\n                      cluster.getTypeFactory().createTypeWithNullability(\n                              newAggOutputProjects.getType(),\n                              true),\n                      newAggOutputProjects));\n\n      RelNode newAggOutputProject =\n              RelOptUtil.createProject(\n                      newAggregate,\n                      newAggOutputProjectList,\n                      null);\n\n      call.transformTo(newAggOutputProject);\n\n      removeCorVarFromTree(correlate);\n    }",
            "2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512 +\n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591 +\n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  ",
            "    public void onMatch(RelOptRuleCall call) {\n      final LogicalCorrelate correlate = call.rel(0);\n      final RelNode left = call.rel(1);\n      final LogicalProject aggOutputProject = call.rel(2);\n      final LogicalAggregate aggregate = call.rel(3);\n      final LogicalProject aggInputProject = call.rel(4);\n      RelNode right = call.rel(5);\n      final RelOptCluster cluster = correlate.getCluster();\n\n      setCurrent(call.getPlanner().getRoot(), correlate);\n\n      // check for this pattern\n      // The pattern matching could be simplified if rules can be applied\n      // during decorrelation,\n      //\n      // CorrelateRel(left correlation, condition = true)\n      //   LeftInputRel\n      //   LogicalProject-A (a RexNode)\n      //     LogicalAggregate (groupby (0), agg0(), agg1()...)\n      //       LogicalProject-B (references coVar)\n      //         rightInputRel\n\n      // check aggOutputProject projects only one expression\n      final List<RexNode> aggOutputProjects = aggOutputProject.getProjects();\n      if (aggOutputProjects.size() != 1) {\n        return;\n      }\n\n      final JoinRelType joinType = correlate.getJoinType().toJoinType();\n      // corRel.getCondition was here, however Correlate was updated so it\n      // never includes a join condition. The code was not modified for brevity.\n      RexNode joinCond = rexBuilder.makeLiteral(true);\n      if ((joinType != JoinRelType.LEFT)\n              || (joinCond != rexBuilder.makeLiteral(true))) {\n        return;\n      }\n\n      // check that the agg is on the entire input\n      if (!aggregate.getGroupSet().isEmpty()) {\n        return;\n      }\n\n      final List<RexNode> aggInputProjects = aggInputProject.getProjects();\n\n      final List<AggregateCall> aggCalls = aggregate.getAggCallList();\n      final Set<Integer> isCountStar = Sets.newHashSet();\n\n      // mark if agg produces count(*) which needs to reference the\n      // nullIndicator after the transformation.\n      int k = -1;\n      for (AggregateCall aggCall : aggCalls) {\n        ++k;\n        if ((aggCall.getAggregation() instanceof SqlCountAggFunction)\n                && (aggCall.getArgList().size() == 0)) {\n          isCountStar.add(k);\n        }\n      }\n\n      if ((right instanceof LogicalFilter)\n              && cm.mapRefRelToCorRef.containsKey(right)) {\n        // rightInputRel has this shape:\n        //\n        //       LogicalFilter (references corvar)\n        //         FilterInputRel\n        LogicalFilter filter = (LogicalFilter) right;\n        right = filter.getInput();\n\n        assert right instanceof HepRelVertex;\n        right = ((HepRelVertex) right).getCurrentRel();\n\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // check filter condition type First extract the correlation out\n        // of the filter\n\n        // First breaking up the filter conditions into equality\n        // comparisons between rightJoinKeys(from the original\n        // filterInputRel) and correlatedJoinKeys. correlatedJoinKeys\n        // can only be RexFieldAccess, while rightJoinKeys can be\n        // expressions. These comparisons are AND'ed together.\n        List<RexNode> rightJoinKeys = Lists.newArrayList();\n        List<RexNode> tmpCorrelatedJoinKeys = Lists.newArrayList();\n        RelOptUtil.splitCorrelatedFilterCondition(\n                filter,\n                rightJoinKeys,\n                tmpCorrelatedJoinKeys,\n                true);\n\n        // make sure the correlated reference forms a unique key check\n        // that the columns referenced in these comparisons form an\n        // unique key of the leftInputRel\n        List<RexFieldAccess> correlatedJoinKeys = Lists.newArrayList();\n        List<RexInputRef> correlatedInputRefJoinKeys = Lists.newArrayList();\n        for (RexNode joinKey : tmpCorrelatedJoinKeys) {\n          assert joinKey instanceof RexFieldAccess;\n          correlatedJoinKeys.add((RexFieldAccess) joinKey);\n          RexNode correlatedInputRef =\n                  removeCorrelationExpr(joinKey, false);\n          assert correlatedInputRef instanceof RexInputRef;\n          correlatedInputRefJoinKeys.add(\n                  (RexInputRef) correlatedInputRef);\n        }\n\n        // check that the columns referenced in rightJoinKeys form an\n        // unique key of the filterInputRel\n        if (correlatedInputRefJoinKeys.isEmpty()) {\n          return;\n        }\n\n        // The join filters out the nulls.  So, it's ok if there are\n        // nulls in the join keys.\n        final RelMetadataQuery mq = call.getMetadataQuery();\n        if (!RelMdUtil.areColumnsDefinitelyUniqueWhenNullsFiltered(mq, left,\n                correlatedInputRefJoinKeys)) {\n          //SQL2REL_LOGGER.fine(correlatedJoinKeys.toString()\n           //       + \"are not unique keys for \"\n            //      + left.toString());\n          return;\n        }\n\n        // check cor var references are valid\n        if (!checkCorVars(correlate,\n                aggInputProject,\n                filter,\n                correlatedJoinKeys)) {\n          return;\n        }\n\n        // Rewrite the above plan:\n        //\n        // CorrelateRel(left correlation, condition = true)\n        //   LeftInputRel\n        //   LogicalProject-A (a RexNode)\n        //     LogicalAggregate (groupby(0), agg0(),agg1()...)\n        //       LogicalProject-B (may reference coVar)\n        //         LogicalFilter (references corVar)\n        //           RightInputRel (no correlated reference)\n        //\n\n        // to this plan:\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs)\n        //                 agg0(rewritten expression),\n        //                 agg1()...)\n        //     LogicalProject-B' (rewriten original projected exprs)\n        //       LogicalJoin(replace corvar w/ input ref from LeftInputRel)\n        //         LeftInputRel\n        //         RightInputRel\n        //\n\n        // In the case where agg is count(*) or count($corVar), it is\n        // changed to count(nullIndicator).\n        // Note:  any non-nullable field from the RHS can be used as\n        // the indicator however a \"true\" field is added to the\n        // projection list from the RHS for simplicity to avoid\n        // searching for non-null fields.\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs),\n        //                 count(nullIndicator), other aggs...)\n        //     LogicalProject-B' (all left input refs plus\n        //                    the rewritten original projected exprs)\n        //       LogicalJoin(replace corvar to input ref from LeftInputRel)\n        //         LeftInputRel\n        //         LogicalProject (everything from RightInputRel plus\n        //                     the nullIndicator \"true\")\n        //           RightInputRel\n        //\n\n        // first change the filter condition into a join condition\n        joinCond =\n                removeCorrelationExpr(filter.getCondition(), false);\n      } else if (cm.mapRefRelToCorRef.containsKey(aggInputProject)) {\n        // check rightInputRel contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // check cor var references are valid\n        if (!checkCorVars(correlate, aggInputProject, null, null)) {\n          return;\n        }\n\n        int nFields = left.getRowType().getFieldCount();\n        ImmutableBitSet allCols = ImmutableBitSet.range(nFields);\n\n        // leftInputRel contains unique keys\n        // i.e. each row is distinct and can group by on all the left\n        // fields\n        final RelMetadataQuery mq = call.getMetadataQuery();\n        if (!RelMdUtil.areColumnsDefinitelyUnique(mq, left, allCols)) {\n          //SQL2REL_LOGGER.fine(\"There are no unique keys for \" + left);\n          return;\n        }\n        //\n        // Rewrite the above plan:\n        //\n        // CorrelateRel(left correlation, condition = true)\n        //   LeftInputRel\n        //   LogicalProject-A (a RexNode)\n        //     LogicalAggregate (groupby(0), agg0(), agg1()...)\n        //       LogicalProject-B (references coVar)\n        //         RightInputRel (no correlated reference)\n        //\n\n        // to this plan:\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs)\n        //                 agg0(rewritten expression),\n        //                 agg1()...)\n        //     LogicalProject-B' (rewriten original projected exprs)\n        //       LogicalJoin (LOJ cond = true)\n        //         LeftInputRel\n        //         RightInputRel\n        //\n\n        // In the case where agg is count($corVar), it is changed to\n        // count(nullIndicator).\n        // Note:  any non-nullable field from the RHS can be used as\n        // the indicator however a \"true\" field is added to the\n        // projection list from the RHS for simplicity to avoid\n        // searching for non-null fields.\n        //\n        // LogicalProject-A' (all gby keys + rewritten nullable ProjExpr)\n        //   LogicalAggregate (groupby(all left input refs),\n        //                 count(nullIndicator), other aggs...)\n        //     LogicalProject-B' (all left input refs plus\n        //                    the rewritten original projected exprs)\n        //       LogicalJoin(replace corvar to input ref from LeftInputRel)\n        //         LeftInputRel\n        //         LogicalProject (everything from RightInputRel plus\n        //                     the nullIndicator \"true\")\n        //           RightInputRel\n      } else {\n        return;\n      }\n\n      RelDataType leftInputFieldType = left.getRowType();\n      int leftInputFieldCount = leftInputFieldType.getFieldCount();\n      int joinOutputProjExprCount =\n              leftInputFieldCount + aggInputProjects.size() + 1;\n\n      right =\n              createProjectWithAdditionalExprs(right,\n                      ImmutableList.of(\n                              Pair.<RexNode, String>of(rexBuilder.makeLiteral(true),\n                                      \"nullIndicator\")));\n\n      LogicalJoin join =\n              LogicalJoin.create(left, right, joinCond,\n                      ImmutableSet.<CorrelationId>of(), joinType);\n\n      // To the consumer of joinOutputProjRel, nullIndicator is located\n      // at the end\n      int nullIndicatorPos = join.getRowType().getFieldCount() - 1;\n\n      RexInputRef nullIndicator =\n              new RexInputRef(\n                      nullIndicatorPos,\n                      cluster.getTypeFactory().createTypeWithNullability(\n                              join.getRowType().getFieldList()\n                                      .get(nullIndicatorPos).getType(),\n                              true));\n\n      // first project all group-by keys plus the transformed agg input\n      List<RexNode> joinOutputProjects = Lists.newArrayList();\n\n      // LOJ Join preserves LHS types\n      for (int i = 0; i < leftInputFieldCount; i++) {\n        joinOutputProjects.add(\n                rexBuilder.makeInputRef(\n                        leftInputFieldType.getFieldList().get(i).getType(), i));\n      }\n\n      for (RexNode aggInputProjExpr : aggInputProjects) {\n        joinOutputProjects.add(\n                removeCorrelationExpr(aggInputProjExpr,\n                        joinType.generatesNullsOnRight(),\n                        nullIndicator));\n      }\n\n      joinOutputProjects.add(\n              rexBuilder.makeInputRef(join, nullIndicatorPos));\n\n      RelNode joinOutputProject =\n              RelOptUtil.createProject(\n                      join,\n                      joinOutputProjects,\n                      null);\n\n      // nullIndicator is now at a different location in the output of\n      // the join\n      nullIndicatorPos = joinOutputProjExprCount - 1;\n\n      final int groupCount = leftInputFieldCount;\n\n      List<AggregateCall> newAggCalls = Lists.newArrayList();\n      k = -1;\n      for (AggregateCall aggCall : aggCalls) {\n        ++k;\n        final List<Integer> argList;\n\n        if (isCountStar.contains(k)) {\n          // this is a count(*), transform it to count(nullIndicator)\n          // the null indicator is located at the end\n          argList = Collections.singletonList(nullIndicatorPos);\n        } else {\n          argList = Lists.newArrayList();\n\n          for (int aggArg : aggCall.getArgList()) {\n            argList.add(aggArg + groupCount);\n          }\n        }\n\n        int filterArg = aggCall.filterArg < 0 ? aggCall.filterArg\n                : aggCall.filterArg + groupCount;\n        newAggCalls.add(\n                aggCall.adaptTo(joinOutputProject, argList, filterArg,\n                        aggregate.getGroupCount(), groupCount));\n      }\n\n      ImmutableBitSet groupSet =\n              ImmutableBitSet.range(groupCount);\n      LogicalAggregate newAggregate =\n              LogicalAggregate.create(joinOutputProject,\n                      false,\n                      groupSet,\n                      null,\n                      newAggCalls);\n\n      List<RexNode> newAggOutputProjectList = Lists.newArrayList();\n      for (int i : groupSet) {\n        newAggOutputProjectList.add(\n                rexBuilder.makeInputRef(newAggregate, i));\n      }\n\n      RexNode newAggOutputProjects =\n              removeCorrelationExpr(aggOutputProjects.get(0), false);\n      newAggOutputProjectList.add(\n              rexBuilder.makeCast(\n                      cluster.getTypeFactory().createTypeWithNullability(\n                              newAggOutputProjects.getType(),\n                              true),\n                      newAggOutputProjects));\n\n      RelNode newAggOutputProject =\n              RelOptUtil.createProject(\n                      newAggregate,\n                      newAggOutputProjectList,\n                      null);\n\n      call.transformTo(newAggOutputProject);\n\n      removeCorVarFromTree(correlate);\n    }"
        ],
        [
            "HiveCalciteUtil::getPredsNotPushedAlready(Set,RelNode,List)",
            " 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687 -\n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  ",
            "  /**\n   * Given a list of predicates to push down, this methods returns the set of predicates\n   * that still need to be pushed. Predicates need to be pushed because 1) their String\n   * representation is not included in input set of predicates to exclude, or 2) they are\n   * already in the subtree rooted at the input node.\n   * This method updates the set of predicates to exclude with the String representation\n   * of the predicates in the output and in the subtree.\n   *\n   * @param predicatesToExclude String representation of predicates that should be excluded\n   * @param inp root of the subtree\n   * @param predsToPushDown candidate predicates to push down through the subtree\n   * @return list of predicates to push down\n   */\n  public static ImmutableList<RexNode> getPredsNotPushedAlready(Set<String> predicatesToExclude,\n          RelNode inp, List<RexNode> predsToPushDown) {\n    // Bail out if there is nothing to push\n    if (predsToPushDown.isEmpty()) {\n      return ImmutableList.of();\n    }\n    // Build map to not convert multiple times, further remove already included predicates\n    Map<String,RexNode> stringToRexNode = Maps.newLinkedHashMap();\n    for (RexNode r : predsToPushDown) {\n      String rexNodeString = r.toString();\n      if (predicatesToExclude.add(rexNodeString)) {\n        stringToRexNode.put(rexNodeString, r);\n      }\n    }\n    if (stringToRexNode.isEmpty()) {\n      return ImmutableList.of();\n    }\n    // Finally exclude preds that are already in the subtree as given by the metadata provider\n    // Note: this is the last step, trying to avoid the expensive call to the metadata provider\n    //       if possible\n    Set<String> predicatesInSubtree = Sets.newHashSet();\n    for (RexNode pred : RelMetadataQuery.instance().getPulledUpPredicates(inp).pulledUpPredicates) {\n      predicatesInSubtree.add(pred.toString());\n      predicatesInSubtree.addAll(Lists.transform(RelOptUtil.conjunctions(pred), REX_STR_FN));\n    }\n    final ImmutableList.Builder<RexNode> newConjuncts = ImmutableList.builder();\n    for (Entry<String,RexNode> e : stringToRexNode.entrySet()) {\n      if (predicatesInSubtree.add(e.getKey())) {\n        newConjuncts.add(e.getValue());\n      }\n    }\n    predicatesToExclude.addAll(predicatesInSubtree);\n    return newConjuncts.build();\n  }",
            " 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687 +\n 688 +\n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  ",
            "  /**\n   * Given a list of predicates to push down, this methods returns the set of predicates\n   * that still need to be pushed. Predicates need to be pushed because 1) their String\n   * representation is not included in input set of predicates to exclude, or 2) they are\n   * already in the subtree rooted at the input node.\n   * This method updates the set of predicates to exclude with the String representation\n   * of the predicates in the output and in the subtree.\n   *\n   * @param predicatesToExclude String representation of predicates that should be excluded\n   * @param inp root of the subtree\n   * @param predsToPushDown candidate predicates to push down through the subtree\n   * @return list of predicates to push down\n   */\n  public static ImmutableList<RexNode> getPredsNotPushedAlready(Set<String> predicatesToExclude,\n          RelNode inp, List<RexNode> predsToPushDown) {\n    // Bail out if there is nothing to push\n    if (predsToPushDown.isEmpty()) {\n      return ImmutableList.of();\n    }\n    // Build map to not convert multiple times, further remove already included predicates\n    Map<String,RexNode> stringToRexNode = Maps.newLinkedHashMap();\n    for (RexNode r : predsToPushDown) {\n      String rexNodeString = r.toString();\n      if (predicatesToExclude.add(rexNodeString)) {\n        stringToRexNode.put(rexNodeString, r);\n      }\n    }\n    if (stringToRexNode.isEmpty()) {\n      return ImmutableList.of();\n    }\n    // Finally exclude preds that are already in the subtree as given by the metadata provider\n    // Note: this is the last step, trying to avoid the expensive call to the metadata provider\n    //       if possible\n    Set<String> predicatesInSubtree = Sets.newHashSet();\n    final RelMetadataQuery mq = inp.getCluster().getMetadataQuery();\n    for (RexNode pred : mq.getPulledUpPredicates(inp).pulledUpPredicates) {\n      predicatesInSubtree.add(pred.toString());\n      predicatesInSubtree.addAll(Lists.transform(RelOptUtil.conjunctions(pred), REX_STR_FN));\n    }\n    final ImmutableList.Builder<RexNode> newConjuncts = ImmutableList.builder();\n    for (Entry<String,RexNode> e : stringToRexNode.entrySet()) {\n      if (predicatesInSubtree.add(e.getKey())) {\n        newConjuncts.add(e.getValue());\n      }\n    }\n    predicatesToExclude.addAll(predicatesInSubtree);\n    return newConjuncts.build();\n  }"
        ],
        [
            "HiveOnTezCostModel::TezMapJoinAlgorithm::getCost(HiveJoin)",
            " 242  \n 243  \n 244 -\n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = HiveAlgorithmsUtil.computeMapJoinCPUCost(cardinalities, streaming);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezMapJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null\n              ? 1 : mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n      final double ioCost = algoUtils.computeMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }",
            " 242  \n 243  \n 244 +\n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = HashTable  construction  cost  +\n      //               join cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      ImmutableBitSet.Builder streamingBuilder = ImmutableBitSet.builder();\n      switch (join.getStreamingSide()) {\n        case LEFT_RELATION:\n          streamingBuilder.set(0);\n          break;\n        case RIGHT_RELATION:\n          streamingBuilder.set(1);\n          break;\n        default:\n          return null;\n      }\n      ImmutableBitSet streaming = streamingBuilder.build();\n      final double cpuCost = HiveAlgorithmsUtil.computeMapJoinCPUCost(cardinalities, streaming);\n      // 3. IO cost = cost of transferring small tables to join node *\n      //              degree of parallelism\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezMapJoinAlgorithm.INSTANCE);\n      final int parallelism = mq.splitCount(join) == null\n              ? 1 : mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n      final double ioCost = algoUtils.computeMapJoinIOCost(relationInfos, streaming, parallelism);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }"
        ],
        [
            "HiveAggregateJoinTransposeRule::onMatch(RelOptRuleCall)",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    final Join join = call.rel(1);\n    final RexBuilder rexBuilder = aggregate.getCluster().getRexBuilder();\n\n    // If any aggregate functions do not support splitting, bail out\n    // If any aggregate call has a filter, bail out\n    for (AggregateCall aggregateCall : aggregate.getAggCallList()) {\n      if (aggregateCall.getAggregation().unwrap(SqlSplittableAggFunction.class)\n          == null) {\n        return;\n      }\n      if (aggregateCall.filterArg >= 0) {\n        return;\n      }\n    }\n\n    // If it is not an inner join, we do not push the\n    // aggregate operator\n    if (join.getJoinType() != JoinRelType.INNER) {\n      return;\n    }\n\n    if (!allowFunctions && !aggregate.getAggCallList().isEmpty()) {\n      return;\n    }\n\n    // Do the columns used by the join appear in the output of the aggregate?\n    RelMetadataQuery mq = RelMetadataQuery.instance();\n    final ImmutableBitSet aggregateColumns = aggregate.getGroupSet();\n    final ImmutableBitSet keyColumns = keyColumns(aggregateColumns,\n        mq.getPulledUpPredicates(join).pulledUpPredicates);\n    final ImmutableBitSet joinColumns =\n        RelOptUtil.InputFinder.bits(join.getCondition());\n    final boolean allColumnsInAggregate =\n        keyColumns.contains(joinColumns);\n    final ImmutableBitSet belowAggregateColumns =\n        aggregateColumns.union(joinColumns);\n\n    // Split join condition\n    final List<Integer> leftKeys = Lists.newArrayList();\n    final List<Integer> rightKeys = Lists.newArrayList();\n    final List<Boolean> filterNulls = Lists.newArrayList();\n    RexNode nonEquiConj =\n        RelOptUtil.splitJoinCondition(join.getLeft(), join.getRight(),\n            join.getCondition(), leftKeys, rightKeys, filterNulls);\n    // If it contains non-equi join conditions, we bail out\n    if (!nonEquiConj.isAlwaysTrue()) {\n      return;\n    }\n\n    // Push each aggregate function down to each side that contains all of its\n    // arguments. Note that COUNT(*), because it has no arguments, can go to\n    // both sides.\n    final Map<Integer, Integer> map = new HashMap<>();\n    final List<Side> sides = new ArrayList<>();\n    int uniqueCount = 0;\n    int offset = 0;\n    int belowOffset = 0;\n    for (int s = 0; s < 2; s++) {\n      final Side side = new Side();\n      final RelNode joinInput = join.getInput(s);\n      int fieldCount = joinInput.getRowType().getFieldCount();\n      final ImmutableBitSet fieldSet =\n          ImmutableBitSet.range(offset, offset + fieldCount);\n      final ImmutableBitSet belowAggregateKeyNotShifted =\n          belowAggregateColumns.intersect(fieldSet);\n      for (Ord<Integer> c : Ord.zip(belowAggregateKeyNotShifted)) {\n        map.put(c.e, belowOffset + c.i);\n      }\n      final ImmutableBitSet belowAggregateKey =\n          belowAggregateKeyNotShifted.shift(-offset);\n      final boolean unique;\n      if (!allowFunctions) {\n        assert aggregate.getAggCallList().isEmpty();\n        // If there are no functions, it doesn't matter as much whether we\n        // aggregate the inputs before the join, because there will not be\n        // any functions experiencing a cartesian product effect.\n        //\n        // But finding out whether the input is already unique requires a call\n        // to areColumnsUnique that currently (until [CALCITE-794] \"Detect\n        // cycles when computing statistics\" is fixed) places a heavy load on\n        // the metadata system.\n        //\n        // So we choose to imagine the the input is already unique, which is\n        // untrue but harmless.\n        //\n        unique = true;\n      } else {\n        final Boolean unique0 =\n            mq.areColumnsUnique(joinInput, belowAggregateKey);\n        unique = unique0 != null && unique0;\n      }\n      if (unique) {\n        ++uniqueCount;\n        side.newInput = joinInput;\n      } else {\n        List<AggregateCall> belowAggCalls = new ArrayList<>();\n        final SqlSplittableAggFunction.Registry<AggregateCall>\n            belowAggCallRegistry = registry(belowAggCalls);\n        final Mappings.TargetMapping mapping =\n            s == 0\n                ? Mappings.createIdentity(fieldCount)\n                : Mappings.createShiftMapping(fieldCount + offset, 0, offset,\n                    fieldCount);\n        for (Ord<AggregateCall> aggCall : Ord.zip(aggregate.getAggCallList())) {\n          final SqlAggFunction aggregation = aggCall.e.getAggregation();\n          final SqlSplittableAggFunction splitter =\n              Preconditions.checkNotNull(\n                  aggregation.unwrap(SqlSplittableAggFunction.class));\n          final AggregateCall call1;\n          if (fieldSet.contains(ImmutableBitSet.of(aggCall.e.getArgList()))) {\n            call1 = splitter.split(aggCall.e, mapping);\n          } else {\n            call1 = splitter.other(rexBuilder.getTypeFactory(), aggCall.e);\n          }\n          if (call1 != null) {\n            side.split.put(aggCall.i,\n                belowAggregateKey.cardinality()\n                    + belowAggCallRegistry.register(call1));\n          }\n        }\n        side.newInput = aggregateFactory.createAggregate(joinInput, false,\n            belowAggregateKey, null, belowAggCalls);\n      }\n      offset += fieldCount;\n      belowOffset += side.newInput.getRowType().getFieldCount();\n      sides.add(side);\n    }\n\n    if (uniqueCount == 2) {\n      // Both inputs to the join are unique. There is nothing to be gained by\n      // this rule. In fact, this aggregate+join may be the result of a previous\n      // invocation of this rule; if we continue we might loop forever.\n      return;\n    }\n\n    // Update condition\n    final Mapping mapping = (Mapping) Mappings.target(\n        new Function<Integer, Integer>() {\n          @Override\n          public Integer apply(Integer a0) {\n            return map.get(a0);\n          }\n        },\n        join.getRowType().getFieldCount(),\n        belowOffset);\n    final RexNode newCondition =\n        RexUtil.apply(mapping, join.getCondition());\n\n    // Create new join\n    RelNode newJoin = joinFactory.createJoin(sides.get(0).newInput,\n        sides.get(1).newInput, newCondition, join.getJoinType(),\n        join.getVariablesStopped(), join.isSemiJoinDone());\n\n    // Aggregate above to sum up the sub-totals\n    final List<AggregateCall> newAggCalls = new ArrayList<>();\n    final int groupIndicatorCount =\n        aggregate.getGroupCount() + aggregate.getIndicatorCount();\n    final int newLeftWidth = sides.get(0).newInput.getRowType().getFieldCount();\n    final List<RexNode> projects =\n        new ArrayList<>(rexBuilder.identityProjects(newJoin.getRowType()));\n    for (Ord<AggregateCall> aggCall : Ord.zip(aggregate.getAggCallList())) {\n      final SqlAggFunction aggregation = aggCall.e.getAggregation();\n      final SqlSplittableAggFunction splitter =\n          Preconditions.checkNotNull(\n              aggregation.unwrap(SqlSplittableAggFunction.class));\n      final Integer leftSubTotal = sides.get(0).split.get(aggCall.i);\n      final Integer rightSubTotal = sides.get(1).split.get(aggCall.i);\n      newAggCalls.add(\n          splitter.topSplit(rexBuilder, registry(projects),\n              groupIndicatorCount, newJoin.getRowType(), aggCall.e,\n              leftSubTotal == null ? -1 : leftSubTotal,\n              rightSubTotal == null ? -1 : rightSubTotal + newLeftWidth));\n    }\n    RelNode r = newJoin;\n  b:\n    if (allColumnsInAggregate && newAggCalls.isEmpty() &&\n      RelOptUtil.areRowTypesEqual(r.getRowType(), aggregate.getRowType(), false)) {\n      // no need to aggregate\n    } else {\n      r = RelOptUtil.createProject(r, projects, null, true,\n              relBuilderFactory.create(aggregate.getCluster(), null));\n      if (allColumnsInAggregate) {\n        // let's see if we can convert\n        List<RexNode> projects2 = new ArrayList<>();\n        for (int key : Mappings.apply(mapping, aggregate.getGroupSet())) {\n          projects2.add(rexBuilder.makeInputRef(r, key));\n        }\n        for (AggregateCall newAggCall : newAggCalls) {\n          final SqlSplittableAggFunction splitter =\n              newAggCall.getAggregation()\n                  .unwrap(SqlSplittableAggFunction.class);\n          if (splitter != null) {\n            projects2.add(\n                splitter.singleton(rexBuilder, r.getRowType(), newAggCall));\n          }\n        }\n        if (projects2.size()\n            == aggregate.getGroupSet().cardinality() + newAggCalls.size()) {\n          // We successfully converted agg calls into projects.\n          r = RelOptUtil.createProject(r, projects2, null, true,\n                  relBuilderFactory.create(aggregate.getCluster(), null));\n          break b;\n        }\n      }\n      r = aggregateFactory.createAggregate(r, aggregate.indicator,\n          Mappings.apply(mapping, aggregate.getGroupSet()),\n          Mappings.apply2(mapping, aggregate.getGroupSets()), newAggCalls);\n    }\n\n    // Make a cost based decision to pick cheaper plan\n    RelOptCost afterCost = mq.getCumulativeCost(r);\n    RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n    if (afterCost.isLt(beforeCost)) {\n      call.transformTo(r);\n    }\n  }",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Aggregate aggregate = call.rel(0);\n    final Join join = call.rel(1);\n    final RexBuilder rexBuilder = aggregate.getCluster().getRexBuilder();\n\n    // If any aggregate functions do not support splitting, bail out\n    // If any aggregate call has a filter, bail out\n    for (AggregateCall aggregateCall : aggregate.getAggCallList()) {\n      if (aggregateCall.getAggregation().unwrap(SqlSplittableAggFunction.class)\n          == null) {\n        return;\n      }\n      if (aggregateCall.filterArg >= 0) {\n        return;\n      }\n    }\n\n    // If it is not an inner join, we do not push the\n    // aggregate operator\n    if (join.getJoinType() != JoinRelType.INNER) {\n      return;\n    }\n\n    if (!allowFunctions && !aggregate.getAggCallList().isEmpty()) {\n      return;\n    }\n\n    // Do the columns used by the join appear in the output of the aggregate?\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    final ImmutableBitSet aggregateColumns = aggregate.getGroupSet();\n    final ImmutableBitSet keyColumns = keyColumns(aggregateColumns,\n        mq.getPulledUpPredicates(join).pulledUpPredicates);\n    final ImmutableBitSet joinColumns =\n        RelOptUtil.InputFinder.bits(join.getCondition());\n    final boolean allColumnsInAggregate =\n        keyColumns.contains(joinColumns);\n    final ImmutableBitSet belowAggregateColumns =\n        aggregateColumns.union(joinColumns);\n\n    // Split join condition\n    final List<Integer> leftKeys = Lists.newArrayList();\n    final List<Integer> rightKeys = Lists.newArrayList();\n    final List<Boolean> filterNulls = Lists.newArrayList();\n    RexNode nonEquiConj =\n        RelOptUtil.splitJoinCondition(join.getLeft(), join.getRight(),\n            join.getCondition(), leftKeys, rightKeys, filterNulls);\n    // If it contains non-equi join conditions, we bail out\n    if (!nonEquiConj.isAlwaysTrue()) {\n      return;\n    }\n\n    // Push each aggregate function down to each side that contains all of its\n    // arguments. Note that COUNT(*), because it has no arguments, can go to\n    // both sides.\n    final Map<Integer, Integer> map = new HashMap<>();\n    final List<Side> sides = new ArrayList<>();\n    int uniqueCount = 0;\n    int offset = 0;\n    int belowOffset = 0;\n    for (int s = 0; s < 2; s++) {\n      final Side side = new Side();\n      final RelNode joinInput = join.getInput(s);\n      int fieldCount = joinInput.getRowType().getFieldCount();\n      final ImmutableBitSet fieldSet =\n          ImmutableBitSet.range(offset, offset + fieldCount);\n      final ImmutableBitSet belowAggregateKeyNotShifted =\n          belowAggregateColumns.intersect(fieldSet);\n      for (Ord<Integer> c : Ord.zip(belowAggregateKeyNotShifted)) {\n        map.put(c.e, belowOffset + c.i);\n      }\n      final ImmutableBitSet belowAggregateKey =\n          belowAggregateKeyNotShifted.shift(-offset);\n      final boolean unique;\n      if (!allowFunctions) {\n        assert aggregate.getAggCallList().isEmpty();\n        // If there are no functions, it doesn't matter as much whether we\n        // aggregate the inputs before the join, because there will not be\n        // any functions experiencing a cartesian product effect.\n        //\n        // But finding out whether the input is already unique requires a call\n        // to areColumnsUnique that currently (until [CALCITE-794] \"Detect\n        // cycles when computing statistics\" is fixed) places a heavy load on\n        // the metadata system.\n        //\n        // So we choose to imagine the the input is already unique, which is\n        // untrue but harmless.\n        //\n        unique = true;\n      } else {\n        final Boolean unique0 =\n            mq.areColumnsUnique(joinInput, belowAggregateKey);\n        unique = unique0 != null && unique0;\n      }\n      if (unique) {\n        ++uniqueCount;\n        side.newInput = joinInput;\n      } else {\n        List<AggregateCall> belowAggCalls = new ArrayList<>();\n        final SqlSplittableAggFunction.Registry<AggregateCall>\n            belowAggCallRegistry = registry(belowAggCalls);\n        final Mappings.TargetMapping mapping =\n            s == 0\n                ? Mappings.createIdentity(fieldCount)\n                : Mappings.createShiftMapping(fieldCount + offset, 0, offset,\n                    fieldCount);\n        for (Ord<AggregateCall> aggCall : Ord.zip(aggregate.getAggCallList())) {\n          final SqlAggFunction aggregation = aggCall.e.getAggregation();\n          final SqlSplittableAggFunction splitter =\n              Preconditions.checkNotNull(\n                  aggregation.unwrap(SqlSplittableAggFunction.class));\n          final AggregateCall call1;\n          if (fieldSet.contains(ImmutableBitSet.of(aggCall.e.getArgList()))) {\n            call1 = splitter.split(aggCall.e, mapping);\n          } else {\n            call1 = splitter.other(rexBuilder.getTypeFactory(), aggCall.e);\n          }\n          if (call1 != null) {\n            side.split.put(aggCall.i,\n                belowAggregateKey.cardinality()\n                    + belowAggCallRegistry.register(call1));\n          }\n        }\n        side.newInput = aggregateFactory.createAggregate(joinInput, false,\n            belowAggregateKey, null, belowAggCalls);\n      }\n      offset += fieldCount;\n      belowOffset += side.newInput.getRowType().getFieldCount();\n      sides.add(side);\n    }\n\n    if (uniqueCount == 2) {\n      // Both inputs to the join are unique. There is nothing to be gained by\n      // this rule. In fact, this aggregate+join may be the result of a previous\n      // invocation of this rule; if we continue we might loop forever.\n      return;\n    }\n\n    // Update condition\n    final Mapping mapping = (Mapping) Mappings.target(\n        new Function<Integer, Integer>() {\n          @Override\n          public Integer apply(Integer a0) {\n            return map.get(a0);\n          }\n        },\n        join.getRowType().getFieldCount(),\n        belowOffset);\n    final RexNode newCondition =\n        RexUtil.apply(mapping, join.getCondition());\n\n    // Create new join\n    RelNode newJoin = joinFactory.createJoin(sides.get(0).newInput,\n        sides.get(1).newInput, newCondition, join.getJoinType(),\n        join.getVariablesStopped(), join.isSemiJoinDone());\n\n    // Aggregate above to sum up the sub-totals\n    final List<AggregateCall> newAggCalls = new ArrayList<>();\n    final int groupIndicatorCount =\n        aggregate.getGroupCount() + aggregate.getIndicatorCount();\n    final int newLeftWidth = sides.get(0).newInput.getRowType().getFieldCount();\n    final List<RexNode> projects =\n        new ArrayList<>(rexBuilder.identityProjects(newJoin.getRowType()));\n    for (Ord<AggregateCall> aggCall : Ord.zip(aggregate.getAggCallList())) {\n      final SqlAggFunction aggregation = aggCall.e.getAggregation();\n      final SqlSplittableAggFunction splitter =\n          Preconditions.checkNotNull(\n              aggregation.unwrap(SqlSplittableAggFunction.class));\n      final Integer leftSubTotal = sides.get(0).split.get(aggCall.i);\n      final Integer rightSubTotal = sides.get(1).split.get(aggCall.i);\n      newAggCalls.add(\n          splitter.topSplit(rexBuilder, registry(projects),\n              groupIndicatorCount, newJoin.getRowType(), aggCall.e,\n              leftSubTotal == null ? -1 : leftSubTotal,\n              rightSubTotal == null ? -1 : rightSubTotal + newLeftWidth));\n    }\n    RelNode r = newJoin;\n  b:\n    if (allColumnsInAggregate && newAggCalls.isEmpty() &&\n      RelOptUtil.areRowTypesEqual(r.getRowType(), aggregate.getRowType(), false)) {\n      // no need to aggregate\n    } else {\n      r = RelOptUtil.createProject(r, projects, null, true,\n              relBuilderFactory.create(aggregate.getCluster(), null));\n      if (allColumnsInAggregate) {\n        // let's see if we can convert\n        List<RexNode> projects2 = new ArrayList<>();\n        for (int key : Mappings.apply(mapping, aggregate.getGroupSet())) {\n          projects2.add(rexBuilder.makeInputRef(r, key));\n        }\n        for (AggregateCall newAggCall : newAggCalls) {\n          final SqlSplittableAggFunction splitter =\n              newAggCall.getAggregation()\n                  .unwrap(SqlSplittableAggFunction.class);\n          if (splitter != null) {\n            projects2.add(\n                splitter.singleton(rexBuilder, r.getRowType(), newAggCall));\n          }\n        }\n        if (projects2.size()\n            == aggregate.getGroupSet().cardinality() + newAggCalls.size()) {\n          // We successfully converted agg calls into projects.\n          r = RelOptUtil.createProject(r, projects2, null, true,\n                  relBuilderFactory.create(aggregate.getCluster(), null));\n          break b;\n        }\n      }\n      r = aggregateFactory.createAggregate(r, aggregate.indicator,\n          Mappings.apply(mapping, aggregate.getGroupSet()),\n          Mappings.apply2(mapping, aggregate.getGroupSets()), newAggCalls);\n    }\n\n    // Make a cost based decision to pick cheaper plan\n    RelOptCost afterCost = mq.getCumulativeCost(r);\n    RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n    if (afterCost.isLt(beforeCost)) {\n      call.transformTo(r);\n    }\n  }"
        ],
        [
            "HiveAlgorithmsUtil::getJoinMemory(HiveJoin,MapJoinStreamingRelation)",
            " 311  \n 312  \n 313 -\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "  public static Double getJoinMemory(HiveJoin join, MapJoinStreamingRelation streamingSide) {\n    Double memory = 0.0;\n    RelMetadataQuery mq = RelMetadataQuery.instance();\n    if (streamingSide == MapJoinStreamingRelation.NONE ||\n            streamingSide == MapJoinStreamingRelation.RIGHT_RELATION) {\n      // Left side\n      final Double leftAvgRowSize = mq.getAverageRowSize(join.getLeft());\n      final Double leftRowCount = mq.getRowCount(join.getLeft());\n      if (leftAvgRowSize == null || leftRowCount == null) {\n        return null;\n      }\n      memory += leftAvgRowSize * leftRowCount;\n    }\n    if (streamingSide == MapJoinStreamingRelation.NONE ||\n            streamingSide == MapJoinStreamingRelation.LEFT_RELATION) {\n      // Right side\n      final Double rightAvgRowSize = mq.getAverageRowSize(join.getRight());\n      final Double rightRowCount = mq.getRowCount(join.getRight());\n      if (rightAvgRowSize == null || rightRowCount == null) {\n        return null;\n      }\n      memory += rightAvgRowSize * rightRowCount;\n    }\n    return memory;\n  }",
            " 312  \n 313  \n 314 +\n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  ",
            "  public static Double getJoinMemory(HiveJoin join, MapJoinStreamingRelation streamingSide) {\n    Double memory = 0.0;\n    final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n    if (streamingSide == MapJoinStreamingRelation.NONE ||\n            streamingSide == MapJoinStreamingRelation.RIGHT_RELATION) {\n      // Left side\n      final Double leftAvgRowSize = mq.getAverageRowSize(join.getLeft());\n      final Double leftRowCount = mq.getRowCount(join.getLeft());\n      if (leftAvgRowSize == null || leftRowCount == null) {\n        return null;\n      }\n      memory += leftAvgRowSize * leftRowCount;\n    }\n    if (streamingSide == MapJoinStreamingRelation.NONE ||\n            streamingSide == MapJoinStreamingRelation.LEFT_RELATION) {\n      // Right side\n      final Double rightAvgRowSize = mq.getAverageRowSize(join.getRight());\n      final Double rightRowCount = mq.getRowCount(join.getRight());\n      if (rightAvgRowSize == null || rightRowCount == null) {\n        return null;\n      }\n      memory += rightAvgRowSize * rightRowCount;\n    }\n    return memory;\n  }"
        ],
        [
            "HiveRelDecorrelator::RemoveCorrelationForScalarProjectRule::onMatch(RelOptRuleCall)",
            "2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297 -\n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  ",
            "    public void onMatch(RelOptRuleCall call) {\n      final LogicalCorrelate correlate = call.rel(0);\n      final RelNode left = call.rel(1);\n      final LogicalAggregate aggregate = call.rel(2);\n      final LogicalProject project = call.rel(3);\n      RelNode right = call.rel(4);\n      final RelOptCluster cluster = correlate.getCluster();\n\n      setCurrent(call.getPlanner().getRoot(), correlate);\n\n      // Check for this pattern.\n      // The pattern matching could be simplified if rules can be applied\n      // during decorrelation.\n      //\n      // CorrelateRel(left correlation, condition = true)\n      //   LeftInputRel\n      //   LogicalAggregate (groupby (0) single_value())\n      //     LogicalProject-A (may reference coVar)\n      //       RightInputRel\n      final JoinRelType joinType = correlate.getJoinType().toJoinType();\n\n      // corRel.getCondition was here, however Correlate was updated so it\n      // never includes a join condition. The code was not modified for brevity.\n      RexNode joinCond = rexBuilder.makeLiteral(true);\n      if ((joinType != JoinRelType.LEFT)\n              || (joinCond != rexBuilder.makeLiteral(true))) {\n        return;\n      }\n\n      // check that the agg is of the following type:\n      // doing a single_value() on the entire input\n      if ((!aggregate.getGroupSet().isEmpty())\n              || (aggregate.getAggCallList().size() != 1)\n              || !(aggregate.getAggCallList().get(0).getAggregation()\n              instanceof SqlSingleValueAggFunction)) {\n        return;\n      }\n\n      // check this project only projects one expression, i.e. scalar\n      // subqueries.\n      if (project.getProjects().size() != 1) {\n        return;\n      }\n\n      int nullIndicatorPos;\n\n      if ((right instanceof LogicalFilter)\n              && cm.mapRefRelToCorRef.containsKey(right)) {\n        // rightInputRel has this shape:\n        //\n        //       LogicalFilter (references corvar)\n        //         FilterInputRel\n\n        // If rightInputRel is a filter and contains correlated\n        // reference, make sure the correlated keys in the filter\n        // condition forms a unique key of the RHS.\n\n        LogicalFilter filter = (LogicalFilter) right;\n        right = filter.getInput();\n\n        assert right instanceof HepRelVertex;\n        right = ((HepRelVertex) right).getCurrentRel();\n\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // extract the correlation out of the filter\n\n        // First breaking up the filter conditions into equality\n        // comparisons between rightJoinKeys(from the original\n        // filterInputRel) and correlatedJoinKeys. correlatedJoinKeys\n        // can be expressions, while rightJoinKeys need to be input\n        // refs. These comparisons are AND'ed together.\n        List<RexNode> tmpRightJoinKeys = Lists.newArrayList();\n        List<RexNode> correlatedJoinKeys = Lists.newArrayList();\n        RelOptUtil.splitCorrelatedFilterCondition(\n                filter,\n                tmpRightJoinKeys,\n                correlatedJoinKeys,\n                false);\n\n        // check that the columns referenced in these comparisons form\n        // an unique key of the filterInputRel\n        final List<RexInputRef> rightJoinKeys = new ArrayList<>();\n        for (RexNode key : tmpRightJoinKeys) {\n          assert key instanceof RexInputRef;\n          rightJoinKeys.add((RexInputRef) key);\n        }\n\n        // check that the columns referenced in rightJoinKeys form an\n        // unique key of the filterInputRel\n        if (rightJoinKeys.isEmpty()) {\n          return;\n        }\n\n        // The join filters out the nulls.  So, it's ok if there are\n        // nulls in the join keys.\n        final RelMetadataQuery mq = RelMetadataQuery.instance();\n        if (!RelMdUtil.areColumnsDefinitelyUniqueWhenNullsFiltered(mq, right,\n                rightJoinKeys)) {\n          //SQL2REL_LOGGER.fine(rightJoinKeys.toString()\n           //       + \"are not unique keys for \"\n            //      + right.toString());\n          return;\n        }\n\n        RexUtil.FieldAccessFinder visitor =\n                new RexUtil.FieldAccessFinder();\n        RexUtil.apply(visitor, correlatedJoinKeys, null);\n        List<RexFieldAccess> correlatedKeyList =\n                visitor.getFieldAccessList();\n\n        if (!checkCorVars(correlate, project, filter, correlatedKeyList)) {\n          return;\n        }\n\n        // Change the plan to this structure.\n        // Note that the aggregateRel is removed.\n        //\n        // LogicalProject-A' (replace corvar to input ref from the LogicalJoin)\n        //   LogicalJoin (replace corvar to input ref from LeftInputRel)\n        //     LeftInputRel\n        //     RightInputRel(oreviously FilterInputRel)\n\n        // Change the filter condition into a join condition\n        joinCond =\n                removeCorrelationExpr(filter.getCondition(), false);\n\n        nullIndicatorPos =\n                left.getRowType().getFieldCount()\n                        + rightJoinKeys.get(0).getIndex();\n      } else if (cm.mapRefRelToCorRef.containsKey(project)) {\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        if (!checkCorVars(correlate, project, null, null)) {\n          return;\n        }\n\n        // Change the plan to this structure.\n        //\n        // LogicalProject-A' (replace corvar to input ref from LogicalJoin)\n        //   LogicalJoin (left, condition = true)\n        //     LeftInputRel\n        //     LogicalAggregate(groupby(0), single_value(0), s_v(1)....)\n        //       LogicalProject-B (everything from input plus literal true)\n        //         ProjInputRel\n\n        // make the new projRel to provide a null indicator\n        right =\n                createProjectWithAdditionalExprs(right,\n                        ImmutableList.of(\n                                Pair.<RexNode, String>of(\n                                        rexBuilder.makeLiteral(true), \"nullIndicator\")));\n\n        // make the new aggRel\n        right =\n                RelOptUtil.createSingleValueAggRel(cluster, right);\n\n        // The last field:\n        //     single_value(true)\n        // is the nullIndicator\n        nullIndicatorPos =\n                left.getRowType().getFieldCount()\n                        + right.getRowType().getFieldCount() - 1;\n      } else {\n        return;\n      }\n\n      // make the new join rel\n      LogicalJoin join =\n              LogicalJoin.create(left, right, joinCond,\n                      ImmutableSet.<CorrelationId>of(), joinType);\n\n      RelNode newProject =\n              projectJoinOutputWithNullability(join, project, nullIndicatorPos);\n\n      call.transformTo(newProject);\n\n      removeCorVarFromTree(correlate);\n    }",
            "2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297 +\n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  ",
            "    public void onMatch(RelOptRuleCall call) {\n      final LogicalCorrelate correlate = call.rel(0);\n      final RelNode left = call.rel(1);\n      final LogicalAggregate aggregate = call.rel(2);\n      final LogicalProject project = call.rel(3);\n      RelNode right = call.rel(4);\n      final RelOptCluster cluster = correlate.getCluster();\n\n      setCurrent(call.getPlanner().getRoot(), correlate);\n\n      // Check for this pattern.\n      // The pattern matching could be simplified if rules can be applied\n      // during decorrelation.\n      //\n      // CorrelateRel(left correlation, condition = true)\n      //   LeftInputRel\n      //   LogicalAggregate (groupby (0) single_value())\n      //     LogicalProject-A (may reference coVar)\n      //       RightInputRel\n      final JoinRelType joinType = correlate.getJoinType().toJoinType();\n\n      // corRel.getCondition was here, however Correlate was updated so it\n      // never includes a join condition. The code was not modified for brevity.\n      RexNode joinCond = rexBuilder.makeLiteral(true);\n      if ((joinType != JoinRelType.LEFT)\n              || (joinCond != rexBuilder.makeLiteral(true))) {\n        return;\n      }\n\n      // check that the agg is of the following type:\n      // doing a single_value() on the entire input\n      if ((!aggregate.getGroupSet().isEmpty())\n              || (aggregate.getAggCallList().size() != 1)\n              || !(aggregate.getAggCallList().get(0).getAggregation()\n              instanceof SqlSingleValueAggFunction)) {\n        return;\n      }\n\n      // check this project only projects one expression, i.e. scalar\n      // subqueries.\n      if (project.getProjects().size() != 1) {\n        return;\n      }\n\n      int nullIndicatorPos;\n\n      if ((right instanceof LogicalFilter)\n              && cm.mapRefRelToCorRef.containsKey(right)) {\n        // rightInputRel has this shape:\n        //\n        //       LogicalFilter (references corvar)\n        //         FilterInputRel\n\n        // If rightInputRel is a filter and contains correlated\n        // reference, make sure the correlated keys in the filter\n        // condition forms a unique key of the RHS.\n\n        LogicalFilter filter = (LogicalFilter) right;\n        right = filter.getInput();\n\n        assert right instanceof HepRelVertex;\n        right = ((HepRelVertex) right).getCurrentRel();\n\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        // extract the correlation out of the filter\n\n        // First breaking up the filter conditions into equality\n        // comparisons between rightJoinKeys(from the original\n        // filterInputRel) and correlatedJoinKeys. correlatedJoinKeys\n        // can be expressions, while rightJoinKeys need to be input\n        // refs. These comparisons are AND'ed together.\n        List<RexNode> tmpRightJoinKeys = Lists.newArrayList();\n        List<RexNode> correlatedJoinKeys = Lists.newArrayList();\n        RelOptUtil.splitCorrelatedFilterCondition(\n                filter,\n                tmpRightJoinKeys,\n                correlatedJoinKeys,\n                false);\n\n        // check that the columns referenced in these comparisons form\n        // an unique key of the filterInputRel\n        final List<RexInputRef> rightJoinKeys = new ArrayList<>();\n        for (RexNode key : tmpRightJoinKeys) {\n          assert key instanceof RexInputRef;\n          rightJoinKeys.add((RexInputRef) key);\n        }\n\n        // check that the columns referenced in rightJoinKeys form an\n        // unique key of the filterInputRel\n        if (rightJoinKeys.isEmpty()) {\n          return;\n        }\n\n        // The join filters out the nulls.  So, it's ok if there are\n        // nulls in the join keys.\n        final RelMetadataQuery mq = call.getMetadataQuery();\n        if (!RelMdUtil.areColumnsDefinitelyUniqueWhenNullsFiltered(mq, right,\n                rightJoinKeys)) {\n          //SQL2REL_LOGGER.fine(rightJoinKeys.toString()\n           //       + \"are not unique keys for \"\n            //      + right.toString());\n          return;\n        }\n\n        RexUtil.FieldAccessFinder visitor =\n                new RexUtil.FieldAccessFinder();\n        RexUtil.apply(visitor, correlatedJoinKeys, null);\n        List<RexFieldAccess> correlatedKeyList =\n                visitor.getFieldAccessList();\n\n        if (!checkCorVars(correlate, project, filter, correlatedKeyList)) {\n          return;\n        }\n\n        // Change the plan to this structure.\n        // Note that the aggregateRel is removed.\n        //\n        // LogicalProject-A' (replace corvar to input ref from the LogicalJoin)\n        //   LogicalJoin (replace corvar to input ref from LeftInputRel)\n        //     LeftInputRel\n        //     RightInputRel(oreviously FilterInputRel)\n\n        // Change the filter condition into a join condition\n        joinCond =\n                removeCorrelationExpr(filter.getCondition(), false);\n\n        nullIndicatorPos =\n                left.getRowType().getFieldCount()\n                        + rightJoinKeys.get(0).getIndex();\n      } else if (cm.mapRefRelToCorRef.containsKey(project)) {\n        // check filter input contains no correlation\n        if (RelOptUtil.getVariablesUsed(right).size() > 0) {\n          return;\n        }\n\n        if (!checkCorVars(correlate, project, null, null)) {\n          return;\n        }\n\n        // Change the plan to this structure.\n        //\n        // LogicalProject-A' (replace corvar to input ref from LogicalJoin)\n        //   LogicalJoin (left, condition = true)\n        //     LeftInputRel\n        //     LogicalAggregate(groupby(0), single_value(0), s_v(1)....)\n        //       LogicalProject-B (everything from input plus literal true)\n        //         ProjInputRel\n\n        // make the new projRel to provide a null indicator\n        right =\n                createProjectWithAdditionalExprs(right,\n                        ImmutableList.of(\n                                Pair.<RexNode, String>of(\n                                        rexBuilder.makeLiteral(true), \"nullIndicator\")));\n\n        // make the new aggRel\n        right =\n                RelOptUtil.createSingleValueAggRel(cluster, right);\n\n        // The last field:\n        //     single_value(true)\n        // is the nullIndicator\n        nullIndicatorPos =\n                left.getRowType().getFieldCount()\n                        + right.getRowType().getFieldCount() - 1;\n      } else {\n        return;\n      }\n\n      // make the new join rel\n      LogicalJoin join =\n              LogicalJoin.create(left, right, joinCond,\n                      ImmutableSet.<CorrelationId>of(), joinType);\n\n      RelNode newProject =\n              projectJoinOutputWithNullability(join, project, nullIndicatorPos);\n\n      call.transformTo(newProject);\n\n      removeCorVarFromTree(correlate);\n    }"
        ],
        [
            "HiveUnionPullUpConstantsRule::onMatch(RelOptRuleCall)",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Union union = call.rel(0);\n\n    final int count = union.getRowType().getFieldCount();\n    if (count == 1) {\n      // No room for optimization since we cannot create an empty\n      // Project operator.\n      return;\n    }\n\n    final RexBuilder rexBuilder = union.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = RelMetadataQuery.instance();\n    final RelOptPredicateList predicates = mq.getPulledUpPredicates(union);\n    if (predicates == null) {\n      return;\n    }\n\n    Map<RexNode, RexNode> conditionsExtracted = HiveReduceExpressionsRule.predicateConstants(\n            RexNode.class, rexBuilder, predicates);\n    Map<RexNode, RexNode> constants = new HashMap<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(union, i);\n      if (conditionsExtracted.containsKey(expr)) {\n        constants.put(expr, conditionsExtracted.get(expr));\n      }\n    }\n\n    // None of the expressions are constant. Nothing to do.\n    if (constants.isEmpty()) {\n      return;\n    }\n\n    // Create expressions for Project operators before and after the Union\n    List<RelDataTypeField> fields = union.getRowType().getFieldList();\n    List<RexNode> topChildExprs = new ArrayList<>();\n    List<String> topChildExprsFields = new ArrayList<>();\n    List<RexNode> refs = new ArrayList<>();\n    ImmutableBitSet.Builder refsIndexBuilder = ImmutableBitSet.builder();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(union, i);\n      RelDataTypeField field = fields.get(i);\n      if (constants.containsKey(expr)) {\n        topChildExprs.add(constants.get(expr));\n        topChildExprsFields.add(field.getName());\n      } else {\n        topChildExprs.add(expr);\n        topChildExprsFields.add(field.getName());\n        refs.add(expr);\n        refsIndexBuilder.set(i);\n      }\n    }\n    ImmutableBitSet refsIndex = refsIndexBuilder.build();\n\n    // Update top Project positions\n    final Mappings.TargetMapping mapping =\n            RelOptUtil.permutation(refs, union.getInput(0).getRowType()).inverse();\n    topChildExprs = ImmutableList.copyOf(RexUtil.apply(mapping, topChildExprs));\n\n    // Create new Project-Union-Project sequences\n    final RelBuilder relBuilder = call.builder();\n    for (int i = 0; i < union.getInputs().size() ; i++) {\n      RelNode input = union.getInput(i);\n      List<Pair<RexNode, String>> newChildExprs = new ArrayList<>();\n      for (int j = 0; j < refsIndex.cardinality(); j++ ) {\n        int pos = refsIndex.nth(j);\n        newChildExprs.add(\n                Pair.<RexNode, String>of(rexBuilder.makeInputRef(input, pos),\n                        input.getRowType().getFieldList().get(pos).getName()));\n      }\n      if (newChildExprs.isEmpty()) {\n        // At least a single item in project is required.\n        newChildExprs.add(Pair.<RexNode,String>of(\n                topChildExprs.get(0), topChildExprsFields.get(0)));\n      }\n      // Add the input with project on top\n      relBuilder.push(input);\n      relBuilder.project(Pair.left(newChildExprs), Pair.right(newChildExprs));\n    }\n    relBuilder.union(union.all, union.getInputs().size());\n    // Create top Project fixing nullability of fields\n    relBuilder.project(topChildExprs, topChildExprsFields);\n    relBuilder.convert(union.getRowType(), false);\n\n    call.transformTo(relBuilder.build());\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    final Union union = call.rel(0);\n\n    final int count = union.getRowType().getFieldCount();\n    if (count == 1) {\n      // No room for optimization since we cannot create an empty\n      // Project operator.\n      return;\n    }\n\n    final RexBuilder rexBuilder = union.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    final RelOptPredicateList predicates = mq.getPulledUpPredicates(union);\n    if (predicates == null) {\n      return;\n    }\n\n    Map<RexNode, RexNode> conditionsExtracted = HiveReduceExpressionsRule.predicateConstants(\n            RexNode.class, rexBuilder, predicates);\n    Map<RexNode, RexNode> constants = new HashMap<>();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(union, i);\n      if (conditionsExtracted.containsKey(expr)) {\n        constants.put(expr, conditionsExtracted.get(expr));\n      }\n    }\n\n    // None of the expressions are constant. Nothing to do.\n    if (constants.isEmpty()) {\n      return;\n    }\n\n    // Create expressions for Project operators before and after the Union\n    List<RelDataTypeField> fields = union.getRowType().getFieldList();\n    List<RexNode> topChildExprs = new ArrayList<>();\n    List<String> topChildExprsFields = new ArrayList<>();\n    List<RexNode> refs = new ArrayList<>();\n    ImmutableBitSet.Builder refsIndexBuilder = ImmutableBitSet.builder();\n    for (int i = 0; i < count ; i++) {\n      RexNode expr = rexBuilder.makeInputRef(union, i);\n      RelDataTypeField field = fields.get(i);\n      if (constants.containsKey(expr)) {\n        topChildExprs.add(constants.get(expr));\n        topChildExprsFields.add(field.getName());\n      } else {\n        topChildExprs.add(expr);\n        topChildExprsFields.add(field.getName());\n        refs.add(expr);\n        refsIndexBuilder.set(i);\n      }\n    }\n    ImmutableBitSet refsIndex = refsIndexBuilder.build();\n\n    // Update top Project positions\n    final Mappings.TargetMapping mapping =\n            RelOptUtil.permutation(refs, union.getInput(0).getRowType()).inverse();\n    topChildExprs = ImmutableList.copyOf(RexUtil.apply(mapping, topChildExprs));\n\n    // Create new Project-Union-Project sequences\n    final RelBuilder relBuilder = call.builder();\n    for (int i = 0; i < union.getInputs().size() ; i++) {\n      RelNode input = union.getInput(i);\n      List<Pair<RexNode, String>> newChildExprs = new ArrayList<>();\n      for (int j = 0; j < refsIndex.cardinality(); j++ ) {\n        int pos = refsIndex.nth(j);\n        newChildExprs.add(\n                Pair.<RexNode, String>of(rexBuilder.makeInputRef(input, pos),\n                        input.getRowType().getFieldList().get(pos).getName()));\n      }\n      if (newChildExprs.isEmpty()) {\n        // At least a single item in project is required.\n        newChildExprs.add(Pair.<RexNode,String>of(\n                topChildExprs.get(0), topChildExprsFields.get(0)));\n      }\n      // Add the input with project on top\n      relBuilder.push(input);\n      relBuilder.project(Pair.left(newChildExprs), Pair.right(newChildExprs));\n    }\n    relBuilder.union(union.all, union.getInputs().size());\n    // Create top Project fixing nullability of fields\n    relBuilder.project(topChildExprs, topChildExprsFields);\n    relBuilder.convert(union.getRowType(), false);\n\n    call.transformTo(relBuilder.build());\n  }"
        ],
        [
            "HiveOnTezCostModel::TezCommonJoinAlgorithm::getCost(HiveJoin)",
            " 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = sorting cost (for each relation) +\n      //               total merge cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      double cpuCost;\n      try {\n        cpuCost = algoUtils.computeSortMergeCPUCost(cardinalities, join.getSortedInputs());\n      } catch (CalciteSemanticException e) {\n        LOG.trace(\"Failed to compute sort merge cpu cost \", e);\n        return null;\n      }\n      // 3. IO cost = cost of writing intermediary results to local FS +\n      //              cost of reading from local FS for transferring to join +\n      //              cost of transferring map outputs to Join operator\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      final double ioCost = algoUtils.computeSortMergeIOCost(relationInfos);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }",
            " 131  \n 132  \n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "    @Override\n    public RelOptCost getCost(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      // 1. Sum of input cardinalities\n      final Double leftRCount = mq.getRowCount(join.getLeft());\n      final Double rightRCount = mq.getRowCount(join.getRight());\n      if (leftRCount == null || rightRCount == null) {\n        return null;\n      }\n      final double rCount = leftRCount + rightRCount;\n      // 2. CPU cost = sorting cost (for each relation) +\n      //               total merge cost\n      ImmutableList<Double> cardinalities = new ImmutableList.Builder<Double>().\n              add(leftRCount).\n              add(rightRCount).\n              build();\n      double cpuCost;\n      try {\n        cpuCost = algoUtils.computeSortMergeCPUCost(cardinalities, join.getSortedInputs());\n      } catch (CalciteSemanticException e) {\n        LOG.trace(\"Failed to compute sort merge cpu cost \", e);\n        return null;\n      }\n      // 3. IO cost = cost of writing intermediary results to local FS +\n      //              cost of reading from local FS for transferring to join +\n      //              cost of transferring map outputs to Join operator\n      final Double leftRAverageSize = mq.getAverageRowSize(join.getLeft());\n      final Double rightRAverageSize = mq.getAverageRowSize(join.getRight());\n      if (leftRAverageSize == null || rightRAverageSize == null) {\n        return null;\n      }\n      ImmutableList<Pair<Double,Double>> relationInfos = new ImmutableList.Builder<Pair<Double,Double>>().\n              add(new Pair<Double,Double>(leftRCount,leftRAverageSize)).\n              add(new Pair<Double,Double>(rightRCount,rightRAverageSize)).\n              build();\n      final double ioCost = algoUtils.computeSortMergeIOCost(relationInfos);\n      // 4. Result\n      return HiveCost.FACTORY.makeCost(rCount, cpuCost, ioCost);\n    }"
        ],
        [
            "HiveOnTezCostModel::TezSMBJoinAlgorithm::isExecutable(HiveJoin)",
            " 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560 -\n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  ",
            "    @Override\n    public boolean isExecutable(HiveJoin join) {\n      // Requirements: for SMB, sorted by their keys on both sides and bucketed.\n      // Get key columns\n      JoinPredicateInfo joinPredInfo = join.getJoinPredicateInfo();\n      List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n      for (int i=0; i<join.getInputs().size(); i++) {\n        RelNode input = join.getInputs().get(i);\n        // Is smbJoin possible? We need correct order\n        boolean orderFound;\n        try {\n          orderFound = join.getSortedInputs().get(i);\n        } catch (CalciteSemanticException e) {\n          LOG.trace(\"Not possible to do SMB Join \",e);\n          return false;\n        }\n        if (!orderFound) {\n          return false;\n        }\n        // Is smbJoin possible? We need correct bucketing\n        RelDistribution distribution = RelMetadataQuery.instance().distribution(input);\n        if (distribution.getType() != Type.HASH_DISTRIBUTED) {\n          return false;\n        }\n        if (!distribution.getKeys().containsAll(joinKeysInChildren.get(i))) {\n          return false;\n        }\n      }\n      return true;\n    }",
            " 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548 +\n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563 +\n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  ",
            "    @Override\n    public boolean isExecutable(HiveJoin join) {\n      // Requirements: for SMB, sorted by their keys on both sides and bucketed.\n      // Get key columns\n      JoinPredicateInfo joinPredInfo = join.getJoinPredicateInfo();\n      List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n      joinKeysInChildren.add(\n              ImmutableIntList.copyOf(\n                      joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      for (int i=0; i<join.getInputs().size(); i++) {\n        RelNode input = join.getInputs().get(i);\n        // Is smbJoin possible? We need correct order\n        boolean orderFound;\n        try {\n          orderFound = join.getSortedInputs().get(i);\n        } catch (CalciteSemanticException e) {\n          LOG.trace(\"Not possible to do SMB Join \",e);\n          return false;\n        }\n        if (!orderFound) {\n          return false;\n        }\n        // Is smbJoin possible? We need correct bucketing\n        RelDistribution distribution = mq.distribution(input);\n        if (distribution.getType() != Type.HASH_DISTRIBUTED) {\n          return false;\n        }\n        if (!distribution.getKeys().containsAll(joinKeysInChildren.get(i))) {\n          return false;\n        }\n      }\n      return true;\n    }"
        ],
        [
            "HiveSortUnionReduceRule::onMatch(RelOptRuleCall)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  public void onMatch(RelOptRuleCall call) {\n    final HiveSortLimit sort = call.rel(0);\n    final HiveUnion union = call.rel(1);\n    List<RelNode> inputs = new ArrayList<>();\n    // Thus we use 'finishPushSortPastUnion' as a flag to identify if we have finished pushing the\n    // sort past a union.\n    boolean finishPushSortPastUnion = true;\n    final int offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n    for (RelNode input : union.getInputs()) {\n      // If we do not reduce the input size, we bail out\n      if (RexLiteral.intValue(sort.fetch) + offset < RelMetadataQuery.instance().getRowCount(input)) {\n        finishPushSortPastUnion = false;\n        // Here we do some query rewrite. We first get the new fetchRN, which is\n        // a sum of offset and fetch.\n        // We then push it through by creating a new branchSort with the new\n        // fetchRN but no offset.\n        RexNode fetchRN = sort.getCluster().getRexBuilder()\n            .makeExactLiteral(BigDecimal.valueOf(RexLiteral.intValue(sort.fetch) + offset));\n        HiveSortLimit branchSort = sort.copy(sort.getTraitSet(), input, sort.getCollation(), null,\n            fetchRN);\n        branchSort.setRuleCreated(true);\n        inputs.add(branchSort);\n      } else {\n        inputs.add(input);\n      }\n    }\n    // there is nothing to change\n    if (finishPushSortPastUnion) {\n      return;\n    }\n    // create new union and sort\n    HiveUnion unionCopy = (HiveUnion) union.copy(union.getTraitSet(), inputs, union.all);\n    HiveSortLimit result = sort.copy(sort.getTraitSet(), unionCopy, sort.getCollation(), sort.offset,\n        sort.fetch);\n    call.transformTo(result);\n  }",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "  public void onMatch(RelOptRuleCall call) {\n    final HiveSortLimit sort = call.rel(0);\n    final HiveUnion union = call.rel(1);\n    List<RelNode> inputs = new ArrayList<>();\n    // Thus we use 'finishPushSortPastUnion' as a flag to identify if we have finished pushing the\n    // sort past a union.\n    boolean finishPushSortPastUnion = true;\n    final int offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n    for (RelNode input : union.getInputs()) {\n      // If we do not reduce the input size, we bail out\n      if (RexLiteral.intValue(sort.fetch) + offset < call.getMetadataQuery().getRowCount(input)) {\n        finishPushSortPastUnion = false;\n        // Here we do some query rewrite. We first get the new fetchRN, which is\n        // a sum of offset and fetch.\n        // We then push it through by creating a new branchSort with the new\n        // fetchRN but no offset.\n        RexNode fetchRN = sort.getCluster().getRexBuilder()\n            .makeExactLiteral(BigDecimal.valueOf(RexLiteral.intValue(sort.fetch) + offset));\n        HiveSortLimit branchSort = sort.copy(sort.getTraitSet(), input, sort.getCollation(), null,\n            fetchRN);\n        branchSort.setRuleCreated(true);\n        inputs.add(branchSort);\n      } else {\n        inputs.add(input);\n      }\n    }\n    // there is nothing to change\n    if (finishPushSortPastUnion) {\n      return;\n    }\n    // create new union and sort\n    HiveUnion unionCopy = (HiveUnion) union.copy(union.getTraitSet(), inputs, union.all);\n    HiveSortLimit result = sort.copy(sort.getTraitSet(), unionCopy, sort.getCollation(), sort.offset,\n        sort.fetch);\n    call.transformTo(result);\n  }"
        ],
        [
            "HiveJoin::getStreamingSide()",
            " 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  public MapJoinStreamingRelation getStreamingSide() {\n    RelMetadataQuery mq = RelMetadataQuery.instance();\n    Double leftInputSize = mq.memory(left);\n    Double rightInputSize = mq.memory(right);\n    if (leftInputSize == null && rightInputSize == null) {\n      return MapJoinStreamingRelation.NONE;\n    } else if (leftInputSize != null &&\n            (rightInputSize == null ||\n            (leftInputSize < rightInputSize))) {\n      return MapJoinStreamingRelation.RIGHT_RELATION;\n    } else if (rightInputSize != null &&\n            (leftInputSize == null ||\n            (rightInputSize <= leftInputSize))) {\n      return MapJoinStreamingRelation.LEFT_RELATION;\n    }\n    return MapJoinStreamingRelation.NONE;\n  }",
            " 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  public MapJoinStreamingRelation getStreamingSide() {\n    RelMetadataQuery mq = left.getCluster().getMetadataQuery();\n    Double leftInputSize = mq.memory(left);\n    Double rightInputSize = mq.memory(right);\n    if (leftInputSize == null && rightInputSize == null) {\n      return MapJoinStreamingRelation.NONE;\n    } else if (leftInputSize != null &&\n            (rightInputSize == null ||\n            (leftInputSize < rightInputSize))) {\n      return MapJoinStreamingRelation.RIGHT_RELATION;\n    } else if (rightInputSize != null &&\n            (leftInputSize == null ||\n            (rightInputSize <= leftInputSize))) {\n      return MapJoinStreamingRelation.LEFT_RELATION;\n    }\n    return MapJoinStreamingRelation.NONE;\n  }"
        ],
        [
            "HiveJoin::getSortedInputs()",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 -\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  ",
            "  public ImmutableBitSet getSortedInputs() throws CalciteSemanticException {\n    ImmutableBitSet.Builder sortedInputsBuilder = ImmutableBitSet.builder();\n    JoinPredicateInfo joinPredInfo = HiveCalciteUtil.JoinPredicateInfo.\n            constructJoinPredicateInfo(this);\n    List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n    joinKeysInChildren.add(\n            ImmutableIntList.copyOf(\n                    joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n    joinKeysInChildren.add(\n            ImmutableIntList.copyOf(\n                    joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n    for (int i=0; i<this.getInputs().size(); i++) {\n      boolean correctOrderFound = RelCollations.contains(\n          RelMetadataQuery.instance().collations(this.getInputs().get(i)),\n          joinKeysInChildren.get(i));\n      if (correctOrderFound) {\n        sortedInputsBuilder.set(i);\n      }\n    }\n    return sortedInputsBuilder.build();\n  }",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203 +\n 204  \n 205  \n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "  public ImmutableBitSet getSortedInputs() throws CalciteSemanticException {\n    ImmutableBitSet.Builder sortedInputsBuilder = ImmutableBitSet.builder();\n    JoinPredicateInfo joinPredInfo = HiveCalciteUtil.JoinPredicateInfo.\n            constructJoinPredicateInfo(this);\n    List<ImmutableIntList> joinKeysInChildren = new ArrayList<ImmutableIntList>();\n    joinKeysInChildren.add(\n            ImmutableIntList.copyOf(\n                    joinPredInfo.getProjsFromLeftPartOfJoinKeysInChildSchema()));\n    joinKeysInChildren.add(\n            ImmutableIntList.copyOf(\n                    joinPredInfo.getProjsFromRightPartOfJoinKeysInChildSchema()));\n\n    final RelMetadataQuery mq = this.left.getCluster().getMetadataQuery();\n    for (int i=0; i<this.getInputs().size(); i++) {\n      boolean correctOrderFound = RelCollations.contains(\n          mq.collations(this.getInputs().get(i)),\n          joinKeysInChildren.get(i));\n      if (correctOrderFound) {\n        sortedInputsBuilder.set(i);\n      }\n    }\n    return sortedInputsBuilder.build();\n  }"
        ],
        [
            "HiveAlgorithmsUtil::isFittingIntoMemory(Double,RelNode,int)",
            " 201  \n 202 -\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  ",
            "  public static boolean isFittingIntoMemory(Double maxSize, RelNode input, int buckets) {\n    Double currentMemory = RelMetadataQuery.instance().cumulativeMemoryWithinPhase(input);\n    if (currentMemory != null) {\n      if(currentMemory / buckets > maxSize) {\n        return false;\n      }\n      return true;\n    }\n    return false;\n  }",
            " 201  \n 202 +\n 203 +\n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "  public static boolean isFittingIntoMemory(Double maxSize, RelNode input, int buckets) {\n    final RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n    Double currentMemory = mq.cumulativeMemoryWithinPhase(input);\n    if (currentMemory != null) {\n      if(currentMemory / buckets > maxSize) {\n        return false;\n      }\n      return true;\n    }\n    return false;\n  }"
        ],
        [
            "HiveOnTezCostModel::TezSMBJoinAlgorithm::getCumulativeMemoryWithinPhaseSplit(HiveJoin)",
            " 640  \n 641  \n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      RelMetadataQuery mq = RelMetadataQuery.instance();\n      // TODO: Split count is not same as no of buckets\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezSMBJoinAlgorithm.INSTANCE);\n\n      final Double memoryWithinPhase = mq.cumulativeMemoryWithinPhase(join);\n      final Integer splitCount = mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (memoryWithinPhase == null || splitCount == null) {\n        return null;\n      }\n      return memoryWithinPhase / splitCount;\n    }",
            " 643  \n 644  \n 645 +\n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      // TODO: Split count is not same as no of buckets\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezSMBJoinAlgorithm.INSTANCE);\n\n      final Double memoryWithinPhase = mq.cumulativeMemoryWithinPhase(join);\n      final Integer splitCount = mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (memoryWithinPhase == null || splitCount == null) {\n        return null;\n      }\n      return memoryWithinPhase / splitCount;\n    }"
        ],
        [
            "HiveRemoveSqCountCheck::onMatch(RelOptRuleCall)",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 -\n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "  @Override public void onMatch(RelOptRuleCall call) {\n    final Join topJoin= call.rel(0);\n    final Join join = call.rel(2);\n    final Aggregate aggregate = call.rel(6);\n\n    // in presence of grouping sets we can't remove sq_count_check\n    if(aggregate.indicator) {\n      return;\n    }\n\n    final int groupCount = aggregate.getGroupCount();\n\n    final RexBuilder rexBuilder = aggregate.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = RelMetadataQuery.instance();\n    final RelOptPredicateList predicates =\n        mq.getPulledUpPredicates(aggregate.getInput());\n    if (predicates == null) {\n      return;\n    }\n    final NavigableMap<Integer, RexNode> map = new TreeMap<>();\n    for (int key : aggregate.getGroupSet()) {\n      final RexInputRef ref =\n          rexBuilder.makeInputRef(aggregate.getInput(), key);\n      if (predicates.constantMap.containsKey(ref)) {\n        map.put(key, predicates.constantMap.get(ref));\n      }\n    }\n\n    // None of the group expressions are constant. Nothing to do.\n    if (map.isEmpty()) {\n      return;\n    }\n\n    if (groupCount == map.size()) {\n      // join(left, join.getRight)\n      RelNode newJoin = HiveJoin.getJoin(topJoin.getCluster(), join.getLeft(),  topJoin.getRight(),\n          topJoin.getCondition(), topJoin.getJoinType());\n      call.transformTo(newJoin);\n    }\n  }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 +\n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "  @Override public void onMatch(RelOptRuleCall call) {\n    final Join topJoin= call.rel(0);\n    final Join join = call.rel(2);\n    final Aggregate aggregate = call.rel(6);\n\n    // in presence of grouping sets we can't remove sq_count_check\n    if(aggregate.indicator) {\n      return;\n    }\n\n    final int groupCount = aggregate.getGroupCount();\n\n    final RexBuilder rexBuilder = aggregate.getCluster().getRexBuilder();\n    final RelMetadataQuery mq = call.getMetadataQuery();\n    final RelOptPredicateList predicates =\n        mq.getPulledUpPredicates(aggregate.getInput());\n    if (predicates == null) {\n      return;\n    }\n    final NavigableMap<Integer, RexNode> map = new TreeMap<>();\n    for (int key : aggregate.getGroupSet()) {\n      final RexInputRef ref =\n          rexBuilder.makeInputRef(aggregate.getInput(), key);\n      if (predicates.constantMap.containsKey(ref)) {\n        map.put(key, predicates.constantMap.get(ref));\n      }\n    }\n\n    // None of the group expressions are constant. Nothing to do.\n    if (map.isEmpty()) {\n      return;\n    }\n\n    if (groupCount == map.size()) {\n      // join(left, join.getRight)\n      RelNode newJoin = HiveJoin.getJoin(topJoin.getCluster(), join.getLeft(),  topJoin.getRight(),\n          topJoin.getCondition(), topJoin.getJoinType());\n      call.transformTo(newJoin);\n    }\n  }"
        ],
        [
            "HiveOnTezCostModel::TezCommonJoinAlgorithm::getCumulativeMemoryWithinPhaseSplit(HiveJoin)",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193 -\n 194 -\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezCommonJoinAlgorithm.INSTANCE);\n\n      final Double memoryWithinPhase =\n          RelMetadataQuery.instance().cumulativeMemoryWithinPhase(join);\n      final Integer splitCount = RelMetadataQuery.instance().splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (memoryWithinPhase == null || splitCount == null) {\n        return null;\n      }\n\n      return memoryWithinPhase / splitCount;\n    }",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193 +\n 194 +\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "    @Override\n    public Double getCumulativeMemoryWithinPhaseSplit(HiveJoin join) {\n      JoinAlgorithm oldAlgo = join.getJoinAlgorithm();\n      join.setJoinAlgorithm(TezCommonJoinAlgorithm.INSTANCE);\n\n      final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n      final Double memoryWithinPhase = mq.cumulativeMemoryWithinPhase(join);\n      final Integer splitCount = mq.splitCount(join);\n      join.setJoinAlgorithm(oldAlgo);\n\n      if (memoryWithinPhase == null || splitCount == null) {\n        return null;\n      }\n\n      return memoryWithinPhase / splitCount;\n    }"
        ],
        [
            "HiveAlgorithmsUtil::getSplitCountWithoutRepartition(HiveJoin)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 -\n 362  ",
            "  public static Integer getSplitCountWithoutRepartition(HiveJoin join) {\n    RelNode largeInput;\n    if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n      largeInput = join.getLeft();\n    } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n      largeInput = join.getRight();\n    } else {\n      return null;\n    }\n    return RelMetadataQuery.instance().splitCount(largeInput);\n  }",
            " 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364  ",
            "  public static Integer getSplitCountWithoutRepartition(HiveJoin join) {\n    RelNode largeInput;\n    if (join.getStreamingSide() == MapJoinStreamingRelation.LEFT_RELATION) {\n      largeInput = join.getLeft();\n    } else if (join.getStreamingSide() == MapJoinStreamingRelation.RIGHT_RELATION) {\n      largeInput = join.getRight();\n    } else {\n      return null;\n    }\n    final RelMetadataQuery mq = join.getCluster().getMetadataQuery();\n    return mq.splitCount(largeInput);\n  }"
        ],
        [
            "HiveJoinPushTransitivePredicatesRule::onMatch(RelOptRuleCall)",
            "  79  \n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    Join join = call.rel(0);\n\n    RelOptPredicateList preds = RelMetadataQuery.instance().getPulledUpPredicates(join);\n\n    HiveRulesRegistry registry = call.getPlanner().getContext().unwrap(HiveRulesRegistry.class);\n    assert registry != null;\n    RexBuilder rB = join.getCluster().getRexBuilder();\n    RelNode lChild = join.getLeft();\n    RelNode rChild = join.getRight();\n\n    Set<String> leftPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 0));\n    List<RexNode> leftPreds = getValidPreds(join.getCluster(), lChild,\n            leftPushedPredicates, preds.leftInferredPredicates, lChild.getRowType());\n    Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n    List<RexNode> rightPreds = getValidPreds(join.getCluster(), rChild,\n            rightPushedPredicates, preds.rightInferredPredicates, rChild.getRowType());\n\n    RexNode newLeftPredicate = RexUtil.composeConjunction(rB, leftPreds, false);\n    RexNode newRightPredicate = RexUtil.composeConjunction(rB, rightPreds, false);\n    if (newLeftPredicate.isAlwaysTrue() && newRightPredicate.isAlwaysTrue()) {\n      return;\n    }\n\n    if (!newLeftPredicate.isAlwaysTrue()) {\n      RelNode curr = lChild;\n      lChild = filterFactory.createFilter(lChild, newLeftPredicate);\n      call.getPlanner().onCopy(curr, lChild);\n    }\n\n    if (!newRightPredicate.isAlwaysTrue()) {\n      RelNode curr = rChild;\n      rChild = filterFactory.createFilter(rChild, newRightPredicate);\n      call.getPlanner().onCopy(curr, rChild);\n    }\n\n    RelNode newRel = join.copy(join.getTraitSet(), join.getCondition(),\n        lChild, rChild, join.getJoinType(), join.isSemiJoinDone());\n    call.getPlanner().onCopy(join, newRel);\n\n    // Register information about pushed predicates\n    registry.getPushedPredicates(newRel, 0).addAll(leftPushedPredicates);\n    registry.getPushedPredicates(newRel, 1).addAll(rightPushedPredicates);\n\n    call.transformTo(newRel);\n  }",
            "  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "  @Override\n  public void onMatch(RelOptRuleCall call) {\n    Join join = call.rel(0);\n\n    RelOptPredicateList preds = call.getMetadataQuery().getPulledUpPredicates(join);\n\n    HiveRulesRegistry registry = call.getPlanner().getContext().unwrap(HiveRulesRegistry.class);\n    assert registry != null;\n    RexBuilder rB = join.getCluster().getRexBuilder();\n    RelNode lChild = join.getLeft();\n    RelNode rChild = join.getRight();\n\n    Set<String> leftPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 0));\n    List<RexNode> leftPreds = getValidPreds(join.getCluster(), lChild,\n            leftPushedPredicates, preds.leftInferredPredicates, lChild.getRowType());\n    Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n    List<RexNode> rightPreds = getValidPreds(join.getCluster(), rChild,\n            rightPushedPredicates, preds.rightInferredPredicates, rChild.getRowType());\n\n    RexNode newLeftPredicate = RexUtil.composeConjunction(rB, leftPreds, false);\n    RexNode newRightPredicate = RexUtil.composeConjunction(rB, rightPreds, false);\n    if (newLeftPredicate.isAlwaysTrue() && newRightPredicate.isAlwaysTrue()) {\n      return;\n    }\n\n    if (!newLeftPredicate.isAlwaysTrue()) {\n      RelNode curr = lChild;\n      lChild = filterFactory.createFilter(lChild, newLeftPredicate);\n      call.getPlanner().onCopy(curr, lChild);\n    }\n\n    if (!newRightPredicate.isAlwaysTrue()) {\n      RelNode curr = rChild;\n      rChild = filterFactory.createFilter(rChild, newRightPredicate);\n      call.getPlanner().onCopy(curr, rChild);\n    }\n\n    RelNode newRel = join.copy(join.getTraitSet(), join.getCondition(),\n        lChild, rChild, join.getJoinType(), join.isSemiJoinDone());\n    call.getPlanner().onCopy(join, newRel);\n\n    // Register information about pushed predicates\n    registry.getPushedPredicates(newRel, 0).addAll(leftPushedPredicates);\n    registry.getPushedPredicates(newRel, 1).addAll(rightPushedPredicates);\n\n    call.transformTo(newRel);\n  }"
        ]
    ],
    "eb7a8f909b6ace29bc64995a1f14b36ad28259ef": [
        [
            "CombineHiveInputFormat::sampleSplits(List)",
            " 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 -\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "  /**\n   * This function is used to sample inputs for clauses like \"TABLESAMPLE(1 PERCENT)\"\n   *\n   * First, splits are grouped by alias they are for. If one split serves more than one\n   * alias or not for any sampled alias, we just directly add it to returned list.\n   * Then we find a list of exclusive splits for every alias to be sampled.\n   * For each alias, we start from position of seedNumber%totalNumber, and keep add\n   * splits until the total size hits percentage.\n   * @param splits\n   * @return the sampled splits\n   */\n  private List<CombineFileSplit> sampleSplits(List<CombineFileSplit> splits) {\n    HashMap<String, SplitSample> nameToSamples = mrwork.getNameToSplitSample();\n    List<CombineFileSplit> retLists = new ArrayList<CombineFileSplit>();\n    Map<String, ArrayList<CombineFileSplit>> aliasToSplitList = new HashMap<String, ArrayList<CombineFileSplit>>();\n    Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n    Map<Path, ArrayList<String>> pathToAliasesNoScheme = removeScheme(pathToAliases);\n\n    // Populate list of exclusive splits for every sampled alias\n    //\n    for (CombineFileSplit split : splits) {\n      String alias = null;\n      for (Path path : split.getPaths()) {\n        boolean schemeless = path.toUri().getScheme() == null;\n        List<String> l = HiveFileFormatUtils.doGetAliasesFromPath(\n            schemeless ? pathToAliasesNoScheme : pathToAliases, path);\n        // a path for a split unqualified the split from being sampled if:\n        // 1. it serves more than one alias\n        // 2. the alias it serves is not sampled\n        // 3. it serves different alias than another path for the same split\n        if (l.size() != 1 || !nameToSamples.containsKey(l.get(0)) ||\n            (alias != null && l.get(0) != alias)) {\n          alias = null;\n          break;\n        }\n        alias = l.get(0);\n      }\n\n      if (alias != null) {\n        // split exclusively serves alias, which needs to be sampled\n        // add it to the split list of the alias.\n        if (!aliasToSplitList.containsKey(alias)) {\n          aliasToSplitList.put(alias, new ArrayList<CombineFileSplit>());\n        }\n        aliasToSplitList.get(alias).add(split);\n      } else {\n        // The split doesn't exclusively serve one alias\n        retLists.add(split);\n      }\n    }\n\n    // for every sampled alias, we figure out splits to be sampled and add\n    // them to return list\n    //\n    for (Map.Entry<String, ArrayList<CombineFileSplit>> entry: aliasToSplitList.entrySet()) {\n      ArrayList<CombineFileSplit> splitList = entry.getValue();\n      long totalSize = 0;\n      for (CombineFileSplit split : splitList) {\n        totalSize += split.getLength();\n      }\n\n      SplitSample splitSample = nameToSamples.get(entry.getKey());\n\n      long targetSize = splitSample.getTargetSize(totalSize);\n      int startIndex = splitSample.getSeedNum() % splitList.size();\n      long size = 0;\n      for (int i = 0; i < splitList.size(); i++) {\n        CombineFileSplit split = splitList.get((startIndex + i) % splitList.size());\n        retLists.add(split);\n        long splitgLength = split.getLength();\n        if (size + splitgLength >= targetSize) {\n          LOG.info(\"Sample alias \" + entry.getValue() + \" using \" + (i + 1) + \"splits\");\n          if (size + splitgLength > targetSize) {\n            ((InputSplitShim)split).shrinkSplit(targetSize - size);\n          }\n          break;\n        }\n        size += splitgLength;\n      }\n\n    }\n\n    return retLists;\n  }",
            " 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 +\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "  /**\n   * This function is used to sample inputs for clauses like \"TABLESAMPLE(1 PERCENT)\"\n   *\n   * First, splits are grouped by alias they are for. If one split serves more than one\n   * alias or not for any sampled alias, we just directly add it to returned list.\n   * Then we find a list of exclusive splits for every alias to be sampled.\n   * For each alias, we start from position of seedNumber%totalNumber, and keep add\n   * splits until the total size hits percentage.\n   * @param splits\n   * @return the sampled splits\n   */\n  private List<CombineFileSplit> sampleSplits(List<CombineFileSplit> splits) {\n    HashMap<String, SplitSample> nameToSamples = mrwork.getNameToSplitSample();\n    List<CombineFileSplit> retLists = new ArrayList<CombineFileSplit>();\n    Map<String, ArrayList<CombineFileSplit>> aliasToSplitList = new HashMap<String, ArrayList<CombineFileSplit>>();\n    Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n    Map<Path, ArrayList<String>> pathToAliasesNoScheme = removeScheme(pathToAliases);\n\n    // Populate list of exclusive splits for every sampled alias\n    //\n    for (CombineFileSplit split : splits) {\n      String alias = null;\n      for (Path path : split.getPaths()) {\n        boolean schemeless = path.toUri().getScheme() == null;\n        List<String> l = HiveFileFormatUtils.doGetAliasesFromPath(\n            schemeless ? pathToAliasesNoScheme : pathToAliases, path);\n        // a path for a split unqualified the split from being sampled if:\n        // 1. it serves more than one alias\n        // 2. the alias it serves is not sampled\n        // 3. it serves different alias than another path for the same split\n        if (l.size() != 1 || !nameToSamples.containsKey(l.get(0)) ||\n            (alias != null && !alias.equals(l.get(0)))) {\n          alias = null;\n          break;\n        }\n        alias = l.get(0);\n      }\n\n      if (alias != null) {\n        // split exclusively serves alias, which needs to be sampled\n        // add it to the split list of the alias.\n        if (!aliasToSplitList.containsKey(alias)) {\n          aliasToSplitList.put(alias, new ArrayList<CombineFileSplit>());\n        }\n        aliasToSplitList.get(alias).add(split);\n      } else {\n        // The split doesn't exclusively serve one alias\n        retLists.add(split);\n      }\n    }\n\n    // for every sampled alias, we figure out splits to be sampled and add\n    // them to return list\n    //\n    for (Map.Entry<String, ArrayList<CombineFileSplit>> entry: aliasToSplitList.entrySet()) {\n      ArrayList<CombineFileSplit> splitList = entry.getValue();\n      long totalSize = 0;\n      for (CombineFileSplit split : splitList) {\n        totalSize += split.getLength();\n      }\n\n      SplitSample splitSample = nameToSamples.get(entry.getKey());\n\n      long targetSize = splitSample.getTargetSize(totalSize);\n      int startIndex = splitSample.getSeedNum() % splitList.size();\n      long size = 0;\n      for (int i = 0; i < splitList.size(); i++) {\n        CombineFileSplit split = splitList.get((startIndex + i) % splitList.size());\n        retLists.add(split);\n        long splitgLength = split.getLength();\n        if (size + splitgLength >= targetSize) {\n          LOG.info(\"Sample alias \" + entry.getValue() + \" using \" + (i + 1) + \"splits\");\n          if (size + splitgLength > targetSize) {\n            ((InputSplitShim)split).shrinkSplit(targetSize - size);\n          }\n          break;\n        }\n        size += splitgLength;\n      }\n\n    }\n\n    return retLists;\n  }"
        ],
        [
            "GenVectorCode::generateFilterTruncStringColumnBetween(String)",
            "1427  \n1428  \n1429  \n1430  \n1431 -\n1432  \n1433  \n1434 -\n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  ",
            "  private void generateFilterTruncStringColumnBetween(String[] tdesc) throws IOException {\n    String truncStringTypeName = tdesc[1];\n    String truncStringHiveType;\n    String truncStringHiveGetBytes;\n    if (truncStringTypeName == \"Char\") {\n      truncStringHiveType = \"HiveChar\";\n      truncStringHiveGetBytes = \"getStrippedValue().getBytes()\";\n    } else if (truncStringTypeName == \"VarChar\") {\n      truncStringHiveType = \"HiveVarchar\";\n      truncStringHiveGetBytes = \"getValue().getBytes()\";\n    } else {\n      throw new Error(\"Unsupported string type: \" + truncStringTypeName);\n    }\n    String optionalNot = tdesc[2];\n    String className = \"Filter\" + truncStringTypeName + \"Column\" + (optionalNot.equals(\"!\") ? \"Not\" : \"\")\n        + \"Between\";\n        // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<TruncStringTypeName>\", truncStringTypeName);\n    templateString = templateString.replaceAll(\"<TruncStringHiveType>\", truncStringHiveType);\n    templateString = templateString.replaceAll(\"<TruncStringHiveGetBytes>\", truncStringHiveGetBytes);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<OptionalNot>\", optionalNot);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }",
            "1427  \n1428  \n1429  \n1430  \n1431 +\n1432  \n1433  \n1434 +\n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  ",
            "  private void generateFilterTruncStringColumnBetween(String[] tdesc) throws IOException {\n    String truncStringTypeName = tdesc[1];\n    String truncStringHiveType;\n    String truncStringHiveGetBytes;\n    if (\"Char\".equals(truncStringTypeName)) {\n      truncStringHiveType = \"HiveChar\";\n      truncStringHiveGetBytes = \"getStrippedValue().getBytes()\";\n    } else if (\"VarChar\".equals(truncStringTypeName)) {\n      truncStringHiveType = \"HiveVarchar\";\n      truncStringHiveGetBytes = \"getValue().getBytes()\";\n    } else {\n      throw new Error(\"Unsupported string type: \" + truncStringTypeName);\n    }\n    String optionalNot = tdesc[2];\n    String className = \"Filter\" + truncStringTypeName + \"Column\" + (optionalNot.equals(\"!\") ? \"Not\" : \"\")\n        + \"Between\";\n        // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<TruncStringTypeName>\", truncStringTypeName);\n    templateString = templateString.replaceAll(\"<TruncStringHiveType>\", truncStringHiveType);\n    templateString = templateString.replaceAll(\"<TruncStringHiveGetBytes>\", truncStringHiveGetBytes);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<OptionalNot>\", optionalNot);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }"
        ],
        [
            "TezCompiler::SemiJoinRemovalIfNoStatsProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916 -\n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n      if (sjInfo == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      for (AggregationDesc agg : aggregationDescs) {\n        if (agg.getGenericUDAFName() != \"bloom_filter\") {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        if (udafBloomFilterEvaluator.hasHintEntries())\n          return null; // Created using hint, skip it\n\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          if (sjInfo.getIsHint()) {\n            throw new SemanticException(\"Removing hinted semijoin due to lack to stats\" +\n            \" or exceeding max bloom filter entries\");\n          }\n          // Remove the semijoin optimization branch along with ALL the mappings\n          // The parent GB2 has all the branches. Collect them and remove them.\n          for (Operator<?> op : gbOp.getChildOperators()) {\n            ReduceSinkOperator rsFinal = (ReduceSinkOperator) op;\n            TableScanOperator ts = pCtx.getRsToSemiJoinBranchInfo().\n                    get(rsFinal).getTsOp();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                      + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                      + \"Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rsFinal);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rsFinal, ts);\n          }\n          return null;\n        }\n      }\n\n      // At this point, hinted semijoin case has been handled already\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      TableScanOperator ts = sjInfo.getTsOp();\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                    + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n          GenTezUtils.removeBranch(rs);\n          GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n        }\n      }\n      return null;\n    }",
            " 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916 +\n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n      if (sjInfo == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      for (AggregationDesc agg : aggregationDescs) {\n        if (!\"bloom_filter\".equals(agg.getGenericUDAFName())) {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        if (udafBloomFilterEvaluator.hasHintEntries())\n          return null; // Created using hint, skip it\n\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          if (sjInfo.getIsHint()) {\n            throw new SemanticException(\"Removing hinted semijoin due to lack to stats\" +\n            \" or exceeding max bloom filter entries\");\n          }\n          // Remove the semijoin optimization branch along with ALL the mappings\n          // The parent GB2 has all the branches. Collect them and remove them.\n          for (Operator<?> op : gbOp.getChildOperators()) {\n            ReduceSinkOperator rsFinal = (ReduceSinkOperator) op;\n            TableScanOperator ts = pCtx.getRsToSemiJoinBranchInfo().\n                    get(rsFinal).getTsOp();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                      + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                      + \"Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rsFinal);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rsFinal, ts);\n          }\n          return null;\n        }\n      }\n\n      // At this point, hinted semijoin case has been handled already\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      TableScanOperator ts = sjInfo.getTsOp();\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                    + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n          }\n          GenTezUtils.removeBranch(rs);\n          GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n        }\n      }\n      return null;\n    }"
        ],
        [
            "DefaultAccumuloRowIdFactory::createRowId(ObjectInspector)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  ",
            "  @Override\n  public LazyObjectBase createRowId(ObjectInspector inspector) throws SerDeException {\n    // LazyObject can only be binary when it's not a string as well\n//    return LazyFactory.createLazyObject(inspector,\n//            ColumnEncoding.BINARY == rowIdMapping.getEncoding());\n    return LazyFactory.createLazyObject(inspector,\n        inspector.getTypeName() != TypeInfoFactory.stringTypeInfo.getTypeName()\n            && ColumnEncoding.BINARY == rowIdMapping.getEncoding());\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81  \n  82  ",
            "  @Override\n  public LazyObjectBase createRowId(ObjectInspector inspector) throws SerDeException {\n    // LazyObject can only be binary when it's not a string as well\n//    return LazyFactory.createLazyObject(inspector,\n//            ColumnEncoding.BINARY == rowIdMapping.getEncoding());\n    return LazyFactory.createLazyObject(inspector,\n            !TypeInfoFactory.stringTypeInfo.getTypeName().equals(inspector.getTypeName())\n            && ColumnEncoding.BINARY == rowIdMapping.getEncoding());\n  }"
        ],
        [
            "HashTableDummyOperator::equals(Object)",
            "  80  \n  81  \n  82 -\n  83 -\n  84  ",
            "  @Override\n  public boolean equals(Object obj) {\n    return super.equals(obj) || (obj instanceof HashTableDummyOperator)\n        && (((HashTableDummyOperator)obj).operatorId == operatorId);\n  }",
            "  80  \n  81  \n  82 +\n  83  ",
            "  @Override\n  public boolean equals(Object obj) {\n    return super.equals(obj) || (obj instanceof HashTableDummyOperator) && ((HashTableDummyOperator)obj).operatorId.equals(operatorId);\n  }"
        ],
        [
            "SqlFunctionConverter::getHiveUDF(SqlOperator,RelDataType,int)",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  public static GenericUDF getHiveUDF(SqlOperator op, RelDataType dt, int argsLength) {\n    String name = reverseOperatorMap.get(op);\n    if (name == null) {\n      name = op.getName();\n    }\n    // Make sure we handle unary + and - correctly.\n    if (argsLength == 1) {\n      if (name == \"+\") {\n        name = FunctionRegistry.UNARY_PLUS_FUNC_NAME;\n      } else if (name == \"-\") {\n        name = FunctionRegistry.UNARY_MINUS_FUNC_NAME;\n      }\n    }\n    FunctionInfo hFn;\n    try {\n      hFn = name != null ? FunctionRegistry.getFunctionInfo(name) : null;\n    } catch (SemanticException e) {\n      LOG.warn(\"Failed to load udf \" + name, e);\n      hFn = null;\n    }\n    if (hFn == null) {\n      try {\n        hFn = handleExplicitCast(op, dt);\n      } catch (SemanticException e) {\n        LOG.warn(\"Failed to load udf \" + name, e);\n        hFn = null;\n      }\n    }\n    return hFn == null ? null : hFn.getGenericUDF();\n  }",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  public static GenericUDF getHiveUDF(SqlOperator op, RelDataType dt, int argsLength) {\n    String name = reverseOperatorMap.get(op);\n    if (name == null) {\n      name = op.getName();\n    }\n    // Make sure we handle unary + and - correctly.\n    if (argsLength == 1) {\n      if (\"+\".equals(name)) {\n        name = FunctionRegistry.UNARY_PLUS_FUNC_NAME;\n      } else if (\"-\".equals(name)) {\n        name = FunctionRegistry.UNARY_MINUS_FUNC_NAME;\n      }\n    }\n    FunctionInfo hFn;\n    try {\n      hFn = name != null ? FunctionRegistry.getFunctionInfo(name) : null;\n    } catch (SemanticException e) {\n      LOG.warn(\"Failed to load udf \" + name, e);\n      hFn = null;\n    }\n    if (hFn == null) {\n      try {\n        hFn = handleExplicitCast(op, dt);\n      } catch (SemanticException e) {\n        LOG.warn(\"Failed to load udf \" + name, e);\n        hFn = null;\n      }\n    }\n    return hFn == null ? null : hFn.getGenericUDF();\n  }"
        ],
        [
            "HCatAuthUtil::isAuthorizationEnabled(Configuration)",
            "  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 -\n  44 -\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  ",
            "  public static boolean isAuthorizationEnabled(Configuration conf) {\n    if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n      return false;\n    }\n    // If the V2 api of authorizer in use, the session state getAuthorizer return null.\n    // Here we disable authorization if we use V2 api or the DefaultHiveAuthorizationProvider\n    // The additional authorization checks happening in hcatalog are designed to\n    // work with  storage based authorization (on client side). It should not try doing\n    // additional checks if a V2 authorizer or DefaultHiveAuthorizationProvider is in use.\n    // The recommended configuration is to use storage based authorization in metastore server.\n    // However, if user define a custom V1 authorization, it will be honored.\n    if (SessionState.get().getAuthorizer() == null ||\n        HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER)\n        == DefaultHiveAuthorizationProvider.class.getName()) {\n      LOG.info(\"Metastore authorizer is skipped for V2 authorizer or\"\n        + \" DefaultHiveAuthorizationProvider\");\n      return false;\n    }\n    return true;\n  }",
            "  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 +\n  44  \n  45  \n  46  \n  47  \n  48  \n  49  ",
            "  public static boolean isAuthorizationEnabled(Configuration conf) {\n    if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n      return false;\n    }\n    // If the V2 api of authorizer in use, the session state getAuthorizer return null.\n    // Here we disable authorization if we use V2 api or the DefaultHiveAuthorizationProvider\n    // The additional authorization checks happening in hcatalog are designed to\n    // work with  storage based authorization (on client side). It should not try doing\n    // additional checks if a V2 authorizer or DefaultHiveAuthorizationProvider is in use.\n    // The recommended configuration is to use storage based authorization in metastore server.\n    // However, if user define a custom V1 authorization, it will be honored.\n    if (SessionState.get().getAuthorizer() == null ||\n        DefaultHiveAuthorizationProvider.class.getName().equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER))) {\n      LOG.info(\"Metastore authorizer is skipped for V2 authorizer or\"\n        + \" DefaultHiveAuthorizationProvider\");\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "SQLStdHiveAccessController::initUserRoles()",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  ",
            "  /**\n   * (Re-)initialize currentRoleNames if necessary.\n   * @throws HiveAuthzPluginException\n   */\n  private void initUserRoles() throws HiveAuthzPluginException {\n    //to aid in testing through .q files, authenticator is passed as argument to\n    // the interface. this helps in being able to switch the user within a session.\n    // so we need to check if the user has changed\n    String newUserName = authenticator.getUserName();\n    if(currentUserName == newUserName){\n      //no need to (re-)initialize the currentUserName, currentRoles fields\n      return;\n    }\n    this.currentUserName = newUserName;\n    this.currentRoles = getRolesFromMS();\n    LOG.info(\"Current user : \" + currentUserName + \", Current Roles : \" + currentRoles);\n  }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  ",
            "  /**\n   * (Re-)initialize currentRoleNames if necessary.\n   * @throws HiveAuthzPluginException\n   */\n  private void initUserRoles() throws HiveAuthzPluginException {\n    //to aid in testing through .q files, authenticator is passed as argument to\n    // the interface. this helps in being able to switch the user within a session.\n    // so we need to check if the user has changed\n    String newUserName = authenticator.getUserName();\n    if (Objects.equals(currentUserName, newUserName)) {\n      //no need to (re-)initialize the currentUserName, currentRoles fields\n      return;\n    }\n    this.currentUserName = newUserName;\n    this.currentRoles = getRolesFromMS();\n    LOG.info(\"Current user : \" + currentUserName + \", Current Roles : \" + currentRoles);\n  }"
        ],
        [
            "HiveMaterializedViewsRegistry::ViewKey::equals(Object)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 -\n 362 -\n 363  ",
            "    @Override\n    public boolean equals(Object obj) {\n      if(this == obj) {\n        return true;\n      }\n      if((obj == null) || (obj.getClass() != this.getClass())) {\n        return false;\n      }\n      ViewKey viewKey = (ViewKey) obj;\n      return creationDate == viewKey.creationDate &&\n          (viewName == viewKey.viewName || (viewName != null && viewName.equals(viewKey.viewName)));\n    }",
            " 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363  ",
            "    @Override\n    public boolean equals(Object obj) {\n      if(this == obj) {\n        return true;\n      }\n      if((obj == null) || (obj.getClass() != this.getClass())) {\n        return false;\n      }\n      ViewKey viewKey = (ViewKey) obj;\n      return creationDate == viewKey.creationDate && Objects.equals(viewName, viewKey.viewName);\n    }"
        ],
        [
            "ErrorAndSolution::equals(Object)",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 -\n  49 -\n  50  ",
            "  @Override\n  public boolean equals(Object o) {\n    if (!(o instanceof ErrorAndSolution)) {\n      return false;\n    }\n    ErrorAndSolution e = (ErrorAndSolution)o;\n\n    return e.error == this.error && e.solution == this.solution;\n  }",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51  ",
            "  @Override\n  public boolean equals(Object o) {\n    if (!(o instanceof ErrorAndSolution)) {\n      return false;\n    }\n    ErrorAndSolution e = (ErrorAndSolution)o;\n    return Objects.equals(e.error, error) && Objects.equals(e.solution, solution);\n  }"
        ],
        [
            "GenVectorCode::generateStringCompareTruncStringScalar(String,String,String)",
            "2046  \n2047  \n2048  \n2049  \n2050  \n2051 -\n2052  \n2053  \n2054 -\n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  ",
            "  private void generateStringCompareTruncStringScalar(String[] tdesc, String className, String baseClassName)\n      throws IOException {\n    String truncStringTypeName = tdesc[1];\n    String truncStringHiveType;\n    String truncStringHiveGetBytes;\n    if (truncStringTypeName == \"Char\") {\n      truncStringHiveType = \"HiveChar\";\n      truncStringHiveGetBytes = \"getStrippedValue().getBytes()\";\n    } else if (truncStringTypeName == \"VarChar\") {\n      truncStringHiveType = \"HiveVarchar\";\n      truncStringHiveGetBytes = \"getValue().getBytes()\";\n    } else {\n      throw new Error(\"Unsupported string type: \" + truncStringTypeName);\n    }\n    String operatorSymbol = tdesc[3];\n    // Read the template into a string;\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    // Expand, and write result\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<BaseClassName>\", baseClassName);\n    templateString = templateString.replaceAll(\"<OperatorSymbol>\", operatorSymbol);\n    templateString = templateString.replaceAll(\"<TruncStringTypeName>\", truncStringTypeName);\n    templateString = templateString.replaceAll(\"<TruncStringHiveType>\", truncStringHiveType);\n    templateString = templateString.replaceAll(\"<TruncStringHiveGetBytes>\", truncStringHiveGetBytes);\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }",
            "2046  \n2047  \n2048  \n2049  \n2050  \n2051 +\n2052  \n2053  \n2054 +\n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  ",
            "  private void generateStringCompareTruncStringScalar(String[] tdesc, String className, String baseClassName)\n      throws IOException {\n    String truncStringTypeName = tdesc[1];\n    String truncStringHiveType;\n    String truncStringHiveGetBytes;\n    if (\"Char\".equals(truncStringTypeName)) {\n      truncStringHiveType = \"HiveChar\";\n      truncStringHiveGetBytes = \"getStrippedValue().getBytes()\";\n    } else if (\"VarChar\".equals(truncStringTypeName)) {\n      truncStringHiveType = \"HiveVarchar\";\n      truncStringHiveGetBytes = \"getValue().getBytes()\";\n    } else {\n      throw new Error(\"Unsupported string type: \" + truncStringTypeName);\n    }\n    String operatorSymbol = tdesc[3];\n    // Read the template into a string;\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    // Expand, and write result\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<BaseClassName>\", baseClassName);\n    templateString = templateString.replaceAll(\"<OperatorSymbol>\", operatorSymbol);\n    templateString = templateString.replaceAll(\"<TruncStringTypeName>\", truncStringTypeName);\n    templateString = templateString.replaceAll(\"<TruncStringHiveType>\", truncStringHiveType);\n    templateString = templateString.replaceAll(\"<TruncStringHiveGetBytes>\", truncStringHiveGetBytes);\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }"
        ]
    ],
    "262d8f992e63ac4aa65e36665fb22748546c511c": [
        [
            "SharedWorkOptimizer::areMergeable(ParseContext,SharedWorkOptimizerCache,TableScanOperator,TableScanOperator)",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447 -\n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 -\n 458 -\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469 -\n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481 -\n 482  \n 483  \n 484  \n 485  \n 486  ",
            "  private static boolean areMergeable(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,\n          TableScanOperator tsOp1, TableScanOperator tsOp2) throws SemanticException {\n    // First we check if the two table scan operators can actually be merged\n    // If schemas do not match, we currently do not merge\n    List<String> prevTsOpNeededColumns = tsOp1.getNeededColumns();\n    List<String> tsOpNeededColumns = tsOp2.getNeededColumns();\n    if (prevTsOpNeededColumns.size() != tsOpNeededColumns.size()) {\n      return false;\n    }\n    boolean notEqual = false;\n    for (int i = 0; i < prevTsOpNeededColumns.size(); i++) {\n      if (!prevTsOpNeededColumns.get(i).equals(tsOpNeededColumns.get(i))) {\n        notEqual = true;\n        break;\n      }\n    }\n    if (notEqual) {\n      return false;\n    }\n    // If row limit does not match, we currently do not merge\n    if (tsOp1.getConf().getRowLimit() != tsOp2.getConf().getRowLimit()) {\n      return false;\n    }\n    // If partitions do not match, we currently do not merge\n    PrunedPartitionList prevTsOpPPList = pctx.getPrunedPartitions(tsOp1);\n    PrunedPartitionList tsOpPPList = pctx.getPrunedPartitions(tsOp2);\n    if (!prevTsOpPPList.getPartitions().equals(tsOpPPList.getPartitions())) {\n      return false;\n    }\n    // If is a DPP, check if actually it refers to same target, column, etc.\n    // Further, the DPP value needs to be generated from same subtree\n    List<Operator<?>> dppsOp1 = new ArrayList<>(optimizerCache.tableScanToDPPSource.get(tsOp1));\n    List<Operator<?>> dppsOp2 = new ArrayList<>(optimizerCache.tableScanToDPPSource.get(tsOp2));\n    if (dppsOp1.isEmpty() && dppsOp2.isEmpty()) {\n      return true;\n    }\n    for (int i = 0; i < dppsOp1.size(); i++) {\n      Operator<?> op = dppsOp1.get(i);\n      if (op instanceof ReduceSinkOperator) {\n        Set<Operator<?>> ascendants =\n            findAscendantWorkOperators(pctx, optimizerCache, op);\n        if (ascendants.contains(tsOp2)) {\n          dppsOp1.remove(i);\n          i--;\n        }\n      }\n    }\n    for (int i = 0; i < dppsOp2.size(); i++) {\n      Operator<?> op = dppsOp2.get(i);\n      if (op instanceof ReduceSinkOperator) {\n        Set<Operator<?>> ascendants =\n            findAscendantWorkOperators(pctx, optimizerCache, op);\n        if (ascendants.contains(tsOp1)) {\n          dppsOp2.remove(i);\n          i--;\n        }\n      }\n    }\n    if (dppsOp1.size() != dppsOp2.size()) {\n      // Only first or second operator contains DPP pruning\n      return false;\n    }\n    // Check if DPP branches are equal\n    for (int i = 0; i < dppsOp1.size(); i++) {\n      Operator<?> dppOp1 = dppsOp1.get(i);\n      BitSet bs = new BitSet();\n      for (int j = 0; j < dppsOp2.size(); j++) {\n        if (!bs.get(j)) {\n          // If not visited yet\n          Operator<?> dppOp2 = dppsOp2.get(j);\n          if (compareAndGatherOps(pctx, dppOp1, dppOp2) != null) {\n            // The DPP operator/branch are equal\n            bs.set(j);\n            break;\n          }\n        }\n      }\n      if (bs.cardinality() == i) {\n        return false;\n      }\n    }\n    return true;\n  }",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 +\n 447 +\n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 +\n 458 +\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467 +\n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481 +\n 482  \n 483  \n 484  \n 485  \n 486  ",
            "  private static boolean areMergeable(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,\n          TableScanOperator tsOp1, TableScanOperator tsOp2) throws SemanticException {\n    // First we check if the two table scan operators can actually be merged\n    // If schemas do not match, we currently do not merge\n    List<String> prevTsOpNeededColumns = tsOp1.getNeededColumns();\n    List<String> tsOpNeededColumns = tsOp2.getNeededColumns();\n    if (prevTsOpNeededColumns.size() != tsOpNeededColumns.size()) {\n      return false;\n    }\n    boolean notEqual = false;\n    for (int i = 0; i < prevTsOpNeededColumns.size(); i++) {\n      if (!prevTsOpNeededColumns.get(i).equals(tsOpNeededColumns.get(i))) {\n        notEqual = true;\n        break;\n      }\n    }\n    if (notEqual) {\n      return false;\n    }\n    // If row limit does not match, we currently do not merge\n    if (tsOp1.getConf().getRowLimit() != tsOp2.getConf().getRowLimit()) {\n      return false;\n    }\n    // If partitions do not match, we currently do not merge\n    PrunedPartitionList prevTsOpPPList = pctx.getPrunedPartitions(tsOp1);\n    PrunedPartitionList tsOpPPList = pctx.getPrunedPartitions(tsOp2);\n    if (!prevTsOpPPList.getPartitions().equals(tsOpPPList.getPartitions())) {\n      return false;\n    }\n    // If is a DPP, check if actually it refers to same target, column, etc.\n    // Further, the DPP value needs to be generated from same subtree\n    List<Operator<?>> dppsOp1 = new ArrayList<>(optimizerCache.tableScanToDPPSource.get(tsOp1));\n    List<Operator<?>> dppsOp2 = new ArrayList<>(optimizerCache.tableScanToDPPSource.get(tsOp2));\n    if (dppsOp1.isEmpty() && dppsOp2.isEmpty()) {\n      return true;\n    }\n    for (int i = 0; i < dppsOp1.size(); i++) {\n      Operator<?> op = dppsOp1.get(i);\n      if (op instanceof ReduceSinkOperator) {\n        Set<Operator<?>> ascendants =\n            findAscendantWorkOperators(pctx, optimizerCache, op);\n        if (ascendants.contains(tsOp2)) {\n          // This should not happen, we cannot merge\n          return false;\n        }\n      }\n    }\n    for (int i = 0; i < dppsOp2.size(); i++) {\n      Operator<?> op = dppsOp2.get(i);\n      if (op instanceof ReduceSinkOperator) {\n        Set<Operator<?>> ascendants =\n            findAscendantWorkOperators(pctx, optimizerCache, op);\n        if (ascendants.contains(tsOp1)) {\n          // This should not happen, we cannot merge\n          return false;\n        }\n      }\n    }\n    if (dppsOp1.size() != dppsOp2.size()) {\n      // Only first or second operator contains DPP pruning\n      return false;\n    }\n    // Check if DPP branches are equal\n    BitSet bs = new BitSet();\n    for (int i = 0; i < dppsOp1.size(); i++) {\n      Operator<?> dppOp1 = dppsOp1.get(i);\n      for (int j = 0; j < dppsOp2.size(); j++) {\n        if (!bs.get(j)) {\n          // If not visited yet\n          Operator<?> dppOp2 = dppsOp2.get(j);\n          if (compareAndGatherOps(pctx, dppOp1, dppOp2) != null) {\n            // The DPP operator/branch are equal\n            bs.set(j);\n            break;\n          }\n        }\n      }\n      if (bs.cardinality() < i + 1) {\n        return false;\n      }\n    }\n    return true;\n  }"
        ],
        [
            "SharedWorkOptimizer::extractSharedOptimizationInfo(ParseContext,SharedWorkOptimizerCache,TableScanOperator,TableScanOperator)",
            " 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533 -\n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542 -\n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  private static SharedResult extractSharedOptimizationInfo(ParseContext pctx,\n          SharedWorkOptimizerCache optimizerCache,\n          TableScanOperator retainableTsOp,\n          TableScanOperator discardableTsOp) throws SemanticException {\n    Set<Operator<?>> retainableOps = new LinkedHashSet<>();\n    Set<Operator<?>> discardableOps = new LinkedHashSet<>();\n    Set<Operator<?>> discardableInputOps = new HashSet<>();\n    long dataSize = 0l;\n    long maxDataSize = 0l;\n\n    retainableOps.add(retainableTsOp);\n    discardableOps.add(discardableTsOp);\n    Operator<?> equalOp1 = retainableTsOp;\n    Operator<?> equalOp2 = discardableTsOp;\n    if (equalOp1.getNumChild() > 1 || equalOp2.getNumChild() > 1) {\n      // TODO: Support checking multiple child operators to merge further.\n      discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n      return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n    }\n    Operator<?> currentOp1 = retainableTsOp.getChildOperators().get(0);\n    Operator<?> currentOp2 = discardableTsOp.getChildOperators().get(0);\n\n    // Special treatment for Filter operator that ignores the DPP predicates\n    if (currentOp1 instanceof FilterOperator && currentOp2 instanceof FilterOperator) {\n      boolean equalFilters = false;\n      FilterDesc op1Conf = ((FilterOperator) currentOp1).getConf();\n      FilterDesc op2Conf = ((FilterOperator) currentOp2).getConf();\n\n      if (op1Conf.getIsSamplingPred() == op2Conf.getIsSamplingPred() &&\n          StringUtils.equals(op1Conf.getSampleDescExpr(), op2Conf.getSampleDescExpr())) {\n        Multiset<String> conjsOp1String = extractConjsIgnoringDPPPreds(op1Conf.getPredicate());\n        Multiset<String> conjsOp2String = extractConjsIgnoringDPPPreds(op2Conf.getPredicate());\n        if (conjsOp1String.equals(conjsOp2String)) {\n          equalFilters = true;\n        }\n      }\n\n      if (equalFilters) {\n        equalOp1 = currentOp1;\n        equalOp2 = currentOp2;\n        retainableOps.add(equalOp1);\n        discardableOps.add(equalOp2);\n        if (currentOp1.getChildOperators().size() > 1 ||\n                currentOp2.getChildOperators().size() > 1) {\n          // TODO: Support checking multiple child operators to merge further.\n          discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableInputOps));\n          discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n          discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n          return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n        }\n        currentOp1 = currentOp1.getChildOperators().get(0);\n        currentOp2 = currentOp2.getChildOperators().get(0);\n      } else {\n        // Bail out\n        discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableInputOps));\n        discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n        discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n        return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n      }\n    }\n\n    // Try to merge rest of operators\n    while (!(currentOp1 instanceof ReduceSinkOperator)) {\n      // Check whether current operators are equal\n      if (!compareOperator(pctx, currentOp1, currentOp2)) {\n        // If they are not equal, we could zip up till here\n        break;\n      }\n      if (currentOp1.getParentOperators().size() !=\n              currentOp2.getParentOperators().size()) {\n        // If they are not equal, we could zip up till here\n        break;\n      }\n      if (currentOp1.getParentOperators().size() > 1) {\n        List<Operator<?>> discardableOpsForCurrentOp = new ArrayList<>();\n        int idx = 0;\n        for (; idx < currentOp1.getParentOperators().size(); idx++) {\n          Operator<?> parentOp1 = currentOp1.getParentOperators().get(idx);\n          Operator<?> parentOp2 = currentOp2.getParentOperators().get(idx);\n          if (parentOp1 == equalOp1 && parentOp2 == equalOp2) {\n            continue;\n          }\n          if ((parentOp1 == equalOp1 && parentOp2 != equalOp2) ||\n                  (parentOp1 != equalOp1 && parentOp2 == equalOp2)) {\n            // Input operator is not in the same position\n            break;\n          }\n          // Compare input\n          List<Operator<?>> removeOpsForCurrentInput = compareAndGatherOps(pctx, parentOp1, parentOp2);\n          if (removeOpsForCurrentInput == null) {\n            // Inputs are not the same, bail out\n            break;\n          }\n          // Add inputs to ops to remove\n          discardableOpsForCurrentOp.addAll(removeOpsForCurrentInput);\n        }\n        if (idx != currentOp1.getParentOperators().size()) {\n          // If inputs are not equal, we could zip up till here\n          break;\n        }\n        discardableInputOps.addAll(discardableOpsForCurrentOp);\n      }\n\n      equalOp1 = currentOp1;\n      equalOp2 = currentOp2;\n      retainableOps.add(equalOp1);\n      discardableOps.add(equalOp2);\n      if (equalOp1 instanceof MapJoinOperator) {\n        MapJoinOperator mop = (MapJoinOperator) equalOp1;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n      if (currentOp1.getChildOperators().size() > 1 ||\n              currentOp2.getChildOperators().size() > 1) {\n        // TODO: Support checking multiple child operators to merge further.\n        break;\n      }\n      // Update for next iteration\n      currentOp1 = currentOp1.getChildOperators().get(0);\n      currentOp2 = currentOp2.getChildOperators().get(0);\n    }\n\n    // Add the rest to the memory consumption\n    Set<Operator<?>> opsWork1 = findWorkOperators(optimizerCache, currentOp1);\n    for (Operator<?> op : opsWork1) {\n      if (op instanceof MapJoinOperator && !retainableOps.contains(op)) {\n        MapJoinOperator mop = (MapJoinOperator) op;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n    }\n    Set<Operator<?>> opsWork2 = findWorkOperators(optimizerCache, currentOp2);\n    for (Operator<?> op : opsWork2) {\n      if (op instanceof MapJoinOperator && !discardableOps.contains(op)) {\n        MapJoinOperator mop = (MapJoinOperator) op;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n    }\n\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableInputOps));\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n    return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n  }",
            " 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  ",
            "  private static SharedResult extractSharedOptimizationInfo(ParseContext pctx,\n          SharedWorkOptimizerCache optimizerCache,\n          TableScanOperator retainableTsOp,\n          TableScanOperator discardableTsOp) throws SemanticException {\n    Set<Operator<?>> retainableOps = new LinkedHashSet<>();\n    Set<Operator<?>> discardableOps = new LinkedHashSet<>();\n    Set<Operator<?>> discardableInputOps = new HashSet<>();\n    long dataSize = 0l;\n    long maxDataSize = 0l;\n\n    retainableOps.add(retainableTsOp);\n    discardableOps.add(discardableTsOp);\n    Operator<?> equalOp1 = retainableTsOp;\n    Operator<?> equalOp2 = discardableTsOp;\n    if (equalOp1.getNumChild() > 1 || equalOp2.getNumChild() > 1) {\n      // TODO: Support checking multiple child operators to merge further.\n      discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n      return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n    }\n    Operator<?> currentOp1 = retainableTsOp.getChildOperators().get(0);\n    Operator<?> currentOp2 = discardableTsOp.getChildOperators().get(0);\n\n    // Special treatment for Filter operator that ignores the DPP predicates\n    if (currentOp1 instanceof FilterOperator && currentOp2 instanceof FilterOperator) {\n      boolean equalFilters = false;\n      FilterDesc op1Conf = ((FilterOperator) currentOp1).getConf();\n      FilterDesc op2Conf = ((FilterOperator) currentOp2).getConf();\n\n      if (op1Conf.getIsSamplingPred() == op2Conf.getIsSamplingPred() &&\n          StringUtils.equals(op1Conf.getSampleDescExpr(), op2Conf.getSampleDescExpr())) {\n        Multiset<String> conjsOp1String = extractConjsIgnoringDPPPreds(op1Conf.getPredicate());\n        Multiset<String> conjsOp2String = extractConjsIgnoringDPPPreds(op2Conf.getPredicate());\n        if (conjsOp1String.equals(conjsOp2String)) {\n          equalFilters = true;\n        }\n      }\n\n      if (equalFilters) {\n        equalOp1 = currentOp1;\n        equalOp2 = currentOp2;\n        retainableOps.add(equalOp1);\n        discardableOps.add(equalOp2);\n        if (currentOp1.getChildOperators().size() > 1 ||\n                currentOp2.getChildOperators().size() > 1) {\n          // TODO: Support checking multiple child operators to merge further.\n          discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n          discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n          return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n        }\n        currentOp1 = currentOp1.getChildOperators().get(0);\n        currentOp2 = currentOp2.getChildOperators().get(0);\n      } else {\n        // Bail out\n        discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n        discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n        return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n      }\n    }\n\n    // Try to merge rest of operators\n    while (!(currentOp1 instanceof ReduceSinkOperator)) {\n      // Check whether current operators are equal\n      if (!compareOperator(pctx, currentOp1, currentOp2)) {\n        // If they are not equal, we could zip up till here\n        break;\n      }\n      if (currentOp1.getParentOperators().size() !=\n              currentOp2.getParentOperators().size()) {\n        // If they are not equal, we could zip up till here\n        break;\n      }\n      if (currentOp1.getParentOperators().size() > 1) {\n        List<Operator<?>> discardableOpsForCurrentOp = new ArrayList<>();\n        int idx = 0;\n        for (; idx < currentOp1.getParentOperators().size(); idx++) {\n          Operator<?> parentOp1 = currentOp1.getParentOperators().get(idx);\n          Operator<?> parentOp2 = currentOp2.getParentOperators().get(idx);\n          if (parentOp1 == equalOp1 && parentOp2 == equalOp2) {\n            continue;\n          }\n          if ((parentOp1 == equalOp1 && parentOp2 != equalOp2) ||\n                  (parentOp1 != equalOp1 && parentOp2 == equalOp2)) {\n            // Input operator is not in the same position\n            break;\n          }\n          // Compare input\n          List<Operator<?>> removeOpsForCurrentInput = compareAndGatherOps(pctx, parentOp1, parentOp2);\n          if (removeOpsForCurrentInput == null) {\n            // Inputs are not the same, bail out\n            break;\n          }\n          // Add inputs to ops to remove\n          discardableOpsForCurrentOp.addAll(removeOpsForCurrentInput);\n        }\n        if (idx != currentOp1.getParentOperators().size()) {\n          // If inputs are not equal, we could zip up till here\n          break;\n        }\n        discardableInputOps.addAll(discardableOpsForCurrentOp);\n      }\n\n      equalOp1 = currentOp1;\n      equalOp2 = currentOp2;\n      retainableOps.add(equalOp1);\n      discardableOps.add(equalOp2);\n      if (equalOp1 instanceof MapJoinOperator) {\n        MapJoinOperator mop = (MapJoinOperator) equalOp1;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n      if (currentOp1.getChildOperators().size() > 1 ||\n              currentOp2.getChildOperators().size() > 1) {\n        // TODO: Support checking multiple child operators to merge further.\n        break;\n      }\n      // Update for next iteration\n      currentOp1 = currentOp1.getChildOperators().get(0);\n      currentOp2 = currentOp2.getChildOperators().get(0);\n    }\n\n    // Add the rest to the memory consumption\n    Set<Operator<?>> opsWork1 = findWorkOperators(optimizerCache, currentOp1);\n    for (Operator<?> op : opsWork1) {\n      if (op instanceof MapJoinOperator && !retainableOps.contains(op)) {\n        MapJoinOperator mop = (MapJoinOperator) op;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n    }\n    Set<Operator<?>> opsWork2 = findWorkOperators(optimizerCache, currentOp2);\n    for (Operator<?> op : opsWork2) {\n      if (op instanceof MapJoinOperator && !discardableOps.contains(op)) {\n        MapJoinOperator mop = (MapJoinOperator) op;\n        dataSize = StatsUtils.safeAdd(dataSize, mop.getConf().getInMemoryDataSize());\n        maxDataSize = mop.getConf().getMemoryMonitorInfo().getAdjustedNoConditionalTaskSize();\n      }\n    }\n\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableInputOps));\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, discardableOps));\n    discardableInputOps.addAll(gatherDPPBranchOps(pctx, optimizerCache, retainableOps, discardableInputOps));\n    return new SharedResult(retainableOps, discardableOps, discardableInputOps, dataSize, maxDataSize);\n  }"
        ]
    ],
    "5971015ee50dbbf729625a94dfc891d994e145a9": [
        [
            "AlterPartitionHandler::handle(Context)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    LOG.info(\"Processing#{} ALTER_PARTITION message : {}\", fromEventId(), event.getMessage());\n\n    if (Scenario.ALTER == scenario) {\n      withinContext.replicationSpec.setIsMetadataOnly(true);\n      Table qlMdTable = new Table(tableObject);\n      List<Partition> partitions = new ArrayList<>();\n      partitions.add(new Partition(qlMdTable, after));\n      Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n      EximUtil.createExportDump(\n          metaDataPath.getFileSystem(withinContext.hiveConf),\n          metaDataPath,\n          qlMdTable,\n          partitions,\n          withinContext.replicationSpec);\n    }\n    DumpMetaData dmd = withinContext.createDmd(this);\n    dmd.setPayload(event.getMessage());\n    dmd.write();\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    LOG.info(\"Processing#{} ALTER_PARTITION message : {}\", fromEventId(), event.getMessage());\n\n    Table qlMdTable = new Table(tableObject);\n    if (!EximUtil.shouldExportTable(withinContext.replicationSpec, qlMdTable)) {\n      return;\n    }\n\n    if (Scenario.ALTER == scenario) {\n      withinContext.replicationSpec.setIsMetadataOnly(true);\n      List<Partition> partitions = new ArrayList<>();\n      partitions.add(new Partition(qlMdTable, after));\n      Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n      EximUtil.createExportDump(\n          metaDataPath.getFileSystem(withinContext.hiveConf),\n          metaDataPath,\n          qlMdTable,\n          partitions,\n          withinContext.replicationSpec);\n    }\n    DumpMetaData dmd = withinContext.createDmd(this);\n    dmd.setPayload(event.getMessage());\n    dmd.write();\n  }"
        ],
        [
            "AlterTableHandler::handle(Context)",
            "  77  \n  78  \n  79 -\n  80 -\n  81 -\n  82 -\n  83 -\n  84 -\n  85 -\n  86 -\n  87 -\n  88 -\n  89 -\n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95  \n  96  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    {\n      LOG.info(\"Processing#{} ALTER_TABLE message : {}\", fromEventId(), event.getMessage());\n      if (Scenario.ALTER == scenario) {\n        withinContext.replicationSpec.setIsMetadataOnly(true);\n        Table qlMdTableAfter = new Table(after);\n        Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n        EximUtil.createExportDump(\n            metaDataPath.getFileSystem(withinContext.hiveConf),\n            metaDataPath,\n            qlMdTableAfter,\n            null,\n            withinContext.replicationSpec);\n      }\n      DumpMetaData dmd = withinContext.createDmd(this);\n      dmd.setPayload(event.getMessage());\n      dmd.write();\n    }\n  }",
            "  78  \n  79  \n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97  \n  98 +\n  99 +\n 100 +\n 101 +\n 102  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    LOG.info(\"Processing#{} ALTER_TABLE message : {}\", fromEventId(), event.getMessage());\n\n    Table qlMdTableBefore = new Table(before);\n    if (!EximUtil.shouldExportTable(withinContext.replicationSpec, qlMdTableBefore)) {\n      return;\n    }\n\n    if (Scenario.ALTER == scenario) {\n      withinContext.replicationSpec.setIsMetadataOnly(true);\n      Table qlMdTableAfter = new Table(after);\n      Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n      EximUtil.createExportDump(\n        metaDataPath.getFileSystem(withinContext.hiveConf),\n        metaDataPath,\n        qlMdTableAfter,\n        null,\n        withinContext.replicationSpec);\n    }\n \n    DumpMetaData dmd = withinContext.createDmd(this);\n    dmd.setPayload(event.getMessage());\n    dmd.write();\n  }"
        ],
        [
            "AlterTableHandler::AlterTableHandler(NotificationEvent)",
            "  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  \n  65  ",
            "  AlterTableHandler(NotificationEvent event) throws Exception {\n    super(event);\n    AlterTableMessage atm = deserializer.getAlterTableMessage(event.getMessage());\n    org.apache.hadoop.hive.metastore.api.Table before = atm.getTableObjBefore();\n    after = atm.getTableObjAfter();\n    isTruncateOp = atm.getIsTruncateOp();\n    scenario = scenarioType(before, after);\n  }",
            "  59  \n  60  \n  61  \n  62 +\n  63  \n  64  \n  65  \n  66  ",
            "  AlterTableHandler(NotificationEvent event) throws Exception {\n    super(event);\n    AlterTableMessage atm = deserializer.getAlterTableMessage(event.getMessage());\n    before = atm.getTableObjBefore();\n    after = atm.getTableObjAfter();\n    isTruncateOp = atm.getIsTruncateOp();\n    scenario = scenarioType(before, after);\n  }"
        ],
        [
            "TestReplicationScenarios::testSkipTables()",
            "3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022 -\n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030 -\n3031 -\n3032  \n3033  \n3034  \n3035  \n3036  \n3037  ",
            "  @Test\n  public void testSkipTables() throws IOException {\n    String testName = \"skipTables\";\n    String dbName = createDB(testName, driver);\n\n    // Create table\n    run(\"CREATE TABLE \" + dbName + \".acid_table (key int, value int) PARTITIONED BY (load_date date) \" +\n        \"CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\", driver);\n    verifyIfTableExist(dbName, \"acid_table\", metaStoreClient);\n\n    // Bootstrap test\n    advanceDumpDir();\n    run(\"REPL DUMP \" + dbName, driver);\n    String replDumpLocn = getResult(0, 0,driver);\n    String replDumpId = getResult(0, 1, true, driver);\n    LOG.info(\"Bootstrap-Dump: Dumped to {} with id {}\", replDumpLocn, replDumpId);\n    run(\"REPL LOAD \" + dbName + \"_dupe FROM '\" + replDumpLocn + \"'\", driverMirror);\n    verifyIfTableNotExist(dbName + \"_dupe\", \"acid_table\", metaStoreClientMirror);\n\n    // // Create another table for incremental repl verification\n    run(\"CREATE TABLE \" + dbName + \".acid_table_incremental (key int, value int) PARTITIONED BY (load_date date) \" +\n        \"CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\", driver);\n    verifyIfTableExist(dbName, \"acid_table_incremental\", metaStoreClient);\n\n    // Perform REPL-DUMP/LOAD\n    advanceDumpDir();\n    run(\"REPL DUMP \" + dbName + \" FROM \" + replDumpId, driver);\n    String incrementalDumpLocn = getResult(0,0,driver);\n    String incrementalDumpId = getResult(0,1,true,driver);\n    LOG.info(\"Incremental-dump: Dumped to {} with id {}\", incrementalDumpLocn, incrementalDumpId);\n    run(\"EXPLAIN REPL LOAD \" + dbName + \"_dupe FROM '\" + incrementalDumpLocn + \"'\", driverMirror);\n    printOutput(driverMirror);\n    run(\"REPL LOAD \" + dbName + \"_dupe FROM '\"+incrementalDumpLocn+\"'\", driverMirror);\n    verifyIfTableNotExist(dbName + \"_dupe\", \"acid_table_incremental\", metaStoreClientMirror);\n  }",
            "3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022 +\n3023 +\n3024 +\n3025 +\n3026 +\n3027 +\n3028 +\n3029 +\n3030 +\n3031 +\n3032 +\n3033 +\n3034 +\n3035 +\n3036 +\n3037 +\n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045 +\n3046 +\n3047  \n3048  \n3049  \n3050  \n3051  \n3052  ",
            "  @Test\n  public void testSkipTables() throws IOException {\n    String testName = \"skipTables\";\n    String dbName = createDB(testName, driver);\n\n    // Create table\n    run(\"CREATE TABLE \" + dbName + \".acid_table (key int, value int) PARTITIONED BY (load_date date) \" +\n        \"CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\", driver);\n    verifyIfTableExist(dbName, \"acid_table\", metaStoreClient);\n\n    // Bootstrap test\n    advanceDumpDir();\n    run(\"REPL DUMP \" + dbName, driver);\n    String replDumpLocn = getResult(0, 0,driver);\n    String replDumpId = getResult(0, 1, true, driver);\n    LOG.info(\"Bootstrap-Dump: Dumped to {} with id {}\", replDumpLocn, replDumpId);\n    run(\"REPL LOAD \" + dbName + \"_dupe FROM '\" + replDumpLocn + \"'\", driverMirror);\n    verifyIfTableNotExist(dbName + \"_dupe\", \"acid_table\", metaStoreClientMirror);\n\n    // Test alter table\n    run(\"ALTER TABLE \" + dbName + \".acid_table RENAME TO \" + dbName + \".acid_table_rename\", driver);\n    verifyIfTableExist(dbName, \"acid_table_rename\", metaStoreClient);\n\n    // Perform REPL-DUMP/LOAD\n    advanceDumpDir();\n    run(\"REPL DUMP \" + dbName + \" FROM \" + replDumpId, driver);\n    String incrementalDumpLocn = getResult(0,0,driver);\n    String incrementalDumpId = getResult(0,1,true,driver);\n    LOG.info(\"Incremental-dump: Dumped to {} with id {}\", incrementalDumpLocn, incrementalDumpId);\n    run(\"EXPLAIN REPL LOAD \" + dbName + \"_dupe FROM '\" + incrementalDumpLocn + \"'\", driverMirror);\n    printOutput(driverMirror);\n    run(\"REPL LOAD \" + dbName + \"_dupe FROM '\"+incrementalDumpLocn+\"'\", driverMirror);\n    verifyIfTableNotExist(dbName + \"_dupe\", \"acid_table_rename\", metaStoreClientMirror);\n\n    // Create another table for incremental repl verification\n    run(\"CREATE TABLE \" + dbName + \".acid_table_incremental (key int, value int) PARTITIONED BY (load_date date) \" +\n        \"CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\", driver);\n    verifyIfTableExist(dbName, \"acid_table_incremental\", metaStoreClient);\n\n    // Perform REPL-DUMP/LOAD\n    advanceDumpDir();\n    run(\"REPL DUMP \" + dbName + \" FROM \" + replDumpId, driver);\n    incrementalDumpLocn = getResult(0,0,driver);\n    incrementalDumpId = getResult(0,1,true,driver);\n    LOG.info(\"Incremental-dump: Dumped to {} with id {}\", incrementalDumpLocn, incrementalDumpId);\n    run(\"EXPLAIN REPL LOAD \" + dbName + \"_dupe FROM '\" + incrementalDumpLocn + \"'\", driverMirror);\n    printOutput(driverMirror);\n    run(\"REPL LOAD \" + dbName + \"_dupe FROM '\"+incrementalDumpLocn+\"'\", driverMirror);\n    verifyIfTableNotExist(dbName + \"_dupe\", \"acid_table_incremental\", metaStoreClientMirror);\n  }"
        ],
        [
            "InsertHandler::handle(Context)",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    InsertMessage insertMsg = deserializer.getInsertMessage(event.getMessage());\n    org.apache.hadoop.hive.ql.metadata.Table qlMdTable = tableObject(insertMsg);\n    List<Partition> qlPtns = null;\n    if (qlMdTable.isPartitioned() && (null != insertMsg.getPtnObj())) {\n      qlPtns = Collections.singletonList(partitionObject(qlMdTable, insertMsg));\n    }\n    Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n\n    // Mark the replace type based on INSERT-INTO or INSERT_OVERWRITE operation\n    withinContext.replicationSpec.setIsReplace(insertMsg.isReplace());\n    EximUtil.createExportDump(metaDataPath.getFileSystem(withinContext.hiveConf), metaDataPath,\n        qlMdTable, qlPtns,\n        withinContext.replicationSpec);\n    Iterable<String> files = insertMsg.getFiles();\n\n    if (files != null) {\n      Path dataPath;\n      if ((null == qlPtns) || qlPtns.isEmpty()) {\n        dataPath = new Path(withinContext.eventRoot, EximUtil.DATA_PATH_NAME);\n      } else {\n        /*\n         * Insert into/overwrite operation shall operate on one or more partitions or even partitions from multiple\n         * tables. But, Insert event is generated for each partition to which the data is inserted. So, qlPtns list\n         * will have only one entry.\n         */\n        assert(1 == qlPtns.size());\n        dataPath = new Path(withinContext.eventRoot, qlPtns.get(0).getName());\n      }\n\n      // encoded filename/checksum of files, write into _files\n      try (BufferedWriter fileListWriter = writer(withinContext, dataPath)) {\n        for (String file : files) {\n          fileListWriter.write(file + \"\\n\");\n        }\n      }\n    }\n\n    LOG.info(\"Processing#{} INSERT message : {}\", fromEventId(), event.getMessage());\n    DumpMetaData dmd = withinContext.createDmd(this);\n    dmd.setPayload(event.getMessage());\n    dmd.write();\n  }",
            "  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48 +\n  49 +\n  50 +\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    InsertMessage insertMsg = deserializer.getInsertMessage(event.getMessage());\n    org.apache.hadoop.hive.ql.metadata.Table qlMdTable = tableObject(insertMsg);\n\n    if (!EximUtil.shouldExportTable(withinContext.replicationSpec, qlMdTable)) {\n      return;\n    }\n\n    List<Partition> qlPtns = null;\n    if (qlMdTable.isPartitioned() && (null != insertMsg.getPtnObj())) {\n      qlPtns = Collections.singletonList(partitionObject(qlMdTable, insertMsg));\n    }\n    Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n\n    // Mark the replace type based on INSERT-INTO or INSERT_OVERWRITE operation\n    withinContext.replicationSpec.setIsReplace(insertMsg.isReplace());\n    EximUtil.createExportDump(metaDataPath.getFileSystem(withinContext.hiveConf), metaDataPath,\n        qlMdTable, qlPtns,\n        withinContext.replicationSpec);\n    Iterable<String> files = insertMsg.getFiles();\n\n    if (files != null) {\n      Path dataPath;\n      if ((null == qlPtns) || qlPtns.isEmpty()) {\n        dataPath = new Path(withinContext.eventRoot, EximUtil.DATA_PATH_NAME);\n      } else {\n        /*\n         * Insert into/overwrite operation shall operate on one or more partitions or even partitions from multiple\n         * tables. But, Insert event is generated for each partition to which the data is inserted. So, qlPtns list\n         * will have only one entry.\n         */\n        assert(1 == qlPtns.size());\n        dataPath = new Path(withinContext.eventRoot, qlPtns.get(0).getName());\n      }\n\n      // encoded filename/checksum of files, write into _files\n      try (BufferedWriter fileListWriter = writer(withinContext, dataPath)) {\n        for (String file : files) {\n          fileListWriter.write(file + \"\\n\");\n        }\n      }\n    }\n\n    LOG.info(\"Processing#{} INSERT message : {}\", fromEventId(), event.getMessage());\n    DumpMetaData dmd = withinContext.createDmd(this);\n    dmd.setPayload(event.getMessage());\n    dmd.write();\n  }"
        ],
        [
            "AddPartitionHandler::handle(Context)",
            "  45  \n  46  \n  47 -\n  48  \n  49 -\n  50 -\n  51 -\n  52 -\n  53 -\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    AddPartitionMessage apm = deserializer.getAddPartitionMessage(event.getMessage());\n    LOG.info(\"Processing#{} ADD_PARTITION message : {}\", fromEventId(), event.getMessage());\n    Iterable<org.apache.hadoop.hive.metastore.api.Partition> ptns = apm.getPartitionObjs();\n    if ((ptns == null) || (!ptns.iterator().hasNext())) {\n      LOG.debug(\"Event#{} was an ADD_PTN_EVENT with no partitions\");\n      return;\n    }\n    org.apache.hadoop.hive.metastore.api.Table tobj = apm.getTableObj();\n    if (tobj == null) {\n      LOG.debug(\"Event#{} was a ADD_PTN_EVENT with no table listed\");\n      return;\n    }\n\n    final Table qlMdTable = new Table(tobj);\n    Iterable<Partition> qlPtns = Iterables.transform(\n        ptns,\n        new Function<org.apache.hadoop.hive.metastore.api.Partition, Partition>() {\n          @Nullable\n          @Override\n          public Partition apply(@Nullable org.apache.hadoop.hive.metastore.api.Partition input) {\n            if (input == null) {\n              return null;\n            }\n            try {\n              return new Partition(qlMdTable, input);\n            } catch (HiveException e) {\n              throw new IllegalArgumentException(e);\n            }\n          }\n        }\n    );\n\n    Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n    EximUtil.createExportDump(\n        metaDataPath.getFileSystem(withinContext.hiveConf),\n        metaDataPath,\n        qlMdTable,\n        qlPtns,\n        withinContext.replicationSpec);\n\n    Iterator<PartitionFiles> partitionFilesIter = apm.getPartitionFilesIter().iterator();\n    for (Partition qlPtn : qlPtns) {\n      Iterable<String> files = partitionFilesIter.next().getFiles();\n      if (files != null) {\n        // encoded filename/checksum of files, write into _files\n        try (BufferedWriter fileListWriter = writer(withinContext, qlPtn)) {\n          for (String file : files) {\n            fileListWriter.write(file);\n            fileListWriter.newLine();\n          }\n        }\n      }\n    }\n    withinContext.createDmd(this).write();\n  }",
            "  45  \n  46  \n  47  \n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65 +\n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "  @Override\n  public void handle(Context withinContext) throws Exception {\n    LOG.info(\"Processing#{} ADD_PARTITION message : {}\", fromEventId(), event.getMessage());\n\n    AddPartitionMessage apm = deserializer.getAddPartitionMessage(event.getMessage());\n    org.apache.hadoop.hive.metastore.api.Table tobj = apm.getTableObj();\n    if (tobj == null) {\n      LOG.debug(\"Event#{} was a ADD_PTN_EVENT with no table listed\");\n      return;\n    }\n\n    final Table qlMdTable = new Table(tobj);\n    if (!EximUtil.shouldExportTable(withinContext.replicationSpec, qlMdTable)) {\n      return;\n    }\n\n    Iterable<org.apache.hadoop.hive.metastore.api.Partition> ptns = apm.getPartitionObjs();\n    if ((ptns == null) || (!ptns.iterator().hasNext())) {\n      LOG.debug(\"Event#{} was an ADD_PTN_EVENT with no partitions\");\n      return;\n    }\n\n    Iterable<Partition> qlPtns = Iterables.transform(\n        ptns,\n        new Function<org.apache.hadoop.hive.metastore.api.Partition, Partition>() {\n          @Nullable\n          @Override\n          public Partition apply(@Nullable org.apache.hadoop.hive.metastore.api.Partition input) {\n            if (input == null) {\n              return null;\n            }\n            try {\n              return new Partition(qlMdTable, input);\n            } catch (HiveException e) {\n              throw new IllegalArgumentException(e);\n            }\n          }\n        }\n    );\n\n    Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);\n    EximUtil.createExportDump(\n        metaDataPath.getFileSystem(withinContext.hiveConf),\n        metaDataPath,\n        qlMdTable,\n        qlPtns,\n        withinContext.replicationSpec);\n\n    Iterator<PartitionFiles> partitionFilesIter = apm.getPartitionFilesIter().iterator();\n    for (Partition qlPtn : qlPtns) {\n      Iterable<String> files = partitionFilesIter.next().getFiles();\n      if (files != null) {\n        // encoded filename/checksum of files, write into _files\n        try (BufferedWriter fileListWriter = writer(withinContext, qlPtn)) {\n          for (String file : files) {\n            fileListWriter.write(file);\n            fileListWriter.newLine();\n          }\n        }\n      }\n    }\n    withinContext.createDmd(this).write();\n  }"
        ]
    ],
    "1e4097ef767246538c8386f52c49f5207433a98e": [
        [
            "LlapRecordReader::setError(Throwable)",
            " 379  \n 380  \n 381  \n 382 -\n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "  @Override\n  public void setError(Throwable t) {\n    counters.incrCounter(LlapIOCounters.NUM_ERRORS);\n    LlapIoImpl.LOG.info(\"setError called; closed {}, done {}, err {}, pending {}\",\n        isClosed, isDone, pendingError, pendingData.size());\n    assert t != null;\n    synchronized (pendingData) {\n      pendingError = t;\n      pendingData.notifyAll();\n    }\n  }",
            " 379  \n 380  \n 381  \n 382 +\n 383  \n 384 +\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  ",
            "  @Override\n  public void setError(Throwable t) {\n    counters.incrCounter(LlapIOCounters.NUM_ERRORS);\n    LlapIoImpl.LOG.debug(\"setError called; current state closed {}, done {}, err {}, pending {}\",\n        isClosed, isDone, pendingError, pendingData.size());\n    LlapIoImpl.LOG.warn(\"setError called with an error\", t);\n    assert t != null;\n    synchronized (pendingData) {\n      pendingError = t;\n      pendingData.notifyAll();\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedColumns(int,StripeInformation,OrcProto,List,List,boolean,boolean,Consumer)",
            " 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 -\n 379 -\n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 -\n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465 -\n 466  \n 467 -\n 468 -\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481 -\n 482 -\n 483 -\n 484 -\n 485 -\n 486 -\n 487 -\n 488 -\n 489 -\n 490 -\n 491 -\n 492 -\n 493 -\n 494 -\n 495 -\n 496 -\n 497 -\n 498 -\n 499 -\n 500  \n 501 -\n 502  \n 503  \n 504  \n 505 -\n 506 -\n 507 -\n 508 -\n 509  \n 510  \n 511  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[] rgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      ColumnEncoding enc = encodings.get(i);\n      colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n      trace.logColumnRead(i, colRgIx, enc.getKind());\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        trace.logSkipStream(colIx, streamKind, offset, length);\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, false);\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, rgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (rgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(\n        listToRead.get(), stripeOffset, hasFileId, toRelease);\n\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    try {\n      // 4. Finally, decompress data, map per RG, and return to caller.\n      // We go by RG and not by column because that is how data is processed.\n      int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n      for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n        if (rgs != null && !rgs[rgIx]) {\n          continue; // RG filtered.\n        }\n        boolean isLastRg = rgIx == rgCount - 1;\n        // Create the batch we will use to return data for this RG.\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        trace.logStartRg(rgIx);\n        boolean hasError = true;\n        try {\n          ecb.init(fileKey, stripeIx, rgIx, included.length);\n          for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n            ColumnReadContext ctx = colCtxs[colIx];\n            if (ctx == null) continue; // This column is not included.\n            if (isTracingEnabled) {\n              LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n            }\n            OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n                nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n            ecb.initOrcColumn(ctx.colIx);\n            trace.logStartCol(ctx.colIx);\n            for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n              StreamContext sctx = ctx.streams[streamIx];\n              ColumnStreamData cb = null;\n              try {\n                if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n                  // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n                  if (isTracingEnabled) {\n                    LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                        + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n                  }\n                  trace.logStartStripeStream(sctx.kind);\n                  if (sctx.stripeLevelStream == null) {\n                    sctx.stripeLevelStream = POOLS.csdPool.take();\n                    // We will be using this for each RG while also sending RGs to processing.\n                    // To avoid buffers being unlocked, run refcount one ahead; so each RG \n                    // processing will decref once, and the last one will unlock the buffers.\n                    sctx.stripeLevelStream.incRef();\n                    // For stripe-level streams we don't need the extra refcount on the block.\n                    // See class comment about refcounts.\n                    long unlockUntilCOffset = sctx.offset + sctx.length;\n                    DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                        sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                        unlockUntilCOffset, sctx.offset, toRelease);\n                    if (lastCached != null) {\n                      iter = lastCached;\n                    }\n                  }\n                  sctx.stripeLevelStream.incRef();\n                  cb = sctx.stripeLevelStream;\n                } else {\n                  // This stream can be separated by RG using index. Let's do that.\n                  // Offset to where this RG begins.\n                  long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n                  // Offset relative to the beginning of the stream of where this RG ends.\n                  long nextCOffsetRel = isLastRg ? sctx.length\n                      : nextIndex.getPositions(sctx.streamIndexOffset);\n                  // Offset before which this RG is guaranteed to end. Can only be estimated.\n                  // We estimate the same way for compressed and uncompressed for now.\n                  long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                      isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n                  // As we read, we can unlock initial refcounts for the buffers that end before\n                  // the data that we need for this RG.\n                  long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n                  cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx,\n                      cOffset, endCOffset, isCompressed, unlockUntilCOffset);\n                  boolean isStartOfStream = sctx.bufferIter == null;\n                  DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                      (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                      unlockUntilCOffset, sctx.offset, toRelease);\n                  if (lastCached != null) {\n                    sctx.bufferIter = iter = lastCached;\n                  }\n                }\n                ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n              } catch (Exception ex) {\n                DiskRangeList drl = toRead == null ? null : toRead.next;\n                LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                    + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n                throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n              }\n            }\n          }\n          hasError = false;\n        } finally {\n          if (hasError) {\n            releaseEcbRefCountsOnError(ecb);\n          }\n        }\n        // After this, the non-initial refcounts are the responsibility of the consumer.\n        consumer.consumeData(ecb);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after preparing all the data \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PREREAD);\n    } finally {\n      // Release the unreleased stripe-level buffers. See class comment about refcounts.\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ColumnReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          if (sctx == null || sctx.stripeLevelStream == null) continue;\n          if (0 != sctx.stripeLevelStream.decRef()) continue;\n          // Note - this is a little bit confusing; the special treatment of stripe-level buffers\n          // is because we run the ColumnStreamData refcount one ahead (as specified above). It\n          // may look like this would release the buffers too many times (one release from the\n          // consumer, one from releaseInitialRefcounts below, and one here); however, this is\n          // merely handling a special case where all the batches that are sharing the stripe-\n          // level stream have been processed before we got here; they have all decRef-ed the CSD,\n          // but have not released the buffers because of that extra refCount. So, this is\n          // essentially the \"consumer\" refcount being released here.\n          for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Unlocking {} at the end of processing\", buf);\n            }\n            cacheWrapper.releaseBuffer(buf);\n          }\n        }\n      }\n\n      releaseInitialRefcounts(toRead.next);\n      // Release buffers as we are done with all the streams... also see toRelease comment.\n      releaseBuffers(toRelease.keySet(), true);\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }",
            " 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 +\n 378 +\n 379 +\n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390 +\n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466 +\n 467  \n 468 +\n 469 +\n 470 +\n 471 +\n 472 +\n 473 +\n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 +\n 486  \n 487 +\n 488 +\n 489 +\n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496 +\n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502 +\n 503 +\n 504 +\n 505 +\n 506 +\n 507 +\n 508 +\n 509  \n 510  \n 511  \n 512 +\n 513 +\n 514 +\n 515 +\n 516 +\n 517 +\n 518  \n 519  \n 520  \n 521  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] included, boolean[] rgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[included.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < included.length; ++i) {\n      if (!included[i]) continue;\n      ColumnEncoding enc = encodings.get(i);\n      colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n      trace.logColumnRead(i, colRgIx, enc.getKind());\n    }\n    boolean isCompressed = (codec != null);\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!included[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || included[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        trace.logSkipStream(colIx, streamKind, offset, length);\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, false);\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, rgs,\n            codec != null, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (rgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, included.length);\n        consumer.consumeData(ecb);\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(\n        listToRead.get(), stripeOffset, hasFileId, toRelease);\n\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    boolean hasError = true;\n    try {\n      int rgCount = (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n      for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n        if (rgs != null && !rgs[rgIx]) {\n          continue; // RG filtered.\n        }\n        boolean isLastRg = rgIx == rgCount - 1;\n        // Create the batch we will use to return data for this RG.\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        trace.logStartRg(rgIx);\n        boolean hasErrorForEcb = true;\n        try {\n          ecb.init(fileKey, stripeIx, rgIx, included.length);\n          for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n            ColumnReadContext ctx = colCtxs[colIx];\n            if (ctx == null) continue; // This column is not included.\n            if (isTracingEnabled) {\n              LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n            }\n            OrcProto.RowIndexEntry index = ctx.rowIndex.getEntry(rgIx),\n                nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n            ecb.initOrcColumn(ctx.colIx);\n            trace.logStartCol(ctx.colIx);\n            for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n              StreamContext sctx = ctx.streams[streamIx];\n              ColumnStreamData cb = null;\n              try {\n                if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {\n                  // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n                  if (isTracingEnabled) {\n                    LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                        + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n                  }\n                  trace.logStartStripeStream(sctx.kind);\n                  if (sctx.stripeLevelStream == null) {\n                    sctx.stripeLevelStream = POOLS.csdPool.take();\n                    // We will be using this for each RG while also sending RGs to processing.\n                    // To avoid buffers being unlocked, run refcount one ahead; so each RG \n                    // processing will decref once, and the last one will unlock the buffers.\n                    sctx.stripeLevelStream.incRef();\n                    // For stripe-level streams we don't need the extra refcount on the block.\n                    // See class comment about refcounts.\n                    long unlockUntilCOffset = sctx.offset + sctx.length;\n                    DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                        sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                        unlockUntilCOffset, sctx.offset, toRelease);\n                    if (lastCached != null) {\n                      iter = lastCached;\n                    }\n                  }\n                  sctx.stripeLevelStream.incRef();\n                  cb = sctx.stripeLevelStream;\n                } else {\n                  // This stream can be separated by RG using index. Let's do that.\n                  // Offset to where this RG begins.\n                  long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n                  // Offset relative to the beginning of the stream of where this RG ends.\n                  long nextCOffsetRel = isLastRg ? sctx.length\n                      : nextIndex.getPositions(sctx.streamIndexOffset);\n                  // Offset before which this RG is guaranteed to end. Can only be estimated.\n                  // We estimate the same way for compressed and uncompressed for now.\n                  long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                      isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n                  // As we read, we can unlock initial refcounts for the buffers that end before\n                  // the data that we need for this RG.\n                  long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n                  cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx,\n                      cOffset, endCOffset, isCompressed, unlockUntilCOffset);\n                  boolean isStartOfStream = sctx.bufferIter == null;\n                  DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                      (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                      unlockUntilCOffset, sctx.offset, toRelease);\n                  if (lastCached != null) {\n                    sctx.bufferIter = iter = lastCached;\n                  }\n                }\n                ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n              } catch (Exception ex) {\n                DiskRangeList drl = toRead == null ? null : toRead.next;\n                LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                    + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n                throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n              }\n            }\n          }\n          hasErrorForEcb = false;\n        } finally {\n          if (hasErrorForEcb) {\n            try {\n              releaseEcbRefCountsOnError(ecb);\n            } catch (Throwable t) {\n              LOG.error(\"Error during the cleanup of an error; ignoring\", t);\n            }\n          }\n        }\n        // After this, the non-initial refcounts are the responsibility of the consumer.\n        consumer.consumeData(ecb);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after preparing all the data \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PREREAD);\n      hasError = false;\n    } finally {\n      try {\n        // Release the unreleased stripe-level buffers. See class comment about refcounts.\n        for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n          ColumnReadContext ctx = colCtxs[colIx];\n          if (ctx == null) continue; // This column is not included.\n          for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n            StreamContext sctx = ctx.streams[streamIx];\n            if (sctx == null || sctx.stripeLevelStream == null) continue;\n            if (0 != sctx.stripeLevelStream.decRef()) continue;\n            // Note - this is a little bit confusing; the special treatment of stripe-level buffers\n            // is because we run the ColumnStreamData refcount one ahead (as specified above). It\n            // may look like this would release the buffers too many times (one release from the\n            // consumer, one from releaseInitialRefcounts below, and one here); however, this is\n            // merely handling a special case where all the batches that are sharing the stripe-\n            // level stream have been processed before we got here; they have all decRef-ed the CSD,\n            // but have not released the buffers because of that extra refCount. So, this is\n            // essentially the \"consumer\" refcount being released here.\n            for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Unlocking {} at the end of processing\", buf);\n              }\n              cacheWrapper.releaseBuffer(buf);\n            }\n          }\n        }\n        releaseInitialRefcounts(toRead.next);\n        // Release buffers as we are done with all the streams... also see toRelease comment.\n        releaseBuffers(toRelease.keySet(), true);\n      } catch (Throwable t) {\n        if (!hasError) throw new IOException(t);\n        LOG.error(\"Error during the cleanup after another error; ignoring\", t);\n      }\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }"
        ],
        [
            "EncodedReaderImpl::preReadUncompressedStreams(long,ReadContext,MutateHelper,IdentityHashMap)",
            "1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942 -\n1943 -\n1944 -\n1945 -\n1946  \n1947  \n1948  \n1949  \n1950  ",
            "  private DiskRangeList preReadUncompressedStreams(long stripeOffset, ReadContext[] colCtxs,\n      MutateHelper toRead, IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (codec != null) return toRead.next;\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    boolean hasError = true;\n    try {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length, sctx.kind);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease.clear();\n      }\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n      hasError = false;\n    } finally {\n      // At this point, everything in the list is going to have a refcount of one. Unless it\n      // failed between the allocation and the incref for a single item, we should be ok. \n      if (hasError) {\n        releaseInitialRefcounts(toRead.next);\n        if (toRelease != null) {\n          releaseBuffers(toRelease.keySet(), true);\n          toRelease.clear();\n        }\n      }\n    }\n    return toRead.next; // Reset the iter to start.\n  }",
            "1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978 +\n1979 +\n1980 +\n1981 +\n1982 +\n1983 +\n1984 +\n1985 +\n1986  \n1987  \n1988  \n1989  \n1990  ",
            "  private DiskRangeList preReadUncompressedStreams(long stripeOffset, ReadContext[] colCtxs,\n      MutateHelper toRead, IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (codec != null) return toRead.next;\n    DiskRangeList iter = toRead.next;  // Keep \"toRead\" list for future use, don't extract().\n    boolean hasError = true;\n    try {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          DiskRangeList newIter = preReadUncompressedStream(\n              stripeOffset, iter, sctx.offset, sctx.offset + sctx.length, sctx.kind);\n          if (newIter != null) {\n            iter = newIter;\n          }\n        }\n      }\n      // Release buffers as we are done with all the streams... also see toRelease comment.\\\n      // With uncompressed streams, we know we are done earlier.\n      if (toRelease != null) {\n        releaseBuffers(toRelease.keySet(), true);\n        toRelease.clear();\n      }\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"Disk ranges after pre-read (file \" + fileKey + \", base offset \"\n            + stripeOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      iter = toRead.next; // Reset the iter to start.\n      hasError = false;\n    } finally {\n      // At this point, everything in the list is going to have a refcount of one. Unless it\n      // failed between the allocation and the incref for a single item, we should be ok. \n      if (hasError) {\n        try {\n          releaseInitialRefcounts(toRead.next);\n          if (toRelease != null) {\n            releaseBuffers(toRelease.keySet(), true);\n            toRelease.clear();\n          }\n        } catch (Throwable t) {\n          LOG.error(\"Error during the cleanup after another error; ignoring\", t);\n        }\n      }\n    }\n    return toRead.next; // Reset the iter to start.\n  }"
        ],
        [
            "EncodedReaderImpl::releaseInitialRefcounts(DiskRangeList)",
            " 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  ",
            "  private void releaseInitialRefcounts(DiskRangeList current) {\n    while (current != null) {\n      DiskRangeList toFree = current;\n      current = current.next;\n      if (!(toFree instanceof CacheChunk)) continue;\n      CacheChunk cc = (CacheChunk)toFree;\n      if (cc.getBuffer() == null) continue;\n      MemoryBuffer buffer = cc.getBuffer();\n      cacheWrapper.releaseBuffer(buffer);\n      cc.setBuffer(null);\n    }\n  }",
            " 632  \n 633  \n 634  \n 635  \n 636 +\n 637 +\n 638 +\n 639 +\n 640 +\n 641 +\n 642 +\n 643 +\n 644 +\n 645 +\n 646 +\n 647 +\n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  ",
            "  private void releaseInitialRefcounts(DiskRangeList current) {\n    while (current != null) {\n      DiskRangeList toFree = current;\n      current = current.next;\n      if (toFree instanceof ProcCacheChunk) {\n        ProcCacheChunk pcc = (ProcCacheChunk)toFree;\n        if (pcc.originalData != null) {\n          // This can only happen in case of failure - we read some data, but didn't decompress\n          // it. Deallocate the buffer directly, do not decref.\n          if (pcc.getBuffer() != null) {\n            cacheWrapper.getAllocator().deallocate(pcc.getBuffer());\n          }\n          continue;\n        }\n        \n      }\n      if (!(toFree instanceof CacheChunk)) continue;\n      CacheChunk cc = (CacheChunk)toFree;\n      if (cc.getBuffer() == null) continue;\n      MemoryBuffer buffer = cc.getBuffer();\n      cacheWrapper.releaseBuffer(buffer);\n      cc.setBuffer(null);\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedStream(long,DiskRangeList,long,long,ColumnStreamData,long,long,IdentityHashMap)",
            " 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844 -\n 845  \n 846  \n 847  \n 848 -\n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    boolean isCompressed = codec != null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n        cacheWrapper.getDataBufferFactory());\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        decompressChunk(chunk.originalData, codec, dest);\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      chunk.originalData = null;\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      cacheWrapper.reuseBuffer(chunk.getBuffer());\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869 +\n 870 +\n 871 +\n 872 +\n 873 +\n 874 +\n 875 +\n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    boolean isCompressed = codec != null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n        cacheWrapper.getDataBufferFactory());\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        decompressChunk(chunk.originalData, codec, dest);\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      // After we set originalData to null, we incref the buffer and the cleanup would decref it.\n      // Note that this assumes the failure during incref means incref didn't occur.\n      try {\n        cacheWrapper.reuseBuffer(chunk.getBuffer());\n      } finally {\n        chunk.originalData = null;\n      }\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }"
        ],
        [
            "EncodedReaderImpl::readIndexStreams(OrcIndex,StripeInformation,List,boolean,boolean)",
            "1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850 -\n1851 -\n1852 -\n1853 -\n1854 -\n1855 -\n1856 -\n1857 -\n1858 -\n1859 -\n1860 -\n1861 -\n1862 -\n1863 -\n1864 -\n1865 -\n1866 -\n1867 -\n1868 -\n1869 -\n1870 -\n1871 -\n1872 -\n1873 -\n1874 -\n1875 -\n1876 -\n1877 -\n1878 -\n1879 -\n1880 -\n1881 -\n1882 -\n1883 -\n1884 -\n1885  \n1886 -\n1887 -\n1888 -\n1889 -\n1890 -\n1891 -\n1892  \n1893  \n1894 -\n1895  \n1896 -\n1897 -\n1898 -\n1899  \n1900 -\n1901 -\n1902 -\n1903 -\n1904  \n1905  ",
            "  @Override\n  public void readIndexStreams(OrcIndex index, StripeInformation stripe,\n      List<OrcProto.Stream> streams, boolean[] included, boolean[] sargColumns)\n          throws IOException {\n    long stripeOffset = stripe.getOffset();\n    DiskRangeList indexRanges = planIndexReading(\n        fileSchema, streams, true, included, sargColumns, version, index.getBloomFilterKinds());\n    if (indexRanges == null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n    ReadContext[] colCtxs = new ReadContext[included.length];\n    int colRgIx = -1;\n    for (int i = 0; i < included.length; ++i) {\n      if (!included[i] && (sargColumns == null || !sargColumns[i])) continue;\n      colCtxs[i] = new ReadContext(i, ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    long offset = 0;\n    for (OrcProto.Stream stream : streams) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      // See planIndexReading - only read non-row-index streams if involved in SARGs.\n      if ((StreamName.getArea(streamKind) == StreamName.Area.INDEX)\n          && ((sargColumns != null && sargColumns[colIx])\n              || (included[colIx] && streamKind == Kind.ROW_INDEX))) {\n          colCtxs[colIx].addStream(offset, stream, -1);\n        if (isTracingEnabled) {\n          LOG.trace(\"Adding stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(indexRanges, stripeOffset, hasFileId, toRelease);\n\n    // 3. For uncompressed case, we need some special processing before read.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    // 4. Decompress the data.\n    for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n      ReadContext ctx = colCtxs[colIx];\n      if (ctx == null) continue; // This column is not included.\n      for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n        StreamContext sctx = ctx.streams[streamIx];\n        try {\n          if (isTracingEnabled) {\n            LOG.trace(\"Getting index stream \" + sctx.kind + \" for column \" + ctx.colIx\n                + \" at \" + sctx.offset + \", \" + sctx.length);\n          }\n          ColumnStreamData csd = POOLS.csdPool.take();\n          long endCOffset = sctx.offset + sctx.length;\n          DiskRangeList lastCached = readEncodedStream(stripeOffset, iter, sctx.offset,\n              endCOffset, csd, endCOffset, sctx.offset, toRelease);\n          if (lastCached != null) {\n            iter = lastCached;\n          }\n          CodedInputStream cis = CodedInputStream.newInstance(\n              new IndexStream(csd.getCacheBuffers(), sctx.length));\n          cis.setSizeLimit(InStream.PROTOBUF_MESSAGE_MAX_LIMIT);\n          switch (sctx.kind) {\n            case ROW_INDEX:\n              index.getRowGroupIndex()[colIx] = OrcProto.RowIndex.parseFrom(cis);\n              break;\n            case BLOOM_FILTER:\n            case BLOOM_FILTER_UTF8:\n              index.getBloomFilterIndex()[colIx] = OrcProto.BloomFilterIndex.parseFrom(cis);\n              break;\n            default:\n              throw new AssertionError(\"Unexpected index stream type \" + sctx.kind);\n          }\n          // We are done with the buffers; unlike data blocks, we are also the consumer. Release.\n          for (MemoryBuffer buf : csd.getCacheBuffers()) {\n            if (buf == null) continue;\n            cacheWrapper.releaseBuffer(buf);\n          }\n        } catch (Exception ex) {\n          DiskRangeList drl = toRead == null ? null : toRead.next;\n          LOG.error(\"Error getting stream \" + sctx.kind + \" for column \" + ctx.colIx\n              + \" at \" + sctx.offset + \", \" + sctx.length + \"; toRead \"\n              + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n          throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n        }\n      }\n    }\n\n    if (isTracingEnabled) {\n      LOG.trace(\"Disk ranges after preparing all the data \"\n          + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n    }\n\n    // Release the unreleased buffers. See class comment about refcounts.\n    releaseInitialRefcounts(toRead.next);\n    releaseBuffers(toRelease.keySet(), true);\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }",
            "1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877 +\n1878 +\n1879 +\n1880 +\n1881 +\n1882 +\n1883 +\n1884 +\n1885 +\n1886 +\n1887 +\n1888 +\n1889 +\n1890 +\n1891 +\n1892 +\n1893 +\n1894 +\n1895 +\n1896 +\n1897 +\n1898 +\n1899 +\n1900 +\n1901 +\n1902 +\n1903 +\n1904 +\n1905 +\n1906 +\n1907 +\n1908 +\n1909 +\n1910 +\n1911 +\n1912 +\n1913 +\n1914 +\n1915 +\n1916 +\n1917 +\n1918 +\n1919 +\n1920 +\n1921  \n1922  \n1923  \n1924  \n1925 +\n1926 +\n1927 +\n1928 +\n1929 +\n1930 +\n1931 +\n1932 +\n1933 +\n1934 +\n1935 +\n1936 +\n1937 +\n1938 +\n1939  \n1940  \n1941  ",
            "  @Override\n  public void readIndexStreams(OrcIndex index, StripeInformation stripe,\n      List<OrcProto.Stream> streams, boolean[] included, boolean[] sargColumns)\n          throws IOException {\n    long stripeOffset = stripe.getOffset();\n    DiskRangeList indexRanges = planIndexReading(\n        fileSchema, streams, true, included, sargColumns, version, index.getBloomFilterKinds());\n    if (indexRanges == null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n    ReadContext[] colCtxs = new ReadContext[included.length];\n    int colRgIx = -1;\n    for (int i = 0; i < included.length; ++i) {\n      if (!included[i] && (sargColumns == null || !sargColumns[i])) continue;\n      colCtxs[i] = new ReadContext(i, ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n    }\n    long offset = 0;\n    for (OrcProto.Stream stream : streams) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      // See planIndexReading - only read non-row-index streams if involved in SARGs.\n      if ((StreamName.getArea(streamKind) == StreamName.Area.INDEX)\n          && ((sargColumns != null && sargColumns[colIx])\n              || (included[colIx] && streamKind == Kind.ROW_INDEX))) {\n          colCtxs[colIx].addStream(offset, stream, -1);\n        if (isTracingEnabled) {\n          LOG.trace(\"Adding stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(indexRanges, stripeOffset, hasFileId, toRelease);\n\n    // 3. For uncompressed case, we need some special processing before read.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    // 4. Decompress the data.\n    boolean hasError = true;\n    try {\n      for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n        ReadContext ctx = colCtxs[colIx];\n        if (ctx == null) continue; // This column is not included.\n        for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n          StreamContext sctx = ctx.streams[streamIx];\n          try {\n            if (isTracingEnabled) {\n              LOG.trace(\"Getting index stream \" + sctx.kind + \" for column \" + ctx.colIx\n                  + \" at \" + sctx.offset + \", \" + sctx.length);\n            }\n            ColumnStreamData csd = POOLS.csdPool.take();\n            long endCOffset = sctx.offset + sctx.length;\n            DiskRangeList lastCached = readEncodedStream(stripeOffset, iter, sctx.offset,\n                endCOffset, csd, endCOffset, sctx.offset, toRelease);\n            if (lastCached != null) {\n              iter = lastCached;\n            }\n            CodedInputStream cis = CodedInputStream.newInstance(\n                new IndexStream(csd.getCacheBuffers(), sctx.length));\n            cis.setSizeLimit(InStream.PROTOBUF_MESSAGE_MAX_LIMIT);\n            switch (sctx.kind) {\n              case ROW_INDEX:\n                index.getRowGroupIndex()[colIx] = OrcProto.RowIndex.parseFrom(cis);\n                break;\n              case BLOOM_FILTER:\n              case BLOOM_FILTER_UTF8:\n                index.getBloomFilterIndex()[colIx] = OrcProto.BloomFilterIndex.parseFrom(cis);\n                break;\n              default:\n                throw new AssertionError(\"Unexpected index stream type \" + sctx.kind);\n            }\n            // We are done with the buffers; unlike data blocks, we are also the consumer. Release.\n            for (MemoryBuffer buf : csd.getCacheBuffers()) {\n              if (buf == null) continue;\n              cacheWrapper.releaseBuffer(buf);\n            }\n          } catch (Exception ex) {\n            DiskRangeList drl = toRead == null ? null : toRead.next;\n            LOG.error(\"Error getting stream \" + sctx.kind + \" for column \" + ctx.colIx\n                + \" at \" + sctx.offset + \", \" + sctx.length + \"; toRead \"\n                + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n            throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n          }\n        }\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after preparing all the data \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      hasError = false;\n    } finally {\n      // Release the unreleased buffers. See class comment about refcounts.\n      try {\n        releaseInitialRefcounts(toRead.next);\n        releaseBuffers(toRelease.keySet(), true);\n      } catch (Throwable t) {\n        if (!hasError) throw new IOException(t);\n        LOG.error(\"Error during the cleanup after another error; ignoring\", t);\n      }\n    }\n    releaseCacheChunksIntoObjectPool(toRead.next);\n  }"
        ]
    ],
    "b9af60c54e2523d9b5b9cfd9d62b83ee6bb2f3e7": [
        [
            "RexNodeConverter::convert(ExprNodeConstantDesc)",
            " 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622 -\n 623 -\n 624  \n 625 -\n 626 -\n 627 -\n 628 -\n 629 -\n 630 -\n 631 -\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  ",
            "  protected RexNode convert(ExprNodeConstantDesc literal) throws CalciteSemanticException {\n    RexBuilder rexBuilder = cluster.getRexBuilder();\n    RelDataTypeFactory dtFactory = rexBuilder.getTypeFactory();\n    PrimitiveTypeInfo hiveType = (PrimitiveTypeInfo) literal.getTypeInfo();\n    RelDataType calciteDataType = TypeConverter.convert(hiveType, dtFactory);\n\n    PrimitiveCategory hiveTypeCategory = hiveType.getPrimitiveCategory();\n\n    ConstantObjectInspector coi = literal.getWritableObjectInspector();\n    Object value = ObjectInspectorUtils.copyToStandardJavaObject(coi.getWritableConstantValue(),\n        coi);\n\n    RexNode calciteLiteral = null;\n    // If value is null, the type should also be VOID.\n    if (value == null) {\n      hiveTypeCategory = PrimitiveCategory.VOID;\n    }\n    // TODO: Verify if we need to use ConstantObjectInspector to unwrap data\n    switch (hiveTypeCategory) {\n    case BOOLEAN:\n      calciteLiteral = rexBuilder.makeLiteral(((Boolean) value).booleanValue());\n      break;\n    case BYTE:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Byte) value), calciteDataType);\n      break;\n    case SHORT:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Short) value), calciteDataType);\n      break;\n    case INT:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Integer) value));\n      break;\n    case LONG:\n      calciteLiteral = rexBuilder.makeBigintLiteral(new BigDecimal((Long) value));\n      break;\n    // TODO: is Decimal an exact numeric or approximate numeric?\n    case DECIMAL:\n      if (value instanceof HiveDecimal) {\n        value = ((HiveDecimal) value).bigDecimalValue();\n      } else if (value instanceof Decimal128) {\n        value = ((Decimal128) value).toBigDecimal();\n      }\n      if (value == null) {\n        // We have found an invalid decimal value while enforcing precision and\n        // scale. Ideally,\n        // we would replace it with null here, which is what Hive does. However,\n        // we need to plumb\n        // this thru up somehow, because otherwise having different expression\n        // type in AST causes\n        // the plan generation to fail after CBO, probably due to some residual\n        // state in SA/QB.\n        // For now, we will not run CBO in the presence of invalid decimal\n        // literals.\n        throw new CalciteSemanticException(\"Expression \" + literal.getExprString()\n            + \" is not a valid decimal\", UnsupportedFeature.Invalid_decimal);\n        // TODO: return createNullLiteral(literal);\n      }\n      BigDecimal bd = (BigDecimal) value;\n      BigInteger unscaled = bd.unscaledValue();\n      if (unscaled.compareTo(MIN_LONG_BI) >= 0 && unscaled.compareTo(MAX_LONG_BI) <= 0) {\n        calciteLiteral = rexBuilder.makeExactLiteral(bd);\n      } else {\n        // CBO doesn't support unlimited precision decimals. In practice, this\n        // will work...\n        // An alternative would be to throw CboSemanticException and fall back\n        // to no CBO.\n        RelDataType relType = cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL,\n            unscaled.toString().length(), bd.scale());\n        calciteLiteral = rexBuilder.makeExactLiteral(bd, relType);\n      }\n      break;\n    case FLOAT:\n      calciteLiteral = rexBuilder.makeApproxLiteral(\n              new BigDecimal(Float.toString((Float)value)), calciteDataType);\n      break;\n    case DOUBLE:\n      // TODO: The best solution is to support NaN in expression reduction.\n      if (Double.isNaN((Double) value)) {\n        throw new CalciteSemanticException(\"NaN\", UnsupportedFeature.Invalid_decimal);\n      }\n      calciteLiteral = rexBuilder.makeApproxLiteral(\n              new BigDecimal(Double.toString((Double)value)), calciteDataType);\n      break;\n    case CHAR:\n      if (value instanceof HiveChar) {\n        value = ((HiveChar) value).getValue();\n      }\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case VARCHAR:\n      if (value instanceof HiveVarchar) {\n        value = ((HiveVarchar) value).getValue();\n      }\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case STRING:\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case DATE:\n      final Calendar cal = Calendar.getInstance(Locale.getDefault());\n      cal.setTime((Date) value);\n      calciteLiteral = rexBuilder.makeDateLiteral(DateString.fromCalendarFields(cal));\n      break;\n    case TIMESTAMP:\n      final TimestampString tsString;\n      if (value instanceof Calendar) {\n        tsString = TimestampString.fromCalendarFields((Calendar) value);\n      } else {\n        final Timestamp ts = (Timestamp) value;\n        final Calendar calt = Calendar.getInstance(Locale.getDefault());\n        calt.setTimeInMillis(ts.getTime());\n        tsString = TimestampString.fromCalendarFields(calt).withNanos(ts.getNanos());\n      }\n      // Must call makeLiteral, not makeTimestampLiteral\n      // to have the RexBuilder.roundTime logic kick in\n      calciteLiteral = rexBuilder.makeLiteral(\n        tsString,\n        rexBuilder.getTypeFactory().createSqlType(\n          SqlTypeName.TIMESTAMP,\n          rexBuilder.getTypeFactory().getTypeSystem().getDefaultPrecision(SqlTypeName.TIMESTAMP)),\n        false);\n      break;\n    case INTERVAL_YEAR_MONTH:\n      // Calcite year-month literal value is months as BigDecimal\n      BigDecimal totalMonths = BigDecimal.valueOf(((HiveIntervalYearMonth) value).getTotalMonths());\n      calciteLiteral = rexBuilder.makeIntervalLiteral(totalMonths,\n          new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, new SqlParserPos(1,1)));\n      break;\n    case INTERVAL_DAY_TIME:\n      // Calcite day-time interval is millis value as BigDecimal\n      // Seconds converted to millis\n      BigDecimal secsValueBd = BigDecimal\n       .valueOf(((HiveIntervalDayTime) value).getTotalSeconds() * 1000);\n      // Nanos converted to millis\n       BigDecimal nanosValueBd = BigDecimal.valueOf(((HiveIntervalDayTime)\n       value).getNanos(), 6);\n       calciteLiteral =\n       rexBuilder.makeIntervalLiteral(secsValueBd.add(nanosValueBd),\n       new SqlIntervalQualifier(TimeUnit.MILLISECOND, null, new\n       SqlParserPos(1, 1)));\n       break;\n    case VOID:\n      calciteLiteral = cluster.getRexBuilder().makeLiteral(null,\n          cluster.getTypeFactory().createSqlType(SqlTypeName.NULL), true);\n      break;\n    case BINARY:\n    case UNKNOWN:\n    default:\n      throw new RuntimeException(\"UnSupported Literal\");\n    }\n\n    return calciteLiteral;\n  }",
            " 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622 +\n 623 +\n 624 +\n 625 +\n 626 +\n 627 +\n 628 +\n 629 +\n 630 +\n 631 +\n 632  \n 633 +\n 634 +\n 635 +\n 636  \n 637 +\n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  ",
            "  protected RexNode convert(ExprNodeConstantDesc literal) throws CalciteSemanticException {\n    RexBuilder rexBuilder = cluster.getRexBuilder();\n    RelDataTypeFactory dtFactory = rexBuilder.getTypeFactory();\n    PrimitiveTypeInfo hiveType = (PrimitiveTypeInfo) literal.getTypeInfo();\n    RelDataType calciteDataType = TypeConverter.convert(hiveType, dtFactory);\n\n    PrimitiveCategory hiveTypeCategory = hiveType.getPrimitiveCategory();\n\n    ConstantObjectInspector coi = literal.getWritableObjectInspector();\n    Object value = ObjectInspectorUtils.copyToStandardJavaObject(coi.getWritableConstantValue(),\n        coi);\n\n    RexNode calciteLiteral = null;\n    // If value is null, the type should also be VOID.\n    if (value == null) {\n      hiveTypeCategory = PrimitiveCategory.VOID;\n    }\n    // TODO: Verify if we need to use ConstantObjectInspector to unwrap data\n    switch (hiveTypeCategory) {\n    case BOOLEAN:\n      calciteLiteral = rexBuilder.makeLiteral(((Boolean) value).booleanValue());\n      break;\n    case BYTE:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Byte) value), calciteDataType);\n      break;\n    case SHORT:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Short) value), calciteDataType);\n      break;\n    case INT:\n      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Integer) value));\n      break;\n    case LONG:\n      calciteLiteral = rexBuilder.makeBigintLiteral(new BigDecimal((Long) value));\n      break;\n    // TODO: is Decimal an exact numeric or approximate numeric?\n    case DECIMAL:\n      if (value instanceof HiveDecimal) {\n        value = ((HiveDecimal) value).bigDecimalValue();\n      } else if (value instanceof Decimal128) {\n        value = ((Decimal128) value).toBigDecimal();\n      }\n      if (value == null) {\n        // We have found an invalid decimal value while enforcing precision and\n        // scale. Ideally,\n        // we would replace it with null here, which is what Hive does. However,\n        // we need to plumb\n        // this thru up somehow, because otherwise having different expression\n        // type in AST causes\n        // the plan generation to fail after CBO, probably due to some residual\n        // state in SA/QB.\n        // For now, we will not run CBO in the presence of invalid decimal\n        // literals.\n        throw new CalciteSemanticException(\"Expression \" + literal.getExprString()\n            + \" is not a valid decimal\", UnsupportedFeature.Invalid_decimal);\n        // TODO: return createNullLiteral(literal);\n      }\n      BigDecimal bd = (BigDecimal) value;\n      BigInteger unscaled = bd.unscaledValue();\n\n\n      int precision = bd.unscaledValue().abs().toString().length();\n      int scale = bd.scale();\n      RelDataType relType;\n\n      if (precision > scale) {\n        // bd is greater than or equal to 1\n        relType =\n            cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL, precision, scale);\n      } else {\n        // bd is less than 1\n        relType =\n            cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL, scale + 1, scale);\n      }\n      calciteLiteral = rexBuilder.makeExactLiteral(bd, relType);\n      break;\n    case FLOAT:\n      calciteLiteral = rexBuilder.makeApproxLiteral(\n              new BigDecimal(Float.toString((Float)value)), calciteDataType);\n      break;\n    case DOUBLE:\n      // TODO: The best solution is to support NaN in expression reduction.\n      if (Double.isNaN((Double) value)) {\n        throw new CalciteSemanticException(\"NaN\", UnsupportedFeature.Invalid_decimal);\n      }\n      calciteLiteral = rexBuilder.makeApproxLiteral(\n              new BigDecimal(Double.toString((Double)value)), calciteDataType);\n      break;\n    case CHAR:\n      if (value instanceof HiveChar) {\n        value = ((HiveChar) value).getValue();\n      }\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case VARCHAR:\n      if (value instanceof HiveVarchar) {\n        value = ((HiveVarchar) value).getValue();\n      }\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case STRING:\n      calciteLiteral = rexBuilder.makeCharLiteral(asUnicodeString((String) value));\n      break;\n    case DATE:\n      final Calendar cal = Calendar.getInstance(Locale.getDefault());\n      cal.setTime((Date) value);\n      calciteLiteral = rexBuilder.makeDateLiteral(DateString.fromCalendarFields(cal));\n      break;\n    case TIMESTAMP:\n      final TimestampString tsString;\n      if (value instanceof Calendar) {\n        tsString = TimestampString.fromCalendarFields((Calendar) value);\n      } else {\n        final Timestamp ts = (Timestamp) value;\n        final Calendar calt = Calendar.getInstance(Locale.getDefault());\n        calt.setTimeInMillis(ts.getTime());\n        tsString = TimestampString.fromCalendarFields(calt).withNanos(ts.getNanos());\n      }\n      // Must call makeLiteral, not makeTimestampLiteral\n      // to have the RexBuilder.roundTime logic kick in\n      calciteLiteral = rexBuilder.makeLiteral(\n        tsString,\n        rexBuilder.getTypeFactory().createSqlType(\n          SqlTypeName.TIMESTAMP,\n          rexBuilder.getTypeFactory().getTypeSystem().getDefaultPrecision(SqlTypeName.TIMESTAMP)),\n        false);\n      break;\n    case INTERVAL_YEAR_MONTH:\n      // Calcite year-month literal value is months as BigDecimal\n      BigDecimal totalMonths = BigDecimal.valueOf(((HiveIntervalYearMonth) value).getTotalMonths());\n      calciteLiteral = rexBuilder.makeIntervalLiteral(totalMonths,\n          new SqlIntervalQualifier(TimeUnit.YEAR, TimeUnit.MONTH, new SqlParserPos(1,1)));\n      break;\n    case INTERVAL_DAY_TIME:\n      // Calcite day-time interval is millis value as BigDecimal\n      // Seconds converted to millis\n      BigDecimal secsValueBd = BigDecimal\n       .valueOf(((HiveIntervalDayTime) value).getTotalSeconds() * 1000);\n      // Nanos converted to millis\n       BigDecimal nanosValueBd = BigDecimal.valueOf(((HiveIntervalDayTime)\n       value).getNanos(), 6);\n       calciteLiteral =\n       rexBuilder.makeIntervalLiteral(secsValueBd.add(nanosValueBd),\n       new SqlIntervalQualifier(TimeUnit.MILLISECOND, null, new\n       SqlParserPos(1, 1)));\n       break;\n    case VOID:\n      calciteLiteral = cluster.getRexBuilder().makeLiteral(null,\n          cluster.getTypeFactory().createSqlType(SqlTypeName.NULL), true);\n      break;\n    case BINARY:\n    case UNKNOWN:\n    default:\n      throw new RuntimeException(\"UnSupported Literal\");\n    }\n\n    return calciteLiteral;\n  }"
        ]
    ],
    "f973243a59c8fa94f932bdede1a13bd96275089a": [
        [
            "OpTraitsRulesProcFactory::ReduceSinkRule::process(Node,Stack,NodeProcessorCtx,Object)",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96 -\n  97 -\n  98 -\n  99 -\n 100 -\n 101 -\n 102 -\n 103 -\n 104 -\n 105  \n 106  \n 107 -\n 108  \n 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      ReduceSinkOperator rs = (ReduceSinkOperator)nd;\n      List<String> bucketCols = new ArrayList<String>();\n      if (rs.getColumnExprMap() != null) {\n        for (ExprNodeDesc exprDesc : rs.getConf().getKeyCols()) {\n          for (Entry<String, ExprNodeDesc> entry : rs.getColumnExprMap().entrySet()) {\n            if (exprDesc.isSame(entry.getValue())) {\n              bucketCols.add(entry.getKey());\n            }\n          }\n        }\n      }\n\n      List<List<String>> listBucketCols = new ArrayList<List<String>>();\n      listBucketCols.add(bucketCols);\n      int numBuckets = -1;\n      int numReduceSinks = 1;\n      OpTraits parentOpTraits = rs.getParentOperators().get(0).getConf().getTraits();\n      if (parentOpTraits != null) {\n        numBuckets = parentOpTraits.getNumBuckets();\n        numReduceSinks += parentOpTraits.getNumReduceSinks();\n      }\n      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n      rs.setOpTraits(opTraits);\n      return null;\n    }",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124 +\n 125 +\n 126 +\n 127 +\n 128 +\n 129 +\n 130 +\n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136  \n 137  \n 138  \n 139  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      ReduceSinkOperator rs = (ReduceSinkOperator)nd;\n\n      List<List<String>> listBucketCols = new ArrayList<List<String>>();\n      int numBuckets = -1;\n      int numReduceSinks = 1;\n      OpTraits parentOpTraits = rs.getParentOperators().get(0).getOpTraits();\n      if (parentOpTraits != null) {\n        numBuckets = parentOpTraits.getNumBuckets();\n        numReduceSinks += parentOpTraits.getNumReduceSinks();\n      }\n\n      List<String> bucketCols = new ArrayList<>();\n      if (parentOpTraits != null &&\n              parentOpTraits.getBucketColNames() != null) {\n        if (parentOpTraits.getBucketColNames().size() > 0) {\n          for (List<String> cols : parentOpTraits.getBucketColNames()) {\n            for (String col : cols) {\n              for (Entry<String, ExprNodeDesc> entry : rs.getColumnExprMap().entrySet()) {\n                // Fetch the column expression. There should be atleast one.\n                Map<Integer, ExprNodeDesc> colMap = new HashMap<>();\n                boolean found = false;\n                ExprNodeDescUtils.getExprNodeColumnDesc(entry.getValue(), colMap);\n                for (Integer hashCode : colMap.keySet()) {\n                  ExprNodeColumnDesc expr = (ExprNodeColumnDesc) colMap.get(hashCode);\n                  if (expr.getColumn().equals(col)) {\n                    bucketCols.add(entry.getKey());\n                    found = true;\n                    break;\n                  }\n                }\n                if (found) break;\n              } // column exprmap.\n            } // cols\n          }\n        } else {\n          // fallback to old mechanism which serves SMB Joins.\n          for (ExprNodeDesc exprDesc : rs.getConf().getKeyCols()) {\n            for (Entry<String, ExprNodeDesc> entry : rs.getColumnExprMap().entrySet()) {\n              if (exprDesc.isSame(entry.getValue())) {\n                bucketCols.add(entry.getKey());\n              }\n            }\n          }\n        }\n      }\n\n      listBucketCols.add(bucketCols);\n      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n      rs.setOpTraits(opTraits);\n      return null;\n    }"
        ],
        [
            "OpTraitsRulesProcFactory::JoinRule::getOutputColNames(JoinOperator,List,byte)",
            " 321  \n 322  \n 323 -\n 324 -\n 325 -\n 326 -\n 327 -\n 328 -\n 329 -\n 330 -\n 331 -\n 332 -\n 333 -\n 334 -\n 335 -\n 336 -\n 337 -\n 338 -\n 339 -\n 340  \n 341 -\n 342 -\n 343 -\n 344  \n 345 -\n 346 -\n 347  \n 348  \n 349  \n 350 -\n 351 -\n 352  \n 353  \n 354 -\n 355 -\n 356  ",
            "    private List<String> getOutputColNames(JoinOperator joinOp, List<List<String>> parentColNames,\n        byte pos) {\n      if (parentColNames != null) {\n        List<String> bucketColNames = new ArrayList<String>();\n\n        // guaranteed that there is only 1 list within this list because\n        // a reduce sink always brings down the bucketing cols to a single list.\n        // may not be true with correlation operators (mux-demux)\n        List<String> colNames = parentColNames.get(0);\n        for (String colName : colNames) {\n          for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n            if (exprNode instanceof ExprNodeColumnDesc) {\n              if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n                for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                  if (entry.getValue().isSame(exprNode)) {\n                    bucketColNames.add(entry.getKey());\n                    // we have found the colName\n                    break;\n                  }\n                }\n              } else {\n                // continue on to the next exprNode to find a match\n                continue;\n              }\n              // we have found the colName. No need to search more exprNodes.\n              break;\n            }\n          }\n        }\n\n        return bucketColNames;\n      }\n\n      // no col names in parent\n      return null;\n    }",
            " 342  \n 343  \n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349 +\n 350 +\n 351 +\n 352 +\n 353 +\n 354 +\n 355 +\n 356 +\n 357 +\n 358 +\n 359 +\n 360 +\n 361 +\n 362 +\n 363  \n 364  \n 365 +\n 366 +\n 367 +\n 368  \n 369 +\n 370 +\n 371  \n 372  \n 373  \n 374  \n 375 +\n 376  ",
            "    private List<String> getOutputColNames(JoinOperator joinOp, List<List<String>> parentColNames,\n        byte pos) {\n      if (parentColNames == null) {\n        // no col names in parent\n        return null;\n      }\n      List<String> bucketColNames = new ArrayList<>();\n\n      // guaranteed that there is only 1 list within this list because\n      // a reduce sink always brings down the bucketing cols to a single list.\n      // may not be true with correlation operators (mux-demux)\n      List<String> colNames = parentColNames.size() > 0 ? parentColNames.get(0) : new ArrayList<>();\n      for (String colName : colNames) {\n        for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n          if (exprNode instanceof ExprNodeColumnDesc) {\n            if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n              for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                if (entry.getValue().isSame(exprNode)) {\n                  bucketColNames.add(entry.getKey());\n                  // we have found the colName\n                  break;\n                }\n              }\n            } else {\n              // continue on to the next exprNode to find a match\n              continue;\n            }\n            // we have found the colName. No need to search more exprNodes.\n            break;\n          }\n        }\n      }\n\n      return bucketColNames;\n    }"
        ],
        [
            "ConvertJoinMapJoin::convertJoinBucketMapJoin(JoinOperator,OptimizeTezProcContext,int,TezBucketJoinProcCtx)",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 -\n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  ",
            "  private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n\n    if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {\n      LOG.info(\"Check conversion to bucket map join failed.\");\n      return false;\n    }\n\n    MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePosition, true);\n    if (mapJoinOp == null) {\n      LOG.debug(\"Conversion to bucket map join failed.\");\n      return false;\n    }\n    MapJoinDesc joinDesc = mapJoinOp.getConf();\n    joinDesc.setBucketMapJoin(true);\n\n    // we can set the traits for this join operator\n    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n    mapJoinOp.setOpTraits(opTraits);\n    mapJoinOp.setStatistics(joinOp.getStatistics());\n    setNumberOfBucketsOnChildren(mapJoinOp);\n\n    // Once the conversion is done, we can set the partitioner to bucket cols on the small table\n    Map<String, Integer> bigTableBucketNumMapping = new HashMap<String, Integer>();\n    bigTableBucketNumMapping.put(joinDesc.getBigTableAlias(), tezBucketJoinProcCtx.getNumBuckets());\n    joinDesc.setBigTableBucketNumMapping(bigTableBucketNumMapping);\n\n    return true;\n  }",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389 +\n 390 +\n 391 +\n 392 +\n 393 +\n 394 +\n 395 +\n 396 +\n 397 +\n 398 +\n 399 +\n 400 +\n 401 +\n 402 +\n 403 +\n 404 +\n 405 +\n 406 +\n 407 +\n 408 +\n 409 +\n 410 +\n 411 +\n 412 +\n 413 +\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 +\n 435 +\n 436 +\n 437 +\n 438 +\n 439 +\n 440 +\n 441 +\n 442 +\n 443 +\n 444 +\n 445 +\n 446 +\n 447 +\n 448 +\n 449 +\n 450  \n 451  ",
            "  private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n\n    if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {\n      LOG.info(\"Check conversion to bucket map join failed.\");\n      return false;\n    }\n\n    // Incase the join has extra keys other than bucketed columns, partition keys need to be updated\n    // on small table(s).\n    ReduceSinkOperator bigTableRS = (ReduceSinkOperator)joinOp.getParentOperators().get(bigTablePosition);\n    OpTraits opTraits = bigTableRS.getOpTraits();\n    List<List<String>> listBucketCols = opTraits.getBucketColNames();\n    ArrayList<ExprNodeDesc> bigTablePartitionCols = bigTableRS.getConf().getPartitionCols();\n    boolean updatePartitionCols = false;\n    List<Integer> positions = new ArrayList<>();\n\n    if (listBucketCols.get(0).size() != bigTablePartitionCols.size()) {\n      updatePartitionCols = true;\n      // Prepare updated partition columns for small table(s).\n      // Get the positions of bucketed columns\n\n      int i = 0;\n      Map<String, ExprNodeDesc> colExprMap = bigTableRS.getColumnExprMap();\n      for (ExprNodeDesc bigTableExpr : bigTablePartitionCols) {\n        // It is guaranteed there is only 1 list within listBucketCols.\n        for (String colName : listBucketCols.get(0)) {\n          if (colExprMap.get(colName).isSame(bigTableExpr)) {\n            positions.add(i++);\n          }\n        }\n      }\n    }\n\n    MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePosition, true);\n    if (mapJoinOp == null) {\n      LOG.debug(\"Conversion to bucket map join failed.\");\n      return false;\n    }\n    MapJoinDesc joinDesc = mapJoinOp.getConf();\n    joinDesc.setBucketMapJoin(true);\n\n    // we can set the traits for this join operator\n    opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n    mapJoinOp.setOpTraits(opTraits);\n    mapJoinOp.setStatistics(joinOp.getStatistics());\n    setNumberOfBucketsOnChildren(mapJoinOp);\n\n    // Once the conversion is done, we can set the partitioner to bucket cols on the small table\n    Map<String, Integer> bigTableBucketNumMapping = new HashMap<String, Integer>();\n    bigTableBucketNumMapping.put(joinDesc.getBigTableAlias(), tezBucketJoinProcCtx.getNumBuckets());\n    joinDesc.setBigTableBucketNumMapping(bigTableBucketNumMapping);\n\n    // Update the partition columns in small table to ensure correct routing of hash tables.\n    if (updatePartitionCols) {\n      // use the positions to only pick the partitionCols which are required\n      // on the small table side.\n      for (Operator<?> op : mapJoinOp.getParentOperators()) {\n        if (!(op instanceof ReduceSinkOperator)) continue;;\n\n        ReduceSinkOperator rsOp = (ReduceSinkOperator) op;\n        ArrayList<ExprNodeDesc> newPartitionCols = new ArrayList<>();\n        ArrayList<ExprNodeDesc> partitionCols = rsOp.getConf().getPartitionCols();\n        for (Integer position : positions) {\n          newPartitionCols.add(partitionCols.get(position));\n        }\n        rsOp.getConf().setPartitionCols(newPartitionCols);\n      }\n    }\n    return true;\n  }"
        ]
    ],
    "abcadab7c219c1458bdcab7a9379a928e3ea9305": [
        [
            "BaseSemanticAnalyzer::generateConstraintInfos(ASTNode,List,List)",
            " 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 -\n 797 -\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  ",
            "  /**\n   * Get the constraint from the AST and populate the cstrInfos with the required\n   * information.\n   * @param child  The node with the constraint token\n   * @param columnNames The name of the columns for the primary key\n   * @param cstrInfos Constraint information\n   * @throws SemanticException\n   */\n  private static void generateConstraintInfos(ASTNode child, List<String> columnNames,\n      List<ConstraintInfo> cstrInfos) throws SemanticException {\n    // The ANTLR grammar looks like :\n    // 1. KW_CONSTRAINT idfr=identifier KW_PRIMARY KW_KEY pkCols=columnParenthesesList\n    //  constraintOptsCreate?\n    // -> ^(TOK_PRIMARY_KEY $pkCols $idfr constraintOptsCreate?)\n    // when the user specifies the constraint name.\n    // 2.  KW_PRIMARY KW_KEY columnParenthesesList\n    // constraintOptsCreate?\n    // -> ^(TOK_PRIMARY_KEY columnParenthesesList constraintOptsCreate?)\n    // when the user does not specify the constraint name.\n    // Default values\n    String constraintName = null;\n    boolean enable = true;\n    boolean validate = true;\n    boolean rely = false;\n    for (int i = 0; i < child.getChildCount(); i++) {\n      ASTNode grandChild = (ASTNode) child.getChild(i);\n      int type = grandChild.getToken().getType();\n      if (type == HiveParser.TOK_CONSTRAINT_NAME) {\n        constraintName = unescapeIdentifier(grandChild.getChild(0).getText().toLowerCase());\n      } else if (type == HiveParser.TOK_ENABLE) {\n        enable = true;\n        // validate is true by default if we enable the constraint\n        validate = true;\n      } else if (type == HiveParser.TOK_DISABLE) {\n        enable = false;\n        // validate is false by default if we disable the constraint\n        validate = false;\n      } else if (type == HiveParser.TOK_VALIDATE) {\n        validate = true;\n      } else if (type == HiveParser.TOK_NOVALIDATE) {\n        validate = false;\n      } else if (type == HiveParser.TOK_RELY) {\n        rely = true;\n      }\n    }\n    if (enable) {\n      throw new SemanticException(\n          ErrorMsg.INVALID_CSTR_SYNTAX.getMsg(\"ENABLE feature not supported yet. \"\n              + \"Please use DISABLE instead.\"));\n    }\n    if (validate) {\n      throw new SemanticException(\n        ErrorMsg.INVALID_CSTR_SYNTAX.getMsg(\"VALIDATE feature not supported yet. \"\n              + \"Please use NOVALIDATE instead.\"));\n    }\n\n    for (String columnName : columnNames) {\n      cstrInfos.add(new ConstraintInfo(columnName, constraintName,\n          enable, validate, rely));\n    }\n  }",
            " 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 +\n 797 +\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  ",
            "  /**\n   * Get the constraint from the AST and populate the cstrInfos with the required\n   * information.\n   * @param child  The node with the constraint token\n   * @param columnNames The name of the columns for the primary key\n   * @param cstrInfos Constraint information\n   * @throws SemanticException\n   */\n  private static void generateConstraintInfos(ASTNode child, List<String> columnNames,\n      List<ConstraintInfo> cstrInfos) throws SemanticException {\n    // The ANTLR grammar looks like :\n    // 1. KW_CONSTRAINT idfr=identifier KW_PRIMARY KW_KEY pkCols=columnParenthesesList\n    //  constraintOptsCreate?\n    // -> ^(TOK_PRIMARY_KEY $pkCols $idfr constraintOptsCreate?)\n    // when the user specifies the constraint name.\n    // 2.  KW_PRIMARY KW_KEY columnParenthesesList\n    // constraintOptsCreate?\n    // -> ^(TOK_PRIMARY_KEY columnParenthesesList constraintOptsCreate?)\n    // when the user does not specify the constraint name.\n    // Default values\n    String constraintName = null;\n    boolean enable = true;\n    boolean validate = true;\n    boolean rely = false;\n    for (int i = 0; i < child.getChildCount(); i++) {\n      ASTNode grandChild = (ASTNode) child.getChild(i);\n      int type = grandChild.getToken().getType();\n      if (type == HiveParser.TOK_CONSTRAINT_NAME) {\n        constraintName = unescapeIdentifier(grandChild.getChild(0).getText().toLowerCase());\n      } else if (type == HiveParser.TOK_ENABLE) {\n        enable = true;\n        // validate is true by default if we enable the constraint\n        validate = true;\n      } else if (type == HiveParser.TOK_DISABLE) {\n        enable = false;\n        // validate is false by default if we disable the constraint\n        validate = false;\n      } else if (type == HiveParser.TOK_VALIDATE) {\n        validate = true;\n      } else if (type == HiveParser.TOK_NOVALIDATE) {\n        validate = false;\n      } else if (type == HiveParser.TOK_RELY) {\n        rely = true;\n      }\n    }\n    if (enable) {\n      throw new SemanticException(\n          ErrorMsg.INVALID_CSTR_SYNTAX.getMsg(\"ENABLE/ENFORCED feature not supported yet. \"\n              + \"Please use DISABLE/NOT ENFORCED instead.\"));\n    }\n    if (validate) {\n      throw new SemanticException(\n        ErrorMsg.INVALID_CSTR_SYNTAX.getMsg(\"VALIDATE feature not supported yet. \"\n              + \"Please use NOVALIDATE instead.\"));\n    }\n\n    for (String columnName : columnNames) {\n      cstrInfos.add(new ConstraintInfo(columnName, constraintName,\n          enable, validate, rely));\n    }\n  }"
        ]
    ],
    "52e1ba15d11e06f5236c7fb814500e6be38f3ac2": [
        [
            "FunctionTask::dropPermanentFunction(Hive,DropFunctionDesc)",
            " 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260  \n 261  \n 262  \n 263  ",
            "  private int dropPermanentFunction(Hive db, DropFunctionDesc dropFunctionDesc) {\n    try {\n      String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n          dropFunctionDesc.getFunctionName());\n      String dbName = qualifiedNameParts[0];\n      String funcName = qualifiedNameParts[1];\n\n      String registeredName = FunctionUtils.qualifyFunctionName(funcName, dbName);\n      FunctionRegistry.unregisterPermanentFunction(registeredName);\n      db.dropFunction(dbName, funcName);\n\n      return 0;\n    } catch (Exception e) {\n      LOG.info(\"drop function: \" + StringUtils.stringifyException(e));\n      console.printError(\"FAILED: error during drop function: \" + StringUtils.stringifyException(e));\n      return 1;\n    }\n  }",
            " 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  ",
            "  private int dropPermanentFunction(Hive db, DropFunctionDesc dropFunctionDesc) {\n    try {\n      String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n          dropFunctionDesc.getFunctionName());\n      String dbName = qualifiedNameParts[0];\n      String funcName = qualifiedNameParts[1];\n\n      String registeredName = FunctionUtils.qualifyFunctionName(funcName, dbName);\n      FunctionRegistry.unregisterPermanentFunction(registeredName);\n      db.dropFunction(dbName, funcName);\n\n      return 0;\n    } catch (Exception e) {\n      LOG.info(\"drop function: \", e);\n      console.printError(\"FAILED: error during drop function: \" + StringUtils.stringifyException(e));\n      return 1;\n    }\n  }"
        ],
        [
            "DDLTask::showTxns(Hive,ShowTxnsDesc)",
            "3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040 -\n3041  \n3042  \n3043  \n3044  \n3045  \n3046  ",
            "  private int showTxns(Hive db, ShowTxnsDesc desc) throws HiveException {\n    // Call the metastore to get the currently queued and running compactions.\n    GetOpenTxnsInfoResponse rsp = db.showTransactions();\n\n    // Write the results into the file\n    DataOutputStream os = getOutputStream(desc.getResFile());\n    try {\n      // Write a header\n      os.writeBytes(\"Transaction ID\");\n      os.write(separator);\n      os.writeBytes(\"Transaction State\");\n      os.write(separator);\n      os.writeBytes(\"Started Time\");\n      os.write(separator);\n      os.writeBytes(\"Last Heartbeat Time\");\n      os.write(separator);\n      os.writeBytes(\"User\");\n      os.write(separator);\n      os.writeBytes(\"Hostname\");\n      os.write(terminator);\n\n      for (TxnInfo txn : rsp.getOpen_txns()) {\n        os.writeBytes(Long.toString(txn.getId()));\n        os.write(separator);\n        os.writeBytes(txn.getState().toString());\n        os.write(separator);\n        os.writeBytes(Long.toString(txn.getStartedTime()));\n        os.write(separator);\n        os.writeBytes(Long.toString(txn.getLastHeartbeatTime()));\n        os.write(separator);\n        os.writeBytes(txn.getUser());\n        os.write(separator);\n        os.writeBytes(txn.getHostname());\n        os.write(terminator);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"show transactions: \" + stringifyException(e));\n      return 1;\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }",
            "3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042 +\n3043  \n3044  \n3045  \n3046  \n3047  \n3048  ",
            "  private int showTxns(Hive db, ShowTxnsDesc desc) throws HiveException {\n    // Call the metastore to get the currently queued and running compactions.\n    GetOpenTxnsInfoResponse rsp = db.showTransactions();\n\n    // Write the results into the file\n    DataOutputStream os = getOutputStream(desc.getResFile());\n    try {\n      // Write a header\n      os.writeBytes(\"Transaction ID\");\n      os.write(separator);\n      os.writeBytes(\"Transaction State\");\n      os.write(separator);\n      os.writeBytes(\"Started Time\");\n      os.write(separator);\n      os.writeBytes(\"Last Heartbeat Time\");\n      os.write(separator);\n      os.writeBytes(\"User\");\n      os.write(separator);\n      os.writeBytes(\"Hostname\");\n      os.write(terminator);\n\n      for (TxnInfo txn : rsp.getOpen_txns()) {\n        os.writeBytes(Long.toString(txn.getId()));\n        os.write(separator);\n        os.writeBytes(txn.getState().toString());\n        os.write(separator);\n        os.writeBytes(Long.toString(txn.getStartedTime()));\n        os.write(separator);\n        os.writeBytes(Long.toString(txn.getLastHeartbeatTime()));\n        os.write(separator);\n        os.writeBytes(txn.getUser());\n        os.write(separator);\n        os.writeBytes(txn.getHostname());\n        os.write(terminator);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"show transactions: \", e);\n      return 1;\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }"
        ],
        [
            "MoveTask::execute(DriverContext)",
            " 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 -\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n      Utilities.FILE_OP_LOGGER.trace(\"Executing MoveWork \" + System.identityHashCode(work)\n        + \" with \" + work.getLoadFileWork() + \"; \" + work.getLoadTableWork() + \"; \"\n        + work.getLoadMultiFilesWork());\n    }\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        if (targetPath.equals(sourcePath)) {\n          Utilities.FILE_OP_LOGGER.debug(\"MoveTask not moving \" + sourcePath);\n        } else {\n          Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving \" + sourcePath + \" to \" + targetPath);\n          if(lfd.getWriteType() == AcidUtils.Operation.INSERT) {\n            //'targetPath' is table root of un-partitioned table/partition\n            //'sourcePath' result of 'select ...' part of CTAS statement\n            assert lfd.getIsDfsDir();\n            FileSystem srcFs = sourcePath.getFileSystem(conf);\n            List<Path> newFiles = new ArrayList<>();\n            Hive.moveAcidFiles(srcFs, srcFs.globStatus(sourcePath), targetPath, newFiles);\n          }\n          else {\n            moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n          }\n        }\n      }\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      // This is also used for MM table conversion.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        List<String> targetPrefixes = lmfd.getTargetPrefixes();\n        for (int i = 0; i <lmfd.getSourceDirs().size(); ++i) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          String filePrefix = targetPrefixes == null ? null : targetPrefixes.get(i);\n          FileSystem destFs = destPath.getFileSystem(conf);\n          if (filePrefix == null) {\n            if (!destFs.exists(destPath.getParent())) {\n              destFs.mkdirs(destPath.getParent());\n            }\n            Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving (multi-file) \" + srcPath + \" to \" + destPath);\n            moveFile(srcPath, destPath, isDfsDir);\n          } else {\n            if (!destFs.exists(destPath)) {\n              destFs.mkdirs(destPath);\n            }\n            FileSystem srcFs = srcPath.getFileSystem(conf);\n            FileStatus[] children = srcFs.listStatus(srcPath);\n            if (children != null) {\n              for (FileStatus child : children) {\n                Path childSrc = child.getPath();\n                Path childDest = new Path(destPath, filePrefix + childSrc.getName());\n                Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving (multi-file) \" + childSrc + \" to \" + childDest);\n                moveFile(childSrc, childDest, isDfsDir);\n              }\n            } else {\n              Utilities.FILE_OP_LOGGER.debug(\"MoveTask skipping empty directory (multi-file) \" + srcPath);\n            }\n            if (!srcFs.delete(srcPath, false)) {\n              throw new IOException(\"Couldn't delete \" + srcPath + \" after moving all the files\");\n            }\n          }\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n          Utilities.FILE_OP_LOGGER.trace(mesg.toString() + \" \" + mesg_detail);\n        }\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        checkFileFormats(db, tbd, table);\n\n        boolean isFullAcidOp = work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID\n            && !tbd.isMmTable();\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n            Utilities.FILE_OP_LOGGER.trace(\"loadTable called from \" + tbd.getSourcePath()\n              + \" into \" + tbd.getTable().getTableName());\n          }\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getLoadFileType(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd), isFullAcidOp, hasFollowingStatsTask(),\n              tbd.getTxnId(), tbd.getStmtId(), tbd.isMmTable());\n          if (work.getOutputs() != null) {\n            DDLTask.addIfAbsentByName(new WriteEntity(table,\n              getWriteType(tbd, work.getLoadTableWork().getWriteType())), work.getOutputs());\n          }\n        } else {\n          LOG.info(\"Partition is: \" + tbd.getPartitionSpec().toString());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          TaskInformation ti = new TaskInformation(this, tbd.getSourcePath().toUri().toString());\n          inferTaskInformation(ti);\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n            dc = handleDynParts(db, table, tbd, ti, dpCtx);\n          } else { // static partitions\n            dc = handleStaticParts(db, table, tbd, ti);\n          }\n        }\n        if (work.getLineagState() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          work.getLineagState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (HiveException he) {\n      int errorCode = 1;\n\n      if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {\n        errorCode = he.getCanonicalErrorMsg().getErrorCode();\n        if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {\n          console.printError(\"Failed with exception \" + he.getMessage(), \"\\n\"\n              + StringUtils.stringifyException(he));\n        } else {\n          console.printError(\"Failed with exception \" + he.getMessage()\n              + \"\\nRemote Exception: \" + he.getRemoteErrorMsg());\n          console.printInfo(\"\\n\", StringUtils.stringifyException(he),false);\n        }\n      }\n\n      setException(he);\n      return errorCode;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }",
            " 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n      Utilities.FILE_OP_LOGGER.trace(\"Executing MoveWork \" + System.identityHashCode(work)\n        + \" with \" + work.getLoadFileWork() + \"; \" + work.getLoadTableWork() + \"; \"\n        + work.getLoadMultiFilesWork());\n    }\n\n    try {\n      if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return 0;\n      }\n      Hive db = getHive();\n\n      // Do any hive related operations like moving tables and files\n      // to appropriate locations\n      LoadFileDesc lfd = work.getLoadFileWork();\n      if (lfd != null) {\n        Path targetPath = lfd.getTargetDir();\n        Path sourcePath = lfd.getSourcePath();\n        if (targetPath.equals(sourcePath)) {\n          Utilities.FILE_OP_LOGGER.debug(\"MoveTask not moving \" + sourcePath);\n        } else {\n          Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving \" + sourcePath + \" to \" + targetPath);\n          if(lfd.getWriteType() == AcidUtils.Operation.INSERT) {\n            //'targetPath' is table root of un-partitioned table/partition\n            //'sourcePath' result of 'select ...' part of CTAS statement\n            assert lfd.getIsDfsDir();\n            FileSystem srcFs = sourcePath.getFileSystem(conf);\n            List<Path> newFiles = new ArrayList<>();\n            Hive.moveAcidFiles(srcFs, srcFs.globStatus(sourcePath), targetPath, newFiles);\n          }\n          else {\n            moveFile(sourcePath, targetPath, lfd.getIsDfsDir());\n          }\n        }\n      }\n      // Multi-file load is for dynamic partitions when some partitions do not\n      // need to merge and they can simply be moved to the target directory.\n      // This is also used for MM table conversion.\n      LoadMultiFilesDesc lmfd = work.getLoadMultiFilesWork();\n      if (lmfd != null) {\n        boolean isDfsDir = lmfd.getIsDfsDir();\n        List<String> targetPrefixes = lmfd.getTargetPrefixes();\n        for (int i = 0; i <lmfd.getSourceDirs().size(); ++i) {\n          Path srcPath = lmfd.getSourceDirs().get(i);\n          Path destPath = lmfd.getTargetDirs().get(i);\n          String filePrefix = targetPrefixes == null ? null : targetPrefixes.get(i);\n          FileSystem destFs = destPath.getFileSystem(conf);\n          if (filePrefix == null) {\n            if (!destFs.exists(destPath.getParent())) {\n              destFs.mkdirs(destPath.getParent());\n            }\n            Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving (multi-file) \" + srcPath + \" to \" + destPath);\n            moveFile(srcPath, destPath, isDfsDir);\n          } else {\n            if (!destFs.exists(destPath)) {\n              destFs.mkdirs(destPath);\n            }\n            FileSystem srcFs = srcPath.getFileSystem(conf);\n            FileStatus[] children = srcFs.listStatus(srcPath);\n            if (children != null) {\n              for (FileStatus child : children) {\n                Path childSrc = child.getPath();\n                Path childDest = new Path(destPath, filePrefix + childSrc.getName());\n                Utilities.FILE_OP_LOGGER.debug(\"MoveTask moving (multi-file) \" + childSrc + \" to \" + childDest);\n                moveFile(childSrc, childDest, isDfsDir);\n              }\n            } else {\n              Utilities.FILE_OP_LOGGER.debug(\"MoveTask skipping empty directory (multi-file) \" + srcPath);\n            }\n            if (!srcFs.delete(srcPath, false)) {\n              throw new IOException(\"Couldn't delete \" + srcPath + \" after moving all the files\");\n            }\n          }\n        }\n      }\n\n      // Next we do this for tables and partitions\n      LoadTableDesc tbd = work.getLoadTableWork();\n      if (tbd != null) {\n        StringBuilder mesg = new StringBuilder(\"Loading data to table \")\n            .append( tbd.getTable().getTableName());\n        if (tbd.getPartitionSpec().size() > 0) {\n          mesg.append(\" partition (\");\n          Map<String, String> partSpec = tbd.getPartitionSpec();\n          for (String key: partSpec.keySet()) {\n            mesg.append(key).append('=').append(partSpec.get(key)).append(\", \");\n          }\n          mesg.setLength(mesg.length()-2);\n          mesg.append(')');\n        }\n        String mesg_detail = \" from \" + tbd.getSourcePath();\n        if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n          Utilities.FILE_OP_LOGGER.trace(mesg.toString() + \" \" + mesg_detail);\n        }\n        console.printInfo(mesg.toString(), mesg_detail);\n        Table table = db.getTable(tbd.getTable().getTableName());\n\n        checkFileFormats(db, tbd, table);\n\n        boolean isFullAcidOp = work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID\n            && !tbd.isMmTable();\n\n        // Create a data container\n        DataContainer dc = null;\n        if (tbd.getPartitionSpec().size() == 0) {\n          dc = new DataContainer(table.getTTable());\n          if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n            Utilities.FILE_OP_LOGGER.trace(\"loadTable called from \" + tbd.getSourcePath()\n              + \" into \" + tbd.getTable().getTableName());\n          }\n          db.loadTable(tbd.getSourcePath(), tbd.getTable().getTableName(), tbd.getLoadFileType(),\n              work.isSrcLocal(), isSkewedStoredAsDirs(tbd), isFullAcidOp, hasFollowingStatsTask(),\n              tbd.getTxnId(), tbd.getStmtId(), tbd.isMmTable());\n          if (work.getOutputs() != null) {\n            DDLTask.addIfAbsentByName(new WriteEntity(table,\n              getWriteType(tbd, work.getLoadTableWork().getWriteType())), work.getOutputs());\n          }\n        } else {\n          LOG.info(\"Partition is: {}\", tbd.getPartitionSpec());\n\n          // Check if the bucketing and/or sorting columns were inferred\n          TaskInformation ti = new TaskInformation(this, tbd.getSourcePath().toUri().toString());\n          inferTaskInformation(ti);\n          // deal with dynamic partitions\n          DynamicPartitionCtx dpCtx = tbd.getDPCtx();\n          if (dpCtx != null && dpCtx.getNumDPCols() > 0) { // dynamic partitions\n            dc = handleDynParts(db, table, tbd, ti, dpCtx);\n          } else { // static partitions\n            dc = handleStaticParts(db, table, tbd, ti);\n          }\n        }\n        if (work.getLineagState() != null && dc != null) {\n          // If we are doing an update or a delete the number of columns in the table will not\n          // match the number of columns in the file sink.  For update there will be one too many\n          // (because of the ROW__ID), and in the case of the delete there will be just the\n          // ROW__ID, which we don't need to worry about from a lineage perspective.\n          List<FieldSchema> tableCols = null;\n          switch (work.getLoadTableWork().getWriteType()) {\n            case DELETE:\n            case UPDATE:\n              // Pass an empty list as no columns will be written to the file.\n              // TODO I should be able to make this work for update\n              tableCols = new ArrayList<>();\n              break;\n\n            default:\n              tableCols = table.getCols();\n              break;\n          }\n          work.getLineagState().setLineage(tbd.getSourcePath(), dc, tableCols);\n        }\n        releaseLocks(tbd);\n      }\n\n      return 0;\n    } catch (HiveException he) {\n      int errorCode = 1;\n\n      if (he.getCanonicalErrorMsg() != ErrorMsg.GENERIC_ERROR) {\n        errorCode = he.getCanonicalErrorMsg().getErrorCode();\n        if (he.getCanonicalErrorMsg() == ErrorMsg.UNRESOLVED_RT_EXCEPTION) {\n          console.printError(\"Failed with exception \" + he.getMessage(), \"\\n\"\n              + StringUtils.stringifyException(he));\n        } else {\n          console.printError(\"Failed with exception \" + he.getMessage()\n              + \"\\nRemote Exception: \" + he.getRemoteErrorMsg());\n          console.printInfo(\"\\n\", StringUtils.stringifyException(he),false);\n        }\n      }\n\n      setException(he);\n      return errorCode;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      setException(e);\n      return (1);\n    }\n  }"
        ],
        [
            "ColumnStatsUpdateTask::readDateValue(String)",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329 -\n 330  \n 331  \n 332  ",
            "  private Date readDateValue(String dateStr) {\n    // try either yyyy-mm-dd, or integer representing days since epoch\n    try {\n      DateWritable writableVal = new DateWritable(java.sql.Date.valueOf(dateStr));\n      return new Date(writableVal.getDays());\n    } catch (IllegalArgumentException err) {\n      // Fallback to integer parsing\n      LOG.debug(\"Reading date value as days since epoch: \" + dateStr);\n      return new Date(Long.parseLong(dateStr));\n    }\n  }",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329 +\n 330  \n 331  \n 332  ",
            "  private Date readDateValue(String dateStr) {\n    // try either yyyy-mm-dd, or integer representing days since epoch\n    try {\n      DateWritable writableVal = new DateWritable(java.sql.Date.valueOf(dateStr));\n      return new Date(writableVal.getDays());\n    } catch (IllegalArgumentException err) {\n      // Fallback to integer parsing\n      LOG.debug(\"Reading date value as days since epoch: {}\", dateStr);\n      return new Date(Long.parseLong(dateStr));\n    }\n  }"
        ],
        [
            "DDLTask::showCreateTable(Hive,DataOutputStream,String)",
            "2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475 -\n2476  \n2477  \n2478  \n2479  \n2480  ",
            "  private int showCreateTable(Hive db, DataOutputStream outStream, String tableName)\n      throws HiveException {\n    final String EXTERNAL = \"external\";\n    final String TEMPORARY = \"temporary\";\n    final String LIST_COLUMNS = \"columns\";\n    final String TBL_COMMENT = \"tbl_comment\";\n    final String LIST_PARTITIONS = \"partitions\";\n    final String SORT_BUCKET = \"sort_bucket\";\n    final String SKEWED_INFO = \"tbl_skewedinfo\";\n    final String ROW_FORMAT = \"row_format\";\n    final String TBL_LOCATION = \"tbl_location\";\n    final String TBL_PROPERTIES = \"tbl_properties\";\n    boolean needsLocation = true;\n    StringBuilder createTab_str = new StringBuilder();\n\n    Table tbl = db.getTable(tableName, false);\n    List<String> duplicateProps = new ArrayList<String>();\n    try {\n      needsLocation = doesTableNeedLocation(tbl);\n\n      if (tbl.isView()) {\n        String createTab_stmt = \"CREATE VIEW `\" + tableName + \"` AS \" +\n            tbl.getViewExpandedText();\n        outStream.write(createTab_stmt.getBytes(StandardCharsets.UTF_8));\n        return 0;\n      }\n\n      createTab_str.append(\"CREATE <\" + TEMPORARY + \"><\" + EXTERNAL + \">TABLE `\");\n      createTab_str.append(tableName + \"`(\\n\");\n      createTab_str.append(\"<\" + LIST_COLUMNS + \">)\\n\");\n      createTab_str.append(\"<\" + TBL_COMMENT + \">\\n\");\n      createTab_str.append(\"<\" + LIST_PARTITIONS + \">\\n\");\n      createTab_str.append(\"<\" + SORT_BUCKET + \">\\n\");\n      createTab_str.append(\"<\" + SKEWED_INFO + \">\\n\");\n      createTab_str.append(\"<\" + ROW_FORMAT + \">\\n\");\n      if (needsLocation) {\n        createTab_str.append(\"LOCATION\\n\");\n        createTab_str.append(\"<\" + TBL_LOCATION + \">\\n\");\n      }\n      createTab_str.append(\"TBLPROPERTIES (\\n\");\n      createTab_str.append(\"<\" + TBL_PROPERTIES + \">)\\n\");\n      ST createTab_stmt = new ST(createTab_str.toString());\n\n      // For cases where the table is temporary\n      String tbl_temp = \"\";\n      if (tbl.isTemporary()) {\n        duplicateProps.add(\"TEMPORARY\");\n        tbl_temp = \"TEMPORARY \";\n      }\n      // For cases where the table is external\n      String tbl_external = \"\";\n      if (tbl.getTableType() == TableType.EXTERNAL_TABLE) {\n        duplicateProps.add(\"EXTERNAL\");\n        tbl_external = \"EXTERNAL \";\n      }\n\n      // Columns\n      String tbl_columns = \"\";\n      List<FieldSchema> cols = tbl.getCols();\n      List<String> columns = new ArrayList<String>();\n      for (FieldSchema col : cols) {\n        String columnDesc = \"  `\" + col.getName() + \"` \" + col.getType();\n        if (col.getComment() != null) {\n          columnDesc = columnDesc + \" COMMENT '\"\n              + HiveStringUtils.escapeHiveCommand(col.getComment()) + \"'\";\n        }\n        columns.add(columnDesc);\n      }\n      tbl_columns = StringUtils.join(columns, \", \\n\");\n\n      // Table comment\n      String tbl_comment = \"\";\n      String tabComment = tbl.getProperty(\"comment\");\n      if (tabComment != null) {\n        duplicateProps.add(\"comment\");\n        tbl_comment = \"COMMENT '\"\n            + HiveStringUtils.escapeHiveCommand(tabComment) + \"'\";\n      }\n\n      // Partitions\n      String tbl_partitions = \"\";\n      List<FieldSchema> partKeys = tbl.getPartitionKeys();\n      if (partKeys.size() > 0) {\n        tbl_partitions += \"PARTITIONED BY ( \\n\";\n        List<String> partCols = new ArrayList<String>();\n        for (FieldSchema partKey : partKeys) {\n          String partColDesc = \"  `\" + partKey.getName() + \"` \" + partKey.getType();\n          if (partKey.getComment() != null) {\n            partColDesc = partColDesc + \" COMMENT '\"\n                + HiveStringUtils.escapeHiveCommand(partKey.getComment()) + \"'\";\n          }\n          partCols.add(partColDesc);\n        }\n        tbl_partitions += StringUtils.join(partCols, \", \\n\");\n        tbl_partitions += \")\";\n      }\n\n      // Clusters (Buckets)\n      String tbl_sort_bucket = \"\";\n      List<String> buckCols = tbl.getBucketCols();\n      if (buckCols.size() > 0) {\n        duplicateProps.add(\"SORTBUCKETCOLSPREFIX\");\n        tbl_sort_bucket += \"CLUSTERED BY ( \\n  \";\n        tbl_sort_bucket += StringUtils.join(buckCols, \", \\n  \");\n        tbl_sort_bucket += \") \\n\";\n        List<Order> sortCols = tbl.getSortCols();\n        if (sortCols.size() > 0) {\n          tbl_sort_bucket += \"SORTED BY ( \\n\";\n          // Order\n          List<String> sortKeys = new ArrayList<String>();\n          for (Order sortCol : sortCols) {\n            String sortKeyDesc = \"  \" + sortCol.getCol() + \" \";\n            if (sortCol.getOrder() == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_ASC) {\n              sortKeyDesc = sortKeyDesc + \"ASC\";\n            }\n            else if (sortCol.getOrder() == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_DESC) {\n              sortKeyDesc = sortKeyDesc + \"DESC\";\n            }\n            sortKeys.add(sortKeyDesc);\n          }\n          tbl_sort_bucket += StringUtils.join(sortKeys, \", \\n\");\n          tbl_sort_bucket += \") \\n\";\n        }\n        tbl_sort_bucket += \"INTO \" + tbl.getNumBuckets() + \" BUCKETS\";\n      }\n\n      // Skewed Info\n      StringBuilder tbl_skewedinfo = new StringBuilder();\n      SkewedInfo skewedInfo = tbl.getSkewedInfo();\n      if (skewedInfo != null && !skewedInfo.getSkewedColNames().isEmpty()) {\n        tbl_skewedinfo.append(\"SKEWED BY (\" + StringUtils.join(skewedInfo.getSkewedColNames(), \",\") + \")\\n\");\n        tbl_skewedinfo.append(\"  ON (\");\n        List<String> colValueList = new ArrayList<String>();\n        for (List<String> colValues : skewedInfo.getSkewedColValues()) {\n          colValueList.add(\"('\" + StringUtils.join(colValues, \"','\") + \"')\");\n        }\n        tbl_skewedinfo.append(StringUtils.join(colValueList, \",\") + \")\");\n        if (tbl.isStoredAsSubDirectories()) {\n          tbl_skewedinfo.append(\"\\n  STORED AS DIRECTORIES\");\n        }\n      }\n\n      // Row format (SerDe)\n      StringBuilder tbl_row_format = new StringBuilder();\n      StorageDescriptor sd = tbl.getTTable().getSd();\n      SerDeInfo serdeInfo = sd.getSerdeInfo();\n      Map<String, String> serdeParams = serdeInfo.getParameters();\n      tbl_row_format.append(\"ROW FORMAT SERDE \\n\");\n      tbl_row_format.append(\"  '\"\n          + HiveStringUtils.escapeHiveCommand(serdeInfo.getSerializationLib()) + \"' \\n\");\n      if (tbl.getStorageHandler() == null) {\n        // If serialization.format property has the default value, it will not to be included in\n        // SERDE properties\n        if (Warehouse.DEFAULT_SERIALIZATION_FORMAT.equals(serdeParams.get(\n            serdeConstants.SERIALIZATION_FORMAT))){\n          serdeParams.remove(serdeConstants.SERIALIZATION_FORMAT);\n        }\n        if (!serdeParams.isEmpty()) {\n          appendSerdeParams(tbl_row_format, serdeParams).append(\" \\n\");\n        }\n        tbl_row_format.append(\"STORED AS INPUTFORMAT \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(sd.getInputFormat()) + \"' \\n\");\n        tbl_row_format.append(\"OUTPUTFORMAT \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(sd.getOutputFormat()) + \"'\");\n      } else {\n        duplicateProps.add(META_TABLE_STORAGE);\n        tbl_row_format.append(\"STORED BY \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(tbl.getParameters().get(\n            META_TABLE_STORAGE)) + \"' \\n\");\n        // SerDe Properties\n        if (!serdeParams.isEmpty()) {\n          appendSerdeParams(tbl_row_format, serdeInfo.getParameters());\n        }\n      }\n      String tbl_location = \"  '\" + HiveStringUtils.escapeHiveCommand(sd.getLocation()) + \"'\";\n\n      // Table properties\n      duplicateProps.addAll(Arrays.asList(StatsSetupConst.TABLE_PARAMS_STATS_KEYS));\n      String tbl_properties = propertiesToString(tbl.getParameters(), duplicateProps);\n\n      createTab_stmt.add(TEMPORARY, tbl_temp);\n      createTab_stmt.add(EXTERNAL, tbl_external);\n      createTab_stmt.add(LIST_COLUMNS, tbl_columns);\n      createTab_stmt.add(TBL_COMMENT, tbl_comment);\n      createTab_stmt.add(LIST_PARTITIONS, tbl_partitions);\n      createTab_stmt.add(SORT_BUCKET, tbl_sort_bucket);\n      createTab_stmt.add(SKEWED_INFO, tbl_skewedinfo);\n      createTab_stmt.add(ROW_FORMAT, tbl_row_format);\n      // Table location should not be printed with hbase backed tables\n      if (needsLocation) {\n        createTab_stmt.add(TBL_LOCATION, tbl_location);\n      }\n      createTab_stmt.add(TBL_PROPERTIES, tbl_properties);\n\n      outStream.write(createTab_stmt.render().getBytes(StandardCharsets.UTF_8));\n    } catch (IOException e) {\n      LOG.info(\"show create table: \" + stringifyException(e));\n      return 1;\n    }\n\n    return 0;\n  }",
            "2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477 +\n2478  \n2479  \n2480  \n2481  \n2482  ",
            "  private int showCreateTable(Hive db, DataOutputStream outStream, String tableName)\n      throws HiveException {\n    final String EXTERNAL = \"external\";\n    final String TEMPORARY = \"temporary\";\n    final String LIST_COLUMNS = \"columns\";\n    final String TBL_COMMENT = \"tbl_comment\";\n    final String LIST_PARTITIONS = \"partitions\";\n    final String SORT_BUCKET = \"sort_bucket\";\n    final String SKEWED_INFO = \"tbl_skewedinfo\";\n    final String ROW_FORMAT = \"row_format\";\n    final String TBL_LOCATION = \"tbl_location\";\n    final String TBL_PROPERTIES = \"tbl_properties\";\n    boolean needsLocation = true;\n    StringBuilder createTab_str = new StringBuilder();\n\n    Table tbl = db.getTable(tableName, false);\n    List<String> duplicateProps = new ArrayList<String>();\n    try {\n      needsLocation = doesTableNeedLocation(tbl);\n\n      if (tbl.isView()) {\n        String createTab_stmt = \"CREATE VIEW `\" + tableName + \"` AS \" +\n            tbl.getViewExpandedText();\n        outStream.write(createTab_stmt.getBytes(StandardCharsets.UTF_8));\n        return 0;\n      }\n\n      createTab_str.append(\"CREATE <\" + TEMPORARY + \"><\" + EXTERNAL + \">TABLE `\");\n      createTab_str.append(tableName + \"`(\\n\");\n      createTab_str.append(\"<\" + LIST_COLUMNS + \">)\\n\");\n      createTab_str.append(\"<\" + TBL_COMMENT + \">\\n\");\n      createTab_str.append(\"<\" + LIST_PARTITIONS + \">\\n\");\n      createTab_str.append(\"<\" + SORT_BUCKET + \">\\n\");\n      createTab_str.append(\"<\" + SKEWED_INFO + \">\\n\");\n      createTab_str.append(\"<\" + ROW_FORMAT + \">\\n\");\n      if (needsLocation) {\n        createTab_str.append(\"LOCATION\\n\");\n        createTab_str.append(\"<\" + TBL_LOCATION + \">\\n\");\n      }\n      createTab_str.append(\"TBLPROPERTIES (\\n\");\n      createTab_str.append(\"<\" + TBL_PROPERTIES + \">)\\n\");\n      ST createTab_stmt = new ST(createTab_str.toString());\n\n      // For cases where the table is temporary\n      String tbl_temp = \"\";\n      if (tbl.isTemporary()) {\n        duplicateProps.add(\"TEMPORARY\");\n        tbl_temp = \"TEMPORARY \";\n      }\n      // For cases where the table is external\n      String tbl_external = \"\";\n      if (tbl.getTableType() == TableType.EXTERNAL_TABLE) {\n        duplicateProps.add(\"EXTERNAL\");\n        tbl_external = \"EXTERNAL \";\n      }\n\n      // Columns\n      String tbl_columns = \"\";\n      List<FieldSchema> cols = tbl.getCols();\n      List<String> columns = new ArrayList<String>();\n      for (FieldSchema col : cols) {\n        String columnDesc = \"  `\" + col.getName() + \"` \" + col.getType();\n        if (col.getComment() != null) {\n          columnDesc = columnDesc + \" COMMENT '\"\n              + HiveStringUtils.escapeHiveCommand(col.getComment()) + \"'\";\n        }\n        columns.add(columnDesc);\n      }\n      tbl_columns = StringUtils.join(columns, \", \\n\");\n\n      // Table comment\n      String tbl_comment = \"\";\n      String tabComment = tbl.getProperty(\"comment\");\n      if (tabComment != null) {\n        duplicateProps.add(\"comment\");\n        tbl_comment = \"COMMENT '\"\n            + HiveStringUtils.escapeHiveCommand(tabComment) + \"'\";\n      }\n\n      // Partitions\n      String tbl_partitions = \"\";\n      List<FieldSchema> partKeys = tbl.getPartitionKeys();\n      if (partKeys.size() > 0) {\n        tbl_partitions += \"PARTITIONED BY ( \\n\";\n        List<String> partCols = new ArrayList<String>();\n        for (FieldSchema partKey : partKeys) {\n          String partColDesc = \"  `\" + partKey.getName() + \"` \" + partKey.getType();\n          if (partKey.getComment() != null) {\n            partColDesc = partColDesc + \" COMMENT '\"\n                + HiveStringUtils.escapeHiveCommand(partKey.getComment()) + \"'\";\n          }\n          partCols.add(partColDesc);\n        }\n        tbl_partitions += StringUtils.join(partCols, \", \\n\");\n        tbl_partitions += \")\";\n      }\n\n      // Clusters (Buckets)\n      String tbl_sort_bucket = \"\";\n      List<String> buckCols = tbl.getBucketCols();\n      if (buckCols.size() > 0) {\n        duplicateProps.add(\"SORTBUCKETCOLSPREFIX\");\n        tbl_sort_bucket += \"CLUSTERED BY ( \\n  \";\n        tbl_sort_bucket += StringUtils.join(buckCols, \", \\n  \");\n        tbl_sort_bucket += \") \\n\";\n        List<Order> sortCols = tbl.getSortCols();\n        if (sortCols.size() > 0) {\n          tbl_sort_bucket += \"SORTED BY ( \\n\";\n          // Order\n          List<String> sortKeys = new ArrayList<String>();\n          for (Order sortCol : sortCols) {\n            String sortKeyDesc = \"  \" + sortCol.getCol() + \" \";\n            if (sortCol.getOrder() == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_ASC) {\n              sortKeyDesc = sortKeyDesc + \"ASC\";\n            }\n            else if (sortCol.getOrder() == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_DESC) {\n              sortKeyDesc = sortKeyDesc + \"DESC\";\n            }\n            sortKeys.add(sortKeyDesc);\n          }\n          tbl_sort_bucket += StringUtils.join(sortKeys, \", \\n\");\n          tbl_sort_bucket += \") \\n\";\n        }\n        tbl_sort_bucket += \"INTO \" + tbl.getNumBuckets() + \" BUCKETS\";\n      }\n\n      // Skewed Info\n      StringBuilder tbl_skewedinfo = new StringBuilder();\n      SkewedInfo skewedInfo = tbl.getSkewedInfo();\n      if (skewedInfo != null && !skewedInfo.getSkewedColNames().isEmpty()) {\n        tbl_skewedinfo.append(\"SKEWED BY (\" + StringUtils.join(skewedInfo.getSkewedColNames(), \",\") + \")\\n\");\n        tbl_skewedinfo.append(\"  ON (\");\n        List<String> colValueList = new ArrayList<String>();\n        for (List<String> colValues : skewedInfo.getSkewedColValues()) {\n          colValueList.add(\"('\" + StringUtils.join(colValues, \"','\") + \"')\");\n        }\n        tbl_skewedinfo.append(StringUtils.join(colValueList, \",\") + \")\");\n        if (tbl.isStoredAsSubDirectories()) {\n          tbl_skewedinfo.append(\"\\n  STORED AS DIRECTORIES\");\n        }\n      }\n\n      // Row format (SerDe)\n      StringBuilder tbl_row_format = new StringBuilder();\n      StorageDescriptor sd = tbl.getTTable().getSd();\n      SerDeInfo serdeInfo = sd.getSerdeInfo();\n      Map<String, String> serdeParams = serdeInfo.getParameters();\n      tbl_row_format.append(\"ROW FORMAT SERDE \\n\");\n      tbl_row_format.append(\"  '\"\n          + HiveStringUtils.escapeHiveCommand(serdeInfo.getSerializationLib()) + \"' \\n\");\n      if (tbl.getStorageHandler() == null) {\n        // If serialization.format property has the default value, it will not to be included in\n        // SERDE properties\n        if (Warehouse.DEFAULT_SERIALIZATION_FORMAT.equals(serdeParams.get(\n            serdeConstants.SERIALIZATION_FORMAT))){\n          serdeParams.remove(serdeConstants.SERIALIZATION_FORMAT);\n        }\n        if (!serdeParams.isEmpty()) {\n          appendSerdeParams(tbl_row_format, serdeParams).append(\" \\n\");\n        }\n        tbl_row_format.append(\"STORED AS INPUTFORMAT \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(sd.getInputFormat()) + \"' \\n\");\n        tbl_row_format.append(\"OUTPUTFORMAT \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(sd.getOutputFormat()) + \"'\");\n      } else {\n        duplicateProps.add(META_TABLE_STORAGE);\n        tbl_row_format.append(\"STORED BY \\n  '\"\n            + HiveStringUtils.escapeHiveCommand(tbl.getParameters().get(\n            META_TABLE_STORAGE)) + \"' \\n\");\n        // SerDe Properties\n        if (!serdeParams.isEmpty()) {\n          appendSerdeParams(tbl_row_format, serdeInfo.getParameters());\n        }\n      }\n      String tbl_location = \"  '\" + HiveStringUtils.escapeHiveCommand(sd.getLocation()) + \"'\";\n\n      // Table properties\n      duplicateProps.addAll(Arrays.asList(StatsSetupConst.TABLE_PARAMS_STATS_KEYS));\n      String tbl_properties = propertiesToString(tbl.getParameters(), duplicateProps);\n\n      createTab_stmt.add(TEMPORARY, tbl_temp);\n      createTab_stmt.add(EXTERNAL, tbl_external);\n      createTab_stmt.add(LIST_COLUMNS, tbl_columns);\n      createTab_stmt.add(TBL_COMMENT, tbl_comment);\n      createTab_stmt.add(LIST_PARTITIONS, tbl_partitions);\n      createTab_stmt.add(SORT_BUCKET, tbl_sort_bucket);\n      createTab_stmt.add(SKEWED_INFO, tbl_skewedinfo);\n      createTab_stmt.add(ROW_FORMAT, tbl_row_format);\n      // Table location should not be printed with hbase backed tables\n      if (needsLocation) {\n        createTab_stmt.add(TBL_LOCATION, tbl_location);\n      }\n      createTab_stmt.add(TBL_PROPERTIES, tbl_properties);\n\n      outStream.write(createTab_stmt.render().getBytes(StandardCharsets.UTF_8));\n    } catch (IOException e) {\n      LOG.info(\"show create table: \", e);\n      return 1;\n    }\n\n    return 0;\n  }"
        ],
        [
            "StatsNoJobTask::aggregateStats(ExecutorService,Hive)",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  ",
            "  private int aggregateStats(ExecutorService threadPool, Hive db) {\n    int ret = 0;\n\n    try {\n      Collection<Partition> partitions = null;\n      if (work.getPrunedPartitionList() == null) {\n        partitions = getPartitionsList();\n      } else {\n        partitions = work.getPrunedPartitionList().getPartitions();\n      }\n\n      // non-partitioned table\n      if (partitions == null) {\n        org.apache.hadoop.hive.metastore.api.Table tTable = table.getTTable();\n        Map<String, String> parameters = tTable.getParameters();\n        try {\n          Path dir = new Path(tTable.getSd().getLocation());\n          LOG.debug(\"Aggregating stats for \" + dir);\n          long numRows = 0;\n          long rawDataSize = 0;\n          long fileSize = 0;\n          long numFiles = 0;\n          FileSystem fs = dir.getFileSystem(conf);\n          FileStatus[] fileList = HiveStatsUtils.getFileStatusRecurse(dir, -1, fs);\n\n          boolean statsAvailable = false;\n          for(FileStatus file: fileList) {\n            LOG.debug(\"Computing stats for \" + file);\n            if (!file.isDir()) {\n              InputFormat<?, ?> inputFormat = ReflectionUtil.newInstance(\n                  table.getInputFormatClass(), jc);\n              InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0, new String[] { table\n                  .getDataLocation().toString() });\n              if (file.getLen() == 0) {\n                numFiles += 1;\n                statsAvailable = true;\n              } else {\n                org.apache.hadoop.mapred.RecordReader<?, ?> recordReader =\n                    inputFormat.getRecordReader(dummySplit, jc, Reporter.NULL);\n                StatsProvidingRecordReader statsRR;\n                if (recordReader instanceof StatsProvidingRecordReader) {\n                  statsRR = (StatsProvidingRecordReader) recordReader;\n                  numRows += statsRR.getStats().getRowCount();\n                  rawDataSize += statsRR.getStats().getRawDataSize();\n                  fileSize += file.getLen();\n                  numFiles += 1;\n                  statsAvailable = true;\n                }\n                recordReader.close();\n              }\n            }\n          }\n\n          if (statsAvailable) {\n            parameters.put(StatsSetupConst.ROW_COUNT, String.valueOf(numRows));\n            parameters.put(StatsSetupConst.RAW_DATA_SIZE, String.valueOf(rawDataSize));\n            parameters.put(StatsSetupConst.TOTAL_SIZE, String.valueOf(fileSize));\n            parameters.put(StatsSetupConst.NUM_FILES, String.valueOf(numFiles));\n            EnvironmentContext environmentContext = new EnvironmentContext();\n            environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED, StatsSetupConst.TASK);\n\n            db.alterTable(tableFullName, new Table(tTable), environmentContext);\n\n            String msg = \"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']';\n            if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n              Utilities.FILE_OP_LOGGER.trace(msg);\n            }\n            console.printInfo(msg);\n          }\n        } catch (Exception e) {\n          console.printInfo(\"[Warning] could not update stats for \" + tableFullName + \".\",\n              \"Failed with exception \" + e.getMessage() + \"\\n\" + StringUtils.stringifyException(e));\n        }\n      } else {\n\n        // Partitioned table\n        for (Partition partn : partitions) {\n          threadPool.execute(new StatsCollection(partn));\n        }\n\n        LOG.debug(\"Stats collection waiting for threadpool to shutdown..\");\n        shutdownAndAwaitTermination(threadPool);\n        LOG.debug(\"Stats collection threadpool shutdown successful.\");\n\n        ret = updatePartitions(db);\n      }\n\n    } catch (Exception e) {\n      // Fail the query if the stats are supposed to be reliable\n      if (work.isStatsReliable()) {\n        ret = -1;\n      }\n    }\n\n    // The return value of 0 indicates success,\n    // anything else indicates failure\n    return ret;\n  }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293 +\n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "  private int aggregateStats(ExecutorService threadPool, Hive db) {\n    int ret = 0;\n\n    try {\n      Collection<Partition> partitions = null;\n      if (work.getPrunedPartitionList() == null) {\n        partitions = getPartitionsList();\n      } else {\n        partitions = work.getPrunedPartitionList().getPartitions();\n      }\n\n      // non-partitioned table\n      if (partitions == null) {\n        org.apache.hadoop.hive.metastore.api.Table tTable = table.getTTable();\n        Map<String, String> parameters = tTable.getParameters();\n        try {\n          Path dir = new Path(tTable.getSd().getLocation());\n          LOG.debug(\"Aggregating stats for \" + dir);\n          long numRows = 0;\n          long rawDataSize = 0;\n          long fileSize = 0;\n          long numFiles = 0;\n          FileSystem fs = dir.getFileSystem(conf);\n          FileStatus[] fileList = HiveStatsUtils.getFileStatusRecurse(dir, -1, fs);\n\n          boolean statsAvailable = false;\n          for(FileStatus file: fileList) {\n            LOG.debug(\"Computing stats for \" + file);\n            if (!file.isDir()) {\n              InputFormat<?, ?> inputFormat = ReflectionUtil.newInstance(\n                  table.getInputFormatClass(), jc);\n              InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0, new String[] { table\n                  .getDataLocation().toString() });\n              if (file.getLen() == 0) {\n                numFiles += 1;\n                statsAvailable = true;\n              } else {\n                org.apache.hadoop.mapred.RecordReader<?, ?> recordReader =\n                    inputFormat.getRecordReader(dummySplit, jc, Reporter.NULL);\n                StatsProvidingRecordReader statsRR;\n                if (recordReader instanceof StatsProvidingRecordReader) {\n                  statsRR = (StatsProvidingRecordReader) recordReader;\n                  numRows += statsRR.getStats().getRowCount();\n                  rawDataSize += statsRR.getStats().getRawDataSize();\n                  fileSize += file.getLen();\n                  numFiles += 1;\n                  statsAvailable = true;\n                }\n                recordReader.close();\n              }\n            }\n          }\n\n          if (statsAvailable) {\n            parameters.put(StatsSetupConst.ROW_COUNT, String.valueOf(numRows));\n            parameters.put(StatsSetupConst.RAW_DATA_SIZE, String.valueOf(rawDataSize));\n            parameters.put(StatsSetupConst.TOTAL_SIZE, String.valueOf(fileSize));\n            parameters.put(StatsSetupConst.NUM_FILES, String.valueOf(numFiles));\n            EnvironmentContext environmentContext = new EnvironmentContext();\n            environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED, StatsSetupConst.TASK);\n\n            db.alterTable(tableFullName, new Table(tTable), environmentContext);\n\n            String msg = \"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']';\n            if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n              Utilities.FILE_OP_LOGGER.trace(msg);\n            }\n            console.printInfo(msg);\n            LOG.debug(\"Table {} does not provide stats.\", tableFullName);\n          }\n        } catch (Exception e) {\n          console.printInfo(\"[Warning] could not update stats for \" + tableFullName + \".\",\n              \"Failed with exception \" + e.getMessage() + \"\\n\" + StringUtils.stringifyException(e));\n        }\n      } else {\n\n        // Partitioned table\n        for (Partition partn : partitions) {\n          threadPool.execute(new StatsCollection(partn));\n        }\n\n        LOG.debug(\"Stats collection waiting for threadpool to shutdown..\");\n        shutdownAndAwaitTermination(threadPool);\n        LOG.debug(\"Stats collection threadpool shutdown successful.\");\n\n        ret = updatePartitions(db);\n      }\n\n    } catch (Exception e) {\n      // Fail the query if the stats are supposed to be reliable\n      if (work.isStatsReliable()) {\n        ret = -1;\n      }\n    }\n\n    // The return value of 0 indicates success,\n    // anything else indicates failure\n    return ret;\n  }"
        ],
        [
            "DDLTask::describeTable(Hive,DescTableDesc)",
            "3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394 -\n3395  \n3396  \n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424  \n3425  \n3426  \n3427  \n3428  \n3429  \n3430  \n3431  \n3432  \n3433  \n3434  \n3435  \n3436  \n3437  \n3438  \n3439  \n3440  \n3441  \n3442  \n3443  \n3444  \n3445  \n3446  \n3447  \n3448  \n3449  \n3450  \n3451  \n3452  \n3453  \n3454  \n3455  \n3456  \n3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514 -\n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  ",
            "  /**\n   * Write the description of a table to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param descTbl\n   *          This is the table we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   * @throws MetaException\n   */\n  private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException, MetaException {\n    String colPath = descTbl.getColumnPath();\n    String tableName = descTbl.getTableName();\n\n    // describe the table - populate the output stream\n    Table tbl = db.getTable(tableName, false);\n    if (tbl == null) {\n      throw new HiveException(ErrorMsg.INVALID_TABLE, tableName);\n    }\n    Partition part = null;\n    if (descTbl.getPartSpec() != null) {\n      part = db.getPartition(tbl, descTbl.getPartSpec(), false);\n      if (part == null) {\n        throw new HiveException(ErrorMsg.INVALID_PARTITION,\n            StringUtils.join(descTbl.getPartSpec().keySet(), ','), tableName);\n      }\n      tbl = part.getTable();\n    }\n\n    DataOutputStream outStream = getOutputStream(descTbl.getResFile());\n    try {\n      LOG.debug(\"DDLTask: got data for \" + tbl.getTableName());\n\n      List<FieldSchema> cols = null;\n      List<ColumnStatisticsObj> colStats = null;\n\n      Deserializer deserializer = tbl.getDeserializer(true);\n      if (deserializer instanceof AbstractSerDe) {\n        String errorMsgs = ((AbstractSerDe) deserializer).getConfigurationErrors();\n        if (errorMsgs != null && !errorMsgs.isEmpty()) {\n          throw new SQLException(errorMsgs);\n        }\n      }\n\n      if (colPath.equals(tableName)) {\n        cols = (part == null || tbl.getTableType() == TableType.VIRTUAL_VIEW) ?\n            tbl.getCols() : part.getCols();\n\n        if (!descTbl.isFormatted()) {\n          cols.addAll(tbl.getPartCols());\n        }\n\n        if (tbl.isPartitioned() && part == null) {\n          // No partitioned specified for partitioned table, lets fetch all.\n          Map<String,String> tblProps = tbl.getParameters() == null ? new HashMap<String,String>() : tbl.getParameters();\n          Map<String, Long> valueMap = new HashMap<>();\n          Map<String, Boolean> stateMap = new HashMap<>();\n          for (String stat : StatsSetupConst.supportedStats) {\n            valueMap.put(stat, 0L);\n            stateMap.put(stat, true);\n          }\n          PartitionIterable parts = new PartitionIterable(db, tbl, null, conf.getIntVar(HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX));\n          int numParts = 0;\n          for (Partition partition : parts) {\n            Map<String, String> props = partition.getParameters();\n            Boolean state = StatsSetupConst.areBasicStatsUptoDate(props);\n            for (String stat : StatsSetupConst.supportedStats) {\n              stateMap.put(stat, stateMap.get(stat) && state);\n              if (props != null && props.get(stat) != null) {\n                valueMap.put(stat, valueMap.get(stat) + Long.parseLong(props.get(stat)));\n              }\n            }\n            numParts++;\n          }\n          for (String stat : StatsSetupConst.supportedStats) {\n            StatsSetupConst.setBasicStatsState(tblProps, Boolean.toString(stateMap.get(stat)));\n            tblProps.put(stat, valueMap.get(stat).toString());\n          }\n          tblProps.put(StatsSetupConst.NUM_PARTITIONS, Integer.toString(numParts));\n          tbl.setParameters(tblProps);\n        }\n      } else {\n        if (descTbl.isFormatted()) {\n          // when column name is specified in describe table DDL, colPath will\n          // will be table_name.column_name\n          String colName = colPath.split(\"\\\\.\")[1];\n          String[] dbTab = Utilities.getDbTableName(tableName);\n          List<String> colNames = new ArrayList<String>();\n          colNames.add(colName.toLowerCase());\n          if (null == part) {\n            if (tbl.isPartitioned()) {\n              Map<String,String> tblProps = tbl.getParameters() == null ? new HashMap<String,String>() : tbl.getParameters();\n              if (tbl.isPartitionKey(colNames.get(0))) {\n                FieldSchema partCol = tbl.getPartColByName(colNames.get(0));\n                cols = Collections.singletonList(partCol);\n                PartitionIterable parts = new PartitionIterable(db, tbl, null, conf.getIntVar(HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX));\n                ColumnInfo ci = new ColumnInfo(partCol.getName(),TypeInfoUtils.getTypeInfoFromTypeString(partCol.getType()),null,false);\n                ColStatistics cs = StatsUtils.getColStatsForPartCol(ci, parts, conf);\n                ColumnStatisticsData data = new ColumnStatisticsData();\n                ColStatistics.Range r = cs.getRange();\n                StatObjectConverter.fillColumnStatisticsData(partCol.getType(), data, r == null ? null : r.minValue, r == null ? null : r.maxValue,\n                    r == null ? null : r.minValue, r == null ? null : r.maxValue, r == null ? null : r.minValue.toString(), r == null ? null : r.maxValue.toString(),\n                    cs.getNumNulls(), cs.getCountDistint(), null, cs.getAvgColLen(), cs.getAvgColLen(), cs.getNumTrues(), cs.getNumFalses());\n                ColumnStatisticsObj cso = new ColumnStatisticsObj(partCol.getName(), partCol.getType(), data);\n                colStats = Collections.singletonList(cso);\n                StatsSetupConst.setColumnStatsState(tblProps, colNames);\n              } else {\n                cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n                List<String> parts = db.getPartitionNames(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), (short) -1);\n                AggrStats aggrStats = db.getAggrColStatsFor(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), colNames, parts);\n                colStats = aggrStats.getColStats();\n                if (parts.size() == aggrStats.getPartsFound()) {\n                  StatsSetupConst.setColumnStatsState(tblProps, colNames);\n                } else {\n                  StatsSetupConst.removeColumnStatsState(tblProps, colNames);\n                }\n              }\n              tbl.setParameters(tblProps);\n            } else {\n              cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n              colStats = db.getTableColumnStatistics(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), colNames);\n            }\n          } else {\n            List<String> partitions = new ArrayList<String>();\n            partitions.add(part.getName());\n            cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n            colStats = db.getPartitionColumnStatistics(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), partitions, colNames).get(part.getName());\n          }\n        } else {\n          cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n        }\n      }\n      PrimaryKeyInfo pkInfo = null;\n      ForeignKeyInfo fkInfo = null;\n      UniqueConstraint ukInfo = null;\n      NotNullConstraint nnInfo = null;\n      if (descTbl.isExt() || descTbl.isFormatted()) {\n        pkInfo = db.getPrimaryKeys(tbl.getDbName(), tbl.getTableName());\n        fkInfo = db.getForeignKeys(tbl.getDbName(), tbl.getTableName());\n        ukInfo = db.getUniqueConstraints(tbl.getDbName(), tbl.getTableName());\n        nnInfo = db.getNotNullConstraints(tbl.getDbName(), tbl.getTableName());\n      }\n      fixDecimalColumnTypeName(cols);\n      // In case the query is served by HiveServer2, don't pad it with spaces,\n      // as HiveServer2 output is consumed by JDBC/ODBC clients.\n      boolean isOutputPadded = !SessionState.get().isHiveServerQuery();\n      formatter.describeTable(outStream, colPath, tableName, tbl, part,\n          cols, descTbl.isFormatted(), descTbl.isExt(),\n          isOutputPadded, colStats,\n          pkInfo, fkInfo, ukInfo, nnInfo);\n\n      LOG.debug(\"DDLTask: written data for \" + tbl.getTableName());\n\n    } catch (SQLException e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, tableName);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n\n    return 0;\n  }",
            "3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383  \n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394  \n3395  \n3396 +\n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424  \n3425  \n3426  \n3427  \n3428  \n3429  \n3430  \n3431  \n3432  \n3433  \n3434  \n3435  \n3436  \n3437  \n3438  \n3439  \n3440  \n3441  \n3442  \n3443  \n3444  \n3445  \n3446  \n3447  \n3448  \n3449  \n3450  \n3451  \n3452  \n3453  \n3454  \n3455  \n3456  \n3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516 +\n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  ",
            "  /**\n   * Write the description of a table to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param descTbl\n   *          This is the table we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   * @throws MetaException\n   */\n  private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException, MetaException {\n    String colPath = descTbl.getColumnPath();\n    String tableName = descTbl.getTableName();\n\n    // describe the table - populate the output stream\n    Table tbl = db.getTable(tableName, false);\n    if (tbl == null) {\n      throw new HiveException(ErrorMsg.INVALID_TABLE, tableName);\n    }\n    Partition part = null;\n    if (descTbl.getPartSpec() != null) {\n      part = db.getPartition(tbl, descTbl.getPartSpec(), false);\n      if (part == null) {\n        throw new HiveException(ErrorMsg.INVALID_PARTITION,\n            StringUtils.join(descTbl.getPartSpec().keySet(), ','), tableName);\n      }\n      tbl = part.getTable();\n    }\n\n    DataOutputStream outStream = getOutputStream(descTbl.getResFile());\n    try {\n      LOG.debug(\"DDLTask: got data for {}\", tableName);\n\n      List<FieldSchema> cols = null;\n      List<ColumnStatisticsObj> colStats = null;\n\n      Deserializer deserializer = tbl.getDeserializer(true);\n      if (deserializer instanceof AbstractSerDe) {\n        String errorMsgs = ((AbstractSerDe) deserializer).getConfigurationErrors();\n        if (errorMsgs != null && !errorMsgs.isEmpty()) {\n          throw new SQLException(errorMsgs);\n        }\n      }\n\n      if (colPath.equals(tableName)) {\n        cols = (part == null || tbl.getTableType() == TableType.VIRTUAL_VIEW) ?\n            tbl.getCols() : part.getCols();\n\n        if (!descTbl.isFormatted()) {\n          cols.addAll(tbl.getPartCols());\n        }\n\n        if (tbl.isPartitioned() && part == null) {\n          // No partitioned specified for partitioned table, lets fetch all.\n          Map<String,String> tblProps = tbl.getParameters() == null ? new HashMap<String,String>() : tbl.getParameters();\n          Map<String, Long> valueMap = new HashMap<>();\n          Map<String, Boolean> stateMap = new HashMap<>();\n          for (String stat : StatsSetupConst.supportedStats) {\n            valueMap.put(stat, 0L);\n            stateMap.put(stat, true);\n          }\n          PartitionIterable parts = new PartitionIterable(db, tbl, null, conf.getIntVar(HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX));\n          int numParts = 0;\n          for (Partition partition : parts) {\n            Map<String, String> props = partition.getParameters();\n            Boolean state = StatsSetupConst.areBasicStatsUptoDate(props);\n            for (String stat : StatsSetupConst.supportedStats) {\n              stateMap.put(stat, stateMap.get(stat) && state);\n              if (props != null && props.get(stat) != null) {\n                valueMap.put(stat, valueMap.get(stat) + Long.parseLong(props.get(stat)));\n              }\n            }\n            numParts++;\n          }\n          for (String stat : StatsSetupConst.supportedStats) {\n            StatsSetupConst.setBasicStatsState(tblProps, Boolean.toString(stateMap.get(stat)));\n            tblProps.put(stat, valueMap.get(stat).toString());\n          }\n          tblProps.put(StatsSetupConst.NUM_PARTITIONS, Integer.toString(numParts));\n          tbl.setParameters(tblProps);\n        }\n      } else {\n        if (descTbl.isFormatted()) {\n          // when column name is specified in describe table DDL, colPath will\n          // will be table_name.column_name\n          String colName = colPath.split(\"\\\\.\")[1];\n          String[] dbTab = Utilities.getDbTableName(tableName);\n          List<String> colNames = new ArrayList<String>();\n          colNames.add(colName.toLowerCase());\n          if (null == part) {\n            if (tbl.isPartitioned()) {\n              Map<String,String> tblProps = tbl.getParameters() == null ? new HashMap<String,String>() : tbl.getParameters();\n              if (tbl.isPartitionKey(colNames.get(0))) {\n                FieldSchema partCol = tbl.getPartColByName(colNames.get(0));\n                cols = Collections.singletonList(partCol);\n                PartitionIterable parts = new PartitionIterable(db, tbl, null, conf.getIntVar(HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_MAX));\n                ColumnInfo ci = new ColumnInfo(partCol.getName(),TypeInfoUtils.getTypeInfoFromTypeString(partCol.getType()),null,false);\n                ColStatistics cs = StatsUtils.getColStatsForPartCol(ci, parts, conf);\n                ColumnStatisticsData data = new ColumnStatisticsData();\n                ColStatistics.Range r = cs.getRange();\n                StatObjectConverter.fillColumnStatisticsData(partCol.getType(), data, r == null ? null : r.minValue, r == null ? null : r.maxValue,\n                    r == null ? null : r.minValue, r == null ? null : r.maxValue, r == null ? null : r.minValue.toString(), r == null ? null : r.maxValue.toString(),\n                    cs.getNumNulls(), cs.getCountDistint(), null, cs.getAvgColLen(), cs.getAvgColLen(), cs.getNumTrues(), cs.getNumFalses());\n                ColumnStatisticsObj cso = new ColumnStatisticsObj(partCol.getName(), partCol.getType(), data);\n                colStats = Collections.singletonList(cso);\n                StatsSetupConst.setColumnStatsState(tblProps, colNames);\n              } else {\n                cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n                List<String> parts = db.getPartitionNames(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), (short) -1);\n                AggrStats aggrStats = db.getAggrColStatsFor(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), colNames, parts);\n                colStats = aggrStats.getColStats();\n                if (parts.size() == aggrStats.getPartsFound()) {\n                  StatsSetupConst.setColumnStatsState(tblProps, colNames);\n                } else {\n                  StatsSetupConst.removeColumnStatsState(tblProps, colNames);\n                }\n              }\n              tbl.setParameters(tblProps);\n            } else {\n              cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n              colStats = db.getTableColumnStatistics(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), colNames);\n            }\n          } else {\n            List<String> partitions = new ArrayList<String>();\n            partitions.add(part.getName());\n            cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n            colStats = db.getPartitionColumnStatistics(dbTab[0].toLowerCase(), dbTab[1].toLowerCase(), partitions, colNames).get(part.getName());\n          }\n        } else {\n          cols = Hive.getFieldsFromDeserializer(colPath, deserializer);\n        }\n      }\n      PrimaryKeyInfo pkInfo = null;\n      ForeignKeyInfo fkInfo = null;\n      UniqueConstraint ukInfo = null;\n      NotNullConstraint nnInfo = null;\n      if (descTbl.isExt() || descTbl.isFormatted()) {\n        pkInfo = db.getPrimaryKeys(tbl.getDbName(), tbl.getTableName());\n        fkInfo = db.getForeignKeys(tbl.getDbName(), tbl.getTableName());\n        ukInfo = db.getUniqueConstraints(tbl.getDbName(), tbl.getTableName());\n        nnInfo = db.getNotNullConstraints(tbl.getDbName(), tbl.getTableName());\n      }\n      fixDecimalColumnTypeName(cols);\n      // In case the query is served by HiveServer2, don't pad it with spaces,\n      // as HiveServer2 output is consumed by JDBC/ODBC clients.\n      boolean isOutputPadded = !SessionState.get().isHiveServerQuery();\n      formatter.describeTable(outStream, colPath, tableName, tbl, part,\n          cols, descTbl.isFormatted(), descTbl.isExt(),\n          isOutputPadded, colStats,\n          pkInfo, fkInfo, ukInfo, nnInfo);\n\n      LOG.debug(\"DDLTask: written data for {}\", tableName);\n\n    } catch (SQLException e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, tableName);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n\n    return 0;\n  }"
        ],
        [
            "DDLTask::switchDatabase(Hive,SwitchDatabaseDesc)",
            "4560  \n4561  \n4562  \n4563  \n4564  \n4565  \n4566  \n4567  \n4568  \n4569  \n4570  \n4571  \n4572  \n4573  \n4574  \n4575  \n4576  \n4577  \n4578  \n4579  \n4580  \n4581  \n4582  \n4583 -\n4584 -\n4585  \n4586  \n4587  \n4588  \n4589  \n4590  \n4591  ",
            "  /**\n   * Switch to a different Database\n   * @param db\n   * @param switchDb\n   * @return Always returns 0\n   * @throws HiveException\n   */\n  private int switchDatabase(Hive db, SwitchDatabaseDesc switchDb)\n      throws HiveException {\n    String dbName = switchDb.getDatabaseName();\n    if (!db.databaseExists(dbName)) {\n      throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);\n    }\n    SessionState.get().setCurrentDatabase(dbName);\n\n    // set database specific parameters\n    Database database = db.getDatabase(dbName);\n    assert(database != null);\n    Map<String, String> dbParams = database.getParameters();\n    if (dbParams != null) {\n      for (HiveConf.ConfVars var: HiveConf.dbVars) {\n        String newValue = dbParams.get(var.varname);\n        if (newValue != null) {\n          LOG.info(\"Changing \" + var.varname +\n              \" from \" + conf.getVar(var) + \" to \" + newValue);\n          conf.setVar(var, newValue);\n        }\n      }\n    }\n\n    return 0;\n  }",
            "4562  \n4563  \n4564  \n4565  \n4566  \n4567  \n4568  \n4569  \n4570  \n4571  \n4572  \n4573  \n4574  \n4575  \n4576  \n4577  \n4578  \n4579  \n4580  \n4581  \n4582  \n4583  \n4584  \n4585 +\n4586 +\n4587  \n4588  \n4589  \n4590  \n4591  \n4592  \n4593  ",
            "  /**\n   * Switch to a different Database\n   * @param db\n   * @param switchDb\n   * @return Always returns 0\n   * @throws HiveException\n   */\n  private int switchDatabase(Hive db, SwitchDatabaseDesc switchDb)\n      throws HiveException {\n    String dbName = switchDb.getDatabaseName();\n    if (!db.databaseExists(dbName)) {\n      throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);\n    }\n    SessionState.get().setCurrentDatabase(dbName);\n\n    // set database specific parameters\n    Database database = db.getDatabase(dbName);\n    assert(database != null);\n    Map<String, String> dbParams = database.getParameters();\n    if (dbParams != null) {\n      for (HiveConf.ConfVars var: HiveConf.dbVars) {\n        String newValue = dbParams.get(var.varname);\n        if (newValue != null) {\n          LOG.info(\"Changing {} from {} to {}\", var.varname, conf.getVar(var),\n            newValue);\n          conf.setVar(var, newValue);\n        }\n      }\n    }\n\n    return 0;\n  }"
        ],
        [
            "DDLTask::createTable(Hive,CreateTableDesc)",
            "4593  \n4594  \n4595  \n4596  \n4597  \n4598  \n4599  \n4600  \n4601  \n4602  \n4603  \n4604  \n4605  \n4606  \n4607  \n4608  \n4609  \n4610  \n4611 -\n4612 -\n4613  \n4614  \n4615  \n4616  \n4617  \n4618  \n4619  \n4620  \n4621  \n4622  \n4623  \n4624  \n4625  \n4626  \n4627  \n4628  \n4629  \n4630  \n4631  \n4632  \n4633  \n4634  \n4635  \n4636  \n4637  \n4638  \n4639  \n4640  \n4641  \n4642  \n4643  \n4644  \n4645  \n4646  \n4647  \n4648  \n4649  \n4650  \n4651  \n4652  \n4653  \n4654  \n4655  \n4656  \n4657  \n4658  \n4659  \n4660  \n4661  \n4662  \n4663  ",
            "  /**\n   * Create a new table.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtTbl\n   *          This is the table we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {\n    // create the table\n    Table tbl = crtTbl.toTable(conf);\n    List<SQLPrimaryKey> primaryKeys = crtTbl.getPrimaryKeys();\n    List<SQLForeignKey> foreignKeys = crtTbl.getForeignKeys();\n    List<SQLUniqueConstraint> uniqueConstraints = crtTbl.getUniqueConstraints();\n    List<SQLNotNullConstraint> notNullConstraints = crtTbl.getNotNullConstraints();\n    LOG.info(\"creating table \" + tbl.getDbName() + \".\" + tbl.getTableName() + \" on \" +\n            tbl.getDataLocation());\n\n    if (crtTbl.getReplicationSpec().isInReplicationScope() && (!crtTbl.getReplaceMode())){\n      // if this is a replication spec, then replace-mode semantics might apply.\n      // if we're already asking for a table replacement, then we can skip this check.\n      // however, otherwise, if in replication scope, and we've not been explicitly asked\n      // to replace, we should check if the object we're looking at exists, and if so,\n      // trigger replace-mode semantics.\n      Table existingTable = db.getTable(tbl.getDbName(), tbl.getTableName(), false);\n      if (existingTable != null){\n        if (crtTbl.getReplicationSpec().allowEventReplacementInto(existingTable.getParameters())){\n          crtTbl.setReplaceMode(true); // we replace existing table.\n        } else {\n          LOG.debug(\"DDLTask: Create Table is skipped as table {} is newer than update\",\n                  crtTbl.getTableName());\n          return 0; // no replacement, the existing table state is newer than our update.\n        }\n      }\n    }\n\n    // create the table\n    if (crtTbl.getReplaceMode()) {\n      // replace-mode creates are really alters using CreateTableDesc.\n      try {\n        db.alterTable(tbl.getDbName()+\".\"+tbl.getTableName(),tbl,null);\n      } catch (InvalidOperationException e) {\n        throw new HiveException(\"Unable to alter table. \" + e.getMessage(), e);\n      }\n    } else {\n      if ((foreignKeys != null && foreignKeys.size() > 0 ) ||\n          (primaryKeys != null && primaryKeys.size() > 0) ||\n          (uniqueConstraints != null && uniqueConstraints.size() > 0) ||\n          (notNullConstraints != null && notNullConstraints.size() > 0)) {\n        db.createTable(tbl, crtTbl.getIfNotExists(), primaryKeys, foreignKeys,\n                uniqueConstraints, notNullConstraints);\n      } else {\n        db.createTable(tbl, crtTbl.getIfNotExists());\n      }\n      Long mmWriteId = crtTbl.getInitialMmWriteId();\n      if (crtTbl.isCTAS() || mmWriteId != null) {\n        Table createdTable = db.getTable(tbl.getDbName(), tbl.getTableName());\n        if (crtTbl.isCTAS()) {\n          DataContainer dc = new DataContainer(createdTable.getTTable());\n          SessionState.get().getLineageState().setLineage(\n                  createdTable.getPath(), dc, createdTable.getCols()\n          );\n        }\n      }\n    }\n    addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }",
            "4595  \n4596  \n4597  \n4598  \n4599  \n4600  \n4601  \n4602  \n4603  \n4604  \n4605  \n4606  \n4607  \n4608  \n4609  \n4610  \n4611  \n4612  \n4613 +\n4614 +\n4615  \n4616  \n4617  \n4618  \n4619  \n4620  \n4621  \n4622  \n4623  \n4624  \n4625  \n4626  \n4627  \n4628  \n4629  \n4630  \n4631  \n4632  \n4633  \n4634  \n4635  \n4636  \n4637  \n4638  \n4639  \n4640  \n4641  \n4642  \n4643  \n4644  \n4645  \n4646  \n4647  \n4648  \n4649  \n4650  \n4651  \n4652  \n4653  \n4654  \n4655  \n4656  \n4657  \n4658  \n4659  \n4660  \n4661  \n4662  \n4663  \n4664  \n4665  ",
            "  /**\n   * Create a new table.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtTbl\n   *          This is the table we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {\n    // create the table\n    Table tbl = crtTbl.toTable(conf);\n    List<SQLPrimaryKey> primaryKeys = crtTbl.getPrimaryKeys();\n    List<SQLForeignKey> foreignKeys = crtTbl.getForeignKeys();\n    List<SQLUniqueConstraint> uniqueConstraints = crtTbl.getUniqueConstraints();\n    List<SQLNotNullConstraint> notNullConstraints = crtTbl.getNotNullConstraints();\n    LOG.info(\"creating table {}.{} on {}\", tbl.getDbName(), tbl.getTableName(),\n      tbl.getDataLocation());\n\n    if (crtTbl.getReplicationSpec().isInReplicationScope() && (!crtTbl.getReplaceMode())){\n      // if this is a replication spec, then replace-mode semantics might apply.\n      // if we're already asking for a table replacement, then we can skip this check.\n      // however, otherwise, if in replication scope, and we've not been explicitly asked\n      // to replace, we should check if the object we're looking at exists, and if so,\n      // trigger replace-mode semantics.\n      Table existingTable = db.getTable(tbl.getDbName(), tbl.getTableName(), false);\n      if (existingTable != null){\n        if (crtTbl.getReplicationSpec().allowEventReplacementInto(existingTable.getParameters())){\n          crtTbl.setReplaceMode(true); // we replace existing table.\n        } else {\n          LOG.debug(\"DDLTask: Create Table is skipped as table {} is newer than update\",\n                  crtTbl.getTableName());\n          return 0; // no replacement, the existing table state is newer than our update.\n        }\n      }\n    }\n\n    // create the table\n    if (crtTbl.getReplaceMode()) {\n      // replace-mode creates are really alters using CreateTableDesc.\n      try {\n        db.alterTable(tbl.getDbName()+\".\"+tbl.getTableName(),tbl,null);\n      } catch (InvalidOperationException e) {\n        throw new HiveException(\"Unable to alter table. \" + e.getMessage(), e);\n      }\n    } else {\n      if ((foreignKeys != null && foreignKeys.size() > 0 ) ||\n          (primaryKeys != null && primaryKeys.size() > 0) ||\n          (uniqueConstraints != null && uniqueConstraints.size() > 0) ||\n          (notNullConstraints != null && notNullConstraints.size() > 0)) {\n        db.createTable(tbl, crtTbl.getIfNotExists(), primaryKeys, foreignKeys,\n                uniqueConstraints, notNullConstraints);\n      } else {\n        db.createTable(tbl, crtTbl.getIfNotExists());\n      }\n      Long mmWriteId = crtTbl.getInitialMmWriteId();\n      if (crtTbl.isCTAS() || mmWriteId != null) {\n        Table createdTable = db.getTable(tbl.getDbName(), tbl.getTableName());\n        if (crtTbl.isCTAS()) {\n          DataContainer dc = new DataContainer(createdTable.getTTable());\n          SessionState.get().getLineageState().setLineage(\n                  createdTable.getPath(), dc, createdTable.getCols()\n          );\n        }\n      }\n    }\n    addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }"
        ],
        [
            "DDLTask::showLocks(Hive,ShowLocksDesc)",
            "2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803 -\n2804  \n2805  \n2806 -\n2807  \n2808  \n2809  \n2810  \n2811  \n2812  \n2813  \n2814  ",
            "  /**\n   * Write a list of the current locks to a file.\n   * @param db\n   *\n   * @param showLocks\n   *          the locks we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showLocks(Hive db, ShowLocksDesc showLocks) throws HiveException {\n    Context ctx = driverContext.getCtx();\n    HiveTxnManager txnManager = ctx.getHiveTxnManager();\n    HiveLockManager lockMgr = txnManager.getLockManager();\n\n    if (txnManager.useNewShowLocksFormat()) {\n      return showLocksNewFormat(showLocks, lockMgr);\n    }\n\n    boolean isExt = showLocks.isExt();\n    if (lockMgr == null) {\n      throw new HiveException(\"show Locks LockManager not specified\");\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showLocks.getResFile());\n    try {\n      List<HiveLock> locks = null;\n\n      if (showLocks.getTableName() == null) {\n        // TODO should be doing security check here.  Users should not be\n        // able to see each other's locks.\n        locks = lockMgr.getLocks(false, isExt);\n      }\n      else {\n        locks = lockMgr.getLocks(HiveLockObject.createFrom(db,\n            showLocks.getTableName(), showLocks.getPartSpec()),\n            true, isExt);\n      }\n\n      Collections.sort(locks, new Comparator<HiveLock>() {\n\n        @Override\n        public int compare(HiveLock o1, HiveLock o2) {\n          int cmp = o1.getHiveLockObject().getName().compareTo(o2.getHiveLockObject().getName());\n          if (cmp == 0) {\n            if (o1.getHiveLockMode() == o2.getHiveLockMode()) {\n              return cmp;\n            }\n            // EXCLUSIVE locks occur before SHARED locks\n            if (o1.getHiveLockMode() == HiveLockMode.EXCLUSIVE) {\n              return -1;\n            }\n            return +1;\n          }\n          return cmp;\n        }\n\n      });\n\n      Iterator<HiveLock> locksIter = locks.iterator();\n\n      while (locksIter.hasNext()) {\n        HiveLock lock = locksIter.next();\n        outStream.writeBytes(lock.getHiveLockObject().getDisplayName());\n        outStream.write(separator);\n        outStream.writeBytes(lock.getHiveLockMode().toString());\n        if (isExt) {\n          HiveLockObjectData lockData = lock.getHiveLockObject().getData();\n          if (lockData != null) {\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_QUERYID:\" + lockData.getQueryId());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_TIME:\" + lockData.getLockTime());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_MODE:\" + lockData.getLockMode());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_QUERYSTRING:\" + lockData.getQueryStr());\n          }\n        }\n        outStream.write(terminator);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e.toString(), e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803  \n2804  \n2805 +\n2806  \n2807  \n2808 +\n2809  \n2810  \n2811  \n2812  \n2813  \n2814  \n2815  \n2816  ",
            "  /**\n   * Write a list of the current locks to a file.\n   * @param db\n   *\n   * @param showLocks\n   *          the locks we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showLocks(Hive db, ShowLocksDesc showLocks) throws HiveException {\n    Context ctx = driverContext.getCtx();\n    HiveTxnManager txnManager = ctx.getHiveTxnManager();\n    HiveLockManager lockMgr = txnManager.getLockManager();\n\n    if (txnManager.useNewShowLocksFormat()) {\n      return showLocksNewFormat(showLocks, lockMgr);\n    }\n\n    boolean isExt = showLocks.isExt();\n    if (lockMgr == null) {\n      throw new HiveException(\"show Locks LockManager not specified\");\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showLocks.getResFile());\n    try {\n      List<HiveLock> locks = null;\n\n      if (showLocks.getTableName() == null) {\n        // TODO should be doing security check here.  Users should not be\n        // able to see each other's locks.\n        locks = lockMgr.getLocks(false, isExt);\n      }\n      else {\n        locks = lockMgr.getLocks(HiveLockObject.createFrom(db,\n            showLocks.getTableName(), showLocks.getPartSpec()),\n            true, isExt);\n      }\n\n      Collections.sort(locks, new Comparator<HiveLock>() {\n\n        @Override\n        public int compare(HiveLock o1, HiveLock o2) {\n          int cmp = o1.getHiveLockObject().getName().compareTo(o2.getHiveLockObject().getName());\n          if (cmp == 0) {\n            if (o1.getHiveLockMode() == o2.getHiveLockMode()) {\n              return cmp;\n            }\n            // EXCLUSIVE locks occur before SHARED locks\n            if (o1.getHiveLockMode() == HiveLockMode.EXCLUSIVE) {\n              return -1;\n            }\n            return +1;\n          }\n          return cmp;\n        }\n\n      });\n\n      Iterator<HiveLock> locksIter = locks.iterator();\n\n      while (locksIter.hasNext()) {\n        HiveLock lock = locksIter.next();\n        outStream.writeBytes(lock.getHiveLockObject().getDisplayName());\n        outStream.write(separator);\n        outStream.writeBytes(lock.getHiveLockMode().toString());\n        if (isExt) {\n          HiveLockObjectData lockData = lock.getHiveLockObject().getData();\n          if (lockData != null) {\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_QUERYID:\" + lockData.getQueryId());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_TIME:\" + lockData.getLockTime());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_MODE:\" + lockData.getLockMode());\n            outStream.write(terminator);\n            outStream.writeBytes(\"LOCK_QUERYSTRING:\" + lockData.getQueryStr());\n          }\n        }\n        outStream.write(terminator);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e.toString(), e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "StatsNoJobTask::StatsCollection::run()",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 -\n 189  \n 190  \n 191  \n 192  \n 193 -\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  ",
            "    @Override\n    public void run() {\n\n      // get the list of partitions\n      org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n      Map<String, String> parameters = tPart.getParameters();\n\n      try {\n        Path dir = new Path(tPart.getSd().getLocation());\n        long numRows = 0;\n        long rawDataSize = 0;\n        long fileSize = 0;\n        long numFiles = 0;\n        FileSystem fs = dir.getFileSystem(conf);\n        FileStatus[] fileList = HiveStatsUtils.getFileStatusRecurse(dir, -1, fs);\n\n        boolean statsAvailable = false;\n        for(FileStatus file: fileList) {\n          if (!file.isDir()) {\n            InputFormat<?, ?> inputFormat = ReflectionUtil.newInstance(\n                partn.getInputFormatClass(), jc);\n            InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0,\n                new String[] { partn.getLocation() });\n            org.apache.hadoop.mapred.RecordReader<?, ?> recordReader =\n                inputFormat.getRecordReader(dummySplit, jc, Reporter.NULL);\n            StatsProvidingRecordReader statsRR;\n            if (recordReader instanceof StatsProvidingRecordReader) {\n              statsRR = (StatsProvidingRecordReader) recordReader;\n              rawDataSize += statsRR.getStats().getRawDataSize();\n              numRows += statsRR.getStats().getRowCount();\n              fileSize += file.getLen();\n              numFiles += 1;\n              statsAvailable = true;\n            }\n            recordReader.close();\n          }\n        }\n\n        if (statsAvailable) {\n          parameters.put(StatsSetupConst.ROW_COUNT, String.valueOf(numRows));\n          parameters.put(StatsSetupConst.RAW_DATA_SIZE, String.valueOf(rawDataSize));\n          parameters.put(StatsSetupConst.TOTAL_SIZE, String.valueOf(fileSize));\n          parameters.put(StatsSetupConst.NUM_FILES, String.valueOf(numFiles));\n\n          partUpdates.put(tPart.getSd().getLocation(), new Partition(table, tPart));\n\n          // printout console and debug logs\n          String threadName = Thread.currentThread().getName();\n          String msg = \"Partition \" + tableFullName + partn.getSpec() + \" stats: [\"\n              + toString(parameters) + ']';\n          LOG.debug(threadName + \": \" + msg);\n          console.printInfo(msg);\n        } else {\n          String threadName = Thread.currentThread().getName();\n          String msg = \"Partition \" + tableFullName + partn.getSpec() + \" does not provide stats.\";\n          LOG.debug(threadName + \": \" + msg);\n        }\n      } catch (Exception e) {\n        console.printInfo(\"[Warning] could not update stats for \" + tableFullName + partn.getSpec()\n            + \".\",\n            \"Failed with exception \" + e.getMessage() + \"\\n\" + StringUtils.stringifyException(e));\n\n        // Before updating the partition params, if any partition params is null\n        // and if statsReliable is true then updatePartition() function  will fail\n        // the task by returning 1\n        if (work.isStatsReliable()) {\n          partUpdates.put(tPart.getSd().getLocation(), null);\n        }\n      }\n    }",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  ",
            "    @Override\n    public void run() {\n\n      // get the list of partitions\n      org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n      Map<String, String> parameters = tPart.getParameters();\n\n      try {\n        Path dir = new Path(tPart.getSd().getLocation());\n        long numRows = 0;\n        long rawDataSize = 0;\n        long fileSize = 0;\n        long numFiles = 0;\n        FileSystem fs = dir.getFileSystem(conf);\n        FileStatus[] fileList = HiveStatsUtils.getFileStatusRecurse(dir, -1, fs);\n\n        boolean statsAvailable = false;\n        for(FileStatus file: fileList) {\n          if (!file.isDir()) {\n            InputFormat<?, ?> inputFormat = ReflectionUtil.newInstance(\n                partn.getInputFormatClass(), jc);\n            InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0,\n                new String[] { partn.getLocation() });\n            org.apache.hadoop.mapred.RecordReader<?, ?> recordReader =\n                inputFormat.getRecordReader(dummySplit, jc, Reporter.NULL);\n            StatsProvidingRecordReader statsRR;\n            if (recordReader instanceof StatsProvidingRecordReader) {\n              statsRR = (StatsProvidingRecordReader) recordReader;\n              rawDataSize += statsRR.getStats().getRawDataSize();\n              numRows += statsRR.getStats().getRowCount();\n              fileSize += file.getLen();\n              numFiles += 1;\n              statsAvailable = true;\n            }\n            recordReader.close();\n          }\n        }\n\n        if (statsAvailable) {\n          parameters.put(StatsSetupConst.ROW_COUNT, String.valueOf(numRows));\n          parameters.put(StatsSetupConst.RAW_DATA_SIZE, String.valueOf(rawDataSize));\n          parameters.put(StatsSetupConst.TOTAL_SIZE, String.valueOf(fileSize));\n          parameters.put(StatsSetupConst.NUM_FILES, String.valueOf(numFiles));\n\n          partUpdates.put(tPart.getSd().getLocation(), new Partition(table, tPart));\n\n          // printout console and debug logs\n          String threadName = Thread.currentThread().getName();\n          String msg = \"Partition \" + tableFullName + partn.getSpec() + \" stats: [\"\n              + toString(parameters) + ']';\n          LOG.debug(\"{}: {}\", threadName, msg);\n          console.printInfo(msg);\n        } else {\n          String threadName = Thread.currentThread().getName();\n          String msg = \"Partition \" + tableFullName + partn.getSpec() + \" does not provide stats.\";\n          LOG.debug(\"{}: {}\", threadName, msg);\n        }\n      } catch (Exception e) {\n        console.printInfo(\"[Warning] could not update stats for \" + tableFullName + partn.getSpec()\n            + \".\",\n            \"Failed with exception \" + e.getMessage() + \"\\n\" + StringUtils.stringifyException(e));\n\n        // Before updating the partition params, if any partition params is null\n        // and if statsReliable is true then updatePartition() function  will fail\n        // the task by returning 1\n        if (work.isStatsReliable()) {\n          partUpdates.put(tPart.getSd().getLocation(), null);\n        }\n      }\n    }"
        ],
        [
            "MoveTask::moveFileInDfs(Path,Path,HiveConf)",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142 -\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  private void moveFileInDfs (Path sourcePath, Path targetPath, HiveConf conf)\n      throws HiveException, IOException {\n\n    final FileSystem srcFs, tgtFs;\n    try {\n      tgtFs = targetPath.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get dest fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n    try {\n      srcFs = sourcePath.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get src fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n\n    // if source exists, rename. Otherwise, create a empty directory\n    if (srcFs.exists(sourcePath)) {\n      Path deletePath = null;\n      // If it multiple level of folder are there fs.rename is failing so first\n      // create the targetpath.getParent() if it not exist\n      if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_INSERT_INTO_MULTILEVEL_DIRS)) {\n        deletePath = createTargetPath(targetPath, tgtFs);\n      }\n      Hive.clearDestForSubDirSrc(conf, targetPath, sourcePath, false);\n      if (!Hive.moveFile(conf, sourcePath, targetPath, true, false)) {\n        try {\n          if (deletePath != null) {\n            tgtFs.delete(deletePath, true);\n          }\n        } catch (IOException e) {\n          LOG.info(\"Unable to delete the path created for facilitating rename\"\n              + deletePath);\n        }\n        throw new HiveException(\"Unable to rename: \" + sourcePath\n            + \" to: \" + targetPath);\n      }\n    } else if (!tgtFs.mkdirs(targetPath)) {\n      throw new HiveException(\"Unable to make directory: \" + targetPath);\n    }\n  }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  private void moveFileInDfs (Path sourcePath, Path targetPath, HiveConf conf)\n      throws HiveException, IOException {\n\n    final FileSystem srcFs, tgtFs;\n    try {\n      tgtFs = targetPath.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get dest fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n    try {\n      srcFs = sourcePath.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get src fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n\n    // if source exists, rename. Otherwise, create a empty directory\n    if (srcFs.exists(sourcePath)) {\n      Path deletePath = null;\n      // If it multiple level of folder are there fs.rename is failing so first\n      // create the targetpath.getParent() if it not exist\n      if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_INSERT_INTO_MULTILEVEL_DIRS)) {\n        deletePath = createTargetPath(targetPath, tgtFs);\n      }\n      Hive.clearDestForSubDirSrc(conf, targetPath, sourcePath, false);\n      if (!Hive.moveFile(conf, sourcePath, targetPath, true, false)) {\n        try {\n          if (deletePath != null) {\n            tgtFs.delete(deletePath, true);\n          }\n        } catch (IOException e) {\n          LOG.info(\"Unable to delete the path created for facilitating rename: {}\",\n            deletePath);\n        }\n        throw new HiveException(\"Unable to rename: \" + sourcePath\n            + \" to: \" + targetPath);\n      }\n    } else if (!tgtFs.mkdirs(targetPath)) {\n      throw new HiveException(\"Unable to make directory: \" + targetPath);\n    }\n  }"
        ],
        [
            "DDLTask::validateSerDe(String,HiveConf)",
            "4494  \n4495  \n4496  \n4497  \n4498  \n4499  \n4500  \n4501  \n4502  \n4503 -\n4504  \n4505  \n4506  \n4507  \n4508  ",
            "  /**\n   * Check if the given serde is valid.\n   */\n  public static void validateSerDe(String serdeName, HiveConf conf) throws HiveException {\n    try {\n\n      Deserializer d = ReflectionUtil.newInstance(conf.getClassByName(serdeName).\n          asSubclass(Deserializer.class), conf);\n      if (d != null) {\n        LOG.debug(\"Found class for \" + serdeName);\n      }\n    } catch (Exception e) {\n      throw new HiveException(\"Cannot validate serde: \" + serdeName, e);\n    }\n  }",
            "4496  \n4497  \n4498  \n4499  \n4500  \n4501  \n4502  \n4503  \n4504  \n4505 +\n4506  \n4507  \n4508  \n4509  \n4510  ",
            "  /**\n   * Check if the given serde is valid.\n   */\n  public static void validateSerDe(String serdeName, HiveConf conf) throws HiveException {\n    try {\n\n      Deserializer d = ReflectionUtil.newInstance(conf.getClassByName(serdeName).\n          asSubclass(Deserializer.class), conf);\n      if (d != null) {\n        LOG.debug(\"Found class for {}\", serdeName);\n      }\n    } catch (Exception e) {\n      throw new HiveException(\"Cannot validate serde: \" + serdeName, e);\n    }\n  }"
        ],
        [
            "DDLTask::showFunctions(Hive,ShowFunctionsDesc)",
            "2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681 -\n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688 -\n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707 -\n2708  \n2709  \n2710 -\n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  ",
            "  /**\n   * Write a list of the user defined functions to a file.\n   * @param db\n   *\n   * @param showFuncs\n   *          are the functions we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showFunctions(Hive db, ShowFunctionsDesc showFuncs) throws HiveException {\n    // get the tables for the desired patten - populate the output stream\n    Set<String> funcs = null;\n    if (showFuncs.getPattern() != null) {\n      LOG.info(\"pattern: \" + showFuncs.getPattern());\n      if (showFuncs.getIsLikePattern()) {\n         funcs = FunctionRegistry.getFunctionNamesByLikePattern(showFuncs.getPattern());\n      } else {\n         console.printInfo(\"SHOW FUNCTIONS is deprecated, please use SHOW FUNCTIONS LIKE instead.\");\n         funcs = FunctionRegistry.getFunctionNames(showFuncs.getPattern());\n      }\n      LOG.info(\"results : \" + funcs.size());\n    } else {\n      funcs = FunctionRegistry.getFunctionNames();\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showFuncs.getResFile());\n    try {\n      SortedSet<String> sortedFuncs = new TreeSet<String>(funcs);\n      // To remove the primitive types\n      sortedFuncs.removeAll(serdeConstants.PrimitiveTypes);\n      Iterator<String> iterFuncs = sortedFuncs.iterator();\n\n      while (iterFuncs.hasNext()) {\n        // create a row per table name\n        outStream.writeBytes(iterFuncs.next());\n        outStream.write(terminator);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683 +\n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690 +\n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709 +\n2710  \n2711  \n2712 +\n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  ",
            "  /**\n   * Write a list of the user defined functions to a file.\n   * @param db\n   *\n   * @param showFuncs\n   *          are the functions we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showFunctions(Hive db, ShowFunctionsDesc showFuncs) throws HiveException {\n    // get the tables for the desired patten - populate the output stream\n    Set<String> funcs = null;\n    if (showFuncs.getPattern() != null) {\n      LOG.info(\"pattern: {}\", showFuncs.getPattern());\n      if (showFuncs.getIsLikePattern()) {\n         funcs = FunctionRegistry.getFunctionNamesByLikePattern(showFuncs.getPattern());\n      } else {\n         console.printInfo(\"SHOW FUNCTIONS is deprecated, please use SHOW FUNCTIONS LIKE instead.\");\n         funcs = FunctionRegistry.getFunctionNames(showFuncs.getPattern());\n      }\n      LOG.info(\"results : {}\", funcs.size());\n    } else {\n      funcs = FunctionRegistry.getFunctionNames();\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showFuncs.getResFile());\n    try {\n      SortedSet<String> sortedFuncs = new TreeSet<String>(funcs);\n      // To remove the primitive types\n      sortedFuncs.removeAll(serdeConstants.PrimitiveTypes);\n      Iterator<String> iterFuncs = sortedFuncs.iterator();\n\n      while (iterFuncs.hasNext()) {\n        // create a row per table name\n        outStream.writeBytes(iterFuncs.next());\n        outStream.write(terminator);\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "StatsNoJobTask::execute(DriverContext)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    LOG.info(\"Executing stats (no job) task\");\n\n    String tableName = \"\";\n    ExecutorService threadPool = null;\n    Hive db = getHive();\n    try {\n      tableName = work.getTableSpecs().tableName;\n      table = db.getTable(tableName);\n      int numThreads = HiveConf.getIntVar(conf, ConfVars.HIVE_STATS_GATHER_NUM_THREADS);\n      tableFullName = table.getDbName() + \".\" + table.getTableName();\n      threadPool = Executors.newFixedThreadPool(numThreads,\n          new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"StatsNoJobTask-Thread-%d\")\n              .build());\n      partUpdates = new MapMaker().concurrencyLevel(numThreads).makeMap();\n      LOG.info(\"Initialized threadpool for stats computation with \" + numThreads + \" threads\");\n    } catch (HiveException e) {\n      LOG.error(\"Cannot get table \" + tableName, e);\n      console.printError(\"Cannot get table \" + tableName, e.toString());\n    }\n\n    return aggregateStats(threadPool, db);\n  }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    LOG.info(\"Executing stats (no job) task\");\n\n    String tableName = \"\";\n    ExecutorService threadPool = null;\n    Hive db = getHive();\n    try {\n      tableName = work.getTableSpecs().tableName;\n      table = db.getTable(tableName);\n      int numThreads = HiveConf.getIntVar(conf, ConfVars.HIVE_STATS_GATHER_NUM_THREADS);\n      tableFullName = table.getDbName() + \".\" + table.getTableName();\n      threadPool = Executors.newFixedThreadPool(numThreads,\n          new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"StatsNoJobTask-Thread-%d\")\n              .build());\n      partUpdates = new MapMaker().concurrencyLevel(numThreads).makeMap();\n      LOG.info(\"Initialized threadpool for stats computation with {} threads\", numThreads);\n    } catch (HiveException e) {\n      LOG.error(\"Cannot get table {}\", tableName, e);\n      console.printError(\"Cannot get table \" + tableName, e.toString());\n    }\n\n    return aggregateStats(threadPool, db);\n  }"
        ],
        [
            "DDLTask::showIndexes(Hive,ShowIndexesDesc)",
            "2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548 -\n2549  \n2550  \n2551 -\n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  ",
            "  /**\n   * Write a list of indexes to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showIndexes\n   *          These are the indexes we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showIndexes(Hive db, ShowIndexesDesc showIndexes) throws HiveException {\n    // get the indexes for the table and populate the output\n    String tableName = showIndexes.getTableName();\n    Table tbl = null;\n    List<Index> indexes = null;\n\n    tbl = db.getTable(tableName);\n\n    indexes = db.getIndexes(tbl.getDbName(), tbl.getTableName(), (short) -1);\n\n    // In case the query is served by HiveServer2, don't pad it with spaces,\n    // as HiveServer2 output is consumed by JDBC/ODBC clients.\n    boolean isOutputPadded = !SessionState.get().isHiveServerQuery();\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showIndexes.getResFile());\n    try {\n      if (showIndexes.isFormatted()) {\n        // column headers\n        outStream.write(MetaDataFormatUtils.getIndexColumnsHeader().getBytes(StandardCharsets.UTF_8));\n      }\n\n      for (Index index : indexes)\n      {\n        outStream.write(MetaDataFormatUtils.getIndexInformation(index, isOutputPadded).getBytes(StandardCharsets.UTF_8));\n      }\n    } catch (FileNotFoundException e) {\n      LOG.info(\"show indexes: \" + stringifyException(e));\n      throw new HiveException(e.toString());\n    } catch (IOException e) {\n      LOG.info(\"show indexes: \" + stringifyException(e));\n      throw new HiveException(e.toString());\n    } catch (Exception e) {\n      throw new HiveException(e.toString());\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n\n    return 0;\n  }",
            "2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550 +\n2551  \n2552  \n2553 +\n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  ",
            "  /**\n   * Write a list of indexes to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showIndexes\n   *          These are the indexes we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showIndexes(Hive db, ShowIndexesDesc showIndexes) throws HiveException {\n    // get the indexes for the table and populate the output\n    String tableName = showIndexes.getTableName();\n    Table tbl = null;\n    List<Index> indexes = null;\n\n    tbl = db.getTable(tableName);\n\n    indexes = db.getIndexes(tbl.getDbName(), tbl.getTableName(), (short) -1);\n\n    // In case the query is served by HiveServer2, don't pad it with spaces,\n    // as HiveServer2 output is consumed by JDBC/ODBC clients.\n    boolean isOutputPadded = !SessionState.get().isHiveServerQuery();\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showIndexes.getResFile());\n    try {\n      if (showIndexes.isFormatted()) {\n        // column headers\n        outStream.write(MetaDataFormatUtils.getIndexColumnsHeader().getBytes(StandardCharsets.UTF_8));\n      }\n\n      for (Index index : indexes)\n      {\n        outStream.write(MetaDataFormatUtils.getIndexInformation(index, isOutputPadded).getBytes(StandardCharsets.UTF_8));\n      }\n    } catch (FileNotFoundException e) {\n      LOG.info(\"show indexes: \", e);\n      throw new HiveException(e.toString());\n    } catch (IOException e) {\n      LOG.info(\"show indexes: \", e);\n      throw new HiveException(e.toString());\n    } catch (Exception e) {\n      throw new HiveException(e.toString());\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n\n    return 0;\n  }"
        ],
        [
            "FunctionTask::dropMacro(DropMacroDesc)",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242  \n 243  ",
            "  private int dropMacro(DropMacroDesc dropMacroDesc) {\n    try {\n      FunctionRegistry.unregisterTemporaryUDF(dropMacroDesc.getMacroName());\n      return 0;\n    } catch (HiveException e) {\n      LOG.info(\"drop macro: \" + StringUtils.stringifyException(e));\n      return 1;\n    }\n  }",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242  \n 243  ",
            "  private int dropMacro(DropMacroDesc dropMacroDesc) {\n    try {\n      FunctionRegistry.unregisterTemporaryUDF(dropMacroDesc.getMacroName());\n      return 0;\n    } catch (HiveException e) {\n      LOG.info(\"drop macro: \", e);\n      return 1;\n    }\n  }"
        ],
        [
            "DDLTask::addIfAbsentByName(WriteEntity,Set)",
            "3725  \n3726  \n3727  \n3728  \n3729  \n3730  \n3731  \n3732  \n3733  \n3734  \n3735  \n3736  \n3737  \n3738  \n3739  \n3740  \n3741  \n3742  \n3743  \n3744  \n3745  \n3746 -\n3747 -\n3748  \n3749  \n3750  \n3751  \n3752  \n3753  ",
            "  /**\n   * There are many places where \"duplicate\" Read/WriteEnity objects are added.  The way this was\n   * initially implemented, the duplicate just replaced the previous object.\n   * (work.getOutputs() is a Set and WriteEntity#equals() relies on name)\n   * This may be benign for ReadEntity and perhaps was benign for WriteEntity before WriteType was\n   * added. Now that WriteEntity has a WriteType it replaces it with one with possibly different\n   * {@link org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteType}.  It's hard to imagine\n   * how this is desirable.\n   *\n   * As of HIVE-14993, WriteEntity with different WriteType must be considered different.\n   * So WriteEntity create in DDLTask cause extra output in golden files, but only because\n   * DDLTask sets a different WriteType for the same Entity.\n   *\n   * In the spirit of bug-for-bug compatibility, this method ensures we only add new\n   * WriteEntity if it's really new.\n   *\n   * @return {@code true} if item was added\n   */\n  static boolean addIfAbsentByName(WriteEntity newWriteEntity, Set<WriteEntity> outputs) {\n    for(WriteEntity writeEntity : outputs) {\n      if(writeEntity.getName().equalsIgnoreCase(newWriteEntity.getName())) {\n        LOG.debug(\"Ignoring request to add \" + newWriteEntity.toStringDetail() + \" because \" +\n          writeEntity.toStringDetail() + \" is present\");\n        return false;\n      }\n    }\n    outputs.add(newWriteEntity);\n    return true;\n  }",
            "3727  \n3728  \n3729  \n3730  \n3731  \n3732  \n3733  \n3734  \n3735  \n3736  \n3737  \n3738  \n3739  \n3740  \n3741  \n3742  \n3743  \n3744  \n3745  \n3746  \n3747  \n3748 +\n3749 +\n3750  \n3751  \n3752  \n3753  \n3754  \n3755  ",
            "  /**\n   * There are many places where \"duplicate\" Read/WriteEnity objects are added.  The way this was\n   * initially implemented, the duplicate just replaced the previous object.\n   * (work.getOutputs() is a Set and WriteEntity#equals() relies on name)\n   * This may be benign for ReadEntity and perhaps was benign for WriteEntity before WriteType was\n   * added. Now that WriteEntity has a WriteType it replaces it with one with possibly different\n   * {@link org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteType}.  It's hard to imagine\n   * how this is desirable.\n   *\n   * As of HIVE-14993, WriteEntity with different WriteType must be considered different.\n   * So WriteEntity create in DDLTask cause extra output in golden files, but only because\n   * DDLTask sets a different WriteType for the same Entity.\n   *\n   * In the spirit of bug-for-bug compatibility, this method ensures we only add new\n   * WriteEntity if it's really new.\n   *\n   * @return {@code true} if item was added\n   */\n  static boolean addIfAbsentByName(WriteEntity newWriteEntity, Set<WriteEntity> outputs) {\n    for(WriteEntity writeEntity : outputs) {\n      if(writeEntity.getName().equalsIgnoreCase(newWriteEntity.getName())) {\n        LOG.debug(\"Ignoring request to add {} because {} is present\",\n          newWriteEntity.toStringDetail(), writeEntity.toStringDetail());\n        return false;\n      }\n    }\n    outputs.add(newWriteEntity);\n    return true;\n  }"
        ],
        [
            "DDLTask::describeFunction(Hive,DescFunctionDesc)",
            "3124  \n3125  \n3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185 -\n3186  \n3187  \n3188 -\n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  ",
            "  /**\n   * Shows a description of a function.\n   * @param db\n   *\n   * @param descFunc\n   *          is the function we are describing\n   * @throws HiveException\n   */\n  private int describeFunction(Hive db, DescFunctionDesc descFunc) throws HiveException, SQLException {\n    String funcName = descFunc.getName();\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(descFunc.getResFile());\n    try {\n      // get the function documentation\n      Description desc = null;\n      Class<?> funcClass = null;\n      FunctionInfo functionInfo = FunctionRegistry.getFunctionInfo(funcName);\n      if (functionInfo != null) {\n        funcClass = functionInfo.getFunctionClass();\n      }\n      if (funcClass != null) {\n        desc = AnnotationUtils.getAnnotation(funcClass, Description.class);\n      }\n      if (desc != null) {\n        outStream.writeBytes(desc.value().replace(\"_FUNC_\", funcName));\n        if (descFunc.isExtended()) {\n          Set<String> synonyms = FunctionRegistry.getFunctionSynonyms(funcName);\n          if (synonyms.size() > 0) {\n            outStream.writeBytes(\"\\nSynonyms: \" + join(synonyms, \", \"));\n          }\n          if (desc.extended().length() > 0) {\n            outStream.writeBytes(\"\\n\"\n                + desc.extended().replace(\"_FUNC_\", funcName));\n          }\n        }\n      } else {\n        if (funcClass != null) {\n          outStream.writeBytes(\"There is no documentation for function '\"\n              + funcName + \"'\");\n        } else {\n          outStream.writeBytes(\"Function '\" + funcName + \"' does not exist.\");\n        }\n      }\n\n      outStream.write(terminator);\n      if (descFunc.isExtended()) {\n        if (funcClass != null) {\n          outStream.writeBytes(\"Function class:\" + funcClass.getName() + \"\\n\");\n        }\n        if (functionInfo != null) {\n          outStream.writeBytes(\"Function type:\" + functionInfo.getFunctionType() + \"\\n\");\n          FunctionResource[] resources = functionInfo.getResources();\n          if (resources != null) {\n            for (FunctionResource resource : resources) {\n              outStream.writeBytes(\"Resource:\" + resource.getResourceURI() + \"\\n\");\n            }\n          }\n        }\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"describe function: \" + stringifyException(e));\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"describe function: \" + stringifyException(e));\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187 +\n3188  \n3189  \n3190 +\n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  ",
            "  /**\n   * Shows a description of a function.\n   * @param db\n   *\n   * @param descFunc\n   *          is the function we are describing\n   * @throws HiveException\n   */\n  private int describeFunction(Hive db, DescFunctionDesc descFunc) throws HiveException, SQLException {\n    String funcName = descFunc.getName();\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(descFunc.getResFile());\n    try {\n      // get the function documentation\n      Description desc = null;\n      Class<?> funcClass = null;\n      FunctionInfo functionInfo = FunctionRegistry.getFunctionInfo(funcName);\n      if (functionInfo != null) {\n        funcClass = functionInfo.getFunctionClass();\n      }\n      if (funcClass != null) {\n        desc = AnnotationUtils.getAnnotation(funcClass, Description.class);\n      }\n      if (desc != null) {\n        outStream.writeBytes(desc.value().replace(\"_FUNC_\", funcName));\n        if (descFunc.isExtended()) {\n          Set<String> synonyms = FunctionRegistry.getFunctionSynonyms(funcName);\n          if (synonyms.size() > 0) {\n            outStream.writeBytes(\"\\nSynonyms: \" + join(synonyms, \", \"));\n          }\n          if (desc.extended().length() > 0) {\n            outStream.writeBytes(\"\\n\"\n                + desc.extended().replace(\"_FUNC_\", funcName));\n          }\n        }\n      } else {\n        if (funcClass != null) {\n          outStream.writeBytes(\"There is no documentation for function '\"\n              + funcName + \"'\");\n        } else {\n          outStream.writeBytes(\"Function '\" + funcName + \"' does not exist.\");\n        }\n      }\n\n      outStream.write(terminator);\n      if (descFunc.isExtended()) {\n        if (funcClass != null) {\n          outStream.writeBytes(\"Function class:\" + funcClass.getName() + \"\\n\");\n        }\n        if (functionInfo != null) {\n          outStream.writeBytes(\"Function type:\" + functionInfo.getFunctionType() + \"\\n\");\n          FunctionResource[] resources = functionInfo.getResources();\n          if (resources != null) {\n            for (FunctionResource resource : resources) {\n              outStream.writeBytes(\"Resource:\" + resource.getResourceURI() + \"\\n\");\n            }\n          }\n        }\n      }\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"describe function: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"describe function: \", e);\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::truncateTable(Hive,TruncateTableDesc)",
            "4950  \n4951  \n4952  \n4953  \n4954  \n4955  \n4956  \n4957  \n4958  \n4959  \n4960  \n4961  \n4962  \n4963  \n4964  \n4965  \n4966  \n4967  \n4968  \n4969  \n4970  \n4971  \n4972  \n4973  \n4974  \n4975  \n4976  \n4977 -\n4978 -\n4979 -\n4980  \n4981  \n4982  \n4983  \n4984  \n4985  \n4986  \n4987  \n4988  \n4989  ",
            "  private int truncateTable(Hive db, TruncateTableDesc truncateTableDesc) throws HiveException {\n\n    if (truncateTableDesc.getColumnIndexes() != null) {\n      ColumnTruncateWork truncateWork = new ColumnTruncateWork(\n          truncateTableDesc.getColumnIndexes(), truncateTableDesc.getInputDir(),\n          truncateTableDesc.getOutputDir());\n      truncateWork.setListBucketingCtx(truncateTableDesc.getLbCtx());\n      truncateWork.setMapperCannotSpanPartns(true);\n      DriverContext driverCxt = new DriverContext();\n      ColumnTruncateTask taskExec = new ColumnTruncateTask();\n      taskExec.initialize(queryState, null, driverCxt, null);\n      taskExec.setWork(truncateWork);\n      taskExec.setQueryPlan(this.getQueryPlan());\n      subtask = taskExec;\n      int ret = taskExec.execute(driverCxt);\n      if (subtask.getException() != null) {\n        setException(subtask.getException());\n      }\n      return ret;\n    }\n\n    String tableName = truncateTableDesc.getTableName();\n    Map<String, String> partSpec = truncateTableDesc.getPartSpec();\n\n    if (!allowOperationInReplicationScope(db, tableName, partSpec, truncateTableDesc.getReplicationSpec())) {\n      // no truncate, the table is missing either due to drop/rename which follows the truncate.\n      // or the existing table is newer than our update.\n      LOG.debug(\"DDLTask: Truncate Table/Partition is skipped as table {} / partition {} is newer than update\",\n              tableName,\n              (partSpec == null) ? \"null\" : FileUtils.makePartName(new ArrayList(partSpec.keySet()), new ArrayList(partSpec.values())));\n      return 0;\n    }\n\n    try {\n      db.truncateTable(tableName, partSpec);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR);\n    }\n    return 0;\n  }",
            "4952  \n4953  \n4954  \n4955  \n4956  \n4957  \n4958  \n4959  \n4960  \n4961  \n4962  \n4963  \n4964  \n4965  \n4966  \n4967  \n4968  \n4969  \n4970  \n4971  \n4972  \n4973  \n4974  \n4975  \n4976  \n4977  \n4978  \n4979 +\n4980 +\n4981 +\n4982 +\n4983 +\n4984  \n4985  \n4986  \n4987  \n4988  \n4989  \n4990  \n4991  \n4992  \n4993  ",
            "  private int truncateTable(Hive db, TruncateTableDesc truncateTableDesc) throws HiveException {\n\n    if (truncateTableDesc.getColumnIndexes() != null) {\n      ColumnTruncateWork truncateWork = new ColumnTruncateWork(\n          truncateTableDesc.getColumnIndexes(), truncateTableDesc.getInputDir(),\n          truncateTableDesc.getOutputDir());\n      truncateWork.setListBucketingCtx(truncateTableDesc.getLbCtx());\n      truncateWork.setMapperCannotSpanPartns(true);\n      DriverContext driverCxt = new DriverContext();\n      ColumnTruncateTask taskExec = new ColumnTruncateTask();\n      taskExec.initialize(queryState, null, driverCxt, null);\n      taskExec.setWork(truncateWork);\n      taskExec.setQueryPlan(this.getQueryPlan());\n      subtask = taskExec;\n      int ret = taskExec.execute(driverCxt);\n      if (subtask.getException() != null) {\n        setException(subtask.getException());\n      }\n      return ret;\n    }\n\n    String tableName = truncateTableDesc.getTableName();\n    Map<String, String> partSpec = truncateTableDesc.getPartSpec();\n\n    if (!allowOperationInReplicationScope(db, tableName, partSpec, truncateTableDesc.getReplicationSpec())) {\n      // no truncate, the table is missing either due to drop/rename which follows the truncate.\n      // or the existing table is newer than our update.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"DDLTask: Truncate Table/Partition is skipped as table {} / partition {} is newer than update\",\n          tableName,\n          (partSpec == null) ? \"null\" : FileUtils.makePartName(new ArrayList<>(partSpec.keySet()), new ArrayList<>(partSpec.values())));\n      }\n      return 0;\n    }\n\n    try {\n      db.truncateTable(tableName, partSpec);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR);\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::alterIndex(Hive,AlterIndexDesc)",
            "1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128 -\n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  ",
            "  private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException {\n\n    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n      throw new UnsupportedOperationException(\"Indexes unsupported for Tez execution engine\");\n    }\n\n    String baseTableName = alterIndex.getBaseTableName();\n    String indexName = alterIndex.getIndexName();\n    Index idx = db.getIndex(baseTableName, indexName);\n\n    switch(alterIndex.getOp()) {\n    case ADDPROPS:\n      idx.getParameters().putAll(alterIndex.getProps());\n      break;\n    case UPDATETIMESTAMP:\n      try {\n        Map<String, String> props = new HashMap<String, String>();\n        Map<Map<String, String>, Long> basePartTs = new HashMap<Map<String, String>, Long>();\n\n        Table baseTbl = db.getTable(baseTableName);\n\n        if (baseTbl.isPartitioned()) {\n          List<Partition> baseParts;\n          if (alterIndex.getSpec() != null) {\n            baseParts = db.getPartitions(baseTbl, alterIndex.getSpec());\n          } else {\n            baseParts = db.getPartitions(baseTbl);\n          }\n          if (baseParts != null) {\n            for (Partition p : baseParts) {\n              Path dataLocation = p.getDataLocation();\n              FileSystem fs = dataLocation.getFileSystem(db.getConf());\n              FileStatus fss = fs.getFileStatus(dataLocation);\n              long lastModificationTime = fss.getModificationTime();\n\n              FileStatus[] parts = fs.listStatus(dataLocation, FileUtils.HIDDEN_FILES_PATH_FILTER);\n              if (parts != null && parts.length > 0) {\n                for (FileStatus status : parts) {\n                  if (status.getModificationTime() > lastModificationTime) {\n                    lastModificationTime = status.getModificationTime();\n                  }\n                }\n              }\n              basePartTs.put(p.getSpec(), lastModificationTime);\n            }\n          }\n        } else {\n          FileSystem fs = baseTbl.getPath().getFileSystem(db.getConf());\n          FileStatus fss = fs.getFileStatus(baseTbl.getPath());\n          basePartTs.put(null, fss.getModificationTime());\n        }\n        for (Map<String, String> spec : basePartTs.keySet()) {\n          if (spec != null) {\n            props.put(spec.toString(), basePartTs.get(spec).toString());\n          } else {\n            props.put(\"base_timestamp\", basePartTs.get(null).toString());\n          }\n        }\n        idx.getParameters().putAll(props);\n      } catch (HiveException e) {\n        throw new HiveException(\"ERROR: Failed to update index timestamps\");\n      } catch (IOException e) {\n        throw new HiveException(\"ERROR: Failed to look up timestamps on filesystem\");\n      }\n\n      break;\n    default:\n      console.printError(\"Unsupported Alter command\");\n      return 1;\n    }\n\n    // set last modified by properties\n    if (!updateModifiedParameters(idx.getParameters(), conf)) {\n      return 1;\n    }\n\n    try {\n      db.alterIndex(baseTableName, indexName, idx);\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter index: \" + stringifyException(e));\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      return 1;\n    }\n    return 0;\n  }",
            "1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128 +\n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  ",
            "  private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException {\n\n    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n      throw new UnsupportedOperationException(\"Indexes unsupported for Tez execution engine\");\n    }\n\n    String baseTableName = alterIndex.getBaseTableName();\n    String indexName = alterIndex.getIndexName();\n    Index idx = db.getIndex(baseTableName, indexName);\n\n    switch(alterIndex.getOp()) {\n    case ADDPROPS:\n      idx.getParameters().putAll(alterIndex.getProps());\n      break;\n    case UPDATETIMESTAMP:\n      try {\n        Map<String, String> props = new HashMap<String, String>();\n        Map<Map<String, String>, Long> basePartTs = new HashMap<Map<String, String>, Long>();\n\n        Table baseTbl = db.getTable(baseTableName);\n\n        if (baseTbl.isPartitioned()) {\n          List<Partition> baseParts;\n          if (alterIndex.getSpec() != null) {\n            baseParts = db.getPartitions(baseTbl, alterIndex.getSpec());\n          } else {\n            baseParts = db.getPartitions(baseTbl);\n          }\n          if (baseParts != null) {\n            for (Partition p : baseParts) {\n              Path dataLocation = p.getDataLocation();\n              FileSystem fs = dataLocation.getFileSystem(db.getConf());\n              FileStatus fss = fs.getFileStatus(dataLocation);\n              long lastModificationTime = fss.getModificationTime();\n\n              FileStatus[] parts = fs.listStatus(dataLocation, FileUtils.HIDDEN_FILES_PATH_FILTER);\n              if (parts != null && parts.length > 0) {\n                for (FileStatus status : parts) {\n                  if (status.getModificationTime() > lastModificationTime) {\n                    lastModificationTime = status.getModificationTime();\n                  }\n                }\n              }\n              basePartTs.put(p.getSpec(), lastModificationTime);\n            }\n          }\n        } else {\n          FileSystem fs = baseTbl.getPath().getFileSystem(db.getConf());\n          FileStatus fss = fs.getFileStatus(baseTbl.getPath());\n          basePartTs.put(null, fss.getModificationTime());\n        }\n        for (Map<String, String> spec : basePartTs.keySet()) {\n          if (spec != null) {\n            props.put(spec.toString(), basePartTs.get(spec).toString());\n          } else {\n            props.put(\"base_timestamp\", basePartTs.get(null).toString());\n          }\n        }\n        idx.getParameters().putAll(props);\n      } catch (HiveException e) {\n        throw new HiveException(\"ERROR: Failed to update index timestamps\");\n      } catch (IOException e) {\n        throw new HiveException(\"ERROR: Failed to look up timestamps on filesystem\");\n      }\n\n      break;\n    default:\n      console.printError(\"Unsupported Alter command\");\n      return 1;\n    }\n\n    // set last modified by properties\n    if (!updateModifiedParameters(idx.getParameters(), conf)) {\n      return 1;\n    }\n\n    try {\n      db.alterIndex(baseTableName, indexName, idx);\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter index: \", e);\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      return 1;\n    }\n    return 0;\n  }"
        ],
        [
            "StatsTask::aggregateStats(Hive)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242 -\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266 -\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327 -\n 328 -\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  ",
            "  private int aggregateStats(Hive db) {\n\n    StatsAggregator statsAggregator = null;\n    int ret = 0;\n    StatsCollectionContext scc = null;\n    EnvironmentContext environmentContext = null;\n    try {\n      // Stats setup:\n      final Warehouse wh = new Warehouse(conf);\n      if (!getWork().getNoStatsAggregator() && !getWork().isNoScanAnalyzeCommand()) {\n        try {\n          scc = getContext();\n          statsAggregator = createStatsAggregator(scc, conf);\n        } catch (HiveException e) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n            throw e;\n          }\n          console.printError(ErrorMsg.STATS_SKIPPING_BY_ERROR.getErrorCodedMsg(e.toString()));\n        }\n      }\n\n      List<Partition> partitions = getPartitionsList(db);\n      boolean atomic = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_ATOMIC);\n\n      String tableFullName = table.getDbName() + \".\" + table.getTableName();\n\n      if (partitions == null) {\n        org.apache.hadoop.hive.metastore.api.Table tTable = table.getTTable();\n        Map<String, String> parameters = tTable.getParameters();\n        // In the following scenarios, we need to reset the stats to true.\n        // work.getTableSpecs() != null means analyze command\n        // work.getLoadTableDesc().getReplace() is true means insert overwrite command \n        // work.getLoadFileDesc().getDestinationCreateTable().isEmpty() means CTAS etc.\n        // acidTable will not have accurate stats unless it is set through analyze command.\n        if (work.getTableSpecs() == null && AcidUtils.isFullAcidTable(table)) {\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n        } else if (work.getTableSpecs() != null\n            || (work.getLoadTableDesc() != null\n                && (work.getLoadTableDesc().getLoadFileType() == LoadFileType.REPLACE_ALL))\n            || (work.getLoadFileDesc() != null && !work.getLoadFileDesc()\n                .getDestinationCreateTable().isEmpty())) {\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);\n        }\n        // non-partitioned tables:\n        if (!existStats(parameters) && atomic) {\n          return 0;\n        }\n\n        // The collectable stats for the aggregator needs to be cleared.\n        // For eg. if a file is being loaded, the old number of rows are not valid\n        if (work.isClearAggregatorStats()) {\n          // we choose to keep the invalid stats and only change the setting.\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n        }\n\n        updateQuickStats(wh, parameters, tTable.getSd());\n        if (StatsSetupConst.areBasicStatsUptoDate(parameters)) {\n          if (statsAggregator != null) {\n            String prefix = getAggregationPrefix(table, null);\n            updateStats(statsAggregator, parameters, prefix, atomic);\n          }\n          // write table stats to metastore\n          if (!getWork().getNoStatsAggregator()) {\n            environmentContext = new EnvironmentContext();\n            environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED,\n                StatsSetupConst.TASK);\n          }\n        }\n\n        getHive().alterTable(tableFullName, new Table(tTable), environmentContext);\n        if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {\n          console.printInfo(\"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']');\n        }\n        if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n          Utilities.FILE_OP_LOGGER.trace(\n              \"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']');\n        }\n      } else {\n        // Partitioned table:\n        // Need to get the old stats of the partition\n        // and update the table stats based on the old and new stats.\n        List<Partition> updates = new ArrayList<Partition>();\n\n        //Get the file status up-front for all partitions. Beneficial in cases of blob storage systems\n        final Map<String, FileStatus[]> fileStatusMap = new ConcurrentHashMap<String, FileStatus[]>();\n        int poolSize = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 1);\n        // In case thread count is set to 0, use single thread.\n        poolSize = Math.max(poolSize, 1);\n        final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n          new ThreadFactoryBuilder().setDaemon(true)\n            .setNameFormat(\"stats-updater-thread-%d\")\n            .build());\n        final List<Future<Void>> futures = Lists.newLinkedList();\n        LOG.debug(\"Getting file stats of all partitions. threadpool size:\" + poolSize);\n        try {\n          for(final Partition partn : partitions) {\n            final String partitionName = partn.getName();\n            final org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n            Map<String, String> parameters = tPart.getParameters();\n\n            if (!existStats(parameters) && atomic) {\n              continue;\n            }\n            futures.add(pool.submit(new Callable<Void>() {\n              @Override\n              public Void call() throws Exception {\n                FileStatus[] partfileStatus = wh.getFileStatusesForSD(tPart.getSd());\n                fileStatusMap.put(partitionName,  partfileStatus);\n                return null;\n              }\n            }));\n          }\n          pool.shutdown();\n          for(Future<Void> future : futures) {\n            future.get();\n          }\n        } catch (InterruptedException e) {\n          LOG.debug(\"Cancelling \" + futures.size() + \" file stats lookup tasks\");\n          //cancel other futures\n          for (Future future : futures) {\n            future.cancel(true);\n          }\n          // Fail the query if the stats are supposed to be reliable\n          if (work.isStatsReliable()) {\n            ret = 1;\n          }\n        } finally {\n          if (pool != null) {\n            pool.shutdownNow();\n          }\n          LOG.debug(\"Finished getting file stats of all partitions\");\n        }\n\n        for (Partition partn : partitions) {\n          //\n          // get the old partition stats\n          //\n          org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n          Map<String, String> parameters = tPart.getParameters();\n          if (work.getTableSpecs() == null && AcidUtils.isFullAcidTable(table)) {\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n          } else if (work.getTableSpecs() != null\n              || (work.getLoadTableDesc() != null\n                  && (work.getLoadTableDesc().getLoadFileType() == LoadFileType.REPLACE_ALL))\n              || (work.getLoadFileDesc() != null && !work.getLoadFileDesc()\n                  .getDestinationCreateTable().isEmpty())) {\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);\n          }\n          //only when the stats exist, it is added to fileStatusMap\n          if (!fileStatusMap.containsKey(partn.getName())) {\n            continue;\n          }\n\n          // The collectable stats for the aggregator needs to be cleared.\n          // For eg. if a file is being loaded, the old number of rows are not valid\n          if (work.isClearAggregatorStats()) {\n            // we choose to keep the invalid stats and only change the setting.\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n          }\n\n          updateQuickStats(parameters, fileStatusMap.get(partn.getName()));\n          if (StatsSetupConst.areBasicStatsUptoDate(parameters)) {\n            if (statsAggregator != null) {\n              String prefix = getAggregationPrefix(table, partn);\n              updateStats(statsAggregator, parameters, prefix, atomic);\n            }\n            if (!getWork().getNoStatsAggregator()) {\n              environmentContext = new EnvironmentContext();\n              environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED,\n                  StatsSetupConst.TASK);\n            }\n          }\n          updates.add(new Partition(table, tPart));\n\n          if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {\n            console.printInfo(\"Partition \" + tableFullName + partn.getSpec() +\n            \" stats: [\" + toString(parameters) + ']');\n          }\n          LOG.info(\"Partition \" + tableFullName + partn.getSpec() +\n              \" stats: [\" + toString(parameters) + ']');\n        }\n        if (!updates.isEmpty()) {\n          db.alterPartitions(tableFullName, updates, environmentContext);\n        }\n      }\n\n    } catch (Exception e) {\n      console.printInfo(\"[Warning] could not update stats.\",\n          \"Failed with exception \" + e.getMessage() + \"\\n\"\n              + StringUtils.stringifyException(e));\n\n      // Fail the query if the stats are supposed to be reliable\n      if (work.isStatsReliable()) {\n        ret = 1;\n      }\n    } finally {\n      if (statsAggregator != null) {\n        statsAggregator.closeConnection(scc);\n      }\n    }\n    // The return value of 0 indicates success,\n    // anything else indicates failure\n    return ret;\n  }",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243 +\n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267 +\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328 +\n 329 +\n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  ",
            "  private int aggregateStats(Hive db) {\n\n    StatsAggregator statsAggregator = null;\n    int ret = 0;\n    StatsCollectionContext scc = null;\n    EnvironmentContext environmentContext = null;\n    try {\n      // Stats setup:\n      final Warehouse wh = new Warehouse(conf);\n      if (!getWork().getNoStatsAggregator() && !getWork().isNoScanAnalyzeCommand()) {\n        try {\n          scc = getContext();\n          statsAggregator = createStatsAggregator(scc, conf);\n        } catch (HiveException e) {\n          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n            throw e;\n          }\n          console.printError(ErrorMsg.STATS_SKIPPING_BY_ERROR.getErrorCodedMsg(e.toString()));\n        }\n      }\n\n      List<Partition> partitions = getPartitionsList(db);\n      boolean atomic = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_ATOMIC);\n\n      String tableFullName = table.getDbName() + \".\" + table.getTableName();\n\n      if (partitions == null) {\n        org.apache.hadoop.hive.metastore.api.Table tTable = table.getTTable();\n        Map<String, String> parameters = tTable.getParameters();\n        // In the following scenarios, we need to reset the stats to true.\n        // work.getTableSpecs() != null means analyze command\n        // work.getLoadTableDesc().getReplace() is true means insert overwrite command \n        // work.getLoadFileDesc().getDestinationCreateTable().isEmpty() means CTAS etc.\n        // acidTable will not have accurate stats unless it is set through analyze command.\n        if (work.getTableSpecs() == null && AcidUtils.isFullAcidTable(table)) {\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n        } else if (work.getTableSpecs() != null\n            || (work.getLoadTableDesc() != null\n                && (work.getLoadTableDesc().getLoadFileType() == LoadFileType.REPLACE_ALL))\n            || (work.getLoadFileDesc() != null && !work.getLoadFileDesc()\n                .getDestinationCreateTable().isEmpty())) {\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);\n        }\n        // non-partitioned tables:\n        if (!existStats(parameters) && atomic) {\n          return 0;\n        }\n\n        // The collectable stats for the aggregator needs to be cleared.\n        // For eg. if a file is being loaded, the old number of rows are not valid\n        if (work.isClearAggregatorStats()) {\n          // we choose to keep the invalid stats and only change the setting.\n          StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n        }\n\n        updateQuickStats(wh, parameters, tTable.getSd());\n        if (StatsSetupConst.areBasicStatsUptoDate(parameters)) {\n          if (statsAggregator != null) {\n            String prefix = getAggregationPrefix(table, null);\n            updateStats(statsAggregator, parameters, prefix, atomic);\n          }\n          // write table stats to metastore\n          if (!getWork().getNoStatsAggregator()) {\n            environmentContext = new EnvironmentContext();\n            environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED,\n                StatsSetupConst.TASK);\n          }\n        }\n\n        getHive().alterTable(tableFullName, new Table(tTable), environmentContext);\n        if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {\n          console.printInfo(\"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']');\n        }\n        LOG.info(\"Table {} stats: [{}]\", tableFullName, toString(parameters));\n        if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {\n          Utilities.FILE_OP_LOGGER.trace(\n              \"Table \" + tableFullName + \" stats: [\" + toString(parameters) + ']');\n        }\n      } else {\n        // Partitioned table:\n        // Need to get the old stats of the partition\n        // and update the table stats based on the old and new stats.\n        List<Partition> updates = new ArrayList<Partition>();\n\n        //Get the file status up-front for all partitions. Beneficial in cases of blob storage systems\n        final Map<String, FileStatus[]> fileStatusMap = new ConcurrentHashMap<String, FileStatus[]>();\n        int poolSize = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 1);\n        // In case thread count is set to 0, use single thread.\n        poolSize = Math.max(poolSize, 1);\n        final ExecutorService pool = Executors.newFixedThreadPool(poolSize,\n          new ThreadFactoryBuilder().setDaemon(true)\n            .setNameFormat(\"stats-updater-thread-%d\")\n            .build());\n        final List<Future<Void>> futures = Lists.newLinkedList();\n        LOG.debug(\"Getting file stats of all partitions. threadpool size: {}\", poolSize);\n        try {\n          for(final Partition partn : partitions) {\n            final String partitionName = partn.getName();\n            final org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n            Map<String, String> parameters = tPart.getParameters();\n\n            if (!existStats(parameters) && atomic) {\n              continue;\n            }\n            futures.add(pool.submit(new Callable<Void>() {\n              @Override\n              public Void call() throws Exception {\n                FileStatus[] partfileStatus = wh.getFileStatusesForSD(tPart.getSd());\n                fileStatusMap.put(partitionName,  partfileStatus);\n                return null;\n              }\n            }));\n          }\n          pool.shutdown();\n          for(Future<Void> future : futures) {\n            future.get();\n          }\n        } catch (InterruptedException e) {\n          LOG.debug(\"Cancelling {} file stats lookup tasks\", futures.size());\n          //cancel other futures\n          for (Future future : futures) {\n            future.cancel(true);\n          }\n          // Fail the query if the stats are supposed to be reliable\n          if (work.isStatsReliable()) {\n            ret = 1;\n          }\n        } finally {\n          if (pool != null) {\n            pool.shutdownNow();\n          }\n          LOG.debug(\"Finished getting file stats of all partitions\");\n        }\n\n        for (Partition partn : partitions) {\n          //\n          // get the old partition stats\n          //\n          org.apache.hadoop.hive.metastore.api.Partition tPart = partn.getTPartition();\n          Map<String, String> parameters = tPart.getParameters();\n          if (work.getTableSpecs() == null && AcidUtils.isFullAcidTable(table)) {\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n          } else if (work.getTableSpecs() != null\n              || (work.getLoadTableDesc() != null\n                  && (work.getLoadTableDesc().getLoadFileType() == LoadFileType.REPLACE_ALL))\n              || (work.getLoadFileDesc() != null && !work.getLoadFileDesc()\n                  .getDestinationCreateTable().isEmpty())) {\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.TRUE);\n          }\n          //only when the stats exist, it is added to fileStatusMap\n          if (!fileStatusMap.containsKey(partn.getName())) {\n            continue;\n          }\n\n          // The collectable stats for the aggregator needs to be cleared.\n          // For eg. if a file is being loaded, the old number of rows are not valid\n          if (work.isClearAggregatorStats()) {\n            // we choose to keep the invalid stats and only change the setting.\n            StatsSetupConst.setBasicStatsState(parameters, StatsSetupConst.FALSE);\n          }\n\n          updateQuickStats(parameters, fileStatusMap.get(partn.getName()));\n          if (StatsSetupConst.areBasicStatsUptoDate(parameters)) {\n            if (statsAggregator != null) {\n              String prefix = getAggregationPrefix(table, partn);\n              updateStats(statsAggregator, parameters, prefix, atomic);\n            }\n            if (!getWork().getNoStatsAggregator()) {\n              environmentContext = new EnvironmentContext();\n              environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED,\n                  StatsSetupConst.TASK);\n            }\n          }\n          updates.add(new Partition(table, tPart));\n\n          if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {\n            console.printInfo(\"Partition \" + tableFullName + partn.getSpec() +\n            \" stats: [\" + toString(parameters) + ']');\n          }\n          LOG.info(\"Partition {}{} stats: [{}]\", tableFullName, partn.getSpec(),\n            toString(parameters));\n        }\n        if (!updates.isEmpty()) {\n          db.alterPartitions(tableFullName, updates, environmentContext);\n        }\n      }\n\n    } catch (Exception e) {\n      console.printInfo(\"[Warning] could not update stats.\",\n          \"Failed with exception \" + e.getMessage() + \"\\n\"\n              + StringUtils.stringifyException(e));\n\n      // Fail the query if the stats are supposed to be reliable\n      if (work.isStatsReliable()) {\n        ret = 1;\n      }\n    } finally {\n      if (statsAggregator != null) {\n        statsAggregator.closeConnection(scc);\n      }\n    }\n    // The return value of 0 indicates success,\n    // anything else indicates failure\n    return ret;\n  }"
        ],
        [
            "DDLTask::failed(Throwable)",
            " 660  \n 661  \n 662  \n 663  \n 664  \n 665 -\n 666  ",
            "  private void failed(Throwable e) {\n    while (e.getCause() != null && e.getClass() == RuntimeException.class) {\n      e = e.getCause();\n    }\n    setException(e);\n    LOG.error(stringifyException(e));\n  }",
            " 660  \n 661  \n 662  \n 663  \n 664  \n 665 +\n 666  ",
            "  private void failed(Throwable e) {\n    while (e.getCause() != null && e.getClass() == RuntimeException.class) {\n      e = e.getCause();\n    }\n    setException(e);\n    LOG.error(\"Failed\", e);\n  }"
        ],
        [
            "StatsTask::execute(DriverContext)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n      return 0;\n    }\n    LOG.info(\"Executing stats task\");\n    // Make sure that it is either an ANALYZE, INSERT OVERWRITE (maybe load) or CTAS command\n    short workComponentsPresent = 0;\n    if (work.getLoadTableDesc() != null) {\n      workComponentsPresent++;\n    }\n    if (work.getTableSpecs() != null) {\n      workComponentsPresent++;\n    }\n    if (work.getLoadFileDesc() != null) {\n      workComponentsPresent++;\n    }\n\n    assert (workComponentsPresent == 1);\n\n    String tableName = \"\";\n    Hive hive = getHive();\n    try {\n      if (work.getLoadTableDesc() != null) {\n        tableName = work.getLoadTableDesc().getTable().getTableName();\n      } else if (work.getTableSpecs() != null){\n        tableName = work.getTableSpecs().tableName;\n      } else {\n        tableName = work.getLoadFileDesc().getDestinationCreateTable();\n      }\n\n      table = hive.getTable(tableName);\n\n    } catch (HiveException e) {\n      LOG.error(\"Cannot get table \" + tableName, e);\n      console.printError(\"Cannot get table \" + tableName, e.toString());\n    }\n\n    return aggregateStats(hive);\n\n  }",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    if (driverContext.getCtx().getExplainAnalyze() == AnalyzeState.RUNNING) {\n      return 0;\n    }\n    LOG.info(\"Executing stats task\");\n    // Make sure that it is either an ANALYZE, INSERT OVERWRITE (maybe load) or CTAS command\n    short workComponentsPresent = 0;\n    if (work.getLoadTableDesc() != null) {\n      workComponentsPresent++;\n    }\n    if (work.getTableSpecs() != null) {\n      workComponentsPresent++;\n    }\n    if (work.getLoadFileDesc() != null) {\n      workComponentsPresent++;\n    }\n\n    assert (workComponentsPresent == 1);\n\n    String tableName = \"\";\n    Hive hive = getHive();\n    try {\n      if (work.getLoadTableDesc() != null) {\n        tableName = work.getLoadTableDesc().getTable().getTableName();\n      } else if (work.getTableSpecs() != null){\n        tableName = work.getTableSpecs().tableName;\n      } else {\n        tableName = work.getLoadFileDesc().getDestinationCreateTable();\n      }\n\n      table = hive.getTable(tableName);\n\n    } catch (HiveException e) {\n      LOG.error(\"Cannot get table {}\", tableName, e);\n      console.printError(\"Cannot get table \" + tableName, e.toString());\n    }\n\n    return aggregateStats(hive);\n\n  }"
        ],
        [
            "DDLTask::alterTable(Hive,AlterTableDesc)",
            "3628  \n3629  \n3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678  \n3679  \n3680  \n3681  \n3682  \n3683  \n3684  \n3685  \n3686  \n3687  \n3688  \n3689  \n3690  \n3691  \n3692  \n3693  \n3694  \n3695  \n3696  \n3697  \n3698  \n3699  \n3700  \n3701  \n3702  \n3703  \n3704 -\n3705  \n3706  \n3707  \n3708  \n3709  \n3710  \n3711  \n3712  \n3713  \n3714  \n3715  \n3716  \n3717  \n3718  \n3719  \n3720  \n3721  \n3722  \n3723  \n3724  ",
            "  /**\n   * Alter a given table.\n   *\n   * @param db\n   *          The database in question.\n   * @param alterTbl\n   *          This is the table we're altering.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n    if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {\n      String names[] = Utilities.getDbTableName(alterTbl.getOldName());\n      if (Utils.isBootstrapDumpInProgress(db, names[0])) {\n        LOG.error(\"DDLTask: Rename Table not allowed as bootstrap dump in progress\");\n        throw new HiveException(\"Rename Table: Not allowed as bootstrap dump in progress\");\n      }\n    }\n\n    // alter the table\n    Table tbl = db.getTable(alterTbl.getOldName());\n\n    List<Partition> allPartitions = null;\n    if (alterTbl.getPartSpec() != null) {\n      Map<String, String> partSpec = alterTbl.getPartSpec();\n      if (DDLSemanticAnalyzer.isFullSpec(tbl, partSpec)) {\n        allPartitions = new ArrayList<Partition>();\n        Partition part = db.getPartition(tbl, partSpec, false);\n        if (part == null) {\n          // User provided a fully specified partition spec but it doesn't exist, fail.\n          throw new HiveException(ErrorMsg.INVALID_PARTITION,\n                StringUtils.join(alterTbl.getPartSpec().keySet(), ',') + \" for table \" + alterTbl.getOldName());\n\n        }\n        allPartitions.add(part);\n      } else {\n        // DDLSemanticAnalyzer has already checked if partial partition specs are allowed,\n        // thus we should not need to check it here.\n        allPartitions = db.getPartitions(tbl, alterTbl.getPartSpec());\n      }\n    }\n\n    // Don't change the table object returned by the metastore, as we'll mess with it's caches.\n    Table oldTbl = tbl;\n    tbl = oldTbl.copy();\n    // Handle child tasks here. We could add them directly whereever we need,\n    // but let's make it a little bit more explicit.\n    if (allPartitions != null) {\n      // Alter all partitions\n      for (Partition part : allPartitions) {\n        addChildTasks(alterTableOrSinglePartition(alterTbl, tbl, part));\n      }\n    } else {\n      // Just alter the table\n      addChildTasks(alterTableOrSinglePartition(alterTbl, tbl, null));\n    }\n\n    if (allPartitions == null) {\n      updateModifiedParameters(tbl.getTTable().getParameters(), conf);\n      tbl.checkValidity(conf);\n    } else {\n      for (Partition tmpPart: allPartitions) {\n        updateModifiedParameters(tmpPart.getParameters(), conf);\n      }\n    }\n\n    try {\n      if (allPartitions == null) {\n        db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), alterTbl.getEnvironmentContext());\n      } else {\n        db.alterPartitions(Warehouse.getQualifiedName(tbl.getTTable()), allPartitions, alterTbl.getEnvironmentContext());\n      }\n      // Add constraints if necessary\n      addConstraints(db, alterTbl);\n    } catch (InvalidOperationException e) {\n      LOG.error(\"alter table: \" + stringifyException(e));\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR);\n    }\n\n    // This is kind of hacky - the read entity contains the old table, whereas\n    // the write entity\n    // contains the new table. This is needed for rename - both the old and the\n    // new table names are\n    // passed\n    // Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer.\n    if (allPartitions != null ) {\n      for (Partition tmpPart: allPartitions) {\n        work.getInputs().add(new ReadEntity(tmpPart));\n        addIfAbsentByName(new WriteEntity(tmpPart, WriteEntity.WriteType.DDL_NO_LOCK));\n      }\n    } else {\n      work.getInputs().add(new ReadEntity(oldTbl));\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    }\n    return 0;\n  }",
            "3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678  \n3679  \n3680  \n3681  \n3682  \n3683  \n3684  \n3685  \n3686  \n3687  \n3688  \n3689  \n3690  \n3691  \n3692  \n3693  \n3694  \n3695  \n3696  \n3697  \n3698  \n3699  \n3700  \n3701  \n3702  \n3703  \n3704  \n3705  \n3706 +\n3707  \n3708  \n3709  \n3710  \n3711  \n3712  \n3713  \n3714  \n3715  \n3716  \n3717  \n3718  \n3719  \n3720  \n3721  \n3722  \n3723  \n3724  \n3725  \n3726  ",
            "  /**\n   * Alter a given table.\n   *\n   * @param db\n   *          The database in question.\n   * @param alterTbl\n   *          This is the table we're altering.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n    if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {\n      String names[] = Utilities.getDbTableName(alterTbl.getOldName());\n      if (Utils.isBootstrapDumpInProgress(db, names[0])) {\n        LOG.error(\"DDLTask: Rename Table not allowed as bootstrap dump in progress\");\n        throw new HiveException(\"Rename Table: Not allowed as bootstrap dump in progress\");\n      }\n    }\n\n    // alter the table\n    Table tbl = db.getTable(alterTbl.getOldName());\n\n    List<Partition> allPartitions = null;\n    if (alterTbl.getPartSpec() != null) {\n      Map<String, String> partSpec = alterTbl.getPartSpec();\n      if (DDLSemanticAnalyzer.isFullSpec(tbl, partSpec)) {\n        allPartitions = new ArrayList<Partition>();\n        Partition part = db.getPartition(tbl, partSpec, false);\n        if (part == null) {\n          // User provided a fully specified partition spec but it doesn't exist, fail.\n          throw new HiveException(ErrorMsg.INVALID_PARTITION,\n                StringUtils.join(alterTbl.getPartSpec().keySet(), ',') + \" for table \" + alterTbl.getOldName());\n\n        }\n        allPartitions.add(part);\n      } else {\n        // DDLSemanticAnalyzer has already checked if partial partition specs are allowed,\n        // thus we should not need to check it here.\n        allPartitions = db.getPartitions(tbl, alterTbl.getPartSpec());\n      }\n    }\n\n    // Don't change the table object returned by the metastore, as we'll mess with it's caches.\n    Table oldTbl = tbl;\n    tbl = oldTbl.copy();\n    // Handle child tasks here. We could add them directly whereever we need,\n    // but let's make it a little bit more explicit.\n    if (allPartitions != null) {\n      // Alter all partitions\n      for (Partition part : allPartitions) {\n        addChildTasks(alterTableOrSinglePartition(alterTbl, tbl, part));\n      }\n    } else {\n      // Just alter the table\n      addChildTasks(alterTableOrSinglePartition(alterTbl, tbl, null));\n    }\n\n    if (allPartitions == null) {\n      updateModifiedParameters(tbl.getTTable().getParameters(), conf);\n      tbl.checkValidity(conf);\n    } else {\n      for (Partition tmpPart: allPartitions) {\n        updateModifiedParameters(tmpPart.getParameters(), conf);\n      }\n    }\n\n    try {\n      if (allPartitions == null) {\n        db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), alterTbl.getEnvironmentContext());\n      } else {\n        db.alterPartitions(Warehouse.getQualifiedName(tbl.getTTable()), allPartitions, alterTbl.getEnvironmentContext());\n      }\n      // Add constraints if necessary\n      addConstraints(db, alterTbl);\n    } catch (InvalidOperationException e) {\n      LOG.error(\"alter table: \", e);\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR);\n    }\n\n    // This is kind of hacky - the read entity contains the old table, whereas\n    // the write entity\n    // contains the new table. This is needed for rename - both the old and the\n    // new table names are\n    // passed\n    // Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer.\n    if (allPartitions != null ) {\n      for (Partition tmpPart: allPartitions) {\n        work.getInputs().add(new ReadEntity(tmpPart));\n        addIfAbsentByName(new WriteEntity(tmpPart, WriteEntity.WriteType.DDL_NO_LOCK));\n      }\n    } else {\n      work.getInputs().add(new ReadEntity(oldTbl));\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    }\n    return 0;\n  }"
        ],
        [
            "ReplCopyTask::getLoadCopyTask(ReplicationSpec,Path,Path,HiveConf)",
            " 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "  public static Task<?> getLoadCopyTask(ReplicationSpec replicationSpec, Path srcPath, Path dstPath, HiveConf conf) {\n    Task<?> copyTask = null;\n    LOG.debug(\"ReplCopyTask:getLoadCopyTask: \"+srcPath + \"=>\" + dstPath);\n    if ((replicationSpec != null) && replicationSpec.isInReplicationScope()){\n      ReplCopyWork rcwork = new ReplCopyWork(srcPath, dstPath, false);\n      LOG.debug(\"ReplCopyTask:\\trcwork\");\n      if (replicationSpec.isLazy()) {\n        LOG.debug(\"ReplCopyTask:\\tlazy\");\n        rcwork.setReadSrcAsFilesList(true);\n\n        // It is assumed isLazy flag is set only for REPL LOAD flow.\n        // IMPORT always do deep copy. So, distCpDoAsUser will be null by default in ReplCopyWork.\n        String distCpDoAsUser = conf.getVar(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER);\n        rcwork.setDistCpDoAsUser(distCpDoAsUser);\n      }\n      copyTask = TaskFactory.get(rcwork, conf, true);\n    } else {\n      LOG.debug(\"ReplCopyTask:\\tcwork\");\n      copyTask = TaskFactory.get(new CopyWork(srcPath, dstPath, false), conf, true);\n    }\n    return copyTask;\n  }",
            " 218  \n 219  \n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  ",
            "  public static Task<?> getLoadCopyTask(ReplicationSpec replicationSpec, Path srcPath, Path dstPath, HiveConf conf) {\n    Task<?> copyTask = null;\n    LOG.debug(\"ReplCopyTask:getLoadCopyTask: {}=>{}\", srcPath, dstPath);\n    if ((replicationSpec != null) && replicationSpec.isInReplicationScope()){\n      ReplCopyWork rcwork = new ReplCopyWork(srcPath, dstPath, false);\n      LOG.debug(\"ReplCopyTask:\\trcwork\");\n      if (replicationSpec.isLazy()) {\n        LOG.debug(\"ReplCopyTask:\\tlazy\");\n        rcwork.setReadSrcAsFilesList(true);\n\n        // It is assumed isLazy flag is set only for REPL LOAD flow.\n        // IMPORT always do deep copy. So, distCpDoAsUser will be null by default in ReplCopyWork.\n        String distCpDoAsUser = conf.getVar(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER);\n        rcwork.setDistCpDoAsUser(distCpDoAsUser);\n      }\n      copyTask = TaskFactory.get(rcwork, conf, true);\n    } else {\n      LOG.debug(\"ReplCopyTask:\\tcwork\");\n      copyTask = TaskFactory.get(new CopyWork(srcPath, dstPath, false), conf, true);\n    }\n    return copyTask;\n  }"
        ],
        [
            "FunctionTask::dropTemporaryFunction(DropFunctionDesc)",
            " 265  \n 266  \n 267  \n 268  \n 269  \n 270 -\n 271  \n 272  \n 273  ",
            "  private int dropTemporaryFunction(DropFunctionDesc dropFunctionDesc) {\n    try {\n      FunctionRegistry.unregisterTemporaryUDF(dropFunctionDesc.getFunctionName());\n      return 0;\n    } catch (HiveException e) {\n      LOG.info(\"drop function: \" + StringUtils.stringifyException(e));\n      return 1;\n    }\n  }",
            " 265  \n 266  \n 267  \n 268  \n 269  \n 270 +\n 271  \n 272  \n 273  ",
            "  private int dropTemporaryFunction(DropFunctionDesc dropFunctionDesc) {\n    try {\n      FunctionRegistry.unregisterTemporaryUDF(dropFunctionDesc.getFunctionName());\n      return 0;\n    } catch (HiveException e) {\n      LOG.info(\"drop function: \", e);\n      return 1;\n    }\n  }"
        ],
        [
            "CopyTask::copyOnePath(Path,Path)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87 -\n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "  protected int copyOnePath(Path fromPath, Path toPath) {\n    FileSystem dstFs = null;\n    try {\n      console.printInfo(\"Copying data from \" + fromPath.toString(), \" to \"\n          + toPath.toString());\n\n      FileSystem srcFs = fromPath.getFileSystem(conf);\n      dstFs = toPath.getFileSystem(conf);\n\n      FileStatus[] srcs = matchFilesOrDir(srcFs, fromPath, work.doSkipSourceMmDirs());\n      if (srcs == null || srcs.length == 0) {\n        if (work.isErrorOnSrcEmpty()) {\n          console.printError(\"No files matching path: \" + fromPath.toString());\n          return 3;\n        } else {\n          return 0;\n        }\n      }\n\n      if (!FileUtils.mkdir(dstFs, toPath, conf)) {\n        console.printError(\"Cannot make target directory: \" + toPath.toString());\n        return 2;\n      }\n\n      for (FileStatus oneSrc : srcs) {\n        console.printInfo(\"Copying file: \" + oneSrc.getPath().toString());\n        LOG.debug(\"Copying file: \" + oneSrc.getPath().toString());\n        if (!FileUtils.copy(srcFs, oneSrc.getPath(), dstFs, toPath,\n            false, // delete source\n            true, // overwrite destination\n            conf)) {\n          console.printError(\"Failed to copy: '\" + oneSrc.getPath().toString()\n              + \"to: '\" + toPath.toString() + \"'\");\n          return 1;\n        }\n      }\n      return 0;\n\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      return (1);\n    }\n  }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87 +\n  88 +\n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  protected int copyOnePath(Path fromPath, Path toPath) {\n    FileSystem dstFs = null;\n    try {\n      console.printInfo(\"Copying data from \" + fromPath.toString(), \" to \"\n          + toPath.toString());\n\n      FileSystem srcFs = fromPath.getFileSystem(conf);\n      dstFs = toPath.getFileSystem(conf);\n\n      FileStatus[] srcs = matchFilesOrDir(srcFs, fromPath, work.doSkipSourceMmDirs());\n      if (srcs == null || srcs.length == 0) {\n        if (work.isErrorOnSrcEmpty()) {\n          console.printError(\"No files matching path: \" + fromPath.toString());\n          return 3;\n        } else {\n          return 0;\n        }\n      }\n\n      if (!FileUtils.mkdir(dstFs, toPath, conf)) {\n        console.printError(\"Cannot make target directory: \" + toPath.toString());\n        return 2;\n      }\n\n      for (FileStatus oneSrc : srcs) {\n        String oneSrcPathStr = oneSrc.getPath().toString();\n        console.printInfo(\"Copying file: \" + oneSrcPathStr);\n        LOG.debug(\"Copying file: {}\", oneSrcPathStr);\n        if (!FileUtils.copy(srcFs, oneSrc.getPath(), dstFs, toPath,\n            false, // delete source\n            true, // overwrite destination\n            conf)) {\n          console.printError(\"Failed to copy: '\" + oneSrcPathStr\n              + \"to: '\" + toPath.toString() + \"'\");\n          return 1;\n        }\n      }\n      return 0;\n\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      return (1);\n    }\n  }"
        ],
        [
            "DDLTask::showLocksNewFormat(ShowLocksDesc,HiveLockManager)",
            "2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899  \n2900  \n2901  \n2902  \n2903  \n2904  \n2905  \n2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925 -\n2926  \n2927  \n2928 -\n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  ",
            "  private int showLocksNewFormat(ShowLocksDesc showLocks, HiveLockManager lm)\n      throws  HiveException {\n\n    DbLockManager lockMgr;\n    if (!(lm instanceof DbLockManager)) {\n      throw new RuntimeException(\"New lock format only supported with db lock manager.\");\n    }\n    lockMgr = (DbLockManager)lm;\n\n    String dbName = showLocks.getDbName();\n    String tblName = showLocks.getTableName();\n    Map<String, String> partSpec = showLocks.getPartSpec();\n    if (dbName == null && tblName != null) {\n      dbName = SessionState.get().getCurrentDatabase();\n    }\n\n    ShowLocksRequest rqst = new ShowLocksRequest();\n    rqst.setDbname(dbName);\n    rqst.setTablename(tblName);\n    if (partSpec != null) {\n      List<String> keyList = new ArrayList<String>();\n      List<String> valList = new ArrayList<String>();\n      for (String partKey : partSpec.keySet()) {\n        String partVal = partSpec.remove(partKey);\n        keyList.add(partKey);\n        valList.add(partVal);\n      }\n      String partName = FileUtils.makePartName(keyList, valList);\n      rqst.setPartname(partName);\n    }\n\n    ShowLocksResponse rsp = lockMgr.getLocks(rqst);\n\n    // write the results in the file\n    DataOutputStream os = getOutputStream(showLocks.getResFile());\n    try {\n      dumpLockInfo(os, rsp);\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \" + stringifyException(e));\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e.toString());\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }",
            "2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899  \n2900  \n2901  \n2902  \n2903  \n2904  \n2905  \n2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927 +\n2928  \n2929  \n2930 +\n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  ",
            "  private int showLocksNewFormat(ShowLocksDesc showLocks, HiveLockManager lm)\n      throws  HiveException {\n\n    DbLockManager lockMgr;\n    if (!(lm instanceof DbLockManager)) {\n      throw new RuntimeException(\"New lock format only supported with db lock manager.\");\n    }\n    lockMgr = (DbLockManager)lm;\n\n    String dbName = showLocks.getDbName();\n    String tblName = showLocks.getTableName();\n    Map<String, String> partSpec = showLocks.getPartSpec();\n    if (dbName == null && tblName != null) {\n      dbName = SessionState.get().getCurrentDatabase();\n    }\n\n    ShowLocksRequest rqst = new ShowLocksRequest();\n    rqst.setDbname(dbName);\n    rqst.setTablename(tblName);\n    if (partSpec != null) {\n      List<String> keyList = new ArrayList<String>();\n      List<String> valList = new ArrayList<String>();\n      for (String partKey : partSpec.keySet()) {\n        String partVal = partSpec.remove(partKey);\n        keyList.add(partKey);\n        valList.add(partVal);\n      }\n      String partName = FileUtils.makePartName(keyList, valList);\n      rqst.setPartname(partName);\n    }\n\n    ShowLocksResponse rsp = lockMgr.getLocks(rqst);\n\n    // write the results in the file\n    DataOutputStream os = getOutputStream(showLocks.getResFile());\n    try {\n      dumpLockInfo(os, rsp);\n    } catch (FileNotFoundException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.warn(\"show function: \", e);\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e.toString());\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }"
        ],
        [
            "ColumnStatsTask::constructColumnStatsFromPackedRows(Hive)",
            " 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  ",
            "  private List<ColumnStatistics> constructColumnStatsFromPackedRows(\n      Hive db) throws HiveException, MetaException, IOException {\n\n    String currentDb = work.getCurrentDatabaseName();\n    String tableName = work.getColStats().getTableName();\n    String partName = null;\n    List<String> colName = work.getColStats().getColName();\n    List<String> colType = work.getColStats().getColType();\n    boolean isTblLevel = work.getColStats().isTblLevel();\n\n    List<ColumnStatistics> stats = new ArrayList<ColumnStatistics>();\n    InspectableObject packedRow;\n    Table tbl = db.getTable(currentDb, tableName);\n    while ((packedRow = ftOp.getNextRow()) != null) {\n      if (packedRow.oi.getCategory() != ObjectInspector.Category.STRUCT) {\n        throw new HiveException(\"Unexpected object type encountered while unpacking row\");\n      }\n\n      List<ColumnStatisticsObj> statsObjs = new ArrayList<ColumnStatisticsObj>();\n      StructObjectInspector soi = (StructObjectInspector) packedRow.oi;\n      List<? extends StructField> fields = soi.getAllStructFieldRefs();\n      List<Object> list = soi.getStructFieldsDataAsList(packedRow.o);\n\n      List<FieldSchema> partColSchema = tbl.getPartCols();\n      // Partition columns are appended at end, we only care about stats column\n      int numOfStatCols = isTblLevel ? fields.size() : fields.size() - partColSchema.size();\n      for (int i = 0; i < numOfStatCols; i++) {\n        // Get the field objectInspector, fieldName and the field object.\n        ObjectInspector foi = fields.get(i).getFieldObjectInspector();\n        Object f = (list == null ? null : list.get(i));\n        String fieldName = fields.get(i).getFieldName();\n        ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n        statsObj.setColName(colName.get(i));\n        statsObj.setColType(colType.get(i));\n        try {\n          unpackStructObject(foi, f, fieldName, statsObj);\n          statsObjs.add(statsObj);\n        } catch (UnsupportedDoubleException e) {\n          // due to infinity or nan.\n          LOG.info(\"Because \" + colName.get(i) + \" is infinite or NaN, we skip stats.\");\n        }\n      }\n\n      if (!isTblLevel) {\n        List<String> partVals = new ArrayList<String>();\n        // Iterate over partition columns to figure out partition name\n        for (int i = fields.size() - partColSchema.size(); i < fields.size(); i++) {\n          Object partVal = ((PrimitiveObjectInspector)fields.get(i).getFieldObjectInspector()).\n              getPrimitiveJavaObject(list.get(i));\n          partVals.add(partVal == null ? // could be null for default partition\n            this.conf.getVar(ConfVars.DEFAULTPARTITIONNAME) : partVal.toString());\n        }\n        partName = Warehouse.makePartName(partColSchema, partVals);\n      }\n      String [] names = Utilities.getDbTableName(currentDb, tableName);\n      ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1], partName, isTblLevel);\n      ColumnStatistics colStats = new ColumnStatistics();\n      colStats.setStatsDesc(statsDesc);\n      colStats.setStatsObj(statsObjs);\n      if (!statsObjs.isEmpty()) {\n        stats.add(colStats);\n      }\n    }\n    ftOp.clearFetchContext();\n    return stats;\n  }",
            " 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  ",
            "  private List<ColumnStatistics> constructColumnStatsFromPackedRows(\n      Hive db) throws HiveException, MetaException, IOException {\n\n    String currentDb = work.getCurrentDatabaseName();\n    String tableName = work.getColStats().getTableName();\n    String partName = null;\n    List<String> colName = work.getColStats().getColName();\n    List<String> colType = work.getColStats().getColType();\n    boolean isTblLevel = work.getColStats().isTblLevel();\n\n    List<ColumnStatistics> stats = new ArrayList<ColumnStatistics>();\n    InspectableObject packedRow;\n    Table tbl = db.getTable(currentDb, tableName);\n    while ((packedRow = ftOp.getNextRow()) != null) {\n      if (packedRow.oi.getCategory() != ObjectInspector.Category.STRUCT) {\n        throw new HiveException(\"Unexpected object type encountered while unpacking row\");\n      }\n\n      List<ColumnStatisticsObj> statsObjs = new ArrayList<ColumnStatisticsObj>();\n      StructObjectInspector soi = (StructObjectInspector) packedRow.oi;\n      List<? extends StructField> fields = soi.getAllStructFieldRefs();\n      List<Object> list = soi.getStructFieldsDataAsList(packedRow.o);\n\n      List<FieldSchema> partColSchema = tbl.getPartCols();\n      // Partition columns are appended at end, we only care about stats column\n      int numOfStatCols = isTblLevel ? fields.size() : fields.size() - partColSchema.size();\n      for (int i = 0; i < numOfStatCols; i++) {\n        // Get the field objectInspector, fieldName and the field object.\n        ObjectInspector foi = fields.get(i).getFieldObjectInspector();\n        Object f = (list == null ? null : list.get(i));\n        String fieldName = fields.get(i).getFieldName();\n        ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n        statsObj.setColName(colName.get(i));\n        statsObj.setColType(colType.get(i));\n        try {\n          unpackStructObject(foi, f, fieldName, statsObj);\n          statsObjs.add(statsObj);\n        } catch (UnsupportedDoubleException e) {\n          // due to infinity or nan.\n          LOG.info(\"Because {} is infinite or NaN, we skip stats.\",  colName.get(i));\n        }\n      }\n\n      if (!isTblLevel) {\n        List<String> partVals = new ArrayList<String>();\n        // Iterate over partition columns to figure out partition name\n        for (int i = fields.size() - partColSchema.size(); i < fields.size(); i++) {\n          Object partVal = ((PrimitiveObjectInspector)fields.get(i).getFieldObjectInspector()).\n              getPrimitiveJavaObject(list.get(i));\n          partVals.add(partVal == null ? // could be null for default partition\n            this.conf.getVar(ConfVars.DEFAULTPARTITIONNAME) : partVal.toString());\n        }\n        partName = Warehouse.makePartName(partColSchema, partVals);\n      }\n      String [] names = Utilities.getDbTableName(currentDb, tableName);\n      ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1], partName, isTblLevel);\n      ColumnStatistics colStats = new ColumnStatistics();\n      colStats.setStatsDesc(statsDesc);\n      colStats.setStatsObj(statsObjs);\n      if (!statsObjs.isEmpty()) {\n        stats.add(colStats);\n      }\n    }\n    ftOp.clearFetchContext();\n    return stats;\n  }"
        ],
        [
            "FunctionTask::createTemporaryFunction(CreateFunctionDesc)",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217  \n 218  \n 219  \n 220 -\n 221  \n 222  \n 223  ",
            "  private int createTemporaryFunction(CreateFunctionDesc createFunctionDesc) {\n    try {\n      // Add any required resources\n      FunctionResource[] resources = toFunctionResource(createFunctionDesc.getResources());\n      addFunctionResources(resources);\n\n      Class<?> udfClass = getUdfClass(createFunctionDesc);\n      FunctionInfo registered = FunctionRegistry.registerTemporaryUDF(\n          createFunctionDesc.getFunctionName(), udfClass, resources);\n      if (registered != null) {\n        return 0;\n      }\n      console.printError(\"FAILED: Class \" + createFunctionDesc.getClassName()\n          + \" does not implement UDF, GenericUDF, or UDAF\");\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"FAILED: \" + e.toString());\n      LOG.info(\"create function: \" + StringUtils.stringifyException(e));\n      return 1;\n    } catch (ClassNotFoundException e) {\n\n      console.printError(\"FAILED: Class \" + createFunctionDesc.getClassName() + \" not found\");\n      LOG.info(\"create function: \" + StringUtils.stringifyException(e));\n      return 1;\n    }\n  }",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 +\n 216  \n 217  \n 218  \n 219  \n 220 +\n 221  \n 222  \n 223  ",
            "  private int createTemporaryFunction(CreateFunctionDesc createFunctionDesc) {\n    try {\n      // Add any required resources\n      FunctionResource[] resources = toFunctionResource(createFunctionDesc.getResources());\n      addFunctionResources(resources);\n\n      Class<?> udfClass = getUdfClass(createFunctionDesc);\n      FunctionInfo registered = FunctionRegistry.registerTemporaryUDF(\n          createFunctionDesc.getFunctionName(), udfClass, resources);\n      if (registered != null) {\n        return 0;\n      }\n      console.printError(\"FAILED: Class \" + createFunctionDesc.getClassName()\n          + \" does not implement UDF, GenericUDF, or UDAF\");\n      return 1;\n    } catch (HiveException e) {\n      console.printError(\"FAILED: \" + e.toString());\n      LOG.info(\"create function: \", e);\n      return 1;\n    } catch (ClassNotFoundException e) {\n\n      console.printError(\"FAILED: Class \" + createFunctionDesc.getClassName() + \" not found\");\n      LOG.info(\"create function: \", e);\n      return 1;\n    }\n  }"
        ],
        [
            "DDLTask::showTableProperties(Hive,ShowTblPropertiesDesc)",
            "3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307 -\n3308  \n3309  \n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329 -\n3330  \n3331  \n3332  \n3333 -\n3334  \n3335  \n3336 -\n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  ",
            "  /**\n   * Write the properties of a table to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showTblPrpt\n   *          This is the table we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showTableProperties(Hive db, ShowTblPropertiesDesc showTblPrpt) throws HiveException {\n    String tableName = showTblPrpt.getTableName();\n\n    // show table properties - populate the output stream\n    Table tbl = db.getTable(tableName, false);\n    try {\n      if (tbl == null) {\n        String errMsg = \"Table \" + tableName + \" does not exist\";\n        writeToFile(errMsg, showTblPrpt.getResFile());\n        return 0;\n      }\n\n      LOG.info(\"DDLTask: show properties for \" + tbl.getTableName());\n\n      StringBuilder builder = new StringBuilder();\n      String propertyName = showTblPrpt.getPropertyName();\n      if (propertyName != null) {\n        String propertyValue = tbl.getProperty(propertyName);\n        if (propertyValue == null) {\n          String errMsg = \"Table \" + tableName + \" does not have property: \" + propertyName;\n          builder.append(errMsg);\n        }\n        else {\n          builder.append(propertyValue);\n        }\n      }\n      else {\n        Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());\n        for (Entry<String, String> entry : properties.entrySet()) {\n          appendNonNull(builder, entry.getKey(), true);\n          appendNonNull(builder, entry.getValue());\n        }\n      }\n\n      LOG.info(\"DDLTask: written data for showing properties of \" + tbl.getTableName());\n      writeToFile(builder.toString(), showTblPrpt.getResFile());\n\n    } catch (FileNotFoundException e) {\n      LOG.info(\"show table properties: \" + stringifyException(e));\n      return 1;\n    } catch (IOException e) {\n      LOG.info(\"show table properties: \" + stringifyException(e));\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n\n    return 0;\n  }",
            "3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307  \n3308  \n3309 +\n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331 +\n3332  \n3333  \n3334  \n3335 +\n3336  \n3337  \n3338 +\n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  ",
            "  /**\n   * Write the properties of a table to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showTblPrpt\n   *          This is the table we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showTableProperties(Hive db, ShowTblPropertiesDesc showTblPrpt) throws HiveException {\n    String tableName = showTblPrpt.getTableName();\n\n    // show table properties - populate the output stream\n    Table tbl = db.getTable(tableName, false);\n    try {\n      if (tbl == null) {\n        String errMsg = \"Table \" + tableName + \" does not exist\";\n        writeToFile(errMsg, showTblPrpt.getResFile());\n        return 0;\n      }\n\n      LOG.info(\"DDLTask: show properties for {}\", tableName);\n\n      StringBuilder builder = new StringBuilder();\n      String propertyName = showTblPrpt.getPropertyName();\n      if (propertyName != null) {\n        String propertyValue = tbl.getProperty(propertyName);\n        if (propertyValue == null) {\n          String errMsg = \"Table \" + tableName + \" does not have property: \" + propertyName;\n          builder.append(errMsg);\n        }\n        else {\n          builder.append(propertyValue);\n        }\n      }\n      else {\n        Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());\n        for (Entry<String, String> entry : properties.entrySet()) {\n          appendNonNull(builder, entry.getKey(), true);\n          appendNonNull(builder, entry.getValue());\n        }\n      }\n\n      LOG.info(\"DDLTask: written data for showing properties of {}\", tableName);\n      writeToFile(builder.toString(), showTblPrpt.getResFile());\n\n    } catch (FileNotFoundException e) {\n      LOG.info(\"show table properties: \", e);\n      return 1;\n    } catch (IOException e) {\n      LOG.info(\"show table properties: \", e);\n      return 1;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n\n    return 0;\n  }"
        ],
        [
            "DDLTask::renamePartition(Hive,RenamePartitionDesc)",
            "1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174 -\n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  ",
            "  /**\n   * Rename a partition in a table\n   *\n   * @param db\n   *          Database to rename the partition.\n   * @param renamePartitionDesc\n   *          rename old Partition to new one.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   */\n  private int renamePartition(Hive db, RenamePartitionDesc renamePartitionDesc) throws HiveException {\n    String tableName = renamePartitionDesc.getTableName();\n    LinkedHashMap<String, String> oldPartSpec = renamePartitionDesc.getOldPartSpec();\n\n    if (!allowOperationInReplicationScope(db, tableName, oldPartSpec, renamePartitionDesc.getReplicationSpec())) {\n      // no rename, the table is missing either due to drop/rename which follows the current rename.\n      // or the existing table is newer than our update.\n      LOG.debug(\"DDLTask: Rename Partition is skipped as table {} / partition {} is newer than update\",\n              tableName,\n              FileUtils.makePartName(new ArrayList(oldPartSpec.keySet()), new ArrayList(oldPartSpec.values())));\n      return 0;\n    }\n\n    String names[] = Utilities.getDbTableName(tableName);\n    if (Utils.isBootstrapDumpInProgress(db, names[0])) {\n      LOG.error(\"DDLTask: Rename Partition not allowed as bootstrap dump in progress\");\n      throw new HiveException(\"Rename Partition: Not allowed as bootstrap dump in progress\");\n    }\n\n    Table tbl = db.getTable(tableName);\n    Partition oldPart = db.getPartition(tbl, oldPartSpec, false);\n    if (oldPart == null) {\n      String partName = FileUtils.makePartName(new ArrayList<String>(oldPartSpec.keySet()),\n          new ArrayList<String>(oldPartSpec.values()));\n      throw new HiveException(\"Rename partition: source partition [\" + partName\n          + \"] does not exist.\");\n    }\n    Partition part = db.getPartition(tbl, oldPartSpec, false);\n    part.setValues(renamePartitionDesc.getNewPartSpec());\n    db.renamePartition(tbl, oldPartSpec, part);\n    Partition newPart = db.getPartition(tbl, renamePartitionDesc.getNewPartSpec(), false);\n    work.getInputs().add(new ReadEntity(oldPart));\n    // We've already obtained a lock on the table, don't lock the partition too\n    addIfAbsentByName(new WriteEntity(newPart, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }",
            "1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172 +\n1173  \n1174  \n1175 +\n1176 +\n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  ",
            "  /**\n   * Rename a partition in a table\n   *\n   * @param db\n   *          Database to rename the partition.\n   * @param renamePartitionDesc\n   *          rename old Partition to new one.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   */\n  private int renamePartition(Hive db, RenamePartitionDesc renamePartitionDesc) throws HiveException {\n    String tableName = renamePartitionDesc.getTableName();\n    LinkedHashMap<String, String> oldPartSpec = renamePartitionDesc.getOldPartSpec();\n\n    if (!allowOperationInReplicationScope(db, tableName, oldPartSpec, renamePartitionDesc.getReplicationSpec())) {\n      // no rename, the table is missing either due to drop/rename which follows the current rename.\n      // or the existing table is newer than our update.\n      if (LOG.isDebugEnabled()) {\n      LOG.debug(\"DDLTask: Rename Partition is skipped as table {} / partition {} is newer than update\",\n              tableName,\n              FileUtils.makePartName(new ArrayList<>(oldPartSpec.keySet()), new ArrayList<>(oldPartSpec.values())));\n      }\n      return 0;\n    }\n\n    String names[] = Utilities.getDbTableName(tableName);\n    if (Utils.isBootstrapDumpInProgress(db, names[0])) {\n      LOG.error(\"DDLTask: Rename Partition not allowed as bootstrap dump in progress\");\n      throw new HiveException(\"Rename Partition: Not allowed as bootstrap dump in progress\");\n    }\n\n    Table tbl = db.getTable(tableName);\n    Partition oldPart = db.getPartition(tbl, oldPartSpec, false);\n    if (oldPart == null) {\n      String partName = FileUtils.makePartName(new ArrayList<String>(oldPartSpec.keySet()),\n          new ArrayList<String>(oldPartSpec.values()));\n      throw new HiveException(\"Rename partition: source partition [\" + partName\n          + \"] does not exist.\");\n    }\n    Partition part = db.getPartition(tbl, oldPartSpec, false);\n    part.setValues(renamePartitionDesc.getNewPartSpec());\n    db.renamePartition(tbl, oldPartSpec, part);\n    Partition newPart = db.getPartition(tbl, renamePartitionDesc.getNewPartSpec(), false);\n    work.getInputs().add(new ReadEntity(oldPart));\n    // We've already obtained a lock on the table, don't lock the partition too\n    addIfAbsentByName(new WriteEntity(newPart, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }"
        ],
        [
            "StatsNoJobTask::updatePartitions(Hive)",
            " 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337  \n 338  \n 339  \n 340  ",
            "  private int updatePartitions(Hive db) throws InvalidOperationException, HiveException {\n    if (!partUpdates.isEmpty()) {\n      List<Partition> updatedParts = Lists.newArrayList(partUpdates.values());\n      if (updatedParts.contains(null) && work.isStatsReliable()) {\n        LOG.debug(\"Stats requested to be reliable. Empty stats found and hence failing the task.\");\n        return -1;\n      } else {\n        LOG.debug(\"Bulk updating partitions..\");\n        EnvironmentContext environmentContext = new EnvironmentContext();\n        environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED, StatsSetupConst.TASK);\n        db.alterPartitions(tableFullName, Lists.newArrayList(partUpdates.values()),\n            environmentContext);\n        LOG.debug(\"Bulk updated \" + partUpdates.values().size() + \" partitions.\");\n      }\n    }\n    return 0;\n  }",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  ",
            "  private int updatePartitions(Hive db) throws InvalidOperationException, HiveException {\n    if (!partUpdates.isEmpty()) {\n      List<Partition> updatedParts = Lists.newArrayList(partUpdates.values());\n      if (updatedParts.contains(null) && work.isStatsReliable()) {\n        LOG.debug(\"Stats requested to be reliable. Empty stats found and hence failing the task.\");\n        return -1;\n      } else {\n        LOG.debug(\"Bulk updating partitions..\");\n        EnvironmentContext environmentContext = new EnvironmentContext();\n        environmentContext.putToProperties(StatsSetupConst.STATS_GENERATED, StatsSetupConst.TASK);\n        db.alterPartitions(tableFullName, Lists.newArrayList(partUpdates.values()),\n            environmentContext);\n        LOG.debug(\"Bulk updated {} partitions.\", partUpdates.values().size());\n      }\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::createView(Hive,CreateViewDesc)",
            "4814  \n4815  \n4816  \n4817  \n4818  \n4819  \n4820  \n4821  \n4822  \n4823  \n4824  \n4825  \n4826  \n4827  \n4828  \n4829  \n4830  \n4831  \n4832  \n4833  \n4834  \n4835  \n4836  \n4837  \n4838  \n4839  \n4840  \n4841  \n4842  \n4843  \n4844  \n4845  \n4846  \n4847  \n4848  \n4849  \n4850  \n4851  \n4852  \n4853  \n4854  \n4855  \n4856  \n4857  \n4858  \n4859  \n4860  \n4861  \n4862  \n4863  \n4864  \n4865  \n4866  \n4867  \n4868  \n4869  \n4870  \n4871  \n4872  \n4873  \n4874  \n4875  \n4876  \n4877  \n4878  \n4879  \n4880  \n4881  \n4882  \n4883  \n4884  \n4885  \n4886  \n4887  \n4888  \n4889  \n4890  \n4891  \n4892  \n4893  \n4894  \n4895  \n4896  \n4897  \n4898  \n4899  \n4900  \n4901  \n4902  \n4903  \n4904  \n4905  \n4906  \n4907  \n4908  \n4909  \n4910 -\n4911 -\n4912  \n4913  \n4914 -\n4915 -\n4916  \n4917  \n4918  \n4919  \n4920  \n4921  \n4922  \n4923  \n4924  \n4925  \n4926  \n4927  \n4928  \n4929  \n4930  \n4931  \n4932  \n4933  \n4934  \n4935  \n4936  \n4937  \n4938  \n4939  \n4940  \n4941  \n4942  \n4943  \n4944  \n4945  \n4946  \n4947  \n4948  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (crtView.getOrReplace() && oldview != null) {\n      if (!crtView.isMaterialized()) {\n        // replace existing view\n        // remove the existing partition columns from the field schema\n        oldview.setViewOriginalText(crtView.getViewOriginalText());\n        oldview.setViewExpandedText(crtView.getViewExpandedText());\n        oldview.setFields(crtView.getSchema());\n        if (crtView.getComment() != null) {\n          oldview.setProperty(\"comment\", crtView.getComment());\n        }\n        if (crtView.getTblProps() != null) {\n          oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n        }\n        oldview.setPartCols(crtView.getPartCols());\n        if (crtView.getInputFormat() != null) {\n          oldview.setInputFormatClass(crtView.getInputFormat());\n        }\n        if (crtView.getOutputFormat() != null) {\n          oldview.setOutputFormatClass(crtView.getOutputFormat());\n        }\n        oldview.checkValidity(null);\n        try {\n          db.alterTable(crtView.getViewName(), oldview, null);\n        } catch (InvalidOperationException e) {\n          throw new HiveException(e);\n        }\n        addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n      } else {\n        // This is a replace, so we need an exclusive lock\n        addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_EXCLUSIVE));\n      }\n    } else {\n      // create new view\n      Table tbl = db.newTable(crtView.getViewName());\n      tbl.setViewOriginalText(crtView.getViewOriginalText());\n      tbl.setViewExpandedText(crtView.getViewExpandedText());\n      if (crtView.isMaterialized()) {\n        tbl.setRewriteEnabled(crtView.isRewriteEnabled());\n        tbl.setTableType(TableType.MATERIALIZED_VIEW);\n      } else {\n        tbl.setTableType(TableType.VIRTUAL_VIEW);\n      }\n      tbl.setSerializationLib(null);\n      tbl.clearSerDeInfo();\n      tbl.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        tbl.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        tbl.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n\n      if (crtView.getPartCols() != null) {\n        tbl.setPartCols(crtView.getPartCols());\n      }\n\n      if (crtView.getInputFormat() != null) {\n        tbl.setInputFormatClass(crtView.getInputFormat());\n      }\n\n      if (crtView.getOutputFormat() != null) {\n        tbl.setOutputFormatClass(crtView.getOutputFormat());\n      }\n\n      if (crtView.isMaterialized()) {\n        if (crtView.getLocation() != null) {\n          tbl.setDataLocation(new Path(crtView.getLocation()));\n        }\n\n        if (crtView.getStorageHandler() != null) {\n          tbl.setProperty(\n                  org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE,\n                  crtView.getStorageHandler());\n        }\n        HiveStorageHandler storageHandler = tbl.getStorageHandler();\n\n        /*\n         * If the user didn't specify a SerDe, we use the default.\n         */\n        String serDeClassName;\n        if (crtView.getSerde() == null) {\n          if (storageHandler == null) {\n            serDeClassName = PlanUtils.getDefaultSerDe().getName();\n            LOG.info(\"Default to \" + serDeClassName\n                    + \" for materialized view \" + crtView.getViewName());\n          } else {\n            serDeClassName = storageHandler.getSerDeClass().getName();\n            LOG.info(\"Use StorageHandler-supplied \" + serDeClassName\n                    + \" for materialized view \" + crtView.getViewName());\n          }\n        } else {\n          // let's validate that the serde exists\n          serDeClassName = crtView.getSerde();\n          DDLTask.validateSerDe(serDeClassName, conf);\n        }\n        tbl.setSerializationLib(serDeClassName);\n\n        // To remain consistent, we need to set input and output formats both\n        // at the table level and the storage handler level.\n        tbl.setInputFormatClass(crtView.getInputFormat());\n        tbl.setOutputFormatClass(crtView.getOutputFormat());\n        if (crtView.getInputFormat() != null && !crtView.getInputFormat().isEmpty()) {\n          tbl.getSd().setInputFormat(tbl.getInputFormatClass().getName());\n        }\n        if (crtView.getOutputFormat() != null && !crtView.getOutputFormat().isEmpty()) {\n          tbl.getSd().setOutputFormat(tbl.getOutputFormatClass().getName());\n        }\n      }\n\n      db.createTable(tbl, crtView.getIfNotExists());\n      // Add to cache if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        HiveMaterializedViewsRegistry.get().addMaterializedView(tbl);\n      }\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      SessionState.get().getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }",
            "4816  \n4817  \n4818  \n4819  \n4820  \n4821  \n4822  \n4823  \n4824  \n4825  \n4826  \n4827  \n4828  \n4829  \n4830  \n4831  \n4832  \n4833  \n4834  \n4835  \n4836  \n4837  \n4838  \n4839  \n4840  \n4841  \n4842  \n4843  \n4844  \n4845  \n4846  \n4847  \n4848  \n4849  \n4850  \n4851  \n4852  \n4853  \n4854  \n4855  \n4856  \n4857  \n4858  \n4859  \n4860  \n4861  \n4862  \n4863  \n4864  \n4865  \n4866  \n4867  \n4868  \n4869  \n4870  \n4871  \n4872  \n4873  \n4874  \n4875  \n4876  \n4877  \n4878  \n4879  \n4880  \n4881  \n4882  \n4883  \n4884  \n4885  \n4886  \n4887  \n4888  \n4889  \n4890  \n4891  \n4892  \n4893  \n4894  \n4895  \n4896  \n4897  \n4898  \n4899  \n4900  \n4901  \n4902  \n4903  \n4904  \n4905  \n4906  \n4907  \n4908  \n4909  \n4910  \n4911  \n4912 +\n4913 +\n4914  \n4915  \n4916 +\n4917 +\n4918  \n4919  \n4920  \n4921  \n4922  \n4923  \n4924  \n4925  \n4926  \n4927  \n4928  \n4929  \n4930  \n4931  \n4932  \n4933  \n4934  \n4935  \n4936  \n4937  \n4938  \n4939  \n4940  \n4941  \n4942  \n4943  \n4944  \n4945  \n4946  \n4947  \n4948  \n4949  \n4950  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (crtView.getOrReplace() && oldview != null) {\n      if (!crtView.isMaterialized()) {\n        // replace existing view\n        // remove the existing partition columns from the field schema\n        oldview.setViewOriginalText(crtView.getViewOriginalText());\n        oldview.setViewExpandedText(crtView.getViewExpandedText());\n        oldview.setFields(crtView.getSchema());\n        if (crtView.getComment() != null) {\n          oldview.setProperty(\"comment\", crtView.getComment());\n        }\n        if (crtView.getTblProps() != null) {\n          oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n        }\n        oldview.setPartCols(crtView.getPartCols());\n        if (crtView.getInputFormat() != null) {\n          oldview.setInputFormatClass(crtView.getInputFormat());\n        }\n        if (crtView.getOutputFormat() != null) {\n          oldview.setOutputFormatClass(crtView.getOutputFormat());\n        }\n        oldview.checkValidity(null);\n        try {\n          db.alterTable(crtView.getViewName(), oldview, null);\n        } catch (InvalidOperationException e) {\n          throw new HiveException(e);\n        }\n        addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n      } else {\n        // This is a replace, so we need an exclusive lock\n        addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_EXCLUSIVE));\n      }\n    } else {\n      // create new view\n      Table tbl = db.newTable(crtView.getViewName());\n      tbl.setViewOriginalText(crtView.getViewOriginalText());\n      tbl.setViewExpandedText(crtView.getViewExpandedText());\n      if (crtView.isMaterialized()) {\n        tbl.setRewriteEnabled(crtView.isRewriteEnabled());\n        tbl.setTableType(TableType.MATERIALIZED_VIEW);\n      } else {\n        tbl.setTableType(TableType.VIRTUAL_VIEW);\n      }\n      tbl.setSerializationLib(null);\n      tbl.clearSerDeInfo();\n      tbl.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        tbl.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        tbl.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n\n      if (crtView.getPartCols() != null) {\n        tbl.setPartCols(crtView.getPartCols());\n      }\n\n      if (crtView.getInputFormat() != null) {\n        tbl.setInputFormatClass(crtView.getInputFormat());\n      }\n\n      if (crtView.getOutputFormat() != null) {\n        tbl.setOutputFormatClass(crtView.getOutputFormat());\n      }\n\n      if (crtView.isMaterialized()) {\n        if (crtView.getLocation() != null) {\n          tbl.setDataLocation(new Path(crtView.getLocation()));\n        }\n\n        if (crtView.getStorageHandler() != null) {\n          tbl.setProperty(\n                  org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE,\n                  crtView.getStorageHandler());\n        }\n        HiveStorageHandler storageHandler = tbl.getStorageHandler();\n\n        /*\n         * If the user didn't specify a SerDe, we use the default.\n         */\n        String serDeClassName;\n        if (crtView.getSerde() == null) {\n          if (storageHandler == null) {\n            serDeClassName = PlanUtils.getDefaultSerDe().getName();\n            LOG.info(\"Default to {} for materialized view {}\", serDeClassName,\n              crtView.getViewName());\n          } else {\n            serDeClassName = storageHandler.getSerDeClass().getName();\n            LOG.info(\"Use StorageHandler-supplied {} for materialized view {}\",\n              serDeClassName, crtView.getViewName());\n          }\n        } else {\n          // let's validate that the serde exists\n          serDeClassName = crtView.getSerde();\n          DDLTask.validateSerDe(serDeClassName, conf);\n        }\n        tbl.setSerializationLib(serDeClassName);\n\n        // To remain consistent, we need to set input and output formats both\n        // at the table level and the storage handler level.\n        tbl.setInputFormatClass(crtView.getInputFormat());\n        tbl.setOutputFormatClass(crtView.getOutputFormat());\n        if (crtView.getInputFormat() != null && !crtView.getInputFormat().isEmpty()) {\n          tbl.getSd().setInputFormat(tbl.getInputFormatClass().getName());\n        }\n        if (crtView.getOutputFormat() != null && !crtView.getOutputFormat().isEmpty()) {\n          tbl.getSd().setOutputFormat(tbl.getOutputFormatClass().getName());\n        }\n      }\n\n      db.createTable(tbl, crtView.getIfNotExists());\n      // Add to cache if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        HiveMaterializedViewsRegistry.get().addMaterializedView(tbl);\n      }\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      SessionState.get().getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }"
        ],
        [
            "MoveTask::releaseLocks(LoadTableDesc)",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234 -\n 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "  private void releaseLocks(LoadTableDesc ltd) throws HiveException {\n    // nothing needs to be done\n    if (!conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY)) {\n      return;\n    }\n\n    Context ctx = driverContext.getCtx();\n    if(ctx.getHiveTxnManager().supportsAcid()) {\n      //Acid LM doesn't maintain getOutputLockObjects(); this 'if' just makes it more explicit\n      return;\n    }\n    HiveLockManager lockMgr = ctx.getHiveTxnManager().getLockManager();\n    WriteEntity output = ctx.getLoadTableOutputMap().get(ltd);\n    List<HiveLockObj> lockObjects = ctx.getOutputLockObjects().get(output);\n    if (lockObjects == null) {\n      return;\n    }\n\n    for (HiveLockObj lockObj : lockObjects) {\n      List<HiveLock> locks = lockMgr.getLocks(lockObj.getObj(), false, true);\n      for (HiveLock lock : locks) {\n        if (lock.getHiveLockMode() == lockObj.getMode()) {\n          if (ctx.getHiveLocks().remove(lock)) {\n            LOG.info(\"about to release lock for output: \" + output.toString() +\n                \" lock: \" + lock.getHiveLockObject().getName());\n            try {\n              lockMgr.unlock(lock);\n            } catch (LockException le) {\n              // should be OK since the lock is ephemeral and will eventually be deleted\n              // when the query finishes and zookeeper session is closed.\n              LOG.warn(\"Could not release lock \" + lock.getHiveLockObject().getName());\n            }\n          }\n        }\n      }\n    }\n  }",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 +\n 234 +\n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "  private void releaseLocks(LoadTableDesc ltd) throws HiveException {\n    // nothing needs to be done\n    if (!conf.getBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY)) {\n      return;\n    }\n\n    Context ctx = driverContext.getCtx();\n    if(ctx.getHiveTxnManager().supportsAcid()) {\n      //Acid LM doesn't maintain getOutputLockObjects(); this 'if' just makes it more explicit\n      return;\n    }\n    HiveLockManager lockMgr = ctx.getHiveTxnManager().getLockManager();\n    WriteEntity output = ctx.getLoadTableOutputMap().get(ltd);\n    List<HiveLockObj> lockObjects = ctx.getOutputLockObjects().get(output);\n    if (lockObjects == null) {\n      return;\n    }\n\n    for (HiveLockObj lockObj : lockObjects) {\n      List<HiveLock> locks = lockMgr.getLocks(lockObj.getObj(), false, true);\n      for (HiveLock lock : locks) {\n        if (lock.getHiveLockMode() == lockObj.getMode()) {\n          if (ctx.getHiveLocks().remove(lock)) {\n            LOG.info(\"about to release lock for output: {} lock: {}\", output,\n              lock.getHiveLockObject().getName());\n            try {\n              lockMgr.unlock(lock);\n            } catch (LockException le) {\n              // should be OK since the lock is ephemeral and will eventually be deleted\n              // when the query finishes and zookeeper session is closed.\n              LOG.warn(\"Could not release lock {}\", lock.getHiveLockObject().getName());\n            }\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "DDLTask::showCompactions(Hive,ShowCompactionsDesc)",
            "2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996 -\n2997  \n2998  \n2999  \n3000  \n3001  \n3002  ",
            "  private int showCompactions(Hive db, ShowCompactionsDesc desc) throws HiveException {\n    // Call the metastore to get the status of all known compactions (completed get purged eventually)\n    ShowCompactResponse rsp = db.showCompactions();\n\n    // Write the results into the file\n    final String noVal = \" --- \";\n\n    DataOutputStream os = getOutputStream(desc.getResFile());\n    try {\n      // Write a header\n      os.writeBytes(\"CompactionId\");\n      os.write(separator);\n      os.writeBytes(\"Database\");\n      os.write(separator);\n      os.writeBytes(\"Table\");\n      os.write(separator);\n      os.writeBytes(\"Partition\");\n      os.write(separator);\n      os.writeBytes(\"Type\");\n      os.write(separator);\n      os.writeBytes(\"State\");\n      os.write(separator);\n      os.writeBytes(\"Worker\");\n      os.write(separator);\n      os.writeBytes(\"Start Time\");\n      os.write(separator);\n      os.writeBytes(\"Duration(ms)\");\n      os.write(separator);\n      os.writeBytes(\"HadoopJobId\");\n      os.write(terminator);\n\n      if (rsp.getCompacts() != null) {\n        for (ShowCompactResponseElement e : rsp.getCompacts()) {\n          os.writeBytes(Long.toString(e.getId()));\n          os.write(separator);\n          os.writeBytes(e.getDbname());\n          os.write(separator);\n          os.writeBytes(e.getTablename());\n          os.write(separator);\n          String part = e.getPartitionname();\n          os.writeBytes(part == null ? noVal : part);\n          os.write(separator);\n          os.writeBytes(e.getType().toString());\n          os.write(separator);\n          os.writeBytes(e.getState());\n          os.write(separator);\n          String wid = e.getWorkerid();\n          os.writeBytes(wid == null ? noVal : wid);\n          os.write(separator);\n          os.writeBytes(e.isSetStart() ? Long.toString(e.getStart()) : noVal);\n          os.write(separator);\n          os.writeBytes(e.isSetEndTime() ? Long.toString(e.getEndTime() - e.getStart()) : noVal);\n          os.write(separator);\n          os.writeBytes(e.isSetHadoopJobId() ?  e.getHadoopJobId() : noVal);\n          os.write(terminator);\n        }\n      }\n    } catch (IOException e) {\n      LOG.warn(\"show compactions: \" + stringifyException(e));\n      return 1;\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }",
            "2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998 +\n2999  \n3000  \n3001  \n3002  \n3003  \n3004  ",
            "  private int showCompactions(Hive db, ShowCompactionsDesc desc) throws HiveException {\n    // Call the metastore to get the status of all known compactions (completed get purged eventually)\n    ShowCompactResponse rsp = db.showCompactions();\n\n    // Write the results into the file\n    final String noVal = \" --- \";\n\n    DataOutputStream os = getOutputStream(desc.getResFile());\n    try {\n      // Write a header\n      os.writeBytes(\"CompactionId\");\n      os.write(separator);\n      os.writeBytes(\"Database\");\n      os.write(separator);\n      os.writeBytes(\"Table\");\n      os.write(separator);\n      os.writeBytes(\"Partition\");\n      os.write(separator);\n      os.writeBytes(\"Type\");\n      os.write(separator);\n      os.writeBytes(\"State\");\n      os.write(separator);\n      os.writeBytes(\"Worker\");\n      os.write(separator);\n      os.writeBytes(\"Start Time\");\n      os.write(separator);\n      os.writeBytes(\"Duration(ms)\");\n      os.write(separator);\n      os.writeBytes(\"HadoopJobId\");\n      os.write(terminator);\n\n      if (rsp.getCompacts() != null) {\n        for (ShowCompactResponseElement e : rsp.getCompacts()) {\n          os.writeBytes(Long.toString(e.getId()));\n          os.write(separator);\n          os.writeBytes(e.getDbname());\n          os.write(separator);\n          os.writeBytes(e.getTablename());\n          os.write(separator);\n          String part = e.getPartitionname();\n          os.writeBytes(part == null ? noVal : part);\n          os.write(separator);\n          os.writeBytes(e.getType().toString());\n          os.write(separator);\n          os.writeBytes(e.getState());\n          os.write(separator);\n          String wid = e.getWorkerid();\n          os.writeBytes(wid == null ? noVal : wid);\n          os.write(separator);\n          os.writeBytes(e.isSetStart() ? Long.toString(e.getStart()) : noVal);\n          os.write(separator);\n          os.writeBytes(e.isSetEndTime() ? Long.toString(e.getEndTime() - e.getStart()) : noVal);\n          os.write(separator);\n          os.writeBytes(e.isSetHadoopJobId() ?  e.getHadoopJobId() : noVal);\n          os.write(terminator);\n        }\n      }\n    } catch (IOException e) {\n      LOG.warn(\"show compactions: \", e);\n      return 1;\n    } finally {\n      IOUtils.closeStream(os);\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::createTableLike(Hive,CreateTableLikeDesc)",
            "4666  \n4667  \n4668  \n4669  \n4670  \n4671  \n4672  \n4673  \n4674  \n4675  \n4676  \n4677  \n4678  \n4679  \n4680  \n4681  \n4682  \n4683  \n4684  \n4685  \n4686  \n4687  \n4688  \n4689  \n4690  \n4691  \n4692  \n4693  \n4694  \n4695  \n4696  \n4697  \n4698  \n4699  \n4700  \n4701 -\n4702  \n4703  \n4704  \n4705  \n4706  \n4707  \n4708  \n4709  \n4710  \n4711  \n4712  \n4713  \n4714  \n4715  \n4716  \n4717  \n4718  \n4719  \n4720  \n4721  \n4722  \n4723  \n4724  \n4725  \n4726  \n4727  \n4728  \n4729  \n4730  \n4731  \n4732  \n4733  \n4734  \n4735  \n4736  \n4737  \n4738  \n4739  \n4740  \n4741  \n4742  \n4743  \n4744  \n4745  \n4746  \n4747  \n4748  \n4749  \n4750  \n4751  \n4752  \n4753  \n4754  \n4755  \n4756  \n4757  \n4758  \n4759  \n4760  \n4761  \n4762  \n4763  \n4764  \n4765  \n4766  \n4767  \n4768  \n4769  \n4770  \n4771  \n4772  \n4773  \n4774  \n4775  \n4776  \n4777  \n4778 -\n4779  \n4780  \n4781  \n4782  \n4783  \n4784  \n4785  \n4786  \n4787  \n4788  \n4789  \n4790  \n4791  \n4792  \n4793  \n4794  \n4795  \n4796  \n4797  \n4798  \n4799  \n4800  \n4801  \n4802  \n4803  \n4804  \n4805  \n4806  \n4807  \n4808  \n4809  \n4810  \n4811  \n4812  ",
            "  /**\n   * Create a new table like an existing table.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtTbl\n   *          This is the table we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws Exception {\n    // Get the existing table\n    Table oldtbl = db.getTable(crtTbl.getLikeTableName());\n    Table tbl;\n    if (oldtbl.getTableType() == TableType.VIRTUAL_VIEW ||\n        oldtbl.getTableType() == TableType.MATERIALIZED_VIEW) {\n      String targetTableName = crtTbl.getTableName();\n      tbl=db.newTable(targetTableName);\n\n      if (crtTbl.getTblProps() != null) {\n        tbl.getTTable().getParameters().putAll(crtTbl.getTblProps());\n      }\n\n      tbl.setTableType(TableType.MANAGED_TABLE);\n\n      if (crtTbl.isExternal()) {\n        tbl.setProperty(\"EXTERNAL\", \"TRUE\");\n        tbl.setTableType(TableType.EXTERNAL_TABLE);\n      }\n\n      tbl.setFields(oldtbl.getCols());\n      tbl.setPartCols(oldtbl.getPartCols());\n\n      if (crtTbl.getDefaultSerName() == null) {\n        LOG.info(\"Default to LazySimpleSerDe for table \" + crtTbl.getTableName());\n        tbl.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n      } else {\n        // let's validate that the serde exists\n        validateSerDe(crtTbl.getDefaultSerName());\n        tbl.setSerializationLib(crtTbl.getDefaultSerName());\n      }\n\n      if (crtTbl.getDefaultSerdeProps() != null) {\n        Iterator<Entry<String, String>> iter = crtTbl.getDefaultSerdeProps().entrySet()\n            .iterator();\n        while (iter.hasNext()) {\n          Entry<String, String> m = iter.next();\n          tbl.setSerdeParam(m.getKey(), m.getValue());\n        }\n      }\n\n      tbl.setInputFormatClass(crtTbl.getDefaultInputFormat());\n      tbl.setOutputFormatClass(crtTbl.getDefaultOutputFormat());\n\n      tbl.getTTable().getSd().setInputFormat(\n          tbl.getInputFormatClass().getName());\n      tbl.getTTable().getSd().setOutputFormat(\n          tbl.getOutputFormatClass().getName());\n    } else {\n      tbl=oldtbl;\n\n      // find out database name and table name of target table\n      String targetTableName = crtTbl.getTableName();\n      String[] names = Utilities.getDbTableName(targetTableName);\n\n      tbl.setDbName(names[0]);\n      tbl.setTableName(names[1]);\n\n      // using old table object, hence reset the owner to current user for new table.\n      tbl.setOwner(SessionState.getUserFromAuthenticator());\n\n      if (crtTbl.getLocation() != null) {\n        tbl.setDataLocation(new Path(crtTbl.getLocation()));\n      } else {\n        tbl.unsetDataLocation();\n      }\n\n      Class<? extends Deserializer> serdeClass = oldtbl.getDeserializerClass();\n\n      Map<String, String> params = tbl.getParameters();\n      // We should copy only those table parameters that are specified in the config.\n      SerDeSpec spec = AnnotationUtils.getAnnotation(serdeClass, SerDeSpec.class);\n      String paramsStr = HiveConf.getVar(conf, HiveConf.ConfVars.DDL_CTL_PARAMETERS_WHITELIST);\n\n      Set<String> retainer = new HashSet<String>();\n      // for non-native table, property storage_handler should be retained\n      retainer.add(META_TABLE_STORAGE);\n      if (spec != null && spec.schemaProps() != null) {\n        retainer.addAll(Arrays.asList(spec.schemaProps()));\n      }\n      if (paramsStr != null) {\n        retainer.addAll(Arrays.asList(paramsStr.split(\",\")));\n      }\n      if (!retainer.isEmpty()) {\n        params.keySet().retainAll(retainer);\n      } else {\n        params.clear();\n      }\n\n      if (crtTbl.getTblProps() != null) {\n        params.putAll(crtTbl.getTblProps());\n      }\n\n      if (crtTbl.isUserStorageFormat()) {\n        tbl.setInputFormatClass(crtTbl.getDefaultInputFormat());\n        tbl.setOutputFormatClass(crtTbl.getDefaultOutputFormat());\n        tbl.getTTable().getSd().setInputFormat(\n        tbl.getInputFormatClass().getName());\n        tbl.getTTable().getSd().setOutputFormat(\n        tbl.getOutputFormatClass().getName());\n        if (crtTbl.getDefaultSerName() == null) {\n          LOG.info(\"Default to LazySimpleSerDe for like table \" + crtTbl.getTableName());\n          tbl.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n        } else {\n          // let's validate that the serde exists\n          validateSerDe(crtTbl.getDefaultSerName());\n          tbl.setSerializationLib(crtTbl.getDefaultSerName());\n        }\n      }\n\n      tbl.getTTable().setTemporary(crtTbl.isTemporary());\n\n      if (crtTbl.isExternal()) {\n        tbl.setProperty(\"EXTERNAL\", \"TRUE\");\n        tbl.setTableType(TableType.EXTERNAL_TABLE);\n      } else {\n        tbl.getParameters().remove(\"EXTERNAL\");\n      }\n    }\n\n    // If location is specified - ensure that it is a full qualified name\n    if (DDLTask.doesTableNeedLocation(tbl)) {\n      makeLocationQualified(tbl.getDbName(), tbl.getTTable().getSd(), tbl.getTableName(), conf);\n    }\n\n    if (crtTbl.getLocation() == null && !tbl.isPartitioned()\n        && conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n      StatsSetupConst.setStatsStateForCreateTable(tbl.getTTable().getParameters(),\n          MetaStoreUtils.getColumnNames(tbl.getCols()), StatsSetupConst.TRUE);\n    }\n\n    // create the table\n    db.createTable(tbl, crtTbl.getIfNotExists());\n    addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }",
            "4668  \n4669  \n4670  \n4671  \n4672  \n4673  \n4674  \n4675  \n4676  \n4677  \n4678  \n4679  \n4680  \n4681  \n4682  \n4683  \n4684  \n4685  \n4686  \n4687  \n4688  \n4689  \n4690  \n4691  \n4692  \n4693  \n4694  \n4695  \n4696  \n4697  \n4698  \n4699  \n4700  \n4701  \n4702  \n4703 +\n4704  \n4705  \n4706  \n4707  \n4708  \n4709  \n4710  \n4711  \n4712  \n4713  \n4714  \n4715  \n4716  \n4717  \n4718  \n4719  \n4720  \n4721  \n4722  \n4723  \n4724  \n4725  \n4726  \n4727  \n4728  \n4729  \n4730  \n4731  \n4732  \n4733  \n4734  \n4735  \n4736  \n4737  \n4738  \n4739  \n4740  \n4741  \n4742  \n4743  \n4744  \n4745  \n4746  \n4747  \n4748  \n4749  \n4750  \n4751  \n4752  \n4753  \n4754  \n4755  \n4756  \n4757  \n4758  \n4759  \n4760  \n4761  \n4762  \n4763  \n4764  \n4765  \n4766  \n4767  \n4768  \n4769  \n4770  \n4771  \n4772  \n4773  \n4774  \n4775  \n4776  \n4777  \n4778  \n4779  \n4780 +\n4781  \n4782  \n4783  \n4784  \n4785  \n4786  \n4787  \n4788  \n4789  \n4790  \n4791  \n4792  \n4793  \n4794  \n4795  \n4796  \n4797  \n4798  \n4799  \n4800  \n4801  \n4802  \n4803  \n4804  \n4805  \n4806  \n4807  \n4808  \n4809  \n4810  \n4811  \n4812  \n4813  \n4814  ",
            "  /**\n   * Create a new table like an existing table.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtTbl\n   *          This is the table we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws Exception {\n    // Get the existing table\n    Table oldtbl = db.getTable(crtTbl.getLikeTableName());\n    Table tbl;\n    if (oldtbl.getTableType() == TableType.VIRTUAL_VIEW ||\n        oldtbl.getTableType() == TableType.MATERIALIZED_VIEW) {\n      String targetTableName = crtTbl.getTableName();\n      tbl=db.newTable(targetTableName);\n\n      if (crtTbl.getTblProps() != null) {\n        tbl.getTTable().getParameters().putAll(crtTbl.getTblProps());\n      }\n\n      tbl.setTableType(TableType.MANAGED_TABLE);\n\n      if (crtTbl.isExternal()) {\n        tbl.setProperty(\"EXTERNAL\", \"TRUE\");\n        tbl.setTableType(TableType.EXTERNAL_TABLE);\n      }\n\n      tbl.setFields(oldtbl.getCols());\n      tbl.setPartCols(oldtbl.getPartCols());\n\n      if (crtTbl.getDefaultSerName() == null) {\n        LOG.info(\"Default to LazySimpleSerDe for table {}\", targetTableName);\n        tbl.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n      } else {\n        // let's validate that the serde exists\n        validateSerDe(crtTbl.getDefaultSerName());\n        tbl.setSerializationLib(crtTbl.getDefaultSerName());\n      }\n\n      if (crtTbl.getDefaultSerdeProps() != null) {\n        Iterator<Entry<String, String>> iter = crtTbl.getDefaultSerdeProps().entrySet()\n            .iterator();\n        while (iter.hasNext()) {\n          Entry<String, String> m = iter.next();\n          tbl.setSerdeParam(m.getKey(), m.getValue());\n        }\n      }\n\n      tbl.setInputFormatClass(crtTbl.getDefaultInputFormat());\n      tbl.setOutputFormatClass(crtTbl.getDefaultOutputFormat());\n\n      tbl.getTTable().getSd().setInputFormat(\n          tbl.getInputFormatClass().getName());\n      tbl.getTTable().getSd().setOutputFormat(\n          tbl.getOutputFormatClass().getName());\n    } else {\n      tbl=oldtbl;\n\n      // find out database name and table name of target table\n      String targetTableName = crtTbl.getTableName();\n      String[] names = Utilities.getDbTableName(targetTableName);\n\n      tbl.setDbName(names[0]);\n      tbl.setTableName(names[1]);\n\n      // using old table object, hence reset the owner to current user for new table.\n      tbl.setOwner(SessionState.getUserFromAuthenticator());\n\n      if (crtTbl.getLocation() != null) {\n        tbl.setDataLocation(new Path(crtTbl.getLocation()));\n      } else {\n        tbl.unsetDataLocation();\n      }\n\n      Class<? extends Deserializer> serdeClass = oldtbl.getDeserializerClass();\n\n      Map<String, String> params = tbl.getParameters();\n      // We should copy only those table parameters that are specified in the config.\n      SerDeSpec spec = AnnotationUtils.getAnnotation(serdeClass, SerDeSpec.class);\n      String paramsStr = HiveConf.getVar(conf, HiveConf.ConfVars.DDL_CTL_PARAMETERS_WHITELIST);\n\n      Set<String> retainer = new HashSet<String>();\n      // for non-native table, property storage_handler should be retained\n      retainer.add(META_TABLE_STORAGE);\n      if (spec != null && spec.schemaProps() != null) {\n        retainer.addAll(Arrays.asList(spec.schemaProps()));\n      }\n      if (paramsStr != null) {\n        retainer.addAll(Arrays.asList(paramsStr.split(\",\")));\n      }\n      if (!retainer.isEmpty()) {\n        params.keySet().retainAll(retainer);\n      } else {\n        params.clear();\n      }\n\n      if (crtTbl.getTblProps() != null) {\n        params.putAll(crtTbl.getTblProps());\n      }\n\n      if (crtTbl.isUserStorageFormat()) {\n        tbl.setInputFormatClass(crtTbl.getDefaultInputFormat());\n        tbl.setOutputFormatClass(crtTbl.getDefaultOutputFormat());\n        tbl.getTTable().getSd().setInputFormat(\n        tbl.getInputFormatClass().getName());\n        tbl.getTTable().getSd().setOutputFormat(\n        tbl.getOutputFormatClass().getName());\n        if (crtTbl.getDefaultSerName() == null) {\n          LOG.info(\"Default to LazySimpleSerDe for like table {}\", targetTableName);\n          tbl.setSerializationLib(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n        } else {\n          // let's validate that the serde exists\n          validateSerDe(crtTbl.getDefaultSerName());\n          tbl.setSerializationLib(crtTbl.getDefaultSerName());\n        }\n      }\n\n      tbl.getTTable().setTemporary(crtTbl.isTemporary());\n\n      if (crtTbl.isExternal()) {\n        tbl.setProperty(\"EXTERNAL\", \"TRUE\");\n        tbl.setTableType(TableType.EXTERNAL_TABLE);\n      } else {\n        tbl.getParameters().remove(\"EXTERNAL\");\n      }\n    }\n\n    // If location is specified - ensure that it is a full qualified name\n    if (DDLTask.doesTableNeedLocation(tbl)) {\n      makeLocationQualified(tbl.getDbName(), tbl.getTTable().getSd(), tbl.getTableName(), conf);\n    }\n\n    if (crtTbl.getLocation() == null && !tbl.isPartitioned()\n        && conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n      StatsSetupConst.setStatsStateForCreateTable(tbl.getTTable().getParameters(),\n          MetaStoreUtils.getColumnNames(tbl.getCols()), StatsSetupConst.TRUE);\n    }\n\n    // create the table\n    db.createTable(tbl, crtTbl.getIfNotExists());\n    addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n    return 0;\n  }"
        ],
        [
            "DDLTask::showTablesOrViews(Hive,ShowTablesDesc)",
            "2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618 -\n2619  \n2620 -\n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  ",
            "  /**\n   * Write a list of the tables/views in the database to a file.\n   *\n   * @param db\n   *          The database in context.\n   * @param showDesc\n   *        A ShowTablesDesc for tables or views we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showTablesOrViews(Hive db, ShowTablesDesc showDesc) throws HiveException {\n    // get the tables/views for the desired pattern - populate the output stream\n    List<String> tablesOrViews = null;\n\n    String dbName      = showDesc.getDbName();\n    String pattern     = showDesc.getPattern(); // if null, all tables/views are returned\n    String resultsFile = showDesc.getResFile();\n    TableType type     = showDesc.getType(); // will be null for tables, VIRTUAL_VIEW for views\n\n    if (!db.databaseExists(dbName)) {\n      throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);\n    }\n\n    LOG.debug(\"pattern: \" + pattern);\n    tablesOrViews = db.getTablesByType(dbName, pattern, type);\n    LOG.debug(\"results : \" + tablesOrViews.size());\n\n    // write the results in the file\n    DataOutputStream outStream = null;\n    try {\n      Path resFile = new Path(resultsFile);\n      FileSystem fs = resFile.getFileSystem(conf);\n      outStream = fs.create(resFile);\n\n      SortedSet<String> sortedSet = new TreeSet<String>(tablesOrViews);\n      formatter.showTables(outStream, sortedSet);\n      outStream.close();\n      outStream = null;\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"in database\" + dbName);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620 +\n2621  \n2622 +\n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  ",
            "  /**\n   * Write a list of the tables/views in the database to a file.\n   *\n   * @param db\n   *          The database in context.\n   * @param showDesc\n   *        A ShowTablesDesc for tables or views we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showTablesOrViews(Hive db, ShowTablesDesc showDesc) throws HiveException {\n    // get the tables/views for the desired pattern - populate the output stream\n    List<String> tablesOrViews = null;\n\n    String dbName      = showDesc.getDbName();\n    String pattern     = showDesc.getPattern(); // if null, all tables/views are returned\n    String resultsFile = showDesc.getResFile();\n    TableType type     = showDesc.getType(); // will be null for tables, VIRTUAL_VIEW for views\n\n    if (!db.databaseExists(dbName)) {\n      throw new HiveException(ErrorMsg.DATABASE_NOT_EXISTS, dbName);\n    }\n\n    LOG.debug(\"pattern: {}\", pattern);\n    tablesOrViews = db.getTablesByType(dbName, pattern, type);\n    LOG.debug(\"results : {}\", tablesOrViews.size());\n\n    // write the results in the file\n    DataOutputStream outStream = null;\n    try {\n      Path resFile = new Path(resultsFile);\n      FileSystem fs = resFile.getFileSystem(conf);\n      outStream = fs.create(resFile);\n\n      SortedSet<String> sortedSet = new TreeSet<String>(tablesOrViews);\n      formatter.showTables(outStream, sortedSet);\n      outStream.close();\n      outStream = null;\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"in database\" + dbName);\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "FetchTask::initialize(QueryState,QueryPlan,DriverContext,CompilationOpContext)",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  ",
            "  @Override\n  public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext ctx,\n      CompilationOpContext opContext) {\n    super.initialize(queryState, queryPlan, ctx, opContext);\n    work.initializeForFetch(opContext);\n\n    try {\n      // Create a file system handle\n      JobConf job = new JobConf(conf);\n\n      Operator<?> source = work.getSource();\n      if (source instanceof TableScanOperator) {\n        TableScanOperator ts = (TableScanOperator) source;\n        // push down projections\n        ColumnProjectionUtils.appendReadColumns(\n            job, ts.getNeededColumnIDs(), ts.getNeededColumns(), ts.getNeededNestedColumnPaths());\n        // push down filters\n        HiveInputFormat.pushFilters(job, ts);\n\n        AcidUtils.setTransactionalTableScan(job, ts.getConf().isAcidTable());\n        AcidUtils.setAcidOperationalProperties(job, ts.getConf().getAcidOperationalProperties());\n      }\n      sink = work.getSink();\n      fetch = new FetchOperator(work, job, source, getVirtualColumns(source));\n      source.initialize(conf, new ObjectInspector[]{fetch.getOutputObjectInspector()});\n      totalRows = 0;\n      ExecMapper.setDone(false);\n\n    } catch (Exception e) {\n      // Bail out ungracefully - we should never hit\n      // this here - but would have hit it in SemanticAnalyzer\n      LOG.error(StringUtils.stringifyException(e));\n      throw new RuntimeException(e);\n    }\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  ",
            "  @Override\n  public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext ctx,\n      CompilationOpContext opContext) {\n    super.initialize(queryState, queryPlan, ctx, opContext);\n    work.initializeForFetch(opContext);\n\n    try {\n      // Create a file system handle\n      JobConf job = new JobConf(conf);\n\n      Operator<?> source = work.getSource();\n      if (source instanceof TableScanOperator) {\n        TableScanOperator ts = (TableScanOperator) source;\n        // push down projections\n        ColumnProjectionUtils.appendReadColumns(\n            job, ts.getNeededColumnIDs(), ts.getNeededColumns(), ts.getNeededNestedColumnPaths());\n        // push down filters\n        HiveInputFormat.pushFilters(job, ts);\n\n        AcidUtils.setTransactionalTableScan(job, ts.getConf().isAcidTable());\n        AcidUtils.setAcidOperationalProperties(job, ts.getConf().getAcidOperationalProperties());\n      }\n      sink = work.getSink();\n      fetch = new FetchOperator(work, job, source, getVirtualColumns(source));\n      source.initialize(conf, new ObjectInspector[]{fetch.getOutputObjectInspector()});\n      totalRows = 0;\n      ExecMapper.setDone(false);\n\n    } catch (Exception e) {\n      // Bail out ungracefully - we should never hit\n      // this here - but would have hit it in SemanticAnalyzer\n      LOG.error(\"Initialize failed\", e);\n      throw new RuntimeException(e);\n    }\n  }"
        ],
        [
            "FunctionTask::execute(DriverContext)",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 -\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    CreateFunctionDesc createFunctionDesc = work.getCreateFunctionDesc();\n    if (createFunctionDesc != null) {\n      if (createFunctionDesc.isTemp()) {\n        return createTemporaryFunction(createFunctionDesc);\n      } else {\n        try {\n          if (createFunctionDesc.getReplicationSpec().isInReplicationScope()) {\n            String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n                    createFunctionDesc.getFunctionName());\n            String dbName = qualifiedNameParts[0];\n            String funcName = qualifiedNameParts[1];\n            Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();\n            if (!createFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {\n              // If the database is newer than the create event, then noop it.\n              LOG.debug(\"FunctionTask: Create Function {} is skipped as database {} \" +\n                        \"is newer than update\", funcName, dbName);\n              return 0;\n            }\n          }\n          return createPermanentFunction(Hive.get(conf), createFunctionDesc);\n        } catch (Exception e) {\n          setException(e);\n          LOG.error(stringifyException(e));\n          return 1;\n        }\n      }\n    }\n\n    DropFunctionDesc dropFunctionDesc = work.getDropFunctionDesc();\n    if (dropFunctionDesc != null) {\n      if (dropFunctionDesc.isTemp()) {\n        return dropTemporaryFunction(dropFunctionDesc);\n      } else {\n        try {\n          if (dropFunctionDesc.getReplicationSpec().isInReplicationScope()) {\n            String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n                    dropFunctionDesc.getFunctionName());\n            String dbName = qualifiedNameParts[0];\n            String funcName = qualifiedNameParts[1];\n            Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();\n            if (!dropFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {\n              // If the database is newer than the drop event, then noop it.\n              LOG.debug(\"FunctionTask: Drop Function {} is skipped as database {} \" +\n                        \"is newer than update\", funcName, dbName);\n              return 0;\n            }\n          }\n          return dropPermanentFunction(Hive.get(conf), dropFunctionDesc);\n        } catch (Exception e) {\n          setException(e);\n          LOG.error(stringifyException(e));\n          return 1;\n        }\n      }\n    }\n\n    if (work.getReloadFunctionDesc() != null) {\n      try {\n        Hive.get().reloadFunctions();\n      } catch (Exception e) {\n        setException(e);\n        LOG.error(stringifyException(e));\n        return 1;\n      }\n    }\n\n    CreateMacroDesc createMacroDesc = work.getCreateMacroDesc();\n    if (createMacroDesc != null) {\n      return createMacro(createMacroDesc);\n    }\n\n    DropMacroDesc dropMacroDesc = work.getDropMacroDesc();\n    if (dropMacroDesc != null) {\n      return dropMacro(dropMacroDesc);\n    }\n    return 0;\n  }",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n    CreateFunctionDesc createFunctionDesc = work.getCreateFunctionDesc();\n    if (createFunctionDesc != null) {\n      if (createFunctionDesc.isTemp()) {\n        return createTemporaryFunction(createFunctionDesc);\n      } else {\n        try {\n          if (createFunctionDesc.getReplicationSpec().isInReplicationScope()) {\n            String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n                    createFunctionDesc.getFunctionName());\n            String dbName = qualifiedNameParts[0];\n            String funcName = qualifiedNameParts[1];\n            Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();\n            if (!createFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {\n              // If the database is newer than the create event, then noop it.\n              LOG.debug(\"FunctionTask: Create Function {} is skipped as database {} \" +\n                        \"is newer than update\", funcName, dbName);\n              return 0;\n            }\n          }\n          return createPermanentFunction(Hive.get(conf), createFunctionDesc);\n        } catch (Exception e) {\n          setException(e);\n          LOG.error(\"Failed to create function\", e);\n          return 1;\n        }\n      }\n    }\n\n    DropFunctionDesc dropFunctionDesc = work.getDropFunctionDesc();\n    if (dropFunctionDesc != null) {\n      if (dropFunctionDesc.isTemp()) {\n        return dropTemporaryFunction(dropFunctionDesc);\n      } else {\n        try {\n          if (dropFunctionDesc.getReplicationSpec().isInReplicationScope()) {\n            String[] qualifiedNameParts = FunctionUtils.getQualifiedFunctionNameParts(\n                    dropFunctionDesc.getFunctionName());\n            String dbName = qualifiedNameParts[0];\n            String funcName = qualifiedNameParts[1];\n            Map<String, String> dbProps = Hive.get().getDatabase(dbName).getParameters();\n            if (!dropFunctionDesc.getReplicationSpec().allowEventReplacementInto(dbProps)) {\n              // If the database is newer than the drop event, then noop it.\n              LOG.debug(\"FunctionTask: Drop Function {} is skipped as database {} \" +\n                        \"is newer than update\", funcName, dbName);\n              return 0;\n            }\n          }\n          return dropPermanentFunction(Hive.get(conf), dropFunctionDesc);\n        } catch (Exception e) {\n          setException(e);\n          LOG.error(\"Failed to drop function\", e);\n          return 1;\n        }\n      }\n    }\n\n    if (work.getReloadFunctionDesc() != null) {\n      try {\n        Hive.get().reloadFunctions();\n      } catch (Exception e) {\n        setException(e);\n        LOG.error(\"Failed to reload functions\", e);\n        return 1;\n      }\n    }\n\n    CreateMacroDesc createMacroDesc = work.getCreateMacroDesc();\n    if (createMacroDesc != null) {\n      return createMacro(createMacroDesc);\n    }\n\n    DropMacroDesc dropMacroDesc = work.getDropMacroDesc();\n    if (dropMacroDesc != null) {\n      return dropMacro(dropMacroDesc);\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::addConstraints(Hive,AlterTableDesc)",
            "4268  \n4269  \n4270  \n4271  \n4272  \n4273  \n4274  \n4275  \n4276  \n4277  \n4278  \n4279  \n4280  \n4281  \n4282  \n4283 -\n4284  \n4285  \n4286  \n4287  \n4288  \n4289  \n4290  \n4291  \n4292  \n4293  \n4294  \n4295  \n4296  \n4297  \n4298  \n4299  \n4300  \n4301  ",
            "  private int addConstraints(Hive db, AlterTableDesc alterTbl)\n           throws SemanticException, HiveException {\n    try {\n      // This is either an alter table add foreign key or add primary key command.\n      if (alterTbl.getPrimaryKeyCols() != null && !alterTbl.getPrimaryKeyCols().isEmpty()) {\n        db.addPrimaryKey(alterTbl.getPrimaryKeyCols());\n      }\n      if (alterTbl.getForeignKeyCols() != null && !alterTbl.getForeignKeyCols().isEmpty()) {\n        try {\n          db.addForeignKey(alterTbl.getForeignKeyCols());\n        } catch (HiveException e) {\n          if (e.getCause() instanceof InvalidObjectException\n              && alterTbl.getReplicationSpec()!= null && alterTbl.getReplicationSpec().isInReplicationScope()) {\n            // During repl load, NoSuchObjectException in foreign key shall\n            // ignore as the foreign table may not be part of the replication\n            LOG.debug(e.getMessage());\n          } else {\n            throw e;\n          }\n        }\n      }\n      if (alterTbl.getUniqueConstraintCols() != null\n              && !alterTbl.getUniqueConstraintCols().isEmpty()) {\n        db.addUniqueConstraint(alterTbl.getUniqueConstraintCols());\n      }\n      if (alterTbl.getNotNullConstraintCols() != null\n              && !alterTbl.getNotNullConstraintCols().isEmpty()) {\n        db.addNotNullConstraint(alterTbl.getNotNullConstraintCols());\n      }\n    } catch (NoSuchObjectException e) {\n      throw new HiveException(e);\n    }\n    return 0;\n  }",
            "4270  \n4271  \n4272  \n4273  \n4274  \n4275  \n4276  \n4277  \n4278  \n4279  \n4280  \n4281  \n4282  \n4283  \n4284  \n4285 +\n4286  \n4287  \n4288  \n4289  \n4290  \n4291  \n4292  \n4293  \n4294  \n4295  \n4296  \n4297  \n4298  \n4299  \n4300  \n4301  \n4302  \n4303  ",
            "  private int addConstraints(Hive db, AlterTableDesc alterTbl)\n           throws SemanticException, HiveException {\n    try {\n      // This is either an alter table add foreign key or add primary key command.\n      if (alterTbl.getPrimaryKeyCols() != null && !alterTbl.getPrimaryKeyCols().isEmpty()) {\n        db.addPrimaryKey(alterTbl.getPrimaryKeyCols());\n      }\n      if (alterTbl.getForeignKeyCols() != null && !alterTbl.getForeignKeyCols().isEmpty()) {\n        try {\n          db.addForeignKey(alterTbl.getForeignKeyCols());\n        } catch (HiveException e) {\n          if (e.getCause() instanceof InvalidObjectException\n              && alterTbl.getReplicationSpec()!= null && alterTbl.getReplicationSpec().isInReplicationScope()) {\n            // During repl load, NoSuchObjectException in foreign key shall\n            // ignore as the foreign table may not be part of the replication\n            LOG.debug(\"InvalidObjectException: \", e);\n          } else {\n            throw e;\n          }\n        }\n      }\n      if (alterTbl.getUniqueConstraintCols() != null\n              && !alterTbl.getUniqueConstraintCols().isEmpty()) {\n        db.addUniqueConstraint(alterTbl.getUniqueConstraintCols());\n      }\n      if (alterTbl.getNotNullConstraintCols() != null\n              && !alterTbl.getNotNullConstraintCols().isEmpty()) {\n        db.addNotNullConstraint(alterTbl.getNotNullConstraintCols());\n      }\n    } catch (NoSuchObjectException e) {\n      throw new HiveException(e);\n    }\n    return 0;\n  }"
        ],
        [
            "ExplainTask::execute(DriverContext)",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 -\n 362 -\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 -\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    PrintStream out = null;\n    try {\n      Path resFile = work.getResFile();\n      OutputStream outS = resFile.getFileSystem(conf).create(resFile);\n      out = new PrintStream(outS);\n\n      if (work.isLogical()) {\n        JSONObject jsonLogicalPlan = getJSONLogicalPlan(out, work);\n        if (work.isFormatted()) {\n          out.print(jsonLogicalPlan);\n        }\n      } else if (work.isAuthorize()) {\n        JSONObject jsonAuth = collectAuthRelatedEntities(out, work);\n        if (work.isFormatted()) {\n          out.print(jsonAuth);\n        }\n      } else if (work.getDependency()) {\n        JSONObject jsonDependencies = getJSONDependencies(work);\n        out.print(jsonDependencies);\n      } else {\n        if (work.isUserLevelExplain()) {\n          // Because of the implementation of the JsonParserFactory, we are sure\n          // that we can get a TezJsonParser.\n          JsonParser jsonParser = JsonParserFactory.getParser(conf);\n          work.getConfig().setFormatted(true);\n          JSONObject jsonPlan = getJSONPlan(out, work);\n          if (work.getCboInfo() != null) {\n            jsonPlan.put(\"cboInfo\", work.getCboInfo());\n          }\n          try {\n            jsonParser.print(jsonPlan, out);\n          } catch (Exception e) {\n            // if there is anything wrong happen, we bail out.\n            LOG.error(\"Running explain user level has problem: \" + e.toString()\n                + \". Falling back to normal explain\");\n            work.getConfig().setFormatted(false);\n            work.getConfig().setUserLevelExplain(false);\n            jsonPlan = getJSONPlan(out, work);\n          }\n        } else {\n          JSONObject jsonPlan = getJSONPlan(out, work);\n          if (work.isFormatted()) {\n            // use the parser to get the output operators of RS\n            JsonParser jsonParser = JsonParserFactory.getParser(conf);\n            if (jsonParser != null) {\n              jsonParser.print(jsonPlan, null);\n              LOG.info(\"JsonPlan is augmented to \" + jsonPlan.toString());\n            }\n            out.print(jsonPlan);\n          }\n        }\n      }\n\n      out.close();\n      out = null;\n      return (0);\n    }\n    catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(),\n          \"\\n\" + StringUtils.stringifyException(e));\n      return (1);\n    }\n    finally {\n      IOUtils.closeStream(out);\n    }\n  }",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 +\n 362 +\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 +\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "  @Override\n  public int execute(DriverContext driverContext) {\n\n    PrintStream out = null;\n    try {\n      Path resFile = work.getResFile();\n      OutputStream outS = resFile.getFileSystem(conf).create(resFile);\n      out = new PrintStream(outS);\n\n      if (work.isLogical()) {\n        JSONObject jsonLogicalPlan = getJSONLogicalPlan(out, work);\n        if (work.isFormatted()) {\n          out.print(jsonLogicalPlan);\n        }\n      } else if (work.isAuthorize()) {\n        JSONObject jsonAuth = collectAuthRelatedEntities(out, work);\n        if (work.isFormatted()) {\n          out.print(jsonAuth);\n        }\n      } else if (work.getDependency()) {\n        JSONObject jsonDependencies = getJSONDependencies(work);\n        out.print(jsonDependencies);\n      } else {\n        if (work.isUserLevelExplain()) {\n          // Because of the implementation of the JsonParserFactory, we are sure\n          // that we can get a TezJsonParser.\n          JsonParser jsonParser = JsonParserFactory.getParser(conf);\n          work.getConfig().setFormatted(true);\n          JSONObject jsonPlan = getJSONPlan(out, work);\n          if (work.getCboInfo() != null) {\n            jsonPlan.put(\"cboInfo\", work.getCboInfo());\n          }\n          try {\n            jsonParser.print(jsonPlan, out);\n          } catch (Exception e) {\n            // if there is anything wrong happen, we bail out.\n            LOG.error(\"Running explain user level has problem.\" +\n              \" Falling back to normal explain.\", e);\n            work.getConfig().setFormatted(false);\n            work.getConfig().setUserLevelExplain(false);\n            jsonPlan = getJSONPlan(out, work);\n          }\n        } else {\n          JSONObject jsonPlan = getJSONPlan(out, work);\n          if (work.isFormatted()) {\n            // use the parser to get the output operators of RS\n            JsonParser jsonParser = JsonParserFactory.getParser(conf);\n            if (jsonParser != null) {\n              jsonParser.print(jsonPlan, null);\n              LOG.info(\"JsonPlan is augmented to {}\", jsonPlan);\n            }\n            out.print(jsonPlan);\n          }\n        }\n      }\n\n      out.close();\n      out = null;\n      return (0);\n    }\n    catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(),\n          \"\\n\" + StringUtils.stringifyException(e));\n      return (1);\n    }\n    finally {\n      IOUtils.closeStream(out);\n    }\n  }"
        ],
        [
            "ReplCopyTask::execute(DriverContext)",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 -\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "  @Override\n  protected int execute(DriverContext driverContext) {\n    LOG.debug(\"ReplCopyTask.execute()\");\n    FileSystem dstFs = null;\n    Path toPath = null;\n    try {\n      Path fromPath = work.getFromPaths()[0];\n      toPath = work.getToPaths()[0];\n\n      console.printInfo(\"Copying data from \" + fromPath.toString(), \" to \"\n          + toPath.toString());\n\n      ReplCopyWork rwork = ((ReplCopyWork)work);\n\n      FileSystem srcFs = fromPath.getFileSystem(conf);\n      dstFs = toPath.getFileSystem(conf);\n\n      // This should only be true for copy tasks created from functions, otherwise there should never\n      // be a CM uri in the from path.\n      if (ReplChangeManager.isCMFileUri(fromPath, srcFs)) {\n        String[] result = ReplChangeManager.getFileWithChksumFromURI(fromPath.toString());\n        ReplChangeManager.FileInfo sourceInfo = ReplChangeManager\n            .getFileInfo(new Path(result[0]), result[1], conf);\n        if (FileUtils.copy(\n            sourceInfo.getSrcFs(), sourceInfo.getSourcePath(),\n            dstFs, toPath, false, false, conf)) {\n          return 0;\n        } else {\n          console.printError(\"Failed to copy: '\" + fromPath.toString() + \"to: '\" + toPath.toString()\n              + \"'\");\n          return 1;\n        }\n      }\n\n      List<ReplChangeManager.FileInfo> srcFiles = new ArrayList<>();\n      if (rwork.readSrcAsFilesList()) {\n        // This flow is usually taken for REPL LOAD\n        // Our input is the result of a _files listing, we should expand out _files.\n        srcFiles = filesInFileListing(srcFs, fromPath);\n        LOG.debug(\"ReplCopyTask _files contains:\" + (srcFiles == null ? \"null\" : srcFiles.size()));\n        if ((srcFiles == null) || (srcFiles.isEmpty())) {\n          if (work.isErrorOnSrcEmpty()) {\n            console.printError(\"No _files entry found on source: \" + fromPath.toString());\n            return 5;\n          } else {\n            return 0;\n          }\n        }\n      } else {\n        // This flow is usually taken for IMPORT command\n        FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(srcFs, fromPath);\n        LOG.debug(\"ReplCopyTasks srcs= {}\", (srcs == null ? \"null\" : srcs.length));\n        if (srcs == null || srcs.length == 0) {\n          if (work.isErrorOnSrcEmpty()) {\n            console.printError(\"No files matching path: \" + fromPath.toString());\n            return 3;\n          } else {\n            return 0;\n          }\n        }\n\n        for (FileStatus oneSrc : srcs) {\n          console.printInfo(\"Copying file: \" + oneSrc.getPath().toString());\n          LOG.debug(\"ReplCopyTask :cp:{}=>{}\", oneSrc.getPath(), toPath);\n          srcFiles.add(new ReplChangeManager.FileInfo(oneSrc.getPath().getFileSystem(conf),\n                                                      oneSrc.getPath()));\n        }\n      }\n\n      LOG.debug(\"ReplCopyTask numFiles: {}\", srcFiles.size());\n      if (!FileUtils.mkdir(dstFs, toPath, conf)) {\n        console.printError(\"Cannot make target directory: \" + toPath.toString());\n        return 2;\n      }\n      // Copy the files from different source file systems to one destination directory\n      new CopyUtils(rwork.distCpDoAsUser(), conf).copyAndVerify(dstFs, toPath, srcFiles);\n\n      // If a file is copied from CM path, then need to rename them using original source file name\n      // This is needed to avoid having duplicate files in target if same event is applied twice\n      // where the first event refers to source path and  second event refers to CM path\n      for (ReplChangeManager.FileInfo srcFile : srcFiles) {\n        if (srcFile.isUseSourcePath()) {\n          continue;\n        }\n        String destFileName = srcFile.getCmPath().getName();\n        Path destFile = new Path(toPath, destFileName);\n        if (dstFs.exists(destFile)) {\n          String destFileWithSourceName = srcFile.getSourcePath().getName();\n          Path newDestFile = new Path(toPath, destFileWithSourceName);\n          boolean result = dstFs.rename(destFile, newDestFile);\n          if (!result) {\n            throw new IllegalStateException(\n                \"could not rename \" + destFile.getName() + \" to \" + newDestFile.getName());\n          }\n        }\n      }\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      return (1);\n    }\n  }",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98 +\n  99 +\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112 +\n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  @Override\n  protected int execute(DriverContext driverContext) {\n    LOG.debug(\"ReplCopyTask.execute()\");\n    FileSystem dstFs = null;\n    Path toPath = null;\n    try {\n      Path fromPath = work.getFromPaths()[0];\n      toPath = work.getToPaths()[0];\n\n      console.printInfo(\"Copying data from \" + fromPath.toString(), \" to \"\n          + toPath.toString());\n\n      ReplCopyWork rwork = ((ReplCopyWork)work);\n\n      FileSystem srcFs = fromPath.getFileSystem(conf);\n      dstFs = toPath.getFileSystem(conf);\n\n      // This should only be true for copy tasks created from functions, otherwise there should never\n      // be a CM uri in the from path.\n      if (ReplChangeManager.isCMFileUri(fromPath, srcFs)) {\n        String[] result = ReplChangeManager.getFileWithChksumFromURI(fromPath.toString());\n        ReplChangeManager.FileInfo sourceInfo = ReplChangeManager\n            .getFileInfo(new Path(result[0]), result[1], conf);\n        if (FileUtils.copy(\n            sourceInfo.getSrcFs(), sourceInfo.getSourcePath(),\n            dstFs, toPath, false, false, conf)) {\n          return 0;\n        } else {\n          console.printError(\"Failed to copy: '\" + fromPath.toString() + \"to: '\" + toPath.toString()\n              + \"'\");\n          return 1;\n        }\n      }\n\n      List<ReplChangeManager.FileInfo> srcFiles = new ArrayList<>();\n      if (rwork.readSrcAsFilesList()) {\n        // This flow is usually taken for REPL LOAD\n        // Our input is the result of a _files listing, we should expand out _files.\n        srcFiles = filesInFileListing(srcFs, fromPath);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"ReplCopyTask _files contains: {}\", (srcFiles == null ? \"null\" : srcFiles.size()));\n        }\n        if ((srcFiles == null) || (srcFiles.isEmpty())) {\n          if (work.isErrorOnSrcEmpty()) {\n            console.printError(\"No _files entry found on source: \" + fromPath.toString());\n            return 5;\n          } else {\n            return 0;\n          }\n        }\n      } else {\n        // This flow is usually taken for IMPORT command\n        FileStatus[] srcs = LoadSemanticAnalyzer.matchFilesOrDir(srcFs, fromPath);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"ReplCopyTasks srcs= {}\", (srcs == null ? \"null\" : srcs.length));\n        }\n        if (srcs == null || srcs.length == 0) {\n          if (work.isErrorOnSrcEmpty()) {\n            console.printError(\"No files matching path: \" + fromPath.toString());\n            return 3;\n          } else {\n            return 0;\n          }\n        }\n\n        for (FileStatus oneSrc : srcs) {\n          console.printInfo(\"Copying file: \" + oneSrc.getPath().toString());\n          LOG.debug(\"ReplCopyTask :cp:{}=>{}\", oneSrc.getPath(), toPath);\n          srcFiles.add(new ReplChangeManager.FileInfo(oneSrc.getPath().getFileSystem(conf),\n                                                      oneSrc.getPath()));\n        }\n      }\n\n      LOG.debug(\"ReplCopyTask numFiles: {}\", srcFiles.size());\n      if (!FileUtils.mkdir(dstFs, toPath, conf)) {\n        console.printError(\"Cannot make target directory: \" + toPath.toString());\n        return 2;\n      }\n      // Copy the files from different source file systems to one destination directory\n      new CopyUtils(rwork.distCpDoAsUser(), conf).copyAndVerify(dstFs, toPath, srcFiles);\n\n      // If a file is copied from CM path, then need to rename them using original source file name\n      // This is needed to avoid having duplicate files in target if same event is applied twice\n      // where the first event refers to source path and  second event refers to CM path\n      for (ReplChangeManager.FileInfo srcFile : srcFiles) {\n        if (srcFile.isUseSourcePath()) {\n          continue;\n        }\n        String destFileName = srcFile.getCmPath().getName();\n        Path destFile = new Path(toPath, destFileName);\n        if (dstFs.exists(destFile)) {\n          String destFileWithSourceName = srcFile.getSourcePath().getName();\n          Path newDestFile = new Path(toPath, destFileWithSourceName);\n          boolean result = dstFs.rename(destFile, newDestFile);\n          if (!result) {\n            throw new IllegalStateException(\n                \"could not rename \" + destFile.getName() + \" to \" + newDestFile.getName());\n          }\n        }\n      }\n      return 0;\n    } catch (Exception e) {\n      console.printError(\"Failed with exception \" + e.getMessage(), \"\\n\"\n          + StringUtils.stringifyException(e));\n      return (1);\n    }\n  }"
        ],
        [
            "DDLTask::showDatabases(Hive,ShowDatabasesDesc)",
            "2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575 -\n2576  \n2577  \n2578  \n2579  \n2580 -\n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  ",
            "  /**\n   * Write a list of the available databases to a file.\n   *\n   * @param showDatabasesDesc\n   *          These are the databases we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws HiveException {\n    // get the databases for the desired pattern - populate the output stream\n    List<String> databases = null;\n    if (showDatabasesDesc.getPattern() != null) {\n      LOG.info(\"pattern: \" + showDatabasesDesc.getPattern());\n      databases = db.getDatabasesByPattern(showDatabasesDesc.getPattern());\n    } else {\n      databases = db.getAllDatabases();\n    }\n    LOG.info(\"results : \" + databases.size());\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showDatabasesDesc.getResFile());\n    try {\n      formatter.showDatabases(outStream, databases);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"show databases\");\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577 +\n2578  \n2579  \n2580  \n2581  \n2582 +\n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  ",
            "  /**\n   * Write a list of the available databases to a file.\n   *\n   * @param showDatabasesDesc\n   *          These are the databases we're interested in.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int showDatabases(Hive db, ShowDatabasesDesc showDatabasesDesc) throws HiveException {\n    // get the databases for the desired pattern - populate the output stream\n    List<String> databases = null;\n    if (showDatabasesDesc.getPattern() != null) {\n      LOG.info(\"pattern: {}\", showDatabasesDesc.getPattern());\n      databases = db.getDatabasesByPattern(showDatabasesDesc.getPattern());\n    } else {\n      databases = db.getAllDatabases();\n    }\n    LOG.info(\"results : {}\", databases.size());\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showDatabasesDesc.getResFile());\n    try {\n      formatter.showDatabases(outStream, databases);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"show databases\");\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "ReplCopyTask::filesInFileListing(FileSystem,Path)",
            " 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "  private List<ReplChangeManager.FileInfo> filesInFileListing(FileSystem fs, Path dataPath)\n      throws IOException {\n    Path fileListing = new Path(dataPath, EximUtil.FILES_NAME);\n    LOG.debug(\"ReplCopyTask filesInFileListing() reading \" + fileListing.toUri());\n    if (! fs.exists(fileListing)){\n      LOG.debug(\"ReplCopyTask : _files does not exist\");\n      return null; // Returning null from this fn can serve as an err condition.\n      // On success, but with nothing to return, we can return an empty list.\n    }\n\n    List<ReplChangeManager.FileInfo> filePaths = new ArrayList<>();\n    try (BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(fileListing)))) {\n      // TODO : verify if skipping charset here is okay\n\n      String line = null;\n      while ((line = br.readLine()) != null) {\n        LOG.debug(\"ReplCopyTask :_filesReadLine:\" + line);\n\n        String[] fileWithChksum = ReplChangeManager.getFileWithChksumFromURI(line);\n        try {\n          ReplChangeManager.FileInfo f = ReplChangeManager\n              .getFileInfo(new Path(fileWithChksum[0]), fileWithChksum[1], conf);\n          filePaths.add(f);\n        } catch (MetaException e) {\n          // issue warning for missing file and throw exception\n          LOG.warn(\"Cannot find \" + fileWithChksum[0] + \" in source repo or cmroot\");\n          throw new IOException(e.getMessage());\n        }\n        // Note - we need srcFs rather than fs, because it is possible that the _files lists files\n        // which are from a different filesystem than the fs where the _files file itself was loaded\n        // from. Currently, it is possible, for eg., to do REPL LOAD hdfs://<ip>/dir/ and for the _files\n        // in it to contain hdfs://<name>/ entries, and/or vice-versa, and this causes errors.\n        // It might also be possible that there will be a mix of them in a given _files file.\n        // TODO: revisit close to the end of replv2 dev, to see if our assumption now still holds,\n        // and if not so, optimize.\n      }\n    }\n    return filePaths;\n  }",
            " 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182 +\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "  private List<ReplChangeManager.FileInfo> filesInFileListing(FileSystem fs, Path dataPath)\n      throws IOException {\n    Path fileListing = new Path(dataPath, EximUtil.FILES_NAME);\n    LOG.debug(\"ReplCopyTask filesInFileListing() reading {}\", fileListing.toUri());\n    if (! fs.exists(fileListing)){\n      LOG.debug(\"ReplCopyTask : _files does not exist\");\n      return null; // Returning null from this fn can serve as an err condition.\n      // On success, but with nothing to return, we can return an empty list.\n    }\n\n    List<ReplChangeManager.FileInfo> filePaths = new ArrayList<>();\n    try (BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(fileListing)))) {\n      // TODO : verify if skipping charset here is okay\n\n      String line = null;\n      while ((line = br.readLine()) != null) {\n        LOG.debug(\"ReplCopyTask :_filesReadLine: {}\", line);\n\n        String[] fileWithChksum = ReplChangeManager.getFileWithChksumFromURI(line);\n        try {\n          ReplChangeManager.FileInfo f = ReplChangeManager\n              .getFileInfo(new Path(fileWithChksum[0]), fileWithChksum[1], conf);\n          filePaths.add(f);\n        } catch (MetaException e) {\n          // issue warning for missing file and throw exception\n          LOG.warn(\"Cannot find {} in source repo or cmroot\", fileWithChksum[0]);\n          throw new IOException(e.getMessage());\n        }\n        // Note - we need srcFs rather than fs, because it is possible that the _files lists files\n        // which are from a different filesystem than the fs where the _files file itself was loaded\n        // from. Currently, it is possible, for eg., to do REPL LOAD hdfs://<ip>/dir/ and for the _files\n        // in it to contain hdfs://<name>/ entries, and/or vice-versa, and this causes errors.\n        // It might also be possible that there will be a mix of them in a given _files file.\n        // TODO: revisit close to the end of replv2 dev, to see if our assumption now still holds,\n        // and if not so, optimize.\n      }\n    }\n    return filePaths;\n  }"
        ],
        [
            "MoveTask::handleDynParts(Hive,Table,LoadTableDesc,TaskInformation,DynamicPartitionCtx)",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 -\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  ",
            "  private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,\n      TaskInformation ti, DynamicPartitionCtx dpCtx) throws HiveException,\n      IOException, InvalidOperationException {\n    DataContainer dc;\n    List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n    console.printInfo(System.getProperty(\"line.separator\"));\n    long startTime = System.currentTimeMillis();\n    // load the list of DP partitions and return the list of partition specs\n    // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n    // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n    // After that check the number of DPs created to not exceed the limit and\n    // iterate over it and call loadPartition() here.\n    // The reason we don't do inside HIVE-1361 is the latter is large and we\n    // want to isolate any potential issue it may introduce.\n    Map<Map<String, String>, Partition> dp =\n      db.loadDynamicPartitions(\n        tbd.getSourcePath(),\n        tbd.getTable().getTableName(),\n        tbd.getPartitionSpec(),\n        tbd.getLoadFileType(),\n        dpCtx.getNumDPCols(),\n        (tbd.getLbCtx() == null) ? 0 : tbd.getLbCtx().calculateListBucketingLevel(),\n        work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID &&\n            !tbd.isMmTable(),\n        work.getLoadTableWork().getTxnId(), tbd.getStmtId(), hasFollowingStatsTask(),\n        work.getLoadTableWork().getWriteType());\n\n    // publish DP columns to its subscribers\n    if (dps != null && dps.size() > 0) {\n      pushFeed(FeedType.DYNAMIC_PARTITIONS, dp.values());\n    }\n\n    String loadTime = \"\\t Time taken to load dynamic partitions: \"  +\n        (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\";\n    console.printInfo(loadTime);\n    LOG.info(loadTime);\n\n    if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n      throw new HiveException(\"This query creates no partitions.\" +\n          \" To turn off this error, set hive.error.on.empty.partition=false.\");\n    }\n\n    startTime = System.currentTimeMillis();\n    // for each partition spec, get the partition\n    // and put it to WriteEntity for post-exec hook\n    for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n      Partition partn = entry.getValue();\n\n      // See the comment inside updatePartitionBucketSortColumns.\n      if (!tbd.isMmTable() && (ti.bucketCols != null || ti.sortCols != null)) {\n        updatePartitionBucketSortColumns(\n            db, table, partn, ti.bucketCols, ti.numBuckets, ti.sortCols);\n      }\n\n      WriteEntity enty = new WriteEntity(partn,\n        getWriteType(tbd, work.getLoadTableWork().getWriteType()));\n      if (work.getOutputs() != null) {\n        DDLTask.addIfAbsentByName(enty, work.getOutputs());\n      }\n      // Need to update the queryPlan's output as well so that post-exec hook get executed.\n      // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n      // constructed at compile time and the queryPlan already contains that.\n      // For DP, WriteEntity creation is deferred at this stage so we need to update\n      // queryPlan here.\n      if (queryPlan.getOutputs() == null) {\n        queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n      }\n      queryPlan.getOutputs().add(enty);\n\n      // update columnar lineage for each partition\n      dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n      // Don't set lineage on delete as we don't have all the columns\n      if (work.getLineagState() != null &&\n          work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n          work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n        work.getLineagState().setLineage(tbd.getSourcePath(), dc,\n            table.getCols());\n      }\n      LOG.info(\"\\tLoading partition \" + entry.getKey());\n    }\n    console.printInfo(\"\\t Time taken for adding to write entity : \" +\n        (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n    dc = null; // reset data container to prevent it being added again.\n    return dc;\n  }",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 +\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  ",
            "  private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,\n      TaskInformation ti, DynamicPartitionCtx dpCtx) throws HiveException,\n      IOException, InvalidOperationException {\n    DataContainer dc;\n    List<LinkedHashMap<String, String>> dps = Utilities.getFullDPSpecs(conf, dpCtx);\n\n    console.printInfo(System.getProperty(\"line.separator\"));\n    long startTime = System.currentTimeMillis();\n    // load the list of DP partitions and return the list of partition specs\n    // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions\n    // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.\n    // After that check the number of DPs created to not exceed the limit and\n    // iterate over it and call loadPartition() here.\n    // The reason we don't do inside HIVE-1361 is the latter is large and we\n    // want to isolate any potential issue it may introduce.\n    Map<Map<String, String>, Partition> dp =\n      db.loadDynamicPartitions(\n        tbd.getSourcePath(),\n        tbd.getTable().getTableName(),\n        tbd.getPartitionSpec(),\n        tbd.getLoadFileType(),\n        dpCtx.getNumDPCols(),\n        (tbd.getLbCtx() == null) ? 0 : tbd.getLbCtx().calculateListBucketingLevel(),\n        work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID &&\n            !tbd.isMmTable(),\n        work.getLoadTableWork().getTxnId(), tbd.getStmtId(), hasFollowingStatsTask(),\n        work.getLoadTableWork().getWriteType());\n\n    // publish DP columns to its subscribers\n    if (dps != null && dps.size() > 0) {\n      pushFeed(FeedType.DYNAMIC_PARTITIONS, dp.values());\n    }\n\n    String loadTime = \"\\t Time taken to load dynamic partitions: \"  +\n        (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\";\n    console.printInfo(loadTime);\n    LOG.info(loadTime);\n\n    if (dp.size() == 0 && conf.getBoolVar(HiveConf.ConfVars.HIVE_ERROR_ON_EMPTY_PARTITION)) {\n      throw new HiveException(\"This query creates no partitions.\" +\n          \" To turn off this error, set hive.error.on.empty.partition=false.\");\n    }\n\n    startTime = System.currentTimeMillis();\n    // for each partition spec, get the partition\n    // and put it to WriteEntity for post-exec hook\n    for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {\n      Partition partn = entry.getValue();\n\n      // See the comment inside updatePartitionBucketSortColumns.\n      if (!tbd.isMmTable() && (ti.bucketCols != null || ti.sortCols != null)) {\n        updatePartitionBucketSortColumns(\n            db, table, partn, ti.bucketCols, ti.numBuckets, ti.sortCols);\n      }\n\n      WriteEntity enty = new WriteEntity(partn,\n        getWriteType(tbd, work.getLoadTableWork().getWriteType()));\n      if (work.getOutputs() != null) {\n        DDLTask.addIfAbsentByName(enty, work.getOutputs());\n      }\n      // Need to update the queryPlan's output as well so that post-exec hook get executed.\n      // This is only needed for dynamic partitioning since for SP the the WriteEntity is\n      // constructed at compile time and the queryPlan already contains that.\n      // For DP, WriteEntity creation is deferred at this stage so we need to update\n      // queryPlan here.\n      if (queryPlan.getOutputs() == null) {\n        queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());\n      }\n      queryPlan.getOutputs().add(enty);\n\n      // update columnar lineage for each partition\n      dc = new DataContainer(table.getTTable(), partn.getTPartition());\n\n      // Don't set lineage on delete as we don't have all the columns\n      if (work.getLineagState() != null &&\n          work.getLoadTableWork().getWriteType() != AcidUtils.Operation.DELETE &&\n          work.getLoadTableWork().getWriteType() != AcidUtils.Operation.UPDATE) {\n        work.getLineagState().setLineage(tbd.getSourcePath(), dc,\n            table.getCols());\n      }\n      LOG.info(\"Loading partition \" + entry.getKey());\n    }\n    console.printInfo(\"\\t Time taken for adding to write entity : \" +\n        (System.currentTimeMillis() - startTime)/1000.0 + \" seconds\");\n    dc = null; // reset data container to prevent it being added again.\n    return dc;\n  }"
        ],
        [
            "DDLTask::showTableStatus(Hive,ShowTableStatusDesc)",
            "3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258 -\n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269 -\n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  ",
            "  /**\n   * Write the status of tables to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showTblStatus\n   *          tables we are interested in\n   * @return Return 0 when execution succeeds and above 0 if it fails.\n   */\n  private int showTableStatus(Hive db, ShowTableStatusDesc showTblStatus) throws HiveException {\n    // get the tables for the desired pattern - populate the output stream\n    List<Table> tbls = new ArrayList<Table>();\n    Map<String, String> part = showTblStatus.getPartSpec();\n    Partition par = null;\n    if (part != null) {\n      Table tbl = db.getTable(showTblStatus.getDbName(), showTblStatus.getPattern());\n      par = db.getPartition(tbl, part, false);\n      if (par == null) {\n        throw new HiveException(\"Partition \" + part + \" for table \"\n            + showTblStatus.getPattern() + \" does not exist.\");\n      }\n      tbls.add(tbl);\n    } else {\n      LOG.info(\"pattern: \" + showTblStatus.getPattern());\n      List<String> tblStr = db.getTablesForDb(showTblStatus.getDbName(),\n          showTblStatus.getPattern());\n      SortedSet<String> sortedTbls = new TreeSet<String>(tblStr);\n      Iterator<String> iterTbls = sortedTbls.iterator();\n      while (iterTbls.hasNext()) {\n        // create a row per table name\n        String tblName = iterTbls.next();\n        Table tbl = db.getTable(showTblStatus.getDbName(), tblName);\n        tbls.add(tbl);\n      }\n      LOG.info(\"results : \" + tblStr.size());\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showTblStatus.getResFile());\n    try {\n      formatter.showTableStatus(outStream, db, conf, tbls, part, par);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"show table status\");\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }",
            "3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258  \n3259  \n3260 +\n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271 +\n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  ",
            "  /**\n   * Write the status of tables to a file.\n   *\n   * @param db\n   *          The database in question.\n   * @param showTblStatus\n   *          tables we are interested in\n   * @return Return 0 when execution succeeds and above 0 if it fails.\n   */\n  private int showTableStatus(Hive db, ShowTableStatusDesc showTblStatus) throws HiveException {\n    // get the tables for the desired pattern - populate the output stream\n    List<Table> tbls = new ArrayList<Table>();\n    Map<String, String> part = showTblStatus.getPartSpec();\n    Partition par = null;\n    if (part != null) {\n      Table tbl = db.getTable(showTblStatus.getDbName(), showTblStatus.getPattern());\n      par = db.getPartition(tbl, part, false);\n      if (par == null) {\n        throw new HiveException(\"Partition \" + part + \" for table \"\n            + showTblStatus.getPattern() + \" does not exist.\");\n      }\n      tbls.add(tbl);\n    } else {\n      LOG.info(\"pattern: {}\", showTblStatus.getPattern());\n      List<String> tblStr = db.getTablesForDb(showTblStatus.getDbName(),\n          showTblStatus.getPattern());\n      SortedSet<String> sortedTbls = new TreeSet<String>(tblStr);\n      Iterator<String> iterTbls = sortedTbls.iterator();\n      while (iterTbls.hasNext()) {\n        // create a row per table name\n        String tblName = iterTbls.next();\n        Table tbl = db.getTable(showTblStatus.getDbName(), tblName);\n        tbls.add(tbl);\n      }\n      LOG.info(\"results : {}\", tblStr.size());\n    }\n\n    // write the results in the file\n    DataOutputStream outStream = getOutputStream(showTblStatus.getResFile());\n    try {\n      formatter.showTableStatus(outStream, db, conf, tbls, part, par);\n    } catch (Exception e) {\n      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, \"show table status\");\n    } finally {\n      IOUtils.closeStream(outStream);\n    }\n    return 0;\n  }"
        ],
        [
            "DDLTask::killQuery(Hive,KillQueryDesc)",
            "3053  \n3054  \n3055  \n3056  \n3057  \n3058 -\n3059  \n3060  ",
            "  private int killQuery(Hive db, KillQueryDesc desc) throws HiveException {\n    SessionState sessionState = SessionState.get();\n    for (String queryId : desc.getQueryIds()) {\n      sessionState.getKillQuery().killQuery(queryId);\n    }\n    LOG.info(\"kill query called (\" + desc.getQueryIds().toString() + \")\");\n    return 0;\n  }",
            "3055  \n3056  \n3057  \n3058  \n3059  \n3060 +\n3061  \n3062  ",
            "  private int killQuery(Hive db, KillQueryDesc desc) throws HiveException {\n    SessionState sessionState = SessionState.get();\n    for (String queryId : desc.getQueryIds()) {\n      sessionState.getKillQuery().killQuery(queryId);\n    }\n    LOG.info(\"kill query called ({})\", desc.getQueryIds());\n    return 0;\n  }"
        ]
    ],
    "82cb3d57abf2705069d82807e52177fdb41ff5ca": [
        [
            "ObjectStore::getColumnPrivilegeSet(String,String,String,String,String,List)",
            "5126  \n5127  \n5128  \n5129  \n5130  \n5131  \n5132  \n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145 -\n5146  \n5147  \n5148  \n5149  \n5150  \n5151  \n5152  \n5153  \n5154 -\n5155  \n5156  \n5157  \n5158  \n5159  \n5160  \n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  ",
            "  @Override\n  public PrincipalPrivilegeSet getColumnPrivilegeSet(String dbName,\n      String tableName, String partitionName, String columnName,\n      String userName, List<String> groupNames) throws InvalidObjectException,\n      MetaException {\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n    columnName = normalizeIdentifier(columnName);\n\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> columnUserPriv = new HashMap<>();\n        columnUserPriv.put(userName, getColumnPrivilege(dbName, tableName,\n            columnName, partitionName, userName, PrincipalType.USER));\n        ret.setUserPrivileges(columnUserPriv);\n      }\n      if (groupNames != null && groupNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> columnGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          columnGroupPriv.put(groupName, getColumnPrivilege(dbName, tableName,\n              columnName, partitionName, groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(columnGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (roleNames != null && roleNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> columnRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          columnRolePriv.put(roleName, getColumnPrivilege(dbName, tableName,\n              columnName, partitionName, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(columnRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }",
            "5104  \n5105  \n5106  \n5107  \n5108  \n5109  \n5110  \n5111  \n5112  \n5113  \n5114  \n5115  \n5116  \n5117  \n5118  \n5119  \n5120  \n5121  \n5122  \n5123 +\n5124  \n5125  \n5126  \n5127  \n5128  \n5129  \n5130  \n5131  \n5132 +\n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145  \n5146  \n5147  ",
            "  @Override\n  public PrincipalPrivilegeSet getColumnPrivilegeSet(String dbName,\n      String tableName, String partitionName, String columnName,\n      String userName, List<String> groupNames) throws InvalidObjectException,\n      MetaException {\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n    columnName = normalizeIdentifier(columnName);\n\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> columnUserPriv = new HashMap<>();\n        columnUserPriv.put(userName, getColumnPrivilege(dbName, tableName,\n            columnName, partitionName, userName, PrincipalType.USER));\n        ret.setUserPrivileges(columnUserPriv);\n      }\n      if (CollectionUtils.isNotEmpty(groupNames)) {\n        Map<String, List<PrivilegeGrantInfo>> columnGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          columnGroupPriv.put(groupName, getColumnPrivilege(dbName, tableName,\n              columnName, partitionName, groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(columnGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (CollectionUtils.isNotEmpty(roleNames)) {\n        Map<String, List<PrivilegeGrantInfo>> columnRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          columnRolePriv.put(roleName, getColumnPrivilege(dbName, tableName,\n              columnName, partitionName, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(columnRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::clearClr(ClassLoaderResolver)",
            "9023  \n9024  \n9025  \n9026  \n9027  \n9028  \n9029  \n9030 -\n9031 -\n9032  \n9033  \n9034  ",
            "  private static void clearClr(ClassLoaderResolver clr) throws Exception {\n    if (clr != null){\n      if (clr instanceof ClassLoaderResolverImpl){\n        ClassLoaderResolverImpl clri = (ClassLoaderResolverImpl) clr;\n        long resourcesCleared = clearFieldMap(clri,\"resources\");\n        long loadedClassesCleared = clearFieldMap(clri,\"loadedClasses\");\n        long unloadedClassesCleared = clearFieldMap(clri, \"unloadedClasses\");\n        LOG.debug(\"Cleared ClassLoaderResolverImpl: \" +\n            resourcesCleared + \",\" + loadedClassesCleared + \",\" + unloadedClassesCleared);\n      }\n    }\n  }",
            "8998  \n8999  \n9000  \n9001  \n9002  \n9003  \n9004  \n9005 +\n9006 +\n9007  \n9008  \n9009  ",
            "  private static void clearClr(ClassLoaderResolver clr) throws Exception {\n    if (clr != null){\n      if (clr instanceof ClassLoaderResolverImpl){\n        ClassLoaderResolverImpl clri = (ClassLoaderResolverImpl) clr;\n        long resourcesCleared = clearFieldMap(clri,\"resources\");\n        long loadedClassesCleared = clearFieldMap(clri,\"loadedClasses\");\n        long unloadedClassesCleared = clearFieldMap(clri, \"unloadedClasses\");\n        LOG.debug(\"Cleared ClassLoaderResolverImpl: {}, {}, {}\",\n            resourcesCleared, loadedClassesCleared, unloadedClassesCleared);\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::dropPartitionCommon(MPartition)",
            "2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198 -\n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206 -\n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  ",
            "  /**\n   * Drop an MPartition and cascade deletes (e.g., delete partition privilege grants,\n   *   drop the storage descriptor cleanly, etc.)\n   * @param part - the MPartition to drop\n   * @return whether the transaction committed successfully\n   * @throws InvalidInputException\n   * @throws InvalidObjectException\n   * @throws MetaException\n   * @throws NoSuchObjectException\n   */\n  private boolean dropPartitionCommon(MPartition part) throws NoSuchObjectException, MetaException,\n    InvalidObjectException, InvalidInputException {\n    boolean success = false;\n    try {\n      openTransaction();\n      if (part != null) {\n        List<MFieldSchema> schemas = part.getTable().getPartitionKeys();\n        List<String> colNames = new ArrayList<>();\n        for (MFieldSchema col: schemas) {\n          colNames.add(col.getName());\n        }\n        String partName = FileUtils.makePartName(colNames, part.getValues());\n\n        List<MPartitionPrivilege> partGrants = listPartitionGrants(\n            part.getTable().getDatabase().getName(),\n            part.getTable().getTableName(),\n            Lists.newArrayList(partName));\n\n        if (partGrants != null && partGrants.size() > 0) {\n          pm.deletePersistentAll(partGrants);\n        }\n\n        List<MPartitionColumnPrivilege> partColumnGrants = listPartitionAllColumnGrants(\n            part.getTable().getDatabase().getName(),\n            part.getTable().getTableName(),\n            Lists.newArrayList(partName));\n        if (partColumnGrants != null && partColumnGrants.size() > 0) {\n          pm.deletePersistentAll(partColumnGrants);\n        }\n\n        String dbName = part.getTable().getDatabase().getName();\n        String tableName = part.getTable().getTableName();\n\n        // delete partition level column stats if it exists\n       try {\n          deletePartitionColumnStatistics(dbName, tableName, partName, part.getValues(), null);\n        } catch (NoSuchObjectException e) {\n          LOG.info(\"No column statistics records found to delete\");\n        }\n\n        preDropStorageDescriptor(part.getSd());\n        pm.deletePersistent(part);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }",
            "2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189 +\n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197 +\n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  ",
            "  /**\n   * Drop an MPartition and cascade deletes (e.g., delete partition privilege grants,\n   *   drop the storage descriptor cleanly, etc.)\n   * @param part - the MPartition to drop\n   * @return whether the transaction committed successfully\n   * @throws InvalidInputException\n   * @throws InvalidObjectException\n   * @throws MetaException\n   * @throws NoSuchObjectException\n   */\n  private boolean dropPartitionCommon(MPartition part) throws NoSuchObjectException, MetaException,\n    InvalidObjectException, InvalidInputException {\n    boolean success = false;\n    try {\n      openTransaction();\n      if (part != null) {\n        List<MFieldSchema> schemas = part.getTable().getPartitionKeys();\n        List<String> colNames = new ArrayList<>();\n        for (MFieldSchema col: schemas) {\n          colNames.add(col.getName());\n        }\n        String partName = FileUtils.makePartName(colNames, part.getValues());\n\n        List<MPartitionPrivilege> partGrants = listPartitionGrants(\n            part.getTable().getDatabase().getName(),\n            part.getTable().getTableName(),\n            Lists.newArrayList(partName));\n\n        if (CollectionUtils.isNotEmpty(partGrants)) {\n          pm.deletePersistentAll(partGrants);\n        }\n\n        List<MPartitionColumnPrivilege> partColumnGrants = listPartitionAllColumnGrants(\n            part.getTable().getDatabase().getName(),\n            part.getTable().getTableName(),\n            Lists.newArrayList(partName));\n        if (CollectionUtils.isNotEmpty(partColumnGrants)) {\n          pm.deletePersistentAll(partColumnGrants);\n        }\n\n        String dbName = part.getTable().getDatabase().getName();\n        String tableName = part.getTable().getTableName();\n\n        // delete partition level column stats if it exists\n       try {\n          deletePartitionColumnStatistics(dbName, tableName, partName, part.getValues(), null);\n        } catch (NoSuchObjectException e) {\n          LOG.info(\"No column statistics records found to delete\");\n        }\n\n        preDropStorageDescriptor(part.getSd());\n        pm.deletePersistent(part);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::dropDatabase(String)",
            " 923  \n 924  \n 925  \n 926 -\n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937 -\n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  ",
            "  @Override\n  public boolean dropDatabase(String dbname) throws NoSuchObjectException, MetaException {\n    boolean success = false;\n    LOG.info(\"Dropping database \" + dbname + \" along with all tables\");\n    dbname = normalizeIdentifier(dbname);\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      openTransaction();\n\n      // then drop the database\n      MDatabase db = getMDatabase(dbname);\n      pm.retrieve(db);\n      if (db != null) {\n        List<MDBPrivilege> dbGrants = this.listDatabaseGrants(dbname, queryWrapper);\n        if (dbGrants != null && dbGrants.size() > 0) {\n          pm.deletePersistentAll(dbGrants);\n        }\n        pm.deletePersistent(db);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return success;\n  }",
            " 918  \n 919  \n 920  \n 921 +\n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932 +\n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  ",
            "  @Override\n  public boolean dropDatabase(String dbname) throws NoSuchObjectException, MetaException {\n    boolean success = false;\n    LOG.info(\"Dropping database {} along with all tables\", dbname);\n    dbname = normalizeIdentifier(dbname);\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      openTransaction();\n\n      // then drop the database\n      MDatabase db = getMDatabase(dbname);\n      pm.retrieve(db);\n      if (db != null) {\n        List<MDBPrivilege> dbGrants = this.listDatabaseGrants(dbname, queryWrapper);\n        if (CollectionUtils.isNotEmpty(dbGrants)) {\n          pm.deletePersistentAll(dbGrants);\n        }\n        pm.deletePersistent(db);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::getNumPartitionsByFilter(String,String,String)",
            "3269  \n3270  \n3271  \n3272 -\n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  ",
            "  @Override\n  public int getNumPartitionsByFilter(String dbName, String tblName,\n                                      String filter) throws MetaException, NoSuchObjectException {\n    final ExpressionTree exprTree = (filter != null && !filter.isEmpty())\n        ? PartFilterExprUtil.getFilterParser(filter).tree : ExpressionTree.EMPTY_TREE;\n\n    return new GetHelper<Integer>(dbName, tblName, true, true) {\n      private final SqlFilterForPushdown filter = new SqlFilterForPushdown();\n\n      @Override\n      protected String describeResult() {\n        return \"Partition count\";\n      }\n\n      @Override\n      protected boolean canUseDirectSql(GetHelper<Integer> ctx) throws MetaException {\n        return directSql.generateSqlFilterForPushdown(ctx.getTable(), exprTree, filter);\n      }\n\n      @Override\n      protected Integer getSqlResult(GetHelper<Integer> ctx) throws MetaException {\n        return directSql.getNumPartitionsViaSqlFilter(filter);\n      }\n      @Override\n      protected Integer getJdoResult(\n          GetHelper<Integer> ctx) throws MetaException, NoSuchObjectException {\n        return getNumPartitionsViaOrmFilter(ctx.getTable(), exprTree, true);\n      }\n    }.run(true);\n  }",
            "3253  \n3254  \n3255  \n3256 +\n3257  \n3258  \n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  ",
            "  @Override\n  public int getNumPartitionsByFilter(String dbName, String tblName,\n                                      String filter) throws MetaException, NoSuchObjectException {\n    final ExpressionTree exprTree = org.apache.commons.lang.StringUtils.isNotEmpty(filter)\n        ? PartFilterExprUtil.getFilterParser(filter).tree : ExpressionTree.EMPTY_TREE;\n\n    return new GetHelper<Integer>(dbName, tblName, true, true) {\n      private final SqlFilterForPushdown filter = new SqlFilterForPushdown();\n\n      @Override\n      protected String describeResult() {\n        return \"Partition count\";\n      }\n\n      @Override\n      protected boolean canUseDirectSql(GetHelper<Integer> ctx) throws MetaException {\n        return directSql.generateSqlFilterForPushdown(ctx.getTable(), exprTree, filter);\n      }\n\n      @Override\n      protected Integer getSqlResult(GetHelper<Integer> ctx) throws MetaException {\n        return directSql.getNumPartitionsViaSqlFilter(filter);\n      }\n      @Override\n      protected Integer getJdoResult(\n          GetHelper<Integer> ctx) throws MetaException, NoSuchObjectException {\n        return getNumPartitionsViaOrmFilter(ctx.getTable(), exprTree, true);\n      }\n    }.run(true);\n  }"
        ],
        [
            "ObjectStore::getPartitionPrivilege(String,String,String,String,PrincipalType)",
            "5171  \n5172  \n5173  \n5174  \n5175  \n5176  \n5177  \n5178  \n5179  \n5180  \n5181  \n5182 -\n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189  \n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  ",
            "  private List<PrivilegeGrantInfo> getPartitionPrivilege(String dbName,\n      String tableName, String partName, String principalName,\n      PrincipalType principalType) {\n\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MPartitionPrivilege> userNameTabPartPriv = this\n          .listPrincipalMPartitionGrants(principalName, principalType,\n              dbName, tableName, partName);\n      if (userNameTabPartPriv != null && userNameTabPartPriv.size() > 0) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameTabPartPriv.size());\n        for (int i = 0; i < userNameTabPartPriv.size(); i++) {\n          MPartitionPrivilege item = userNameTabPartPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(),\n              getPrincipalTypeFromStr(item.getGrantorType()), item.getGrantOption()));\n\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }",
            "5149  \n5150  \n5151  \n5152  \n5153  \n5154  \n5155  \n5156  \n5157  \n5158  \n5159  \n5160 +\n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  \n5170  \n5171  \n5172  \n5173  \n5174  ",
            "  private List<PrivilegeGrantInfo> getPartitionPrivilege(String dbName,\n      String tableName, String partName, String principalName,\n      PrincipalType principalType) {\n\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MPartitionPrivilege> userNameTabPartPriv = this\n          .listPrincipalMPartitionGrants(principalName, principalType,\n              dbName, tableName, partName);\n      if (CollectionUtils.isNotEmpty(userNameTabPartPriv)) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameTabPartPriv.size());\n        for (int i = 0; i < userNameTabPartPriv.size(); i++) {\n          MPartitionPrivilege item = userNameTabPartPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(),\n              getPrincipalTypeFromStr(item.getGrantorType()), item.getGrantOption()));\n\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }"
        ],
        [
            "ObjectStore::extractPartitionNamesByFilter(String,String,String,List,boolean,boolean,long)",
            "2441  \n2442  \n2443  \n2444  \n2445 -\n2446 -\n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454 -\n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483 -\n2484  \n2485 -\n2486 -\n2487 -\n2488 -\n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  ",
            "  private PartitionValuesResponse extractPartitionNamesByFilter(String dbName, String tableName, String filter,\n                                                                List<FieldSchema> cols, boolean ascending, boolean applyDistinct, long maxParts)\n      throws MetaException, NoSuchObjectException {\n\n    LOG.info(\"Database: \" + dbName + \" Table:\" + tableName + \" filter\\\"\" + filter + \"\\\" cols:\" + cols);\n    List<String> partitionResults = new ArrayList<String>();\n    List<String> partitionNames = null;\n    List<Partition> partitions = null;\n    Table tbl = getTable(dbName, tableName);\n    try {\n      // Get partitions by name - ascending or descending\n      partitionNames = getPartitionNamesByFilter(dbName, tableName, filter, ascending, maxParts);\n    } catch (MetaException e) {\n      LOG.warn(\"Querying by partition names failed, trying out with partition objects, filter:\" + filter);\n    }\n\n    if (partitionNames == null) {\n      partitions = getPartitionsByFilter(dbName, tableName, filter, (short) maxParts);\n    }\n\n    if (partitions != null) {\n      partitionNames = new ArrayList<String>(partitions.size());\n      for (Partition partition : partitions) {\n        // Check for NULL's just to be safe\n        if (tbl.getPartitionKeys() != null && partition.getValues() != null) {\n          partitionNames.add(Warehouse.makePartName(tbl.getPartitionKeys(), partition.getValues()));\n        }\n      }\n    }\n\n    if (partitionNames == null && partitions == null) {\n      throw new MetaException(\"Cannot obtain list of partitions by filter:\\\"\" + filter +\n          \"\\\" for \" + dbName + \":\" + tableName);\n    }\n\n    if (!ascending) {\n      Collections.sort(partitionNames, Collections.reverseOrder());\n    }\n\n    // Return proper response\n    PartitionValuesResponse response = new PartitionValuesResponse();\n    response.setPartitionValues(new ArrayList<PartitionValuesRow>(partitionNames.size()));\n    LOG.info(\"Converting responses to Partition values for items:\" + partitionNames.size());\n    for (String partName : partitionNames) {\n      ArrayList<String> vals = new ArrayList<String>(tbl.getPartitionKeys().size());\n      for (FieldSchema key : tbl.getPartitionKeys()) {\n        vals.add(null);\n      }\n      PartitionValuesRow row = new PartitionValuesRow();\n      Warehouse.makeValsFromName(partName, vals);\n      for (String value : vals) {\n        row.addToRow(value);\n      }\n      response.addToPartitionValues(row);\n    }\n    return response;\n  }",
            "2432  \n2433  \n2434  \n2435  \n2436 +\n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444 +\n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473 +\n2474  \n2475 +\n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  ",
            "  private PartitionValuesResponse extractPartitionNamesByFilter(String dbName, String tableName, String filter,\n                                                                List<FieldSchema> cols, boolean ascending, boolean applyDistinct, long maxParts)\n      throws MetaException, NoSuchObjectException {\n\n    LOG.info(\"Database: {} Table: {} filter: \\\"{}\\\" cols: {}\", dbName, tableName, filter, cols);\n    List<String> partitionNames = null;\n    List<Partition> partitions = null;\n    Table tbl = getTable(dbName, tableName);\n    try {\n      // Get partitions by name - ascending or descending\n      partitionNames = getPartitionNamesByFilter(dbName, tableName, filter, ascending, maxParts);\n    } catch (MetaException e) {\n      LOG.warn(\"Querying by partition names failed, trying out with partition objects, filter: {}\", filter);\n    }\n\n    if (partitionNames == null) {\n      partitions = getPartitionsByFilter(dbName, tableName, filter, (short) maxParts);\n    }\n\n    if (partitions != null) {\n      partitionNames = new ArrayList<String>(partitions.size());\n      for (Partition partition : partitions) {\n        // Check for NULL's just to be safe\n        if (tbl.getPartitionKeys() != null && partition.getValues() != null) {\n          partitionNames.add(Warehouse.makePartName(tbl.getPartitionKeys(), partition.getValues()));\n        }\n      }\n    }\n\n    if (partitionNames == null && partitions == null) {\n      throw new MetaException(\"Cannot obtain list of partitions by filter:\\\"\" + filter +\n          \"\\\" for \" + dbName + \":\" + tableName);\n    }\n\n    if (!ascending) {\n      Collections.sort(partitionNames, Collections.reverseOrder());\n    }\n\n    // Return proper response\n    PartitionValuesResponse response = new PartitionValuesResponse();\n    response.setPartitionValues(new ArrayList<PartitionValuesRow>(partitionNames.size()));\n    LOG.info(\"Converting responses to Partition values for items: {}\", partitionNames.size());\n    for (String partName : partitionNames) {\n      ArrayList<String> vals = new ArrayList<String>(Collections.nCopies(tbl.getPartitionKeys().size(), null));\n      PartitionValuesRow row = new PartitionValuesRow();\n      Warehouse.makeValsFromName(partName, vals);\n      for (String value : vals) {\n        row.addToRow(value);\n      }\n      response.addToPartitionValues(row);\n    }\n    return response;\n  }"
        ],
        [
            "ObjectStore::dropPartitions(String,String,List)",
            "2143  \n2144  \n2145  \n2146 -\n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  ",
            "  @Override\n  public void dropPartitions(String dbName, String tblName, List<String> partNames)\n      throws MetaException, NoSuchObjectException {\n    if (partNames.isEmpty()) return;\n    boolean success = false;\n    openTransaction();\n    try {\n      // Delete all things.\n      dropPartitionGrantsNoTxn(dbName, tblName, partNames);\n      dropPartitionAllColumnGrantsNoTxn(dbName, tblName, partNames);\n      dropPartitionColumnStatisticsNoTxn(dbName, tblName, partNames);\n\n      // CDs are reused; go thry partition SDs, detach all CDs from SDs, then remove unused CDs.\n      for (MColumnDescriptor mcd : detachCdsFromSdsNoTxn(dbName, tblName, partNames)) {\n        removeUnusedColumnDescriptor(mcd);\n      }\n      dropPartitionsNoTxn(dbName, tblName, partNames);\n      if (!(success = commitTransaction())) {\n        throw new MetaException(\"Failed to drop partitions\"); // Should not happen?\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }",
            "2132  \n2133  \n2134  \n2135 +\n2136 +\n2137 +\n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  ",
            "  @Override\n  public void dropPartitions(String dbName, String tblName, List<String> partNames)\n      throws MetaException, NoSuchObjectException {\n    if (CollectionUtils.isEmpty(partNames)) {\n      return;\n    }\n    boolean success = false;\n    openTransaction();\n    try {\n      // Delete all things.\n      dropPartitionGrantsNoTxn(dbName, tblName, partNames);\n      dropPartitionAllColumnGrantsNoTxn(dbName, tblName, partNames);\n      dropPartitionColumnStatisticsNoTxn(dbName, tblName, partNames);\n\n      // CDs are reused; go thry partition SDs, detach all CDs from SDs, then remove unused CDs.\n      for (MColumnDescriptor mcd : detachCdsFromSdsNoTxn(dbName, tblName, partNames)) {\n        removeUnusedColumnDescriptor(mcd);\n      }\n      dropPartitionsNoTxn(dbName, tblName, partNames);\n      if (!(success = commitTransaction())) {\n        throw new MetaException(\"Failed to drop partitions\"); // Should not happen?\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::getMasterKeys()",
            "8271  \n8272  \n8273  \n8274  \n8275  \n8276  \n8277  \n8278  \n8279  \n8280  \n8281  \n8282  \n8283  \n8284  \n8285  \n8286  \n8287  \n8288  \n8289  \n8290 -\n8291  \n8292  \n8293  ",
            "  @Override\n  public String[] getMasterKeys() {\n    LOG.debug(\"Begin executing getMasterKeys\");\n    boolean committed = false;\n    Query query = null;\n    List<MMasterKey> keys;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class);\n      keys = (List<MMasterKey>) query.execute();\n      pm.retrieveAll(keys);\n      committed = commitTransaction();\n\n      String[] masterKeys = new String[keys.size()];\n      for (int i = 0; i < keys.size(); i++) {\n        masterKeys[i] = keys.get(i).getMasterKey();\n      }\n      return masterKeys;\n    } finally {\n      LOG.debug(\"Done executing getMasterKeys with status : \" + committed);\n      rollbackAndCleanup(committed, query);\n    }\n  }",
            "8247  \n8248  \n8249  \n8250  \n8251  \n8252  \n8253  \n8254  \n8255  \n8256  \n8257  \n8258  \n8259  \n8260  \n8261  \n8262  \n8263  \n8264  \n8265  \n8266 +\n8267  \n8268  \n8269  ",
            "  @Override\n  public String[] getMasterKeys() {\n    LOG.debug(\"Begin executing getMasterKeys\");\n    boolean committed = false;\n    Query query = null;\n    List<MMasterKey> keys;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class);\n      keys = (List<MMasterKey>) query.execute();\n      pm.retrieveAll(keys);\n      committed = commitTransaction();\n\n      String[] masterKeys = new String[keys.size()];\n      for (int i = 0; i < keys.size(); i++) {\n        masterKeys[i] = keys.get(i).getMasterKey();\n      }\n      return masterKeys;\n    } finally {\n      LOG.debug(\"Done executing getMasterKeys with status : {}\", committed);\n      rollbackAndCleanup(committed, query);\n    }\n  }"
        ],
        [
            "ObjectStore::listPartitionNamesPs(String,String,List,short)",
            "2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745 -\n2746  \n2747 -\n2748 -\n2749 -\n2750  \n2751  \n2752  \n2753  \n2754  \n2755  ",
            "  @Override\n  public List<String> listPartitionNamesPs(String dbName, String tableName,\n      List<String> part_vals, short max_parts) throws MetaException, NoSuchObjectException {\n    List<String> partitionNames = new ArrayList<>();\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listPartitionNamesPs\");\n      Collection names = getPartitionPsQueryResults(dbName, tableName,\n          part_vals, max_parts, \"partitionName\", queryWrapper);\n      for (Object o : names) {\n        partitionNames.add((String) o);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return partitionNames;\n  }",
            "2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728 +\n2729  \n2730 +\n2731  \n2732  \n2733  \n2734  \n2735  \n2736  ",
            "  @Override\n  public List<String> listPartitionNamesPs(String dbName, String tableName,\n      List<String> part_vals, short max_parts) throws MetaException, NoSuchObjectException {\n    List<String> partitionNames = new ArrayList<>();\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listPartitionNamesPs\");\n      Collection<String> names = getPartitionPsQueryResults(dbName, tableName,\n          part_vals, max_parts, \"partitionName\", queryWrapper);\n      partitionNames.addAll(names);\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return partitionNames;\n  }"
        ],
        [
            "ObjectStore::listTableNamesByFilter(String,String,short)",
            "3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488 -\n3489 -\n3490 -\n3491 -\n3492  \n3493  \n3494  \n3495  \n3496 -\n3497  \n3498 -\n3499 -\n3500 -\n3501 -\n3502 -\n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  ",
            "  @Override\n  public List<String> listTableNamesByFilter(String dbName, String filter, short maxTables)\n      throws MetaException {\n    boolean success = false;\n    Query query = null;\n    List<String> tableNames = new ArrayList<>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listTableNamesByFilter\");\n      dbName = normalizeIdentifier(dbName);\n      Map<String, Object> params = new HashMap<>();\n      String queryFilterString = makeQueryFilterString(dbName, null, filter, params);\n      query = pm.newQuery(MTable.class);\n      query.declareImports(\"import java.lang.String\");\n      query.setResult(\"tableName\");\n      query.setResultClass(java.lang.String.class);\n      if (maxTables >= 0) {\n        query.setRange(0, maxTables);\n      }\n      LOG.debug(\"filter specified is \" + filter + \",\" + \" JDOQL filter is \" + queryFilterString);\n      for (Entry<String, Object> entry : params.entrySet()) {\n        LOG.debug(\"key: \" + entry.getKey() + \" value: \" + entry.getValue() + \" class: \"\n            + entry.getValue().getClass().getName());\n      }\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      query.setFilter(queryFilterString);\n      Collection names = (Collection)query.executeWithMap(params);\n      // have to emulate \"distinct\", otherwise tables with the same name may be returned\n      Set<String> tableNamesSet = new HashSet<>();\n      for (Iterator i = names.iterator(); i.hasNext();) {\n        tableNamesSet.add((String) i.next());\n      }\n      tableNames = new ArrayList<>(tableNamesSet);\n      LOG.debug(\"Done executing query for listTableNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listTableNamesByFilter\");\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return tableNames;\n  }",
            "3453  \n3454  \n3455  \n3456  \n3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472 +\n3473 +\n3474 +\n3475 +\n3476 +\n3477 +\n3478  \n3479  \n3480  \n3481  \n3482 +\n3483  \n3484 +\n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  ",
            "  @Override\n  public List<String> listTableNamesByFilter(String dbName, String filter, short maxTables)\n      throws MetaException {\n    boolean success = false;\n    Query query = null;\n    List<String> tableNames = new ArrayList<>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listTableNamesByFilter\");\n      dbName = normalizeIdentifier(dbName);\n      Map<String, Object> params = new HashMap<>();\n      String queryFilterString = makeQueryFilterString(dbName, null, filter, params);\n      query = pm.newQuery(MTable.class);\n      query.declareImports(\"import java.lang.String\");\n      query.setResult(\"tableName\");\n      query.setResultClass(java.lang.String.class);\n      if (maxTables >= 0) {\n        query.setRange(0, maxTables);\n      }\n      LOG.debug(\"filter specified is {}, JDOQL filter is {}\", filter, queryFilterString);\n      if (LOG.isDebugEnabled()) {\n        for (Entry<String, Object> entry : params.entrySet()) {\n          LOG.debug(\"key: {} value: {} class: {}\", entry.getKey(), entry.getValue(), \n             entry.getValue().getClass().getName());\n        }\n      }\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      query.setFilter(queryFilterString);\n      Collection<String> names = (Collection<String>)query.executeWithMap(params);\n      // have to emulate \"distinct\", otherwise tables with the same name may be returned\n      tableNames = new ArrayList<>(new HashSet<>(names));\n      LOG.debug(\"Done executing query for listTableNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listTableNamesByFilter\");\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return tableNames;\n  }"
        ],
        [
            "ObjectStore::getDatabases(String)",
            " 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968 -\n 969 -\n 970 -\n 971 -\n 972 -\n 973  \n 974  \n 975  \n 976  \n 977  \n 978  ",
            "  @Override\n  public List<String> getDatabases(String pattern) throws MetaException {\n    if (pattern == null || pattern.equals(\"*\")) {\n      return getAllDatabases();\n    }\n    boolean commited = false;\n    List<String> databases = null;\n    Query query = null;\n    try {\n      openTransaction();\n      // Take the pattern and split it on the | to get all the composing\n      // patterns\n      String[] subpatterns = pattern.trim().split(\"\\\\|\");\n      StringBuilder filterBuilder = new StringBuilder();\n      List<String> parameterVals = new ArrayList<>(subpatterns.length);\n      appendPatternCondition(filterBuilder, \"name\", subpatterns, parameterVals);\n      query = pm.newQuery(MDatabase.class, filterBuilder.toString());\n      query.setResult(\"name\");\n      query.setOrdering(\"name ascending\");\n      Collection names = (Collection) query.executeWithArray(parameterVals.toArray(new String[parameterVals.size()]));\n      databases = new ArrayList<>();\n      for (Iterator i = names.iterator(); i.hasNext();) {\n        databases.add((String) i.next());\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return databases;\n  }",
            " 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963 +\n 964 +\n 965  \n 966  \n 967  \n 968  \n 969  \n 970  ",
            "  @Override\n  public List<String> getDatabases(String pattern) throws MetaException {\n    if (pattern == null || pattern.equals(\"*\")) {\n      return getAllDatabases();\n    }\n    boolean commited = false;\n    List<String> databases = null;\n    Query query = null;\n    try {\n      openTransaction();\n      // Take the pattern and split it on the | to get all the composing\n      // patterns\n      String[] subpatterns = pattern.trim().split(\"\\\\|\");\n      StringBuilder filterBuilder = new StringBuilder();\n      List<String> parameterVals = new ArrayList<>(subpatterns.length);\n      appendPatternCondition(filterBuilder, \"name\", subpatterns, parameterVals);\n      query = pm.newQuery(MDatabase.class, filterBuilder.toString());\n      query.setResult(\"name\");\n      query.setOrdering(\"name ascending\");\n      Collection<String> names = (Collection<String>) query.executeWithArray(parameterVals.toArray(new String[0]));\n      databases = new ArrayList<>(names);\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return databases;\n  }"
        ],
        [
            "ObjectStore::deleteTableColumnStatistics(String,String,String)",
            "8004  \n8005  \n8006  \n8007  \n8008  \n8009 -\n8010 -\n8011 -\n8012  \n8013  \n8014  \n8015  \n8016  \n8017  \n8018  \n8019  \n8020  \n8021  \n8022  \n8023  \n8024  \n8025  \n8026  \n8027  \n8028  \n8029  \n8030  \n8031  \n8032  \n8033  \n8034  \n8035  \n8036  \n8037  \n8038  \n8039  \n8040  \n8041  \n8042  \n8043  \n8044  \n8045  \n8046  \n8047  \n8048  \n8049  \n8050  \n8051  \n8052  \n8053  \n8054  \n8055  \n8056  \n8057  \n8058  \n8059  \n8060  \n8061  \n8062  \n8063  \n8064  \n8065  \n8066  \n8067  \n8068  \n8069  \n8070  \n8071  \n8072  ",
            "  @Override\n  public boolean deleteTableColumnStatistics(String dbName, String tableName, String colName)\n      throws NoSuchObjectException, MetaException, InvalidObjectException, InvalidInputException {\n    boolean ret = false;\n    Query query = null;\n    if (dbName == null) {\n      dbName = Warehouse.DEFAULT_DATABASE_NAME;\n    }\n    if (tableName == null) {\n      throw new InvalidInputException(\"Table name is null.\");\n    }\n    try {\n      openTransaction();\n      MTable mTable = getMTable(dbName, tableName);\n      MTableColumnStatistics mStatsObj;\n      List<MTableColumnStatistics> mStatsObjColl;\n      if (mTable == null) {\n        throw new NoSuchObjectException(\"Table \" + tableName\n            + \"  for which stats deletion is requested doesn't exist\");\n      }\n      query = pm.newQuery(MTableColumnStatistics.class);\n      String filter;\n      String parameters;\n      if (colName != null) {\n        filter = \"table.tableName == t1 && dbName == t2 && colName == t3\";\n        parameters = \"java.lang.String t1, java.lang.String t2, java.lang.String t3\";\n      } else {\n        filter = \"table.tableName == t1 && dbName == t2\";\n        parameters = \"java.lang.String t1, java.lang.String t2\";\n      }\n\n      query.setFilter(filter);\n      query.declareParameters(parameters);\n      if (colName != null) {\n        query.setUnique(true);\n        mStatsObj =\n            (MTableColumnStatistics) query.execute(normalizeIdentifier(tableName),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(colName));\n        pm.retrieve(mStatsObj);\n\n        if (mStatsObj != null) {\n          pm.deletePersistent(mStatsObj);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" col=\" + colName);\n        }\n      } else {\n        mStatsObjColl =\n            (List<MTableColumnStatistics>) query.execute(\n                normalizeIdentifier(tableName),\n                normalizeIdentifier(dbName));\n        pm.retrieveAll(mStatsObjColl);\n        if (mStatsObjColl != null) {\n          pm.deletePersistentAll(mStatsObjColl);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName);\n        }\n      }\n      ret = commitTransaction();\n    } catch (NoSuchObjectException e) {\n      rollbackTransaction();\n      throw e;\n    } finally {\n      rollbackAndCleanup(ret, query);\n    }\n    return ret;\n  }",
            "7981  \n7982  \n7983  \n7984  \n7985  \n7986 +\n7987 +\n7988  \n7989  \n7990  \n7991  \n7992  \n7993  \n7994  \n7995  \n7996  \n7997  \n7998  \n7999  \n8000  \n8001  \n8002  \n8003  \n8004  \n8005  \n8006  \n8007  \n8008  \n8009  \n8010  \n8011  \n8012  \n8013  \n8014  \n8015  \n8016  \n8017  \n8018  \n8019  \n8020  \n8021  \n8022  \n8023  \n8024  \n8025  \n8026  \n8027  \n8028  \n8029  \n8030  \n8031  \n8032  \n8033  \n8034  \n8035  \n8036  \n8037  \n8038  \n8039  \n8040  \n8041  \n8042  \n8043  \n8044  \n8045  \n8046  \n8047  \n8048  ",
            "  @Override\n  public boolean deleteTableColumnStatistics(String dbName, String tableName, String colName)\n      throws NoSuchObjectException, MetaException, InvalidObjectException, InvalidInputException {\n    boolean ret = false;\n    Query query = null;\n    dbName = org.apache.commons.lang.StringUtils.defaultString(dbName,\n      Warehouse.DEFAULT_DATABASE_NAME);\n    if (tableName == null) {\n      throw new InvalidInputException(\"Table name is null.\");\n    }\n    try {\n      openTransaction();\n      MTable mTable = getMTable(dbName, tableName);\n      MTableColumnStatistics mStatsObj;\n      List<MTableColumnStatistics> mStatsObjColl;\n      if (mTable == null) {\n        throw new NoSuchObjectException(\"Table \" + tableName\n            + \"  for which stats deletion is requested doesn't exist\");\n      }\n      query = pm.newQuery(MTableColumnStatistics.class);\n      String filter;\n      String parameters;\n      if (colName != null) {\n        filter = \"table.tableName == t1 && dbName == t2 && colName == t3\";\n        parameters = \"java.lang.String t1, java.lang.String t2, java.lang.String t3\";\n      } else {\n        filter = \"table.tableName == t1 && dbName == t2\";\n        parameters = \"java.lang.String t1, java.lang.String t2\";\n      }\n\n      query.setFilter(filter);\n      query.declareParameters(parameters);\n      if (colName != null) {\n        query.setUnique(true);\n        mStatsObj =\n            (MTableColumnStatistics) query.execute(normalizeIdentifier(tableName),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(colName));\n        pm.retrieve(mStatsObj);\n\n        if (mStatsObj != null) {\n          pm.deletePersistent(mStatsObj);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" col=\" + colName);\n        }\n      } else {\n        mStatsObjColl =\n            (List<MTableColumnStatistics>) query.execute(\n                normalizeIdentifier(tableName),\n                normalizeIdentifier(dbName));\n        pm.retrieveAll(mStatsObjColl);\n        if (mStatsObjColl != null) {\n          pm.deletePersistentAll(mStatsObjColl);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName);\n        }\n      }\n      ret = commitTransaction();\n    } catch (NoSuchObjectException e) {\n      rollbackTransaction();\n      throw e;\n    } finally {\n      rollbackAndCleanup(ret, query);\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::dropConstraint(String,String,String)",
            "9413  \n9414  \n9415  \n9416  \n9417  \n9418  \n9419  \n9420  \n9421  \n9422 -\n9423  \n9424  \n9425  \n9426  \n9427  \n9428  \n9429  \n9430  \n9431  \n9432  \n9433  \n9434  ",
            "  @Override\n  public void dropConstraint(String dbName, String tableName,\n    String constraintName) throws NoSuchObjectException {\n    boolean success = false;\n    try {\n      openTransaction();\n\n      List<MConstraint> tabConstraints = listAllTableConstraintsWithOptionalConstraintName(\n                                         dbName, tableName, constraintName);\n      if (tabConstraints != null && tabConstraints.size() > 0) {\n        pm.deletePersistentAll(tabConstraints);\n      } else {\n        throw new NoSuchObjectException(\"The constraint: \" + constraintName +\n          \" does not exist for the associated table: \" + dbName + \".\" + tableName);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }",
            "9388  \n9389  \n9390  \n9391  \n9392  \n9393  \n9394  \n9395  \n9396  \n9397 +\n9398  \n9399  \n9400  \n9401  \n9402  \n9403  \n9404  \n9405  \n9406  \n9407  \n9408  \n9409  ",
            "  @Override\n  public void dropConstraint(String dbName, String tableName,\n    String constraintName) throws NoSuchObjectException {\n    boolean success = false;\n    try {\n      openTransaction();\n\n      List<MConstraint> tabConstraints = listAllTableConstraintsWithOptionalConstraintName(\n                                         dbName, tableName, constraintName);\n      if (CollectionUtils.isNotEmpty(tabConstraints)) {\n        pm.deletePersistentAll(tabConstraints);\n      } else {\n        throw new NoSuchObjectException(\"The constraint: \" + constraintName +\n          \" does not exist for the associated table: \" + dbName + \".\" + tableName);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::getToken(String)",
            "8152  \n8153  \n8154  \n8155  \n8156  \n8157  \n8158  \n8159  \n8160  \n8161  \n8162  \n8163  \n8164  \n8165  \n8166  \n8167  \n8168  \n8169  \n8170 -\n8171  \n8172  ",
            "  @Override\n  public String getToken(String tokenId) {\n\n    LOG.debug(\"Begin executing getToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (null != token) {\n        pm.retrieve(token);\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing getToken with status : \" + committed);\n    return (null == token) ? null : token.getTokenStr();\n  }",
            "8128  \n8129  \n8130  \n8131  \n8132  \n8133  \n8134  \n8135  \n8136  \n8137  \n8138  \n8139  \n8140  \n8141  \n8142  \n8143  \n8144  \n8145  \n8146 +\n8147  \n8148  ",
            "  @Override\n  public String getToken(String tokenId) {\n\n    LOG.debug(\"Begin executing getToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (null != token) {\n        pm.retrieve(token);\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing getToken with status : {}\", committed);\n    return (null == token) ? null : token.getTokenStr();\n  }"
        ],
        [
            "ObjectStore::removeRole(String)",
            "4663  \n4664  \n4665  \n4666  \n4667  \n4668  \n4669  \n4670  \n4671  \n4672  \n4673  \n4674  \n4675  \n4676 -\n4677  \n4678  \n4679  \n4680  \n4681 -\n4682  \n4683  \n4684  \n4685  \n4686  \n4687  \n4688 -\n4689  \n4690  \n4691  \n4692  \n4693 -\n4694  \n4695  \n4696  \n4697  \n4698  \n4699 -\n4700  \n4701  \n4702  \n4703  \n4704  \n4705 -\n4706  \n4707  \n4708  \n4709  \n4710  \n4711 -\n4712  \n4713  \n4714  \n4715  \n4716  \n4717 -\n4718  \n4719  \n4720  \n4721  \n4722  \n4723  \n4724  \n4725  \n4726  \n4727  \n4728  \n4729  \n4730  ",
            "  @Override\n  public boolean removeRole(String roleName) throws MetaException,\n      NoSuchObjectException {\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      openTransaction();\n      MRole mRol = getMRole(roleName);\n      pm.retrieve(mRol);\n      if (mRol != null) {\n        // first remove all the membership, the membership that this role has\n        // been granted\n        List<MRoleMap> roleMap = listMRoleMembers(mRol.getRoleName());\n        if (roleMap.size() > 0) {\n          pm.deletePersistentAll(roleMap);\n        }\n        List<MRoleMap> roleMember = listMSecurityPrincipalMembershipRole(mRol\n            .getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (roleMember.size() > 0) {\n          pm.deletePersistentAll(roleMember);\n        }\n        queryWrapper.close();\n        // then remove all the grants\n        List<MGlobalPrivilege> userGrants = listPrincipalMGlobalGrants(\n            mRol.getRoleName(), PrincipalType.ROLE);\n        if (userGrants.size() > 0) {\n          pm.deletePersistentAll(userGrants);\n        }\n        List<MDBPrivilege> dbGrants = listPrincipalAllDBGrant(mRol\n            .getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (dbGrants.size() > 0) {\n          pm.deletePersistentAll(dbGrants);\n        }\n        queryWrapper.close();\n        List<MTablePrivilege> tabPartGrants = listPrincipalAllTableGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (tabPartGrants.size() > 0) {\n          pm.deletePersistentAll(tabPartGrants);\n        }\n        queryWrapper.close();\n        List<MPartitionPrivilege> partGrants = listPrincipalAllPartitionGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (partGrants.size() > 0) {\n          pm.deletePersistentAll(partGrants);\n        }\n        queryWrapper.close();\n        List<MTableColumnPrivilege> tblColumnGrants = listPrincipalAllTableColumnGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (tblColumnGrants.size() > 0) {\n          pm.deletePersistentAll(tblColumnGrants);\n        }\n        queryWrapper.close();\n        List<MPartitionColumnPrivilege> partColumnGrants = listPrincipalAllPartitionColumnGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (partColumnGrants.size() > 0) {\n          pm.deletePersistentAll(partColumnGrants);\n        }\n        queryWrapper.close();\n\n        // finally remove the role\n        pm.deletePersistent(mRol);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return success;\n  }",
            "4641  \n4642  \n4643  \n4644  \n4645  \n4646  \n4647  \n4648  \n4649  \n4650  \n4651  \n4652  \n4653  \n4654 +\n4655  \n4656  \n4657  \n4658  \n4659 +\n4660  \n4661  \n4662  \n4663  \n4664  \n4665  \n4666 +\n4667  \n4668  \n4669  \n4670  \n4671 +\n4672  \n4673  \n4674  \n4675  \n4676  \n4677 +\n4678  \n4679  \n4680  \n4681  \n4682  \n4683 +\n4684  \n4685  \n4686  \n4687  \n4688  \n4689 +\n4690  \n4691  \n4692  \n4693  \n4694  \n4695 +\n4696  \n4697  \n4698  \n4699  \n4700  \n4701  \n4702  \n4703  \n4704  \n4705  \n4706  \n4707  \n4708  ",
            "  @Override\n  public boolean removeRole(String roleName) throws MetaException,\n      NoSuchObjectException {\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      openTransaction();\n      MRole mRol = getMRole(roleName);\n      pm.retrieve(mRol);\n      if (mRol != null) {\n        // first remove all the membership, the membership that this role has\n        // been granted\n        List<MRoleMap> roleMap = listMRoleMembers(mRol.getRoleName());\n        if (CollectionUtils.isNotEmpty(roleMap)) {\n          pm.deletePersistentAll(roleMap);\n        }\n        List<MRoleMap> roleMember = listMSecurityPrincipalMembershipRole(mRol\n            .getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(roleMember)) {\n          pm.deletePersistentAll(roleMember);\n        }\n        queryWrapper.close();\n        // then remove all the grants\n        List<MGlobalPrivilege> userGrants = listPrincipalMGlobalGrants(\n            mRol.getRoleName(), PrincipalType.ROLE);\n        if (CollectionUtils.isNotEmpty(userGrants)) {\n          pm.deletePersistentAll(userGrants);\n        }\n        List<MDBPrivilege> dbGrants = listPrincipalAllDBGrant(mRol\n            .getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(dbGrants)) {\n          pm.deletePersistentAll(dbGrants);\n        }\n        queryWrapper.close();\n        List<MTablePrivilege> tabPartGrants = listPrincipalAllTableGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(tabPartGrants)) {\n          pm.deletePersistentAll(tabPartGrants);\n        }\n        queryWrapper.close();\n        List<MPartitionPrivilege> partGrants = listPrincipalAllPartitionGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(partGrants)) {\n          pm.deletePersistentAll(partGrants);\n        }\n        queryWrapper.close();\n        List<MTableColumnPrivilege> tblColumnGrants = listPrincipalAllTableColumnGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(tblColumnGrants)) {\n          pm.deletePersistentAll(tblColumnGrants);\n        }\n        queryWrapper.close();\n        List<MPartitionColumnPrivilege> partColumnGrants = listPrincipalAllPartitionColumnGrants(\n            mRol.getRoleName(), PrincipalType.ROLE, queryWrapper);\n        if (CollectionUtils.isNotEmpty(partColumnGrants)) {\n          pm.deletePersistentAll(partColumnGrants);\n        }\n        queryWrapper.close();\n\n        // finally remove the role\n        pm.deletePersistent(mRol);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::getMPartition(String,String,List)",
            "2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041 -\n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  ",
            "  private MPartition getMPartition(String dbName, String tableName, List<String> part_vals)\n      throws MetaException {\n    List<MPartition> mparts = null;\n    MPartition ret = null;\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      MTable mtbl = getMTable(dbName, tableName);\n      if (mtbl == null) {\n        commited = commitTransaction();\n        return null;\n      }\n      // Change the query to use part_vals instead of the name which is\n      // redundant TODO: callers of this often get part_vals out of name for no reason...\n      String name =\n          Warehouse.makePartName(convertToFieldSchemas(mtbl.getPartitionKeys()), part_vals);\n      query =\n          pm.newQuery(MPartition.class,\n              \"table.tableName == t1 && table.database.name == t2 && partitionName == t3\");\n      query.declareParameters(\"java.lang.String t1, java.lang.String t2, java.lang.String t3\");\n      mparts = (List<MPartition>) query.execute(tableName, dbName, name);\n      pm.retrieveAll(mparts);\n      commited = commitTransaction();\n      // We need to compare partition name with requested name since some DBs\n      // (like MySQL, Derby) considers 'a' = 'a ' whereas others like (Postgres,\n      // Oracle) doesn't exhibit this problem.\n      if (mparts != null && mparts.size() > 0) {\n        if (mparts.size() > 1) {\n          throw new MetaException(\n              \"Expecting only one partition but more than one partitions are found.\");\n        } else {\n          MPartition mpart = mparts.get(0);\n          if (name.equals(mpart.getPartitionName())) {\n            ret = mpart;\n          } else {\n            throw new MetaException(\"Expecting a partition with name \" + name\n                + \", but metastore is returning a partition with name \" + mpart.getPartitionName()\n                + \".\");\n          }\n        }\n      }\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return ret;\n  }",
            "2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030 +\n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  ",
            "  private MPartition getMPartition(String dbName, String tableName, List<String> part_vals)\n      throws MetaException {\n    List<MPartition> mparts = null;\n    MPartition ret = null;\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      MTable mtbl = getMTable(dbName, tableName);\n      if (mtbl == null) {\n        commited = commitTransaction();\n        return null;\n      }\n      // Change the query to use part_vals instead of the name which is\n      // redundant TODO: callers of this often get part_vals out of name for no reason...\n      String name =\n          Warehouse.makePartName(convertToFieldSchemas(mtbl.getPartitionKeys()), part_vals);\n      query =\n          pm.newQuery(MPartition.class,\n              \"table.tableName == t1 && table.database.name == t2 && partitionName == t3\");\n      query.declareParameters(\"java.lang.String t1, java.lang.String t2, java.lang.String t3\");\n      mparts = (List<MPartition>) query.execute(tableName, dbName, name);\n      pm.retrieveAll(mparts);\n      commited = commitTransaction();\n      // We need to compare partition name with requested name since some DBs\n      // (like MySQL, Derby) considers 'a' = 'a ' whereas others like (Postgres,\n      // Oracle) doesn't exhibit this problem.\n      if (CollectionUtils.isNotEmpty(mparts)) {\n        if (mparts.size() > 1) {\n          throw new MetaException(\n              \"Expecting only one partition but more than one partitions are found.\");\n        } else {\n          MPartition mpart = mparts.get(0);\n          if (name.equals(mpart.getPartitionName())) {\n            ret = mpart;\n          } else {\n            throw new MetaException(\"Expecting a partition with name \" + name\n                + \", but metastore is returning a partition with name \" + mpart.getPartitionName()\n                + \".\");\n          }\n        }\n      }\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::getTables(String,String,TableType)",
            "1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329 -\n1330 -\n1331 -\n1332 -\n1333 -\n1334  \n1335  \n1336  \n1337  \n1338  \n1339  ",
            "  @Override\n  public List<String> getTables(String dbName, String pattern, TableType tableType) throws MetaException {\n    boolean commited = false;\n    Query query = null;\n    List<String> tbls = null;\n    try {\n      openTransaction();\n      dbName = normalizeIdentifier(dbName);\n      // Take the pattern and split it on the | to get all the composing\n      // patterns\n      List<String> parameterVals = new ArrayList<>();\n      StringBuilder filterBuilder = new StringBuilder();\n      //adds database.name == dbName to the filter\n      appendSimpleCondition(filterBuilder, \"database.name\", new String[] {dbName}, parameterVals);\n      if(pattern != null) {\n        appendPatternCondition(filterBuilder, \"tableName\", pattern, parameterVals);\n      }\n      if(tableType != null) {\n        appendPatternCondition(filterBuilder, \"tableType\", new String[] {tableType.toString()}, parameterVals);\n      }\n\n      query = pm.newQuery(MTable.class, filterBuilder.toString());\n      query.setResult(\"tableName\");\n      query.setOrdering(\"tableName ascending\");\n      Collection names = (Collection) query.executeWithArray(parameterVals.toArray(new String[parameterVals.size()]));\n      tbls = new ArrayList<>();\n      for (Iterator i = names.iterator(); i.hasNext();) {\n        tbls.add((String) i.next());\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return tbls;\n  }",
            "1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321 +\n1322 +\n1323  \n1324  \n1325  \n1326  \n1327  \n1328  ",
            "  @Override\n  public List<String> getTables(String dbName, String pattern, TableType tableType) throws MetaException {\n    boolean commited = false;\n    Query query = null;\n    List<String> tbls = null;\n    try {\n      openTransaction();\n      dbName = normalizeIdentifier(dbName);\n      // Take the pattern and split it on the | to get all the composing\n      // patterns\n      List<String> parameterVals = new ArrayList<>();\n      StringBuilder filterBuilder = new StringBuilder();\n      //adds database.name == dbName to the filter\n      appendSimpleCondition(filterBuilder, \"database.name\", new String[] {dbName}, parameterVals);\n      if(pattern != null) {\n        appendPatternCondition(filterBuilder, \"tableName\", pattern, parameterVals);\n      }\n      if(tableType != null) {\n        appendPatternCondition(filterBuilder, \"tableType\", new String[] {tableType.toString()}, parameterVals);\n      }\n\n      query = pm.newQuery(MTable.class, filterBuilder.toString());\n      query.setResult(\"tableName\");\n      query.setOrdering(\"tableName ascending\");\n      Collection<String> names = (Collection<String>) query.executeWithArray(parameterVals.toArray(new String[0]));\n      tbls = new ArrayList<>(names);\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return tbls;\n  }"
        ],
        [
            "ObjectStore::configureSSL(Configuration)",
            " 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533 -\n 534  \n 535  \n 536  \n 537  ",
            "  /**\n   * Configure the SSL properties of the connection from provided config\n   * @param conf\n   */\n  private static void configureSSL(Configuration conf) {\n    // SSL support\n    String sslPropString = MetastoreConf.getVar(conf, ConfVars.DBACCESS_SSL_PROPS);\n    if (org.apache.commons.lang.StringUtils.isNotEmpty(sslPropString)) {\n      LOG.info(\"Metastore setting SSL properties of the connection to backed DB\");\n      for (String sslProp : sslPropString.split(\",\")) {\n        String[] pair = sslProp.trim().split(\"=\");\n        if (pair != null && pair.length == 2) {\n          System.setProperty(pair[0].trim(), pair[1].trim());\n        } else {\n          LOG.warn(\"Invalid metastore property value for \" + ConfVars.DBACCESS_SSL_PROPS);\n        }\n      }\n    }\n  }",
            " 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528 +\n 529  \n 530  \n 531  \n 532  ",
            "  /**\n   * Configure the SSL properties of the connection from provided config\n   * @param conf\n   */\n  private static void configureSSL(Configuration conf) {\n    // SSL support\n    String sslPropString = MetastoreConf.getVar(conf, ConfVars.DBACCESS_SSL_PROPS);\n    if (org.apache.commons.lang.StringUtils.isNotEmpty(sslPropString)) {\n      LOG.info(\"Metastore setting SSL properties of the connection to backed DB\");\n      for (String sslProp : sslPropString.split(\",\")) {\n        String[] pair = sslProp.trim().split(\"=\");\n        if (pair != null && pair.length == 2) {\n          System.setProperty(pair[0].trim(), pair[1].trim());\n        } else {\n          LOG.warn(\"Invalid metastore property value for {}\", ConfVars.DBACCESS_SSL_PROPS);\n        }\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::listPartitionValues(String,String,List,boolean,String,boolean,List,long)",
            "2413  \n2414  \n2415  \n2416  \n2417  \n2418  \n2419  \n2420  \n2421  \n2422  \n2423  \n2424 -\n2425  \n2426  \n2427  \n2428  \n2429  \n2430 -\n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  ",
            "  @Override\n  public PartitionValuesResponse listPartitionValues(String dbName, String tableName, List<FieldSchema> cols,\n                                                     boolean applyDistinct, String filter, boolean ascending,\n                                                     List<FieldSchema> order, long maxParts) throws MetaException {\n\n    dbName = dbName.toLowerCase().trim();\n    tableName = tableName.toLowerCase().trim();\n    try {\n      if (filter == null || filter.isEmpty()) {\n        PartitionValuesResponse response =\n            getDistinctValuesForPartitionsNoTxn(dbName, tableName, cols, applyDistinct, ascending, maxParts);\n        LOG.info(\"Number of records fetched: \" + response.getPartitionValues().size());\n        return response;\n      } else {\n        PartitionValuesResponse response =\n            extractPartitionNamesByFilter(dbName, tableName, filter, cols, ascending, applyDistinct, maxParts);\n        if (response != null && response.getPartitionValues() != null) {\n          LOG.info(\"Number of records fetched with filter: \" + response.getPartitionValues().size());\n        }\n        return response;\n      }\n    } catch (Exception t) {\n      LOG.error(\"Exception in ORM\", t);\n      throw new MetaException(\"Error retrieving partition values: \" + t);\n    } finally {\n    }\n  }",
            "2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415 +\n2416  \n2417  \n2418  \n2419  \n2420  \n2421 +\n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  ",
            "  @Override\n  public PartitionValuesResponse listPartitionValues(String dbName, String tableName, List<FieldSchema> cols,\n                                                     boolean applyDistinct, String filter, boolean ascending,\n                                                     List<FieldSchema> order, long maxParts) throws MetaException {\n\n    dbName = dbName.toLowerCase().trim();\n    tableName = tableName.toLowerCase().trim();\n    try {\n      if (filter == null || filter.isEmpty()) {\n        PartitionValuesResponse response =\n            getDistinctValuesForPartitionsNoTxn(dbName, tableName, cols, applyDistinct, ascending, maxParts);\n        LOG.info(\"Number of records fetched: {}\", response.getPartitionValues().size());\n        return response;\n      } else {\n        PartitionValuesResponse response =\n            extractPartitionNamesByFilter(dbName, tableName, filter, cols, ascending, applyDistinct, maxParts);\n        if (response != null && response.getPartitionValues() != null) {\n          LOG.info(\"Number of records fetched with filter: {}\", response.getPartitionValues().size());\n        }\n        return response;\n      }\n    } catch (Exception t) {\n      LOG.error(\"Exception in ORM\", t);\n      throw new MetaException(\"Error retrieving partition values: \" + t);\n    } finally {\n    }\n  }"
        ],
        [
            "ObjectStore::makeParameterDeclarationStringObj(Map)",
            "3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463 -\n3464  \n3465  \n3466  \n3467  ",
            "  private String makeParameterDeclarationStringObj(Map<String, Object> params) {\n    //Create the parameter declaration string\n    StringBuilder paramDecl = new StringBuilder();\n    for (Entry<String, Object> entry : params.entrySet()) {\n      paramDecl.append(\", \");\n      paramDecl.append(entry.getValue().getClass().getName());\n      paramDecl.append(\" \");\n      paramDecl.append(entry.getKey());\n    }\n    return paramDecl.toString();\n  }",
            "3441  \n3442  \n3443  \n3444  \n3445  \n3446  \n3447 +\n3448  \n3449  \n3450  \n3451  ",
            "  private String makeParameterDeclarationStringObj(Map<String, Object> params) {\n    //Create the parameter declaration string\n    StringBuilder paramDecl = new StringBuilder();\n    for (Entry<String, Object> entry : params.entrySet()) {\n      paramDecl.append(\", \");\n      paramDecl.append(entry.getValue().getClass().getName());\n      paramDecl.append(' ');\n      paramDecl.append(entry.getKey());\n    }\n    return paramDecl.toString();\n  }"
        ],
        [
            "ObjectStore::cleanNotificationEvents(int)",
            "8828  \n8829  \n8830  \n8831  \n8832  \n8833  \n8834  \n8835  \n8836  \n8837  \n8838  \n8839 -\n8840  \n8841  \n8842  \n8843  \n8844  \n8845  \n8846  ",
            "  @Override\n  public void cleanNotificationEvents(int olderThan) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      long tmp = System.currentTimeMillis() / 1000 - olderThan;\n      int tooOld = (tmp > Integer.MAX_VALUE) ? 0 : (int) tmp;\n      query = pm.newQuery(MNotificationLog.class, \"eventTime < tooOld\");\n      query.declareParameters(\"java.lang.Integer tooOld\");\n      Collection<MNotificationLog> toBeRemoved = (Collection) query.execute(tooOld);\n      if (toBeRemoved != null && toBeRemoved.size() > 0) {\n        pm.deletePersistentAll(toBeRemoved);\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }",
            "8803  \n8804  \n8805  \n8806  \n8807  \n8808  \n8809  \n8810  \n8811  \n8812  \n8813  \n8814 +\n8815  \n8816  \n8817  \n8818  \n8819  \n8820  \n8821  ",
            "  @Override\n  public void cleanNotificationEvents(int olderThan) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      long tmp = System.currentTimeMillis() / 1000 - olderThan;\n      int tooOld = (tmp > Integer.MAX_VALUE) ? 0 : (int) tmp;\n      query = pm.newQuery(MNotificationLog.class, \"eventTime < tooOld\");\n      query.declareParameters(\"java.lang.Integer tooOld\");\n      Collection<MNotificationLog> toBeRemoved = (Collection) query.execute(tooOld);\n      if (CollectionUtils.isNotEmpty(toBeRemoved)) {\n        pm.deletePersistentAll(toBeRemoved);\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ],
        [
            "ObjectStore::updateMasterKey(Integer,String)",
            "8220  \n8221  \n8222  \n8223  \n8224  \n8225  \n8226  \n8227  \n8228  \n8229  \n8230  \n8231  \n8232  \n8233  \n8234  \n8235  \n8236  \n8237  \n8238  \n8239 -\n8240  \n8241  \n8242  \n8243  \n8244  \n8245  \n8246  ",
            "  @Override\n  public void updateMasterKey(Integer id, String key) throws NoSuchObjectException, MetaException {\n    LOG.debug(\"Begin executing updateMasterKey\");\n    boolean committed = false;\n    Query query = null;\n    MMasterKey masterKey;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class, \"keyId == id\");\n      query.declareParameters(\"java.lang.Integer id\");\n      query.setUnique(true);\n      masterKey = (MMasterKey) query.execute(id);\n      if (null != masterKey) {\n        masterKey.setMasterKey(key);\n      }\n      committed = commitTransaction();\n    } finally {\n      rollbackAndCleanup(committed, query);\n    }\n    LOG.debug(\"Done executing updateMasterKey with status : \" + committed);\n    if (null == masterKey) {\n      throw new NoSuchObjectException(\"No key found with keyId: \" + id);\n    }\n    if (!committed) {\n      throw new MetaException(\"Though key is found, failed to update it. \" + id);\n    }\n  }",
            "8196  \n8197  \n8198  \n8199  \n8200  \n8201  \n8202  \n8203  \n8204  \n8205  \n8206  \n8207  \n8208  \n8209  \n8210  \n8211  \n8212  \n8213  \n8214  \n8215 +\n8216  \n8217  \n8218  \n8219  \n8220  \n8221  \n8222  ",
            "  @Override\n  public void updateMasterKey(Integer id, String key) throws NoSuchObjectException, MetaException {\n    LOG.debug(\"Begin executing updateMasterKey\");\n    boolean committed = false;\n    Query query = null;\n    MMasterKey masterKey;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class, \"keyId == id\");\n      query.declareParameters(\"java.lang.Integer id\");\n      query.setUnique(true);\n      masterKey = (MMasterKey) query.execute(id);\n      if (null != masterKey) {\n        masterKey.setMasterKey(key);\n      }\n      committed = commitTransaction();\n    } finally {\n      rollbackAndCleanup(committed, query);\n    }\n    LOG.debug(\"Done executing updateMasterKey with status : {}\", committed);\n    if (null == masterKey) {\n      throw new NoSuchObjectException(\"No key found with keyId: \" + id);\n    }\n    if (!committed) {\n      throw new MetaException(\"Though key is found, failed to update it. \" + id);\n    }\n  }"
        ],
        [
            "ObjectStore::getPMF()",
            " 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643 -\n 644 -\n 645 -\n 646 -\n 647  \n 648  \n 649  \n 650  \n 651 -\n 652 -\n 653 -\n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  ",
            "  private static synchronized PersistenceManagerFactory getPMF() {\n    if (pmf == null) {\n\n      Configuration conf = MetastoreConf.newMetastoreConf();\n      DataSourceProvider dsp = DataSourceProviderFactory.getDataSourceProvider(conf);\n      if (dsp == null) {\n        pmf = JDOHelper.getPersistenceManagerFactory(prop);\n      } else {\n        try {\n          DataSource ds = dsp.create(conf);\n          Map<Object, Object> dsProperties = new HashMap<>();\n          //Any preexisting datanucleus property should be passed along\n          dsProperties.putAll(prop);\n          dsProperties.put(\"datanucleus.ConnectionFactory\", ds);\n          dsProperties.put(\"javax.jdo.PersistenceManagerFactoryClass\",\n              \"org.datanucleus.api.jdo.JDOPersistenceManagerFactory\");\n          pmf = JDOHelper.getPersistenceManagerFactory(dsProperties);\n        } catch (SQLException e) {\n          LOG.warn(\"Could not create PersistenceManagerFactory using \" +\n              \"connection pool properties, will fall back\", e);\n          pmf = JDOHelper.getPersistenceManagerFactory(prop);\n        }\n      }\n      DataStoreCache dsc = pmf.getDataStoreCache();\n      if (dsc != null) {\n        String objTypes = MetastoreConf.getVar(conf, ConfVars.CACHE_PINOBJTYPES);\n        LOG.info(\"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\\\"\" + objTypes + \"\\\"\");\n        if (objTypes != null && objTypes.length() > 0) {\n          objTypes = objTypes.toLowerCase();\n          String[] typeTokens = objTypes.split(\",\");\n          for (String type : typeTokens) {\n            type = type.trim();\n            if (PINCLASSMAP.containsKey(type)) {\n              dsc.pinAll(true, PINCLASSMAP.get(type));\n            }\n            else {\n              LOG.warn(type + \" is not one of the pinnable object types: \" + org.apache.commons.lang.StringUtils.join(PINCLASSMAP.keySet(), \" \"));\n            }\n          }\n        }\n      } else {\n        LOG.warn(\"PersistenceManagerFactory returned null DataStoreCache object. Unable to initialize object pin types defined by hive.metastore.cache.pinobjtypes\");\n      }\n    }\n    return pmf;\n  }",
            " 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 +\n 640 +\n 641 +\n 642  \n 643  \n 644  \n 645  \n 646 +\n 647 +\n 648 +\n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  ",
            "  private static synchronized PersistenceManagerFactory getPMF() {\n    if (pmf == null) {\n\n      Configuration conf = MetastoreConf.newMetastoreConf();\n      DataSourceProvider dsp = DataSourceProviderFactory.getDataSourceProvider(conf);\n      if (dsp == null) {\n        pmf = JDOHelper.getPersistenceManagerFactory(prop);\n      } else {\n        try {\n          DataSource ds = dsp.create(conf);\n          Map<Object, Object> dsProperties = new HashMap<>();\n          //Any preexisting datanucleus property should be passed along\n          dsProperties.putAll(prop);\n          dsProperties.put(\"datanucleus.ConnectionFactory\", ds);\n          dsProperties.put(\"javax.jdo.PersistenceManagerFactoryClass\",\n              \"org.datanucleus.api.jdo.JDOPersistenceManagerFactory\");\n          pmf = JDOHelper.getPersistenceManagerFactory(dsProperties);\n        } catch (SQLException e) {\n          LOG.warn(\"Could not create PersistenceManagerFactory using \" +\n              \"connection pool properties, will fall back\", e);\n          pmf = JDOHelper.getPersistenceManagerFactory(prop);\n        }\n      }\n      DataStoreCache dsc = pmf.getDataStoreCache();\n      if (dsc != null) {\n        String objTypes = MetastoreConf.getVar(conf, ConfVars.CACHE_PINOBJTYPES);\n        LOG.info(\"Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\\\"{}\\\"\", objTypes);\n        if (org.apache.commons.lang.StringUtils.isNotEmpty(objTypes)) {\n          String[] typeTokens = objTypes.toLowerCase().split(\",\");\n          for (String type : typeTokens) {\n            type = type.trim();\n            if (PINCLASSMAP.containsKey(type)) {\n              dsc.pinAll(true, PINCLASSMAP.get(type));\n            } else {\n              LOG.warn(\"{} is not one of the pinnable object types: {}\", type,\n                org.apache.commons.lang.StringUtils.join(PINCLASSMAP.keySet(), \" \"));\n            }\n          }\n        }\n      } else {\n        LOG.warn(\"PersistenceManagerFactory returned null DataStoreCache object. Unable to initialize object pin types defined by hive.metastore.cache.pinobjtypes\");\n      }\n    }\n    return pmf;\n  }"
        ],
        [
            "ObjectStore::grantPrivileges(PrivilegeBag)",
            "5268  \n5269  \n5270  \n5271  \n5272  \n5273  \n5274  \n5275  \n5276  \n5277  \n5278  \n5279 -\n5280  \n5281  \n5282  \n5283  \n5284  \n5285  \n5286  \n5287  \n5288  \n5289  \n5290  \n5291  \n5292  \n5293  \n5294  \n5295  \n5296  \n5297  \n5298  \n5299  \n5300  \n5301  \n5302  \n5303  \n5304  \n5305  \n5306  \n5307  \n5308  \n5309  \n5310  \n5311  \n5312  \n5313  \n5314  \n5315  \n5316  \n5317  \n5318  \n5319  \n5320  \n5321  \n5322  \n5323  \n5324  \n5325  \n5326  \n5327  \n5328  \n5329  \n5330  \n5331  \n5332  \n5333  \n5334  \n5335  \n5336  \n5337  \n5338  \n5339  \n5340  \n5341  \n5342  \n5343  \n5344  \n5345  \n5346  \n5347  \n5348  \n5349  \n5350  \n5351  \n5352  \n5353  \n5354  \n5355  \n5356  \n5357  \n5358  \n5359  \n5360  \n5361  \n5362  \n5363  \n5364  \n5365  \n5366  \n5367  \n5368  \n5369  \n5370  \n5371  \n5372  \n5373  \n5374  \n5375  \n5376  \n5377  \n5378  \n5379  \n5380  \n5381  \n5382  \n5383  \n5384  \n5385  \n5386  \n5387  \n5388  \n5389  \n5390  \n5391  \n5392  \n5393  \n5394  \n5395  \n5396  \n5397  \n5398  \n5399  \n5400  \n5401  \n5402  \n5403  \n5404  \n5405  \n5406  \n5407  \n5408  \n5409  \n5410  \n5411  \n5412  \n5413  \n5414  \n5415  \n5416  \n5417  \n5418  \n5419  \n5420  \n5421  \n5422  \n5423  \n5424  \n5425  \n5426  \n5427  \n5428  \n5429  \n5430  \n5431  \n5432  \n5433  \n5434  \n5435  \n5436  \n5437  \n5438  \n5439  \n5440  \n5441  \n5442  \n5443  \n5444  \n5445  \n5446  \n5447  \n5448  \n5449  \n5450  \n5451  \n5452  \n5453  \n5454  \n5455  \n5456  \n5457  \n5458  \n5459  \n5460  \n5461  \n5462  \n5463  \n5464  \n5465  \n5466  \n5467  \n5468  \n5469  \n5470  \n5471 -\n5472  \n5473  \n5474  \n5475  \n5476  \n5477  \n5478  \n5479  \n5480  \n5481  ",
            "  @Override\n  public boolean grantPrivileges(PrivilegeBag privileges) throws InvalidObjectException,\n      MetaException, NoSuchObjectException {\n    boolean committed = false;\n    int now = (int) (System.currentTimeMillis() / 1000);\n    try {\n      openTransaction();\n      List<Object> persistentObjs = new ArrayList<>();\n\n      List<HiveObjectPrivilege> privilegeList = privileges.getPrivileges();\n\n      if (privilegeList != null && privilegeList.size() > 0) {\n        Iterator<HiveObjectPrivilege> privIter = privilegeList.iterator();\n        Set<String> privSet = new HashSet<>();\n        while (privIter.hasNext()) {\n          HiveObjectPrivilege privDef = privIter.next();\n          HiveObjectRef hiveObject = privDef.getHiveObject();\n          String privilegeStr = privDef.getGrantInfo().getPrivilege();\n          String[] privs = privilegeStr.split(\",\");\n          String userName = privDef.getPrincipalName();\n          PrincipalType principalType = privDef.getPrincipalType();\n          String grantor = privDef.getGrantInfo().getGrantor();\n          String grantorType = privDef.getGrantInfo().getGrantorType().toString();\n          boolean grantOption = privDef.getGrantInfo().isGrantOption();\n          privSet.clear();\n\n          if(principalType == PrincipalType.ROLE){\n            validateRole(userName);\n          }\n\n          if (hiveObject.getObjectType() == HiveObjectType.GLOBAL) {\n            List<MGlobalPrivilege> globalPrivs = this\n                .listPrincipalMGlobalGrants(userName, principalType);\n            if (globalPrivs != null) {\n              for (MGlobalPrivilege priv : globalPrivs) {\n                if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                  privSet.add(priv.getPrivilege());\n                }\n              }\n            }\n            for (String privilege : privs) {\n              if (privSet.contains(privilege)) {\n                throw new InvalidObjectException(privilege\n                    + \" is already granted by \" + grantor);\n              }\n              MGlobalPrivilege mGlobalPrivs = new MGlobalPrivilege(userName,\n                  principalType.toString(), privilege, now, grantor, grantorType, grantOption);\n              persistentObjs.add(mGlobalPrivs);\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.DATABASE) {\n            MDatabase dbObj = getMDatabase(hiveObject.getDbName());\n            if (dbObj != null) {\n              List<MDBPrivilege> dbPrivs = this.listPrincipalMDBGrants(\n                  userName, principalType, hiveObject.getDbName());\n              if (dbPrivs != null) {\n                for (MDBPrivilege priv : dbPrivs) {\n                  if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on database \"\n                      + hiveObject.getDbName() + \" by \" + grantor);\n                }\n                MDBPrivilege mDb = new MDBPrivilege(userName, principalType\n                    .toString(), dbObj, privilege, now, grantor, grantorType, grantOption);\n                persistentObjs.add(mDb);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.TABLE) {\n            MTable tblObj = getMTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            if (tblObj != null) {\n              List<MTablePrivilege> tablePrivs = this\n                  .listAllMTableGrants(userName, principalType,\n                      hiveObject.getDbName(), hiveObject.getObjectName());\n              if (tablePrivs != null) {\n                for (MTablePrivilege priv : tablePrivs) {\n                  if (priv.getGrantor() != null\n                      && priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on table [\"\n                      + hiveObject.getDbName() + \",\"\n                      + hiveObject.getObjectName() + \"] by \" + grantor);\n                }\n                MTablePrivilege mTab = new MTablePrivilege(\n                    userName, principalType.toString(), tblObj,\n                    privilege, now, grantor, grantorType, grantOption);\n                persistentObjs.add(mTab);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.PARTITION) {\n            MPartition partObj = this.getMPartition(hiveObject.getDbName(),\n                hiveObject.getObjectName(), hiveObject.getPartValues());\n            String partName = null;\n            if (partObj != null) {\n              partName = partObj.getPartitionName();\n              List<MPartitionPrivilege> partPrivs = this\n                  .listPrincipalMPartitionGrants(userName,\n                      principalType, hiveObject.getDbName(), hiveObject\n                          .getObjectName(), partObj.getPartitionName());\n              if (partPrivs != null) {\n                for (MPartitionPrivilege priv : partPrivs) {\n                  if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on partition [\"\n                      + hiveObject.getDbName() + \",\"\n                      + hiveObject.getObjectName() + \",\"\n                      + partName + \"] by \" + grantor);\n                }\n                MPartitionPrivilege mTab = new MPartitionPrivilege(userName,\n                    principalType.toString(), partObj, privilege, now, grantor,\n                    grantorType, grantOption);\n                persistentObjs.add(mTab);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.COLUMN) {\n            MTable tblObj = getMTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            if (tblObj != null) {\n              if (hiveObject.getPartValues() != null) {\n                MPartition partObj = null;\n                List<MPartitionColumnPrivilege> colPrivs = null;\n                partObj = this.getMPartition(hiveObject.getDbName(), hiveObject\n                    .getObjectName(), hiveObject.getPartValues());\n                if (partObj == null) {\n                  continue;\n                }\n                colPrivs = this.listPrincipalMPartitionColumnGrants(\n                    userName, principalType, hiveObject.getDbName(), hiveObject\n                        .getObjectName(), partObj.getPartitionName(),\n                    hiveObject.getColumnName());\n\n                if (colPrivs != null) {\n                  for (MPartitionColumnPrivilege priv : colPrivs) {\n                    if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                      privSet.add(priv.getPrivilege());\n                    }\n                  }\n                }\n                for (String privilege : privs) {\n                  if (privSet.contains(privilege)) {\n                    throw new InvalidObjectException(privilege\n                        + \" is already granted on column \"\n                        + hiveObject.getColumnName() + \" [\"\n                        + hiveObject.getDbName() + \",\"\n                        + hiveObject.getObjectName() + \",\"\n                        + partObj.getPartitionName() + \"] by \" + grantor);\n                  }\n                  MPartitionColumnPrivilege mCol = new MPartitionColumnPrivilege(userName,\n                      principalType.toString(), partObj, hiveObject\n                          .getColumnName(), privilege, now, grantor, grantorType,\n                      grantOption);\n                  persistentObjs.add(mCol);\n                }\n\n              } else {\n                List<MTableColumnPrivilege> colPrivs = null;\n                colPrivs = this.listPrincipalMTableColumnGrants(\n                    userName, principalType, hiveObject.getDbName(), hiveObject\n                        .getObjectName(), hiveObject.getColumnName());\n\n                if (colPrivs != null) {\n                  for (MTableColumnPrivilege priv : colPrivs) {\n                    if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                      privSet.add(priv.getPrivilege());\n                    }\n                  }\n                }\n                for (String privilege : privs) {\n                  if (privSet.contains(privilege)) {\n                    throw new InvalidObjectException(privilege\n                        + \" is already granted on column \"\n                        + hiveObject.getColumnName() + \" [\"\n                        + hiveObject.getDbName() + \",\"\n                        + hiveObject.getObjectName() + \"] by \" + grantor);\n                  }\n                  MTableColumnPrivilege mCol = new MTableColumnPrivilege(userName,\n                      principalType.toString(), tblObj, hiveObject\n                          .getColumnName(), privilege, now, grantor, grantorType,\n                      grantOption);\n                  persistentObjs.add(mCol);\n                }\n              }\n            }\n          }\n        }\n      }\n      if (persistentObjs.size() > 0) {\n        pm.makePersistentAll(persistentObjs);\n      }\n      committed = commitTransaction();\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n    return committed;\n  }",
            "5246  \n5247  \n5248  \n5249  \n5250  \n5251  \n5252  \n5253  \n5254  \n5255  \n5256  \n5257 +\n5258  \n5259  \n5260  \n5261  \n5262  \n5263  \n5264  \n5265  \n5266  \n5267  \n5268  \n5269  \n5270  \n5271  \n5272  \n5273  \n5274  \n5275  \n5276  \n5277  \n5278  \n5279  \n5280  \n5281  \n5282  \n5283  \n5284  \n5285  \n5286  \n5287  \n5288  \n5289  \n5290  \n5291  \n5292  \n5293  \n5294  \n5295  \n5296  \n5297  \n5298  \n5299  \n5300  \n5301  \n5302  \n5303  \n5304  \n5305  \n5306  \n5307  \n5308  \n5309  \n5310  \n5311  \n5312  \n5313  \n5314  \n5315  \n5316  \n5317  \n5318  \n5319  \n5320  \n5321  \n5322  \n5323  \n5324  \n5325  \n5326  \n5327  \n5328  \n5329  \n5330  \n5331  \n5332  \n5333  \n5334  \n5335  \n5336  \n5337  \n5338  \n5339  \n5340  \n5341  \n5342  \n5343  \n5344  \n5345  \n5346  \n5347  \n5348  \n5349  \n5350  \n5351  \n5352  \n5353  \n5354  \n5355  \n5356  \n5357  \n5358  \n5359  \n5360  \n5361  \n5362  \n5363  \n5364  \n5365  \n5366  \n5367  \n5368  \n5369  \n5370  \n5371  \n5372  \n5373  \n5374  \n5375  \n5376  \n5377  \n5378  \n5379  \n5380  \n5381  \n5382  \n5383  \n5384  \n5385  \n5386  \n5387  \n5388  \n5389  \n5390  \n5391  \n5392  \n5393  \n5394  \n5395  \n5396  \n5397  \n5398  \n5399  \n5400  \n5401  \n5402  \n5403  \n5404  \n5405  \n5406  \n5407  \n5408  \n5409  \n5410  \n5411  \n5412  \n5413  \n5414  \n5415  \n5416  \n5417  \n5418  \n5419  \n5420  \n5421  \n5422  \n5423  \n5424  \n5425  \n5426  \n5427  \n5428  \n5429  \n5430  \n5431  \n5432  \n5433  \n5434  \n5435  \n5436  \n5437  \n5438  \n5439  \n5440  \n5441  \n5442  \n5443  \n5444  \n5445  \n5446  \n5447  \n5448  \n5449 +\n5450  \n5451  \n5452  \n5453  \n5454  \n5455  \n5456  \n5457  \n5458  \n5459  ",
            "  @Override\n  public boolean grantPrivileges(PrivilegeBag privileges) throws InvalidObjectException,\n      MetaException, NoSuchObjectException {\n    boolean committed = false;\n    int now = (int) (System.currentTimeMillis() / 1000);\n    try {\n      openTransaction();\n      List<Object> persistentObjs = new ArrayList<>();\n\n      List<HiveObjectPrivilege> privilegeList = privileges.getPrivileges();\n\n      if (CollectionUtils.isNotEmpty(privilegeList)) {\n        Iterator<HiveObjectPrivilege> privIter = privilegeList.iterator();\n        Set<String> privSet = new HashSet<>();\n        while (privIter.hasNext()) {\n          HiveObjectPrivilege privDef = privIter.next();\n          HiveObjectRef hiveObject = privDef.getHiveObject();\n          String privilegeStr = privDef.getGrantInfo().getPrivilege();\n          String[] privs = privilegeStr.split(\",\");\n          String userName = privDef.getPrincipalName();\n          PrincipalType principalType = privDef.getPrincipalType();\n          String grantor = privDef.getGrantInfo().getGrantor();\n          String grantorType = privDef.getGrantInfo().getGrantorType().toString();\n          boolean grantOption = privDef.getGrantInfo().isGrantOption();\n          privSet.clear();\n\n          if(principalType == PrincipalType.ROLE){\n            validateRole(userName);\n          }\n\n          if (hiveObject.getObjectType() == HiveObjectType.GLOBAL) {\n            List<MGlobalPrivilege> globalPrivs = this\n                .listPrincipalMGlobalGrants(userName, principalType);\n            if (globalPrivs != null) {\n              for (MGlobalPrivilege priv : globalPrivs) {\n                if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                  privSet.add(priv.getPrivilege());\n                }\n              }\n            }\n            for (String privilege : privs) {\n              if (privSet.contains(privilege)) {\n                throw new InvalidObjectException(privilege\n                    + \" is already granted by \" + grantor);\n              }\n              MGlobalPrivilege mGlobalPrivs = new MGlobalPrivilege(userName,\n                  principalType.toString(), privilege, now, grantor, grantorType, grantOption);\n              persistentObjs.add(mGlobalPrivs);\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.DATABASE) {\n            MDatabase dbObj = getMDatabase(hiveObject.getDbName());\n            if (dbObj != null) {\n              List<MDBPrivilege> dbPrivs = this.listPrincipalMDBGrants(\n                  userName, principalType, hiveObject.getDbName());\n              if (dbPrivs != null) {\n                for (MDBPrivilege priv : dbPrivs) {\n                  if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on database \"\n                      + hiveObject.getDbName() + \" by \" + grantor);\n                }\n                MDBPrivilege mDb = new MDBPrivilege(userName, principalType\n                    .toString(), dbObj, privilege, now, grantor, grantorType, grantOption);\n                persistentObjs.add(mDb);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.TABLE) {\n            MTable tblObj = getMTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            if (tblObj != null) {\n              List<MTablePrivilege> tablePrivs = this\n                  .listAllMTableGrants(userName, principalType,\n                      hiveObject.getDbName(), hiveObject.getObjectName());\n              if (tablePrivs != null) {\n                for (MTablePrivilege priv : tablePrivs) {\n                  if (priv.getGrantor() != null\n                      && priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on table [\"\n                      + hiveObject.getDbName() + \",\"\n                      + hiveObject.getObjectName() + \"] by \" + grantor);\n                }\n                MTablePrivilege mTab = new MTablePrivilege(\n                    userName, principalType.toString(), tblObj,\n                    privilege, now, grantor, grantorType, grantOption);\n                persistentObjs.add(mTab);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.PARTITION) {\n            MPartition partObj = this.getMPartition(hiveObject.getDbName(),\n                hiveObject.getObjectName(), hiveObject.getPartValues());\n            String partName = null;\n            if (partObj != null) {\n              partName = partObj.getPartitionName();\n              List<MPartitionPrivilege> partPrivs = this\n                  .listPrincipalMPartitionGrants(userName,\n                      principalType, hiveObject.getDbName(), hiveObject\n                          .getObjectName(), partObj.getPartitionName());\n              if (partPrivs != null) {\n                for (MPartitionPrivilege priv : partPrivs) {\n                  if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                    privSet.add(priv.getPrivilege());\n                  }\n                }\n              }\n              for (String privilege : privs) {\n                if (privSet.contains(privilege)) {\n                  throw new InvalidObjectException(privilege\n                      + \" is already granted on partition [\"\n                      + hiveObject.getDbName() + \",\"\n                      + hiveObject.getObjectName() + \",\"\n                      + partName + \"] by \" + grantor);\n                }\n                MPartitionPrivilege mTab = new MPartitionPrivilege(userName,\n                    principalType.toString(), partObj, privilege, now, grantor,\n                    grantorType, grantOption);\n                persistentObjs.add(mTab);\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.COLUMN) {\n            MTable tblObj = getMTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            if (tblObj != null) {\n              if (hiveObject.getPartValues() != null) {\n                MPartition partObj = null;\n                List<MPartitionColumnPrivilege> colPrivs = null;\n                partObj = this.getMPartition(hiveObject.getDbName(), hiveObject\n                    .getObjectName(), hiveObject.getPartValues());\n                if (partObj == null) {\n                  continue;\n                }\n                colPrivs = this.listPrincipalMPartitionColumnGrants(\n                    userName, principalType, hiveObject.getDbName(), hiveObject\n                        .getObjectName(), partObj.getPartitionName(),\n                    hiveObject.getColumnName());\n\n                if (colPrivs != null) {\n                  for (MPartitionColumnPrivilege priv : colPrivs) {\n                    if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                      privSet.add(priv.getPrivilege());\n                    }\n                  }\n                }\n                for (String privilege : privs) {\n                  if (privSet.contains(privilege)) {\n                    throw new InvalidObjectException(privilege\n                        + \" is already granted on column \"\n                        + hiveObject.getColumnName() + \" [\"\n                        + hiveObject.getDbName() + \",\"\n                        + hiveObject.getObjectName() + \",\"\n                        + partObj.getPartitionName() + \"] by \" + grantor);\n                  }\n                  MPartitionColumnPrivilege mCol = new MPartitionColumnPrivilege(userName,\n                      principalType.toString(), partObj, hiveObject\n                          .getColumnName(), privilege, now, grantor, grantorType,\n                      grantOption);\n                  persistentObjs.add(mCol);\n                }\n\n              } else {\n                List<MTableColumnPrivilege> colPrivs = null;\n                colPrivs = this.listPrincipalMTableColumnGrants(\n                    userName, principalType, hiveObject.getDbName(), hiveObject\n                        .getObjectName(), hiveObject.getColumnName());\n\n                if (colPrivs != null) {\n                  for (MTableColumnPrivilege priv : colPrivs) {\n                    if (priv.getGrantor().equalsIgnoreCase(grantor)) {\n                      privSet.add(priv.getPrivilege());\n                    }\n                  }\n                }\n                for (String privilege : privs) {\n                  if (privSet.contains(privilege)) {\n                    throw new InvalidObjectException(privilege\n                        + \" is already granted on column \"\n                        + hiveObject.getColumnName() + \" [\"\n                        + hiveObject.getDbName() + \",\"\n                        + hiveObject.getObjectName() + \"] by \" + grantor);\n                  }\n                  MTableColumnPrivilege mCol = new MTableColumnPrivilege(userName,\n                      principalType.toString(), tblObj, hiveObject\n                          .getColumnName(), privilege, now, grantor, grantorType,\n                      grantOption);\n                  persistentObjs.add(mCol);\n                }\n              }\n            }\n          }\n        }\n      }\n      if (CollectionUtils.isNotEmpty(persistentObjs)) {\n        pm.makePersistentAll(persistentObjs);\n      }\n      committed = commitTransaction();\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n    return committed;\n  }"
        ],
        [
            "ObjectStore::getJDODatabase(String)",
            " 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879 -\n 880 -\n 881  \n 882  ",
            "  public Database getJDODatabase(String name) throws NoSuchObjectException {\n    MDatabase mdb = null;\n    boolean commited = false;\n    try {\n      openTransaction();\n      mdb = getMDatabase(name);\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    Database db = new Database();\n    db.setName(mdb.getName());\n    db.setDescription(mdb.getDescription());\n    db.setLocationUri(mdb.getLocationUri());\n    db.setParameters(convertMap(mdb.getParameters()));\n    db.setOwnerName(mdb.getOwnerName());\n    String type = mdb.getOwnerType();\n    db.setOwnerType((null == type || type.trim().isEmpty()) ? null : PrincipalType.valueOf(type));\n    return db;\n  }",
            " 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873 +\n 874 +\n 875 +\n 876  \n 877  ",
            "  public Database getJDODatabase(String name) throws NoSuchObjectException {\n    MDatabase mdb = null;\n    boolean commited = false;\n    try {\n      openTransaction();\n      mdb = getMDatabase(name);\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    Database db = new Database();\n    db.setName(mdb.getName());\n    db.setDescription(mdb.getDescription());\n    db.setLocationUri(mdb.getLocationUri());\n    db.setParameters(convertMap(mdb.getParameters()));\n    db.setOwnerName(mdb.getOwnerName());\n    String type = org.apache.commons.lang.StringUtils.defaultIfBlank(mdb.getOwnerType(), null);\n    PrincipalType principalType = (type == null) ? null : PrincipalType.valueOf(type);\n    db.setOwnerType(principalType);\n    return db;\n  }"
        ],
        [
            "ObjectStore::getPartitionPrivilegeSet(String,String,String,String,List)",
            "5040  \n5041  \n5042  \n5043  \n5044  \n5045  \n5046  \n5047  \n5048  \n5049  \n5050  \n5051  \n5052  \n5053  \n5054  \n5055  \n5056  \n5057 -\n5058  \n5059  \n5060  \n5061  \n5062  \n5063  \n5064  \n5065  \n5066 -\n5067  \n5068  \n5069  \n5070  \n5071  \n5072  \n5073  \n5074  \n5075  \n5076  \n5077  \n5078  \n5079  \n5080  \n5081  ",
            "  @Override\n  public PrincipalPrivilegeSet getPartitionPrivilegeSet(String dbName,\n      String tableName, String partition, String userName,\n      List<String> groupNames) throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> partUserPriv = new HashMap<>();\n        partUserPriv.put(userName, getPartitionPrivilege(dbName,\n            tableName, partition, userName, PrincipalType.USER));\n        ret.setUserPrivileges(partUserPriv);\n      }\n      if (groupNames != null && groupNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> partGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          partGroupPriv.put(groupName, getPartitionPrivilege(dbName, tableName,\n              partition, groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(partGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (roleNames != null && roleNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> partRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          partRolePriv.put(roleName, getPartitionPrivilege(dbName, tableName,\n              partition, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(partRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }",
            "5018  \n5019  \n5020  \n5021  \n5022  \n5023  \n5024  \n5025  \n5026  \n5027  \n5028  \n5029  \n5030  \n5031  \n5032  \n5033  \n5034  \n5035 +\n5036  \n5037  \n5038  \n5039  \n5040  \n5041  \n5042  \n5043  \n5044 +\n5045  \n5046  \n5047  \n5048  \n5049  \n5050  \n5051  \n5052  \n5053  \n5054  \n5055  \n5056  \n5057  \n5058  \n5059  ",
            "  @Override\n  public PrincipalPrivilegeSet getPartitionPrivilegeSet(String dbName,\n      String tableName, String partition, String userName,\n      List<String> groupNames) throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> partUserPriv = new HashMap<>();\n        partUserPriv.put(userName, getPartitionPrivilege(dbName,\n            tableName, partition, userName, PrincipalType.USER));\n        ret.setUserPrivileges(partUserPriv);\n      }\n      if (CollectionUtils.isNotEmpty(groupNames)) {\n        Map<String, List<PrivilegeGrantInfo>> partGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          partGroupPriv.put(groupName, getPartitionPrivilege(dbName, tableName,\n              partition, groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(partGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (CollectionUtils.isNotEmpty(roleNames)) {\n        Map<String, List<PrivilegeGrantInfo>> partRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          partRolePriv.put(roleName, getPartitionPrivilege(dbName, tableName,\n              partition, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(partRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::removeToken(String)",
            "8130  \n8131  \n8132  \n8133  \n8134  \n8135  \n8136  \n8137  \n8138  \n8139  \n8140  \n8141  \n8142  \n8143  \n8144  \n8145  \n8146  \n8147  \n8148 -\n8149  \n8150  ",
            "  @Override\n  public boolean removeToken(String tokenId) {\n\n    LOG.debug(\"Begin executing removeToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (null != token) {\n        pm.deletePersistent(token);\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing removeToken with status : \" + committed);\n    return committed && (token != null);\n  }",
            "8106  \n8107  \n8108  \n8109  \n8110  \n8111  \n8112  \n8113  \n8114  \n8115  \n8116  \n8117  \n8118  \n8119  \n8120  \n8121  \n8122  \n8123  \n8124 +\n8125  \n8126  ",
            "  @Override\n  public boolean removeToken(String tokenId) {\n\n    LOG.debug(\"Begin executing removeToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (null != token) {\n        pm.deletePersistent(token);\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing removeToken with status : {}\", committed);\n    return committed && (token != null);\n  }"
        ],
        [
            "ObjectStore::dropType(String)",
            "1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085 -\n1086  \n1087  \n1088  \n1089  \n1090  ",
            "  @Override\n  public boolean dropType(String typeName) {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MType.class, \"name == typeName\");\n      query.declareParameters(\"java.lang.String typeName\");\n      query.setUnique(true);\n      MType type = (MType) query.execute(typeName.trim());\n      pm.retrieve(type);\n      if (type != null) {\n        pm.deletePersistent(type);\n      }\n      success = commitTransaction();\n    } catch (JDOObjectNotFoundException e) {\n      success = commitTransaction();\n      LOG.debug(\"type not found \" + typeName, e);\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return success;\n  }",
            "1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077 +\n1078  \n1079  \n1080  \n1081  \n1082  ",
            "  @Override\n  public boolean dropType(String typeName) {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MType.class, \"name == typeName\");\n      query.declareParameters(\"java.lang.String typeName\");\n      query.setUnique(true);\n      MType type = (MType) query.execute(typeName.trim());\n      pm.retrieve(type);\n      if (type != null) {\n        pm.deletePersistent(type);\n      }\n      success = commitTransaction();\n    } catch (JDOObjectNotFoundException e) {\n      success = commitTransaction();\n      LOG.debug(\"type not found {}\", typeName, e);\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::RetryingExecutor::run()",
            "8760  \n8761  \n8762  \n8763  \n8764  \n8765  \n8766  \n8767 -\n8768 -\n8769  \n8770  \n8771  \n8772  \n8773  \n8774  \n8775  \n8776  \n8777  \n8778  \n8779  \n8780  \n8781  \n8782  \n8783  \n8784  \n8785  \n8786  \n8787  \n8788  \n8789  \n8790  ",
            "    public void run() throws MetaException {\n      while (true) {\n        try {\n          command.process();\n          break;\n        } catch (Exception e) {\n          LOG.info(\n              \"Attempting to acquire the DB log notification lock: \" + currentRetries + \" out of \"\n                  + maxRetries + \" retries\", e);\n          if (currentRetries >= maxRetries) {\n            String message =\n                \"Couldn't acquire the DB log notification lock because we reached the maximum\"\n                    + \" # of retries: \" + maxRetries\n                    + \" retries. If this happens too often, then is recommended to \"\n                    + \"increase the maximum number of retries on the\"\n                    + \" hive.notification.sequence.lock.max.retries configuration\";\n            LOG.error(message, e);\n            throw new MetaException(message + \" :: \" + e.getMessage());\n          }\n          currentRetries++;\n          try {\n            Thread.sleep(sleepInterval);\n          } catch (InterruptedException e1) {\n            String msg = \"Couldn't acquire the DB notification log lock on \" + currentRetries\n                + \" retry, because the following error: \";\n            LOG.error(msg, e1);\n            throw new MetaException(msg + e1.getMessage());\n          }\n        }\n      }\n    }",
            "8735  \n8736  \n8737  \n8738  \n8739  \n8740  \n8741  \n8742 +\n8743 +\n8744  \n8745  \n8746  \n8747  \n8748  \n8749  \n8750  \n8751  \n8752  \n8753  \n8754  \n8755  \n8756  \n8757  \n8758  \n8759  \n8760  \n8761  \n8762  \n8763  \n8764  \n8765  ",
            "    public void run() throws MetaException {\n      while (true) {\n        try {\n          command.process();\n          break;\n        } catch (Exception e) {\n          LOG.info(\n              \"Attempting to acquire the DB log notification lock: {} out of {}\" +\n                \" retries\", currentRetries, maxRetries, e);\n          if (currentRetries >= maxRetries) {\n            String message =\n                \"Couldn't acquire the DB log notification lock because we reached the maximum\"\n                    + \" # of retries: \" + maxRetries\n                    + \" retries. If this happens too often, then is recommended to \"\n                    + \"increase the maximum number of retries on the\"\n                    + \" hive.notification.sequence.lock.max.retries configuration\";\n            LOG.error(message, e);\n            throw new MetaException(message + \" :: \" + e.getMessage());\n          }\n          currentRetries++;\n          try {\n            Thread.sleep(sleepInterval);\n          } catch (InterruptedException e1) {\n            String msg = \"Couldn't acquire the DB notification log lock on \" + currentRetries\n                + \" retry, because the following error: \";\n            LOG.error(msg, e1);\n            throw new MetaException(msg + e1.getMessage());\n          }\n        }\n      }\n    }"
        ],
        [
            "ObjectStore::correctAutoStartMechanism(Configuration)",
            " 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610 -\n 611 -\n 612 -\n 613  \n 614  \n 615  ",
            "  /**\n   * Update conf to set datanucleus.autoStartMechanismMode=ignored.\n   * This is necessary to able to use older version of hive against\n   * an upgraded but compatible metastore schema in db from new version\n   * of hive\n   * @param conf\n   */\n  private static void correctAutoStartMechanism(Configuration conf) {\n    final String autoStartKey = \"datanucleus.autoStartMechanismMode\";\n    final String autoStartIgnore = \"ignored\";\n    String currentAutoStartVal = conf.get(autoStartKey);\n    if(currentAutoStartVal != null && !currentAutoStartVal.equalsIgnoreCase(autoStartIgnore)) {\n      LOG.warn(autoStartKey + \" is set to unsupported value \" + conf.get(autoStartKey) +\n          \" . Setting it to value \" + autoStartIgnore);\n    }\n    conf.set(autoStartKey, autoStartIgnore);\n  }",
            " 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606 +\n 607 +\n 608 +\n 609  \n 610  \n 611  ",
            "  /**\n   * Update conf to set datanucleus.autoStartMechanismMode=ignored.\n   * This is necessary to able to use older version of hive against\n   * an upgraded but compatible metastore schema in db from new version\n   * of hive\n   * @param conf\n   */\n  private static void correctAutoStartMechanism(Configuration conf) {\n    final String autoStartKey = \"datanucleus.autoStartMechanismMode\";\n    final String autoStartIgnore = \"ignored\";\n    String currentAutoStartVal = conf.get(autoStartKey);\n    if (!autoStartIgnore.equalsIgnoreCase(currentAutoStartVal)) {\n      LOG.warn(\"{} is set to unsupported value {} . Setting it to value: {}\", autoStartKey,\n        conf.get(autoStartKey), autoStartIgnore);\n    }\n    conf.set(autoStartKey, autoStartIgnore);\n  }"
        ],
        [
            "ObjectStore::createExpressionProxy(Configuration)",
            " 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509 -\n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  ",
            "  /**\n   * Creates the proxy used to evaluate expressions. This is here to prevent circular\n   * dependency - ql -&gt; metastore client &lt;-&gt metastore server -&gt ql. If server and\n   * client are split, this can be removed.\n   * @param conf Configuration.\n   * @return The partition expression proxy.\n   */\n  private static PartitionExpressionProxy createExpressionProxy(Configuration conf) {\n    String className = MetastoreConf.getVar(conf, ConfVars.EXPRESSION_PROXY_CLASS);\n    try {\n      @SuppressWarnings(\"unchecked\")\n      Class<? extends PartitionExpressionProxy> clazz =\n           JavaUtils.getClass(className, PartitionExpressionProxy.class);\n      return JavaUtils.newInstance(clazz, new Class<?>[0], new Object[0]);\n    } catch (MetaException e) {\n      LOG.error(\"Error loading PartitionExpressionProxy\", e);\n      throw new RuntimeException(\"Error loading PartitionExpressionProxy: \" + e.getMessage());\n    }\n  }",
            " 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  ",
            "  /**\n   * Creates the proxy used to evaluate expressions. This is here to prevent circular\n   * dependency - ql -&gt; metastore client &lt;-&gt metastore server -&gt ql. If server and\n   * client are split, this can be removed.\n   * @param conf Configuration.\n   * @return The partition expression proxy.\n   */\n  private static PartitionExpressionProxy createExpressionProxy(Configuration conf) {\n    String className = MetastoreConf.getVar(conf, ConfVars.EXPRESSION_PROXY_CLASS);\n    try {\n      Class<? extends PartitionExpressionProxy> clazz =\n           JavaUtils.getClass(className, PartitionExpressionProxy.class);\n      return JavaUtils.newInstance(clazz, new Class<?>[0], new Object[0]);\n    } catch (MetaException e) {\n      LOG.error(\"Error loading PartitionExpressionProxy\", e);\n      throw new RuntimeException(\"Error loading PartitionExpressionProxy: \" + e.getMessage());\n    }\n  }"
        ],
        [
            "ObjectStore::getTablePrivilege(String,String,String,PrincipalType)",
            "5202  \n5203  \n5204  \n5205  \n5206  \n5207  \n5208  \n5209  \n5210  \n5211 -\n5212  \n5213  \n5214  \n5215  \n5216  \n5217  \n5218  \n5219  \n5220  \n5221  \n5222  \n5223  \n5224  ",
            "  private List<PrivilegeGrantInfo> getTablePrivilege(String dbName,\n      String tableName, String principalName, PrincipalType principalType) {\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MTablePrivilege> userNameTabPartPriv = this\n          .listAllMTableGrants(principalName, principalType,\n              dbName, tableName);\n      if (userNameTabPartPriv != null && userNameTabPartPriv.size() > 0) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameTabPartPriv.size());\n        for (int i = 0; i < userNameTabPartPriv.size(); i++) {\n          MTablePrivilege item = userNameTabPartPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }",
            "5180  \n5181  \n5182  \n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189 +\n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  \n5197  \n5198  \n5199  \n5200  \n5201  \n5202  ",
            "  private List<PrivilegeGrantInfo> getTablePrivilege(String dbName,\n      String tableName, String principalName, PrincipalType principalType) {\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MTablePrivilege> userNameTabPartPriv = this\n          .listAllMTableGrants(principalName, principalType,\n              dbName, tableName);\n      if (CollectionUtils.isNotEmpty(userNameTabPartPriv)) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameTabPartPriv.size());\n        for (int i = 0; i < userNameTabPartPriv.size(); i++) {\n          MTablePrivilege item = userNameTabPartPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }"
        ],
        [
            "ObjectStore::getUserPrivilegeSet(String,List)",
            "4925  \n4926  \n4927  \n4928  \n4929  \n4930  \n4931  \n4932  \n4933  \n4934 -\n4935  \n4936  \n4937  \n4938  \n4939  \n4940  \n4941  \n4942  \n4943  \n4944  \n4945  \n4946  \n4947 -\n4948  \n4949  \n4950  \n4951  \n4952 -\n4953  \n4954  \n4955  \n4956  \n4957  \n4958  \n4959  \n4960  \n4961  \n4962  \n4963  \n4964  \n4965  \n4966  \n4967  \n4968  \n4969  \n4970  \n4971  \n4972  ",
            "  @Override\n  public PrincipalPrivilegeSet getUserPrivilegeSet(String userName,\n      List<String> groupNames) throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        List<MGlobalPrivilege> user = this.listPrincipalMGlobalGrants(userName, PrincipalType.USER);\n        if(user.size()>0) {\n          Map<String, List<PrivilegeGrantInfo>> userPriv = new HashMap<>();\n          List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(user.size());\n          for (int i = 0; i < user.size(); i++) {\n            MGlobalPrivilege item = user.get(i);\n            grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n                .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n                .getGrantorType()), item.getGrantOption()));\n          }\n          userPriv.put(userName, grantInfos);\n          ret.setUserPrivileges(userPriv);\n        }\n      }\n      if (groupNames != null && groupNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> groupPriv = new HashMap<>();\n        for(String groupName: groupNames) {\n          List<MGlobalPrivilege> group =\n              this.listPrincipalMGlobalGrants(groupName, PrincipalType.GROUP);\n          if(group.size()>0) {\n            List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(group.size());\n            for (int i = 0; i < group.size(); i++) {\n              MGlobalPrivilege item = group.get(i);\n              grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n                  .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n                  .getGrantorType()), item.getGrantOption()));\n            }\n            groupPriv.put(groupName, grantInfos);\n          }\n        }\n        ret.setGroupPrivileges(groupPriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }",
            "4903  \n4904  \n4905  \n4906  \n4907  \n4908  \n4909  \n4910  \n4911  \n4912 +\n4913  \n4914  \n4915  \n4916  \n4917  \n4918  \n4919  \n4920  \n4921  \n4922  \n4923  \n4924  \n4925 +\n4926  \n4927  \n4928  \n4929  \n4930 +\n4931  \n4932  \n4933  \n4934  \n4935  \n4936  \n4937  \n4938  \n4939  \n4940  \n4941  \n4942  \n4943  \n4944  \n4945  \n4946  \n4947  \n4948  \n4949  \n4950  ",
            "  @Override\n  public PrincipalPrivilegeSet getUserPrivilegeSet(String userName,\n      List<String> groupNames) throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        List<MGlobalPrivilege> user = this.listPrincipalMGlobalGrants(userName, PrincipalType.USER);\n        if(CollectionUtils.isNotEmpty(user)) {\n          Map<String, List<PrivilegeGrantInfo>> userPriv = new HashMap<>();\n          List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(user.size());\n          for (int i = 0; i < user.size(); i++) {\n            MGlobalPrivilege item = user.get(i);\n            grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n                .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n                .getGrantorType()), item.getGrantOption()));\n          }\n          userPriv.put(userName, grantInfos);\n          ret.setUserPrivileges(userPriv);\n        }\n      }\n      if (CollectionUtils.isNotEmpty(groupNames)) {\n        Map<String, List<PrivilegeGrantInfo>> groupPriv = new HashMap<>();\n        for(String groupName: groupNames) {\n          List<MGlobalPrivilege> group =\n              this.listPrincipalMGlobalGrants(groupName, PrincipalType.GROUP);\n          if(CollectionUtils.isNotEmpty(group)) {\n            List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(group.size());\n            for (int i = 0; i < group.size(); i++) {\n              MGlobalPrivilege item = group.get(i);\n              grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n                  .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n                  .getGrantorType()), item.getGrantOption()));\n            }\n            groupPriv.put(groupName, grantInfos);\n          }\n        }\n        ret.setGroupPrivileges(groupPriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::debugLog(String)",
            "8449  \n8450  \n8451 -\n8452  \n8453  ",
            "  private void debugLog(String message) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(message + getCallStack());\n    }\n  }",
            "8424  \n8425  \n8426 +\n8427  \n8428  ",
            "  private void debugLog(String message) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"{} {}\", message, getCallStack());\n    }\n  }"
        ],
        [
            "ObjectStore::addPartition(Partition)",
            "1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983 -\n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  ",
            "  @Override\n  public boolean addPartition(Partition part) throws InvalidObjectException,\n      MetaException {\n    boolean success = false;\n    boolean commited = false;\n    try {\n      MTable table = this.getMTable(part.getDbName(), part.getTableName());\n      List<MTablePrivilege> tabGrants = null;\n      List<MTableColumnPrivilege> tabColumnGrants = null;\n      if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n        tabGrants = this.listAllTableGrants(part\n            .getDbName(), part.getTableName());\n        tabColumnGrants = this.listTableAllColumnGrants(\n            part.getDbName(), part.getTableName());\n      }\n      openTransaction();\n      MPartition mpart = convertToMPart(part, true);\n      pm.makePersistent(mpart);\n\n      int now = (int)(System.currentTimeMillis()/1000);\n      List<Object> toPersist = new ArrayList<>();\n      if (tabGrants != null) {\n        for (MTablePrivilege tab: tabGrants) {\n          MPartitionPrivilege partGrant = new MPartitionPrivilege(tab\n              .getPrincipalName(), tab.getPrincipalType(),\n              mpart, tab.getPrivilege(), now, tab.getGrantor(), tab\n                  .getGrantorType(), tab.getGrantOption());\n          toPersist.add(partGrant);\n        }\n      }\n\n      if (tabColumnGrants != null) {\n        for (MTableColumnPrivilege col : tabColumnGrants) {\n          MPartitionColumnPrivilege partColumn = new MPartitionColumnPrivilege(col\n              .getPrincipalName(), col.getPrincipalType(), mpart, col\n              .getColumnName(), col.getPrivilege(), now, col.getGrantor(), col\n              .getGrantorType(), col.getGrantOption());\n          toPersist.add(partColumn);\n        }\n\n        if (toPersist.size() > 0) {\n          pm.makePersistentAll(toPersist);\n        }\n      }\n\n      commited = commitTransaction();\n      success = true;\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }",
            "1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972 +\n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  ",
            "  @Override\n  public boolean addPartition(Partition part) throws InvalidObjectException,\n      MetaException {\n    boolean success = false;\n    boolean commited = false;\n    try {\n      MTable table = this.getMTable(part.getDbName(), part.getTableName());\n      List<MTablePrivilege> tabGrants = null;\n      List<MTableColumnPrivilege> tabColumnGrants = null;\n      if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n        tabGrants = this.listAllTableGrants(part\n            .getDbName(), part.getTableName());\n        tabColumnGrants = this.listTableAllColumnGrants(\n            part.getDbName(), part.getTableName());\n      }\n      openTransaction();\n      MPartition mpart = convertToMPart(part, true);\n      pm.makePersistent(mpart);\n\n      int now = (int)(System.currentTimeMillis()/1000);\n      List<Object> toPersist = new ArrayList<>();\n      if (tabGrants != null) {\n        for (MTablePrivilege tab: tabGrants) {\n          MPartitionPrivilege partGrant = new MPartitionPrivilege(tab\n              .getPrincipalName(), tab.getPrincipalType(),\n              mpart, tab.getPrivilege(), now, tab.getGrantor(), tab\n                  .getGrantorType(), tab.getGrantOption());\n          toPersist.add(partGrant);\n        }\n      }\n\n      if (tabColumnGrants != null) {\n        for (MTableColumnPrivilege col : tabColumnGrants) {\n          MPartitionColumnPrivilege partColumn = new MPartitionColumnPrivilege(col\n              .getPrincipalName(), col.getPrincipalType(), mpart, col\n              .getColumnName(), col.getPrivilege(), now, col.getGrantor(), col\n              .getGrantorType(), col.getGrantOption());\n          toPersist.add(partColumn);\n        }\n\n        if (CollectionUtils.isNotEmpty(toPersist)) {\n          pm.makePersistentAll(toPersist);\n        }\n      }\n\n      commited = commitTransaction();\n      success = true;\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::getColumnFromTableColumns(List,String)",
            "3800  \n3801  \n3802  \n3803  \n3804 -\n3805 -\n3806  \n3807  \n3808  \n3809  \n3810  \n3811  ",
            "  private static MFieldSchema getColumnFromTableColumns(List<MFieldSchema> cols, String col) {\n    if (cols == null) {\n      return null;\n    }\n    for (int i = 0; i < cols.size(); i++) {\n      MFieldSchema mfs = cols.get(i);\n      if (mfs.getName().equalsIgnoreCase(col)) {\n        return mfs;\n      }\n    }\n    return null;\n  }",
            "3779  \n3780  \n3781  \n3782  \n3783 +\n3784  \n3785  \n3786  \n3787  \n3788  \n3789  ",
            "  private static MFieldSchema getColumnFromTableColumns(List<MFieldSchema> cols, String col) {\n    if (cols == null) {\n      return null;\n    }\n    for (MFieldSchema mfs : cols) {\n      if (mfs.getName().equalsIgnoreCase(col)) {\n        return mfs;\n      }\n    }\n    return null;\n  }"
        ],
        [
            "ObjectStore::getDBPrivilegeSet(String,String,List)",
            "4998  \n4999  \n5000  \n5001  \n5002  \n5003  \n5004  \n5005  \n5006  \n5007  \n5008  \n5009  \n5010  \n5011  \n5012  \n5013  \n5014 -\n5015  \n5016  \n5017  \n5018  \n5019  \n5020  \n5021  \n5022  \n5023 -\n5024  \n5025  \n5026  \n5027  \n5028  \n5029  \n5030  \n5031  \n5032  \n5033  \n5034  \n5035  \n5036  \n5037  \n5038  ",
            "  @Override\n  public PrincipalPrivilegeSet getDBPrivilegeSet(String dbName,\n      String userName, List<String> groupNames) throws InvalidObjectException,\n      MetaException {\n    boolean commited = false;\n    dbName = normalizeIdentifier(dbName);\n\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> dbUserPriv = new HashMap<>();\n        dbUserPriv.put(userName, getDBPrivilege(dbName, userName,\n            PrincipalType.USER));\n        ret.setUserPrivileges(dbUserPriv);\n      }\n      if (groupNames != null && groupNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> dbGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          dbGroupPriv.put(groupName, getDBPrivilege(dbName, groupName,\n              PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(dbGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (roleNames != null && roleNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> dbRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          dbRolePriv\n              .put(roleName, getDBPrivilege(dbName, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(dbRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }",
            "4976  \n4977  \n4978  \n4979  \n4980  \n4981  \n4982  \n4983  \n4984  \n4985  \n4986  \n4987  \n4988  \n4989  \n4990  \n4991  \n4992 +\n4993  \n4994  \n4995  \n4996  \n4997  \n4998  \n4999  \n5000  \n5001 +\n5002  \n5003  \n5004  \n5005  \n5006  \n5007  \n5008  \n5009  \n5010  \n5011  \n5012  \n5013  \n5014  \n5015  \n5016  ",
            "  @Override\n  public PrincipalPrivilegeSet getDBPrivilegeSet(String dbName,\n      String userName, List<String> groupNames) throws InvalidObjectException,\n      MetaException {\n    boolean commited = false;\n    dbName = normalizeIdentifier(dbName);\n\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> dbUserPriv = new HashMap<>();\n        dbUserPriv.put(userName, getDBPrivilege(dbName, userName,\n            PrincipalType.USER));\n        ret.setUserPrivileges(dbUserPriv);\n      }\n      if (CollectionUtils.isNotEmpty(groupNames)) {\n        Map<String, List<PrivilegeGrantInfo>> dbGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          dbGroupPriv.put(groupName, getDBPrivilege(dbName, groupName,\n              PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(dbGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (CollectionUtils.isNotEmpty(roleNames)) {\n        Map<String, List<PrivilegeGrantInfo>> dbRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          dbRolePriv\n              .put(roleName, getDBPrivilege(dbName, roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(dbRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::listMPartitions(String,String,int,QueryWrapper)",
            "2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776 -\n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  ",
            "  private List<MPartition> listMPartitions(String dbName, String tableName, int max, QueryWrapper queryWrapper) {\n    boolean success = false;\n    List<MPartition> mparts = null;\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listMPartitions\");\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      Query query = queryWrapper.query = pm.newQuery(MPartition.class, \"table.tableName == t1 && table.database.name == t2\");\n      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n      query.setOrdering(\"partitionName ascending\");\n      if (max > 0) {\n        query.setRange(0, max);\n      }\n      mparts = (List<MPartition>) query.execute(tableName, dbName);\n      LOG.debug(\"Done executing query for listMPartitions\");\n      pm.retrieveAll(mparts);\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listMPartitions \" + mparts);\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return mparts;\n  }",
            "2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757 +\n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  ",
            "  private List<MPartition> listMPartitions(String dbName, String tableName, int max, QueryWrapper queryWrapper) {\n    boolean success = false;\n    List<MPartition> mparts = null;\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listMPartitions\");\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      Query query = queryWrapper.query = pm.newQuery(MPartition.class, \"table.tableName == t1 && table.database.name == t2\");\n      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n      query.setOrdering(\"partitionName ascending\");\n      if (max > 0) {\n        query.setRange(0, max);\n      }\n      mparts = (List<MPartition>) query.execute(tableName, dbName);\n      LOG.debug(\"Done executing query for listMPartitions\");\n      pm.retrieveAll(mparts);\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listMPartitions {}\", mparts);\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return mparts;\n  }"
        ],
        [
            "ObjectStore::GetDbHelper::describeResult()",
            "3251  \n3252  \n3253 -\n3254  ",
            "    @Override\n    protected String describeResult() {\n      return \"db details for db \" + dbName;\n    }",
            "3235  \n3236  \n3237 +\n3238  ",
            "    @Override\n    protected String describeResult() {\n      return \"db details for db \".concat(dbName);\n    }"
        ],
        [
            "ObjectStore::initializeHelper(Properties)",
            " 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 -\n 469 -\n 470 -\n 471  \n 472  \n 473  \n 474 -\n 475 -\n 476  ",
            "  /**\n   * private helper to do initialization routine, so we can retry if needed if it fails.\n   * @param dsProps\n   */\n  private void initializeHelper(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    try {\n      String productName = MetaStoreDirectSql.getProductName(pm);\n      sqlGenerator = new SQLGenerator(DatabaseProduct.determineDatabaseProduct(productName), conf);\n    } catch (SQLException e) {\n      LOG.error(\"error trying to figure out the database product\", e);\n      throw new RuntimeException(e);\n    }\n    isInitialized = pm != null;\n    if (isInitialized) {\n      dbType = determineDatabaseProduct();\n      expressionProxy = createExpressionProxy(conf);\n      if (MetastoreConf.getBoolVar(getConf(), ConfVars.TRY_DIRECT_SQL)) {\n        String schema = prop.getProperty(\"javax.jdo.mapping.Schema\");\n        if (schema != null && schema.isEmpty()) {\n          schema = null;\n        }\n        directSql = new MetaStoreDirectSql(pm, conf, schema);\n      }\n    }\n    LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n        \" created in the thread with id: \" + Thread.currentThread().getId());\n  }",
            " 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466 +\n 467  \n 468  \n 469  \n 470 +\n 471 +\n 472  ",
            "  /**\n   * private helper to do initialization routine, so we can retry if needed if it fails.\n   * @param dsProps\n   */\n  private void initializeHelper(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    try {\n      String productName = MetaStoreDirectSql.getProductName(pm);\n      sqlGenerator = new SQLGenerator(DatabaseProduct.determineDatabaseProduct(productName), conf);\n    } catch (SQLException e) {\n      LOG.error(\"error trying to figure out the database product\", e);\n      throw new RuntimeException(e);\n    }\n    isInitialized = pm != null;\n    if (isInitialized) {\n      dbType = determineDatabaseProduct();\n      expressionProxy = createExpressionProxy(conf);\n      if (MetastoreConf.getBoolVar(getConf(), ConfVars.TRY_DIRECT_SQL)) {\n        String schema = prop.getProperty(\"javax.jdo.mapping.Schema\");\n        schema = org.apache.commons.lang.StringUtils.defaultIfBlank(schema, null);\n        directSql = new MetaStoreDirectSql(pm, conf, schema);\n      }\n    }\n    LOG.debug(\"RawStore: {}, with PersistenceManager: {}\" +\n        \" created in the thread with id: {}\", this, pm, Thread.currentThread().getId());\n  }"
        ],
        [
            "ObjectStore::getDatabase(String)",
            " 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841 -\n 842  \n 843  \n 844  \n 845  ",
            "  @Override\n  public Database getDatabase(String name) throws NoSuchObjectException {\n    MetaException ex = null;\n    Database db = null;\n    try {\n      db = getDatabaseInternal(name);\n    } catch (MetaException e) {\n      // Signature restriction to NSOE, and NSOE being a flat exception prevents us from\n      // setting the cause of the NSOE as the MetaException. We should not lose the info\n      // we got here, but it's very likely that the MetaException is irrelevant and is\n      // actually an NSOE message, so we should log it and throw an NSOE with the msg.\n      ex = e;\n    }\n    if (db == null) {\n      LOG.warn(\"Failed to get database \" + name +\", returning NoSuchObjectException\", ex);\n      throw new NoSuchObjectException(name + (ex == null ? \"\" : (\": \" + ex.getMessage())));\n    }\n    return db;\n  }",
            " 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835 +\n 836  \n 837  \n 838  \n 839  ",
            "  @Override\n  public Database getDatabase(String name) throws NoSuchObjectException {\n    MetaException ex = null;\n    Database db = null;\n    try {\n      db = getDatabaseInternal(name);\n    } catch (MetaException e) {\n      // Signature restriction to NSOE, and NSOE being a flat exception prevents us from\n      // setting the cause of the NSOE as the MetaException. We should not lose the info\n      // we got here, but it's very likely that the MetaException is irrelevant and is\n      // actually an NSOE message, so we should log it and throw an NSOE with the msg.\n      ex = e;\n    }\n    if (db == null) {\n      LOG.warn(\"Failed to get database {}, returning NoSuchObjectException\", name, ex);\n      throw new NoSuchObjectException(name + (ex == null ? \"\" : (\": \" + ex.getMessage())));\n    }\n    return db;\n  }"
        ],
        [
            "ObjectStore::listPartitionNamesByFilter(String,String,String,short)",
            "3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  \n3526  \n3527  \n3528  \n3529  \n3530  \n3531  \n3532  \n3533  \n3534  \n3535  \n3536  \n3537  \n3538 -\n3539 -\n3540  \n3541  \n3542  \n3543  \n3544 -\n3545 -\n3546 -\n3547 -\n3548 -\n3549  \n3550  \n3551  \n3552  \n3553  \n3554  \n3555  \n3556  ",
            "  @Override\n  public List<String> listPartitionNamesByFilter(String dbName, String tableName, String filter,\n      short maxParts) throws MetaException {\n    boolean success = false;\n    Query query = null;\n    List<String> partNames = new ArrayList<>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listMPartitionNamesByFilter\");\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      MTable mtable = getMTable(dbName, tableName);\n      if (mtable == null) {\n        // To be consistent with the behavior of listPartitionNames, if the\n        // table or db does not exist, we return an empty list\n        return partNames;\n      }\n      Map<String, Object> params = new HashMap<>();\n      String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);\n      query =\n          pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n              + \"where \" + queryFilterString);\n      if (maxParts >= 0) {\n        // User specified a row limit, set it on the Query\n        query.setRange(0, maxParts);\n      }\n      LOG.debug(\"Filter specified is \" + filter + \",\" + \" JDOQL filter is \" + queryFilterString);\n      LOG.debug(\"Parms is \" + params);\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      query.setOrdering(\"partitionName ascending\");\n      query.setResult(\"partitionName\");\n      Collection names = (Collection) query.executeWithMap(params);\n      partNames = new ArrayList<>();\n      for (Iterator i = names.iterator(); i.hasNext();) {\n        partNames.add((String) i.next());\n      }\n      LOG.debug(\"Done executing query for listMPartitionNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listMPartitionNamesByFilter\");\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return partNames;\n  }",
            "3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520 +\n3521 +\n3522  \n3523  \n3524  \n3525  \n3526 +\n3527 +\n3528  \n3529  \n3530  \n3531  \n3532  \n3533  \n3534  \n3535  ",
            "  @Override\n  public List<String> listPartitionNamesByFilter(String dbName, String tableName, String filter,\n      short maxParts) throws MetaException {\n    boolean success = false;\n    Query query = null;\n    List<String> partNames = new ArrayList<>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing listMPartitionNamesByFilter\");\n      dbName = normalizeIdentifier(dbName);\n      tableName = normalizeIdentifier(tableName);\n      MTable mtable = getMTable(dbName, tableName);\n      if (mtable == null) {\n        // To be consistent with the behavior of listPartitionNames, if the\n        // table or db does not exist, we return an empty list\n        return partNames;\n      }\n      Map<String, Object> params = new HashMap<>();\n      String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);\n      query =\n          pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n              + \"where \" + queryFilterString);\n      if (maxParts >= 0) {\n        // User specified a row limit, set it on the Query\n        query.setRange(0, maxParts);\n      }\n      LOG.debug(\"Filter specified is {}, JDOQL filter is {}\", filter, queryFilterString);\n      LOG.debug(\"Parms is {}\", params);\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      query.setOrdering(\"partitionName ascending\");\n      query.setResult(\"partitionName\");\n      Collection<String> names = (Collection<String>) query.executeWithMap(params);\n      partNames = new ArrayList<>(names);\n      LOG.debug(\"Done executing query for listMPartitionNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for listMPartitionNamesByFilter\");\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    return partNames;\n  }"
        ],
        [
            "ObjectStore::getPartQueryWithParams(String,String,List)",
            "3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022 -\n3023  \n3024  \n3025  \n3026  \n3027  ",
            "  private ObjectPair<Query, Map<String, String>> getPartQueryWithParams(String dbName,\n      String tblName, List<String> partNames) {\n    StringBuilder sb = new StringBuilder(\"table.tableName == t1 && table.database.name == t2 && (\");\n    int n = 0;\n    Map<String, String> params = new HashMap<>();\n    for (Iterator<String> itr = partNames.iterator(); itr.hasNext();) {\n      String pn = \"p\" + n;\n      n++;\n      String part = itr.next();\n      params.put(pn, part);\n      sb.append(\"partitionName == \").append(pn);\n      sb.append(\" || \");\n    }\n    sb.setLength(sb.length() - 4); // remove the last \" || \"\n    sb.append(')');\n    Query query = pm.newQuery();\n    query.setFilter(sb.toString());\n    LOG.debug(\" JDOQL filter is \" + sb.toString());\n    params.put(\"t1\", normalizeIdentifier(tblName));\n    params.put(\"t2\", normalizeIdentifier(dbName));\n    query.declareParameters(makeParameterDeclarationString(params));\n    return new ObjectPair<>(query, params);\n  }",
            "2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003 +\n3004  \n3005  \n3006  \n3007  \n3008  ",
            "  private ObjectPair<Query, Map<String, String>> getPartQueryWithParams(String dbName,\n      String tblName, List<String> partNames) {\n    StringBuilder sb = new StringBuilder(\"table.tableName == t1 && table.database.name == t2 && (\");\n    int n = 0;\n    Map<String, String> params = new HashMap<>();\n    for (Iterator<String> itr = partNames.iterator(); itr.hasNext();) {\n      String pn = \"p\" + n;\n      n++;\n      String part = itr.next();\n      params.put(pn, part);\n      sb.append(\"partitionName == \").append(pn);\n      sb.append(\" || \");\n    }\n    sb.setLength(sb.length() - 4); // remove the last \" || \"\n    sb.append(')');\n    Query query = pm.newQuery();\n    query.setFilter(sb.toString());\n    LOG.debug(\" JDOQL filter is {}\", sb);\n    params.put(\"t1\", normalizeIdentifier(tblName));\n    params.put(\"t2\", normalizeIdentifier(dbName));\n    query.declareParameters(makeParameterDeclarationString(params));\n    return new ObjectPair<>(query, params);\n  }"
        ],
        [
            "ObjectStore::getPartitionsWithAuth(String,String,short,String,List)",
            "2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271 -\n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  ",
            "  @Override\n  public List<Partition> getPartitionsWithAuth(String dbName, String tblName,\n      short max, String userName, List<String> groupNames)\n          throws MetaException, InvalidObjectException {\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      openTransaction();\n      List<MPartition> mparts = listMPartitions(dbName, tblName, max, queryWrapper);\n      List<Partition> parts = new ArrayList<>(mparts.size());\n      if (mparts != null && mparts.size()>0) {\n        for (MPartition mpart : mparts) {\n          MTable mtbl = mpart.getTable();\n          Partition part = convertToPart(mpart);\n          parts.add(part);\n\n          if (\"TRUE\".equalsIgnoreCase(mtbl.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n            String partName = Warehouse.makePartName(this.convertToFieldSchemas(mtbl\n                .getPartitionKeys()), part.getValues());\n            PrincipalPrivilegeSet partAuth = this.getPartitionPrivilegeSet(dbName,\n                tblName, partName, userName, groupNames);\n            part.setPrivileges(partAuth);\n          }\n        }\n      }\n      success =  commitTransaction();\n      return parts;\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n  }",
            "2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262 +\n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  ",
            "  @Override\n  public List<Partition> getPartitionsWithAuth(String dbName, String tblName,\n      short max, String userName, List<String> groupNames)\n          throws MetaException, InvalidObjectException {\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      openTransaction();\n      List<MPartition> mparts = listMPartitions(dbName, tblName, max, queryWrapper);\n      List<Partition> parts = new ArrayList<>(mparts.size());\n      if (CollectionUtils.isNotEmpty(mparts)) {\n        for (MPartition mpart : mparts) {\n          MTable mtbl = mpart.getTable();\n          Partition part = convertToPart(mpart);\n          parts.add(part);\n\n          if (\"TRUE\".equalsIgnoreCase(mtbl.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n            String partName = Warehouse.makePartName(this.convertToFieldSchemas(mtbl\n                .getPartitionKeys()), part.getValues());\n            PrincipalPrivilegeSet partAuth = this.getPartitionPrivilegeSet(dbName,\n                tblName, partName, userName, groupNames);\n            part.setPrivileges(partAuth);\n          }\n        }\n      }\n      success =  commitTransaction();\n      return parts;\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n  }"
        ],
        [
            "ObjectStore::dropPartitionsNoTxn(String,String,List)",
            "2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972 -\n2973  \n2974  ",
            "  private void dropPartitionsNoTxn(String dbName, String tblName, List<String> partNames) {\n    ObjectPair<Query, Map<String, String>> queryWithParams =\n        getPartQueryWithParams(dbName, tblName, partNames);\n    Query query = queryWithParams.getFirst();\n    query.setClass(MPartition.class);\n    long deleted = query.deletePersistentAll(queryWithParams.getSecond());\n    LOG.debug(\"Deleted \" + deleted + \" partition from store\");\n    query.closeAll();\n  }",
            "2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953 +\n2954  \n2955  ",
            "  private void dropPartitionsNoTxn(String dbName, String tblName, List<String> partNames) {\n    ObjectPair<Query, Map<String, String>> queryWithParams =\n        getPartQueryWithParams(dbName, tblName, partNames);\n    Query query = queryWithParams.getFirst();\n    query.setClass(MPartition.class);\n    long deleted = query.deletePersistentAll(queryWithParams.getSecond());\n    LOG.debug(\"Deleted {} partition from store\", deleted);\n    query.closeAll();\n  }"
        ],
        [
            "ObjectStore::getDistinctValuesForPartitionsNoTxn(String,String,List,boolean,boolean,long)",
            "2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589 -\n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  ",
            "  private PartitionValuesResponse getDistinctValuesForPartitionsNoTxn(String dbName, String tableName, List<FieldSchema> cols,\n                                                                      boolean applyDistinct, boolean ascending, long maxParts)\n      throws MetaException {\n\n    try {\n      openTransaction();\n      Query q = pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n          + \"where table.database.name == t1 && table.tableName == t2 \");\n      q.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n\n      // TODO: Ordering seems to affect the distinctness, needs checking, disabling.\n/*\n      if (ascending) {\n        q.setOrdering(\"partitionName ascending\");\n      } else {\n        q.setOrdering(\"partitionName descending\");\n      }\n*/\n      if (maxParts > 0) {\n        q.setRange(0, maxParts);\n      }\n      StringBuilder partValuesSelect = new StringBuilder(256);\n      if (applyDistinct) {\n        partValuesSelect.append(\"DISTINCT \");\n      }\n      List<FieldSchema> partitionKeys = getTable(dbName, tableName).getPartitionKeys();\n      for (FieldSchema key : cols) {\n        partValuesSelect.append(extractPartitionKey(key, partitionKeys)).append(\", \");\n      }\n      partValuesSelect.setLength(partValuesSelect.length() - 2);\n      LOG.info(\"Columns to be selected from Partitions: \" + partValuesSelect);\n      q.setResult(partValuesSelect.toString());\n\n      PartitionValuesResponse response = new PartitionValuesResponse();\n      response.setPartitionValues(new ArrayList<PartitionValuesRow>());\n      if (cols.size() > 1) {\n        List<Object[]> results = (List<Object[]>) q.execute(dbName, tableName);\n        for (Object[] row : results) {\n          PartitionValuesRow rowResponse = new PartitionValuesRow();\n          for (Object columnValue : row) {\n            rowResponse.addToRow((String) columnValue);\n          }\n          response.addToPartitionValues(rowResponse);\n        }\n      } else {\n        List<Object> results = (List<Object>) q.execute(dbName, tableName);\n        for (Object row : results) {\n          PartitionValuesRow rowResponse = new PartitionValuesRow();\n          rowResponse.addToRow((String) row);\n          response.addToPartitionValues(rowResponse);\n        }\n      }\n      q.closeAll();\n      return response;\n    } finally {\n      commitTransaction();\n    }\n  }",
            "2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574 +\n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  ",
            "  private PartitionValuesResponse getDistinctValuesForPartitionsNoTxn(String dbName, String tableName, List<FieldSchema> cols,\n                                                                      boolean applyDistinct, boolean ascending, long maxParts)\n      throws MetaException {\n\n    try {\n      openTransaction();\n      Query q = pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n          + \"where table.database.name == t1 && table.tableName == t2 \");\n      q.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n\n      // TODO: Ordering seems to affect the distinctness, needs checking, disabling.\n/*\n      if (ascending) {\n        q.setOrdering(\"partitionName ascending\");\n      } else {\n        q.setOrdering(\"partitionName descending\");\n      }\n*/\n      if (maxParts > 0) {\n        q.setRange(0, maxParts);\n      }\n      StringBuilder partValuesSelect = new StringBuilder(256);\n      if (applyDistinct) {\n        partValuesSelect.append(\"DISTINCT \");\n      }\n      List<FieldSchema> partitionKeys = getTable(dbName, tableName).getPartitionKeys();\n      for (FieldSchema key : cols) {\n        partValuesSelect.append(extractPartitionKey(key, partitionKeys)).append(\", \");\n      }\n      partValuesSelect.setLength(partValuesSelect.length() - 2);\n      LOG.info(\"Columns to be selected from Partitions: {}\", partValuesSelect);\n      q.setResult(partValuesSelect.toString());\n\n      PartitionValuesResponse response = new PartitionValuesResponse();\n      response.setPartitionValues(new ArrayList<PartitionValuesRow>());\n      if (cols.size() > 1) {\n        List<Object[]> results = (List<Object[]>) q.execute(dbName, tableName);\n        for (Object[] row : results) {\n          PartitionValuesRow rowResponse = new PartitionValuesRow();\n          for (Object columnValue : row) {\n            rowResponse.addToRow((String) columnValue);\n          }\n          response.addToPartitionValues(rowResponse);\n        }\n      } else {\n        List<Object> results = (List<Object>) q.execute(dbName, tableName);\n        for (Object row : results) {\n          PartitionValuesRow rowResponse = new PartitionValuesRow();\n          rowResponse.addToRow((String) row);\n          response.addToPartitionValues(rowResponse);\n        }\n      }\n      q.closeAll();\n      return response;\n    } finally {\n      commitTransaction();\n    }\n  }"
        ],
        [
            "ObjectStore::clearOutPmfClassLoaderCache(PersistenceManagerFactory)",
            "8955  \n8956  \n8957  \n8958  \n8959  \n8960  \n8961  \n8962  \n8963  \n8964  \n8965  \n8966  \n8967  \n8968  \n8969  \n8970  \n8971  \n8972  \n8973  \n8974  \n8975  \n8976  \n8977  \n8978  \n8979  \n8980  \n8981  \n8982  \n8983  \n8984  \n8985  \n8986  \n8987  \n8988  \n8989  \n8990  \n8991  \n8992  \n8993  \n8994  \n8995  \n8996  \n8997  \n8998  \n8999  \n9000  \n9001  \n9002  \n9003  \n9004  \n9005  \n9006  \n9007  \n9008  \n9009  \n9010  \n9011  \n9012  \n9013  \n9014  \n9015  \n9016  \n9017  \n9018  \n9019 -\n9020  \n9021  ",
            "  private static void clearOutPmfClassLoaderCache(PersistenceManagerFactory pmf) {\n    if ((pmf == null) || (!(pmf instanceof JDOPersistenceManagerFactory))) {\n      return;\n    }\n    // NOTE : This is hacky, and this section of code is fragile depending on DN code varnames\n    // so it's likely to stop working at some time in the future, especially if we upgrade DN\n    // versions, so we actively need to find a better way to make sure the leak doesn't happen\n    // instead of just clearing out the cache after every call.\n    JDOPersistenceManagerFactory jdoPmf = (JDOPersistenceManagerFactory) pmf;\n    NucleusContext nc = jdoPmf.getNucleusContext();\n    try {\n      Field pmCache = pmf.getClass().getDeclaredField(\"pmCache\");\n      pmCache.setAccessible(true);\n      Set<JDOPersistenceManager> pmSet = (Set<JDOPersistenceManager>)pmCache.get(pmf);\n      for (JDOPersistenceManager pm : pmSet) {\n        org.datanucleus.ExecutionContext ec = pm.getExecutionContext();\n        if (ec instanceof org.datanucleus.ExecutionContextThreadedImpl) {\n          ClassLoaderResolver clr = ((org.datanucleus.ExecutionContextThreadedImpl)ec).getClassLoaderResolver();\n          clearClr(clr);\n        }\n      }\n      org.datanucleus.plugin.PluginManager pluginManager = jdoPmf.getNucleusContext().getPluginManager();\n      Field registryField = pluginManager.getClass().getDeclaredField(\"registry\");\n      registryField.setAccessible(true);\n      org.datanucleus.plugin.PluginRegistry registry = (org.datanucleus.plugin.PluginRegistry)registryField.get(pluginManager);\n      if (registry instanceof org.datanucleus.plugin.NonManagedPluginRegistry) {\n        org.datanucleus.plugin.NonManagedPluginRegistry nRegistry = (org.datanucleus.plugin.NonManagedPluginRegistry)registry;\n        Field clrField = nRegistry.getClass().getDeclaredField(\"clr\");\n        clrField.setAccessible(true);\n        ClassLoaderResolver clr = (ClassLoaderResolver)clrField.get(nRegistry);\n        clearClr(clr);\n      }\n      if (nc instanceof org.datanucleus.PersistenceNucleusContextImpl) {\n        org.datanucleus.PersistenceNucleusContextImpl pnc = (org.datanucleus.PersistenceNucleusContextImpl)nc;\n        org.datanucleus.store.types.TypeManagerImpl tm = (org.datanucleus.store.types.TypeManagerImpl)pnc.getTypeManager();\n        Field clrField = tm.getClass().getDeclaredField(\"clr\");\n        clrField.setAccessible(true);\n        ClassLoaderResolver clr = (ClassLoaderResolver)clrField.get(tm);\n        clearClr(clr);\n        Field storeMgrField = pnc.getClass().getDeclaredField(\"storeMgr\");\n        storeMgrField.setAccessible(true);\n        org.datanucleus.store.rdbms.RDBMSStoreManager storeMgr = (org.datanucleus.store.rdbms.RDBMSStoreManager)storeMgrField.get(pnc);\n        Field backingStoreField = storeMgr.getClass().getDeclaredField(\"backingStoreByMemberName\");\n        backingStoreField.setAccessible(true);\n        Map<String, Store> backingStoreByMemberName = (Map<String, Store>)backingStoreField.get(storeMgr);\n        for (Store store : backingStoreByMemberName.values()) {\n          org.datanucleus.store.rdbms.scostore.BaseContainerStore baseStore = (org.datanucleus.store.rdbms.scostore.BaseContainerStore)store;\n          clrField = org.datanucleus.store.rdbms.scostore.BaseContainerStore.class.getDeclaredField(\"clr\");\n          clrField.setAccessible(true);\n          clr = (ClassLoaderResolver)clrField.get(baseStore);\n          clearClr(clr);\n        }\n      }\n      Field classLoaderResolverMap = AbstractNucleusContext.class.getDeclaredField(\n          \"classLoaderResolverMap\");\n      classLoaderResolverMap.setAccessible(true);\n      Map<String,ClassLoaderResolver> loaderMap =\n          (Map<String, ClassLoaderResolver>) classLoaderResolverMap.get(nc);\n      for (ClassLoaderResolver clr : loaderMap.values()){\n        clearClr(clr);\n      }\n      classLoaderResolverMap.set(nc, new HashMap<String, ClassLoaderResolver>());\n      LOG.debug(\"Removed cached classloaders from DataNucleus NucleusContext\");\n    } catch (Exception e) {\n      LOG.warn(\"Failed to remove cached classloaders from DataNucleus NucleusContext \", e);\n    }\n  }",
            "8930  \n8931  \n8932  \n8933  \n8934  \n8935  \n8936  \n8937  \n8938  \n8939  \n8940  \n8941  \n8942  \n8943  \n8944  \n8945  \n8946  \n8947  \n8948  \n8949  \n8950  \n8951  \n8952  \n8953  \n8954  \n8955  \n8956  \n8957  \n8958  \n8959  \n8960  \n8961  \n8962  \n8963  \n8964  \n8965  \n8966  \n8967  \n8968  \n8969  \n8970  \n8971  \n8972  \n8973  \n8974  \n8975  \n8976  \n8977  \n8978  \n8979  \n8980  \n8981  \n8982  \n8983  \n8984  \n8985  \n8986  \n8987  \n8988  \n8989  \n8990  \n8991  \n8992  \n8993  \n8994 +\n8995  \n8996  ",
            "  private static void clearOutPmfClassLoaderCache(PersistenceManagerFactory pmf) {\n    if ((pmf == null) || (!(pmf instanceof JDOPersistenceManagerFactory))) {\n      return;\n    }\n    // NOTE : This is hacky, and this section of code is fragile depending on DN code varnames\n    // so it's likely to stop working at some time in the future, especially if we upgrade DN\n    // versions, so we actively need to find a better way to make sure the leak doesn't happen\n    // instead of just clearing out the cache after every call.\n    JDOPersistenceManagerFactory jdoPmf = (JDOPersistenceManagerFactory) pmf;\n    NucleusContext nc = jdoPmf.getNucleusContext();\n    try {\n      Field pmCache = pmf.getClass().getDeclaredField(\"pmCache\");\n      pmCache.setAccessible(true);\n      Set<JDOPersistenceManager> pmSet = (Set<JDOPersistenceManager>)pmCache.get(pmf);\n      for (JDOPersistenceManager pm : pmSet) {\n        org.datanucleus.ExecutionContext ec = pm.getExecutionContext();\n        if (ec instanceof org.datanucleus.ExecutionContextThreadedImpl) {\n          ClassLoaderResolver clr = ((org.datanucleus.ExecutionContextThreadedImpl)ec).getClassLoaderResolver();\n          clearClr(clr);\n        }\n      }\n      org.datanucleus.plugin.PluginManager pluginManager = jdoPmf.getNucleusContext().getPluginManager();\n      Field registryField = pluginManager.getClass().getDeclaredField(\"registry\");\n      registryField.setAccessible(true);\n      org.datanucleus.plugin.PluginRegistry registry = (org.datanucleus.plugin.PluginRegistry)registryField.get(pluginManager);\n      if (registry instanceof org.datanucleus.plugin.NonManagedPluginRegistry) {\n        org.datanucleus.plugin.NonManagedPluginRegistry nRegistry = (org.datanucleus.plugin.NonManagedPluginRegistry)registry;\n        Field clrField = nRegistry.getClass().getDeclaredField(\"clr\");\n        clrField.setAccessible(true);\n        ClassLoaderResolver clr = (ClassLoaderResolver)clrField.get(nRegistry);\n        clearClr(clr);\n      }\n      if (nc instanceof org.datanucleus.PersistenceNucleusContextImpl) {\n        org.datanucleus.PersistenceNucleusContextImpl pnc = (org.datanucleus.PersistenceNucleusContextImpl)nc;\n        org.datanucleus.store.types.TypeManagerImpl tm = (org.datanucleus.store.types.TypeManagerImpl)pnc.getTypeManager();\n        Field clrField = tm.getClass().getDeclaredField(\"clr\");\n        clrField.setAccessible(true);\n        ClassLoaderResolver clr = (ClassLoaderResolver)clrField.get(tm);\n        clearClr(clr);\n        Field storeMgrField = pnc.getClass().getDeclaredField(\"storeMgr\");\n        storeMgrField.setAccessible(true);\n        org.datanucleus.store.rdbms.RDBMSStoreManager storeMgr = (org.datanucleus.store.rdbms.RDBMSStoreManager)storeMgrField.get(pnc);\n        Field backingStoreField = storeMgr.getClass().getDeclaredField(\"backingStoreByMemberName\");\n        backingStoreField.setAccessible(true);\n        Map<String, Store> backingStoreByMemberName = (Map<String, Store>)backingStoreField.get(storeMgr);\n        for (Store store : backingStoreByMemberName.values()) {\n          org.datanucleus.store.rdbms.scostore.BaseContainerStore baseStore = (org.datanucleus.store.rdbms.scostore.BaseContainerStore)store;\n          clrField = org.datanucleus.store.rdbms.scostore.BaseContainerStore.class.getDeclaredField(\"clr\");\n          clrField.setAccessible(true);\n          clr = (ClassLoaderResolver)clrField.get(baseStore);\n          clearClr(clr);\n        }\n      }\n      Field classLoaderResolverMap = AbstractNucleusContext.class.getDeclaredField(\n          \"classLoaderResolverMap\");\n      classLoaderResolverMap.setAccessible(true);\n      Map<String,ClassLoaderResolver> loaderMap =\n          (Map<String, ClassLoaderResolver>) classLoaderResolverMap.get(nc);\n      for (ClassLoaderResolver clr : loaderMap.values()){\n        clearClr(clr);\n      }\n      classLoaderResolverMap.set(nc, new HashMap<String, ClassLoaderResolver>());\n      LOG.debug(\"Removed cached classloaders from DataNucleus NucleusContext\");\n    } catch (Exception e) {\n      LOG.warn(\"Failed to remove cached classloaders from DataNucleus NucleusContext\", e);\n    }\n  }"
        ],
        [
            "ObjectStore::convertToMTable(Table)",
            "1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587 -\n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  ",
            "  private MTable convertToMTable(Table tbl) throws InvalidObjectException,\n      MetaException {\n    if (tbl == null) {\n      return null;\n    }\n    MDatabase mdb = null;\n    try {\n      mdb = getMDatabase(tbl.getDbName());\n    } catch (NoSuchObjectException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new InvalidObjectException(\"Database \" + tbl.getDbName()\n          + \" doesn't exist.\");\n    }\n\n    // If the table has property EXTERNAL set, update table type\n    // accordingly\n    String tableType = tbl.getTableType();\n    boolean isExternal = Boolean.parseBoolean(tbl.getParameters().get(\"EXTERNAL\"));\n    if (TableType.MANAGED_TABLE.toString().equals(tableType)) {\n      if (isExternal) {\n        tableType = TableType.EXTERNAL_TABLE.toString();\n      }\n    }\n    if (TableType.EXTERNAL_TABLE.toString().equals(tableType)) {\n      if (!isExternal) {\n        tableType = TableType.MANAGED_TABLE.toString();\n      }\n    }\n\n    // A new table is always created with a new column descriptor\n    return new MTable(normalizeIdentifier(tbl.getTableName()), mdb,\n        convertToMStorageDescriptor(tbl.getSd()), tbl.getOwner(), tbl\n        .getCreateTime(), tbl.getLastAccessTime(), tbl.getRetention(),\n        convertToMFieldSchemas(tbl.getPartitionKeys()), tbl.getParameters(),\n        tbl.getViewOriginalText(), tbl.getViewExpandedText(), tbl.isRewriteEnabled(),\n        tableType);\n  }",
            "1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576 +\n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  ",
            "  private MTable convertToMTable(Table tbl) throws InvalidObjectException,\n      MetaException {\n    if (tbl == null) {\n      return null;\n    }\n    MDatabase mdb = null;\n    try {\n      mdb = getMDatabase(tbl.getDbName());\n    } catch (NoSuchObjectException e) {\n      LOG.error(\"Could not convert to MTable\", e);\n      throw new InvalidObjectException(\"Database \" + tbl.getDbName()\n          + \" doesn't exist.\");\n    }\n\n    // If the table has property EXTERNAL set, update table type\n    // accordingly\n    String tableType = tbl.getTableType();\n    boolean isExternal = Boolean.parseBoolean(tbl.getParameters().get(\"EXTERNAL\"));\n    if (TableType.MANAGED_TABLE.toString().equals(tableType)) {\n      if (isExternal) {\n        tableType = TableType.EXTERNAL_TABLE.toString();\n      }\n    }\n    if (TableType.EXTERNAL_TABLE.toString().equals(tableType)) {\n      if (!isExternal) {\n        tableType = TableType.MANAGED_TABLE.toString();\n      }\n    }\n\n    // A new table is always created with a new column descriptor\n    return new MTable(normalizeIdentifier(tbl.getTableName()), mdb,\n        convertToMStorageDescriptor(tbl.getSd()), tbl.getOwner(), tbl\n        .getCreateTime(), tbl.getLastAccessTime(), tbl.getRetention(),\n        convertToMFieldSchemas(tbl.getPartitionKeys()), tbl.getParameters(),\n        tbl.getViewOriginalText(), tbl.getViewExpandedText(), tbl.isRewriteEnabled(),\n        tableType);\n  }"
        ],
        [
            "ObjectStore::addToken(String,String)",
            "8107  \n8108  \n8109  \n8110  \n8111  \n8112  \n8113  \n8114  \n8115  \n8116  \n8117  \n8118  \n8119  \n8120  \n8121  \n8122  \n8123  \n8124  \n8125  \n8126 -\n8127  \n8128  ",
            "  @Override\n  public boolean addToken(String tokenId, String delegationToken) {\n\n    LOG.debug(\"Begin executing addToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (token == null) {\n        // add Token, only if it already doesn't exist\n        pm.makePersistent(new MDelegationToken(tokenId, delegationToken));\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing addToken with status : \" + committed);\n    return committed && (token == null);\n  }",
            "8083  \n8084  \n8085  \n8086  \n8087  \n8088  \n8089  \n8090  \n8091  \n8092  \n8093  \n8094  \n8095  \n8096  \n8097  \n8098  \n8099  \n8100  \n8101  \n8102 +\n8103  \n8104  ",
            "  @Override\n  public boolean addToken(String tokenId, String delegationToken) {\n\n    LOG.debug(\"Begin executing addToken\");\n    boolean committed = false;\n    MDelegationToken token;\n    try{\n      openTransaction();\n      token = getTokenFrom(tokenId);\n      if (token == null) {\n        // add Token, only if it already doesn't exist\n        pm.makePersistent(new MDelegationToken(tokenId, delegationToken));\n      }\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing addToken with status : {}\", committed);\n    return committed && (token == null);\n  }"
        ],
        [
            "ObjectStore::createDbGuidAndPersist()",
            "3874  \n3875  \n3876  \n3877  \n3878  \n3879  \n3880  \n3881  \n3882 -\n3883  \n3884  \n3885  \n3886  \n3887  \n3888  \n3889 -\n3890  \n3891  \n3892  \n3893 -\n3894  \n3895  \n3896  \n3897  \n3898  \n3899  \n3900  \n3901  \n3902  \n3903  \n3904  \n3905  ",
            "  private String createDbGuidAndPersist() throws MetaException {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      MMetastoreDBProperties prop = new MMetastoreDBProperties();\n      prop.setPropertykey(\"guid\");\n      final String guid = UUID.randomUUID().toString();\n      LOG.debug(\"Attempting to add a guid \" + guid + \" for the metastore db\");\n      prop.setPropertyValue(guid);\n      prop.setDescription(\"Metastore DB GUID generated on \"\n          + LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")));\n      pm.makePersistent(prop);\n      success = commitTransaction();\n      if (success) {\n        LOG.info(\"Metastore db guid \" + guid + \" created successfully\");\n        return guid;\n      }\n    } catch (Exception e) {\n      LOG.warn(e.getMessage(), e);\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    // it possible that some other HMS instance could have created the guid\n    // at the same time due which this instance could not create a guid above\n    // in such case return the guid already generated\n    final String guid = getGuidFromDB();\n    if (guid == null) {\n      throw new MetaException(\"Unable to create or fetch the metastore database uuid\");\n    }\n    return guid;\n  }",
            "3852  \n3853  \n3854  \n3855  \n3856  \n3857  \n3858  \n3859  \n3860 +\n3861  \n3862  \n3863  \n3864  \n3865  \n3866  \n3867 +\n3868  \n3869  \n3870  \n3871 +\n3872  \n3873  \n3874  \n3875  \n3876  \n3877  \n3878  \n3879  \n3880  \n3881  \n3882  \n3883  ",
            "  private String createDbGuidAndPersist() throws MetaException {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      MMetastoreDBProperties prop = new MMetastoreDBProperties();\n      prop.setPropertykey(\"guid\");\n      final String guid = UUID.randomUUID().toString();\n      LOG.debug(\"Attempting to add a guid {} for the metastore db\", guid);\n      prop.setPropertyValue(guid);\n      prop.setDescription(\"Metastore DB GUID generated on \"\n          + LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\")));\n      pm.makePersistent(prop);\n      success = commitTransaction();\n      if (success) {\n        LOG.info(\"Metastore db guid {} created successfully\", guid);\n        return guid;\n      }\n    } catch (Exception e) {\n      LOG.warn(\"Metastore db guid creation failed\", e);\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    // it possible that some other HMS instance could have created the guid\n    // at the same time due which this instance could not create a guid above\n    // in such case return the guid already generated\n    final String guid = getGuidFromDB();\n    if (guid == null) {\n      throw new MetaException(\"Unable to create or fetch the metastore database uuid\");\n    }\n    return guid;\n  }"
        ],
        [
            "ObjectStore::removeMasterKey(Integer)",
            "8248  \n8249  \n8250  \n8251  \n8252  \n8253  \n8254  \n8255  \n8256  \n8257  \n8258  \n8259  \n8260  \n8261  \n8262  \n8263  \n8264  \n8265  \n8266  \n8267 -\n8268  \n8269  ",
            "  @Override\n  public boolean removeMasterKey(Integer id) {\n    LOG.debug(\"Begin executing removeMasterKey\");\n    boolean success = false;\n    Query query = null;\n    MMasterKey masterKey;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class, \"keyId == id\");\n      query.declareParameters(\"java.lang.Integer id\");\n      query.setUnique(true);\n      masterKey = (MMasterKey) query.execute(id);\n      if (null != masterKey) {\n        pm.deletePersistent(masterKey);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    LOG.debug(\"Done executing removeMasterKey with status : \" + success);\n    return (null != masterKey) && success;\n  }",
            "8224  \n8225  \n8226  \n8227  \n8228  \n8229  \n8230  \n8231  \n8232  \n8233  \n8234  \n8235  \n8236  \n8237  \n8238  \n8239  \n8240  \n8241  \n8242  \n8243 +\n8244  \n8245  ",
            "  @Override\n  public boolean removeMasterKey(Integer id) {\n    LOG.debug(\"Begin executing removeMasterKey\");\n    boolean success = false;\n    Query query = null;\n    MMasterKey masterKey;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMasterKey.class, \"keyId == id\");\n      query.declareParameters(\"java.lang.Integer id\");\n      query.setUnique(true);\n      masterKey = (MMasterKey) query.execute(id);\n      if (null != masterKey) {\n        pm.deletePersistent(masterKey);\n      }\n      success = commitTransaction();\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    LOG.debug(\"Done executing removeMasterKey with status : {}\", success);\n    return (null != masterKey) && success;\n  }"
        ],
        [
            "ObjectStore::getDataSourceProps(Configuration)",
            " 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 -\n 553 -\n 554 -\n 555 -\n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591 -\n 592  \n 593  \n 594  \n 595  \n 596  \n 597  ",
            "  /**\n   * Properties specified in hive-default.xml override the properties specified\n   * in jpox.properties.\n   */\n  @SuppressWarnings(\"nls\")\n  private static Properties getDataSourceProps(Configuration conf) {\n    Properties prop = new Properties();\n    correctAutoStartMechanism(conf);\n\n    // First, go through and set all our values for datanucleus and javax.jdo parameters.  This\n    // has to be a separate first step because we don't set the default values in the config object.\n    for (ConfVars var : MetastoreConf.dataNucleusAndJdoConfs) {\n      String confVal = MetastoreConf.getAsString(conf, var);\n      Object prevVal = prop.setProperty(var.getVarname(), confVal);\n      if (LOG.isDebugEnabled() && MetastoreConf.isPrintable(var.getVarname())) {\n        LOG.debug(\"Overriding \" + var.getVarname() + \" value \" + prevVal\n            + \" from  jpox.properties with \" + confVal);\n      }\n    }\n\n    // Now, we need to look for any values that the user set that MetastoreConf doesn't know about.\n    // TODO Commenting this out for now, as it breaks because the conf values aren't getting properly\n    // interpolated in case of variables.  See HIVE-17788.\n    /*\n    for (Map.Entry<String, String> e : conf) {\n      if (e.getKey().startsWith(\"datanucleus.\") || e.getKey().startsWith(\"javax.jdo.\")) {\n        // We have to handle this differently depending on whether it is a value known to\n        // MetastoreConf or not.  If it is, we need to get the default value if a value isn't\n        // provided.  If not, we just set whatever the user has set.\n        Object prevVal = prop.setProperty(e.getKey(), e.getValue());\n        if (LOG.isDebugEnabled() && MetastoreConf.isPrintable(e.getKey())) {\n          LOG.debug(\"Overriding \" + e.getKey() + \" value \" + prevVal\n              + \" from  jpox.properties with \" + e.getValue());\n        }\n      }\n    }\n    */\n\n    // Password may no longer be in the conf, use getPassword()\n    try {\n      String passwd = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n      if (passwd != null && !passwd.isEmpty()) {\n        // We can get away with the use of varname here because varname == hiveName for PWD\n        prop.setProperty(ConfVars.PWD.getVarname(), passwd);\n      }\n    } catch (IOException err) {\n      throw new RuntimeException(\"Error getting metastore password: \" + err.getMessage(), err);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      for (Entry<Object, Object> e : prop.entrySet()) {\n        if (MetastoreConf.isPrintable(e.getKey().toString())) {\n          LOG.debug(e.getKey() + \" = \" + e.getValue());\n        }\n      }\n    }\n\n    return prop;\n  }",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547 +\n 548 +\n 549 +\n 550 +\n 551 +\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576 +\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587 +\n 588  \n 589  \n 590  \n 591  \n 592  \n 593  ",
            "  /**\n   * Properties specified in hive-default.xml override the properties specified\n   * in jpox.properties.\n   */\n  @SuppressWarnings(\"nls\")\n  private static Properties getDataSourceProps(Configuration conf) {\n    Properties prop = new Properties();\n    correctAutoStartMechanism(conf);\n\n    // First, go through and set all our values for datanucleus and javax.jdo parameters.  This\n    // has to be a separate first step because we don't set the default values in the config object.\n    for (ConfVars var : MetastoreConf.dataNucleusAndJdoConfs) {\n      String confVal = MetastoreConf.getAsString(conf, var);\n      String varName = var.getVarname();\n      Object prevVal = prop.setProperty(varName, confVal);\n      if (MetastoreConf.isPrintable(varName)) {\n        LOG.debug(\"Overriding {} value {} from jpox.properties with {}\",\n          varName, prevVal, confVal);\n      }\n    }\n\n    // Now, we need to look for any values that the user set that MetastoreConf doesn't know about.\n    // TODO Commenting this out for now, as it breaks because the conf values aren't getting properly\n    // interpolated in case of variables.  See HIVE-17788.\n    /*\n    for (Map.Entry<String, String> e : conf) {\n      if (e.getKey().startsWith(\"datanucleus.\") || e.getKey().startsWith(\"javax.jdo.\")) {\n        // We have to handle this differently depending on whether it is a value known to\n        // MetastoreConf or not.  If it is, we need to get the default value if a value isn't\n        // provided.  If not, we just set whatever the user has set.\n        Object prevVal = prop.setProperty(e.getKey(), e.getValue());\n        if (LOG.isDebugEnabled() && MetastoreConf.isPrintable(e.getKey())) {\n          LOG.debug(\"Overriding \" + e.getKey() + \" value \" + prevVal\n              + \" from  jpox.properties with \" + e.getValue());\n        }\n      }\n    }\n    */\n\n    // Password may no longer be in the conf, use getPassword()\n    try {\n      String passwd = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n      if (org.apache.commons.lang.StringUtils.isNotEmpty(passwd)) {\n        // We can get away with the use of varname here because varname == hiveName for PWD\n        prop.setProperty(ConfVars.PWD.getVarname(), passwd);\n      }\n    } catch (IOException err) {\n      throw new RuntimeException(\"Error getting metastore password: \" + err.getMessage(), err);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      for (Entry<Object, Object> e : prop.entrySet()) {\n        if (MetastoreConf.isPrintable(e.getKey().toString())) {\n          LOG.debug(\"{} = {}\", e.getKey(), e.getValue());\n        }\n      }\n    }\n\n    return prop;\n  }"
        ],
        [
            "ObjectStore::getDBPrivilege(String,String,PrincipalType)",
            "4974  \n4975  \n4976  \n4977  \n4978  \n4979  \n4980  \n4981  \n4982 -\n4983  \n4984  \n4985  \n4986  \n4987  \n4988  \n4989  \n4990  \n4991  \n4992  \n4993  \n4994  \n4995  ",
            "  public List<PrivilegeGrantInfo> getDBPrivilege(String dbName,\n      String principalName, PrincipalType principalType)\n      throws InvalidObjectException, MetaException {\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MDBPrivilege> userNameDbPriv = this.listPrincipalMDBGrants(\n          principalName, principalType, dbName);\n      if (userNameDbPriv != null && userNameDbPriv.size() > 0) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameDbPriv.size());\n        for (int i = 0; i < userNameDbPriv.size(); i++) {\n          MDBPrivilege item = userNameDbPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }",
            "4952  \n4953  \n4954  \n4955  \n4956  \n4957  \n4958  \n4959  \n4960 +\n4961  \n4962  \n4963  \n4964  \n4965  \n4966  \n4967  \n4968  \n4969  \n4970  \n4971  \n4972  \n4973  ",
            "  public List<PrivilegeGrantInfo> getDBPrivilege(String dbName,\n      String principalName, PrincipalType principalType)\n      throws InvalidObjectException, MetaException {\n    dbName = normalizeIdentifier(dbName);\n\n    if (principalName != null) {\n      List<MDBPrivilege> userNameDbPriv = this.listPrincipalMDBGrants(\n          principalName, principalType, dbName);\n      if (CollectionUtils.isNotEmpty(userNameDbPriv)) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameDbPriv.size());\n        for (int i = 0; i < userNameDbPriv.size(); i++) {\n          MDBPrivilege item = userNameDbPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }"
        ],
        [
            "ObjectStore::addForeignKeys(List,boolean,List,List)",
            "3936  \n3937  \n3938  \n3939  \n3940  \n3941 -\n3942  \n3943  \n3944  \n3945  \n3946  \n3947  \n3948  \n3949  \n3950  \n3951  \n3952  \n3953  \n3954  \n3955  \n3956  \n3957  \n3958  \n3959  \n3960  \n3961  \n3962  \n3963  \n3964  \n3965  \n3966  \n3967  \n3968  \n3969  \n3970  \n3971  \n3972  \n3973  \n3974  \n3975  \n3976  \n3977  \n3978  \n3979  \n3980  \n3981  \n3982  \n3983  \n3984  \n3985  \n3986  \n3987  \n3988  \n3989  \n3990  \n3991  \n3992  \n3993  \n3994  \n3995  \n3996  \n3997  \n3998  \n3999  \n4000  \n4001  \n4002  \n4003  \n4004  \n4005  \n4006  \n4007  \n4008  \n4009  \n4010  \n4011  \n4012  \n4013  \n4014  \n4015  \n4016  \n4017  \n4018  \n4019  \n4020  \n4021  \n4022  \n4023  \n4024  \n4025  \n4026  \n4027  \n4028  \n4029  \n4030  \n4031  \n4032  \n4033  \n4034  \n4035  \n4036  \n4037  \n4038  \n4039  \n4040  \n4041  \n4042  \n4043  \n4044  \n4045  \n4046  \n4047  \n4048  \n4049  \n4050  \n4051  \n4052  \n4053  \n4054  \n4055  \n4056  \n4057  \n4058  \n4059  \n4060  \n4061  \n4062  \n4063  \n4064  \n4065  \n4066  \n4067  \n4068  \n4069  \n4070  \n4071  \n4072  \n4073  \n4074  \n4075  \n4076  \n4077  \n4078  \n4079  \n4080  \n4081  \n4082  \n4083  \n4084  \n4085  \n4086  \n4087  \n4088  \n4089  \n4090  \n4091  \n4092  \n4093  \n4094  \n4095  \n4096  \n4097  \n4098  \n4099  \n4100  \n4101  ",
            "  private List<String> addForeignKeys(List<SQLForeignKey> foreignKeys, boolean retrieveCD,\n      List<SQLPrimaryKey> primaryKeys, List<SQLUniqueConstraint> uniqueConstraints)\n          throws InvalidObjectException, MetaException {\n    List<String> fkNames = new ArrayList<>();\n\n    if (foreignKeys.size() > 0) {\n      List<MConstraint> mpkfks = new ArrayList<>();\n      String currentConstraintName = null;\n      // We start iterating through the foreign keys. This list might contain more than a single\n      // foreign key, and each foreign key might contain multiple columns. The outer loop retrieves\n      // the information that is common for a single key (table information) while the inner loop\n      // checks / adds information about each column.\n      for (int i = 0; i < foreignKeys.size(); i++) {\n        final String fkTableDB = normalizeIdentifier(foreignKeys.get(i).getFktable_db());\n        final String fkTableName = normalizeIdentifier(foreignKeys.get(i).getFktable_name());\n        // If retrieveCD is false, we do not need to do a deep retrieval of the Table Column Descriptor.\n        // For instance, this is the case when we are creating the table.\n        final AttachedMTableInfo nChildTable = getMTable(fkTableDB, fkTableName, retrieveCD);\n        final MTable childTable = nChildTable.mtbl;\n        if (childTable == null) {\n          throw new InvalidObjectException(\"Child table not found: \" + fkTableName);\n        }\n        MColumnDescriptor childCD = retrieveCD ? nChildTable.mcd : childTable.getSd().getCD();\n        final List<MFieldSchema> childCols = childCD == null || childCD.getCols() == null ?\n            new ArrayList<>() : new ArrayList<>(childCD.getCols());\n        if (childTable.getPartitionKeys() != null) {\n          childCols.addAll(childTable.getPartitionKeys());\n        }\n\n        final String pkTableDB = normalizeIdentifier(foreignKeys.get(i).getPktable_db());\n        final String pkTableName = normalizeIdentifier(foreignKeys.get(i).getPktable_name());\n        // For primary keys, we retrieve the column descriptors if retrieveCD is true (which means\n        // it is an alter table statement) or if it is a create table statement but we are\n        // referencing another table instead of self for the primary key.\n        final AttachedMTableInfo nParentTable;\n        final MTable parentTable;\n        MColumnDescriptor parentCD;\n        final List<MFieldSchema> parentCols;\n        final List<SQLPrimaryKey> existingTablePrimaryKeys;\n        final List<SQLUniqueConstraint> existingTableUniqueConstraints;\n        final boolean sameTable = fkTableDB.equals(pkTableDB) && fkTableName.equals(pkTableName);\n        if (sameTable) {\n          nParentTable = nChildTable;\n          parentTable = childTable;\n          parentCD = childCD;\n          parentCols = childCols;\n          existingTablePrimaryKeys = primaryKeys;\n          existingTableUniqueConstraints = uniqueConstraints;\n        } else {\n          nParentTable = getMTable(pkTableDB, pkTableName, true);\n          parentTable = nParentTable.mtbl;\n          if (parentTable == null) {\n            throw new InvalidObjectException(\"Parent table not found: \" + pkTableName);\n          }\n          parentCD = nParentTable.mcd;\n          parentCols = parentCD == null || parentCD.getCols() == null ?\n              new ArrayList<>() : new ArrayList<>(parentCD.getCols());\n          if (parentTable.getPartitionKeys() != null) {\n            parentCols.addAll(parentTable.getPartitionKeys());\n          }\n          existingTablePrimaryKeys = getPrimaryKeys(pkTableDB, pkTableName);\n          existingTableUniqueConstraints = getUniqueConstraints(pkTableDB, pkTableName);\n        }\n\n        // Here we build an aux structure that is used to verify that the foreign key that is declared\n        // is actually referencing a valid primary key or unique key. We also check that the types of\n        // the columns correspond.\n        if (existingTablePrimaryKeys.isEmpty() && existingTableUniqueConstraints.isEmpty()) {\n          throw new MetaException(\n              \"Trying to define foreign key but there are no primary keys or unique keys for referenced table\");\n        }\n        final Set<String> validPKsOrUnique = generateValidPKsOrUniqueSignatures(parentCols,\n            existingTablePrimaryKeys, existingTableUniqueConstraints);\n\n        StringBuilder fkSignature = new StringBuilder();\n        StringBuilder referencedKSignature = new StringBuilder();\n        for (; i < foreignKeys.size(); i++) {\n          final SQLForeignKey foreignKey = foreignKeys.get(i);\n          final String fkColumnName = normalizeIdentifier(foreignKey.getFkcolumn_name());\n          int childIntegerIndex = getColumnIndexFromTableColumns(childCD.getCols(), fkColumnName);\n          if (childIntegerIndex == -1) {\n            if (childTable.getPartitionKeys() != null) {\n              childCD = null;\n              childIntegerIndex = getColumnIndexFromTableColumns(childTable.getPartitionKeys(), fkColumnName);\n            }\n            if (childIntegerIndex == -1) {\n              throw new InvalidObjectException(\"Child column not found: \" + fkColumnName);\n            }\n          }\n\n          final String pkColumnName = normalizeIdentifier(foreignKey.getPkcolumn_name());\n          int parentIntegerIndex = getColumnIndexFromTableColumns(parentCD.getCols(), pkColumnName);\n          if (parentIntegerIndex == -1) {\n            if (parentTable.getPartitionKeys() != null) {\n              parentCD = null;\n              parentIntegerIndex = getColumnIndexFromTableColumns(parentTable.getPartitionKeys(), pkColumnName);\n            }\n            if (parentIntegerIndex == -1) {\n              throw new InvalidObjectException(\"Parent column not found: \" + pkColumnName);\n            }\n          }\n\n          if (foreignKey.getFk_name() == null) {\n            // When there is no explicit foreign key name associated with the constraint and the key is composite,\n            // we expect the foreign keys to be send in order in the input list.\n            // Otherwise, the below code will break.\n            // If this is the first column of the FK constraint, generate the foreign key name\n            // NB: The below code can result in race condition where duplicate names can be generated (in theory).\n            // However, this scenario can be ignored for practical purposes because of\n            // the uniqueness of the generated constraint name.\n            if (foreignKey.getKey_seq() == 1) {\n              currentConstraintName = generateConstraintName(\n                fkTableDB, fkTableName, pkTableDB, pkTableName, pkColumnName, fkColumnName, \"fk\");\n            }\n          } else {\n            currentConstraintName = normalizeIdentifier(foreignKey.getFk_name());\n          }\n          fkNames.add(currentConstraintName);\n          Integer updateRule = foreignKey.getUpdate_rule();\n          Integer deleteRule = foreignKey.getDelete_rule();\n          int enableValidateRely = (foreignKey.isEnable_cstr() ? 4 : 0) +\n                  (foreignKey.isValidate_cstr() ? 2 : 0) + (foreignKey.isRely_cstr() ? 1 : 0);\n          MConstraint mpkfk = new MConstraint(\n            currentConstraintName,\n            MConstraint.FOREIGN_KEY_CONSTRAINT,\n            foreignKey.getKey_seq(),\n            deleteRule,\n            updateRule,\n            enableValidateRely,\n            parentTable,\n            childTable,\n            parentCD,\n            childCD,\n            childIntegerIndex,\n            parentIntegerIndex\n          );\n          mpkfks.add(mpkfk);\n\n          final String fkColType = getColumnFromTableColumns(childCols, fkColumnName).getType();\n          fkSignature.append(\n              generateColNameTypeSignature(fkColumnName, fkColType));\n          referencedKSignature.append(\n              generateColNameTypeSignature(pkColumnName, fkColType));\n\n          if (i + 1 < foreignKeys.size() && foreignKeys.get(i + 1).getKey_seq() == 1) {\n            // Next one is a new key, we bail out from the inner loop\n            break;\n          }\n        }\n        String referenced = referencedKSignature.toString();\n        if (!validPKsOrUnique.contains(referenced)) {\n          throw new MetaException(\n              \"Foreign key references \" + referenced + \" but no corresponding \"\n              + \"primary key or unique key exists. Possible keys: \" + validPKsOrUnique);\n        }\n        if (sameTable && fkSignature.toString().equals(referenced)) {\n          throw new MetaException(\n              \"Cannot be both foreign key and primary/unique key on same table: \" + referenced);\n        }\n        fkSignature = new StringBuilder();\n        referencedKSignature = new StringBuilder();\n      }\n      pm.makePersistentAll(mpkfks);\n    }\n    return fkNames;\n  }",
            "3914  \n3915  \n3916  \n3917  \n3918  \n3919 +\n3920  \n3921  \n3922  \n3923  \n3924  \n3925  \n3926  \n3927  \n3928  \n3929  \n3930  \n3931  \n3932  \n3933  \n3934  \n3935  \n3936  \n3937  \n3938  \n3939  \n3940  \n3941  \n3942  \n3943  \n3944  \n3945  \n3946  \n3947  \n3948  \n3949  \n3950  \n3951  \n3952  \n3953  \n3954  \n3955  \n3956  \n3957  \n3958  \n3959  \n3960  \n3961  \n3962  \n3963  \n3964  \n3965  \n3966  \n3967  \n3968  \n3969  \n3970  \n3971  \n3972  \n3973  \n3974  \n3975  \n3976  \n3977  \n3978  \n3979  \n3980  \n3981  \n3982  \n3983  \n3984  \n3985  \n3986  \n3987  \n3988  \n3989  \n3990  \n3991  \n3992  \n3993  \n3994  \n3995  \n3996  \n3997  \n3998  \n3999  \n4000  \n4001  \n4002  \n4003  \n4004  \n4005  \n4006  \n4007  \n4008  \n4009  \n4010  \n4011  \n4012  \n4013  \n4014  \n4015  \n4016  \n4017  \n4018  \n4019  \n4020  \n4021  \n4022  \n4023  \n4024  \n4025  \n4026  \n4027  \n4028  \n4029  \n4030  \n4031  \n4032  \n4033  \n4034  \n4035  \n4036  \n4037  \n4038  \n4039  \n4040  \n4041  \n4042  \n4043  \n4044  \n4045  \n4046  \n4047  \n4048  \n4049  \n4050  \n4051  \n4052  \n4053  \n4054  \n4055  \n4056  \n4057  \n4058  \n4059  \n4060  \n4061  \n4062  \n4063  \n4064  \n4065  \n4066  \n4067  \n4068  \n4069  \n4070  \n4071  \n4072  \n4073  \n4074  \n4075  \n4076  \n4077  \n4078  \n4079  ",
            "  private List<String> addForeignKeys(List<SQLForeignKey> foreignKeys, boolean retrieveCD,\n      List<SQLPrimaryKey> primaryKeys, List<SQLUniqueConstraint> uniqueConstraints)\n          throws InvalidObjectException, MetaException {\n    List<String> fkNames = new ArrayList<>();\n\n    if (CollectionUtils.isNotEmpty(foreignKeys)) {\n      List<MConstraint> mpkfks = new ArrayList<>();\n      String currentConstraintName = null;\n      // We start iterating through the foreign keys. This list might contain more than a single\n      // foreign key, and each foreign key might contain multiple columns. The outer loop retrieves\n      // the information that is common for a single key (table information) while the inner loop\n      // checks / adds information about each column.\n      for (int i = 0; i < foreignKeys.size(); i++) {\n        final String fkTableDB = normalizeIdentifier(foreignKeys.get(i).getFktable_db());\n        final String fkTableName = normalizeIdentifier(foreignKeys.get(i).getFktable_name());\n        // If retrieveCD is false, we do not need to do a deep retrieval of the Table Column Descriptor.\n        // For instance, this is the case when we are creating the table.\n        final AttachedMTableInfo nChildTable = getMTable(fkTableDB, fkTableName, retrieveCD);\n        final MTable childTable = nChildTable.mtbl;\n        if (childTable == null) {\n          throw new InvalidObjectException(\"Child table not found: \" + fkTableName);\n        }\n        MColumnDescriptor childCD = retrieveCD ? nChildTable.mcd : childTable.getSd().getCD();\n        final List<MFieldSchema> childCols = childCD == null || childCD.getCols() == null ?\n            new ArrayList<>() : new ArrayList<>(childCD.getCols());\n        if (childTable.getPartitionKeys() != null) {\n          childCols.addAll(childTable.getPartitionKeys());\n        }\n\n        final String pkTableDB = normalizeIdentifier(foreignKeys.get(i).getPktable_db());\n        final String pkTableName = normalizeIdentifier(foreignKeys.get(i).getPktable_name());\n        // For primary keys, we retrieve the column descriptors if retrieveCD is true (which means\n        // it is an alter table statement) or if it is a create table statement but we are\n        // referencing another table instead of self for the primary key.\n        final AttachedMTableInfo nParentTable;\n        final MTable parentTable;\n        MColumnDescriptor parentCD;\n        final List<MFieldSchema> parentCols;\n        final List<SQLPrimaryKey> existingTablePrimaryKeys;\n        final List<SQLUniqueConstraint> existingTableUniqueConstraints;\n        final boolean sameTable = fkTableDB.equals(pkTableDB) && fkTableName.equals(pkTableName);\n        if (sameTable) {\n          nParentTable = nChildTable;\n          parentTable = childTable;\n          parentCD = childCD;\n          parentCols = childCols;\n          existingTablePrimaryKeys = primaryKeys;\n          existingTableUniqueConstraints = uniqueConstraints;\n        } else {\n          nParentTable = getMTable(pkTableDB, pkTableName, true);\n          parentTable = nParentTable.mtbl;\n          if (parentTable == null) {\n            throw new InvalidObjectException(\"Parent table not found: \" + pkTableName);\n          }\n          parentCD = nParentTable.mcd;\n          parentCols = parentCD == null || parentCD.getCols() == null ?\n              new ArrayList<>() : new ArrayList<>(parentCD.getCols());\n          if (parentTable.getPartitionKeys() != null) {\n            parentCols.addAll(parentTable.getPartitionKeys());\n          }\n          existingTablePrimaryKeys = getPrimaryKeys(pkTableDB, pkTableName);\n          existingTableUniqueConstraints = getUniqueConstraints(pkTableDB, pkTableName);\n        }\n\n        // Here we build an aux structure that is used to verify that the foreign key that is declared\n        // is actually referencing a valid primary key or unique key. We also check that the types of\n        // the columns correspond.\n        if (existingTablePrimaryKeys.isEmpty() && existingTableUniqueConstraints.isEmpty()) {\n          throw new MetaException(\n              \"Trying to define foreign key but there are no primary keys or unique keys for referenced table\");\n        }\n        final Set<String> validPKsOrUnique = generateValidPKsOrUniqueSignatures(parentCols,\n            existingTablePrimaryKeys, existingTableUniqueConstraints);\n\n        StringBuilder fkSignature = new StringBuilder();\n        StringBuilder referencedKSignature = new StringBuilder();\n        for (; i < foreignKeys.size(); i++) {\n          final SQLForeignKey foreignKey = foreignKeys.get(i);\n          final String fkColumnName = normalizeIdentifier(foreignKey.getFkcolumn_name());\n          int childIntegerIndex = getColumnIndexFromTableColumns(childCD.getCols(), fkColumnName);\n          if (childIntegerIndex == -1) {\n            if (childTable.getPartitionKeys() != null) {\n              childCD = null;\n              childIntegerIndex = getColumnIndexFromTableColumns(childTable.getPartitionKeys(), fkColumnName);\n            }\n            if (childIntegerIndex == -1) {\n              throw new InvalidObjectException(\"Child column not found: \" + fkColumnName);\n            }\n          }\n\n          final String pkColumnName = normalizeIdentifier(foreignKey.getPkcolumn_name());\n          int parentIntegerIndex = getColumnIndexFromTableColumns(parentCD.getCols(), pkColumnName);\n          if (parentIntegerIndex == -1) {\n            if (parentTable.getPartitionKeys() != null) {\n              parentCD = null;\n              parentIntegerIndex = getColumnIndexFromTableColumns(parentTable.getPartitionKeys(), pkColumnName);\n            }\n            if (parentIntegerIndex == -1) {\n              throw new InvalidObjectException(\"Parent column not found: \" + pkColumnName);\n            }\n          }\n\n          if (foreignKey.getFk_name() == null) {\n            // When there is no explicit foreign key name associated with the constraint and the key is composite,\n            // we expect the foreign keys to be send in order in the input list.\n            // Otherwise, the below code will break.\n            // If this is the first column of the FK constraint, generate the foreign key name\n            // NB: The below code can result in race condition where duplicate names can be generated (in theory).\n            // However, this scenario can be ignored for practical purposes because of\n            // the uniqueness of the generated constraint name.\n            if (foreignKey.getKey_seq() == 1) {\n              currentConstraintName = generateConstraintName(\n                fkTableDB, fkTableName, pkTableDB, pkTableName, pkColumnName, fkColumnName, \"fk\");\n            }\n          } else {\n            currentConstraintName = normalizeIdentifier(foreignKey.getFk_name());\n          }\n          fkNames.add(currentConstraintName);\n          Integer updateRule = foreignKey.getUpdate_rule();\n          Integer deleteRule = foreignKey.getDelete_rule();\n          int enableValidateRely = (foreignKey.isEnable_cstr() ? 4 : 0) +\n                  (foreignKey.isValidate_cstr() ? 2 : 0) + (foreignKey.isRely_cstr() ? 1 : 0);\n          MConstraint mpkfk = new MConstraint(\n            currentConstraintName,\n            MConstraint.FOREIGN_KEY_CONSTRAINT,\n            foreignKey.getKey_seq(),\n            deleteRule,\n            updateRule,\n            enableValidateRely,\n            parentTable,\n            childTable,\n            parentCD,\n            childCD,\n            childIntegerIndex,\n            parentIntegerIndex\n          );\n          mpkfks.add(mpkfk);\n\n          final String fkColType = getColumnFromTableColumns(childCols, fkColumnName).getType();\n          fkSignature.append(\n              generateColNameTypeSignature(fkColumnName, fkColType));\n          referencedKSignature.append(\n              generateColNameTypeSignature(pkColumnName, fkColType));\n\n          if (i + 1 < foreignKeys.size() && foreignKeys.get(i + 1).getKey_seq() == 1) {\n            // Next one is a new key, we bail out from the inner loop\n            break;\n          }\n        }\n        String referenced = referencedKSignature.toString();\n        if (!validPKsOrUnique.contains(referenced)) {\n          throw new MetaException(\n              \"Foreign key references \" + referenced + \" but no corresponding \"\n              + \"primary key or unique key exists. Possible keys: \" + validPKsOrUnique);\n        }\n        if (sameTable && fkSignature.toString().equals(referenced)) {\n          throw new MetaException(\n              \"Cannot be both foreign key and primary/unique key on same table: \" + referenced);\n        }\n        fkSignature = new StringBuilder();\n        referencedKSignature = new StringBuilder();\n      }\n      pm.makePersistentAll(mpkfks);\n    }\n    return fkNames;\n  }"
        ],
        [
            "ObjectStore::addNotificationEvent(NotificationEvent)",
            "8796  \n8797  \n8798  \n8799  \n8800  \n8801  \n8802  \n8803  \n8804  \n8805  \n8806  \n8807 -\n8808  \n8809  \n8810  \n8811  \n8812  \n8813  \n8814  \n8815  \n8816  \n8817  \n8818  \n8819  \n8820  \n8821  \n8822  \n8823  \n8824  \n8825  \n8826  ",
            "  @Override\n  public void addNotificationEvent(NotificationEvent entry) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      lockForUpdate();\n      Query objectQuery = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) objectQuery.execute();\n      MNotificationNextId mNotificationNextId = null;\n      boolean needToPersistId;\n      if (ids == null || ids.size() == 0) {\n        mNotificationNextId = new MNotificationNextId(1L);\n        needToPersistId = true;\n      } else {\n        mNotificationNextId = ids.iterator().next();\n        needToPersistId = false;\n      }\n      entry.setEventId(mNotificationNextId.getNextEventId());\n      mNotificationNextId.incrementEventId();\n      if (needToPersistId) {\n        pm.makePersistent(mNotificationNextId);\n      }\n      pm.makePersistent(translateThriftToDb(entry));\n      commited = commitTransaction();\n    } catch (Exception e) {\n      LOG.error(\"couldnot get lock for update\", e);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }",
            "8771  \n8772  \n8773  \n8774  \n8775  \n8776  \n8777  \n8778  \n8779  \n8780  \n8781  \n8782 +\n8783  \n8784  \n8785  \n8786  \n8787  \n8788  \n8789  \n8790  \n8791  \n8792  \n8793  \n8794  \n8795  \n8796  \n8797  \n8798  \n8799  \n8800  \n8801  ",
            "  @Override\n  public void addNotificationEvent(NotificationEvent entry) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      lockForUpdate();\n      Query objectQuery = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) objectQuery.execute();\n      MNotificationNextId mNotificationNextId = null;\n      boolean needToPersistId;\n      if (CollectionUtils.isEmpty(ids)) {\n        mNotificationNextId = new MNotificationNextId(1L);\n        needToPersistId = true;\n      } else {\n        mNotificationNextId = ids.iterator().next();\n        needToPersistId = false;\n      }\n      entry.setEventId(mNotificationNextId.getNextEventId());\n      mNotificationNextId.incrementEventId();\n      if (needToPersistId) {\n        pm.makePersistent(mNotificationNextId);\n      }\n      pm.makePersistent(translateThriftToDb(entry));\n      commited = commitTransaction();\n    } catch (Exception e) {\n      LOG.error(\"couldnot get lock for update\", e);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ],
        [
            "ObjectStore::getColumnPrivilege(String,String,String,String,String,PrincipalType)",
            "5226  \n5227  \n5228  \n5229  \n5230  \n5231  \n5232  \n5233  \n5234  \n5235  \n5236  \n5237  \n5238 -\n5239  \n5240  \n5241  \n5242  \n5243  \n5244  \n5245  \n5246  \n5247  \n5248  \n5249  \n5250  \n5251  \n5252  \n5253 -\n5254  \n5255  \n5256  \n5257  \n5258  \n5259  \n5260  \n5261  \n5262  \n5263  \n5264  \n5265  \n5266  ",
            "  private List<PrivilegeGrantInfo> getColumnPrivilege(String dbName,\n      String tableName, String columnName, String partitionName,\n      String principalName, PrincipalType principalType) {\n\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n    columnName = normalizeIdentifier(columnName);\n\n    if (partitionName == null) {\n      List<MTableColumnPrivilege> userNameColumnPriv = this\n          .listPrincipalMTableColumnGrants(principalName, principalType,\n              dbName, tableName, columnName);\n      if (userNameColumnPriv != null && userNameColumnPriv.size() > 0) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameColumnPriv.size());\n        for (int i = 0; i < userNameColumnPriv.size(); i++) {\n          MTableColumnPrivilege item = userNameColumnPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    } else {\n      List<MPartitionColumnPrivilege> userNameColumnPriv = this\n          .listPrincipalMPartitionColumnGrants(principalName,\n              principalType, dbName, tableName, partitionName, columnName);\n      if (userNameColumnPriv != null && userNameColumnPriv.size() > 0) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameColumnPriv.size());\n        for (int i = 0; i < userNameColumnPriv.size(); i++) {\n          MPartitionColumnPrivilege item = userNameColumnPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }",
            "5204  \n5205  \n5206  \n5207  \n5208  \n5209  \n5210  \n5211  \n5212  \n5213  \n5214  \n5215  \n5216 +\n5217  \n5218  \n5219  \n5220  \n5221  \n5222  \n5223  \n5224  \n5225  \n5226  \n5227  \n5228  \n5229  \n5230  \n5231 +\n5232  \n5233  \n5234  \n5235  \n5236  \n5237  \n5238  \n5239  \n5240  \n5241  \n5242  \n5243  \n5244  ",
            "  private List<PrivilegeGrantInfo> getColumnPrivilege(String dbName,\n      String tableName, String columnName, String partitionName,\n      String principalName, PrincipalType principalType) {\n\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n    columnName = normalizeIdentifier(columnName);\n\n    if (partitionName == null) {\n      List<MTableColumnPrivilege> userNameColumnPriv = this\n          .listPrincipalMTableColumnGrants(principalName, principalType,\n              dbName, tableName, columnName);\n      if (CollectionUtils.isNotEmpty(userNameColumnPriv)) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameColumnPriv.size());\n        for (int i = 0; i < userNameColumnPriv.size(); i++) {\n          MTableColumnPrivilege item = userNameColumnPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    } else {\n      List<MPartitionColumnPrivilege> userNameColumnPriv = this\n          .listPrincipalMPartitionColumnGrants(principalName,\n              principalType, dbName, tableName, partitionName, columnName);\n      if (CollectionUtils.isNotEmpty(userNameColumnPriv)) {\n        List<PrivilegeGrantInfo> grantInfos = new ArrayList<>(\n            userNameColumnPriv.size());\n        for (int i = 0; i < userNameColumnPriv.size(); i++) {\n          MPartitionColumnPrivilege item = userNameColumnPriv.get(i);\n          grantInfos.add(new PrivilegeGrantInfo(item.getPrivilege(), item\n              .getCreateTime(), item.getGrantor(), getPrincipalTypeFromStr(item\n              .getGrantorType()), item.getGrantOption()));\n        }\n        return grantInfos;\n      }\n    }\n    return new ArrayList<>(0);\n  }"
        ],
        [
            "ObjectStore::convertToMFunction(Function)",
            "8496  \n8497  \n8498  \n8499  \n8500  \n8501  \n8502  \n8503  \n8504  \n8505 -\n8506  \n8507  \n8508  \n8509  \n8510  \n8511  \n8512  \n8513  \n8514  \n8515  \n8516  \n8517  \n8518  ",
            "  private MFunction convertToMFunction(Function func) throws InvalidObjectException {\n    if (func == null) {\n      return null;\n    }\n\n    MDatabase mdb = null;\n    try {\n      mdb = getMDatabase(func.getDbName());\n    } catch (NoSuchObjectException e) {\n      LOG.error(StringUtils.stringifyException(e));\n      throw new InvalidObjectException(\"Database \" + func.getDbName() + \" doesn't exist.\");\n    }\n\n    MFunction mfunc = new MFunction(func.getFunctionName(),\n        mdb,\n        func.getClassName(),\n        func.getOwnerName(),\n        func.getOwnerType().name(),\n        func.getCreateTime(),\n        func.getFunctionType().getValue(),\n        convertToMResourceUriList(func.getResourceUris()));\n    return mfunc;\n  }",
            "8471  \n8472  \n8473  \n8474  \n8475  \n8476  \n8477  \n8478  \n8479  \n8480 +\n8481  \n8482  \n8483  \n8484  \n8485  \n8486  \n8487  \n8488  \n8489  \n8490  \n8491  \n8492  \n8493  ",
            "  private MFunction convertToMFunction(Function func) throws InvalidObjectException {\n    if (func == null) {\n      return null;\n    }\n\n    MDatabase mdb = null;\n    try {\n      mdb = getMDatabase(func.getDbName());\n    } catch (NoSuchObjectException e) {\n      LOG.error(\"Database does not exist\", e);\n      throw new InvalidObjectException(\"Database \" + func.getDbName() + \" doesn't exist.\");\n    }\n\n    MFunction mfunc = new MFunction(func.getFunctionName(),\n        mdb,\n        func.getClassName(),\n        func.getOwnerName(),\n        func.getOwnerType().name(),\n        func.getCreateTime(),\n        func.getFunctionType().getValue(),\n        convertToMResourceUriList(func.getResourceUris()));\n    return mfunc;\n  }"
        ],
        [
            "ObjectStore::revokePrivileges(PrivilegeBag,boolean)",
            "5483  \n5484  \n5485  \n5486  \n5487  \n5488  \n5489  \n5490  \n5491  \n5492  \n5493  \n5494 -\n5495  \n5496  \n5497  \n5498  \n5499  \n5500  \n5501  \n5502  \n5503  \n5504  \n5505  \n5506  \n5507  \n5508  \n5509  \n5510  \n5511  \n5512  \n5513  \n5514  \n5515  \n5516  \n5517  \n5518  \n5519  \n5520  \n5521  \n5522  \n5523  \n5524  \n5525  \n5526  \n5527  \n5528  \n5529  \n5530  \n5531  \n5532  \n5533  \n5534  \n5535  \n5536  \n5537  \n5538  \n5539  \n5540  \n5541  \n5542  \n5543  \n5544  \n5545  \n5546  \n5547  \n5548  \n5549  \n5550  \n5551  \n5552  \n5553  \n5554  \n5555  \n5556  \n5557  \n5558  \n5559  \n5560  \n5561  \n5562  \n5563  \n5564  \n5565  \n5566  \n5567  \n5568  \n5569  \n5570  \n5571  \n5572  \n5573  \n5574  \n5575  \n5576  \n5577  \n5578  \n5579  \n5580  \n5581  \n5582  \n5583  \n5584  \n5585  \n5586  \n5587  \n5588  \n5589  \n5590  \n5591  \n5592  \n5593  \n5594  \n5595  \n5596  \n5597  \n5598  \n5599  \n5600  \n5601  \n5602  \n5603  \n5604  \n5605  \n5606  \n5607  \n5608  \n5609  \n5610  \n5611  \n5612  \n5613  \n5614  \n5615  \n5616  \n5617  \n5618  \n5619  \n5620  \n5621  \n5622  \n5623  \n5624  \n5625  \n5626  \n5627  \n5628  \n5629  \n5630  \n5631  \n5632  \n5633  \n5634  \n5635  \n5636  \n5637  \n5638  \n5639  \n5640  \n5641  \n5642  \n5643  \n5644  \n5645  \n5646  \n5647  \n5648  \n5649  \n5650  \n5651  \n5652  \n5653  \n5654  \n5655  \n5656  \n5657  \n5658  \n5659  \n5660  \n5661  \n5662  \n5663  \n5664  \n5665  \n5666  \n5667  \n5668  \n5669  \n5670  \n5671  \n5672  \n5673  \n5674  \n5675  \n5676  \n5677  \n5678  \n5679  \n5680  \n5681  \n5682  \n5683  \n5684  \n5685  \n5686  \n5687  \n5688  \n5689  \n5690  \n5691  \n5692  \n5693  \n5694  \n5695  \n5696  \n5697  \n5698  \n5699  \n5700  \n5701  \n5702  \n5703  \n5704  \n5705  \n5706  \n5707  \n5708  \n5709  \n5710 -\n5711  \n5712  \n5713  \n5714  \n5715  \n5716  \n5717  \n5718  \n5719  \n5720  \n5721  \n5722  \n5723  \n5724  \n5725  ",
            "  @Override\n  public boolean revokePrivileges(PrivilegeBag privileges, boolean grantOption)\n      throws InvalidObjectException, MetaException, NoSuchObjectException {\n    boolean committed = false;\n    try {\n      openTransaction();\n      List<Object> persistentObjs = new ArrayList<>();\n\n      List<HiveObjectPrivilege> privilegeList = privileges.getPrivileges();\n\n\n      if (privilegeList != null && privilegeList.size() > 0) {\n        Iterator<HiveObjectPrivilege> privIter = privilegeList.iterator();\n\n        while (privIter.hasNext()) {\n          HiveObjectPrivilege privDef = privIter.next();\n          HiveObjectRef hiveObject = privDef.getHiveObject();\n          String privilegeStr = privDef.getGrantInfo().getPrivilege();\n          if (privilegeStr == null || privilegeStr.trim().equals(\"\")) {\n            continue;\n          }\n          String[] privs = privilegeStr.split(\",\");\n          String userName = privDef.getPrincipalName();\n          PrincipalType principalType = privDef.getPrincipalType();\n\n          if (hiveObject.getObjectType() == HiveObjectType.GLOBAL) {\n            List<MGlobalPrivilege> mSecUser = this.listPrincipalMGlobalGrants(\n                userName, principalType);\n            boolean found = false;\n            if (mSecUser != null) {\n              for (String privilege : privs) {\n                for (MGlobalPrivilege userGrant : mSecUser) {\n                  String userGrantPrivs = userGrant.getPrivilege();\n                  if (privilege.equals(userGrantPrivs)) {\n                    found = true;\n                    if (grantOption) {\n                      if (userGrant.getGrantOption()) {\n                        userGrant.setGrantOption(false);\n                      } else {\n                        throw new MetaException(\"User \" + userName\n                            + \" does not have grant option with privilege \" + privilege);\n                      }\n                    }\n                    persistentObjs.add(userGrant);\n                    break;\n                  }\n                }\n                if (!found) {\n                  throw new InvalidObjectException(\n                      \"No user grant found for privileges \" + privilege);\n                }\n              }\n            }\n\n          } else if (hiveObject.getObjectType() == HiveObjectType.DATABASE) {\n            MDatabase dbObj = getMDatabase(hiveObject.getDbName());\n            if (dbObj != null) {\n              String db = hiveObject.getDbName();\n              boolean found = false;\n              List<MDBPrivilege> dbGrants = this.listPrincipalMDBGrants(\n                  userName, principalType, db);\n              for (String privilege : privs) {\n                for (MDBPrivilege dbGrant : dbGrants) {\n                  String dbGrantPriv = dbGrant.getPrivilege();\n                  if (privilege.equals(dbGrantPriv)) {\n                    found = true;\n                    if (grantOption) {\n                      if (dbGrant.getGrantOption()) {\n                        dbGrant.setGrantOption(false);\n                      } else {\n                        throw new MetaException(\"User \" + userName\n                            + \" does not have grant option with privilege \" + privilege);\n                      }\n                    }\n                    persistentObjs.add(dbGrant);\n                    break;\n                  }\n                }\n                if (!found) {\n                  throw new InvalidObjectException(\n                      \"No database grant found for privileges \" + privilege\n                          + \" on database \" + db);\n                }\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.TABLE) {\n            boolean found = false;\n            List<MTablePrivilege> tableGrants = this\n                .listAllMTableGrants(userName, principalType,\n                    hiveObject.getDbName(), hiveObject.getObjectName());\n            for (String privilege : privs) {\n              for (MTablePrivilege tabGrant : tableGrants) {\n                String tableGrantPriv = tabGrant.getPrivilege();\n                if (privilege.equalsIgnoreCase(tableGrantPriv)) {\n                  found = true;\n                  if (grantOption) {\n                    if (tabGrant.getGrantOption()) {\n                      tabGrant.setGrantOption(false);\n                    } else {\n                      throw new MetaException(\"User \" + userName\n                          + \" does not have grant option with privilege \" + privilege);\n                    }\n                  }\n                  persistentObjs.add(tabGrant);\n                  break;\n                }\n              }\n              if (!found) {\n                throw new InvalidObjectException(\"No grant (\" + privilege\n                    + \") found \" + \" on table \" + hiveObject.getObjectName()\n                    + \", database is \" + hiveObject.getDbName());\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.PARTITION) {\n\n            boolean found = false;\n            Table tabObj = this.getTable(hiveObject.getDbName(), hiveObject.getObjectName());\n            String partName = null;\n            if (hiveObject.getPartValues() != null) {\n              partName = Warehouse.makePartName(tabObj.getPartitionKeys(), hiveObject.getPartValues());\n            }\n            List<MPartitionPrivilege> partitionGrants = this\n                .listPrincipalMPartitionGrants(userName, principalType,\n                    hiveObject.getDbName(), hiveObject.getObjectName(), partName);\n            for (String privilege : privs) {\n              for (MPartitionPrivilege partGrant : partitionGrants) {\n                String partPriv = partGrant.getPrivilege();\n                if (partPriv.equalsIgnoreCase(privilege)) {\n                  found = true;\n                  if (grantOption) {\n                    if (partGrant.getGrantOption()) {\n                      partGrant.setGrantOption(false);\n                    } else {\n                      throw new MetaException(\"User \" + userName\n                          + \" does not have grant option with privilege \" + privilege);\n                    }\n                  }\n                  persistentObjs.add(partGrant);\n                  break;\n                }\n              }\n              if (!found) {\n                throw new InvalidObjectException(\"No grant (\" + privilege\n                    + \") found \" + \" on table \" + tabObj.getTableName()\n                    + \", partition is \" + partName + \", database is \" + tabObj.getDbName());\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.COLUMN) {\n\n            Table tabObj = this.getTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            String partName = null;\n            if (hiveObject.getPartValues() != null) {\n              partName = Warehouse.makePartName(tabObj.getPartitionKeys(),\n                  hiveObject.getPartValues());\n            }\n\n            if (partName != null) {\n              List<MPartitionColumnPrivilege> mSecCol = listPrincipalMPartitionColumnGrants(\n                  userName, principalType, hiveObject.getDbName(), hiveObject\n                      .getObjectName(), partName, hiveObject.getColumnName());\n              boolean found = false;\n              if (mSecCol != null) {\n                for (String privilege : privs) {\n                  for (MPartitionColumnPrivilege col : mSecCol) {\n                    String colPriv = col.getPrivilege();\n                    if (colPriv.equalsIgnoreCase(privilege)) {\n                      found = true;\n                      if (grantOption) {\n                        if (col.getGrantOption()) {\n                          col.setGrantOption(false);\n                        } else {\n                          throw new MetaException(\"User \" + userName\n                              + \" does not have grant option with privilege \" + privilege);\n                        }\n                      }\n                      persistentObjs.add(col);\n                      break;\n                    }\n                  }\n                  if (!found) {\n                    throw new InvalidObjectException(\"No grant (\" + privilege\n                        + \") found \" + \" on table \" + tabObj.getTableName()\n                        + \", partition is \" + partName + \", column name = \"\n                        + hiveObject.getColumnName() + \", database is \"\n                        + tabObj.getDbName());\n                  }\n                }\n              }\n            } else {\n              List<MTableColumnPrivilege> mSecCol = listPrincipalMTableColumnGrants(\n                  userName, principalType, hiveObject.getDbName(), hiveObject\n                      .getObjectName(), hiveObject.getColumnName());\n              boolean found = false;\n              if (mSecCol != null) {\n                for (String privilege : privs) {\n                  for (MTableColumnPrivilege col : mSecCol) {\n                    String colPriv = col.getPrivilege();\n                    if (colPriv.equalsIgnoreCase(privilege)) {\n                      found = true;\n                      if (grantOption) {\n                        if (col.getGrantOption()) {\n                          col.setGrantOption(false);\n                        } else {\n                          throw new MetaException(\"User \" + userName\n                              + \" does not have grant option with privilege \" + privilege);\n                        }\n                      }\n                      persistentObjs.add(col);\n                      break;\n                    }\n                  }\n                  if (!found) {\n                    throw new InvalidObjectException(\"No grant (\" + privilege\n                        + \") found \" + \" on table \" + tabObj.getTableName()\n                        + \", column name = \"\n                        + hiveObject.getColumnName() + \", database is \"\n                        + tabObj.getDbName());\n                  }\n                }\n              }\n            }\n\n          }\n        }\n      }\n\n      if (persistentObjs.size() > 0) {\n        if (grantOption) {\n          // If grant option specified, only update the privilege, don't remove it.\n          // Grant option has already been removed from the privileges in the section above\n        } else {\n          pm.deletePersistentAll(persistentObjs);\n        }\n      }\n      committed = commitTransaction();\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n    return committed;\n  }",
            "5461  \n5462  \n5463  \n5464  \n5465  \n5466  \n5467  \n5468  \n5469  \n5470  \n5471  \n5472 +\n5473  \n5474  \n5475  \n5476  \n5477  \n5478  \n5479  \n5480  \n5481  \n5482  \n5483  \n5484  \n5485  \n5486  \n5487  \n5488  \n5489  \n5490  \n5491  \n5492  \n5493  \n5494  \n5495  \n5496  \n5497  \n5498  \n5499  \n5500  \n5501  \n5502  \n5503  \n5504  \n5505  \n5506  \n5507  \n5508  \n5509  \n5510  \n5511  \n5512  \n5513  \n5514  \n5515  \n5516  \n5517  \n5518  \n5519  \n5520  \n5521  \n5522  \n5523  \n5524  \n5525  \n5526  \n5527  \n5528  \n5529  \n5530  \n5531  \n5532  \n5533  \n5534  \n5535  \n5536  \n5537  \n5538  \n5539  \n5540  \n5541  \n5542  \n5543  \n5544  \n5545  \n5546  \n5547  \n5548  \n5549  \n5550  \n5551  \n5552  \n5553  \n5554  \n5555  \n5556  \n5557  \n5558  \n5559  \n5560  \n5561  \n5562  \n5563  \n5564  \n5565  \n5566  \n5567  \n5568  \n5569  \n5570  \n5571  \n5572  \n5573  \n5574  \n5575  \n5576  \n5577  \n5578  \n5579  \n5580  \n5581  \n5582  \n5583  \n5584  \n5585  \n5586  \n5587  \n5588  \n5589  \n5590  \n5591  \n5592  \n5593  \n5594  \n5595  \n5596  \n5597  \n5598  \n5599  \n5600  \n5601  \n5602  \n5603  \n5604  \n5605  \n5606  \n5607  \n5608  \n5609  \n5610  \n5611  \n5612  \n5613  \n5614  \n5615  \n5616  \n5617  \n5618  \n5619  \n5620  \n5621  \n5622  \n5623  \n5624  \n5625  \n5626  \n5627  \n5628  \n5629  \n5630  \n5631  \n5632  \n5633  \n5634  \n5635  \n5636  \n5637  \n5638  \n5639  \n5640  \n5641  \n5642  \n5643  \n5644  \n5645  \n5646  \n5647  \n5648  \n5649  \n5650  \n5651  \n5652  \n5653  \n5654  \n5655  \n5656  \n5657  \n5658  \n5659  \n5660  \n5661  \n5662  \n5663  \n5664  \n5665  \n5666  \n5667  \n5668  \n5669  \n5670  \n5671  \n5672  \n5673  \n5674  \n5675  \n5676  \n5677  \n5678  \n5679  \n5680  \n5681  \n5682  \n5683  \n5684  \n5685  \n5686  \n5687  \n5688 +\n5689  \n5690  \n5691  \n5692  \n5693  \n5694  \n5695  \n5696  \n5697  \n5698  \n5699  \n5700  \n5701  \n5702  \n5703  ",
            "  @Override\n  public boolean revokePrivileges(PrivilegeBag privileges, boolean grantOption)\n      throws InvalidObjectException, MetaException, NoSuchObjectException {\n    boolean committed = false;\n    try {\n      openTransaction();\n      List<Object> persistentObjs = new ArrayList<>();\n\n      List<HiveObjectPrivilege> privilegeList = privileges.getPrivileges();\n\n\n      if (CollectionUtils.isNotEmpty(privilegeList)) {\n        Iterator<HiveObjectPrivilege> privIter = privilegeList.iterator();\n\n        while (privIter.hasNext()) {\n          HiveObjectPrivilege privDef = privIter.next();\n          HiveObjectRef hiveObject = privDef.getHiveObject();\n          String privilegeStr = privDef.getGrantInfo().getPrivilege();\n          if (privilegeStr == null || privilegeStr.trim().equals(\"\")) {\n            continue;\n          }\n          String[] privs = privilegeStr.split(\",\");\n          String userName = privDef.getPrincipalName();\n          PrincipalType principalType = privDef.getPrincipalType();\n\n          if (hiveObject.getObjectType() == HiveObjectType.GLOBAL) {\n            List<MGlobalPrivilege> mSecUser = this.listPrincipalMGlobalGrants(\n                userName, principalType);\n            boolean found = false;\n            if (mSecUser != null) {\n              for (String privilege : privs) {\n                for (MGlobalPrivilege userGrant : mSecUser) {\n                  String userGrantPrivs = userGrant.getPrivilege();\n                  if (privilege.equals(userGrantPrivs)) {\n                    found = true;\n                    if (grantOption) {\n                      if (userGrant.getGrantOption()) {\n                        userGrant.setGrantOption(false);\n                      } else {\n                        throw new MetaException(\"User \" + userName\n                            + \" does not have grant option with privilege \" + privilege);\n                      }\n                    }\n                    persistentObjs.add(userGrant);\n                    break;\n                  }\n                }\n                if (!found) {\n                  throw new InvalidObjectException(\n                      \"No user grant found for privileges \" + privilege);\n                }\n              }\n            }\n\n          } else if (hiveObject.getObjectType() == HiveObjectType.DATABASE) {\n            MDatabase dbObj = getMDatabase(hiveObject.getDbName());\n            if (dbObj != null) {\n              String db = hiveObject.getDbName();\n              boolean found = false;\n              List<MDBPrivilege> dbGrants = this.listPrincipalMDBGrants(\n                  userName, principalType, db);\n              for (String privilege : privs) {\n                for (MDBPrivilege dbGrant : dbGrants) {\n                  String dbGrantPriv = dbGrant.getPrivilege();\n                  if (privilege.equals(dbGrantPriv)) {\n                    found = true;\n                    if (grantOption) {\n                      if (dbGrant.getGrantOption()) {\n                        dbGrant.setGrantOption(false);\n                      } else {\n                        throw new MetaException(\"User \" + userName\n                            + \" does not have grant option with privilege \" + privilege);\n                      }\n                    }\n                    persistentObjs.add(dbGrant);\n                    break;\n                  }\n                }\n                if (!found) {\n                  throw new InvalidObjectException(\n                      \"No database grant found for privileges \" + privilege\n                          + \" on database \" + db);\n                }\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.TABLE) {\n            boolean found = false;\n            List<MTablePrivilege> tableGrants = this\n                .listAllMTableGrants(userName, principalType,\n                    hiveObject.getDbName(), hiveObject.getObjectName());\n            for (String privilege : privs) {\n              for (MTablePrivilege tabGrant : tableGrants) {\n                String tableGrantPriv = tabGrant.getPrivilege();\n                if (privilege.equalsIgnoreCase(tableGrantPriv)) {\n                  found = true;\n                  if (grantOption) {\n                    if (tabGrant.getGrantOption()) {\n                      tabGrant.setGrantOption(false);\n                    } else {\n                      throw new MetaException(\"User \" + userName\n                          + \" does not have grant option with privilege \" + privilege);\n                    }\n                  }\n                  persistentObjs.add(tabGrant);\n                  break;\n                }\n              }\n              if (!found) {\n                throw new InvalidObjectException(\"No grant (\" + privilege\n                    + \") found \" + \" on table \" + hiveObject.getObjectName()\n                    + \", database is \" + hiveObject.getDbName());\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.PARTITION) {\n\n            boolean found = false;\n            Table tabObj = this.getTable(hiveObject.getDbName(), hiveObject.getObjectName());\n            String partName = null;\n            if (hiveObject.getPartValues() != null) {\n              partName = Warehouse.makePartName(tabObj.getPartitionKeys(), hiveObject.getPartValues());\n            }\n            List<MPartitionPrivilege> partitionGrants = this\n                .listPrincipalMPartitionGrants(userName, principalType,\n                    hiveObject.getDbName(), hiveObject.getObjectName(), partName);\n            for (String privilege : privs) {\n              for (MPartitionPrivilege partGrant : partitionGrants) {\n                String partPriv = partGrant.getPrivilege();\n                if (partPriv.equalsIgnoreCase(privilege)) {\n                  found = true;\n                  if (grantOption) {\n                    if (partGrant.getGrantOption()) {\n                      partGrant.setGrantOption(false);\n                    } else {\n                      throw new MetaException(\"User \" + userName\n                          + \" does not have grant option with privilege \" + privilege);\n                    }\n                  }\n                  persistentObjs.add(partGrant);\n                  break;\n                }\n              }\n              if (!found) {\n                throw new InvalidObjectException(\"No grant (\" + privilege\n                    + \") found \" + \" on table \" + tabObj.getTableName()\n                    + \", partition is \" + partName + \", database is \" + tabObj.getDbName());\n              }\n            }\n          } else if (hiveObject.getObjectType() == HiveObjectType.COLUMN) {\n\n            Table tabObj = this.getTable(hiveObject.getDbName(), hiveObject\n                .getObjectName());\n            String partName = null;\n            if (hiveObject.getPartValues() != null) {\n              partName = Warehouse.makePartName(tabObj.getPartitionKeys(),\n                  hiveObject.getPartValues());\n            }\n\n            if (partName != null) {\n              List<MPartitionColumnPrivilege> mSecCol = listPrincipalMPartitionColumnGrants(\n                  userName, principalType, hiveObject.getDbName(), hiveObject\n                      .getObjectName(), partName, hiveObject.getColumnName());\n              boolean found = false;\n              if (mSecCol != null) {\n                for (String privilege : privs) {\n                  for (MPartitionColumnPrivilege col : mSecCol) {\n                    String colPriv = col.getPrivilege();\n                    if (colPriv.equalsIgnoreCase(privilege)) {\n                      found = true;\n                      if (grantOption) {\n                        if (col.getGrantOption()) {\n                          col.setGrantOption(false);\n                        } else {\n                          throw new MetaException(\"User \" + userName\n                              + \" does not have grant option with privilege \" + privilege);\n                        }\n                      }\n                      persistentObjs.add(col);\n                      break;\n                    }\n                  }\n                  if (!found) {\n                    throw new InvalidObjectException(\"No grant (\" + privilege\n                        + \") found \" + \" on table \" + tabObj.getTableName()\n                        + \", partition is \" + partName + \", column name = \"\n                        + hiveObject.getColumnName() + \", database is \"\n                        + tabObj.getDbName());\n                  }\n                }\n              }\n            } else {\n              List<MTableColumnPrivilege> mSecCol = listPrincipalMTableColumnGrants(\n                  userName, principalType, hiveObject.getDbName(), hiveObject\n                      .getObjectName(), hiveObject.getColumnName());\n              boolean found = false;\n              if (mSecCol != null) {\n                for (String privilege : privs) {\n                  for (MTableColumnPrivilege col : mSecCol) {\n                    String colPriv = col.getPrivilege();\n                    if (colPriv.equalsIgnoreCase(privilege)) {\n                      found = true;\n                      if (grantOption) {\n                        if (col.getGrantOption()) {\n                          col.setGrantOption(false);\n                        } else {\n                          throw new MetaException(\"User \" + userName\n                              + \" does not have grant option with privilege \" + privilege);\n                        }\n                      }\n                      persistentObjs.add(col);\n                      break;\n                    }\n                  }\n                  if (!found) {\n                    throw new InvalidObjectException(\"No grant (\" + privilege\n                        + \") found \" + \" on table \" + tabObj.getTableName()\n                        + \", column name = \"\n                        + hiveObject.getColumnName() + \", database is \"\n                        + tabObj.getDbName());\n                  }\n                }\n              }\n            }\n\n          }\n        }\n      }\n\n      if (CollectionUtils.isNotEmpty(persistentObjs)) {\n        if (grantOption) {\n          // If grant option specified, only update the privilege, don't remove it.\n          // Grant option has already been removed from the privileges in the section above\n        } else {\n          pm.deletePersistentAll(persistentObjs);\n        }\n      }\n      committed = commitTransaction();\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n    return committed;\n  }"
        ],
        [
            "ObjectStore::GetHelper::commit()",
            "3205  \n3206  \n3207  \n3208 -\n3209 -\n3210  \n3211  \n3212  ",
            "    private T commit() {\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(describeResult() + \" retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n      return results;\n    }",
            "3186  \n3187  \n3188  \n3189 +\n3190 +\n3191 +\n3192 +\n3193 +\n3194  \n3195  \n3196  ",
            "    private T commit() {\n      success = commitTransaction();\n      if (doTrace) {\n        double time = ((System.nanoTime() - start) / 1000000.0);\n        String result = describeResult();\n        String retrieveType = doUseDirectSql ? \"SQL\" : \"ORM\";\n\n        LOG.debug(\"{} retrieved using {} in {}ms\", result, retrieveType, time);\n      }\n      return results;\n    }"
        ],
        [
            "ObjectStore::setMetaStoreSchemaVersion(String,String)",
            "8406  \n8407  \n8408  \n8409  \n8410  \n8411  \n8412  \n8413  \n8414 -\n8415  \n8416  \n8417 -\n8418  \n8419  \n8420  \n8421  \n8422  \n8423  \n8424  \n8425  \n8426  \n8427  \n8428  \n8429  \n8430  \n8431  \n8432  \n8433  \n8434  \n8435  \n8436  \n8437  ",
            "  @Override\n  public void setMetaStoreSchemaVersion(String schemaVersion, String comment) throws MetaException {\n    MVersionTable mSchemaVer;\n    boolean commited = false;\n    boolean recordVersion =\n      MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION_RECORD_VERSION);\n    if (!recordVersion) {\n      LOG.warn(\"setMetaStoreSchemaVersion called but recording version is disabled: \" +\n        \"version = \" + schemaVersion + \", comment = \" + comment);\n      return;\n    }\n    LOG.warn(\"Setting metastore schema version in db to \" + schemaVersion);\n\n    try {\n      mSchemaVer = getMSchemaVersion();\n    } catch (NoSuchObjectException e) {\n      // if the version doesn't exist, then create it\n      mSchemaVer = new MVersionTable();\n    }\n\n    mSchemaVer.setSchemaVersion(schemaVersion);\n    mSchemaVer.setVersionComment(comment);\n    try {\n      openTransaction();\n      pm.makePersistent(mSchemaVer);\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n  }",
            "8381  \n8382  \n8383  \n8384  \n8385  \n8386  \n8387  \n8388  \n8389 +\n8390  \n8391  \n8392 +\n8393  \n8394  \n8395  \n8396  \n8397  \n8398  \n8399  \n8400  \n8401  \n8402  \n8403  \n8404  \n8405  \n8406  \n8407  \n8408  \n8409  \n8410  \n8411  \n8412  ",
            "  @Override\n  public void setMetaStoreSchemaVersion(String schemaVersion, String comment) throws MetaException {\n    MVersionTable mSchemaVer;\n    boolean commited = false;\n    boolean recordVersion =\n      MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION_RECORD_VERSION);\n    if (!recordVersion) {\n      LOG.warn(\"setMetaStoreSchemaVersion called but recording version is disabled: \" +\n        \"version = {}, comment = {}\", schemaVersion, comment);\n      return;\n    }\n    LOG.warn(\"Setting metastore schema version in db to {}\", schemaVersion);\n\n    try {\n      mSchemaVer = getMSchemaVersion();\n    } catch (NoSuchObjectException e) {\n      // if the version doesn't exist, then create it\n      mSchemaVer = new MVersionTable();\n    }\n\n    mSchemaVer.setSchemaVersion(schemaVersion);\n    mSchemaVer.setVersionComment(comment);\n    try {\n      openTransaction();\n      pm.makePersistent(mSchemaVer);\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::addPartitions(String,String,List)",
            "1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860 -\n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  ",
            "  @Override\n  public boolean addPartitions(String dbName, String tblName, List<Partition> parts)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    openTransaction();\n    try {\n      List<MTablePrivilege> tabGrants = null;\n      List<MTableColumnPrivilege> tabColumnGrants = null;\n      MTable table = this.getMTable(dbName, tblName);\n      if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n        tabGrants = this.listAllTableGrants(dbName, tblName);\n        tabColumnGrants = this.listTableAllColumnGrants(dbName, tblName);\n      }\n      List<Object> toPersist = new ArrayList<>();\n      for (Partition part : parts) {\n        if (!part.getTableName().equals(tblName) || !part.getDbName().equals(dbName)) {\n          throw new MetaException(\"Partition does not belong to target table \"\n              + dbName + \".\" + tblName + \": \" + part);\n        }\n        MPartition mpart = convertToMPart(part, true);\n        toPersist.add(mpart);\n        int now = (int)(System.currentTimeMillis()/1000);\n        if (tabGrants != null) {\n          for (MTablePrivilege tab: tabGrants) {\n            toPersist.add(new MPartitionPrivilege(tab.getPrincipalName(),\n                tab.getPrincipalType(), mpart, tab.getPrivilege(), now,\n                tab.getGrantor(), tab.getGrantorType(), tab.getGrantOption()));\n          }\n        }\n\n        if (tabColumnGrants != null) {\n          for (MTableColumnPrivilege col : tabColumnGrants) {\n            toPersist.add(new MPartitionColumnPrivilege(col.getPrincipalName(),\n                col.getPrincipalType(), mpart, col.getColumnName(), col.getPrivilege(),\n                now, col.getGrantor(), col.getGrantorType(), col.getGrantOption()));\n          }\n        }\n      }\n      if (toPersist.size() > 0) {\n        pm.makePersistentAll(toPersist);\n        pm.flush();\n      }\n\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }",
            "1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849 +\n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  ",
            "  @Override\n  public boolean addPartitions(String dbName, String tblName, List<Partition> parts)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    openTransaction();\n    try {\n      List<MTablePrivilege> tabGrants = null;\n      List<MTableColumnPrivilege> tabColumnGrants = null;\n      MTable table = this.getMTable(dbName, tblName);\n      if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(\"PARTITION_LEVEL_PRIVILEGE\"))) {\n        tabGrants = this.listAllTableGrants(dbName, tblName);\n        tabColumnGrants = this.listTableAllColumnGrants(dbName, tblName);\n      }\n      List<Object> toPersist = new ArrayList<>();\n      for (Partition part : parts) {\n        if (!part.getTableName().equals(tblName) || !part.getDbName().equals(dbName)) {\n          throw new MetaException(\"Partition does not belong to target table \"\n              + dbName + \".\" + tblName + \": \" + part);\n        }\n        MPartition mpart = convertToMPart(part, true);\n        toPersist.add(mpart);\n        int now = (int)(System.currentTimeMillis()/1000);\n        if (tabGrants != null) {\n          for (MTablePrivilege tab: tabGrants) {\n            toPersist.add(new MPartitionPrivilege(tab.getPrincipalName(),\n                tab.getPrincipalType(), mpart, tab.getPrivilege(), now,\n                tab.getGrantor(), tab.getGrantorType(), tab.getGrantOption()));\n          }\n        }\n\n        if (tabColumnGrants != null) {\n          for (MTableColumnPrivilege col : tabColumnGrants) {\n            toPersist.add(new MPartitionColumnPrivilege(col.getPrincipalName(),\n                col.getPrincipalType(), mpart, col.getColumnName(), col.getPrivilege(),\n                now, col.getGrantor(), col.getGrantorType(), col.getGrantOption()));\n          }\n        }\n      }\n      if (CollectionUtils.isNotEmpty(toPersist)) {\n        pm.makePersistentAll(toPersist);\n        pm.flush();\n      }\n\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::deletePartitionColumnStatistics(String,String,String,List,String)",
            "7925  \n7926  \n7927  \n7928  \n7929  \n7930  \n7931 -\n7932 -\n7933 -\n7934  \n7935  \n7936  \n7937  \n7938  \n7939  \n7940  \n7941  \n7942  \n7943  \n7944  \n7945  \n7946  \n7947  \n7948  \n7949  \n7950  \n7951  \n7952  \n7953  \n7954  \n7955  \n7956  \n7957  \n7958  \n7959  \n7960  \n7961  \n7962  \n7963  \n7964  \n7965  \n7966  \n7967  \n7968  \n7969  \n7970  \n7971  \n7972  \n7973  \n7974  \n7975  \n7976  \n7977  \n7978  \n7979  \n7980  \n7981  \n7982  \n7983  \n7984  \n7985  \n7986  \n7987  \n7988  \n7989  \n7990  \n7991  \n7992  \n7993  \n7994  \n7995  \n7996  \n7997  \n7998  \n7999  \n8000  \n8001  \n8002  ",
            "  @Override\n  public boolean deletePartitionColumnStatistics(String dbName, String tableName, String partName,\n      List<String> partVals, String colName) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, InvalidInputException {\n    boolean ret = false;\n    Query query = null;\n    if (dbName == null) {\n      dbName = Warehouse.DEFAULT_DATABASE_NAME;\n    }\n    if (tableName == null) {\n      throw new InvalidInputException(\"Table name is null.\");\n    }\n    try {\n      openTransaction();\n      MTable mTable = getMTable(dbName, tableName);\n      MPartitionColumnStatistics mStatsObj;\n      List<MPartitionColumnStatistics> mStatsObjColl;\n      if (mTable == null) {\n        throw new NoSuchObjectException(\"Table \" + tableName\n            + \"  for which stats deletion is requested doesn't exist\");\n      }\n      MPartition mPartition = getMPartition(dbName, tableName, partVals);\n      if (mPartition == null) {\n        throw new NoSuchObjectException(\"Partition \" + partName\n            + \" for which stats deletion is requested doesn't exist\");\n      }\n      query = pm.newQuery(MPartitionColumnStatistics.class);\n      String filter;\n      String parameters;\n      if (colName != null) {\n        filter =\n            \"partition.partitionName == t1 && dbName == t2 && tableName == t3 && \"\n                + \"colName == t4\";\n        parameters =\n            \"java.lang.String t1, java.lang.String t2, \"\n                + \"java.lang.String t3, java.lang.String t4\";\n      } else {\n        filter = \"partition.partitionName == t1 && dbName == t2 && tableName == t3\";\n        parameters = \"java.lang.String t1, java.lang.String t2, java.lang.String t3\";\n      }\n      query.setFilter(filter);\n      query.declareParameters(parameters);\n      if (colName != null) {\n        query.setUnique(true);\n        mStatsObj =\n            (MPartitionColumnStatistics) query.executeWithArray(partName.trim(),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(tableName),\n                normalizeIdentifier(colName));\n        pm.retrieve(mStatsObj);\n        if (mStatsObj != null) {\n          pm.deletePersistent(mStatsObj);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" partition=\" + partName + \" col=\" + colName);\n        }\n      } else {\n        mStatsObjColl =\n            (List<MPartitionColumnStatistics>) query.execute(partName.trim(),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(tableName));\n        pm.retrieveAll(mStatsObjColl);\n        if (mStatsObjColl != null) {\n          pm.deletePersistentAll(mStatsObjColl);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" partition\" + partName);\n        }\n      }\n      ret = commitTransaction();\n    } catch (NoSuchObjectException e) {\n      rollbackTransaction();\n      throw e;\n    } finally {\n      rollbackAndCleanup(ret, query);\n    }\n    return ret;\n  }",
            "7903  \n7904  \n7905  \n7906  \n7907  \n7908  \n7909 +\n7910 +\n7911  \n7912  \n7913  \n7914  \n7915  \n7916  \n7917  \n7918  \n7919  \n7920  \n7921  \n7922  \n7923  \n7924  \n7925  \n7926  \n7927  \n7928  \n7929  \n7930  \n7931  \n7932  \n7933  \n7934  \n7935  \n7936  \n7937  \n7938  \n7939  \n7940  \n7941  \n7942  \n7943  \n7944  \n7945  \n7946  \n7947  \n7948  \n7949  \n7950  \n7951  \n7952  \n7953  \n7954  \n7955  \n7956  \n7957  \n7958  \n7959  \n7960  \n7961  \n7962  \n7963  \n7964  \n7965  \n7966  \n7967  \n7968  \n7969  \n7970  \n7971  \n7972  \n7973  \n7974  \n7975  \n7976  \n7977  \n7978  \n7979  ",
            "  @Override\n  public boolean deletePartitionColumnStatistics(String dbName, String tableName, String partName,\n      List<String> partVals, String colName) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, InvalidInputException {\n    boolean ret = false;\n    Query query = null;\n    dbName = org.apache.commons.lang.StringUtils.defaultString(dbName,\n      Warehouse.DEFAULT_DATABASE_NAME);\n    if (tableName == null) {\n      throw new InvalidInputException(\"Table name is null.\");\n    }\n    try {\n      openTransaction();\n      MTable mTable = getMTable(dbName, tableName);\n      MPartitionColumnStatistics mStatsObj;\n      List<MPartitionColumnStatistics> mStatsObjColl;\n      if (mTable == null) {\n        throw new NoSuchObjectException(\"Table \" + tableName\n            + \"  for which stats deletion is requested doesn't exist\");\n      }\n      MPartition mPartition = getMPartition(dbName, tableName, partVals);\n      if (mPartition == null) {\n        throw new NoSuchObjectException(\"Partition \" + partName\n            + \" for which stats deletion is requested doesn't exist\");\n      }\n      query = pm.newQuery(MPartitionColumnStatistics.class);\n      String filter;\n      String parameters;\n      if (colName != null) {\n        filter =\n            \"partition.partitionName == t1 && dbName == t2 && tableName == t3 && \"\n                + \"colName == t4\";\n        parameters =\n            \"java.lang.String t1, java.lang.String t2, \"\n                + \"java.lang.String t3, java.lang.String t4\";\n      } else {\n        filter = \"partition.partitionName == t1 && dbName == t2 && tableName == t3\";\n        parameters = \"java.lang.String t1, java.lang.String t2, java.lang.String t3\";\n      }\n      query.setFilter(filter);\n      query.declareParameters(parameters);\n      if (colName != null) {\n        query.setUnique(true);\n        mStatsObj =\n            (MPartitionColumnStatistics) query.executeWithArray(partName.trim(),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(tableName),\n                normalizeIdentifier(colName));\n        pm.retrieve(mStatsObj);\n        if (mStatsObj != null) {\n          pm.deletePersistent(mStatsObj);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" partition=\" + partName + \" col=\" + colName);\n        }\n      } else {\n        mStatsObjColl =\n            (List<MPartitionColumnStatistics>) query.execute(partName.trim(),\n                normalizeIdentifier(dbName),\n                normalizeIdentifier(tableName));\n        pm.retrieveAll(mStatsObjColl);\n        if (mStatsObjColl != null) {\n          pm.deletePersistentAll(mStatsObjColl);\n        } else {\n          throw new NoSuchObjectException(\"Column stats doesn't exist for db=\" + dbName + \" table=\"\n              + tableName + \" partition\" + partName);\n        }\n      }\n      ret = commitTransaction();\n    } catch (NoSuchObjectException e) {\n      rollbackTransaction();\n      throw e;\n    } finally {\n      rollbackAndCleanup(ret, query);\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::getTablePrivilegeSet(String,String,String,List)",
            "5083  \n5084  \n5085  \n5086  \n5087  \n5088  \n5089  \n5090  \n5091  \n5092  \n5093  \n5094  \n5095  \n5096  \n5097  \n5098  \n5099  \n5100 -\n5101  \n5102  \n5103  \n5104  \n5105  \n5106  \n5107  \n5108  \n5109 -\n5110  \n5111  \n5112  \n5113  \n5114  \n5115  \n5116  \n5117  \n5118  \n5119  \n5120  \n5121  \n5122  \n5123  \n5124  ",
            "  @Override\n  public PrincipalPrivilegeSet getTablePrivilegeSet(String dbName,\n      String tableName, String userName, List<String> groupNames)\n      throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> tableUserPriv = new HashMap<>();\n        tableUserPriv.put(userName, getTablePrivilege(dbName,\n            tableName, userName, PrincipalType.USER));\n        ret.setUserPrivileges(tableUserPriv);\n      }\n      if (groupNames != null && groupNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> tableGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          tableGroupPriv.put(groupName, getTablePrivilege(dbName, tableName,\n              groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(tableGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (roleNames != null && roleNames.size() > 0) {\n        Map<String, List<PrivilegeGrantInfo>> tableRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          tableRolePriv.put(roleName, getTablePrivilege(dbName, tableName,\n              roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(tableRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }",
            "5061  \n5062  \n5063  \n5064  \n5065  \n5066  \n5067  \n5068  \n5069  \n5070  \n5071  \n5072  \n5073  \n5074  \n5075  \n5076  \n5077  \n5078 +\n5079  \n5080  \n5081  \n5082  \n5083  \n5084  \n5085  \n5086  \n5087 +\n5088  \n5089  \n5090  \n5091  \n5092  \n5093  \n5094  \n5095  \n5096  \n5097  \n5098  \n5099  \n5100  \n5101  \n5102  ",
            "  @Override\n  public PrincipalPrivilegeSet getTablePrivilegeSet(String dbName,\n      String tableName, String userName, List<String> groupNames)\n      throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    PrincipalPrivilegeSet ret = new PrincipalPrivilegeSet();\n    tableName = normalizeIdentifier(tableName);\n    dbName = normalizeIdentifier(dbName);\n\n    try {\n      openTransaction();\n      if (userName != null) {\n        Map<String, List<PrivilegeGrantInfo>> tableUserPriv = new HashMap<>();\n        tableUserPriv.put(userName, getTablePrivilege(dbName,\n            tableName, userName, PrincipalType.USER));\n        ret.setUserPrivileges(tableUserPriv);\n      }\n      if (CollectionUtils.isNotEmpty(groupNames)) {\n        Map<String, List<PrivilegeGrantInfo>> tableGroupPriv = new HashMap<>();\n        for (String groupName : groupNames) {\n          tableGroupPriv.put(groupName, getTablePrivilege(dbName, tableName,\n              groupName, PrincipalType.GROUP));\n        }\n        ret.setGroupPrivileges(tableGroupPriv);\n      }\n      Set<String> roleNames = listAllRolesInHierarchy(userName, groupNames);\n      if (CollectionUtils.isNotEmpty(roleNames)) {\n        Map<String, List<PrivilegeGrantInfo>> tableRolePriv = new HashMap<>();\n        for (String roleName : roleNames) {\n          tableRolePriv.put(roleName, getTablePrivilege(dbName, tableName,\n              roleName, PrincipalType.ROLE));\n        }\n        ret.setRolePrivileges(tableRolePriv);\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return ret;\n  }"
        ],
        [
            "ObjectStore::writeMTableColumnStatistics(Table,MTableColumnStatistics,MTableColumnStatistics)",
            "7423  \n7424  \n7425  \n7426  \n7427  \n7428  \n7429  \n7430  \n7431  \n7432 -\n7433 -\n7434  \n7435  \n7436  \n7437  \n7438  \n7439  \n7440  \n7441  \n7442  \n7443  \n7444  \n7445  \n7446  \n7447  \n7448  \n7449  ",
            "  private void writeMTableColumnStatistics(Table table, MTableColumnStatistics mStatsObj,\n      MTableColumnStatistics oldStats) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, InvalidInputException {\n    String dbName = mStatsObj.getDbName();\n    String tableName = mStatsObj.getTableName();\n    String colName = mStatsObj.getColName();\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      LOG.info(\"Updating table level column statistics for db=\" + dbName + \" tableName=\" + tableName\n        + \" colName=\" + colName);\n      validateTableCols(table, Lists.newArrayList(colName));\n\n      if (oldStats != null) {\n        StatObjectConverter.setFieldsIntoOldStats(mStatsObj, oldStats);\n      } else {\n        if (sqlGenerator.getDbProduct().equals(DatabaseProduct.POSTGRES) && mStatsObj.getBitVector() == null) {\n          // workaround for DN bug in persisting nulls in pg bytea column\n          // instead set empty bit vector with header.\n          mStatsObj.setBitVector(new byte[] {'H','L'});\n        }\n        pm.makePersistent(mStatsObj);\n      }\n    } finally {\n      queryWrapper.close();\n    }\n  }",
            "7401  \n7402  \n7403  \n7404  \n7405  \n7406  \n7407  \n7408  \n7409  \n7410 +\n7411 +\n7412  \n7413  \n7414  \n7415  \n7416  \n7417  \n7418  \n7419  \n7420  \n7421  \n7422  \n7423  \n7424  \n7425  \n7426  \n7427  ",
            "  private void writeMTableColumnStatistics(Table table, MTableColumnStatistics mStatsObj,\n      MTableColumnStatistics oldStats) throws NoSuchObjectException, MetaException,\n      InvalidObjectException, InvalidInputException {\n    String dbName = mStatsObj.getDbName();\n    String tableName = mStatsObj.getTableName();\n    String colName = mStatsObj.getColName();\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      LOG.info(\"Updating table level column statistics for db={} tableName={}\" +\n        \" colName={}\", tableName, dbName, colName);\n      validateTableCols(table, Lists.newArrayList(colName));\n\n      if (oldStats != null) {\n        StatObjectConverter.setFieldsIntoOldStats(mStatsObj, oldStats);\n      } else {\n        if (sqlGenerator.getDbProduct().equals(DatabaseProduct.POSTGRES) && mStatsObj.getBitVector() == null) {\n          // workaround for DN bug in persisting nulls in pg bytea column\n          // instead set empty bit vector with header.\n          mStatsObj.setBitVector(new byte[] {'H','L'});\n        }\n        pm.makePersistent(mStatsObj);\n      }\n    } finally {\n      queryWrapper.close();\n    }\n  }"
        ],
        [
            "ObjectStore::shutdown()",
            " 670  \n 671  \n 672  \n 673 -\n 674 -\n 675  \n 676  \n 677  \n 678  ",
            "  @Override\n  public void shutdown() {\n    if (pm != null) {\n      LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n          \" will be shutdown\");\n      pm.close();\n      pm = null;\n    }\n  }",
            " 665  \n 666  \n 667 +\n 668  \n 669  \n 670  \n 671  \n 672  ",
            "  @Override\n  public void shutdown() {\n    LOG.debug(\"RawStore: {}, with PersistenceManager: {} will be shutdown\", this, pm);\n    if (pm != null) {\n      pm.close();\n      pm = null;\n    }\n  }"
        ],
        [
            "ObjectStore::makeQueryFilterString(String,Table,ExpressionTree,Map,boolean)",
            "3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424  \n3425  \n3426  \n3427  \n3428  \n3429  \n3430  \n3431  \n3432  \n3433  \n3434  \n3435  \n3436  \n3437  \n3438  \n3439  \n3440 -\n3441  \n3442  \n3443  \n3444 -\n3445  \n3446  ",
            "  /**\n   * Makes a JDO query filter string for tables or partitions.\n   * @param dbName Database name.\n   * @param table Table. If null, the query returned is over tables in a database.\n   *   If not null, the query returned is over partitions in a table.\n   * @param tree The expression tree from which JDOQL filter will be made.\n   * @param params Parameters for the filter. Some parameters may be added here.\n   * @param isValidatedFilter Whether the filter was pre-validated for JDOQL pushdown\n   *   by the client; if it was and we fail to create a filter, we will throw.\n   * @return Resulting filter. Can be null if isValidatedFilter is false, and there was error.\n   */\n  private String makeQueryFilterString(String dbName, Table table, ExpressionTree tree,\n      Map<String, Object> params, boolean isValidatedFilter) throws MetaException {\n    assert tree != null;\n    FilterBuilder queryBuilder = new FilterBuilder(isValidatedFilter);\n    if (table != null) {\n      queryBuilder.append(\"table.tableName == t1 && table.database.name == t2\");\n      params.put(\"t1\", table.getTableName());\n      params.put(\"t2\", table.getDbName());\n    } else {\n      queryBuilder.append(\"database.name == dbName\");\n      params.put(\"dbName\", dbName);\n    }\n\n    tree.generateJDOFilterFragment(getConf(), table, params, queryBuilder);\n    if (queryBuilder.hasError()) {\n      assert !isValidatedFilter;\n      LOG.info(\"JDO filter pushdown cannot be used: \" + queryBuilder.getErrorMessage());\n      return null;\n    }\n    String jdoFilter = queryBuilder.getFilter();\n    LOG.debug(\"jdoFilter = \" + jdoFilter);\n    return jdoFilter;\n  }",
            "3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424 +\n3425  \n3426  \n3427  \n3428 +\n3429  \n3430  ",
            "  /**\n   * Makes a JDO query filter string for tables or partitions.\n   * @param dbName Database name.\n   * @param table Table. If null, the query returned is over tables in a database.\n   *   If not null, the query returned is over partitions in a table.\n   * @param tree The expression tree from which JDOQL filter will be made.\n   * @param params Parameters for the filter. Some parameters may be added here.\n   * @param isValidatedFilter Whether the filter was pre-validated for JDOQL pushdown\n   *   by the client; if it was and we fail to create a filter, we will throw.\n   * @return Resulting filter. Can be null if isValidatedFilter is false, and there was error.\n   */\n  private String makeQueryFilterString(String dbName, Table table, ExpressionTree tree,\n      Map<String, Object> params, boolean isValidatedFilter) throws MetaException {\n    assert tree != null;\n    FilterBuilder queryBuilder = new FilterBuilder(isValidatedFilter);\n    if (table != null) {\n      queryBuilder.append(\"table.tableName == t1 && table.database.name == t2\");\n      params.put(\"t1\", table.getTableName());\n      params.put(\"t2\", table.getDbName());\n    } else {\n      queryBuilder.append(\"database.name == dbName\");\n      params.put(\"dbName\", dbName);\n    }\n\n    tree.generateJDOFilterFragment(getConf(), table, params, queryBuilder);\n    if (queryBuilder.hasError()) {\n      assert !isValidatedFilter;\n      LOG.info(\"JDO filter pushdown cannot be used: {}\", queryBuilder.getErrorMessage());\n      return null;\n    }\n    String jdoFilter = queryBuilder.getFilter();\n    LOG.debug(\"jdoFilter = {}\", jdoFilter);\n    return jdoFilter;\n  }"
        ],
        [
            "ObjectStore::initialize(Properties)",
            " 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397 -\n 398 -\n 399  \n 400  \n 401  \n 402  \n 403 -\n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "  @SuppressWarnings(\"nls\")\n  private void initialize(Properties dsProps) {\n    int retryLimit = MetastoreConf.getIntVar(conf, ConfVars.HMSHANDLERATTEMPTS);\n    long retryInterval = MetastoreConf.getTimeVar(conf,\n        ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int numTries = retryLimit;\n\n    while (numTries > 0){\n      try {\n        initializeHelper(dsProps);\n        return; // If we reach here, we succeed.\n      } catch (Exception e){\n        numTries--;\n        boolean retriable = isRetriableException(e);\n        if ((numTries > 0) && retriable){\n          LOG.info(\"Retriable exception while instantiating ObjectStore, retrying. \"\n              + numTries + \" tries left\", e);\n          try {\n            Thread.sleep(retryInterval);\n          } catch (InterruptedException ie) {\n            // Restore the interrupted status, since we do not want to catch it.\n            LOG.debug(\"Interrupted while sleeping before retrying.\",ie);\n            Thread.currentThread().interrupt();\n          }\n          // If we're here, we'll proceed down the next while loop iteration.\n        } else {\n          // we've reached our limit, throw the last one.\n          if (retriable){\n            LOG.warn(\"Exception retry limit reached, not retrying any longer.\",\n              e);\n          } else {\n            LOG.debug(\"Non-retriable exception during ObjectStore initialize.\", e);\n          }\n          throw e;\n        }\n      }\n    }\n  }",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396 +\n 397  \n 398  \n 399  \n 400  \n 401 +\n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  @SuppressWarnings(\"nls\")\n  private void initialize(Properties dsProps) {\n    int retryLimit = MetastoreConf.getIntVar(conf, ConfVars.HMSHANDLERATTEMPTS);\n    long retryInterval = MetastoreConf.getTimeVar(conf,\n        ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int numTries = retryLimit;\n\n    while (numTries > 0){\n      try {\n        initializeHelper(dsProps);\n        return; // If we reach here, we succeed.\n      } catch (Exception e){\n        numTries--;\n        boolean retriable = isRetriableException(e);\n        if ((numTries > 0) && retriable){\n          LOG.info(\"Retriable exception while instantiating ObjectStore, retrying. \" +\n              \"{} tries left\", numTries, e);\n          try {\n            Thread.sleep(retryInterval);\n          } catch (InterruptedException ie) {\n            // Restore the interrupted status, since we do not want to catch it.\n            LOG.debug(\"Interrupted while sleeping before retrying.\", ie);\n            Thread.currentThread().interrupt();\n          }\n          // If we're here, we'll proceed down the next while loop iteration.\n        } else {\n          // we've reached our limit, throw the last one.\n          if (retriable){\n            LOG.warn(\"Exception retry limit reached, not retrying any longer.\",\n              e);\n          } else {\n            LOG.debug(\"Non-retriable exception during ObjectStore initialize.\", e);\n          }\n          throw e;\n        }\n      }\n    }\n  }"
        ],
        [
            "ObjectStore::getGuidFromDB()",
            "3907  \n3908  \n3909  \n3910  \n3911  \n3912  \n3913  \n3914  \n3915  \n3916  \n3917  \n3918 -\n3919  \n3920  \n3921  \n3922  \n3923  \n3924  \n3925  \n3926 -\n3927  \n3928  \n3929  \n3930  \n3931  \n3932  \n3933  \n3934  ",
            "  private String getGuidFromDB() throws MetaException {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMetastoreDBProperties.class, \"this.propertyKey == key\");\n      query.declareParameters(\"java.lang.String key\");\n      Collection<MMetastoreDBProperties> names = (Collection<MMetastoreDBProperties>) query.execute(\"guid\");\n      List<String> uuids = new ArrayList<>();\n      for (Iterator<MMetastoreDBProperties> i = names.iterator(); i.hasNext();) {\n        String uuid = i.next().getPropertyValue();\n        LOG.debug(\"Found guid \" + uuid);\n        uuids.add(uuid);\n      }\n      success = commitTransaction();\n      if(uuids.size() > 1) {\n        throw new MetaException(\"Multiple uuids found\");\n      }\n      if(!uuids.isEmpty()) {\n        LOG.debug(\"Returning guid of metastore db : \" + uuids.get(0));\n        return uuids.get(0);\n      }\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    LOG.warn(\"Guid for metastore db not found\");\n    return null;\n  }",
            "3885  \n3886  \n3887  \n3888  \n3889  \n3890  \n3891  \n3892  \n3893  \n3894  \n3895  \n3896 +\n3897  \n3898  \n3899  \n3900  \n3901  \n3902  \n3903  \n3904 +\n3905  \n3906  \n3907  \n3908  \n3909  \n3910  \n3911  \n3912  ",
            "  private String getGuidFromDB() throws MetaException {\n    boolean success = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MMetastoreDBProperties.class, \"this.propertyKey == key\");\n      query.declareParameters(\"java.lang.String key\");\n      Collection<MMetastoreDBProperties> names = (Collection<MMetastoreDBProperties>) query.execute(\"guid\");\n      List<String> uuids = new ArrayList<>();\n      for (Iterator<MMetastoreDBProperties> i = names.iterator(); i.hasNext();) {\n        String uuid = i.next().getPropertyValue();\n        LOG.debug(\"Found guid {}\", uuid);\n        uuids.add(uuid);\n      }\n      success = commitTransaction();\n      if(uuids.size() > 1) {\n        throw new MetaException(\"Multiple uuids found\");\n      }\n      if(!uuids.isEmpty()) {\n        LOG.debug(\"Returning guid of metastore db : {}\", uuids.get(0));\n        return uuids.get(0);\n      }\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n    LOG.warn(\"Guid for metastore db not found\");\n    return null;\n  }"
        ],
        [
            "ObjectStore::getPartitionNamesNoTxn(String,String,short)",
            "2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631 -\n2632 -\n2633 -\n2634 -\n2635  \n2636  \n2637  \n2638  \n2639  \n2640  ",
            "  private List<String> getPartitionNamesNoTxn(String dbName, String tableName, short max) {\n    List<String> pns = new ArrayList<>();\n    dbName = normalizeIdentifier(dbName);\n    tableName = normalizeIdentifier(tableName);\n    Query query =\n        pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n            + \"where table.database.name == t1 && table.tableName == t2 \"\n            + \"order by partitionName asc\");\n    query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n    query.setResult(\"partitionName\");\n    if (max > 0) {\n      query.setRange(0, max);\n    }\n    Collection names = (Collection) query.execute(dbName, tableName);\n    for (Iterator i = names.iterator(); i.hasNext();) {\n      pns.add((String) i.next());\n    }\n\n    if (query != null) {\n      query.closeAll();\n    }\n    return pns;\n  }",
            "2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616 +\n2617 +\n2618  \n2619  \n2620  \n2621  \n2622  \n2623  ",
            "  private List<String> getPartitionNamesNoTxn(String dbName, String tableName, short max) {\n    List<String> pns = new ArrayList<>();\n    dbName = normalizeIdentifier(dbName);\n    tableName = normalizeIdentifier(tableName);\n    Query query =\n        pm.newQuery(\"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n            + \"where table.database.name == t1 && table.tableName == t2 \"\n            + \"order by partitionName asc\");\n    query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n    query.setResult(\"partitionName\");\n    if (max > 0) {\n      query.setRange(0, max);\n    }\n    Collection<String> names = (Collection<String>) query.execute(dbName, tableName);\n    pns.addAll(names);\n\n    if (query != null) {\n      query.closeAll();\n    }\n    return pns;\n  }"
        ],
        [
            "ObjectStore::getCurrentNotificationEventId()",
            "8848  \n8849  \n8850  \n8851  \n8852  \n8853  \n8854  \n8855  \n8856  \n8857 -\n8858  \n8859  \n8860  \n8861  \n8862  \n8863  \n8864  \n8865  ",
            "  @Override\n  public CurrentNotificationEventId getCurrentNotificationEventId() {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) query.execute();\n      long id = 0;\n      if (ids != null && ids.size() > 0) {\n        id = ids.iterator().next().getNextEventId() - 1;\n      }\n      commited = commitTransaction();\n      return new CurrentNotificationEventId(id);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }",
            "8823  \n8824  \n8825  \n8826  \n8827  \n8828  \n8829  \n8830  \n8831  \n8832 +\n8833  \n8834  \n8835  \n8836  \n8837  \n8838  \n8839  \n8840  ",
            "  @Override\n  public CurrentNotificationEventId getCurrentNotificationEventId() {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) query.execute();\n      long id = 0;\n      if (CollectionUtils.isNotEmpty(ids)) {\n        id = ids.iterator().next().getNextEventId() - 1;\n      }\n      commited = commitTransaction();\n      return new CurrentNotificationEventId(id);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ],
        [
            "ObjectStore::getPartitionNamesByFilter(String,String,String,boolean,long)",
            "2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528 -\n2529 -\n2530 -\n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541 -\n2542 -\n2543 -\n2544 -\n2545 -\n2546  \n2547  \n2548  \n2549 -\n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  ",
            "  private List<String> getPartitionNamesByFilter(String dbName, String tableName,\n                                                 String filter, boolean ascending, long maxParts)\n      throws MetaException {\n\n    boolean success = false;\n    List<String> partNames = new ArrayList<String>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing getPartitionNamesByFilter\");\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n\n      MTable mtable = getMTable(dbName, tableName);\n      if( mtable == null ) {\n        // To be consistent with the behavior of listPartitionNames, if the\n        // table or db does not exist, we return an empty list\n        return partNames;\n      }\n      Map<String, Object> params = new HashMap<String, Object>();\n      String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);\n      Query query = pm.newQuery(\n          \"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n              + \"where \" + queryFilterString);\n\n      if (maxParts >= 0) {\n        //User specified a row limit, set it on the Query\n        query.setRange(0, maxParts);\n      }\n\n      LOG.debug(\"Filter specified is \" + filter + \",\" +\n          \" JDOQL filter is \" + queryFilterString);\n      LOG.debug(\"Parms is \" + params);\n\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      if (ascending) {\n        query.setOrdering(\"partitionName ascending\");\n      } else {\n        query.setOrdering(\"partitionName descending\");\n      }\n      query.setResult(\"partitionName\");\n\n      Collection names = (Collection) query.executeWithMap(params);\n      partNames = new ArrayList<String>();\n      for (Iterator i = names.iterator(); i.hasNext();) {\n        partNames.add((String) i.next());\n      }\n\n      LOG.debug(\"Done executing query for getPartitionNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for getPartitionNamesByFilter, size:\" + partNames.size());\n      query.closeAll();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return partNames;\n  }",
            "2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515 +\n2516 +\n2517 +\n2518 +\n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529 +\n2530 +\n2531  \n2532  \n2533  \n2534 +\n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  ",
            "  private List<String> getPartitionNamesByFilter(String dbName, String tableName,\n                                                 String filter, boolean ascending, long maxParts)\n      throws MetaException {\n\n    boolean success = false;\n    List<String> partNames = new ArrayList<String>();\n    try {\n      openTransaction();\n      LOG.debug(\"Executing getPartitionNamesByFilter\");\n      dbName = dbName.toLowerCase();\n      tableName = tableName.toLowerCase();\n\n      MTable mtable = getMTable(dbName, tableName);\n      if( mtable == null ) {\n        // To be consistent with the behavior of listPartitionNames, if the\n        // table or db does not exist, we return an empty list\n        return partNames;\n      }\n      Map<String, Object> params = new HashMap<String, Object>();\n      String queryFilterString = makeQueryFilterString(dbName, mtable, filter, params);\n      Query query = pm.newQuery(\n          \"select partitionName from org.apache.hadoop.hive.metastore.model.MPartition \"\n              + \"where \" + queryFilterString);\n\n      if (maxParts >= 0) {\n        //User specified a row limit, set it on the Query\n        query.setRange(0, maxParts);\n      }\n\n      LOG.debug(\"Filter specified is {}, JDOQL filter is {}\", filter,\n        queryFilterString);\n\n      LOG.debug(\"Parms is {}\", params);\n\n      String parameterDeclaration = makeParameterDeclarationStringObj(params);\n      query.declareParameters(parameterDeclaration);\n      if (ascending) {\n        query.setOrdering(\"partitionName ascending\");\n      } else {\n        query.setOrdering(\"partitionName descending\");\n      }\n      query.setResult(\"partitionName\");\n\n      Collection<String> names = (Collection<String>) query.executeWithMap(params);\n      partNames = new ArrayList<String>(names);\n\n      LOG.debug(\"Done executing query for getPartitionNamesByFilter\");\n      success = commitTransaction();\n      LOG.debug(\"Done retrieving all objects for getPartitionNamesByFilter, size: {}\", partNames.size());\n      query.closeAll();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return partNames;\n  }"
        ],
        [
            "ObjectStore::dropTable(String,String)",
            "1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193 -\n1194  \n1195  \n1196  \n1197  \n1198 -\n1199  \n1200  \n1201  \n1202  \n1203 -\n1204  \n1205  \n1206  \n1207  \n1208  \n1209 -\n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216 -\n1217 -\n1218  \n1219  \n1220  \n1221  \n1222 -\n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  ",
            "  @Override\n  public boolean dropTable(String dbName, String tableName) throws MetaException,\n    NoSuchObjectException, InvalidObjectException, InvalidInputException {\n    boolean success = false;\n    try {\n      openTransaction();\n      MTable tbl = getMTable(dbName, tableName);\n      pm.retrieve(tbl);\n      if (tbl != null) {\n        // first remove all the grants\n        List<MTablePrivilege> tabGrants = listAllTableGrants(dbName, tableName);\n        if (tabGrants != null && tabGrants.size() > 0) {\n          pm.deletePersistentAll(tabGrants);\n        }\n        List<MTableColumnPrivilege> tblColGrants = listTableAllColumnGrants(dbName,\n            tableName);\n        if (tblColGrants != null && tblColGrants.size() > 0) {\n          pm.deletePersistentAll(tblColGrants);\n        }\n\n        List<MPartitionPrivilege> partGrants = this.listTableAllPartitionGrants(dbName, tableName);\n        if (partGrants != null && partGrants.size() > 0) {\n          pm.deletePersistentAll(partGrants);\n        }\n\n        List<MPartitionColumnPrivilege> partColGrants = listTableAllPartitionColumnGrants(dbName,\n            tableName);\n        if (partColGrants != null && partColGrants.size() > 0) {\n          pm.deletePersistentAll(partColGrants);\n        }\n        // delete column statistics if present\n        try {\n          deleteTableColumnStatistics(dbName, tableName, null);\n        } catch (NoSuchObjectException e) {\n          LOG.info(\"Found no table level column statistics associated with db \" + dbName +\n          \" table \" + tableName + \" record to delete\");\n        }\n\n        List<MConstraint> tabConstraints = listAllTableConstraintsWithOptionalConstraintName(\n                                           dbName, tableName, null);\n        if (tabConstraints != null && tabConstraints.size() > 0) {\n          pm.deletePersistentAll(tabConstraints);\n        }\n\n        preDropStorageDescriptor(tbl.getSd());\n        // then remove the table\n        pm.deletePersistentAll(tbl);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }",
            "1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185 +\n1186  \n1187  \n1188  \n1189  \n1190 +\n1191  \n1192  \n1193  \n1194  \n1195 +\n1196  \n1197  \n1198  \n1199  \n1200  \n1201 +\n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208 +\n1209 +\n1210  \n1211  \n1212  \n1213  \n1214 +\n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  ",
            "  @Override\n  public boolean dropTable(String dbName, String tableName) throws MetaException,\n    NoSuchObjectException, InvalidObjectException, InvalidInputException {\n    boolean success = false;\n    try {\n      openTransaction();\n      MTable tbl = getMTable(dbName, tableName);\n      pm.retrieve(tbl);\n      if (tbl != null) {\n        // first remove all the grants\n        List<MTablePrivilege> tabGrants = listAllTableGrants(dbName, tableName);\n        if (CollectionUtils.isNotEmpty(tabGrants)) {\n          pm.deletePersistentAll(tabGrants);\n        }\n        List<MTableColumnPrivilege> tblColGrants = listTableAllColumnGrants(dbName,\n            tableName);\n        if (CollectionUtils.isNotEmpty(tblColGrants)) {\n          pm.deletePersistentAll(tblColGrants);\n        }\n\n        List<MPartitionPrivilege> partGrants = this.listTableAllPartitionGrants(dbName, tableName);\n        if (CollectionUtils.isNotEmpty(partGrants)) {\n          pm.deletePersistentAll(partGrants);\n        }\n\n        List<MPartitionColumnPrivilege> partColGrants = listTableAllPartitionColumnGrants(dbName,\n            tableName);\n        if (CollectionUtils.isNotEmpty(partColGrants)) {\n          pm.deletePersistentAll(partColGrants);\n        }\n        // delete column statistics if present\n        try {\n          deleteTableColumnStatistics(dbName, tableName, null);\n        } catch (NoSuchObjectException e) {\n          LOG.info(\"Found no table level column statistics associated with db {}\" +\n          \" table {} record to delete\", dbName, tableName);\n        }\n\n        List<MConstraint> tabConstraints = listAllTableConstraintsWithOptionalConstraintName(\n                                           dbName, tableName, null);\n        if (CollectionUtils.isNotEmpty(tabConstraints)) {\n          pm.deletePersistentAll(tabConstraints);\n        }\n\n        preDropStorageDescriptor(tbl.getSd());\n        // then remove the table\n        pm.deletePersistentAll(tbl);\n      }\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return success;\n  }"
        ],
        [
            "ObjectStore::addMasterKey(String)",
            "8198  \n8199  \n8200  \n8201  \n8202  \n8203  \n8204  \n8205  \n8206  \n8207  \n8208  \n8209  \n8210  \n8211  \n8212 -\n8213  \n8214  \n8215  \n8216  \n8217  \n8218  ",
            "  @Override\n  public int addMasterKey(String key) throws MetaException{\n    LOG.debug(\"Begin executing addMasterKey\");\n    boolean committed = false;\n    MMasterKey masterKey = new MMasterKey(key);\n    try{\n      openTransaction();\n      pm.makePersistent(masterKey);\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing addMasterKey with status : \" + committed);\n    if (committed) {\n      return ((IntIdentity)pm.getObjectId(masterKey)).getKey();\n    } else {\n      throw new MetaException(\"Failed to add master key.\");\n    }\n  }",
            "8174  \n8175  \n8176  \n8177  \n8178  \n8179  \n8180  \n8181  \n8182  \n8183  \n8184  \n8185  \n8186  \n8187  \n8188 +\n8189  \n8190  \n8191  \n8192  \n8193  \n8194  ",
            "  @Override\n  public int addMasterKey(String key) throws MetaException{\n    LOG.debug(\"Begin executing addMasterKey\");\n    boolean committed = false;\n    MMasterKey masterKey = new MMasterKey(key);\n    try{\n      openTransaction();\n      pm.makePersistent(masterKey);\n      committed = commitTransaction();\n    } finally {\n      if(!committed) {\n        rollbackTransaction();\n      }\n    }\n    LOG.debug(\"Done executing addMasterKey with status : {}\", committed);\n    if (committed) {\n      return ((IntIdentity)pm.getObjectId(masterKey)).getKey();\n    } else {\n      throw new MetaException(\"Failed to add master key.\");\n    }\n  }"
        ],
        [
            "ObjectStore::checkSchema()",
            "8309  \n8310  \n8311  \n8312  \n8313  \n8314  \n8315  \n8316  \n8317  \n8318  \n8319  \n8320  \n8321  \n8322  \n8323  \n8324 -\n8325  \n8326 -\n8327 -\n8328 -\n8329  \n8330  \n8331  \n8332  \n8333  \n8334  \n8335 -\n8336  \n8337  \n8338  \n8339  \n8340  \n8341  \n8342  \n8343 -\n8344 -\n8345 -\n8346  \n8347  \n8348  \n8349  \n8350  \n8351  \n8352  \n8353  ",
            "  private synchronized void checkSchema() throws MetaException {\n    // recheck if it got verified by another thread while we were waiting\n    if (isSchemaVerified.get()) {\n      return;\n    }\n\n    boolean strictValidation = MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION);\n    // read the schema version stored in metastore db\n    String dbSchemaVer = getMetaStoreSchemaVersion();\n    // version of schema for this version of hive\n    IMetaStoreSchemaInfo metastoreSchemaInfo = MetaStoreSchemaInfoFactory.get(getConf());\n    String hiveSchemaVer = metastoreSchemaInfo.getHiveSchemaVersion();\n\n    if (dbSchemaVer == null) {\n      if (strictValidation) {\n        throw new MetaException(\"Version information not found in metastore. \");\n      } else {\n        LOG.warn(\"Version information not found in metastore. \"\n            + ConfVars.SCHEMA_VERIFICATION.toString() +\n            \" is not enabled so recording the schema version \" +\n            hiveSchemaVer);\n        setMetaStoreSchemaVersion(hiveSchemaVer,\n          \"Set by MetaStore \" + USER + \"@\" + HOSTNAME);\n      }\n    } else {\n      if (metastoreSchemaInfo.isVersionCompatible(hiveSchemaVer, dbSchemaVer)) {\n        LOG.debug(\"Found expected HMS version of \" + dbSchemaVer);\n      } else {\n        // metastore schema version is different than Hive distribution needs\n        if (strictValidation) {\n          throw new MetaException(\"Hive Schema version \" + hiveSchemaVer +\n              \" does not match metastore's schema version \" + dbSchemaVer +\n              \" Metastore is not upgraded or corrupt\");\n        } else {\n          LOG.error(\"Version information found in metastore differs \" + dbSchemaVer +\n              \" from expected schema version \" + hiveSchemaVer +\n              \". Schema verififcation is disabled \" + ConfVars.SCHEMA_VERIFICATION);\n          setMetaStoreSchemaVersion(hiveSchemaVer,\n            \"Set by MetaStore \" + USER + \"@\" + HOSTNAME);\n        }\n      }\n    }\n    isSchemaVerified.set(true);\n    return;\n  }",
            "8285  \n8286  \n8287  \n8288  \n8289  \n8290  \n8291  \n8292  \n8293  \n8294  \n8295  \n8296  \n8297  \n8298  \n8299  \n8300 +\n8301  \n8302 +\n8303 +\n8304  \n8305  \n8306  \n8307  \n8308  \n8309  \n8310 +\n8311  \n8312  \n8313  \n8314  \n8315  \n8316  \n8317  \n8318 +\n8319 +\n8320 +\n8321  \n8322  \n8323  \n8324  \n8325  \n8326  \n8327  \n8328  ",
            "  private synchronized void checkSchema() throws MetaException {\n    // recheck if it got verified by another thread while we were waiting\n    if (isSchemaVerified.get()) {\n      return;\n    }\n\n    boolean strictValidation = MetastoreConf.getBoolVar(getConf(), ConfVars.SCHEMA_VERIFICATION);\n    // read the schema version stored in metastore db\n    String dbSchemaVer = getMetaStoreSchemaVersion();\n    // version of schema for this version of hive\n    IMetaStoreSchemaInfo metastoreSchemaInfo = MetaStoreSchemaInfoFactory.get(getConf());\n    String hiveSchemaVer = metastoreSchemaInfo.getHiveSchemaVersion();\n\n    if (dbSchemaVer == null) {\n      if (strictValidation) {\n        throw new MetaException(\"Version information not found in metastore.\");\n      } else {\n        LOG.warn(\"Version information not found in metastore. {} is not \" +\n          \"enabled so recording the schema version {}\", ConfVars.SCHEMA_VERIFICATION,\n            hiveSchemaVer);\n        setMetaStoreSchemaVersion(hiveSchemaVer,\n          \"Set by MetaStore \" + USER + \"@\" + HOSTNAME);\n      }\n    } else {\n      if (metastoreSchemaInfo.isVersionCompatible(hiveSchemaVer, dbSchemaVer)) {\n        LOG.debug(\"Found expected HMS version of {}\", dbSchemaVer);\n      } else {\n        // metastore schema version is different than Hive distribution needs\n        if (strictValidation) {\n          throw new MetaException(\"Hive Schema version \" + hiveSchemaVer +\n              \" does not match metastore's schema version \" + dbSchemaVer +\n              \" Metastore is not upgraded or corrupt\");\n        } else {\n          LOG.error(\"Version information found in metastore differs {} \" +\n              \"from expected schema version {}. Schema verififcation is disabled {}\", \n              dbSchemaVer, hiveSchemaVer, ConfVars.SCHEMA_VERIFICATION);\n          setMetaStoreSchemaVersion(hiveSchemaVer,\n            \"Set by MetaStore \" + USER + \"@\" + HOSTNAME);\n        }\n      }\n    }\n    isSchemaVerified.set(true);\n    return;\n  }"
        ],
        [
            "ObjectStore::getTableObjectsByName(String,List)",
            "1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518 -\n1519  \n1520  \n1521  \n1522 -\n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  ",
            "  @Override\n  public List<Table> getTableObjectsByName(String db, List<String> tbl_names) throws MetaException,\n      UnknownDBException {\n    List<Table> tables = new ArrayList<>();\n    boolean committed = false;\n    Query dbExistsQuery = null;\n    Query query = null;\n    try {\n      openTransaction();\n      db = normalizeIdentifier(db);\n      dbExistsQuery = pm.newQuery(MDatabase.class, \"name == db\");\n      dbExistsQuery.declareParameters(\"java.lang.String db\");\n      dbExistsQuery.setUnique(true);\n      dbExistsQuery.setResult(\"name\");\n      String dbNameIfExists = (String) dbExistsQuery.execute(db);\n      if (dbNameIfExists == null || dbNameIfExists.isEmpty()) {\n        throw new UnknownDBException(\"Could not find database \" + db);\n      }\n\n      List<String> lowered_tbl_names = new ArrayList<>();\n      for (String t : tbl_names) {\n        lowered_tbl_names.add(normalizeIdentifier(t));\n      }\n      query = pm.newQuery(MTable.class);\n      query.setFilter(\"database.name == db && tbl_names.contains(tableName)\");\n      query.declareParameters(\"java.lang.String db, java.util.Collection tbl_names\");\n      Collection mtables = (Collection) query.execute(db, lowered_tbl_names);\n      for (Iterator iter = mtables.iterator(); iter.hasNext();) {\n        tables.add(convertToTable((MTable) iter.next()));\n      }\n      committed = commitTransaction();\n    } finally {\n      rollbackAndCleanup(committed, query);\n      if (dbExistsQuery != null) {\n        dbExistsQuery.closeAll();\n      }\n    }\n    return tables;\n  }",
            "1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507 +\n1508  \n1509  \n1510  \n1511 +\n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  ",
            "  @Override\n  public List<Table> getTableObjectsByName(String db, List<String> tbl_names) throws MetaException,\n      UnknownDBException {\n    List<Table> tables = new ArrayList<>();\n    boolean committed = false;\n    Query dbExistsQuery = null;\n    Query query = null;\n    try {\n      openTransaction();\n      db = normalizeIdentifier(db);\n      dbExistsQuery = pm.newQuery(MDatabase.class, \"name == db\");\n      dbExistsQuery.declareParameters(\"java.lang.String db\");\n      dbExistsQuery.setUnique(true);\n      dbExistsQuery.setResult(\"name\");\n      String dbNameIfExists = (String) dbExistsQuery.execute(db);\n      if (org.apache.commons.lang.StringUtils.isEmpty(dbNameIfExists)) {\n        throw new UnknownDBException(\"Could not find database \" + db);\n      }\n\n      List<String> lowered_tbl_names = new ArrayList<>(tbl_names.size());\n      for (String t : tbl_names) {\n        lowered_tbl_names.add(normalizeIdentifier(t));\n      }\n      query = pm.newQuery(MTable.class);\n      query.setFilter(\"database.name == db && tbl_names.contains(tableName)\");\n      query.declareParameters(\"java.lang.String db, java.util.Collection tbl_names\");\n      Collection mtables = (Collection) query.execute(db, lowered_tbl_names);\n      for (Iterator iter = mtables.iterator(); iter.hasNext();) {\n        tables.add(convertToTable((MTable) iter.next()));\n      }\n      committed = commitTransaction();\n    } finally {\n      rollbackAndCleanup(committed, query);\n      if (dbExistsQuery != null) {\n        dbExistsQuery.closeAll();\n      }\n    }\n    return tables;\n  }"
        ],
        [
            "ObjectStore::getAllTokenIdentifiers()",
            "8174  \n8175  \n8176  \n8177  \n8178  \n8179  \n8180  \n8181  \n8182  \n8183  \n8184  \n8185  \n8186  \n8187  \n8188  \n8189  \n8190  \n8191  \n8192  \n8193 -\n8194  \n8195  \n8196  ",
            "  @Override\n  public List<String> getAllTokenIdentifiers() {\n    LOG.debug(\"Begin executing getAllTokenIdentifiers\");\n    boolean committed = false;\n    Query query = null;\n    List<String> tokenIdents = new ArrayList<>();\n\n    try {\n      openTransaction();\n      query = pm.newQuery(MDelegationToken.class);\n      List<MDelegationToken> tokens = (List<MDelegationToken>) query.execute();\n      pm.retrieveAll(tokens);\n      committed = commitTransaction();\n\n      for (MDelegationToken token : tokens) {\n        tokenIdents.add(token.getTokenIdentifier());\n      }\n      return tokenIdents;\n    } finally {\n      LOG.debug(\"Done executing getAllTokenIdentifers with status : \" + committed);\n      rollbackAndCleanup(committed, query);\n    }\n  }",
            "8150  \n8151  \n8152  \n8153  \n8154  \n8155  \n8156  \n8157  \n8158  \n8159  \n8160  \n8161  \n8162  \n8163  \n8164  \n8165  \n8166  \n8167  \n8168  \n8169 +\n8170  \n8171  \n8172  ",
            "  @Override\n  public List<String> getAllTokenIdentifiers() {\n    LOG.debug(\"Begin executing getAllTokenIdentifiers\");\n    boolean committed = false;\n    Query query = null;\n    List<String> tokenIdents = new ArrayList<>();\n\n    try {\n      openTransaction();\n      query = pm.newQuery(MDelegationToken.class);\n      List<MDelegationToken> tokens = (List<MDelegationToken>) query.execute();\n      pm.retrieveAll(tokens);\n      committed = commitTransaction();\n\n      for (MDelegationToken token : tokens) {\n        tokenIdents.add(token.getTokenIdentifier());\n      }\n      return tokenIdents;\n    } finally {\n      LOG.debug(\"Done executing getAllTokenIdentifers with status : {}\", committed);\n      rollbackAndCleanup(committed, query);\n    }\n  }"
        ]
    ],
    "21e18dea4fb3e6e9f9c70282eabdc04a0be569b2": [
        [
            "TransactionalValidationListener::handleAlterTableTransactionalProp(PreAlterTableEvent)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "  /**\n   * once a table is marked transactional, you cannot go back.  Enforce this.\n   * Also in current version, 'transactional_properties' of the table cannot be altered after\n   * the table is created. Any attempt to alter it will throw a MetaException.\n   */\n  private void handleAlterTableTransactionalProp(PreAlterTableEvent context) throws MetaException {\n    Table newTable = context.getNewTable();\n    Map<String, String> parameters = newTable.getParameters();\n    if (parameters == null || parameters.isEmpty()) {\n      return;\n    }\n    Set<String> keys = new HashSet<>(parameters.keySet());\n    String transactionalValue = null;\n    boolean transactionalValuePresent = false;\n    boolean isTransactionalPropertiesPresent = false;\n    String transactionalPropertiesValue = null;\n    boolean hasValidTransactionalValue = false;\n\n    for (String key : keys) {\n      if(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        transactionalValuePresent = true;\n        transactionalValue = parameters.get(key);\n        parameters.remove(key);\n      }\n      if(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        isTransactionalPropertiesPresent = true;\n        transactionalPropertiesValue = parameters.get(key);\n        // Do not remove the parameter yet, because we have separate initialization routine\n        // that will use it down below.\n      }\n    }\n    Table oldTable = context.getOldTable();\n    String oldTransactionalValue = null;\n    String oldTransactionalPropertiesValue = null;\n    for (String key : oldTable.getParameters().keySet()) {\n      if (hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        oldTransactionalValue = oldTable.getParameters().get(key);\n      }\n      if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        oldTransactionalPropertiesValue = oldTable.getParameters().get(key);\n      }\n    }\n\n    if (transactionalValuePresent && \"false\".equalsIgnoreCase(transactionalValue)) {\n      transactionalValuePresent = false;\n      transactionalValue = null;\n    }\n\n    if (transactionalValuePresent) {\n      //normalize prop name\n      parameters.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, transactionalValue);\n    }\n    if (\"true\".equalsIgnoreCase(transactionalValue) && !\"true\".equalsIgnoreCase(oldTransactionalValue)) {\n      if(!isTransactionalPropertiesPresent) {\n        normazlieTransactionalPropertyDefault(newTable);\n        isTransactionalPropertiesPresent = true;\n        transactionalPropertiesValue = DEFAULT_TRANSACTIONAL_PROPERTY;\n      }\n      //only need to check conformance if alter table enabled acid\n      if (!conformToAcid(newTable)) {\n        // INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing\n        if (transactionalPropertiesValue == null || !\"insert_only\".equalsIgnoreCase(transactionalPropertiesValue)) {\n          throw new MetaException(\"The table must be stored using an ACID compliant format (such as ORC)\");\n        }\n      }\n\n      if (newTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        throw new MetaException(Warehouse.getQualifiedName(newTable) +\n            \" cannot be declared transactional because it's an external table\");\n      }\n      validateTableStructure(context.getHandler(), newTable);\n      hasValidTransactionalValue = true;\n    }\n\n\n\n    if (oldTransactionalValue == null ? transactionalValue == null\n                                     : oldTransactionalValue.equalsIgnoreCase(transactionalValue)) {\n      //this covers backward compat cases where this prop may have been set already\n      hasValidTransactionalValue = true;\n    }\n\n    if (!hasValidTransactionalValue && !MetaStoreUtils.isInsertOnlyTableParam(oldTable.getParameters())) {\n      // if here, there is attempt to set transactional to something other than 'true'\n      // and NOT the same value it was before\n      throw new MetaException(\"TBLPROPERTIES with 'transactional'='true' cannot be unset\");\n    }\n\n    if (isTransactionalPropertiesPresent) {\n      // Now validate transactional_properties for the table.\n      if (oldTransactionalValue == null) {\n        // If this is the first time the table is being initialized to 'transactional=true',\n        // any valid value can be set for the 'transactional_properties'.\n        initializeTransactionalProperties(newTable);\n      } else {\n        // If the table was already marked as 'transactional=true', then the new value of\n        // 'transactional_properties' must match the old value. Any attempt to alter the previous\n        // value will throw an error. An exception will still be thrown if the previous value was\n        // null and an attempt is made to set it. This behaviour can be changed in the future.\n        if ((oldTransactionalPropertiesValue == null\n            || !oldTransactionalPropertiesValue.equalsIgnoreCase(transactionalPropertiesValue))\n            && !MetaStoreUtils.isInsertOnlyTableParam(oldTable.getParameters())) {\n          throw new MetaException(\"TBLPROPERTIES with 'transactional_properties' cannot be \"\n              + \"altered after the table is created\");\n        }\n      }\n    }\n    checkSorted(newTable);\n  }",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165 +\n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "  /**\n   * once a table is marked transactional, you cannot go back.  Enforce this.\n   * Also in current version, 'transactional_properties' of the table cannot be altered after\n   * the table is created. Any attempt to alter it will throw a MetaException.\n   */\n  private void handleAlterTableTransactionalProp(PreAlterTableEvent context) throws MetaException {\n    Table newTable = context.getNewTable();\n    Map<String, String> parameters = newTable.getParameters();\n    if (parameters == null || parameters.isEmpty()) {\n      return;\n    }\n    Set<String> keys = new HashSet<>(parameters.keySet());\n    String transactionalValue = null;\n    boolean transactionalValuePresent = false;\n    boolean isTransactionalPropertiesPresent = false;\n    String transactionalPropertiesValue = null;\n    boolean hasValidTransactionalValue = false;\n\n    for (String key : keys) {\n      if(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        transactionalValuePresent = true;\n        transactionalValue = parameters.get(key);\n        parameters.remove(key);\n      }\n      if(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        isTransactionalPropertiesPresent = true;\n        transactionalPropertiesValue = parameters.get(key);\n        // Do not remove the parameter yet, because we have separate initialization routine\n        // that will use it down below.\n      }\n    }\n    Table oldTable = context.getOldTable();\n    String oldTransactionalValue = null;\n    String oldTransactionalPropertiesValue = null;\n    for (String key : oldTable.getParameters().keySet()) {\n      if (hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        oldTransactionalValue = oldTable.getParameters().get(key);\n      }\n      if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        oldTransactionalPropertiesValue = oldTable.getParameters().get(key);\n      }\n    }\n\n    if (transactionalValuePresent && \"false\".equalsIgnoreCase(transactionalValue)) {\n      transactionalValuePresent = false;\n      transactionalValue = null;\n    }\n\n    if (transactionalValuePresent) {\n      //normalize prop name\n      parameters.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, transactionalValue);\n    }\n    if (\"true\".equalsIgnoreCase(transactionalValue) && !\"true\".equalsIgnoreCase(oldTransactionalValue)) {\n      if(!isTransactionalPropertiesPresent) {\n        normazlieTransactionalPropertyDefault(newTable);\n        isTransactionalPropertiesPresent = true;\n        transactionalPropertiesValue = DEFAULT_TRANSACTIONAL_PROPERTY;\n      }\n      //only need to check conformance if alter table enabled acid\n      if (!conformToAcid(newTable)) {\n        // INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing\n        if (transactionalPropertiesValue == null || !\"insert_only\".equalsIgnoreCase(transactionalPropertiesValue)) {\n          throw new MetaException(\"The table must be stored using an ACID compliant format (such as ORC): \"\n          + Warehouse.getQualifiedName(newTable));\n        }\n      }\n\n      if (newTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        throw new MetaException(Warehouse.getQualifiedName(newTable) +\n            \" cannot be declared transactional because it's an external table\");\n      }\n      validateTableStructure(context.getHandler(), newTable);\n      hasValidTransactionalValue = true;\n    }\n\n\n\n    if (oldTransactionalValue == null ? transactionalValue == null\n                                     : oldTransactionalValue.equalsIgnoreCase(transactionalValue)) {\n      //this covers backward compat cases where this prop may have been set already\n      hasValidTransactionalValue = true;\n    }\n\n    if (!hasValidTransactionalValue && !MetaStoreUtils.isInsertOnlyTableParam(oldTable.getParameters())) {\n      // if here, there is attempt to set transactional to something other than 'true'\n      // and NOT the same value it was before\n      throw new MetaException(\"TBLPROPERTIES with 'transactional'='true' cannot be unset: \"\n          + Warehouse.getQualifiedName(newTable));\n    }\n\n    if (isTransactionalPropertiesPresent) {\n      // Now validate transactional_properties for the table.\n      if (oldTransactionalValue == null) {\n        // If this is the first time the table is being initialized to 'transactional=true',\n        // any valid value can be set for the 'transactional_properties'.\n        initializeTransactionalProperties(newTable);\n      } else {\n        // If the table was already marked as 'transactional=true', then the new value of\n        // 'transactional_properties' must match the old value. Any attempt to alter the previous\n        // value will throw an error. An exception will still be thrown if the previous value was\n        // null and an attempt is made to set it. This behaviour can be changed in the future.\n        if ((oldTransactionalPropertiesValue == null\n            || !oldTransactionalPropertiesValue.equalsIgnoreCase(transactionalPropertiesValue))\n            && !MetaStoreUtils.isInsertOnlyTableParam(oldTable.getParameters())) {\n          throw new MetaException(\"TBLPROPERTIES with 'transactional_properties' cannot be \"\n              + \"altered after the table is created\");\n        }\n      }\n    }\n    checkSorted(newTable);\n  }"
        ],
        [
            "TransactionalValidationListener::conformToAcid(Table)",
            " 273  \n 274  \n 275  \n 276  \n 277 -\n 278  \n 279  \n 280 -\n 281 -\n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289 -\n 290  \n 291  \n 292  \n 293  ",
            "  /**\n   * Check that InputFormatClass/OutputFormatClass should implement\n   * AcidInputFormat/AcidOutputFormat\n   */\n  private boolean conformToAcid(Table table) throws MetaException {\n    StorageDescriptor sd = table.getSd();\n    try {\n      Class inputFormatClass = Class.forName(sd.getInputFormat());\n      Class outputFormatClass = Class.forName(sd.getOutputFormat());\n\n      if (inputFormatClass == null || outputFormatClass == null ||\n          !Class.forName(\"org.apache.hadoop.hive.ql.io.AcidInputFormat\").isAssignableFrom(inputFormatClass) ||\n          !Class.forName(\"org.apache.hadoop.hive.ql.io.AcidOutputFormat\").isAssignableFrom(outputFormatClass)) {\n        return false;\n      }\n    } catch (ClassNotFoundException e) {\n      throw new MetaException(\"Invalid input/output format for table\");\n    }\n\n    return true;\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282 +\n 283  \n 284  \n 285 +\n 286 +\n 287 +\n 288 +\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 +\n 297 +\n 298 +\n 299  \n 300  \n 301  \n 302  ",
            "  /**\n   * Check that InputFormatClass/OutputFormatClass should implement\n   * AcidInputFormat/AcidOutputFormat\n   */\n  public static boolean conformToAcid(Table table) throws MetaException {\n    StorageDescriptor sd = table.getSd();\n    try {\n      Class inputFormatClass = sd.getInputFormat() == null ? null :\n          Class.forName(sd.getInputFormat());\n      Class outputFormatClass = sd.getOutputFormat() == null ? null :\n          Class.forName(sd.getOutputFormat());\n\n      if (inputFormatClass == null || outputFormatClass == null ||\n          !Class.forName(\"org.apache.hadoop.hive.ql.io.AcidInputFormat\").isAssignableFrom(inputFormatClass) ||\n          !Class.forName(\"org.apache.hadoop.hive.ql.io.AcidOutputFormat\").isAssignableFrom(outputFormatClass)) {\n        return false;\n      }\n    } catch (ClassNotFoundException e) {\n      LOG.warn(\"Could not verify InputFormat=\" + sd.getInputFormat() + \" or OutputFormat=\" +\n          sd.getOutputFormat() + \"  for \" + Warehouse.getQualifiedName(table));\n      return false;\n    }\n\n    return true;\n  }"
        ],
        [
            "TransactionalValidationListener::initializeTransactionalProperties(Table)",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314 -\n 315 -\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  ",
            "  private void initializeTransactionalProperties(Table table) throws MetaException {\n    // All new versions of Acid tables created after the introduction of Acid version/type system\n    // can have TRANSACTIONAL_PROPERTIES property defined. This parameter can be used to change\n    // the operational behavior of ACID. However if this parameter is not defined, the new Acid\n    // tables will still behave as the old ones. This is done so to preserve the behavior\n    // in case of rolling downgrade.\n\n    // Initialize transaction table properties with default string value.\n    String tableTransactionalProperties = null;\n\n    Map<String, String> parameters = table.getParameters();\n    if (parameters != null) {\n      Set<String> keys = parameters.keySet();\n      for (String key : keys) {\n        if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n          tableTransactionalProperties = parameters.get(key).toLowerCase();\n          parameters.remove(key);\n          String validationError = validateTransactionalProperties(tableTransactionalProperties);\n          if (validationError != null) {\n            throw new MetaException(\"Invalid transactional properties specified for the \"\n                + \"table with the error \" + validationError);\n          }\n          break;\n        }\n      }\n    }\n\n    if (tableTransactionalProperties != null) {\n      parameters.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n              tableTransactionalProperties);\n    }\n  }",
            " 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324 +\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "  private void initializeTransactionalProperties(Table table) throws MetaException {\n    // All new versions of Acid tables created after the introduction of Acid version/type system\n    // can have TRANSACTIONAL_PROPERTIES property defined. This parameter can be used to change\n    // the operational behavior of ACID. However if this parameter is not defined, the new Acid\n    // tables will still behave as the old ones. This is done so to preserve the behavior\n    // in case of rolling downgrade.\n\n    // Initialize transaction table properties with default string value.\n    String tableTransactionalProperties = null;\n\n    Map<String, String> parameters = table.getParameters();\n    if (parameters != null) {\n      Set<String> keys = parameters.keySet();\n      for (String key : keys) {\n        if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n          tableTransactionalProperties = parameters.get(key).toLowerCase();\n          parameters.remove(key);\n          String validationError = validateTransactionalProperties(tableTransactionalProperties);\n          if (validationError != null) {\n            throw new MetaException(\"Invalid transactional properties specified for \"\n                + Warehouse.getQualifiedName(table) + \" with the error \" + validationError);\n          }\n          break;\n        }\n      }\n    }\n\n    if (tableTransactionalProperties != null) {\n      parameters.put(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES,\n              tableTransactionalProperties);\n    }\n  }"
        ],
        [
            "TestHiveMetaStore::testTransactionalValidation()",
            "2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002 -\n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012 -\n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022 -\n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035 -\n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055 -\n3056  \n3057  \n3058  \n3059  \n3060 -\n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075 -\n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084  \n3085  \n3086  \n3087  \n3088  \n3089  \n3090  ",
            "  @Test\n  public void testTransactionalValidation() throws Throwable {\n    String dbName = \"acidDb\";\n    silentDropDatabase(dbName);\n    Database db = new Database();\n    db.setName(dbName);\n    client.createDatabase(db);\n    String tblName = \"acidTable\";\n    String owner = \"acid\";\n    Map<String, String> fields = new HashMap<String, String>();\n    fields.put(\"name\", serdeConstants.STRING_TYPE_NAME);\n    fields.put(\"income\", serdeConstants.INT_TYPE_NAME);\n\n    Type type = createType(\"Person\", fields);\n\n    Map<String, String> params = new HashMap<String, String>();\n    params.put(\"transactional\", \"\");\n\n    Map<String, String> serdParams = new HashMap<String, String>();\n    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, \"1\");\n    StorageDescriptor sd =  createStorageDescriptor(tblName, type.getFields(), params, serdParams);\n    sd.setNumBuckets(0);\n    sd.unsetBucketCols();\n\n    /// CREATE TABLE scenarios\n\n    // Fail - No \"transactional\" property is specified\n    try {\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"'transactional' property of TBLPROPERTIES may only have value 'true'\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" property is set to an invalid value\n    try {\n      params.clear();\n      params.put(\"transactional\", \"foobar\");\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"'transactional' property of TBLPROPERTIES may only have value 'true'\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" is set to true, but the table is not bucketed\n    try {\n      params.clear();\n      params.put(\"transactional\", \"true\");\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC)\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" is set to true, and the table is bucketed, but doesn't use ORC\n    try {\n      params.clear();\n      params.put(\"transactional\", \"true\");\n      List<String> bucketCols = new ArrayList<String>();\n      bucketCols.add(\"income\");\n      sd.setBucketCols(bucketCols);\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC)\", e.getMessage());\n    }\n\n    // Succeed - \"transactional\" is set to true, and the table is bucketed, and uses ORC\n    params.clear();\n    params.put(\"transactional\", \"true\");\n    List<String> bucketCols = new ArrayList<String>();\n    bucketCols.add(\"income\");\n    sd.setBucketCols(bucketCols);\n    sd.setInputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\");\n    sd.setOutputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\");\n    Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n    Assert.assertTrue(\"CREATE TABLE should succeed\", \"true\".equals(t.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)));\n\n    /// ALTER TABLE scenarios\n\n    // Fail - trying to set \"transactional\" to \"false\" is not allowed\n    try {\n      params.clear();\n      params.put(\"transactional\", \"false\");\n      t = new Table();\n      t.setParameters(params);\n      client.alter_table(dbName, tblName, t);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"TBLPROPERTIES with 'transactional'='true' cannot be unset\", e.getMessage());\n    }\n\n    // Fail - trying to set \"transactional\" to \"true\" but doesn't satisfy bucketing and Input/OutputFormat requirement\n    try {\n      tblName += \"1\";\n      params.clear();\n      sd.unsetBucketCols();\n      sd.setInputFormat(\"org.apache.hadoop.mapred.FileInputFormat\");\n      t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      params.put(\"transactional\", \"true\");\n      t.setParameters(params);\n      client.alter_table(dbName, tblName, t);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC)\", e.getMessage());\n    }\n\n    // Succeed - trying to set \"transactional\" to \"true\", and satisfies bucketing and Input/OutputFormat requirement\n    tblName += \"2\";\n    params.clear();\n    sd.setNumBuckets(1);\n    sd.setBucketCols(bucketCols);\n    sd.setInputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\");\n    t = createTable(dbName, tblName, owner, params, null, sd, 0);\n    params.put(\"transactional\", \"true\");\n    t.setParameters(params);\n    t.setPartitionKeys(Collections.EMPTY_LIST);\n    client.alter_table(dbName, tblName, t);\n    Assert.assertTrue(\"ALTER TABLE should succeed\", \"true\".equals(t.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)));\n  }",
            "2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002 +\n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012 +\n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022 +\n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035 +\n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059 +\n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074 +\n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084  \n3085  \n3086  \n3087  \n3088  \n3089  ",
            "  @Test\n  public void testTransactionalValidation() throws Throwable {\n    String dbName = \"acidDb\";\n    silentDropDatabase(dbName);\n    Database db = new Database();\n    db.setName(dbName);\n    client.createDatabase(db);\n    String tblName = \"acidTable\";\n    String owner = \"acid\";\n    Map<String, String> fields = new HashMap<String, String>();\n    fields.put(\"name\", serdeConstants.STRING_TYPE_NAME);\n    fields.put(\"income\", serdeConstants.INT_TYPE_NAME);\n\n    Type type = createType(\"Person\", fields);\n\n    Map<String, String> params = new HashMap<String, String>();\n    params.put(\"transactional\", \"\");\n\n    Map<String, String> serdParams = new HashMap<String, String>();\n    serdParams.put(serdeConstants.SERIALIZATION_FORMAT, \"1\");\n    StorageDescriptor sd =  createStorageDescriptor(tblName, type.getFields(), params, serdParams);\n    sd.setNumBuckets(0);\n    sd.unsetBucketCols();\n\n    /// CREATE TABLE scenarios\n\n    // Fail - No \"transactional\" property is specified\n    try {\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"'transactional' property of TBLPROPERTIES may only have value 'true': acidDb.acidTable\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" property is set to an invalid value\n    try {\n      params.clear();\n      params.put(\"transactional\", \"foobar\");\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"'transactional' property of TBLPROPERTIES may only have value 'true': acidDb.acidTable\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" is set to true, but the table is not bucketed\n    try {\n      params.clear();\n      params.put(\"transactional\", \"true\");\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC): acidDb.acidTable\", e.getMessage());\n    }\n\n    // Fail - \"transactional\" is set to true, and the table is bucketed, but doesn't use ORC\n    try {\n      params.clear();\n      params.put(\"transactional\", \"true\");\n      List<String> bucketCols = new ArrayList<String>();\n      bucketCols.add(\"income\");\n      sd.setBucketCols(bucketCols);\n      Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC): acidDb.acidTable\", e.getMessage());\n    }\n\n    // Succeed - \"transactional\" is set to true, and the table is bucketed, and uses ORC\n    params.clear();\n    params.put(\"transactional\", \"true\");\n    List<String> bucketCols = new ArrayList<String>();\n    bucketCols.add(\"income\");\n    sd.setBucketCols(bucketCols);\n    sd.setInputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\");\n    sd.setOutputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\");\n    Table t = createTable(dbName, tblName, owner, params, null, sd, 0);\n    Assert.assertTrue(\"CREATE TABLE should succeed\", \"true\".equals(t.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)));\n\n    /// ALTER TABLE scenarios\n\n    // Fail - trying to set \"transactional\" to \"false\" is not allowed\n    try {\n      params.clear();\n      params.put(\"transactional\", \"false\");\n      t.setParameters(params);\n      client.alter_table(dbName, tblName, t);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"TBLPROPERTIES with 'transactional'='true' cannot be unset: aciddb.acidtable\", e.getMessage());\n    }\n\n    // Fail - trying to set \"transactional\" to \"true\" but doesn't satisfy bucketing and Input/OutputFormat requirement\n    try {\n      tblName += \"1\";\n      params.clear();\n      sd.unsetBucketCols();\n      sd.setInputFormat(\"org.apache.hadoop.mapred.FileInputFormat\");\n      t = createTable(dbName, tblName, owner, params, null, sd, 0);\n      params.put(\"transactional\", \"true\");\n      t.setParameters(params);\n      client.alter_table(dbName, tblName, t);\n      Assert.assertTrue(\"Expected exception\", false);\n    } catch (MetaException e) {\n      Assert.assertEquals(\"The table must be stored using an ACID compliant format (such as ORC): aciddb.acidtable1\", e.getMessage());\n    }\n\n    // Succeed - trying to set \"transactional\" to \"true\", and satisfies bucketing and Input/OutputFormat requirement\n    tblName += \"2\";\n    params.clear();\n    sd.setNumBuckets(1);\n    sd.setBucketCols(bucketCols);\n    sd.setInputFormat(\"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\");\n    t = createTable(dbName, tblName, owner, params, null, sd, 0);\n    params.put(\"transactional\", \"true\");\n    t.setParameters(params);\n    t.setPartitionKeys(Collections.EMPTY_LIST);\n    client.alter_table(dbName, tblName, t);\n    Assert.assertTrue(\"ALTER TABLE should succeed\", \"true\".equals(t.getParameters().get(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL)));\n  }"
        ],
        [
            "TransactionalValidationListener::handleCreateTableTransactionalProp(PreCreateTableEvent)",
            " 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260 -\n 261  ",
            "  /**\n   * Normalize case and make sure:\n   * 1. 'true' is the only value to be set for 'transactional' (if set at all)\n   * 2. If set to 'true', we should also enforce bucketing and ORC format\n   */\n  private void handleCreateTableTransactionalProp(PreCreateTableEvent context) throws MetaException {\n    Table newTable = context.getTable();\n    Map<String, String> parameters = newTable.getParameters();\n    if (parameters == null || parameters.isEmpty()) {\n      return;\n    }\n    String transactional = null;\n    String transactionalProperties = null;\n    Set<String> keys = new HashSet<>(parameters.keySet());\n    for(String key : keys) {\n      // Get the \"transactional\" tblproperties value\n      if (hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        transactional = parameters.get(key);\n        parameters.remove(key);\n      }\n\n      // Get the \"transactional_properties\" tblproperties value\n      if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        transactionalProperties = parameters.get(key);\n      }\n    }\n\n    if (transactional == null) {\n      return;\n    }\n\n    if (\"false\".equalsIgnoreCase(transactional)) {\n      // just drop transactional=false.  For backward compatibility in case someone has scripts\n      // with transactional=false\n      LOG.info(\"'transactional'='false' is no longer a valid property and will be ignored\");\n      return;\n    }\n\n    if (\"true\".equalsIgnoreCase(transactional)) {\n      if (!conformToAcid(newTable)) {\n        // INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing\n        if (transactionalProperties == null || !\"insert_only\".equalsIgnoreCase(transactionalProperties)) {\n          throw new MetaException(\"The table must be stored using an ACID compliant format (such as ORC)\");\n        }\n      }\n\n      if (newTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        throw new MetaException(Warehouse.getQualifiedName(newTable) +\n            \" cannot be declared transactional because it's an external table\");\n      }\n\n      // normalize prop name\n      parameters.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, Boolean.TRUE.toString());\n      if(transactionalProperties == null) {\n        normazlieTransactionalPropertyDefault(newTable);\n      }\n      initializeTransactionalProperties(newTable);\n      checkSorted(newTable);\n      return;\n    }\n    // transactional is found, but the value is not in expected range\n    throw new MetaException(\"'transactional' property of TBLPROPERTIES may only have value 'true'\");\n  }",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235 +\n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245 +\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264 +\n 265 +\n 266  ",
            "  /**\n   * Normalize case and make sure:\n   * 1. 'true' is the only value to be set for 'transactional' (if set at all)\n   * 2. If set to 'true', we should also enforce bucketing and ORC format\n   */\n  private void handleCreateTableTransactionalProp(PreCreateTableEvent context) throws MetaException {\n    Table newTable = context.getTable();\n    Map<String, String> parameters = newTable.getParameters();\n    if (parameters == null || parameters.isEmpty()) {\n      return;\n    }\n    String transactional = null;\n    String transactionalProperties = null;\n    Set<String> keys = new HashSet<>(parameters.keySet());\n    for(String key : keys) {\n      // Get the \"transactional\" tblproperties value\n      if (hive_metastoreConstants.TABLE_IS_TRANSACTIONAL.equalsIgnoreCase(key)) {\n        transactional = parameters.get(key);\n        parameters.remove(key);\n      }\n\n      // Get the \"transactional_properties\" tblproperties value\n      if (hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES.equalsIgnoreCase(key)) {\n        transactionalProperties = parameters.get(key);\n      }\n    }\n\n    if (transactional == null) {\n      return;\n    }\n\n    if (\"false\".equalsIgnoreCase(transactional)) {\n      // just drop transactional=false.  For backward compatibility in case someone has scripts\n      // with transactional=false\n      LOG.info(\"'transactional'='false' is no longer a valid property and will be ignored: \" +\n        Warehouse.getQualifiedName(newTable));\n      return;\n    }\n\n    if (\"true\".equalsIgnoreCase(transactional)) {\n      if (!conformToAcid(newTable)) {\n        // INSERT_ONLY tables don't have to conform to ACID requirement like ORC or bucketing\n        if (transactionalProperties == null || !\"insert_only\".equalsIgnoreCase(transactionalProperties)) {\n          throw new MetaException(\"The table must be stored using an ACID compliant format (such as ORC): \"\n              + Warehouse.getQualifiedName(newTable));\n        }\n      }\n\n      if (newTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        throw new MetaException(Warehouse.getQualifiedName(newTable) +\n            \" cannot be declared transactional because it's an external table\");\n      }\n\n      // normalize prop name\n      parameters.put(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, Boolean.TRUE.toString());\n      if(transactionalProperties == null) {\n        normazlieTransactionalPropertyDefault(newTable);\n      }\n      initializeTransactionalProperties(newTable);\n      checkSorted(newTable);\n      return;\n    }\n    // transactional is found, but the value is not in expected range\n    throw new MetaException(\"'transactional' property of TBLPROPERTIES may only have value 'true': \"\n        + Warehouse.getQualifiedName(newTable));\n  }"
        ]
    ],
    "ec7ccc3a452fa125719ca820b5f751ddd00686ec": [
        [
            "ObjectStore::getNextNotification(NotificationEventRequest)",
            "8882  \n8883  \n8884  \n8885  \n8886  \n8887  \n8888  \n8889  \n8890  \n8891  \n8892  \n8893  \n8894  \n8895  \n8896  \n8897  \n8898  \n8899  \n8900  \n8901  \n8902  \n8903  \n8904  \n8905  \n8906  \n8907  \n8908 -\n8909 -\n8910 -\n8911 -\n8912  \n8913  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n\n    NotificationEventResponse result = new NotificationEventResponse();\n    result.setEvents(new ArrayList<>());\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return result;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      int numEvents = 0;\n      while (i.hasNext() && numEvents++ < maxEvents) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      if (!commited) {\n        rollbackAndCleanup(commited, query);\n        return null;\n      }\n    }\n  }",
            "8880  \n8881  \n8882  \n8883  \n8884  \n8885  \n8886  \n8887  \n8888  \n8889  \n8890  \n8891  \n8892  \n8893  \n8894  \n8895  \n8896  \n8897  \n8898  \n8899  \n8900  \n8901  \n8902  \n8903  \n8904  \n8905  \n8906 +\n8907  \n8908  ",
            "  @Override\n  public NotificationEventResponse getNextNotification(NotificationEventRequest rqst) {\n    boolean commited = false;\n    Query query = null;\n\n    NotificationEventResponse result = new NotificationEventResponse();\n    result.setEvents(new ArrayList<>());\n    try {\n      openTransaction();\n      long lastEvent = rqst.getLastEvent();\n      query = pm.newQuery(MNotificationLog.class, \"eventId > lastEvent\");\n      query.declareParameters(\"java.lang.Long lastEvent\");\n      query.setOrdering(\"eventId ascending\");\n      Collection<MNotificationLog> events = (Collection) query.execute(lastEvent);\n      commited = commitTransaction();\n      if (events == null) {\n        return result;\n      }\n      Iterator<MNotificationLog> i = events.iterator();\n      int maxEvents = rqst.getMaxEvents() > 0 ? rqst.getMaxEvents() : Integer.MAX_VALUE;\n      int numEvents = 0;\n      while (i.hasNext() && numEvents++ < maxEvents) {\n        result.addToEvents(translateDbToThrift(i.next()));\n      }\n      return result;\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ],
        [
            "ObjectStore::getFunction(String,String)",
            "8818  \n8819  \n8820  \n8821  \n8822  \n8823  \n8824  \n8825  \n8826  \n8827 -\n8828 -\n8829 -\n8830  \n8831  \n8832  ",
            "  @Override\n  public Function getFunction(String dbName, String funcName) throws MetaException {\n    boolean commited = false;\n    Function func = null;\n    try {\n      openTransaction();\n      func = convertToFunction(getMFunction(dbName, funcName));\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n    return func;\n  }",
            "8818  \n8819  \n8820  \n8821  \n8822 +\n8823  \n8824  \n8825  \n8826  \n8827  \n8828 +\n8829  \n8830  \n8831  ",
            "  @Override\n  public Function getFunction(String dbName, String funcName) throws MetaException {\n    boolean commited = false;\n    Function func = null;\n    Query query = null;\n    try {\n      openTransaction();\n      func = convertToFunction(getMFunction(dbName, funcName));\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return func;\n  }"
        ],
        [
            "ObjectStore::removeUnusedColumnDescriptor(MColumnDescriptor)",
            "3933  \n3934  \n3935  \n3936  \n3937  \n3938  \n3939  \n3940  \n3941  \n3942  \n3943  \n3944 -\n3945  \n3946  \n3947  \n3948  \n3949  \n3950 -\n3951  \n3952  \n3953  \n3954  \n3955  \n3956  \n3957  \n3958  \n3959  \n3960  \n3961  \n3962  \n3963 -\n3964  \n3965  ",
            "  /**\n   * Checks if a column descriptor has any remaining references by storage descriptors\n   * in the db.  If it does not, then delete the CD.  If it does, then do nothing.\n   * @param oldCD the column descriptor to delete if it is no longer referenced anywhere\n   */\n  private void removeUnusedColumnDescriptor(MColumnDescriptor oldCD) {\n    if (oldCD == null) {\n      return;\n    }\n\n    boolean success = false;\n    QueryWrapper queryWrapper = new QueryWrapper();\n\n    try {\n      openTransaction();\n      LOG.debug(\"execute removeUnusedColumnDescriptor\");\n\n      Query query = pm.newQuery(\"select count(1) from \" +\n        \"org.apache.hadoop.hive.metastore.model.MStorageDescriptor where (this.cd == inCD)\");\n      query.declareParameters(\"MColumnDescriptor inCD\");\n      long count = ((Long)query.execute(oldCD)).longValue();\n\n      //if no other SD references this CD, we can throw it out.\n      if (count == 0) {\n        pm.retrieve(oldCD);\n        pm.deletePersistent(oldCD);\n      }\n      success = commitTransaction();\n      LOG.debug(\"successfully deleted a CD in removeUnusedColumnDescriptor\");\n    } finally {\n      rollbackAndCleanup(success, queryWrapper);\n    }\n  }",
            "3933  \n3934  \n3935  \n3936  \n3937  \n3938  \n3939  \n3940  \n3941  \n3942  \n3943  \n3944 +\n3945  \n3946  \n3947  \n3948  \n3949  \n3950 +\n3951  \n3952  \n3953  \n3954  \n3955  \n3956  \n3957  \n3958  \n3959  \n3960  \n3961  \n3962  \n3963 +\n3964  \n3965  ",
            "  /**\n   * Checks if a column descriptor has any remaining references by storage descriptors\n   * in the db.  If it does not, then delete the CD.  If it does, then do nothing.\n   * @param oldCD the column descriptor to delete if it is no longer referenced anywhere\n   */\n  private void removeUnusedColumnDescriptor(MColumnDescriptor oldCD) {\n    if (oldCD == null) {\n      return;\n    }\n\n    boolean success = false;\n    Query query = null;\n\n    try {\n      openTransaction();\n      LOG.debug(\"execute removeUnusedColumnDescriptor\");\n\n      query = pm.newQuery(\"select count(1) from \" +\n        \"org.apache.hadoop.hive.metastore.model.MStorageDescriptor where (this.cd == inCD)\");\n      query.declareParameters(\"MColumnDescriptor inCD\");\n      long count = ((Long)query.execute(oldCD)).longValue();\n\n      //if no other SD references this CD, we can throw it out.\n      if (count == 0) {\n        pm.retrieve(oldCD);\n        pm.deletePersistent(oldCD);\n      }\n      success = commitTransaction();\n      LOG.debug(\"successfully deleted a CD in removeUnusedColumnDescriptor\");\n    } finally {\n      rollbackAndCleanup(success, query);\n    }\n  }"
        ],
        [
            "ObjectStore::getNotNullConstraintsViaJdo(String,String)",
            "9577  \n9578  \n9579  \n9580  \n9581  \n9582  \n9583  \n9584  \n9585  \n9586  \n9587  \n9588  \n9589  \n9590  \n9591  \n9592  \n9593  \n9594  \n9595  \n9596  \n9597  \n9598  \n9599  \n9600  \n9601  \n9602  \n9603  \n9604  \n9605  \n9606 -\n9607 -\n9608 -\n9609 -\n9610 -\n9611 -\n9612  \n9613  \n9614  ",
            "  private List<SQLNotNullConstraint> getNotNullConstraintsViaJdo(String db_name, String tbl_name)\n          throws MetaException {\n    boolean commited = false;\n    List<SQLNotNullConstraint> notNullConstraints = null;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MConstraint.class,\n        \"parentTable.tableName == tbl_name && parentTable.database.name == db_name &&\"\n        + \" constraintType == MConstraint.NOT_NULL_CONSTRAINT\");\n      query.declareParameters(\"java.lang.String tbl_name, java.lang.String db_name\");\n      Collection<?> constraints = (Collection<?>) query.execute(tbl_name, db_name);\n      pm.retrieveAll(constraints);\n      notNullConstraints = new ArrayList<>();\n      for (Iterator<?> i = constraints.iterator(); i.hasNext();) {\n        MConstraint currConstraint = (MConstraint) i.next();\n        List<MFieldSchema> cols = currConstraint.getParentColumn() != null ?\n            currConstraint.getParentColumn().getCols() : currConstraint.getParentTable().getPartitionKeys();\n        int enableValidateRely = currConstraint.getEnableValidateRely();\n        boolean enable = (enableValidateRely & 4) != 0;\n        boolean validate = (enableValidateRely & 2) != 0;\n        boolean rely = (enableValidateRely & 1) != 0;\n        notNullConstraints.add(new SQLNotNullConstraint(db_name,\n         tbl_name,\n         cols.get(currConstraint.getParentIntegerIndex()).getName(),\n         currConstraint.getConstraintName(), enable, validate, rely));\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n    return notNullConstraints;\n  }",
            "9568  \n9569  \n9570  \n9571  \n9572  \n9573  \n9574  \n9575  \n9576  \n9577  \n9578  \n9579  \n9580  \n9581  \n9582  \n9583  \n9584  \n9585  \n9586  \n9587  \n9588  \n9589  \n9590  \n9591  \n9592  \n9593  \n9594  \n9595  \n9596  \n9597 +\n9598  \n9599  \n9600  ",
            "  private List<SQLNotNullConstraint> getNotNullConstraintsViaJdo(String db_name, String tbl_name)\n          throws MetaException {\n    boolean commited = false;\n    List<SQLNotNullConstraint> notNullConstraints = null;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MConstraint.class,\n        \"parentTable.tableName == tbl_name && parentTable.database.name == db_name &&\"\n        + \" constraintType == MConstraint.NOT_NULL_CONSTRAINT\");\n      query.declareParameters(\"java.lang.String tbl_name, java.lang.String db_name\");\n      Collection<?> constraints = (Collection<?>) query.execute(tbl_name, db_name);\n      pm.retrieveAll(constraints);\n      notNullConstraints = new ArrayList<>();\n      for (Iterator<?> i = constraints.iterator(); i.hasNext();) {\n        MConstraint currConstraint = (MConstraint) i.next();\n        List<MFieldSchema> cols = currConstraint.getParentColumn() != null ?\n            currConstraint.getParentColumn().getCols() : currConstraint.getParentTable().getPartitionKeys();\n        int enableValidateRely = currConstraint.getEnableValidateRely();\n        boolean enable = (enableValidateRely & 4) != 0;\n        boolean validate = (enableValidateRely & 2) != 0;\n        boolean rely = (enableValidateRely & 1) != 0;\n        notNullConstraints.add(new SQLNotNullConstraint(db_name,\n         tbl_name,\n         cols.get(currConstraint.getParentIntegerIndex()).getName(),\n         currConstraint.getConstraintName(), enable, validate, rely));\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return notNullConstraints;\n  }"
        ],
        [
            "ObjectStore::getUniqueConstraintsViaJdo(String,String)",
            "9506  \n9507  \n9508  \n9509  \n9510  \n9511  \n9512  \n9513  \n9514  \n9515  \n9516  \n9517  \n9518  \n9519  \n9520  \n9521  \n9522  \n9523  \n9524  \n9525  \n9526  \n9527  \n9528  \n9529  \n9530  \n9531  \n9532  \n9533  \n9534  \n9535  \n9536 -\n9537 -\n9538 -\n9539 -\n9540 -\n9541 -\n9542  \n9543  \n9544  ",
            "  private List<SQLUniqueConstraint> getUniqueConstraintsViaJdo(String db_name, String tbl_name)\n          throws MetaException {\n    boolean commited = false;\n    List<SQLUniqueConstraint> uniqueConstraints = null;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MConstraint.class,\n        \"parentTable.tableName == tbl_name && parentTable.database.name == db_name &&\"\n        + \" constraintType == MConstraint.UNIQUE_CONSTRAINT\");\n      query.declareParameters(\"java.lang.String tbl_name, java.lang.String db_name\");\n      Collection<?> constraints = (Collection<?>) query.execute(tbl_name, db_name);\n      pm.retrieveAll(constraints);\n      uniqueConstraints = new ArrayList<>();\n      for (Iterator<?> i = constraints.iterator(); i.hasNext();) {\n        MConstraint currConstraint = (MConstraint) i.next();\n        List<MFieldSchema> cols = currConstraint.getParentColumn() != null ?\n            currConstraint.getParentColumn().getCols() : currConstraint.getParentTable().getPartitionKeys();\n        int enableValidateRely = currConstraint.getEnableValidateRely();\n        boolean enable = (enableValidateRely & 4) != 0;\n        boolean validate = (enableValidateRely & 2) != 0;\n        boolean rely = (enableValidateRely & 1) != 0;\n        uniqueConstraints.add(new SQLUniqueConstraint(db_name,\n         tbl_name,\n         cols.get(currConstraint.getParentIntegerIndex()).getName(),\n         currConstraint.getPosition(),\n         currConstraint.getConstraintName(), enable, validate, rely));\n      }\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n    return uniqueConstraints;\n  }",
            "9502  \n9503  \n9504  \n9505  \n9506  \n9507  \n9508  \n9509  \n9510  \n9511  \n9512  \n9513  \n9514  \n9515  \n9516  \n9517  \n9518  \n9519  \n9520  \n9521  \n9522  \n9523  \n9524  \n9525  \n9526  \n9527  \n9528  \n9529  \n9530  \n9531  \n9532 +\n9533  \n9534  \n9535  ",
            "  private List<SQLUniqueConstraint> getUniqueConstraintsViaJdo(String db_name, String tbl_name)\n          throws MetaException {\n    boolean commited = false;\n    List<SQLUniqueConstraint> uniqueConstraints = null;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MConstraint.class,\n        \"parentTable.tableName == tbl_name && parentTable.database.name == db_name &&\"\n        + \" constraintType == MConstraint.UNIQUE_CONSTRAINT\");\n      query.declareParameters(\"java.lang.String tbl_name, java.lang.String db_name\");\n      Collection<?> constraints = (Collection<?>) query.execute(tbl_name, db_name);\n      pm.retrieveAll(constraints);\n      uniqueConstraints = new ArrayList<>();\n      for (Iterator<?> i = constraints.iterator(); i.hasNext();) {\n        MConstraint currConstraint = (MConstraint) i.next();\n        List<MFieldSchema> cols = currConstraint.getParentColumn() != null ?\n            currConstraint.getParentColumn().getCols() : currConstraint.getParentTable().getPartitionKeys();\n        int enableValidateRely = currConstraint.getEnableValidateRely();\n        boolean enable = (enableValidateRely & 4) != 0;\n        boolean validate = (enableValidateRely & 2) != 0;\n        boolean rely = (enableValidateRely & 1) != 0;\n        uniqueConstraints.add(new SQLUniqueConstraint(db_name,\n         tbl_name,\n         cols.get(currConstraint.getParentIntegerIndex()).getName(),\n         currConstraint.getPosition(),\n         currConstraint.getConstraintName(), enable, validate, rely));\n      }\n      commited = commitTransaction();\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n    return uniqueConstraints;\n  }"
        ],
        [
            "TestObjectStore::testQueryCloseOnError()",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 -\n 446  \n 447  ",
            "  @Test\n  public void testQueryCloseOnError() throws Exception {\n    ObjectStore spy = Mockito.spy(objectStore);\n    spy.getAllDatabases();\n    spy.getAllFunctions();\n    spy.getAllTables(DB1);\n    spy.getPartitionCount();\n    Mockito.verify(spy, Mockito.times(2))\n        .rollbackAndCleanup(Mockito.anyBoolean(), Mockito.<Query>anyObject());\n  }",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  ",
            "  @Test\n  public void testQueryCloseOnError() throws Exception {\n    ObjectStore spy = Mockito.spy(objectStore);\n    spy.getAllDatabases();\n    spy.getAllFunctions();\n    spy.getAllTables(DB1);\n    spy.getPartitionCount();\n    Mockito.verify(spy, Mockito.times(3))\n        .rollbackAndCleanup(Mockito.anyBoolean(), Mockito.<Query>anyObject());\n  }"
        ],
        [
            "ObjectStore::lockForUpdate()",
            "8932  \n8933  \n8934  \n8935  \n8936  \n8937  \n8938  \n8939  \n8940  \n8941  \n8942  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select \\\"NEXT_EVENT_ID\\\" from \\\"NOTIFICATION_SEQUENCE\\\"\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(conf, () -> {\n      prepareQuotes();\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n    }).run();\n  }",
            "8927  \n8928  \n8929  \n8930  \n8931  \n8932  \n8933  \n8934  \n8935  \n8936 +\n8937  \n8938  ",
            "  private void lockForUpdate() throws MetaException {\n    String selectQuery = \"select \\\"NEXT_EVENT_ID\\\" from \\\"NOTIFICATION_SEQUENCE\\\"\";\n    String selectForUpdateQuery = sqlGenerator.addForUpdateClause(selectQuery);\n    new RetryingExecutor(conf, () -> {\n      prepareQuotes();\n      Query query = pm.newQuery(\"javax.jdo.query.SQL\", selectForUpdateQuery);\n      query.setUnique(true);\n      // only need to execute it to get db Lock\n      query.execute();\n      query.closeAll();\n    }).run();\n  }"
        ],
        [
            "ObjectStore::addNotificationEvent(NotificationEvent)",
            "8999  \n9000  \n9001  \n9002  \n9003  \n9004  \n9005  \n9006 -\n9007 -\n9008  \n9009  \n9010  \n9011  \n9012  \n9013  \n9014  \n9015  \n9016  \n9017  \n9018  \n9019  \n9020  \n9021  \n9022  \n9023  \n9024  \n9025  \n9026  \n9027  \n9028  \n9029  ",
            "  @Override\n  public void addNotificationEvent(NotificationEvent entry) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      lockForUpdate();\n      Query objectQuery = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) objectQuery.execute();\n      MNotificationNextId mNotificationNextId = null;\n      boolean needToPersistId;\n      if (CollectionUtils.isEmpty(ids)) {\n        mNotificationNextId = new MNotificationNextId(1L);\n        needToPersistId = true;\n      } else {\n        mNotificationNextId = ids.iterator().next();\n        needToPersistId = false;\n      }\n      entry.setEventId(mNotificationNextId.getNextEventId());\n      mNotificationNextId.incrementEventId();\n      if (needToPersistId) {\n        pm.makePersistent(mNotificationNextId);\n      }\n      pm.makePersistent(translateThriftToDb(entry));\n      commited = commitTransaction();\n    } catch (Exception e) {\n      LOG.error(\"couldnot get lock for update\", e);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }",
            "8995  \n8996  \n8997  \n8998  \n8999  \n9000  \n9001  \n9002 +\n9003 +\n9004  \n9005  \n9006  \n9007  \n9008  \n9009  \n9010  \n9011  \n9012  \n9013  \n9014  \n9015  \n9016  \n9017  \n9018  \n9019  \n9020  \n9021  \n9022  \n9023  \n9024  \n9025  ",
            "  @Override\n  public void addNotificationEvent(NotificationEvent entry) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      lockForUpdate();\n      query = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) query.execute();\n      MNotificationNextId mNotificationNextId = null;\n      boolean needToPersistId;\n      if (CollectionUtils.isEmpty(ids)) {\n        mNotificationNextId = new MNotificationNextId(1L);\n        needToPersistId = true;\n      } else {\n        mNotificationNextId = ids.iterator().next();\n        needToPersistId = false;\n      }\n      entry.setEventId(mNotificationNextId.getNextEventId());\n      mNotificationNextId.incrementEventId();\n      if (needToPersistId) {\n        pm.makePersistent(mNotificationNextId);\n      }\n      pm.makePersistent(translateThriftToDb(entry));\n      commited = commitTransaction();\n    } catch (Exception e) {\n      LOG.error(\"couldnot get lock for update\", e);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ],
        [
            "ObjectStore::getAllFunctions()",
            "8834  \n8835  \n8836  \n8837  \n8838  \n8839 -\n8840  \n8841  \n8842  \n8843  \n8844  \n8845 -\n8846 -\n8847 -\n8848  \n8849  ",
            "  @Override\n  public List<Function> getAllFunctions() throws MetaException {\n    boolean commited = false;\n    try {\n      openTransaction();\n      Query query = pm.newQuery(MFunction.class);\n      List<MFunction> allFunctions = (List<MFunction>) query.execute();\n      pm.retrieveAll(allFunctions);\n      commited = commitTransaction();\n      return convertToFunctions(allFunctions);\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n  }",
            "8833  \n8834  \n8835  \n8836 +\n8837  \n8838  \n8839 +\n8840  \n8841  \n8842  \n8843  \n8844  \n8845 +\n8846  \n8847  ",
            "  @Override\n  public List<Function> getAllFunctions() throws MetaException {\n    boolean commited = false;\n    Query query = null;\n    try {\n      openTransaction();\n      query = pm.newQuery(MFunction.class);\n      List<MFunction> allFunctions = (List<MFunction>) query.execute();\n      pm.retrieveAll(allFunctions);\n      commited = commitTransaction();\n      return convertToFunctions(allFunctions);\n    } finally {\n      rollbackAndCleanup(commited, query);\n    }\n  }"
        ]
    ],
    "e05e0fa19d7fd7c48617c4a770fa579b7f01f40e": [
        [
            "DruidOutputFormat::getHiveRecordWriter(JobConf,Path,Class,boolean,Properties,Progressable)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195 -\n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 -\n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    // If datasource is in the table properties, it is an INSERT/INSERT OVERWRITE as the datasource\n    // name was already persisted. Otherwise, it is a CT/CTAS and we need to get the name from the\n    // job properties that are set by configureOutputJobProperties in the DruidStorageHandler\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE) == null\n        ? jc.get(Constants.DRUID_DATA_SOURCE)\n        : tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory = jc.get(Constants.DRUID_SEGMENT_INTERMEDIATE_DIRECTORY);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.fromString(segmentGranularity),\n            Granularity.fromString(\n                    tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY) == null\n                            ? \"NONE\"\n                            : tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY)),\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    final boolean approximationAllowed = HiveConf.getBoolVar(jc, HiveConf.ConfVars.HIVE_DRUID_APPROX_RESULT);\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      final PrimitiveObjectInspector.PrimitiveCategory primitiveCategory = ((PrimitiveTypeInfo) columnTypes\n              .get(i)).getPrimitiveCategory();\n      AggregatorFactory af;\n      switch (primitiveCategory) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case DECIMAL:\n          if (approximationAllowed) {\n            af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          } else {\n            throw new UnsupportedOperationException(\n                String.format(\"Druid does not support decimal column type.\" +\n                        \"Either cast column [%s] to double or Enable Approximate Result for Druid by setting property [%s] to true\",\n                    columnNames.get(i), HiveConf.ConfVars.HIVE_DRUID_APPROX_RESULT.varname));\n          }\n          break;\n        case TIMESTAMP:\n          // Granularity column\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          continue;\n        case TIMESTAMPLOCALTZ:\n          // Druid timestamp column\n          String tLocalTZColumnName = columnNames.get(i);\n          if (!tLocalTZColumnName.equals(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN)) {\n            throw new IOException(\"Dimension \" + tLocalTZColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(primitiveCategory) !=\n                  PrimitiveGrouping.STRING_GROUP\n                  && primitiveCategory != PrimitiveObjectInspector.PrimitiveCategory.BOOLEAN) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions,\n                    Lists.newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    Integer maxPartitionSize = HiveConf\n            .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    if (Strings.isNullOrEmpty(basePersistDirectory)) {\n      basePersistDirectory = System.getProperty(\"java.io.tmpdir\");\n    }\n    Integer maxRowInMemory = HiveConf.getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_ROW_IN_MEMORY);\n\n    IndexSpec indexSpec;\n    if (\"concise\".equals(HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BITMAP_FACTORY_TYPE))) {\n      indexSpec = new IndexSpec(new ConciseBitmapSerdeFactory(), null, null, null);\n    } else {\n      indexSpec = new IndexSpec(new RoaringBitmapSerdeFactory(true), null, null, null);\n    }\n    RealtimeTuningConfig realtimeTuningConfig = new RealtimeTuningConfig(maxRowInMemory,\n            null,\n            null,\n            new File(basePersistDirectory, dataSource),\n            new CustomVersioningPolicy(version),\n            null,\n            null,\n            null,\n            indexSpec,\n            true,\n            0,\n            0,\n            true,\n            null,\n            0L\n    );\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig,\n            DruidStorageHandlerUtils.createSegmentPusherForDirectory(segmentDirectory, jc),\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96 +\n  97 +\n  98 +\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198 +\n 199 +\n 200 +\n 201 +\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  ",
            "  @Override\n  public FileSinkOperator.RecordWriter getHiveRecordWriter(\n          JobConf jc,\n          Path finalOutPath,\n          Class<? extends Writable> valueClass,\n          boolean isCompressed,\n          Properties tableProperties,\n          Progressable progress\n  ) throws IOException {\n\n    final String segmentGranularity =\n            tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) != null ?\n                    tableProperties.getProperty(Constants.DRUID_SEGMENT_GRANULARITY) :\n                    HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY);\n    final int targetNumShardsPerGranularity = Integer.parseUnsignedInt(\n        tableProperties.getProperty(Constants.DRUID_TARGET_SHARDS_PER_GRANULARITY, \"0\"));\n    final int maxPartitionSize = targetNumShardsPerGranularity > 0 ? -1 : HiveConf\n        .getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_PARTITION_SIZE);\n    // If datasource is in the table properties, it is an INSERT/INSERT OVERWRITE as the datasource\n    // name was already persisted. Otherwise, it is a CT/CTAS and we need to get the name from the\n    // job properties that are set by configureOutputJobProperties in the DruidStorageHandler\n    final String dataSource = tableProperties.getProperty(Constants.DRUID_DATA_SOURCE) == null\n        ? jc.get(Constants.DRUID_DATA_SOURCE)\n        : tableProperties.getProperty(Constants.DRUID_DATA_SOURCE);\n    final String segmentDirectory = jc.get(Constants.DRUID_SEGMENT_INTERMEDIATE_DIRECTORY);\n\n    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n            Granularity.fromString(segmentGranularity),\n            Granularity.fromString(\n                    tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY) == null\n                            ? \"NONE\"\n                            : tableProperties.getProperty(Constants.DRUID_QUERY_GRANULARITY)),\n            null\n    );\n\n    final String columnNameProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = tableProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n\n    if (StringUtils.isEmpty(columnNameProperty) || StringUtils.isEmpty(columnTypeProperty)) {\n      throw new IllegalStateException(\n              String.format(\"List of columns names [%s] or columns type [%s] is/are not present\",\n                      columnNameProperty, columnTypeProperty\n              ));\n    }\n    ArrayList<String> columnNames = new ArrayList<String>();\n    for (String name : columnNameProperty.split(\",\")) {\n      columnNames.add(name);\n    }\n    if (!columnNames.contains(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN)) {\n      throw new IllegalStateException(\"Timestamp column (' \" + DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN +\n              \"') not specified in create table; list of columns is : \" +\n              tableProperties.getProperty(serdeConstants.LIST_COLUMNS));\n    }\n    ArrayList<TypeInfo> columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n    final boolean approximationAllowed = HiveConf.getBoolVar(jc, HiveConf.ConfVars.HIVE_DRUID_APPROX_RESULT);\n    // Default, all columns that are not metrics or timestamp, are treated as dimensions\n    final List<DimensionSchema> dimensions = new ArrayList<>();\n    ImmutableList.Builder<AggregatorFactory> aggregatorFactoryBuilder = ImmutableList.builder();\n    for (int i = 0; i < columnTypes.size(); i++) {\n      final PrimitiveObjectInspector.PrimitiveCategory primitiveCategory = ((PrimitiveTypeInfo) columnTypes\n              .get(i)).getPrimitiveCategory();\n      AggregatorFactory af;\n      switch (primitiveCategory) {\n        case BYTE:\n        case SHORT:\n        case INT:\n        case LONG:\n          af = new LongSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case FLOAT:\n        case DOUBLE:\n          af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          break;\n        case DECIMAL:\n          if (approximationAllowed) {\n            af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));\n          } else {\n            throw new UnsupportedOperationException(\n                String.format(\"Druid does not support decimal column type.\" +\n                        \"Either cast column [%s] to double or Enable Approximate Result for Druid by setting property [%s] to true\",\n                    columnNames.get(i), HiveConf.ConfVars.HIVE_DRUID_APPROX_RESULT.varname));\n          }\n          break;\n        case TIMESTAMP:\n          // Granularity column\n          String tColumnName = columnNames.get(i);\n          if (!tColumnName.equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME)) {\n            throw new IOException(\"Dimension \" + tColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          continue;\n        case TIMESTAMPLOCALTZ:\n          // Druid timestamp column\n          String tLocalTZColumnName = columnNames.get(i);\n          if (!tLocalTZColumnName.equals(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN)) {\n            throw new IOException(\"Dimension \" + tLocalTZColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          continue;\n        default:\n          // Dimension\n          String dColumnName = columnNames.get(i);\n          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(primitiveCategory) !=\n                  PrimitiveGrouping.STRING_GROUP\n                  && primitiveCategory != PrimitiveObjectInspector.PrimitiveCategory.BOOLEAN) {\n            throw new IOException(\"Dimension \" + dColumnName + \" does not have STRING type: \" +\n                    primitiveCategory);\n          }\n          dimensions.add(new StringDimensionSchema(dColumnName));\n          continue;\n      }\n      aggregatorFactoryBuilder.add(af);\n    }\n    List<AggregatorFactory> aggregatorFactories = aggregatorFactoryBuilder.build();\n    final InputRowParser inputRowParser = new MapInputRowParser(new TimeAndDimsParseSpec(\n            new TimestampSpec(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN, \"auto\", null),\n            new DimensionsSpec(dimensions, Lists\n                .newArrayList(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n                    Constants.DRUID_SHARD_KEY_COL_NAME\n                ), null\n            )\n    ));\n\n    Map<String, Object> inputParser = DruidStorageHandlerUtils.JSON_MAPPER\n            .convertValue(inputRowParser, Map.class);\n\n    final DataSchema dataSchema = new DataSchema(\n            Preconditions.checkNotNull(dataSource, \"Data source name is null\"),\n            inputParser,\n            aggregatorFactories.toArray(new AggregatorFactory[aggregatorFactories.size()]),\n            granularitySpec,\n            DruidStorageHandlerUtils.JSON_MAPPER\n    );\n\n    final String workingPath = jc.get(Constants.DRUID_JOB_WORKING_DIRECTORY);\n    final String version = jc.get(Constants.DRUID_SEGMENT_VERSION);\n    String basePersistDirectory = HiveConf\n            .getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BASE_PERSIST_DIRECTORY);\n    if (Strings.isNullOrEmpty(basePersistDirectory)) {\n      basePersistDirectory = System.getProperty(\"java.io.tmpdir\");\n    }\n    Integer maxRowInMemory = HiveConf.getIntVar(jc, HiveConf.ConfVars.HIVE_DRUID_MAX_ROW_IN_MEMORY);\n\n    IndexSpec indexSpec;\n    if (\"concise\".equals(HiveConf.getVar(jc, HiveConf.ConfVars.HIVE_DRUID_BITMAP_FACTORY_TYPE))) {\n      indexSpec = new IndexSpec(new ConciseBitmapSerdeFactory(), null, null, null);\n    } else {\n      indexSpec = new IndexSpec(new RoaringBitmapSerdeFactory(true), null, null, null);\n    }\n    RealtimeTuningConfig realtimeTuningConfig = new RealtimeTuningConfig(maxRowInMemory,\n            null,\n            null,\n            new File(basePersistDirectory, dataSource),\n            new CustomVersioningPolicy(version),\n            null,\n            null,\n            null,\n            indexSpec,\n            true,\n            0,\n            0,\n            true,\n            null,\n            0L\n    );\n\n    LOG.debug(String.format(\"running with Data schema [%s] \", dataSchema));\n    return new DruidRecordWriter(dataSchema, realtimeTuningConfig,\n            DruidStorageHandlerUtils.createSegmentPusherForDirectory(segmentDirectory, jc),\n            maxPartitionSize, new Path(workingPath, SEGMENTS_DESCRIPTOR_DIR_NAME),\n            finalOutPath.getFileSystem(jc)"
        ],
        [
            "SortedDynPartitionTimeGranularityOptimizer::SortedDynamicPartitionProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151 -\n 152 -\n 153 -\n 154 -\n 155  \n 156  \n 157  \n 158 -\n 159 -\n 160  \n 161  \n 162  \n 163 -\n 164  \n 165  \n 166 -\n 167 -\n 168  \n 169  \n 170  \n 171  \n 172 -\n 173 -\n 174 -\n 175 -\n 176 -\n 177 -\n 178  \n 179  \n 180  \n 181  \n 182 -\n 183 -\n 184 -\n 185  \n 186  \n 187 -\n 188  \n 189  \n 190 -\n 191  \n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 -\n 209 -\n 210 -\n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217  \n 218  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n      final String sh = fsOp.getConf().getTableInfo().getOutputFileFormatClassName();\n      if (parseCtx.getQueryProperties().isQuery() || sh == null || !sh\n              .equals(Constants.DRUID_HIVE_OUTPUT_FORMAT)) {\n        // Bail out, nothing to do\n        return null;\n      }\n      String segmentGranularity = null;\n      final Table table = fsOp.getConf().getTable();\n      if (table != null) {\n        // case the statement is an INSERT\n        segmentGranularity = table.getParameters().get(Constants.DRUID_SEGMENT_GRANULARITY);\n      } else if (parseCtx.getCreateViewDesc() != null) {\n        // case the statement is a CREATE MATERIALIZED VIEW AS\n        segmentGranularity = parseCtx.getCreateViewDesc().getTblProps()\n                .get(Constants.DRUID_SEGMENT_GRANULARITY);\n      } else if (parseCtx.getCreateTable() != null) {\n        // case the statement is a CREATE TABLE AS\n        segmentGranularity = parseCtx.getCreateTable().getTblProps()\n                .get(Constants.DRUID_SEGMENT_GRANULARITY);\n      } else {\n        throw new SemanticException(\"Druid storage handler used but not an INSERT, \"\n                + \"CMVAS or CTAS statement\");\n      }\n      segmentGranularity = !Strings.isNullOrEmpty(segmentGranularity)\n              ? segmentGranularity\n              : HiveConf.getVar(parseCtx.getConf(),\n                      HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY\n              );\n      LOG.info(\"Sorted dynamic partitioning on time granularity optimization kicked in...\");\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      // Create SelectOp with granularity column\n      Operator<? extends OperatorDesc> granularitySelOp = getGranularitySelOp(fsParent, segmentGranularity);\n\n      // Create ReduceSinkOp operator\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(granularitySelOp.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n      // Get the key positions\n      List<Integer> keyPositions = new ArrayList<>();\n      keyPositions.add(allRSCols.size() - 1);\n      List<Integer> sortOrder = new ArrayList<Integer>(1);\n      sortOrder.add(1); // asc\n      List<Integer> sortNullOrder = new ArrayList<Integer>(1);\n      sortNullOrder.add(0); // nulls first\n      ReduceSinkOperator rsOp = getReduceSinkOp(keyPositions, sortOrder,\n          sortNullOrder, allRSCols, granularitySelOp);\n\n      // Create backtrack SelectOp\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (keyPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(granularitySelOp.getSchema());\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n      SelectOperator backtrackSelOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n          selConf, selRS, rsOp);\n\n      // Link backtrack SelectOp to FileSinkOp\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(backtrackSelOp);\n      backtrackSelOp.getChildOperators().add(fsOp);\n\n      // Update file sink descriptor\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      fsOp.getConf().setPartitionCols(rsOp.getConf().getPartitionCols());\n      ColumnInfo ci = new ColumnInfo(granularitySelOp.getSchema().getSignature().get(\n              granularitySelOp.getSchema().getSignature().size() - 1)); // granularity column\n      fsOp.getSchema().getSignature().add(ci);\n\n      LOG.info(\"Inserted \" + granularitySelOp.getOperatorId() + \", \" + rsOp.getOperatorId() + \" and \"\n          + backtrackSelOp.getOperatorId() + \" as parent of \" + fsOp.getOperatorId()\n          + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145 +\n 146  \n 147  \n 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153  \n 154  \n 155  \n 156  \n 157 +\n 158 +\n 159  \n 160  \n 161  \n 162  \n 163 +\n 164 +\n 165  \n 166  \n 167  \n 168  \n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174 +\n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181 +\n 182 +\n 183 +\n 184 +\n 185  \n 186 +\n 187 +\n 188 +\n 189  \n 190  \n 191 +\n 192 +\n 193 +\n 194  \n 195  \n 196  \n 197  \n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211  \n 212  \n 213  \n 214  \n 215 +\n 216 +\n 217  \n 218  \n 219 +\n 220  \n 221  \n 222 +\n 223 +\n 224 +\n 225 +\n 226  \n 227 +\n 228 +\n 229 +\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 +\n 246 +\n 247 +\n 248 +\n 249 +\n 250 +\n 251 +\n 252 +\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n      final String sh = fsOp.getConf().getTableInfo().getOutputFileFormatClassName();\n      if (parseCtx.getQueryProperties().isQuery() || sh == null || !sh\n              .equals(Constants.DRUID_HIVE_OUTPUT_FORMAT)) {\n        // Bail out, nothing to do\n        return null;\n      }\n      String segmentGranularity;\n      final String targetShardsProperty;\n      final Table table = fsOp.getConf().getTable();\n      if (table != null) {\n        // case the statement is an INSERT\n        segmentGranularity = table.getParameters().get(Constants.DRUID_SEGMENT_GRANULARITY);\n        targetShardsProperty =\n            table.getParameters().getOrDefault(Constants.DRUID_TARGET_SHARDS_PER_GRANULARITY, \"0\");\n\n      } else if (parseCtx.getCreateViewDesc() != null) {\n        // case the statement is a CREATE MATERIALIZED VIEW AS\n        segmentGranularity = parseCtx.getCreateViewDesc().getTblProps()\n                .get(Constants.DRUID_SEGMENT_GRANULARITY);\n        targetShardsProperty = parseCtx.getCreateViewDesc().getTblProps()\n            .getOrDefault(Constants.DRUID_TARGET_SHARDS_PER_GRANULARITY, \"0\");\n      } else if (parseCtx.getCreateTable() != null) {\n        // case the statement is a CREATE TABLE AS\n        segmentGranularity = parseCtx.getCreateTable().getTblProps()\n                .get(Constants.DRUID_SEGMENT_GRANULARITY);\n        targetShardsProperty = parseCtx.getCreateTable().getTblProps()\n            .getOrDefault(Constants.DRUID_TARGET_SHARDS_PER_GRANULARITY, \"0\");\n      } else {\n        throw new SemanticException(\"Druid storage handler used but not an INSERT, \"\n                + \"CMVAS or CTAS statement\");\n      }\n      segmentGranularity = Strings.isNullOrEmpty(segmentGranularity) ? HiveConf\n          .getVar(parseCtx.getConf(),\n              HiveConf.ConfVars.HIVE_DRUID_INDEXING_GRANULARITY\n          ) : segmentGranularity;\n      targetShardsPerGranularity = Integer.parseInt(targetShardsProperty);\n\n      LOG.info(\"Sorted dynamic partitioning on time granularity optimization kicked in...\");\n\n      // unlink connection between FS and its parent\n      final Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      if (targetShardsPerGranularity > 0) {\n        partitionKeyPos = fsParent.getSchema().getSignature().size() + 1;\n      }\n      granularityKeyPos = fsParent.getSchema().getSignature().size();\n      // Create SelectOp with granularity column\n      final Operator<? extends OperatorDesc> granularitySelOp = getGranularitySelOp(fsParent,\n              segmentGranularity\n      );\n\n      // Create ReduceSinkOp operator\n      final ArrayList<ColumnInfo> parentCols =\n          Lists.newArrayList(granularitySelOp.getSchema().getSignature());\n      final ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n      // Get the key positions\n      final List<Integer> keyPositions;\n      final List<Integer> sortOrder;\n      final List<Integer> sortNullOrder;\n      //Order matters, assuming later that __time_granularity comes first then __druidPartitionKey\n      if (targetShardsPerGranularity > 0) {\n        keyPositions = Lists.newArrayList(granularityKeyPos, partitionKeyPos);\n        sortOrder = Lists.newArrayList(1, 1); // asc\n        sortNullOrder = Lists.newArrayList(0, 0); // nulls first\n      } else {\n        keyPositions = Lists.newArrayList(granularityKeyPos);\n        sortOrder = Lists.newArrayList(1); // asc\n        sortNullOrder = Lists.newArrayList(0); // nulls first\n      }\n      ReduceSinkOperator rsOp = getReduceSinkOp(keyPositions, sortOrder,\n          sortNullOrder, allRSCols, granularitySelOp);\n\n      // Create backtrack SelectOp\n      final List<ExprNodeDesc> descs = new ArrayList<>(allRSCols.size());\n      final List<String> colNames = new ArrayList<>();\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        final String colName = col.getExprString();\n        colNames.add(colName);\n        if (keyPositions.contains(i)) {\n          descs.add(\n              new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString() + \".\" + colName,\n                  null, false\n              ));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(),\n              ReduceField.VALUE.toString() + \".\" + colName, null, false\n          ));\n        }\n      }\n      RowSchema selRS = new RowSchema(granularitySelOp.getSchema());\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n      SelectOperator backtrackSelOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n          selConf, selRS, rsOp);\n\n      // Link backtrack SelectOp to FileSinkOp\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(backtrackSelOp);\n      backtrackSelOp.getChildOperators().add(fsOp);\n\n      // Update file sink descriptor\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      fsOp.getConf().setPartitionCols(rsOp.getConf().getPartitionCols());\n      final ColumnInfo granularityColumnInfo =\n          new ColumnInfo(granularitySelOp.getSchema().getSignature().get(granularityKeyPos));\n      fsOp.getSchema().getSignature().add(granularityColumnInfo);\n      if (targetShardsPerGranularity > 0) {\n        final ColumnInfo partitionKeyColumnInfo =\n            new ColumnInfo(granularitySelOp.getSchema().getSignature().get(partitionKeyPos));\n        fsOp.getSchema().getSignature().add(partitionKeyColumnInfo);\n      }\n\n      LOG.info(\"Inserted \" + granularitySelOp.getOperatorId() + \", \" + rsOp.getOperatorId() + \" and \"\n          + backtrackSelOp.getOperatorId() + \" as parent of \" + fsOp.getOperatorId()\n          + \" and child of \" + fsParent.getOperatorId());\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }"
        ],
        [
            "DruidRecordWriter::getSegmentIdentifierAndMaybePush(long)",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129 -\n 130 -\n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146 -\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "  /**\n   * This function computes the segment identifier and push the current open segment\n   * The push will occur if max size is reached or the event belongs to the next interval.\n   * Note that this function assumes that timestamps are pseudo sorted.\n   * This function will close and move to the next segment granularity as soon as\n   * an event from the next interval appears. The sorting is done by the previous stage.\n   *\n   * @return segmentIdentifier with of the truncatedTime and maybe push the current open segment.\n   */\n  private SegmentIdentifier getSegmentIdentifierAndMaybePush(long truncatedTime) {\n\n    final Granularity segmentGranularity = dataSchema.getGranularitySpec()\n            .getSegmentGranularity();\n\n    final Interval interval = new Interval(\n            new DateTime(truncatedTime),\n            segmentGranularity.increment(new DateTime(truncatedTime))\n    );\n\n    SegmentIdentifier retVal;\n    if (currentOpenSegment == null) {\n      retVal = new SegmentIdentifier(\n              dataSchema.getDataSource(),\n              interval,\n              tuningConfig.getVersioningPolicy().getVersion(interval),\n              new LinearShardSpec(0)\n      );\n      currentOpenSegment = retVal;\n      return retVal;\n    } else if (currentOpenSegment.getInterval().equals(interval)) {\n      retVal = currentOpenSegment;\n      int rowCount = appenderator.getRowCount(retVal);\n      if (rowCount < maxPartitionSize) {\n        return retVal;\n      } else {\n        retVal = new SegmentIdentifier(\n                dataSchema.getDataSource(),\n                interval,\n                tuningConfig.getVersioningPolicy().getVersion(interval),\n                new LinearShardSpec(currentOpenSegment.getShardSpec().getPartitionNum() + 1)\n        );\n        pushSegments(Lists.newArrayList(currentOpenSegment));\n        LOG.info(\"Creating new partition for segment {}, partition num {}\",\n                retVal.getIdentifierAsString(), retVal.getShardSpec().getPartitionNum());\n        currentOpenSegment = retVal;\n        return retVal;\n      }\n    } else {\n      retVal = new SegmentIdentifier(\n              dataSchema.getDataSource(),\n              interval,\n              tuningConfig.getVersioningPolicy().getVersion(interval),\n              new LinearShardSpec(0)\n      );\n      pushSegments(Lists.newArrayList(currentOpenSegment));\n      LOG.info(\"Creating segment {}\", retVal.getIdentifierAsString());\n      currentOpenSegment = retVal;\n      return retVal;\n    }\n  }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  /**\n   * This function computes the segment identifier and push the current open segment\n   * The push will occur if max size is reached or the event belongs to the next interval.\n   * Note that this function assumes that timestamps are pseudo sorted.\n   * This function will close and move to the next segment granularity as soon as\n   * an event from the next interval appears. The sorting is done by the previous stage.\n   *\n   * @return segmentIdentifier with of the truncatedTime and maybe push the current open segment.\n   */\n  private SegmentIdentifier getSegmentIdentifierAndMaybePush(long truncatedTime) {\n    final Interval interval = new Interval(\n            new DateTime(truncatedTime),\n            segmentGranularity.increment(new DateTime(truncatedTime))\n    );\n\n    SegmentIdentifier retVal;\n    if (currentOpenSegment == null) {\n      currentOpenSegment = new SegmentIdentifier(\n              dataSchema.getDataSource(),\n              interval,\n              tuningConfig.getVersioningPolicy().getVersion(interval),\n              new LinearShardSpec(0)\n      );\n      return currentOpenSegment;\n    } else if (currentOpenSegment.getInterval().equals(interval)) {\n      retVal = currentOpenSegment;\n      int rowCount = appenderator.getRowCount(retVal);\n      if (rowCount < maxPartitionSize) {\n        return retVal;\n      } else {\n        retVal = new SegmentIdentifier(\n                dataSchema.getDataSource(),\n                interval,\n                tuningConfig.getVersioningPolicy().getVersion(interval),\n                new LinearShardSpec(currentOpenSegment.getShardSpec().getPartitionNum() + 1)\n        );\n        pushSegments(Lists.newArrayList(currentOpenSegment));\n        LOG.info(\"Creating new partition for segment {}, partition num {}\",\n                retVal.getIdentifierAsString(), retVal.getShardSpec().getPartitionNum());\n        currentOpenSegment = retVal;\n        return retVal;\n      }\n    } else {\n      retVal = new SegmentIdentifier(\n              dataSchema.getDataSource(),\n              interval,\n              tuningConfig.getVersioningPolicy().getVersion(interval),\n              new LinearShardSpec(0)\n      );\n      pushSegments(Lists.newArrayList(currentOpenSegment));\n      LOG.info(\"Creating segment {}\", retVal.getIdentifierAsString());\n      currentOpenSegment = retVal;\n      return retVal;\n    }\n  }"
        ],
        [
            "DruidRecordWriter::write(Writable)",
            " 238  \n 239  \n 240  \n 241 -\n 242 -\n 243 -\n 244 -\n 245 -\n 246 -\n 247 -\n 248 -\n 249 -\n 250 -\n 251 -\n 252  \n 253  \n 254  \n 255 -\n 256 -\n 257  \n 258  \n 259  \n 260  ",
            "  @Override\n  public void write(Writable w) throws IOException {\n    DruidWritable record = (DruidWritable) w;\n    final long timestamp = (long) record.getValue().get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN);\n    final long truncatedTime = (long) record.getValue()\n            .get(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME);\n\n    InputRow inputRow = new MapBasedInputRow(\n            timestamp,\n            dataSchema.getParser()\n                    .getParseSpec()\n                    .getDimensionsSpec()\n                    .getDimensionNames(),\n            record.getValue()\n    );\n\n    try {\n      appenderator\n              .add(getSegmentIdentifierAndMaybePush(truncatedTime), inputRow, committerSupplier);\n    } catch (SegmentNotWritableException e) {\n      throw new IOException(e);\n    }\n  }",
            " 237  \n 238  \n 239  \n 240 +\n 241 +\n 242 +\n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248 +\n 249  \n 250  \n 251  \n 252 +\n 253 +\n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273 +\n 274 +\n 275 +\n 276 +\n 277 +\n 278 +\n 279 +\n 280 +\n 281 +\n 282 +\n 283 +\n 284 +\n 285  \n 286  \n 287  \n 288  ",
            "  @Override\n  public void write(Writable w) throws IOException {\n    DruidWritable record = (DruidWritable) w;\n    final long timestamp =\n        (long) record.getValue().get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN);\n    final long truncatedTime =\n        (long) record.getValue().get(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME);\n    final int partitionNumber = Math.toIntExact(\n        (long) record.getValue().getOrDefault(Constants.DRUID_SHARD_KEY_COL_NAME, -1l));\n    final InputRow inputRow = new MapBasedInputRow(timestamp,\n        dataSchema.getParser().getParseSpec().getDimensionsSpec().getDimensionNames(),\n        record.getValue()\n    );\n\n    try {\n      if (partitionNumber != -1 && maxPartitionSize == -1) {\n        final Interval interval = new Interval(new DateTime(truncatedTime),\n            segmentGranularity.increment(new DateTime(truncatedTime))\n        );\n\n        if (currentOpenSegment != null) {\n          if (currentOpenSegment.getShardSpec().getPartitionNum() != partitionNumber\n              || !currentOpenSegment.getInterval().equals(interval)) {\n            pushSegments(ImmutableList.of(currentOpenSegment));\n            currentOpenSegment = new SegmentIdentifier(dataSchema.getDataSource(), interval,\n                tuningConfig.getVersioningPolicy().getVersion(interval),\n                new LinearShardSpec(partitionNumber)\n            );\n          }\n        } else if (currentOpenSegment == null) {\n          currentOpenSegment = new SegmentIdentifier(dataSchema.getDataSource(), interval,\n              tuningConfig.getVersioningPolicy().getVersion(interval),\n              new LinearShardSpec(partitionNumber)\n          );\n\n        }\n        appenderator.add(currentOpenSegment, inputRow, committerSupplier);\n\n      } else if (partitionNumber == -1 && maxPartitionSize != -1) {\n        appenderator\n            .add(getSegmentIdentifierAndMaybePush(truncatedTime), inputRow, committerSupplier);\n      } else {\n        throw new IllegalArgumentException(String.format(\n            \"partitionNumber and  maxPartitionSize should be mutually exclusive got partitionNum [%s] and maxPartitionSize [%s]\",\n            partitionNumber, maxPartitionSize\n        ));\n      }\n\n    } catch (SegmentNotWritableException e) {\n      throw new IOException(e);\n    }\n  }"
        ],
        [
            "DruidRecordWriter::DruidRecordWriter(DataSchema,RealtimeTuningConfig,DataSegmentPusher,int,Path,FileSystem)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "  public DruidRecordWriter(\n          DataSchema dataSchema,\n          RealtimeTuningConfig realtimeTuningConfig,\n          DataSegmentPusher dataSegmentPusher,\n          int maxPartitionSize,\n          final Path segmentsDescriptorsDir,\n          final FileSystem fileSystem\n  ) {\n    File basePersistDir = new File(realtimeTuningConfig.getBasePersistDirectory(),\n            UUID.randomUUID().toString()\n    );\n    this.tuningConfig = Preconditions\n            .checkNotNull(realtimeTuningConfig.withBasePersistDirectory(basePersistDir),\n                    \"realtimeTuningConfig is null\"\n            );\n    this.dataSchema = Preconditions.checkNotNull(dataSchema, \"data schema is null\");\n\n    appenderator = Appenderators\n            .createOffline(this.dataSchema, tuningConfig, new FireDepartmentMetrics(),\n                    dataSegmentPusher, DruidStorageHandlerUtils.JSON_MAPPER,\n                    DruidStorageHandlerUtils.INDEX_IO, DruidStorageHandlerUtils.INDEX_MERGER_V9\n            );\n    Preconditions.checkArgument(maxPartitionSize > 0, \"maxPartitionSize need to be greater than 0\");\n    this.maxPartitionSize = maxPartitionSize;\n    appenderator.startJob(); // maybe we need to move this out of the constructor\n    this.segmentsDescriptorDir = Preconditions\n            .checkNotNull(segmentsDescriptorsDir, \"segmentsDescriptorsDir is null\");\n    this.fileSystem = Preconditions.checkNotNull(fileSystem, \"file system is null\");\n    committerSupplier = Suppliers.ofInstance(Committers.nil());\n  }",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118 +\n 119  \n 120  ",
            "  public DruidRecordWriter(\n          DataSchema dataSchema,\n          RealtimeTuningConfig realtimeTuningConfig,\n          DataSegmentPusher dataSegmentPusher,\n          int maxPartitionSize,\n          final Path segmentsDescriptorsDir,\n          final FileSystem fileSystem\n  ) {\n    File basePersistDir = new File(realtimeTuningConfig.getBasePersistDirectory(),\n            UUID.randomUUID().toString()\n    );\n    this.tuningConfig = Preconditions\n            .checkNotNull(realtimeTuningConfig.withBasePersistDirectory(basePersistDir),\n                    \"realtimeTuningConfig is null\"\n            );\n    this.dataSchema = Preconditions.checkNotNull(dataSchema, \"data schema is null\");\n\n    appenderator = Appenderators\n            .createOffline(this.dataSchema, tuningConfig, new FireDepartmentMetrics(),\n                    dataSegmentPusher, DruidStorageHandlerUtils.JSON_MAPPER,\n                    DruidStorageHandlerUtils.INDEX_IO, DruidStorageHandlerUtils.INDEX_MERGER_V9\n            );\n    this.maxPartitionSize = maxPartitionSize;\n    appenderator.startJob(); // maybe we need to move this out of the constructor\n    this.segmentsDescriptorDir = Preconditions\n            .checkNotNull(segmentsDescriptorsDir, \"segmentsDescriptorsDir is null\");\n    this.fileSystem = Preconditions.checkNotNull(fileSystem, \"file system is null\");\n    this.segmentGranularity = this.dataSchema.getGranularitySpec()\n        .getSegmentGranularity();\n    committerSupplier = Suppliers.ofInstance(Committers.nil());\n  }"
        ],
        [
            "DruidSerDe::serialize(Object,ObjectInspector)",
            " 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 -\n 566 -\n 567  \n 568  \n 569  ",
            "  @Override\n  public Writable serialize(Object o, ObjectInspector objectInspector) throws SerDeException {\n    if (objectInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n              + \" can only serialize struct types, but we got: \"\n              + objectInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objectInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(o);\n    // We deserialize the result\n    Map<String, Object> value = new HashMap<>();\n    for (int i = 0; i < columns.length; i++) {\n      if (values.get(i) == null) {\n        // null, we just add it\n        value.put(columns[i], null);\n        continue;\n      }\n      final Object res;\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMPLOCALTZ:\n          res = ((TimestampLocalTZObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getZonedDateTime().toInstant().toEpochMilli();\n          break;\n        case BYTE:\n          res = ((ByteObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case SHORT:\n          res = ((ShortObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case INT:\n          res = ((IntObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case LONG:\n          res = ((LongObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case FLOAT:\n          res = ((FloatObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case DOUBLE:\n          res = ((DoubleObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        case DECIMAL:\n          res = ((HiveDecimalObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).doubleValue();\n          break;\n        case CHAR:\n          res = ((HiveCharObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getValue();\n          break;\n        case VARCHAR:\n          res = ((HiveVarcharObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getValue();\n          break;\n        case STRING:\n          res = ((StringObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i));\n          break;\n        case BOOLEAN:\n          res = ((BooleanObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n      value.put(columns[i], res);\n    }\n    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n            ((TimestampObjectInspector) fields.get(columns.length).getFieldObjectInspector())\n                    .getPrimitiveJavaObject(values.get(columns.length)).getTime()\n    );\n    return new DruidWritable(value);\n  }",
            " 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 +\n 566 +\n 567 +\n 568 +\n 569 +\n 570 +\n 571  \n 572 +\n 573 +\n 574  \n 575 +\n 576 +\n 577 +\n 578 +\n 579 +\n 580 +\n 581 +\n 582 +\n 583 +\n 584 +\n 585 +\n 586 +\n 587 +\n 588 +\n 589 +\n 590  \n 591  ",
            "  @Override\n  public Writable serialize(Object o, ObjectInspector objectInspector) throws SerDeException {\n    if (objectInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n              + \" can only serialize struct types, but we got: \"\n              + objectInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objectInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(o);\n    // We deserialize the result\n    Map<String, Object> value = new HashMap<>();\n    for (int i = 0; i < columns.length; i++) {\n      if (values.get(i) == null) {\n        // null, we just add it\n        value.put(columns[i], null);\n        continue;\n      }\n      final Object res;\n      switch (types[i].getPrimitiveCategory()) {\n        case TIMESTAMPLOCALTZ:\n          res = ((TimestampLocalTZObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getZonedDateTime().toInstant().toEpochMilli();\n          break;\n        case BYTE:\n          res = ((ByteObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case SHORT:\n          res = ((ShortObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case INT:\n          res = ((IntObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case LONG:\n          res = ((LongObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case FLOAT:\n          res = ((FloatObjectInspector) fields.get(i).getFieldObjectInspector()).get(values.get(i));\n          break;\n        case DOUBLE:\n          res = ((DoubleObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        case DECIMAL:\n          res = ((HiveDecimalObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).doubleValue();\n          break;\n        case CHAR:\n          res = ((HiveCharObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getValue();\n          break;\n        case VARCHAR:\n          res = ((HiveVarcharObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i)).getValue();\n          break;\n        case STRING:\n          res = ((StringObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .getPrimitiveJavaObject(values.get(i));\n          break;\n        case BOOLEAN:\n          res = ((BooleanObjectInspector) fields.get(i).getFieldObjectInspector())\n                  .get(values.get(i));\n          break;\n        default:\n          throw new SerDeException(\"Unknown type: \" + types[i].getPrimitiveCategory());\n      }\n      value.put(columns[i], res);\n    }\n    //Extract the partitions keys segments granularity and partition key if any\n    // First Segment Granularity has to be here.\n    final int granularityFieldIndex = columns.length;\n    assert values.size() > granularityFieldIndex;\n    Preconditions.checkArgument(fields.get(granularityFieldIndex).getFieldName()\n        .equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME));\n    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n            ((TimestampObjectInspector) fields.get(granularityFieldIndex).getFieldObjectInspector())\n                    .getPrimitiveJavaObject(values.get(granularityFieldIndex)).getTime()\n    );\n    if (values.size() == columns.length + 2) {\n      // Then partition number if any.\n      final int partitionNumPos = granularityFieldIndex + 1;\n      Preconditions.checkArgument(\n          fields.get(partitionNumPos).getFieldName().equals(Constants.DRUID_SHARD_KEY_COL_NAME),\n          String.format(\"expecting to encounter %s but was %s\", Constants.DRUID_SHARD_KEY_COL_NAME,\n              fields.get(partitionNumPos).getFieldName()\n          )\n      );\n      value.put(Constants.DRUID_SHARD_KEY_COL_NAME,\n          ((LongObjectInspector) fields.get(partitionNumPos).getFieldObjectInspector())\n              .get(values.get(partitionNumPos))\n      );\n    }\n\n    return new DruidWritable(value);\n  }"
        ],
        [
            "SortedDynPartitionTimeGranularityOptimizer::SortedDynamicPartitionProc::getGranularitySelOp(Operator,String)",
            " 220  \n 221 -\n 222  \n 223 -\n 224 -\n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 -\n 246  \n 247 -\n 248 -\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 -\n 312  \n 313  \n 314 -\n 315  \n 316 -\n 317 -\n 318  \n 319 -\n 320  \n 321  \n 322  \n 323  ",
            "    private Operator<? extends OperatorDesc> getGranularitySelOp(\n            Operator<? extends OperatorDesc> fsParent, String segmentGranularity\n    ) throws SemanticException {\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> descs = Lists.newArrayList();\n      List<String> colNames = Lists.newArrayList();\n      int timestampPos = -1;\n      for (int i = 0; i < parentCols.size(); i++) {\n        ColumnInfo ci = parentCols.get(i);\n        ExprNodeColumnDesc columnDesc = new ExprNodeColumnDesc(ci);\n        descs.add(columnDesc);\n        colNames.add(columnDesc.getExprString());\n        if (columnDesc.getTypeInfo().getCategory() == ObjectInspector.Category.PRIMITIVE\n                && ((PrimitiveTypeInfo) columnDesc.getTypeInfo()).getPrimitiveCategory() == PrimitiveCategory.TIMESTAMPLOCALTZ) {\n          if (timestampPos != -1) {\n            throw new SemanticException(\"Multiple columns with timestamp with local time-zone type on query result; \"\n                    + \"could not resolve which one is the timestamp with local time-zone column\");\n          }\n          timestampPos = i;\n        }\n      }\n      if (timestampPos == -1) {\n        throw new SemanticException(\"No column with timestamp with local time-zone type on query result; \"\n                + \"one column should be of timestamp with local time-zone type\");\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      // Granularity (partition) column\n      String udfName;\n\n      Class<? extends UDF> udfClass;\n      switch (segmentGranularity) {\n        case \"YEAR\":\n          udfName = \"floor_year\";\n          udfClass = UDFDateFloorYear.class;\n          break;\n        case \"MONTH\":\n          udfName = \"floor_month\";\n          udfClass = UDFDateFloorMonth.class;\n          break;\n        case \"WEEK\":\n          udfName = \"floor_week\";\n          udfClass = UDFDateFloorWeek.class;\n          break;\n        case \"DAY\":\n          udfName = \"floor_day\";\n          udfClass = UDFDateFloorDay.class;\n          break;\n        case \"HOUR\":\n          udfName = \"floor_hour\";\n          udfClass = UDFDateFloorHour.class;\n          break;\n        case \"MINUTE\":\n          udfName = \"floor_minute\";\n          udfClass = UDFDateFloorMinute.class;\n          break;\n        case \"SECOND\":\n          udfName = \"floor_second\";\n          udfClass = UDFDateFloorSecond.class;\n          break;\n        default:\n          throw new SemanticException(\"Granularity for Druid segment not recognized\");\n      }\n\n      // Timestamp column type in Druid is timestamp with local time-zone, as it represents\n      // a specific instant in time. Thus, we have this value and we need to extract the\n      // granularity to split the data when we are storing it in Druid. However, Druid stores\n      // the data in UTC. Thus, we need to apply the following logic on the data to extract\n      // the granularity correctly:\n      // 1) Read the timestamp with local time-zone value.\n      // 2) Extract UTC epoch (millis) from timestamp with local time-zone.\n      // 3) Cast the long to a timestamp.\n      // 4) Apply the granularity function on the timestamp value.\n      // That way, '2010-01-01 00:00:00 UTC' and '2009-12-31 16:00:00 PST' (same instant)\n      // will end up in the same Druid segment.\n\n      // #1 - Read the column value\n      ExprNodeDesc expr = new ExprNodeColumnDesc(parentCols.get(timestampPos));\n      // #2 - UTC epoch for instant\n      ExprNodeGenericFuncDesc f1 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.longTypeInfo, new GenericUDFEpochMilli(), Lists.newArrayList(expr));\n      // #3 - Cast to timestamp\n      ExprNodeGenericFuncDesc f2 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.timestampTypeInfo, new GenericUDFTimestamp(), Lists.newArrayList(f1));\n      // #4 - We apply the granularity function\n      ExprNodeGenericFuncDesc f3 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.timestampTypeInfo,\n          new GenericUDFBridge(udfName, false, udfClass.getName()),\n          Lists.newArrayList(f2));\n      descs.add(f3);\n      colNames.add(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME);\n      // Add granularity to the row schema\n      ColumnInfo ci = new ColumnInfo(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME, TypeInfoFactory.timestampTypeInfo,\n              selRS.getSignature().get(0).getTabAlias(), false, false);\n      selRS.getSignature().add(ci);\n\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, fsParent);\n\n      return selOp;\n    }",
            " 261  \n 262 +\n 263 +\n 264  \n 265 +\n 266 +\n 267 +\n 268 +\n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 +\n 289  \n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 +\n 323 +\n 324 +\n 325 +\n 326  \n 327  \n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 +\n 358  \n 359  \n 360 +\n 361 +\n 362 +\n 363 +\n 364 +\n 365 +\n 366 +\n 367 +\n 368 +\n 369 +\n 370 +\n 371 +\n 372 +\n 373 +\n 374 +\n 375 +\n 376 +\n 377 +\n 378 +\n 379 +\n 380 +\n 381 +\n 382 +\n 383 +\n 384 +\n 385 +\n 386  \n 387 +\n 388  \n 389 +\n 390  \n 391  \n 392  \n 393  ",
            "    private Operator<? extends OperatorDesc> getGranularitySelOp(\n        Operator<? extends OperatorDesc> fsParent,\n        String segmentGranularity\n    ) throws SemanticException {\n      final ArrayList<ColumnInfo> parentCols =\n          Lists.newArrayList(fsParent.getSchema().getSignature());\n      final ArrayList<ExprNodeDesc> descs = Lists.newArrayList();\n      final List<String> colNames = Lists.newArrayList();\n      int timestampPos = -1;\n      for (int i = 0; i < parentCols.size(); i++) {\n        ColumnInfo ci = parentCols.get(i);\n        ExprNodeColumnDesc columnDesc = new ExprNodeColumnDesc(ci);\n        descs.add(columnDesc);\n        colNames.add(columnDesc.getExprString());\n        if (columnDesc.getTypeInfo().getCategory() == ObjectInspector.Category.PRIMITIVE\n                && ((PrimitiveTypeInfo) columnDesc.getTypeInfo()).getPrimitiveCategory() == PrimitiveCategory.TIMESTAMPLOCALTZ) {\n          if (timestampPos != -1) {\n            throw new SemanticException(\"Multiple columns with timestamp with local time-zone type on query result; \"\n                    + \"could not resolve which one is the timestamp with local time-zone column\");\n          }\n          timestampPos = i;\n        }\n      }\n      if (timestampPos == -1) {\n        throw new SemanticException(\"No column with timestamp with local time-zone type on query result; \"\n                + \"one column should be of timestamp with local time-zone type\");\n      }\n      final RowSchema selRS = new RowSchema(fsParent.getSchema());\n      // Granularity (partition) column\n      final String udfName;\n      Class<? extends UDF> udfClass;\n      switch (segmentGranularity) {\n        case \"YEAR\":\n          udfName = \"floor_year\";\n          udfClass = UDFDateFloorYear.class;\n          break;\n        case \"MONTH\":\n          udfName = \"floor_month\";\n          udfClass = UDFDateFloorMonth.class;\n          break;\n        case \"WEEK\":\n          udfName = \"floor_week\";\n          udfClass = UDFDateFloorWeek.class;\n          break;\n        case \"DAY\":\n          udfName = \"floor_day\";\n          udfClass = UDFDateFloorDay.class;\n          break;\n        case \"HOUR\":\n          udfName = \"floor_hour\";\n          udfClass = UDFDateFloorHour.class;\n          break;\n        case \"MINUTE\":\n          udfName = \"floor_minute\";\n          udfClass = UDFDateFloorMinute.class;\n          break;\n        case \"SECOND\":\n          udfName = \"floor_second\";\n          udfClass = UDFDateFloorSecond.class;\n          break;\n        default:\n          throw new SemanticException(String.format(Locale.ENGLISH,\n              \"Unknown Druid Granularity [%s], Accepted values are [YEAR, MONTH, WEEK, DAY, HOUR, MINUTE, SECOND]\",\n              segmentGranularity\n          ));\n      }\n\n\n      // Timestamp column type in Druid is timestamp with local time-zone, as it represents\n      // a specific instant in time. Thus, we have this value and we need to extract the\n      // granularity to split the data when we are storing it in Druid. However, Druid stores\n      // the data in UTC. Thus, we need to apply the following logic on the data to extract\n      // the granularity correctly:\n      // 1) Read the timestamp with local time-zone value.\n      // 2) Extract UTC epoch (millis) from timestamp with local time-zone.\n      // 3) Cast the long to a timestamp.\n      // 4) Apply the granularity function on the timestamp value.\n      // That way, '2010-01-01 00:00:00 UTC' and '2009-12-31 16:00:00 PST' (same instant)\n      // will end up in the same Druid segment.\n\n      // #1 - Read the column value\n      ExprNodeDesc expr = new ExprNodeColumnDesc(parentCols.get(timestampPos));\n      // #2 - UTC epoch for instant\n      ExprNodeGenericFuncDesc f1 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.longTypeInfo, new GenericUDFEpochMilli(), Lists.newArrayList(expr));\n      // #3 - Cast to timestamp\n      ExprNodeGenericFuncDesc f2 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.timestampTypeInfo, new GenericUDFTimestamp(), Lists.newArrayList(f1));\n      // #4 - We apply the granularity function\n      ExprNodeGenericFuncDesc f3 = new ExprNodeGenericFuncDesc(\n          TypeInfoFactory.timestampTypeInfo,\n          new GenericUDFBridge(udfName, false, udfClass.getName()),\n          Lists.newArrayList(f2));\n      descs.add(f3);\n      colNames.add(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME);\n      // Add granularity to the row schema\n      final ColumnInfo ci = new ColumnInfo(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME, TypeInfoFactory.timestampTypeInfo,\n              selRS.getSignature().get(0).getTabAlias(), false, false);\n      selRS.getSignature().add(ci);\n      if (targetShardsPerGranularity > 0 ) {\n        // add another partitioning key based on floor(1/rand) % targetShardsPerGranularity\n        final ColumnInfo partitionKeyCi =\n            new ColumnInfo(Constants.DRUID_SHARD_KEY_COL_NAME, TypeInfoFactory.longTypeInfo,\n                selRS.getSignature().get(0).getTabAlias(), false, false\n            );\n        final ExprNodeDesc targetNumShardDescNode =\n            new ExprNodeConstantDesc(TypeInfoFactory.intTypeInfo, targetShardsPerGranularity);\n        final ExprNodeGenericFuncDesc randomFn = ExprNodeGenericFuncDesc\n            .newInstance(new GenericUDFBridge(\"rand\", false, UDFRand.class.getName()),\n                Lists.newArrayList()\n            );\n\n        final ExprNodeGenericFuncDesc random = ExprNodeGenericFuncDesc.newInstance(\n            new GenericUDFFloor(), Lists.newArrayList(ExprNodeGenericFuncDesc\n                .newInstance(new GenericUDFOPDivide(),\n                    Lists.newArrayList(new ExprNodeConstantDesc(TypeInfoFactory.doubleTypeInfo, 1.0), randomFn)\n                )));\n        final ExprNodeGenericFuncDesc randModMax = ExprNodeGenericFuncDesc\n            .newInstance(new GenericUDFOPMod(),\n                Lists.newArrayList(random, targetNumShardDescNode)\n            );\n        descs.add(randModMax);\n        colNames.add(Constants.DRUID_SHARD_KEY_COL_NAME);\n        selRS.getSignature().add(partitionKeyCi);\n      }\n      // Create SelectDesc\n      final SelectDesc selConf = new SelectDesc(descs, colNames);\n      // Create Select Operator\n      final SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, fsParent);\n\n      return selOp;\n    }"
        ],
        [
            "SortedDynPartitionTimeGranularityOptimizer::SortedDynamicPartitionProc::getReduceSinkOp(List,List,List,ArrayList,Operator)",
            " 325  \n 326  \n 327 -\n 328 -\n 329 -\n 330  \n 331  \n 332 -\n 333 -\n 334 -\n 335 -\n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  \n 343 -\n 344 -\n 345 -\n 346 -\n 347  \n 348  \n 349  \n 350  \n 351 -\n 352 -\n 353 -\n 354 -\n 355 -\n 356 -\n 357 -\n 358 -\n 359 -\n 360 -\n 361 -\n 362 -\n 363 -\n 364 -\n 365  \n 366  \n 367 -\n 368 -\n 369  \n 370  \n 371  \n 372  \n 373 -\n 374  \n 375 -\n 376  \n 377  \n 378 -\n 379  \n 380  \n 381  \n 382 -\n 383  \n 384  \n 385  \n 386 -\n 387 -\n 388 -\n 389 -\n 390 -\n 391 -\n 392 -\n 393  \n 394  \n 395  \n 396  ",
            "    private ReduceSinkOperator getReduceSinkOp(List<Integer> keyPositions, List<Integer> sortOrder,\n        List<Integer> sortNullOrder, ArrayList<ExprNodeDesc> allCols, Operator<? extends OperatorDesc> parent\n    ) throws SemanticException {\n\n      ArrayList<ExprNodeDesc> keyCols = Lists.newArrayList();\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      for (Integer idx : keyPositions) {\n        keyCols.add(allCols.get(idx).clone());\n      }\n\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (!keyPositions.contains(i)) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      ArrayList<ExprNodeDesc> partCols = Lists.newArrayList();\n      for (Integer idx : keyPositions) {\n        partCols.add(allCols.get(idx).clone());\n      }\n\n      // map _col0 to KEY._col0, etc\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      Map<String, String> nameMapping = new HashMap<>();\n      ArrayList<String> keyColNames = Lists.newArrayList();\n      for (ExprNodeDesc keyCol : keyCols) {\n        String keyColName = keyCol.getExprString();\n        keyColNames.add(keyColName);\n        colExprMap.put(Utilities.ReduceField.KEY + \".\" +keyColName, keyCol);\n        nameMapping.put(keyColName, Utilities.ReduceField.KEY + \".\" + keyColName);\n      }\n      ArrayList<String> valColNames = Lists.newArrayList();\n      for (ExprNodeDesc valCol : valCols) {\n        String colName = valCol.getExprString();\n        valColNames.add(colName);\n        colExprMap.put(Utilities.ReduceField.VALUE + \".\" + colName, valCol);\n        nameMapping.put(colName, Utilities.ReduceField.VALUE + \".\" + colName);\n      }\n\n      // order and null order\n      String orderStr = StringUtils.repeat(\"+\", sortOrder.size());\n      String nullOrderStr = StringUtils.repeat(\"a\", sortNullOrder.size());\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable);\n\n      ArrayList<ColumnInfo> signature = new ArrayList<>();\n      for (int index = 0; index < parent.getSchema().getSignature().size(); index++) {\n        ColumnInfo colInfo = new ColumnInfo(parent.getSchema().getSignature().get(index));\n        colInfo.setInternalName(nameMapping.get(colInfo.getInternalName()));\n        signature.add(colInfo);\n      }\n      ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }",
            " 395  \n 396  \n 397 +\n 398  \n 399  \n 400 +\n 401 +\n 402 +\n 403  \n 404  \n 405 +\n 406  \n 407  \n 408  \n 409  \n 410 +\n 411 +\n 412 +\n 413  \n 414  \n 415  \n 416  \n 417 +\n 418 +\n 419 +\n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428 +\n 429 +\n 430 +\n 431 +\n 432 +\n 433 +\n 434 +\n 435 +\n 436  \n 437  \n 438 +\n 439 +\n 440  \n 441  \n 442  \n 443  \n 444 +\n 445  \n 446 +\n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453 +\n 454  \n 455  \n 456  \n 457 +\n 458 +\n 459 +\n 460 +\n 461 +\n 462 +\n 463 +\n 464 +\n 465 +\n 466 +\n 467 +\n 468  \n 469  \n 470  \n 471  ",
            "    private ReduceSinkOperator getReduceSinkOp(List<Integer> keyPositions, List<Integer> sortOrder,\n        List<Integer> sortNullOrder, ArrayList<ExprNodeDesc> allCols, Operator<? extends OperatorDesc> parent\n    ) {\n      // we will clone here as RS will update bucket column key with its\n      // corresponding with bucket number and hence their OIs\n      final ArrayList<ExprNodeDesc> keyCols = keyPositions.stream()\n          .map(id -> allCols.get(id).clone())\n          .collect(Collectors.toCollection(ArrayList::new));\n      ArrayList<ExprNodeDesc> valCols = Lists.newArrayList();\n      for (int i = 0; i < allCols.size(); i++) {\n        if (i != granularityKeyPos && i != partitionKeyPos) {\n          valCols.add(allCols.get(i).clone());\n        }\n      }\n\n      final ArrayList<ExprNodeDesc> partCols =\n          keyPositions.stream().map(id -> allCols.get(id).clone())\n              .collect(Collectors.toCollection(ArrayList::new));\n\n      // map _col0 to KEY._col0, etc\n      Map<String, ExprNodeDesc> colExprMap = Maps.newHashMap();\n      Map<String, String> nameMapping = new HashMap<>();\n      final ArrayList<String> keyColNames = Lists.newArrayList();\n      final ArrayList<String> valColNames = Lists.newArrayList();\n      keyCols.stream().forEach(exprNodeDesc -> {\n        keyColNames.add(exprNodeDesc.getExprString());\n        colExprMap\n            .put(Utilities.ReduceField.KEY + \".\" + exprNodeDesc.getExprString(), exprNodeDesc);\n        nameMapping.put(exprNodeDesc.getExprString(),\n            Utilities.ReduceField.KEY + \".\" + exprNodeDesc.getName()\n        );\n      });\n      valCols.stream().forEach(exprNodeDesc -> {\n        valColNames.add(exprNodeDesc.getExprString());\n        colExprMap\n            .put(Utilities.ReduceField.VALUE + \".\" + exprNodeDesc.getExprString(), exprNodeDesc);\n        nameMapping.put(exprNodeDesc.getExprString(),\n            Utilities.ReduceField.VALUE + \".\" + exprNodeDesc.getName()\n        );\n      });\n\n\n      // order and null order\n      final String orderStr = StringUtils.repeat(\"+\", sortOrder.size());\n      final String nullOrderStr = StringUtils.repeat(\"a\", sortNullOrder.size());\n\n      // Create Key/Value TableDesc. When the operator plan is split into MR tasks,\n      // the reduce operator will initialize Extract operator with information\n      // from Key and Value TableDesc\n      final List<FieldSchema> fields = PlanUtils.getFieldSchemasFromColumnList(keyCols,\n          keyColNames, 0, \"\");\n      final TableDesc keyTable = PlanUtils.getReduceKeyTableDesc(fields, orderStr, nullOrderStr);\n      List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(valCols,\n          valColNames, 0, \"\");\n      final TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);\n      List<List<Integer>> distinctColumnIndices = Lists.newArrayList();\n\n      // Number of reducers is set to default (-1)\n      final ReduceSinkDesc rsConf = new ReduceSinkDesc(keyCols, keyCols.size(), valCols,\n          keyColNames, distinctColumnIndices, valColNames, -1, partCols, -1, keyTable,\n          valueTable);\n\n      final ArrayList<ColumnInfo> signature =\n          parent.getSchema().getSignature()\n              .stream()\n              .map(e -> new ColumnInfo(e))\n              .map(columnInfo ->\n                {\n                  columnInfo.setInternalName(nameMapping.get(columnInfo.getInternalName()));\n                  return columnInfo;\n               })\n              .collect(Collectors.toCollection(ArrayList::new));\n      final ReduceSinkOperator op = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n          rsConf, new RowSchema(signature), parent);\n      op.setColumnExprMap(colExprMap);\n      return op;\n    }"
        ]
    ],
    "a926179f3d17223886a77e6c9733199e5f8b2b58": [
        [
            "TezCompiler::optimizeOperatorPlan(ParseContext,Set,Set)",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "  @Override\n  protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    // Create the context for the walker\n    OptimizeTezProcContext procCtx = new OptimizeTezProcContext(conf, pCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup dynamic partition pruning where possible\n    runDynamicPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup dynamic partition pruning\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup stats in the operator plan\n    runStatsAnnotation(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup stats in the operator plan\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // run the optimizations that use stats for optimization\n    runStatsDependentOptimizations(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTJOINREDUCEDEDUPLICATION)) {\n      new ReduceSinkJoinDeDuplication().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run reduce sink after join algorithm selection\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    runRemoveDynamicPruningOptimization(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run remove dynamic pruning by size\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    markSemiJoinForDPP(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Mark certain semijoin edges important based \");\n\n    // Removing semijoin optimization when it may not be beneficial\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    removeSemijoinOptimizationByBenefit(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove Semijoins based on cost benefits\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove any parallel edge between semijoin and mapjoin.\n    removeSemijoinsParallelToMapJoin(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove semijoin optimization if it creates a cycle with mapside joins\n    removeSemiJoinCyclesDueToMapsideJoins(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove semijoin optimizations if it creates a cycle with mapside join\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove semijoin optimization if SMB join is created.\n    removeSemijoinOptimizationFromSMBJoins(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove semijoin optimizations if needed\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove bloomfilter if no stats generated\n    removeSemiJoinIfNoStats(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove bloom filter optimizations if needed\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // after the stats phase we might have some cyclic dependencies that we need\n    // to take care of.\n    runCycleAnalysisForPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run cycle analysis for partition pruning\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVE_SHARED_WORK_OPTIMIZATION)) {\n      new SharedWorkOptimizer().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Shared scans optimization\");\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    // Rather than run the full constant folding just need to shortcut AND/OR expressions\n    // involving constant true/false values.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n  }",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 +\n 149 +\n 150 +\n 151 +\n 152 +\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "  @Override\n  protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    // Create the context for the walker\n    OptimizeTezProcContext procCtx = new OptimizeTezProcContext(conf, pCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup dynamic partition pruning where possible\n    runDynamicPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup dynamic partition pruning\");\n\n    // need to run this; to get consistent filterop conditions(for operator tree matching)\n    if (procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup stats in the operator plan\n    runStatsAnnotation(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup stats in the operator plan\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // run the optimizations that use stats for optimization\n    runStatsDependentOptimizations(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTJOINREDUCEDEDUPLICATION)) {\n      new ReduceSinkJoinDeDuplication().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run reduce sink after join algorithm selection\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    runRemoveDynamicPruningOptimization(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run remove dynamic pruning by size\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    markSemiJoinForDPP(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Mark certain semijoin edges important based \");\n\n    // Removing semijoin optimization when it may not be beneficial\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    removeSemijoinOptimizationByBenefit(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove Semijoins based on cost benefits\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove any parallel edge between semijoin and mapjoin.\n    removeSemijoinsParallelToMapJoin(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove semijoin optimization if it creates a cycle with mapside joins\n    removeSemiJoinCyclesDueToMapsideJoins(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove semijoin optimizations if it creates a cycle with mapside join\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove semijoin optimization if SMB join is created.\n    removeSemijoinOptimizationFromSMBJoins(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove semijoin optimizations if needed\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // Remove bloomfilter if no stats generated\n    removeSemiJoinIfNoStats(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Remove bloom filter optimizations if needed\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // after the stats phase we might have some cyclic dependencies that we need\n    // to take care of.\n    runCycleAnalysisForPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run cycle analysis for partition pruning\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVE_SHARED_WORK_OPTIMIZATION)) {\n      new SharedWorkOptimizer().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Shared scans optimization\");\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    // Rather than run the full constant folding just need to shortcut AND/OR expressions\n    // involving constant true/false values.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n  }"
        ],
        [
            "TezCompiler::SemiJoinRemovalIfNoStatsProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n      if (sjInfo == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      for (AggregationDesc agg : aggregationDescs) {\n        if (!\"bloom_filter\".equals(agg.getGenericUDAFName())) {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        if (udafBloomFilterEvaluator.hasHintEntries())\n          return null; // Created using hint, skip it\n\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          if (sjInfo.getIsHint()) {\n            throw new SemanticException(\"Removing hinted semijoin due to lack to stats\" +\n            \" or exceeding max bloom filter entries\");\n          }\n          // Remove the semijoin optimization branch along with ALL the mappings\n          // The parent GB2 has all the branches. Collect them and remove them.\n          for (Node node : gbOp.getChildren()) {\n            ReduceSinkOperator rsFinal = (ReduceSinkOperator) node;\n            TableScanOperator ts = pCtx.getRsToSemiJoinBranchInfo().\n                    get(rsFinal).getTsOp();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                      + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                      + \"Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rsFinal);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rsFinal, ts);\n          }\n          return null;\n        }\n      }\n\n      // At this point, hinted semijoin case has been handled already\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      TableScanOperator ts = sjInfo.getTsOp();\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          if (sjInfo.getShouldRemove()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n      return null;\n    }",
            " 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937 +\n 938  \n 939 +\n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n                          Object... nodeOutputs) throws SemanticException {\n      assert nd instanceof ReduceSinkOperator;\n      ReduceSinkOperator rs = (ReduceSinkOperator) nd;\n      ParseContext pCtx = ((OptimizeTezProcContext) procCtx).parseContext;\n      SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n      if (sjInfo == null) {\n        // nothing to do here.\n        return null;\n      }\n\n      // This is a semijoin branch. The stack should look like,\n      // <Parent Ops>-SEL-GB1-RS1-GB2-RS2\n      GroupByOperator gbOp = (GroupByOperator) (stack.get(stack.size() - 2));\n      GroupByDesc gbDesc = gbOp.getConf();\n      ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();\n      for (AggregationDesc agg : aggregationDescs) {\n        if (!\"bloom_filter\".equals(agg.getGenericUDAFName())) {\n          continue;\n        }\n\n        GenericUDAFBloomFilterEvaluator udafBloomFilterEvaluator =\n                (GenericUDAFBloomFilterEvaluator) agg.getGenericUDAFEvaluator();\n        if (udafBloomFilterEvaluator.hasHintEntries())\n         {\n          return null; // Created using hint, skip it\n        }\n\n        long expectedEntries = udafBloomFilterEvaluator.getExpectedEntries();\n        if (expectedEntries == -1 || expectedEntries >\n                pCtx.getConf().getLongVar(ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES)) {\n          if (sjInfo.getIsHint()) {\n            throw new SemanticException(\"Removing hinted semijoin due to lack to stats\" +\n            \" or exceeding max bloom filter entries\");\n          }\n          // Remove the semijoin optimization branch along with ALL the mappings\n          // The parent GB2 has all the branches. Collect them and remove them.\n          for (Node node : gbOp.getChildren()) {\n            ReduceSinkOperator rsFinal = (ReduceSinkOperator) node;\n            TableScanOperator ts = pCtx.getRsToSemiJoinBranchInfo().\n                    get(rsFinal).getTsOp();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"expectedEntries=\" + expectedEntries + \". \"\n                      + \"Either stats unavailable or expectedEntries exceeded max allowable bloomfilter size. \"\n                      + \"Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rsFinal);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rsFinal, ts);\n          }\n          return null;\n        }\n      }\n\n      // At this point, hinted semijoin case has been handled already\n      // Check if big table is big enough that runtime filtering is\n      // worth it.\n      TableScanOperator ts = sjInfo.getTsOp();\n      if (ts.getStatistics() != null) {\n        long numRows = ts.getStatistics().getNumRows();\n        if (numRows < pCtx.getConf().getLongVar(ConfVars.TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION)) {\n          if (sjInfo.getShouldRemove()) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Insufficient rows (\" + numRows + \") to justify semijoin optimization. Removing semijoin \"\n                      + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n      return null;\n    }"
        ],
        [
            "TezCompiler::findParallelSemiJoinBranch(Operator,TableScanOperator,ParseContext,Map)",
            " 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055 -\n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  ",
            "  private boolean findParallelSemiJoinBranch(Operator<?> mapjoin, TableScanOperator bigTableTS,\n                                             ParseContext parseContext,\n                                             Map<ReduceSinkOperator, TableScanOperator> semijoins) {\n\n    boolean parallelEdges = false;\n    for (Operator<?> op : mapjoin.getParentOperators()) {\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      op = op.getParentOperators().get(0);\n\n      // Follow the Reducesink operator upstream which is on small table side.\n      while (!(op instanceof ReduceSinkOperator) &&\n              !(op instanceof TableScanOperator) &&\n              !(op.getChildren() != null && op.getChildren().size() > 1)) {\n        if (op instanceof MapJoinOperator) {\n          // Pick the correct parent, only one of the parents is not\n          // ReduceSink, that is what we are looking for.\n          for (Operator<?> parentOp : op.getParentOperators()) {\n            if (parentOp instanceof ReduceSinkOperator) {\n              continue;\n            }\n            op = parentOp; // parent in current pipeline\n            continue;\n          }\n        }\n        op = op.getParentOperators().get(0);\n      }\n\n      // Bail out if RS or TS is encountered.\n      if (op instanceof ReduceSinkOperator || op instanceof TableScanOperator) {\n        continue;\n      }\n\n      // A branch is hit.\n      for (Node nd : op.getChildren()) {\n        if (nd instanceof SelectOperator) {\n          Operator<?> child = (Operator<?>) nd;\n\n          while (child.getChildOperators().size() > 0) {\n            child = child.getChildOperators().get(0);\n          }\n\n          // If not ReduceSink Op, skip\n          if (!(child instanceof ReduceSinkOperator)) {\n            // This still could be DPP.\n            if (child instanceof AppMasterEventOperator &&\n                    ((AppMasterEventOperator) child).getConf() instanceof DynamicPruningEventDesc) {\n              // DPP indeed, Set parallel edges true\n              parallelEdges = true;\n            }\n            continue;\n          }\n\n          ReduceSinkOperator rs = (ReduceSinkOperator) child;\n          SemiJoinBranchInfo sjInfo = parseContext.getRsToSemiJoinBranchInfo().get(rs);\n          if (sjInfo == null) continue;\n\n          TableScanOperator ts = sjInfo.getTsOp();\n          if (ts != bigTableTS) {\n            // skip, not the one we are looking for.\n            continue;\n          }\n\n          parallelEdges = true;\n\n          if (sjInfo.getIsHint() || !sjInfo.getShouldRemove()) {\n            // Created by hint, skip it\n            continue;\n          }\n          // Add the semijoin branch to the map\n          semijoins.put(rs, ts);\n        }\n      }\n    }\n    return parallelEdges;\n  }",
            "1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066 +\n1067 +\n1068 +\n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  ",
            "  private boolean findParallelSemiJoinBranch(Operator<?> mapjoin, TableScanOperator bigTableTS,\n                                             ParseContext parseContext,\n                                             Map<ReduceSinkOperator, TableScanOperator> semijoins) {\n\n    boolean parallelEdges = false;\n    for (Operator<?> op : mapjoin.getParentOperators()) {\n      if (!(op instanceof ReduceSinkOperator)) {\n        continue;\n      }\n\n      op = op.getParentOperators().get(0);\n\n      // Follow the Reducesink operator upstream which is on small table side.\n      while (!(op instanceof ReduceSinkOperator) &&\n              !(op instanceof TableScanOperator) &&\n              !(op.getChildren() != null && op.getChildren().size() > 1)) {\n        if (op instanceof MapJoinOperator) {\n          // Pick the correct parent, only one of the parents is not\n          // ReduceSink, that is what we are looking for.\n          for (Operator<?> parentOp : op.getParentOperators()) {\n            if (parentOp instanceof ReduceSinkOperator) {\n              continue;\n            }\n            op = parentOp; // parent in current pipeline\n            continue;\n          }\n        }\n        op = op.getParentOperators().get(0);\n      }\n\n      // Bail out if RS or TS is encountered.\n      if (op instanceof ReduceSinkOperator || op instanceof TableScanOperator) {\n        continue;\n      }\n\n      // A branch is hit.\n      for (Node nd : op.getChildren()) {\n        if (nd instanceof SelectOperator) {\n          Operator<?> child = (Operator<?>) nd;\n\n          while (child.getChildOperators().size() > 0) {\n            child = child.getChildOperators().get(0);\n          }\n\n          // If not ReduceSink Op, skip\n          if (!(child instanceof ReduceSinkOperator)) {\n            // This still could be DPP.\n            if (child instanceof AppMasterEventOperator &&\n                    ((AppMasterEventOperator) child).getConf() instanceof DynamicPruningEventDesc) {\n              // DPP indeed, Set parallel edges true\n              parallelEdges = true;\n            }\n            continue;\n          }\n\n          ReduceSinkOperator rs = (ReduceSinkOperator) child;\n          SemiJoinBranchInfo sjInfo = parseContext.getRsToSemiJoinBranchInfo().get(rs);\n          if (sjInfo == null) {\n            continue;\n          }\n\n          TableScanOperator ts = sjInfo.getTsOp();\n          if (ts != bigTableTS) {\n            // skip, not the one we are looking for.\n            continue;\n          }\n\n          parallelEdges = true;\n\n          if (sjInfo.getIsHint() || !sjInfo.getShouldRemove()) {\n            // Created by hint, skip it\n            continue;\n          }\n          // Add the semijoin branch to the map\n          semijoins.put(rs, ts);\n        }\n      }\n    }\n    return parallelEdges;\n  }"
        ],
        [
            "TezCompiler::removeSemiJoinCyclesDueToMapsideJoins(OptimizeTezProcContext)",
            " 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869 -\n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsToSemiJoinBranchInfo().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n        if (sjInfo == null) continue;\n\n        TableScanOperator ts = sjInfo.getTsOp();\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            if (sjInfo.getIsHint()) {\n              throw new SemanticException(\"Removing hinted semijoin as it is creating cycles with mapside joins \" + rs + \" : \" + ts);\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin cycle due to mapjoin. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }",
            " 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876 +\n 877 +\n 878 +\n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  ",
            "  private static void removeSemiJoinCyclesDueToMapsideJoins(\n          OptimizeTezProcContext procCtx) throws SemanticException {\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            procCtx.parseContext.getRsToSemiJoinBranchInfo().size() == 0) {\n      return;\n    }\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n            new RuleRegExp(\"R1\", MapJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R2\", MapJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R3\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    MapJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n    opRules.put(\n            new RuleRegExp(\"R4\", CommonMergeJoinOperator.getOperatorName() + \"%\" +\n                    CommonMergeJoinOperator.getOperatorName() + \"%\"),\n            new SemiJoinCycleRemovalDueToMapsideJoins());\n\n    SemiJoinCycleRemovalDueTOMapsideJoinContext ctx =\n            new SemiJoinCycleRemovalDueTOMapsideJoinContext();\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, ctx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new PreOrderOnceWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // process the list\n    ParseContext pCtx = procCtx.parseContext;\n    for (Operator<?> parentJoin : ctx.childParentMap.keySet()) {\n      Operator<?> childJoin = ctx.childParentMap.get(parentJoin);\n\n      if (parentJoin.getChildOperators().size() == 1) {\n        continue;\n      }\n\n      for (Operator<?> child : parentJoin.getChildOperators()) {\n        if (!(child instanceof SelectOperator)) {\n          continue;\n        }\n\n        while(child.getChildOperators().size() > 0) {\n          child = child.getChildOperators().get(0);\n        }\n\n        if (!(child instanceof ReduceSinkOperator)) {\n          continue;\n        }\n\n        ReduceSinkOperator rs = ((ReduceSinkOperator) child);\n        SemiJoinBranchInfo sjInfo = pCtx.getRsToSemiJoinBranchInfo().get(rs);\n        if (sjInfo == null) {\n          continue;\n        }\n\n        TableScanOperator ts = sjInfo.getTsOp();\n        // This is a semijoin branch. Find if this is creating a potential\n        // cycle with childJoin.\n        for (Operator<?> parent : childJoin.getParentOperators()) {\n          if (parent == parentJoin) {\n            continue;\n          }\n\n          assert parent instanceof ReduceSinkOperator;\n          while (parent.getParentOperators().size() > 0) {\n            parent = parent.getParentOperators().get(0);\n          }\n\n          if (parent == ts) {\n            // We have a cycle!\n            if (sjInfo.getIsHint()) {\n              throw new SemanticException(\"Removing hinted semijoin as it is creating cycles with mapside joins \" + rs + \" : \" + ts);\n            }\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Semijoin cycle due to mapjoin. Removing semijoin \"\n                  + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(ts));\n            }\n            GenTezUtils.removeBranch(rs);\n            GenTezUtils.removeSemiJoinOperator(pCtx, rs, ts);\n          }\n        }\n      }\n    }\n  }"
        ],
        [
            "TezCompiler::removeCycleOperator(Set,OptimizeTezProcContext)",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269 -\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    // If there is a hint and no operator is removed then throw error\n    boolean hasHint = false;\n    boolean removed = false;\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n          removed = true;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n\n        SemiJoinBranchInfo sjInfo =\n                context.parseContext.getRsToSemiJoinBranchInfo().get(o);\n        if (sjInfo == null ) continue;\n        if (sjInfo.getIsHint()) {\n          // Skipping because of hint. Mark this info,\n          hasHint = true;\n          continue;\n        }\n\n        TableScanOperator ts = sjInfo.getTsOp();\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                        victimTS.getStatistics().getDataSize()) {\n          victimRS = (ReduceSinkOperator) o;\n          victimTS = ts;\n          removed = true;\n        }\n      }\n    }\n\n    // Always set the semijoin optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (hasHint && !removed) {\n      // There is hint but none of the operators removed. Throw error\n      throw new SemanticException(\"The user hint is causing an operator cycle. Please fix it and retry\");\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cycle found. Removing semijoin \"\n            + OperatorUtils.getOpNamePretty(victimRS) + \" - \" + OperatorUtils.getOpNamePretty(victimTS));\n      }\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    } else {\n      // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n          + \". Needed to break cyclic dependency\");\n    }\n    return;\n  }",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 +\n 275 +\n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  ",
            "  private void removeCycleOperator(Set<Operator<?>> component, OptimizeTezProcContext context) throws SemanticException {\n    AppMasterEventOperator victimAM = null;\n    TableScanOperator victimTS = null;\n    ReduceSinkOperator victimRS = null;\n\n    // If there is a hint and no operator is removed then throw error\n    boolean hasHint = false;\n    boolean removed = false;\n    for (Operator<?> o : component) {\n      // Look for AppMasterEventOperator or ReduceSinkOperator\n      if (o instanceof AppMasterEventOperator) {\n        if (victimAM == null\n                || o.getStatistics().getDataSize() < victimAM.getStatistics()\n                .getDataSize()) {\n          victimAM = (AppMasterEventOperator) o;\n          removed = true;\n        }\n      } else if (o instanceof ReduceSinkOperator) {\n\n        SemiJoinBranchInfo sjInfo =\n                context.parseContext.getRsToSemiJoinBranchInfo().get(o);\n        if (sjInfo == null ) {\n          continue;\n        }\n        if (sjInfo.getIsHint()) {\n          // Skipping because of hint. Mark this info,\n          hasHint = true;\n          continue;\n        }\n\n        TableScanOperator ts = sjInfo.getTsOp();\n        // Sanity check\n        assert component.contains(ts);\n\n        if (victimRS == null ||\n                ts.getStatistics().getDataSize() <\n                        victimTS.getStatistics().getDataSize()) {\n          victimRS = (ReduceSinkOperator) o;\n          victimTS = ts;\n          removed = true;\n        }\n      }\n    }\n\n    // Always set the semijoin optimization as victim.\n    Operator<?> victim = victimRS;\n\n    if (victimRS == null && victimAM != null ) {\n        victim = victimAM;\n    } else if (victimAM == null) {\n      // do nothing\n    } else {\n      // Cycle consists of atleast one dynamic partition pruning(DPP)\n      // optimization and atleast one min/max optimization.\n      // DPP is a better optimization unless it ends up scanning the\n      // bigger table for keys instead of the smaller table.\n\n      // Get the parent TS of victimRS.\n      Operator<?> op = victimRS;\n      while(!(op instanceof TableScanOperator)) {\n        op = op.getParentOperators().get(0);\n      }\n      if ((2 * op.getStatistics().getDataSize()) <\n              victimAM.getStatistics().getDataSize()) {\n        victim = victimAM;\n      }\n    }\n\n    if (hasHint && !removed) {\n      // There is hint but none of the operators removed. Throw error\n      throw new SemanticException(\"The user hint is causing an operator cycle. Please fix it and retry\");\n    }\n\n    if (victim == null ||\n            (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n                    context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.removeBranch(victim);\n\n    if (victim == victimRS) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cycle found. Removing semijoin \"\n            + OperatorUtils.getOpNamePretty(victimRS) + \" - \" + OperatorUtils.getOpNamePretty(victimTS));\n      }\n      GenTezUtils.removeSemiJoinOperator(context.parseContext, victimRS, victimTS);\n    } else {\n      // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n          + \". Needed to break cyclic dependency\");\n    }\n    return;\n  }"
        ]
    ],
    "040c0783e01fb3089d7925def7c349d7ac98e4d6": [
        [
            "RexNodeConverter::rewriteExtractDateChildren(SqlOperator,List)",
            " 456  \n 457  \n 458 -\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476 -\n 477 -\n 478  \n 479  ",
            "  private List<RexNode> rewriteExtractDateChildren(SqlOperator op, List<RexNode> childRexNodeLst)\n      throws SemanticException {\n    List<RexNode> newChildRexNodeLst = new ArrayList<RexNode>();\n    if (op == HiveExtractDate.YEAR) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.YEAR));\n    } else if (op == HiveExtractDate.QUARTER) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.QUARTER));\n    } else if (op == HiveExtractDate.MONTH) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.MONTH));\n    } else if (op == HiveExtractDate.WEEK) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.WEEK));\n    } else if (op == HiveExtractDate.DAY) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.DAY));\n    } else if (op == HiveExtractDate.HOUR) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.HOUR));\n    } else if (op == HiveExtractDate.MINUTE) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.MINUTE));\n    } else if (op == HiveExtractDate.SECOND) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.SECOND));\n    }\n    assert childRexNodeLst.size() == 1;\n    newChildRexNodeLst.add(childRexNodeLst.get(0));\n    return newChildRexNodeLst;\n  }",
            " 457  \n 458  \n 459 +\n 460 +\n 461  \n 462  \n 463 +\n 464  \n 465  \n 466 +\n 467  \n 468  \n 469 +\n 470  \n 471  \n 472 +\n 473  \n 474  \n 475 +\n 476  \n 477  \n 478 +\n 479  \n 480  \n 481 +\n 482  \n 483  \n 484 +\n 485 +\n 486 +\n 487  \n 488 +\n 489 +\n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496 +\n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502 +\n 503  \n 504  ",
            "  private List<RexNode> rewriteExtractDateChildren(SqlOperator op, List<RexNode> childRexNodeLst)\n      throws SemanticException {\n    List<RexNode> newChildRexNodeLst = new ArrayList<>(2);\n    final boolean isTimestampLevel;\n    if (op == HiveExtractDate.YEAR) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.YEAR));\n      isTimestampLevel = false;\n    } else if (op == HiveExtractDate.QUARTER) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.QUARTER));\n      isTimestampLevel = false;\n    } else if (op == HiveExtractDate.MONTH) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.MONTH));\n      isTimestampLevel = false;\n    } else if (op == HiveExtractDate.WEEK) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.WEEK));\n      isTimestampLevel = false;\n    } else if (op == HiveExtractDate.DAY) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.DAY));\n      isTimestampLevel = false;\n    } else if (op == HiveExtractDate.HOUR) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.HOUR));\n      isTimestampLevel = true;\n    } else if (op == HiveExtractDate.MINUTE) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.MINUTE));\n      isTimestampLevel = true;\n    } else if (op == HiveExtractDate.SECOND) {\n      newChildRexNodeLst.add(cluster.getRexBuilder().makeFlag(TimeUnitRange.SECOND));\n      isTimestampLevel = true;\n    } else {\n      isTimestampLevel = false;\n    }\n\n    final RexNode child = Iterables.getOnlyElement(childRexNodeLst);\n    if (SqlTypeUtil.isDatetime(child.getType()) || SqlTypeUtil.isInterval(child.getType())) {\n      newChildRexNodeLst.add(child);\n    } else {\n      // We need to add a cast to DATETIME Family\n      if (isTimestampLevel) {\n        newChildRexNodeLst.add(\n            cluster.getRexBuilder().makeCast(cluster.getTypeFactory().createSqlType(SqlTypeName.TIMESTAMP), child));\n      } else {\n        newChildRexNodeLst.add(\n            cluster.getRexBuilder().makeCast(cluster.getTypeFactory().createSqlType(SqlTypeName.DATE), child));\n      }\n    }\n\n    return newChildRexNodeLst;\n  }"
        ]
    ],
    "0ad71121d1883b56d364ca855ccda9bb7bfdb67a": [
        [
            "DynamicPartitionPruningOptimization::process(Node,Stack,NodeProcessorCtx,Object)",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 -\n 194 -\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  ",
            "  @Override\n  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n      throws SemanticException {\n    ParseContext parseContext;\n    if (procCtx instanceof OptimizeTezProcContext) {\n      parseContext = ((OptimizeTezProcContext) procCtx).parseContext;\n    } else if (procCtx instanceof OptimizeSparkProcContext) {\n      parseContext = ((OptimizeSparkProcContext) procCtx).getParseContext();\n    } else {\n      throw new IllegalArgumentException(\"expected parseContext to be either \" +\n          \"OptimizeTezProcContext or OptimizeSparkProcContext, but found \" +\n          procCtx.getClass().getName());\n    }\n\n    FilterOperator filter = (FilterOperator) nd;\n    FilterDesc desc = filter.getConf();\n\n    if (!parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING) &&\n        !parseContext.getConf().isSparkDPPAny()) {\n      // nothing to do when the optimization is off\n      return null;\n    }\n\n    TableScanOperator ts = null;\n\n    if (filter.getParentOperators().size() == 1\n        && filter.getParentOperators().get(0) instanceof TableScanOperator) {\n      ts = (TableScanOperator) filter.getParentOperators().get(0);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Parent: \" + filter.getParentOperators().get(0));\n      LOG.debug(\"Filter: \" + desc.getPredicateString());\n      LOG.debug(\"TableScan: \" + ts);\n    }\n\n    DynamicPartitionPrunerContext removerContext = new DynamicPartitionPrunerContext();\n\n    // collect the dynamic pruning conditions\n    removerContext.dynLists.clear();\n    GenTezUtils.collectDynamicPruningConditions(desc.getPredicate(), removerContext);\n\n    if (ts == null) {\n      // Replace the synthetic predicate with true and bail out\n      for (DynamicListContext ctx : removerContext) {\n        ExprNodeDesc constNode =\n                new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n        replaceExprNode(ctx, desc, constNode);\n      }\n      return false;\n    }\n\n    boolean semiJoin = parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION);\n    if (HiveConf.getVar(parseContext.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n      //TODO HIVE-16862: Implement a similar feature like \"hive.tez.dynamic.semijoin.reduction\" in hive on spark\n      semiJoin = false;\n    }\n\n    for (DynamicListContext ctx : removerContext) {\n      String column = ExprNodeDescUtils.extractColName(ctx.parent);\n      boolean semiJoinAttempted = false;\n\n      if (column != null) {\n        // Need unique IDs to refer to each min/max key value in the DynamicValueRegistry\n        String keyBaseAlias = \"\";\n\n        Table table = ts.getConf().getTableMetadata();\n\n        if (table != null && table.isPartitionKey(column)) {\n          String columnType = table.getPartColByName(column).getType();\n          String alias = ts.getConf().getAlias();\n          PrunedPartitionList plist = parseContext.getPrunedPartitions(alias, ts);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"alias: \" + alias);\n            LOG.debug(\"pruned partition list: \");\n            if (plist != null) {\n              for (Partition p : plist.getPartitions()) {\n                LOG.debug(p.getCompleteName());\n              }\n            }\n          }\n          // If partKey is a constant, we can check whether the partitions\n          // have been already filtered\n          if (plist == null || plist.getPartitions().size() != 0) {\n            LOG.info(\"Dynamic partitioning: \" + table.getCompleteName() + \".\" + column);\n            generateEventOperatorPlan(ctx, parseContext, ts, column, columnType);\n          } else {\n            // all partitions have been statically removed\n            LOG.debug(\"No partition pruning necessary.\");\n          }\n        } else {\n          LOG.debug(\"Column \" + column + \" is not a partition column\");\n          semiJoin = semiJoin && !disableSemiJoinOptDueToExternalTable(parseContext.getConf(), ts, ctx);\n          if (semiJoin && ts.getConf().getFilterExpr() != null) {\n            LOG.debug(\"Initiate semijoin reduction for \" + column + \" (\"\n                + ts.getConf().getFilterExpr().getExprString());\n\n            StringBuilder internalColNameBuilder = new StringBuilder();\n            StringBuilder colNameBuilder = new StringBuilder();\n\n            // Apply best effort to fetch the correct table alias. If not\n            // found, fallback to old logic.\n            StringBuilder tabAliasBuilder = new StringBuilder();\n            if (getColumnInfo(ctx, internalColNameBuilder, colNameBuilder, tabAliasBuilder)) {\n              String colName = colNameBuilder.toString();\n              String tableAlias;\n              if (tabAliasBuilder.length() > 0) {\n                tableAlias = tabAliasBuilder.toString();\n              } else {\n                //falling back\n                Operator<?> op = ctx.generator;\n\n                while (!(op == null || op instanceof TableScanOperator)) {\n                  op = op.getParentOperators().get(0);\n                }\n                tableAlias = (op == null ? \"\" : ((TableScanOperator) op).\n                        getConf().getAlias());\n              }\n\n              // Use the tableAlias to generate keyBaseAlias\n              keyBaseAlias = ctx.generator.getOperatorId() + \"_\" + tableAlias\n                      + \"_\" + colName;\n              Map<String, List<SemiJoinHint>> hints = parseContext.getSemiJoinHints();\n              if (hints != null) {\n                // Create semijoin optimizations ONLY for hinted columns\n                semiJoinAttempted = processSemiJoinHints(\n                        parseContext, ctx, hints, tableAlias,\n                        internalColNameBuilder.toString(), colName, ts,\n                        keyBaseAlias);\n              } else {\n                // fallback to regular logic\n                semiJoinAttempted = generateSemiJoinOperatorPlan(\n                        ctx, parseContext, ts, keyBaseAlias,\n                        internalColNameBuilder.toString(), colName, null);\n              }\n            }\n          }\n        }\n\n        // If semijoin is attempted then replace the condition with a min-max filter\n        // and bloom filter else,\n        // we always remove the condition by replacing it with \"true\"\n        if (semiJoinAttempted) {\n          List<ExprNodeDesc> betweenArgs = new ArrayList<ExprNodeDesc>();\n          betweenArgs.add(new ExprNodeConstantDesc(Boolean.FALSE)); // Do not invert between result\n          // add column expression here\n          betweenArgs.add(ctx.parent.getChildren().get(0));\n          betweenArgs.add(new ExprNodeDynamicValueDesc(new DynamicValue(keyBaseAlias + \"_min\", ctx.desc.getTypeInfo())));\n          betweenArgs.add(new ExprNodeDynamicValueDesc(new DynamicValue(keyBaseAlias + \"_max\", ctx.desc.getTypeInfo())));\n          ExprNodeDesc betweenNode = ExprNodeGenericFuncDesc.newInstance(\n                  FunctionRegistry.getFunctionInfo(\"between\").getGenericUDF(), betweenArgs);\n          // add column expression for bloom filter\n          List<ExprNodeDesc> bloomFilterArgs = new ArrayList<ExprNodeDesc>();\n          bloomFilterArgs.add(ctx.parent.getChildren().get(0));\n          bloomFilterArgs.add(new ExprNodeDynamicValueDesc(\n                  new DynamicValue(keyBaseAlias + \"_bloom_filter\",\n                          TypeInfoFactory.binaryTypeInfo)));\n          ExprNodeDesc bloomFilterNode = ExprNodeGenericFuncDesc.newInstance(\n                  FunctionRegistry.getFunctionInfo(\"in_bloom_filter\").\n                          getGenericUDF(), bloomFilterArgs);\n          List<ExprNodeDesc> andArgs = new ArrayList<ExprNodeDesc>();\n          andArgs.add(betweenNode);\n          andArgs.add(bloomFilterNode);\n          ExprNodeDesc andExpr = ExprNodeGenericFuncDesc.newInstance(\n              FunctionRegistry.getFunctionInfo(\"and\").getGenericUDF(), andArgs);\n          replaceExprNode(ctx, desc, andExpr);\n        } else {\n          ExprNodeDesc replaceNode = new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n          replaceExprNode(ctx, desc, replaceNode);\n        }\n      } else {\n        ExprNodeDesc constNode =\n                new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n        replaceExprNode(ctx, desc, constNode);\n      }\n    }\n    // if we pushed the predicate into the table scan we need to remove the\n    // synthetic conditions there.\n    cleanTableScanFilters(ts);\n\n    return false;\n  }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 +\n 194 +\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  ",
            "  @Override\n  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n      throws SemanticException {\n    ParseContext parseContext;\n    if (procCtx instanceof OptimizeTezProcContext) {\n      parseContext = ((OptimizeTezProcContext) procCtx).parseContext;\n    } else if (procCtx instanceof OptimizeSparkProcContext) {\n      parseContext = ((OptimizeSparkProcContext) procCtx).getParseContext();\n    } else {\n      throw new IllegalArgumentException(\"expected parseContext to be either \" +\n          \"OptimizeTezProcContext or OptimizeSparkProcContext, but found \" +\n          procCtx.getClass().getName());\n    }\n\n    FilterOperator filter = (FilterOperator) nd;\n    FilterDesc desc = filter.getConf();\n\n    if (!parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING) &&\n        !parseContext.getConf().isSparkDPPAny()) {\n      // nothing to do when the optimization is off\n      return null;\n    }\n\n    TableScanOperator ts = null;\n\n    if (filter.getParentOperators().size() == 1\n        && filter.getParentOperators().get(0) instanceof TableScanOperator) {\n      ts = (TableScanOperator) filter.getParentOperators().get(0);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Parent: \" + filter.getParentOperators().get(0));\n      LOG.debug(\"Filter: \" + desc.getPredicateString());\n      LOG.debug(\"TableScan: \" + ts);\n    }\n\n    DynamicPartitionPrunerContext removerContext = new DynamicPartitionPrunerContext();\n\n    // collect the dynamic pruning conditions\n    removerContext.dynLists.clear();\n    GenTezUtils.collectDynamicPruningConditions(desc.getPredicate(), removerContext);\n\n    if (ts == null) {\n      // Replace the synthetic predicate with true and bail out\n      for (DynamicListContext ctx : removerContext) {\n        ExprNodeDesc constNode =\n                new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n        replaceExprNode(ctx, desc, constNode);\n      }\n      return false;\n    }\n\n    boolean semiJoin = parseContext.getConf().getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION);\n    if (HiveConf.getVar(parseContext.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n      //TODO HIVE-16862: Implement a similar feature like \"hive.tez.dynamic.semijoin.reduction\" in hive on spark\n      semiJoin = false;\n    }\n\n    for (DynamicListContext ctx : removerContext) {\n      String column = ExprNodeDescUtils.extractColName(ctx.parent);\n      boolean semiJoinAttempted = false;\n\n      if (column != null) {\n        // Need unique IDs to refer to each min/max key value in the DynamicValueRegistry\n        String keyBaseAlias = \"\";\n\n        Table table = ts.getConf().getTableMetadata();\n\n        if (table != null && table.isPartitionKey(column)) {\n          String columnType = table.getPartColByName(column).getType();\n          String alias = ts.getConf().getAlias();\n          PrunedPartitionList plist = parseContext.getPrunedPartitions(alias, ts);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"alias: \" + alias);\n            LOG.debug(\"pruned partition list: \");\n            if (plist != null) {\n              for (Partition p : plist.getPartitions()) {\n                LOG.debug(p.getCompleteName());\n              }\n            }\n          }\n          // If partKey is a constant, we can check whether the partitions\n          // have been already filtered\n          if (plist == null || plist.getPartitions().size() != 0) {\n            LOG.info(\"Dynamic partitioning: \" + table.getCompleteName() + \".\" + column);\n            generateEventOperatorPlan(ctx, parseContext, ts, column, columnType);\n          } else {\n            // all partitions have been statically removed\n            LOG.debug(\"No partition pruning necessary.\");\n          }\n        } else {\n          LOG.debug(\"Column \" + column + \" is not a partition column\");\n          if (semiJoin && !disableSemiJoinOptDueToExternalTable(parseContext.getConf(), ts, ctx)\n                  && ts.getConf().getFilterExpr() != null) {\n            LOG.debug(\"Initiate semijoin reduction for \" + column + \" (\"\n                + ts.getConf().getFilterExpr().getExprString());\n\n            StringBuilder internalColNameBuilder = new StringBuilder();\n            StringBuilder colNameBuilder = new StringBuilder();\n\n            // Apply best effort to fetch the correct table alias. If not\n            // found, fallback to old logic.\n            StringBuilder tabAliasBuilder = new StringBuilder();\n            if (getColumnInfo(ctx, internalColNameBuilder, colNameBuilder, tabAliasBuilder)) {\n              String colName = colNameBuilder.toString();\n              String tableAlias;\n              if (tabAliasBuilder.length() > 0) {\n                tableAlias = tabAliasBuilder.toString();\n              } else {\n                //falling back\n                Operator<?> op = ctx.generator;\n\n                while (!(op == null || op instanceof TableScanOperator)) {\n                  op = op.getParentOperators().get(0);\n                }\n                tableAlias = (op == null ? \"\" : ((TableScanOperator) op).\n                        getConf().getAlias());\n              }\n\n              // Use the tableAlias to generate keyBaseAlias\n              keyBaseAlias = ctx.generator.getOperatorId() + \"_\" + tableAlias\n                      + \"_\" + colName;\n              Map<String, List<SemiJoinHint>> hints = parseContext.getSemiJoinHints();\n              if (hints != null) {\n                // Create semijoin optimizations ONLY for hinted columns\n                semiJoinAttempted = processSemiJoinHints(\n                        parseContext, ctx, hints, tableAlias,\n                        internalColNameBuilder.toString(), colName, ts,\n                        keyBaseAlias);\n              } else {\n                // fallback to regular logic\n                semiJoinAttempted = generateSemiJoinOperatorPlan(\n                        ctx, parseContext, ts, keyBaseAlias,\n                        internalColNameBuilder.toString(), colName, null);\n              }\n            }\n          }\n        }\n\n        // If semijoin is attempted then replace the condition with a min-max filter\n        // and bloom filter else,\n        // we always remove the condition by replacing it with \"true\"\n        if (semiJoinAttempted) {\n          List<ExprNodeDesc> betweenArgs = new ArrayList<ExprNodeDesc>();\n          betweenArgs.add(new ExprNodeConstantDesc(Boolean.FALSE)); // Do not invert between result\n          // add column expression here\n          betweenArgs.add(ctx.parent.getChildren().get(0));\n          betweenArgs.add(new ExprNodeDynamicValueDesc(new DynamicValue(keyBaseAlias + \"_min\", ctx.desc.getTypeInfo())));\n          betweenArgs.add(new ExprNodeDynamicValueDesc(new DynamicValue(keyBaseAlias + \"_max\", ctx.desc.getTypeInfo())));\n          ExprNodeDesc betweenNode = ExprNodeGenericFuncDesc.newInstance(\n                  FunctionRegistry.getFunctionInfo(\"between\").getGenericUDF(), betweenArgs);\n          // add column expression for bloom filter\n          List<ExprNodeDesc> bloomFilterArgs = new ArrayList<ExprNodeDesc>();\n          bloomFilterArgs.add(ctx.parent.getChildren().get(0));\n          bloomFilterArgs.add(new ExprNodeDynamicValueDesc(\n                  new DynamicValue(keyBaseAlias + \"_bloom_filter\",\n                          TypeInfoFactory.binaryTypeInfo)));\n          ExprNodeDesc bloomFilterNode = ExprNodeGenericFuncDesc.newInstance(\n                  FunctionRegistry.getFunctionInfo(\"in_bloom_filter\").\n                          getGenericUDF(), bloomFilterArgs);\n          List<ExprNodeDesc> andArgs = new ArrayList<ExprNodeDesc>();\n          andArgs.add(betweenNode);\n          andArgs.add(bloomFilterNode);\n          ExprNodeDesc andExpr = ExprNodeGenericFuncDesc.newInstance(\n              FunctionRegistry.getFunctionInfo(\"and\").getGenericUDF(), andArgs);\n          replaceExprNode(ctx, desc, andExpr);\n        } else {\n          ExprNodeDesc replaceNode = new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n          replaceExprNode(ctx, desc, replaceNode);\n        }\n      } else {\n        ExprNodeDesc constNode =\n                new ExprNodeConstantDesc(ctx.parent.getTypeInfo(), true);\n        replaceExprNode(ctx, desc, constNode);\n      }\n    }\n    // if we pushed the predicate into the table scan we need to remove the\n    // synthetic conditions there.\n    cleanTableScanFilters(ts);\n\n    return false;\n  }"
        ]
    ],
    "40eb9a51abcee533e67c980cc3b4a0f1d9c86252": [
        [
            "BuddyAllocator::allocateMultiple(MemoryBuffer,int,BufferObjectFactory)",
            " 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 -\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 -\n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  ",
            "  @Override\n  public void allocateMultiple(MemoryBuffer[] dest, int size, BufferObjectFactory factory)\n      throws AllocatorOutOfMemoryException {\n    assert size > 0 : \"size is \" + size;\n    if (size > maxAllocation) {\n      throw new RuntimeException(\"Trying to allocate \" + size + \"; max is \" + maxAllocation);\n    }\n    int freeListIx = determineFreeListForAllocation(size);\n    int allocLog2 = freeListIx + minAllocLog2;\n    int allocationSize = 1 << allocLog2;\n\n    // If using async, we could also reserve one by one.\n    memoryManager.reserveMemory(dest.length << allocLog2);\n    for (int i = 0; i < dest.length; ++i) {\n      if (dest[i] != null) continue;\n      // Note: this is backward compat only. Should be removed with createUnallocated.\n      dest[i] = factory != null ? factory.create() : createUnallocated();\n    }\n\n    // First try to quickly lock some of the correct-sized free lists and allocate from them.\n    int arenaCount = allocatedArenas.get();\n    if (arenaCount < 0) {\n      arenaCount = -arenaCount - 1; // Next arena is being allocated.\n    }\n\n    // Note: we might want to be smarter if threadId-s are low and there more arenas than threads.\n    long threadId = arenaCount > 1 ? Thread.currentThread().getId() : 0;\n    int destAllocIx = allocateFast(dest, null, 0, dest.length,\n        freeListIx, allocationSize, (int)(threadId % arenaCount), arenaCount);\n    if (destAllocIx == dest.length) return;\n\n    // We called reserveMemory so we know that there's memory waiting for us somewhere.\n    // However, we have a class of rare race conditions related to the order of locking/checking of\n    // different allocation areas. Simple case - say we have 2 arenas, 256Kb available in arena 2.\n    // We look at arena 1; someone deallocs 256Kb from arena 1 and allocs the same from arena 2;\n    // we look at arena 2 and find no memory. Or, for single arena, 2 threads reserve 256k each,\n    // and a single 1Mb block is available. When the 1st thread locks the 1Mb freelist, the 2nd one\n    // might have already examined the 256k and 512k lists, finding nothing. Blocks placed by (1)\n    // into smaller lists after its split is done will not be found by (2); given that freelist\n    // locks don't overlap, (2) may even run completely between the time (1) takes out the 1Mb\n    // block and the time it returns the remaining 768Kb.\n    // Two solutions to this are some form of cross-thread helping (threads putting \"demand\"\n    // into some sort of queues that deallocate and split will examine), or having and \"actor\"\n    // allocator thread (or threads per arena).\n    // The 2nd one is probably much simpler and will allow us to get rid of a lot of sync code.\n    // But for now we will just retry. We will evict more each time.\n    int attempt = 0;\n    boolean isFailed = false;\n    int memoryForceReleased = 0;\n    try {\n      int discardFailed = 0;\n      while (true) {\n        // Try to split bigger blocks.\n        int startArenaIx = (int)((threadId + attempt) % arenaCount);\n        destAllocIx = allocateWithSplit(dest, null, destAllocIx, dest.length,\n            freeListIx, allocationSize, startArenaIx, arenaCount, -1);\n        if (destAllocIx == dest.length) return;\n\n        if (attempt == 0) {\n          // Try to allocate memory if we haven't allocated all the way to maxSize yet; very rare.\n          destAllocIx = allocateWithExpand(\n              dest, destAllocIx, freeListIx, allocationSize, arenaCount);\n          if (destAllocIx == dest.length) return;\n        }\n\n        // Try to force-evict the fragments of the requisite size.\n        boolean hasDiscardedAny = false;\n        DiscardContext ctx = threadCtx.get();\n        // Brute force may discard up to twice as many buffers.\n        int maxListSize = 1 << (doUseBruteDiscard ? freeListIx : (freeListIx - 1));\n        int requiredBlocks = dest.length - destAllocIx;\n        ctx.init(maxListSize, requiredBlocks);\n        // First, try to use the blocks of half size in every arena.\n        if (doUseFreeListDiscard && freeListIx > 0) {\n          discardBlocksBasedOnFreeLists(freeListIx, startArenaIx, arenaCount, ctx);\n          memoryForceReleased += ctx.memoryReleased;\n          hasDiscardedAny = ctx.resultCount > 0;\n          destAllocIx = allocateFromDiscardResult(\n              dest, destAllocIx, freeListIx, allocationSize, ctx);\n          if (destAllocIx == dest.length) return;\n        }\n        // Then, try the brute force search for something to throw away.\n        if (doUseBruteDiscard) {\n          ctx.resetResults();\n          discardBlocksBruteForce(freeListIx, startArenaIx, arenaCount, ctx);\n          memoryForceReleased += ctx.memoryReleased;\n          hasDiscardedAny = hasDiscardedAny || (ctx.resultCount > 0);\n          destAllocIx = allocateFromDiscardResult(\n              dest, destAllocIx, freeListIx, allocationSize, ctx);\n\n          if (destAllocIx == dest.length) return;\n        }\n\n        if (hasDiscardedAny) {\n          discardFailed = 0;\n        } else if (++discardFailed > MAX_DISCARD_ATTEMPTS) {\n          String msg = \"Failed to allocate \" + size + \"; at \" + destAllocIx + \" out of \"\n              + dest.length + \" (entire cache is fragmented and locked, or an internal issue)\";\n          logOomErrorMessage(msg);\n          isFailed = true;\n          throw new AllocatorOutOfMemoryException(msg);\n        }\n        ++attempt;\n      }\n    } finally {\n      memoryManager.releaseMemory(memoryForceReleased);\n      if (!isFailed && attempt >= LOG_DISCARD_ATTEMPTS) {\n        LlapIoImpl.LOG.info(\"Allocation of \" + dest.length + \" buffers of size \" + size + \" took \"\n            + attempt + \" attempts to free enough memory; force-released \" + memoryForceReleased);\n      }\n    }\n  }",
            " 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329 +\n 330 +\n 331 +\n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337 +\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  ",
            "  @Override\n  public void allocateMultiple(MemoryBuffer[] dest, int size, BufferObjectFactory factory)\n      throws AllocatorOutOfMemoryException {\n    assert size > 0 : \"size is \" + size;\n    if (size > maxAllocation) {\n      throw new RuntimeException(\"Trying to allocate \" + size + \"; max is \" + maxAllocation);\n    }\n    int freeListIx = determineFreeListForAllocation(size);\n    int allocLog2 = freeListIx + minAllocLog2;\n    int allocationSize = 1 << allocLog2;\n\n    // If using async, we could also reserve one by one.\n    memoryManager.reserveMemory(dest.length << allocLog2);\n    for (int i = 0; i < dest.length; ++i) {\n      if (dest[i] != null) continue;\n      // Note: this is backward compat only. Should be removed with createUnallocated.\n      dest[i] = factory != null ? factory.create() : createUnallocated();\n    }\n\n    // First try to quickly lock some of the correct-sized free lists and allocate from them.\n    int arenaCount = allocatedArenas.get();\n    if (arenaCount < 0) {\n      arenaCount = -arenaCount - 1; // Next arena is being allocated.\n    }\n\n    // Note: we might want to be smarter if threadId-s are low and there more arenas than threads.\n    long threadId = arenaCount > 1 ? Thread.currentThread().getId() : 0;\n    int destAllocIx = allocateFast(dest, null, 0, dest.length,\n        freeListIx, allocationSize, (int)(threadId % arenaCount), arenaCount);\n    if (destAllocIx == dest.length) return;\n\n    // We called reserveMemory so we know that there's memory waiting for us somewhere.\n    // However, we have a class of rare race conditions related to the order of locking/checking of\n    // different allocation areas. Simple case - say we have 2 arenas, 256Kb available in arena 2.\n    // We look at arena 1; someone deallocs 256Kb from arena 1 and allocs the same from arena 2;\n    // we look at arena 2 and find no memory. Or, for single arena, 2 threads reserve 256k each,\n    // and a single 1Mb block is available. When the 1st thread locks the 1Mb freelist, the 2nd one\n    // might have already examined the 256k and 512k lists, finding nothing. Blocks placed by (1)\n    // into smaller lists after its split is done will not be found by (2); given that freelist\n    // locks don't overlap, (2) may even run completely between the time (1) takes out the 1Mb\n    // block and the time it returns the remaining 768Kb.\n    // Two solutions to this are some form of cross-thread helping (threads putting \"demand\"\n    // into some sort of queues that deallocate and split will examine), or having and \"actor\"\n    // allocator thread (or threads per arena).\n    // The 2nd one is probably much simpler and will allow us to get rid of a lot of sync code.\n    // But for now we will just retry. We will evict more each time.\n    int attempt = 0;\n    boolean isFailed = false;\n    int memoryForceReleased = 0;\n    try {\n      int discardFailed = 0;\n      while (true) {\n        // Try to split bigger blocks.\n        int startArenaIx = (int)((threadId + attempt) % arenaCount);\n        destAllocIx = allocateWithSplit(dest, null, destAllocIx, dest.length,\n            freeListIx, allocationSize, startArenaIx, arenaCount, -1);\n        if (destAllocIx == dest.length) return;\n\n        if (attempt == 0) {\n          // Try to allocate memory if we haven't allocated all the way to maxSize yet; very rare.\n          destAllocIx = allocateWithExpand(\n              dest, destAllocIx, freeListIx, allocationSize, arenaCount);\n          if (destAllocIx == dest.length) return;\n        }\n\n        // Try to force-evict the fragments of the requisite size.\n        boolean hasDiscardedAny = false;\n        DiscardContext ctx = threadCtx.get();\n        // Brute force may discard up to twice as many buffers.\n        int maxListSize = 1 << (doUseBruteDiscard ? freeListIx : (freeListIx - 1));\n        int requiredBlocks = dest.length - destAllocIx;\n        ctx.init(maxListSize, requiredBlocks);\n        // First, try to use the blocks of half size in every arena.\n        if (doUseFreeListDiscard && freeListIx > 0) {\n          discardBlocksBasedOnFreeLists(freeListIx, startArenaIx, arenaCount, ctx);\n          memoryForceReleased += ctx.memoryReleased;\n          hasDiscardedAny = ctx.resultCount > 0;\n          destAllocIx = allocateFromDiscardResult(\n              dest, destAllocIx, freeListIx, allocationSize, ctx);\n          if (destAllocIx == dest.length) return;\n        }\n        // Then, try the brute force search for something to throw away.\n        if (doUseBruteDiscard) {\n          ctx.resetResults();\n          discardBlocksBruteForce(freeListIx, startArenaIx, arenaCount, ctx);\n          memoryForceReleased += ctx.memoryReleased;\n          hasDiscardedAny = hasDiscardedAny || (ctx.resultCount > 0);\n          destAllocIx = allocateFromDiscardResult(\n              dest, destAllocIx, freeListIx, allocationSize, ctx);\n          if (destAllocIx == dest.length) return;\n        }\n\n        if (hasDiscardedAny) {\n          discardFailed = 0;\n        } else if (++discardFailed > MAX_DISCARD_ATTEMPTS) {\n          isFailed = true;\n          // Ensure all-or-nothing allocation.\n          for (int i = 0; i < destAllocIx; ++i) {\n            try {\n              deallocate(dest[i]);\n            } catch (Throwable t) {\n              LlapIoImpl.LOG.info(\"Failed to deallocate after a partially successful allocate: \" + dest[i]);\n            }\n          }\n          String msg = \"Failed to allocate \" + size + \"; at \" + destAllocIx + \" out of \"\n              + dest.length + \" (entire cache is fragmented and locked, or an internal issue)\";\n          logOomErrorMessage(msg);\n          throw new AllocatorOutOfMemoryException(msg);\n        }\n        ++attempt;\n      }\n    } finally {\n      memoryManager.releaseMemory(memoryForceReleased);\n      if (!isFailed && attempt >= LOG_DISCARD_ATTEMPTS) {\n        LlapIoImpl.LOG.info(\"Allocation of \" + dest.length + \" buffers of size \" + size + \" took \"\n            + attempt + \" attempts to free enough memory; force-released \" + memoryForceReleased);\n      }\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedColumns(int,StripeInformation,OrcProto,List,List,boolean,boolean,Consumer)",
            " 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 -\n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] physicalFileIncludes, boolean[] rgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[physicalFileIncludes.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < physicalFileIncludes.length; ++i) {\n      if (!physicalFileIncludes[i]) continue;\n      ColumnEncoding enc = encodings.get(i);\n      colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n      trace.logColumnRead(i, colRgIx, enc.getKind());\n    }\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!physicalFileIncludes[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || physicalFileIncludes[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        trace.logSkipStream(colIx, streamKind, offset, length);\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, false);\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, rgs,\n            isCompressed, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (rgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, physicalFileIncludes.length);\n        try {\n          consumer.consumeData(ecb);\n        } catch (InterruptedException e) {\n          LOG.error(\"IO thread interrupted while queueing data\");\n          throw new IOException(e);\n        }\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(\n        listToRead.get(), stripeOffset, hasFileId, toRelease);\n\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    boolean hasError = true;\n    try {\n      int rgCount = rowIndexStride == 0 ? 1 : (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n      for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n        if (rgs != null && !rgs[rgIx]) {\n          continue; // RG filtered.\n        }\n        boolean isLastRg = rgIx == rgCount - 1;\n        // Create the batch we will use to return data for this RG.\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        trace.logStartRg(rgIx);\n        boolean hasErrorForEcb = true;\n        try {\n          ecb.init(fileKey, stripeIx, rgIx, physicalFileIncludes.length);\n          for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n            ColumnReadContext ctx = colCtxs[colIx];\n            if (ctx == null) continue; // This column is not included\n\n            OrcProto.RowIndexEntry index;\n            OrcProto.RowIndexEntry nextIndex;\n            // index is disabled\n            if (ctx.rowIndex == null) {\n              if (isTracingEnabled) {\n                LOG.trace(\"Row index is null. Likely reading a file with indexes disabled.\");\n              }\n              index = null;\n              nextIndex = null;\n            } else {\n              index = ctx.rowIndex.getEntry(rgIx);\n              nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n            }\n            if (isTracingEnabled) {\n              LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n            }\n            ecb.initOrcColumn(ctx.colIx);\n            trace.logStartCol(ctx.colIx);\n            for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n              StreamContext sctx = ctx.streams[streamIx];\n              ColumnStreamData cb;\n              try {\n                if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding) || index == null) {\n                  // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n                  if (sctx.stripeLevelStream == null) {\n                    if (isTracingEnabled) {\n                      LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                          + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n                    }\n                    trace.logStartStripeStream(sctx.kind);\n                    sctx.stripeLevelStream = POOLS.csdPool.take();\n                    // We will be using this for each RG while also sending RGs to processing.\n                    // To avoid buffers being unlocked, run refcount one ahead; so each RG \n                    // processing will decref once, and the last one will unlock the buffers.\n                    sctx.stripeLevelStream.incRef();\n                    // For stripe-level streams we don't need the extra refcount on the block.\n                    // See class comment about refcounts.\n                    long unlockUntilCOffset = sctx.offset + sctx.length;\n                    DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                        sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                        unlockUntilCOffset, sctx.offset, toRelease);\n                    if (lastCached != null) {\n                      iter = lastCached;\n                    }\n                  }\n                  sctx.stripeLevelStream.incRef();\n                  cb = sctx.stripeLevelStream;\n                } else {\n                  // This stream can be separated by RG using index. Let's do that.\n                  // Offset to where this RG begins.\n                  long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n                  // Offset relative to the beginning of the stream of where this RG ends.\n                  long nextCOffsetRel = isLastRg ? sctx.length\n                      : nextIndex.getPositions(sctx.streamIndexOffset);\n                  // Offset before which this RG is guaranteed to end. Can only be estimated.\n                  // We estimate the same way for compressed and uncompressed for now.\n                  long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                      isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n                  // As we read, we can unlock initial refcounts for the buffers that end before\n                  // the data that we need for this RG.\n                  long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n                  cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx,\n                      cOffset, endCOffset, isCompressed, unlockUntilCOffset);\n                  boolean isStartOfStream = sctx.bufferIter == null;\n                  DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                      (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                      unlockUntilCOffset, sctx.offset, toRelease);\n                  if (lastCached != null) {\n                    sctx.bufferIter = iter = lastCached;\n                  }\n                }\n                ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n              } catch (Exception ex) {\n                DiskRangeList drl = toRead == null ? null : toRead.next;\n                LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                    + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n                throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n              }\n            }\n          }\n          hasErrorForEcb = false;\n        } finally {\n          if (hasErrorForEcb) {\n            releaseEcbRefCountsOnError(ecb);\n          }\n        }\n        try {\n          consumer.consumeData(ecb);\n          // After this, the non-initial refcounts are the responsibility of the consumer.\n        } catch (InterruptedException e) {\n          LOG.error(\"IO thread interrupted while queueing data\");\n          releaseEcbRefCountsOnError(ecb);\n          throw new IOException(e);\n        }\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after preparing all the data \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PREREAD);\n      hasError = false;\n    } finally {\n      try {\n        // Release the unreleased stripe-level buffers. See class comment about refcounts.\n        for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n          ColumnReadContext ctx = colCtxs[colIx];\n          if (ctx == null) continue; // This column is not included.\n          for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n            StreamContext sctx = ctx.streams[streamIx];\n            if (sctx == null || sctx.stripeLevelStream == null) continue;\n            if (0 != sctx.stripeLevelStream.decRef()) continue;\n            // Note - this is a little bit confusing; the special treatment of stripe-level buffers\n            // is because we run the ColumnStreamData refcount one ahead (as specified above). It\n            // may look like this would release the buffers too many times (one release from the\n            // consumer, one from releaseInitialRefcounts below, and one here); however, this is\n            // merely handling a special case where all the batches that are sharing the stripe-\n            // level stream have been processed before we got here; they have all decRef-ed the CSD,\n            // but have not released the buffers because of that extra refCount. So, this is\n            // essentially the \"consumer\" refcount being released here.\n            for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Unlocking {} at the end of processing\", buf);\n              }\n              cacheWrapper.releaseBuffer(buf);\n            }\n          }\n        }\n        releaseInitialRefcounts(toRead.next);\n        // Release buffers as we are done with all the streams... also see toRelease comment.\n        releaseBuffers(toRelease.keySet(), true);\n      } catch (Throwable t) {\n        if (!hasError) throw new IOException(t);\n        LOG.error(\"Error during the cleanup after another error; ignoring\", t);\n      }\n    }\n  }",
            " 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 +\n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 +\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496 +\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  ",
            "  @Override\n  public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,\n      List<OrcProto.Stream> streamList, boolean[] physicalFileIncludes, boolean[] rgs,\n      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {\n    // Note: for now we don't have to setError here, caller will setError if we throw.\n    // We are also not supposed to call setDone, since we are only part of the operation.\n    long stripeOffset = stripe.getOffset();\n    // 1. Figure out what we have to read.\n    long offset = 0; // Stream offset in relation to the stripe.\n    // 1.1. Figure out which columns have a present stream\n    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);\n    if (isTracingEnabled) {\n      LOG.trace(\"The following columns have PRESENT streams: \" + arrayToString(hasNull));\n    }\n\n    // We assume stream list is sorted by column and that non-data\n    // streams do not interleave data streams for the same column.\n    // 1.2. With that in mind, determine disk ranges to read/get from cache (not by stream).\n    ColumnReadContext[] colCtxs = new ColumnReadContext[physicalFileIncludes.length];\n    int colRgIx = -1;\n    // Don't create context for the 0-s column.\n    for (int i = 1; i < physicalFileIncludes.length; ++i) {\n      if (!physicalFileIncludes[i]) continue;\n      ColumnEncoding enc = encodings.get(i);\n      colCtxs[i] = new ColumnReadContext(i, enc, indexes[i], ++colRgIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Creating context: \" + colCtxs[i].toString());\n      }\n      trace.logColumnRead(i, colRgIx, enc.getKind());\n    }\n    CreateHelper listToRead = new CreateHelper();\n    boolean hasIndexOnlyCols = false;\n    for (OrcProto.Stream stream : streamList) {\n      long length = stream.getLength();\n      int colIx = stream.getColumn();\n      OrcProto.Stream.Kind streamKind = stream.getKind();\n      if (!physicalFileIncludes[colIx] || StreamName.getArea(streamKind) != StreamName.Area.DATA) {\n        // We have a stream for included column, but in future it might have no data streams.\n        // It's more like \"has at least one column included that has an index stream\".\n        hasIndexOnlyCols = hasIndexOnlyCols || physicalFileIncludes[colIx];\n        if (isTracingEnabled) {\n          LOG.trace(\"Skipping stream for column \" + colIx + \": \"\n              + streamKind + \" at \" + offset + \", \" + length);\n        }\n        trace.logSkipStream(colIx, streamKind, offset, length);\n        offset += length;\n        continue;\n      }\n      ColumnReadContext ctx = colCtxs[colIx];\n      assert ctx != null;\n      int indexIx = RecordReaderUtils.getIndexPosition(ctx.encoding.getKind(),\n          types.get(colIx).getKind(), streamKind, isCompressed, hasNull[colIx]);\n      ctx.addStream(offset, stream, indexIx);\n      if (isTracingEnabled) {\n        LOG.trace(\"Adding stream for column \" + colIx + \": \" + streamKind + \" at \" + offset\n            + \", \" + length + \", index position \" + indexIx);\n      }\n      if (rgs == null || RecordReaderUtils.isDictionary(streamKind, encodings.get(colIx))) {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, true);\n        RecordReaderUtils.addEntireStreamToRanges(offset, length, listToRead, true);\n        if (isTracingEnabled) {\n          LOG.trace(\"Will read whole stream \" + streamKind + \"; added to \" + listToRead.getTail());\n        }\n      } else {\n        trace.logAddStream(colIx, streamKind, offset, length, indexIx, false);\n        RecordReaderUtils.addRgFilteredStreamToRanges(stream, rgs,\n            isCompressed, indexes[colIx], encodings.get(colIx), types.get(colIx),\n            bufferSize, hasNull[colIx], offset, length, listToRead, true);\n      }\n      offset += length;\n    }\n\n    boolean hasFileId = this.fileKey != null;\n    if (listToRead.get() == null) {\n      // No data to read for this stripe. Check if we have some included index-only columns.\n      // TODO: there may be a bug here. Could there be partial RG filtering on index-only column?\n      if (hasIndexOnlyCols && (rgs == null)) {\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, physicalFileIncludes.length);\n        try {\n          consumer.consumeData(ecb);\n        } catch (InterruptedException e) {\n          LOG.error(\"IO thread interrupted while queueing data\");\n          throw new IOException(e);\n        }\n      } else {\n        LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n      }\n      return;\n    }\n\n    // 2. Now, read all of the ranges from cache or disk.\n    IdentityHashMap<ByteBuffer, Boolean> toRelease = new IdentityHashMap<>();\n    MutateHelper toRead = getDataFromCacheAndDisk(\n        listToRead.get(), stripeOffset, hasFileId, toRelease);\n\n\n    // 3. For uncompressed case, we need some special processing before read.\n    //    Basically, we are trying to create artificial, consistent ranges to cache, as there are\n    //    no CBs in an uncompressed file. At the end of this processing, the list would contain\n    //    either cache buffers, or buffers allocated by us and not cached (if we are only reading\n    //    parts of the data for some ranges and don't want to cache it). Both are represented by\n    //    CacheChunks, so the list is just CacheChunk-s from that point on.\n    DiskRangeList iter = preReadUncompressedStreams(stripeOffset, colCtxs, toRead, toRelease);\n\n    // 4. Finally, decompress data, map per RG, and return to caller.\n    // We go by RG and not by column because that is how data is processed.\n    boolean hasError = true;\n    try {\n      int rgCount = rowIndexStride == 0 ? 1 : (int)Math.ceil((double)stripe.getNumberOfRows() / rowIndexStride);\n      for (int rgIx = 0; rgIx < rgCount; ++rgIx) {\n        if (rgs != null && !rgs[rgIx]) {\n          continue; // RG filtered.\n        }\n        boolean isLastRg = rgIx == rgCount - 1;\n        // Create the batch we will use to return data for this RG.\n        OrcEncodedColumnBatch ecb = POOLS.ecbPool.take();\n        trace.logStartRg(rgIx);\n        boolean hasErrorForEcb = true;\n        try {\n          ecb.init(fileKey, stripeIx, rgIx, physicalFileIncludes.length);\n          for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n            ColumnReadContext ctx = colCtxs[colIx];\n            if (ctx == null) continue; // This column is not included\n\n            OrcProto.RowIndexEntry index;\n            OrcProto.RowIndexEntry nextIndex;\n            // index is disabled\n            if (ctx.rowIndex == null) {\n              if (isTracingEnabled) {\n                LOG.trace(\"Row index is null. Likely reading a file with indexes disabled.\");\n              }\n              index = null;\n              nextIndex = null;\n            } else {\n              index = ctx.rowIndex.getEntry(rgIx);\n              nextIndex = isLastRg ? null : ctx.rowIndex.getEntry(rgIx + 1);\n            }\n            if (isTracingEnabled) {\n              LOG.trace(\"ctx: {} rgIx: {} isLastRg: {} rgCount: {}\", ctx, rgIx, isLastRg, rgCount);\n            }\n            ecb.initOrcColumn(ctx.colIx);\n            trace.logStartCol(ctx.colIx);\n            for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n              StreamContext sctx = ctx.streams[streamIx];\n              ColumnStreamData cb = null;\n              try {\n                if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding) || index == null) {\n                  // This stream is for entire stripe and needed for every RG; uncompress once and reuse.\n                  if (sctx.stripeLevelStream == null) {\n                    if (isTracingEnabled) {\n                      LOG.trace(\"Getting stripe-level stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                          + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \" + sctx.length);\n                    }\n                    trace.logStartStripeStream(sctx.kind);\n                    sctx.stripeLevelStream = POOLS.csdPool.take();\n                    // We will be using this for each RG while also sending RGs to processing.\n                    // To avoid buffers being unlocked, run refcount one ahead; so each RG\n                    // processing will decref once, and the last one will unlock the buffers.\n                    sctx.stripeLevelStream.incRef();\n                    // For stripe-level streams we don't need the extra refcount on the block.\n                    // See class comment about refcounts.\n                    long unlockUntilCOffset = sctx.offset + sctx.length;\n                    DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,\n                        sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,\n                        unlockUntilCOffset, sctx.offset, toRelease);\n                    if (lastCached != null) {\n                      iter = lastCached;\n                    }\n                  }\n                  sctx.stripeLevelStream.incRef();\n                  cb = sctx.stripeLevelStream;\n                } else {\n                  // This stream can be separated by RG using index. Let's do that.\n                  // Offset to where this RG begins.\n                  long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);\n                  // Offset relative to the beginning of the stream of where this RG ends.\n                  long nextCOffsetRel = isLastRg ? sctx.length\n                      : nextIndex.getPositions(sctx.streamIndexOffset);\n                  // Offset before which this RG is guaranteed to end. Can only be estimated.\n                  // We estimate the same way for compressed and uncompressed for now.\n                  long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(\n                      isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);\n                  // As we read, we can unlock initial refcounts for the buffers that end before\n                  // the data that we need for this RG.\n                  long unlockUntilCOffset = sctx.offset + nextCOffsetRel;\n                  cb = createRgColumnStreamData(rgIx, isLastRg, ctx.colIx, sctx,\n                      cOffset, endCOffset, isCompressed, unlockUntilCOffset);\n                  boolean isStartOfStream = sctx.bufferIter == null;\n                  DiskRangeList lastCached = readEncodedStream(stripeOffset,\n                      (isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,\n                      unlockUntilCOffset, sctx.offset, toRelease);\n                  if (lastCached != null) {\n                    sctx.bufferIter = iter = lastCached;\n                  }\n                }\n              } catch (Exception ex) {\n                DiskRangeList drl = toRead == null ? null : toRead.next;\n                LOG.error(\"Error getting stream [\" + sctx.kind + \", \" + ctx.encoding + \"] for\"\n                    + \" column \" + ctx.colIx + \" RG \" + rgIx + \" at \" + sctx.offset + \", \"\n                    + sctx.length + \"; toRead \" + RecordReaderUtils.stringifyDiskRanges(drl), ex);\n                throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n              } finally {\n                // Always add stream data to ecb; releaseEcbRefCountsOnError relies on it.\n                // Otherwise, we won't release consumer refcounts for a partially read stream.\n                if (cb != null) {\n                  ecb.setStreamData(ctx.colIx, sctx.kind.getNumber(), cb);\n                }\n              }\n            }\n          }\n          hasErrorForEcb = false;\n        } finally {\n          if (hasErrorForEcb) {\n            releaseEcbRefCountsOnError(ecb);\n          }\n        }\n        try {\n          consumer.consumeData(ecb);\n          // After this, the non-initial refcounts are the responsibility of the consumer.\n        } catch (InterruptedException e) {\n          LOG.error(\"IO thread interrupted while queueing data\");\n          releaseEcbRefCountsOnError(ecb);\n          throw new IOException(e);\n        }\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Disk ranges after preparing all the data \"\n            + RecordReaderUtils.stringifyDiskRanges(toRead.next));\n      }\n      trace.logRanges(fileKey, stripeOffset, toRead.next, RangesSrc.PREREAD);\n      hasError = false;\n    } finally {\n      try {\n        // Release the unreleased stripe-level buffers. See class comment about refcounts.\n        for (int colIx = 0; colIx < colCtxs.length; ++colIx) {\n          ColumnReadContext ctx = colCtxs[colIx];\n          if (ctx == null) continue; // This column is not included.\n          for (int streamIx = 0; streamIx < ctx.streamCount; ++streamIx) {\n            StreamContext sctx = ctx.streams[streamIx];\n            if (sctx == null || sctx.stripeLevelStream == null) continue;\n            if (0 != sctx.stripeLevelStream.decRef()) continue;\n            // Note - this is a little bit confusing; the special treatment of stripe-level buffers\n            // is because we run the ColumnStreamData refcount one ahead (as specified above). It\n            // may look like this would release the buffers too many times (one release from the\n            // consumer, one from releaseInitialRefcounts below, and one here); however, this is\n            // merely handling a special case where all the batches that are sharing the stripe-\n            // level stream have been processed before we got here; they have all decRef-ed the CSD,\n            // but have not released the buffers because of that extra refCount. So, this is\n            // essentially the \"consumer\" refcount being released here.\n            for (MemoryBuffer buf : sctx.stripeLevelStream.getCacheBuffers()) {\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Unlocking {} at the end of processing\", buf);\n              }\n              cacheWrapper.releaseBuffer(buf);\n            }\n          }\n        }\n        releaseInitialRefcounts(toRead.next);\n        // Release buffers as we are done with all the streams... also see toRelease comment.\n        releaseBuffers(toRelease.keySet(), true);\n      } catch (Throwable t) {\n        if (!hasError) throw new IOException(t);\n        LOG.error(\"Error during the cleanup after another error; ignoring\", t);\n      }\n    }\n  }"
        ],
        [
            "EncodedReaderImpl::readEncodedStream(long,DiskRangeList,long,long,ColumnStreamData,long,long,IdentityHashMap)",
            " 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893 -\n 894 -\n 895  \n 896  \n 897 -\n 898 -\n 899 -\n 900 -\n 901 -\n 902 -\n 903 -\n 904 -\n 905 -\n 906 -\n 907  \n 908  \n 909 -\n 910 -\n 911 -\n 912  \n 913 -\n 914 -\n 915  \n 916 -\n 917 -\n 918 -\n 919 -\n 920 -\n 921 -\n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset, tag);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n        cacheWrapper.getDataBufferFactory());\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n      if (chunk.isOriginalDataCompressed) {\n        boolean isOk = false;\n        try {\n          decompressChunk(chunk.originalData, codec, dest);\n          isOk = true;\n        } finally {\n          if (!isOk) {\n            isCodecFailure = true;\n          }\n        }\n      } else {\n        copyUncompressedChunk(chunk.originalData, dest);\n      }\n\n      if (isTracingEnabled) {\n        LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n      }\n      // After we set originalData to null, we incref the buffer and the cleanup would decref it.\n      // Note that this assumes the failure during incref means incref didn't occur.\n      try {\n        cacheWrapper.reuseBuffer(chunk.getBuffer());\n      } finally {\n        chunk.originalData = null;\n      }\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset, tag);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898 +\n 899 +\n 900 +\n 901 +\n 902 +\n 903 +\n 904 +\n 905 +\n 906 +\n 907 +\n 908 +\n 909 +\n 910 +\n 911 +\n 912 +\n 913 +\n 914 +\n 915 +\n 916  \n 917  \n 918 +\n 919 +\n 920 +\n 921 +\n 922 +\n 923 +\n 924 +\n 925 +\n 926 +\n 927 +\n 928 +\n 929 +\n 930 +\n 931 +\n 932  \n 933 +\n 934 +\n 935  \n 936  \n 937 +\n 938 +\n 939 +\n 940 +\n 941 +\n 942 +\n 943 +\n 944 +\n 945 +\n 946 +\n 947 +\n 948  \n 949 +\n 950 +\n 951 +\n 952 +\n 953 +\n 954 +\n 955 +\n 956 +\n 957 +\n 958 +\n 959 +\n 960 +\n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  ",
            "  /**\n   * Uncompresses part of the stream. RGs can overlap, so we cannot just go and decompress\n   * and remove what we have returned. We will keep iterator as a \"hint\" point.\n   * @param baseOffset Absolute offset of boundaries and ranges relative to file, for cache keys.\n   * @param start Ordered ranges containing file data. Helpful if they point close to cOffset.\n   * @param cOffset Start offset to decompress.\n   * @param endCOffset End offset to decompress; estimate, partial CBs will be ignored.\n   * @param csd Stream data, to add the results.\n   * @param unlockUntilCOffset The offset until which the buffers can be unlocked in cache, as\n   *                           they will not be used in future calls (see the class comment in\n   *                           EncodedReaderImpl about refcounts).\n   * @return Last buffer cached during decompression. Cache buffers are never removed from\n   *         the master list, so they are safe to keep as iterators for various streams.\n   */\n  public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, long cOffset,\n      long endCOffset, ColumnStreamData csd, long unlockUntilCOffset, long streamOffset,\n      IdentityHashMap<ByteBuffer, Boolean> toRelease) throws IOException {\n    if (csd.getCacheBuffers() == null) {\n      csd.setCacheBuffers(new ArrayList<MemoryBuffer>());\n    } else {\n      csd.getCacheBuffers().clear();\n    }\n    if (cOffset == endCOffset) return null;\n    List<ProcCacheChunk> toDecompress = null;\n    List<IncompleteCb> badEstimates = null;\n    List<ByteBuffer> toReleaseCopies = null;\n    if (isCompressed) {\n      toReleaseCopies = new ArrayList<>();\n      toDecompress = new ArrayList<>();\n      badEstimates = new ArrayList<>();\n    }\n\n    // 1. Find our bearings in the stream. Normally, iter will already point either to where we\n    // want to be, or just before. However, RGs can overlap due to encoding, so we may have\n    // to return to a previous block.\n    DiskRangeList current = findExactPosition(start, cOffset);\n    if (isTracingEnabled) {\n      LOG.trace(\"Starting read for [\" + cOffset + \",\" + endCOffset + \") at \" + current);\n    }\n    trace.logStartRead(current);\n\n    CacheChunk lastUncompressed = null;\n\n    // 2. Go thru the blocks; add stuff to results and prepare the decompression work (see below).\n    try {\n      lastUncompressed = isCompressed ?\n          prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset, unlockUntilCOffset,\n              current, csd, toRelease, toReleaseCopies, toDecompress, badEstimates)\n        : prepareRangesForUncompressedRead(\n            cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);\n    } catch (Exception ex) {\n      LOG.error(\"Failed \" + (isCompressed ? \"\" : \"un\") + \"compressed read; cOffset \" + cOffset\n          + \", endCOffset \" + endCOffset + \", streamOffset \" + streamOffset\n          + \", unlockUntilCOffset \" + unlockUntilCOffset + \"; ranges passed in \"\n          + RecordReaderUtils.stringifyDiskRanges(start) + \"; ranges passed to prepare \"\n          + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.\n      throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);\n    }\n\n    // 2.5. Remember the bad estimates for future reference.\n    if (badEstimates != null && !badEstimates.isEmpty()) {\n      // Relies on the fact that cache does not actually store these.\n      DiskRange[] cacheKeys = badEstimates.toArray(new DiskRange[badEstimates.size()]);\n      long[] result = cacheWrapper.putFileData(fileKey, cacheKeys, null, baseOffset, tag);\n      assert result == null; // We don't expect conflicts from bad estimates.\n    }\n\n    if (toDecompress == null || toDecompress.isEmpty()) {\n      releaseBuffers(toReleaseCopies, false);\n      return lastUncompressed; // Nothing to do.\n    }\n\n    // 3. Allocate the buffers, prepare cache keys.\n    // At this point, we have read all the CBs we need to read. cacheBuffers contains some cache\n    // data and some unallocated membufs for decompression. toDecompress contains all the work we\n    // need to do, and each item points to one of the membufs in cacheBuffers as target. The iter\n    // has also been adjusted to point to these buffers instead of compressed data for the ranges.\n    MemoryBuffer[] targetBuffers = new MemoryBuffer[toDecompress.size()];\n    DiskRange[] cacheKeys = new DiskRange[toDecompress.size()];\n    int ix = 0;\n    for (ProcCacheChunk chunk : toDecompress) {\n      cacheKeys[ix] = chunk; // Relies on the fact that cache does not actually store these.\n      targetBuffers[ix] = chunk.getBuffer();\n      ++ix;\n    }\n    boolean isAllocated = false;\n    try {\n      cacheWrapper.getAllocator().allocateMultiple(targetBuffers, bufferSize,\n          cacheWrapper.getDataBufferFactory());\n      isAllocated = true;\n    } finally {\n      // toDecompress/targetBuffers contents are actually already added to some structures that\n      // will be cleaned up on error. Remove the unallocated buffers; keep the cached buffers in.\n      if (!isAllocated) {\n        // Inefficient - this only happens during cleanup on errors.\n        for (MemoryBuffer buf : targetBuffers) {\n          csd.getCacheBuffers().remove(buf);\n        }\n        for (ProcCacheChunk chunk : toDecompress) {\n          chunk.buffer = null;\n        }\n      }\n    }\n\n    // 4. Now decompress (or copy) the data into cache buffers.\n    int decompressedIx = 0;\n    try {\n      while (decompressedIx < toDecompress.size()) {\n        ProcCacheChunk chunk = toDecompress.get(decompressedIx);\n        ByteBuffer dest = chunk.getBuffer().getByteBufferRaw();\n        if (chunk.isOriginalDataCompressed) {\n          boolean isOk = false;\n          try {\n            decompressChunk(chunk.originalData, codec, dest);\n            isOk = true;\n          } finally {\n            if (!isOk) {\n              isCodecFailure = true;\n            }\n          }\n        } else {\n          copyUncompressedChunk(chunk.originalData, dest);\n        }\n\n        if (isTracingEnabled) {\n          LOG.trace(\"Locking \" + chunk.getBuffer() + \" due to reuse (after decompression)\");\n        }\n        // After we set originalData to null, we incref the buffer and the cleanup would decref it.\n        // Note that this assumes the failure during incref means incref didn't occur.\n        try {\n          cacheWrapper.reuseBuffer(chunk.getBuffer());\n        } finally {\n          chunk.originalData = null;\n        }\n        ++decompressedIx;\n      }\n    } finally {\n      // This will only execute on error. Deallocate the remaining allocated buffers explicitly.\n      // The ones that were already incref-ed will be cleaned up with the regular cache buffers.\n      while (decompressedIx < toDecompress.size()) {\n        ProcCacheChunk chunk = toDecompress.get(decompressedIx);\n        csd.getCacheBuffers().remove(chunk.getBuffer());\n        try {\n          cacheWrapper.getAllocator().deallocate(chunk.getBuffer());\n        } catch (Throwable t) {\n          LOG.error(\"Ignoring the cleanup error after another error\", t);\n        }\n        chunk.setBuffer(null);\n      }\n    }\n\n    // 5. Release the copies we made directly to the cleaner.\n    releaseBuffers(toReleaseCopies, false);\n\n    // 6. Finally, put uncompressed data to cache.\n    if (fileKey != null) {\n      long[] collisionMask = cacheWrapper.putFileData(\n          fileKey, cacheKeys, targetBuffers, baseOffset, tag);\n      processCacheCollisions(collisionMask, toDecompress, targetBuffers, csd.getCacheBuffers());\n    }\n\n    // 7. It may happen that we know we won't use some cache buffers anymore (the alternative\n    //    is that we will use the same buffers for other streams in separate calls).\n    //    Release initial refcounts.\n    for (ProcCacheChunk chunk : toDecompress) {\n      ponderReleaseInitialRefcount(unlockUntilCOffset, streamOffset, chunk);\n    }\n\n    return lastUncompressed;\n  }"
        ]
    ],
    "198487557d17d53fa9216a3e814b7eb073da4781": [
        [
            "CalcitePlanner::handleCreateViewDDL(ASTNode)",
            " 634 -\n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641 -\n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  ",
            "  private void handleCreateViewDDL(ASTNode newAST) throws SemanticException {\n    saveViewDefinition();\n    String originalText = createVwDesc.getViewOriginalText();\n    String expandedText = createVwDesc.getViewExpandedText();\n    List<FieldSchema> schema = createVwDesc.getSchema();\n    List<FieldSchema> partitionColumns = createVwDesc.getPartCols();\n    init(false);\n    setAST(newAST);\n    newAST = reAnalyzeViewAfterCbo(newAST);\n    createVwDesc.setViewOriginalText(originalText);\n    createVwDesc.setViewExpandedText(expandedText);\n    createVwDesc.setSchema(schema);\n    createVwDesc.setPartCols(partitionColumns);\n  }",
            " 614 +\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621 +\n 622 +\n 623  \n 624  \n 625  \n 626  \n 627 +\n 628  ",
            "  private ASTNode handleCreateViewDDL(ASTNode ast) throws SemanticException {\n    saveViewDefinition();\n    String originalText = createVwDesc.getViewOriginalText();\n    String expandedText = createVwDesc.getViewExpandedText();\n    List<FieldSchema> schema = createVwDesc.getSchema();\n    List<FieldSchema> partitionColumns = createVwDesc.getPartCols();\n    init(false);\n    setAST(ast);\n    ASTNode newAST = reAnalyzeViewAfterCbo(ast);\n    createVwDesc.setViewOriginalText(originalText);\n    createVwDesc.setViewExpandedText(expandedText);\n    createVwDesc.setSchema(schema);\n    createVwDesc.setPartCols(partitionColumns);\n    return newAST;\n  }"
        ],
        [
            "SemanticAnalyzer::saveViewDefinition()",
            "12525  \n12526  \n12527  \n12528  \n12529  \n12530  \n12531  \n12532  \n12533  \n12534  \n12535  \n12536  \n12537  \n12538  \n12539  \n12540  \n12541  \n12542  \n12543  \n12544  \n12545  \n12546  \n12547  \n12548  \n12549  \n12550  \n12551  \n12552  \n12553  \n12554  \n12555  \n12556  \n12557  \n12558  \n12559  \n12560  \n12561  \n12562 -\n12563 -\n12564 -\n12565 -\n12566 -\n12567 -\n12568 -\n12569 -\n12570  \n12571  \n12572 -\n12573 -\n12574 -\n12575 -\n12576 -\n12577 -\n12578 -\n12579 -\n12580 -\n12581 -\n12582 -\n12583 -\n12584 -\n12585 -\n12586 -\n12587 -\n12588 -\n12589 -\n12590 -\n12591 -\n12592 -\n12593 -\n12594 -\n12595 -\n12596 -\n12597 -\n12598 -\n12599 -\n12600 -\n12601 -\n12602 -\n12603 -\n12604 -\n12605 -\n12606  \n12607 -\n12608 -\n12609 -\n12610 -\n12611 -\n12612 -\n12613 -\n12614 -\n12615 -\n12616 -\n12617 -\n12618 -\n12619 -\n12620  \n12621  \n12622  \n12623 -\n12624  \n12625 -\n12626 -\n12627 -\n12628 -\n12629 -\n12630 -\n12631  \n12632 -\n12633 -\n12634 -\n12635  \n12636 -\n12637 -\n12638 -\n12639 -\n12640  \n12641  \n12642  \n12643  \n12644  ",
            "  protected void saveViewDefinition() throws SemanticException {\n    if (createVwDesc.isMaterialized() && createVwDesc.isReplace()) {\n      // This is a rebuild, there's nothing to do here\n      return;\n    }\n\n    // Make a copy of the statement's result schema, since we may\n    // modify it below as part of imposing view column names.\n    List<FieldSchema> derivedSchema =\n        new ArrayList<FieldSchema>(resultSchema);\n    ParseUtils.validateColumnNameUniqueness(derivedSchema);\n\n    List<FieldSchema> imposedSchema = createVwDesc.getSchema();\n    if (imposedSchema != null) {\n      int explicitColCount = imposedSchema.size();\n      int derivedColCount = derivedSchema.size();\n      if (explicitColCount != derivedColCount) {\n        throw new SemanticException(generateErrorMessage(\n            viewSelect,\n            ErrorMsg.VIEW_COL_MISMATCH.getMsg()));\n      }\n    }\n\n    // Preserve the original view definition as specified by the user.\n    if (createVwDesc.getViewOriginalText() == null) {\n      String originalText = ctx.getTokenRewriteStream().toString(\n          viewSelect.getTokenStartIndex(), viewSelect.getTokenStopIndex());\n      createVwDesc.setViewOriginalText(originalText);\n    }\n\n    // Now expand the view definition with extras such as explicit column\n    // references; this expanded form is what we'll re-parse when the view is\n    // referenced later.\n    unparseTranslator.applyTranslations(ctx.getTokenRewriteStream());\n    String expandedText = ctx.getTokenRewriteStream().toString(\n        viewSelect.getTokenStartIndex(), viewSelect.getTokenStopIndex());\n\n    if (imposedSchema != null) {\n      // Merge the names from the imposed schema into the types\n      // from the derived schema.\n      StringBuilder sb = new StringBuilder();\n      sb.append(\"SELECT \");\n      int n = derivedSchema.size();\n      for (int i = 0; i < n; ++i) {\n        if (i > 0) {\n          sb.append(\", \");\n        }\n        FieldSchema fieldSchema = derivedSchema.get(i);\n        // Modify a copy, not the original\n        fieldSchema = new FieldSchema(fieldSchema);\n        // TODO: there's a potential problem here if some table uses external schema like Avro,\n        //       with a very large type name. It seems like the view does not derive the SerDe from\n        //       the table, so it won't be able to just get the type from the deserializer like the\n        //       table does; we won't be able to properly store the type in the RDBMS metastore.\n        //       Not sure if these large cols could be in resultSchema. Ignore this for now 0_o\n        derivedSchema.set(i, fieldSchema);\n        sb.append(HiveUtils.unparseIdentifier(fieldSchema.getName(), conf));\n        sb.append(\" AS \");\n        String imposedName = imposedSchema.get(i).getName();\n        sb.append(HiveUtils.unparseIdentifier(imposedName, conf));\n        fieldSchema.setName(imposedName);\n        // We don't currently allow imposition of a type\n        fieldSchema.setComment(imposedSchema.get(i).getComment());\n      }\n      sb.append(\" FROM (\");\n      sb.append(expandedText);\n      sb.append(\") \");\n      sb.append(HiveUtils.unparseIdentifier(createVwDesc.getViewName(), conf));\n      expandedText = sb.toString();\n    }\n\n    if (createVwDesc.getPartColNames() != null) {\n      // Make sure all partitioning columns referenced actually\n      // exist and are in the correct order at the end\n      // of the list of columns produced by the view. Also move the field\n      // schema descriptors from derivedSchema to the partitioning key\n      // descriptor.\n      List<String> partColNames = createVwDesc.getPartColNames();\n      if (partColNames.size() > derivedSchema.size()) {\n        throw new SemanticException(\n            ErrorMsg.VIEW_PARTITION_MISMATCH.getMsg());\n      }\n\n      // Get the partition columns from the end of derivedSchema.\n      List<FieldSchema> partitionColumns = derivedSchema.subList(\n          derivedSchema.size() - partColNames.size(),\n          derivedSchema.size());\n\n      // Verify that the names match the PARTITIONED ON clause.\n      Iterator<String> colNameIter = partColNames.iterator();\n      Iterator<FieldSchema> schemaIter = partitionColumns.iterator();\n      while (colNameIter.hasNext()) {\n        String colName = colNameIter.next();\n        FieldSchema fieldSchema = schemaIter.next();\n        if (!fieldSchema.getName().equals(colName)) {\n          throw new SemanticException(\n              ErrorMsg.VIEW_PARTITION_MISMATCH.getMsg());\n        }\n      }\n\n      // Boundary case: require at least one non-partitioned column\n      // for consistency with tables.\n      if (partColNames.size() == derivedSchema.size()) {\n        throw new SemanticException(\n            ErrorMsg.VIEW_PARTITION_TOTAL.getMsg());\n      }\n\n      // Now make a copy.\n      createVwDesc.setPartCols(\n          new ArrayList<FieldSchema>(partitionColumns));\n\n      // Finally, remove the partition columns from the end of derivedSchema.\n      // (Clearing the subList writes through to the underlying\n      // derivedSchema ArrayList.)\n      partitionColumns.clear();\n    }\n\n    createVwDesc.setSchema(derivedSchema);\n    createVwDesc.setViewExpandedText(expandedText);\n  }",
            "12525  \n12526  \n12527  \n12528  \n12529  \n12530  \n12531  \n12532  \n12533  \n12534  \n12535  \n12536  \n12537  \n12538  \n12539  \n12540  \n12541  \n12542  \n12543  \n12544  \n12545  \n12546  \n12547  \n12548  \n12549  \n12550  \n12551  \n12552  \n12553  \n12554  \n12555  \n12556  \n12557  \n12558  \n12559  \n12560  \n12561  \n12562 +\n12563 +\n12564 +\n12565 +\n12566 +\n12567 +\n12568 +\n12569 +\n12570 +\n12571 +\n12572 +\n12573 +\n12574 +\n12575 +\n12576 +\n12577 +\n12578 +\n12579 +\n12580 +\n12581 +\n12582 +\n12583 +\n12584  \n12585 +\n12586  \n12587 +\n12588 +\n12589 +\n12590 +\n12591 +\n12592  \n12593 +\n12594 +\n12595 +\n12596 +\n12597 +\n12598 +\n12599 +\n12600 +\n12601 +\n12602 +\n12603 +\n12604 +\n12605 +\n12606 +\n12607 +\n12608 +\n12609 +\n12610 +\n12611 +\n12612 +\n12613 +\n12614 +\n12615 +\n12616 +\n12617 +\n12618 +\n12619 +\n12620 +\n12621 +\n12622 +\n12623 +\n12624 +\n12625 +\n12626 +\n12627 +\n12628 +\n12629 +\n12630 +\n12631 +\n12632 +\n12633 +\n12634 +\n12635 +\n12636  \n12637  \n12638  \n12639  \n12640 +\n12641 +\n12642 +\n12643 +\n12644  \n12645 +\n12646 +\n12647 +\n12648 +\n12649 +\n12650 +\n12651 +\n12652 +\n12653 +\n12654 +\n12655 +\n12656  \n12657 +\n12658 +\n12659 +\n12660 +\n12661 +\n12662 +\n12663 +\n12664 +\n12665 +\n12666 +\n12667 +\n12668 +\n12669 +\n12670 +\n12671 +\n12672 +\n12673  \n12674  \n12675 +\n12676  \n12677  \n12678  ",
            "  protected void saveViewDefinition() throws SemanticException {\n    if (createVwDesc.isMaterialized() && createVwDesc.isReplace()) {\n      // This is a rebuild, there's nothing to do here\n      return;\n    }\n\n    // Make a copy of the statement's result schema, since we may\n    // modify it below as part of imposing view column names.\n    List<FieldSchema> derivedSchema =\n        new ArrayList<FieldSchema>(resultSchema);\n    ParseUtils.validateColumnNameUniqueness(derivedSchema);\n\n    List<FieldSchema> imposedSchema = createVwDesc.getSchema();\n    if (imposedSchema != null) {\n      int explicitColCount = imposedSchema.size();\n      int derivedColCount = derivedSchema.size();\n      if (explicitColCount != derivedColCount) {\n        throw new SemanticException(generateErrorMessage(\n            viewSelect,\n            ErrorMsg.VIEW_COL_MISMATCH.getMsg()));\n      }\n    }\n\n    // Preserve the original view definition as specified by the user.\n    if (createVwDesc.getViewOriginalText() == null) {\n      String originalText = ctx.getTokenRewriteStream().toString(\n          viewSelect.getTokenStartIndex(), viewSelect.getTokenStopIndex());\n      createVwDesc.setViewOriginalText(originalText);\n    }\n\n    // Now expand the view definition with extras such as explicit column\n    // references; this expanded form is what we'll re-parse when the view is\n    // referenced later.\n    unparseTranslator.applyTranslations(ctx.getTokenRewriteStream());\n    String expandedText = ctx.getTokenRewriteStream().toString(\n        viewSelect.getTokenStartIndex(), viewSelect.getTokenStopIndex());\n\n    if (createVwDesc.isMaterialized()) {\n      if (createVwDesc.getPartColNames() != null) {\n        // If we are creating a materialized view and it has partition columns,\n        // we may need to reorder column projection in expanded query. The reason\n        // is that Hive assumes that in the partition columns are at the end of\n        // the MV schema, and if we do not do this, we will have a mismatch between\n        // the SQL query for the MV and the MV itself.\n        boolean first = true;\n        StringBuilder sb = new StringBuilder();\n        sb.append(\"SELECT \");\n        for (int i = 0; i < derivedSchema.size(); ++i) {\n          FieldSchema fieldSchema = derivedSchema.get(i);\n          if (!createVwDesc.getPartColNames().contains(fieldSchema.getName())) {\n            if (first) {\n              first = false;\n            } else {\n              sb.append(\", \");\n            }\n            sb.append(HiveUtils.unparseIdentifier(fieldSchema.getName(), conf));\n          }\n        }\n        for (String partColName : createVwDesc.getPartColNames()) {\n          sb.append(\", \");\n          sb.append(HiveUtils.unparseIdentifier(partColName, conf));\n        }\n        sb.append(\" FROM (\");\n        sb.append(expandedText);\n        sb.append(\") \");\n        sb.append(HiveUtils.unparseIdentifier(createVwDesc.getViewName(), conf));\n        expandedText = sb.toString();\n      }\n    } else {\n      if (imposedSchema != null) {\n        // Merge the names from the imposed schema into the types\n        // from the derived schema.\n        StringBuilder sb = new StringBuilder();\n        sb.append(\"SELECT \");\n        int n = derivedSchema.size();\n        for (int i = 0; i < n; ++i) {\n          if (i > 0) {\n            sb.append(\", \");\n          }\n          FieldSchema fieldSchema = derivedSchema.get(i);\n          // Modify a copy, not the original\n          fieldSchema = new FieldSchema(fieldSchema);\n          // TODO: there's a potential problem here if some table uses external schema like Avro,\n          //       with a very large type name. It seems like the view does not derive the SerDe from\n          //       the table, so it won't be able to just get the type from the deserializer like the\n          //       table does; we won't be able to properly store the type in the RDBMS metastore.\n          //       Not sure if these large cols could be in resultSchema. Ignore this for now 0_o\n          derivedSchema.set(i, fieldSchema);\n          sb.append(HiveUtils.unparseIdentifier(fieldSchema.getName(), conf));\n          sb.append(\" AS \");\n          String imposedName = imposedSchema.get(i).getName();\n          sb.append(HiveUtils.unparseIdentifier(imposedName, conf));\n          fieldSchema.setName(imposedName);\n          // We don't currently allow imposition of a type\n          fieldSchema.setComment(imposedSchema.get(i).getComment());\n        }\n        sb.append(\" FROM (\");\n        sb.append(expandedText);\n        sb.append(\") \");\n        sb.append(HiveUtils.unparseIdentifier(createVwDesc.getViewName(), conf));\n        expandedText = sb.toString();\n      }\n\n      if (createVwDesc.getPartColNames() != null) {\n        // Make sure all partitioning columns referenced actually\n        // exist and are in the correct order at the end\n        // of the list of columns produced by the view. Also move the field\n        // schema descriptors from derivedSchema to the partitioning key\n        // descriptor.\n        List<String> partColNames = createVwDesc.getPartColNames();\n        if (partColNames.size() > derivedSchema.size()) {\n          throw new SemanticException(\n              ErrorMsg.VIEW_PARTITION_MISMATCH.getMsg());\n        }\n\n        // Get the partition columns from the end of derivedSchema.\n        List<FieldSchema> partitionColumns = derivedSchema.subList(\n            derivedSchema.size() - partColNames.size(),\n            derivedSchema.size());\n\n        // Verify that the names match the PARTITIONED ON clause.\n        Iterator<String> colNameIter = partColNames.iterator();\n        Iterator<FieldSchema> schemaIter = partitionColumns.iterator();\n        while (colNameIter.hasNext()) {\n          String colName = colNameIter.next();\n          FieldSchema fieldSchema = schemaIter.next();\n          if (!fieldSchema.getName().equals(colName)) {\n            throw new SemanticException(\n                ErrorMsg.VIEW_PARTITION_MISMATCH.getMsg());\n          }\n        }\n\n        // Boundary case: require at least one non-partitioned column\n        // for consistency with tables.\n        if (partColNames.size() == derivedSchema.size()) {\n          throw new SemanticException(\n              ErrorMsg.VIEW_PARTITION_TOTAL.getMsg());\n        }\n\n        // Now make a copy.\n        createVwDesc.setPartCols(\n            new ArrayList<FieldSchema>(partitionColumns));\n\n        // Finally, remove the partition columns from the end of derivedSchema.\n        // (Clearing the subList writes through to the underlying\n        // derivedSchema ArrayList.)\n        partitionColumns.clear();\n      }\n    }\n\n    // Set schema and expanded text for the view\n    createVwDesc.setSchema(derivedSchema);\n    createVwDesc.setViewExpandedText(expandedText);\n  }"
        ],
        [
            "CalcitePlanner::genOPTree(ASTNode,PlannerContext)",
            " 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 -\n 492  \n 493 -\n 494  \n 495  \n 496  \n 497 -\n 498 -\n 499 -\n 500 -\n 501 -\n 502 -\n 503 -\n 504 -\n 505 -\n 506 -\n 507 -\n 508 -\n 509 -\n 510 -\n 511 -\n 512 -\n 513 -\n 514 -\n 515 -\n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n      List<ASTNode> oldHints = new ArrayList<>();\n      // Cache the hints before CBO runs and removes them.\n      // Use the hints later in top level QB.\n        getHintsFromQB(getQB(), oldHints);\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS || cboCtx.type == PreCboCtx.Type.VIEW) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              throw new SemanticException(\"Create view is not supported in cbo return path.\");\n            }\n            sinkOp = getOptimizedHiveOPDag();\n            if (oldHints.size() > 0) {\n              LOG.debug(\"Propagating hints to QB: \" + oldHints);\n              getQB().getParseInfo().setHintList(oldHints);\n            }\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 0. Gen Optimized Plan\n            final RelNode newPlan = logicalPlan();\n            // 1. Convert Plan to AST\n            ASTNode newAST = getOptimizedAST(newPlan);\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 1.2. Fix up the query for materialization rebuild\n            if (mvRebuildMode == MaterializationRebuildMode.AGGREGATE_REBUILD) {\n              fixUpASTAggregateIncrementalRebuild(newAST);\n            } else if (mvRebuildMode == MaterializationRebuildMode.NO_AGGREGATE_REBUILD) {\n              fixUpASTNoAggregateIncrementalRebuild(newAST);\n            }\n\n            // 2. Regen OP plan from optimized AST\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              try {\n                handleCreateViewDDL(newAST);\n              } catch (SemanticException e) {\n                throw new CalciteViewSemanticException(e.getMessage());\n              }\n            } else if (cboCtx.type == PreCboCtx.Type.VIEW && materializedView) {\n              // Store text of the ORIGINAL QUERY\n              String originalText = ctx.getTokenRewriteStream().toString(\n                  cboCtx.nodeOfInterest.getTokenStartIndex(),\n                  cboCtx.nodeOfInterest.getTokenStopIndex());\n              unparseTranslator.applyTranslations(ctx.getTokenRewriteStream());\n              String expandedText = ctx.getTokenRewriteStream().toString(\n                  cboCtx.nodeOfInterest.getTokenStartIndex(),\n                  cboCtx.nodeOfInterest.getTokenStopIndex());\n              // Redo create-table/view analysis, because it's not part of\n              // doPhase1.\n              // Use the REWRITTEN AST\n              init(false);\n              setAST(newAST);\n              newAST = reAnalyzeViewAfterCbo(newAST);\n              createVwDesc.setViewOriginalText(originalText);\n              createVwDesc.setViewExpandedText(expandedText);\n              viewSelect = newAST;\n              viewsExpanded = new ArrayList<>();\n              viewsExpanded.add(createVwDesc.getViewName());\n            } else if (cboCtx.type == PreCboCtx.Type.CTAS) {\n              // CTAS\n              init(false);\n              setAST(newAST);\n              newAST = reAnalyzeCTASAfterCbo(newAST);\n            } else {\n              // All others\n              init(false);\n            }\n            if (oldHints.size() > 0) {\n              if (getQB().getParseInfo().getHints() != null) {\n                LOG.warn(\"Hints are not null in the optimized tree; \"\n                    + \"after CBO \" + getQB().getParseInfo().getHints().dump());\n              } else {\n                LOG.debug(\"Propagating hints to QB: \" + oldHints);\n                getQB().getParseInfo().setHintList(oldHints);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (this.ctx.isExplainPlan()) {\n              ExplainConfiguration explainConfig = this.ctx.getExplainConfig();\n              if (explainConfig.isExtended() || explainConfig.isFormatted()) {\n                this.ctx.setOptimizedSql(getOptimizedSql(newPlan));\n              }\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(getOptimizedSql(newPlan));\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if( e instanceof CalciteViewSemanticException) {\n            // non-cbo path retries to execute create view and\n            // we believe it will throw the same error message\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n            // for tests we would like to avoid retrying to catch cbo failures\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else if (e instanceof SemanticException) {\n            throw e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }",
            " 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 +\n 492  \n 493 +\n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "  @Override\n  @SuppressWarnings(\"rawtypes\")\n  Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticException {\n    Operator sinkOp = null;\n    boolean skipCalcitePlan = false;\n\n    if (!runCBO) {\n      skipCalcitePlan = true;\n    } else {\n      PreCboCtx cboCtx = (PreCboCtx) plannerCtx;\n      List<ASTNode> oldHints = new ArrayList<>();\n      // Cache the hints before CBO runs and removes them.\n      // Use the hints later in top level QB.\n        getHintsFromQB(getQB(), oldHints);\n\n      // Note: for now, we don't actually pass the queryForCbo to CBO, because\n      // it accepts qb, not AST, and can also access all the private stuff in\n      // SA. We rely on the fact that CBO ignores the unknown tokens (create\n      // table, destination), so if the query is otherwise ok, it is as if we\n      // did remove those and gave CBO the proper AST. That is kinda hacky.\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS || cboCtx.type == PreCboCtx.Type.VIEW) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canCBOHandleAst(queryForCbo, getQB(), cboCtx);\n      if (queryProperties.hasMultiDestQuery()) {\n        handleMultiDestQuery(ast, cboCtx);\n      }\n\n      if (runCBO) {\n        profilesCBO = obtainCBOProfiles(queryProperties);\n\n        disableJoinMerge = true;\n        boolean reAnalyzeAST = false;\n        final boolean materializedView = getQB().isMaterializedView();\n\n        try {\n          if (this.conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n            if (cboCtx.type == PreCboCtx.Type.VIEW && !materializedView) {\n              throw new SemanticException(\"Create view is not supported in cbo return path.\");\n            }\n            sinkOp = getOptimizedHiveOPDag();\n            if (oldHints.size() > 0) {\n              LOG.debug(\"Propagating hints to QB: \" + oldHints);\n              getQB().getParseInfo().setHintList(oldHints);\n            }\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n          } else {\n            // 0. Gen Optimized Plan\n            final RelNode newPlan = logicalPlan();\n            // 1. Convert Plan to AST\n            ASTNode newAST = getOptimizedAST(newPlan);\n\n            // 1.1. Fix up the query for insert/ctas/materialized views\n            newAST = fixUpAfterCbo(ast, newAST, cboCtx);\n\n            // 1.2. Fix up the query for materialization rebuild\n            if (mvRebuildMode == MaterializationRebuildMode.AGGREGATE_REBUILD) {\n              fixUpASTAggregateIncrementalRebuild(newAST);\n            } else if (mvRebuildMode == MaterializationRebuildMode.NO_AGGREGATE_REBUILD) {\n              fixUpASTNoAggregateIncrementalRebuild(newAST);\n            }\n\n            // 2. Regen OP plan from optimized AST\n            if (cboCtx.type == PreCboCtx.Type.VIEW) {\n              try {\n                viewSelect = handleCreateViewDDL(newAST);\n              } catch (SemanticException e) {\n                throw new CalciteViewSemanticException(e.getMessage());\n              }\n            } else if (cboCtx.type == PreCboCtx.Type.CTAS) {\n              // CTAS\n              init(false);\n              setAST(newAST);\n              newAST = reAnalyzeCTASAfterCbo(newAST);\n            } else {\n              // All others\n              init(false);\n            }\n            if (oldHints.size() > 0) {\n              if (getQB().getParseInfo().getHints() != null) {\n                LOG.warn(\"Hints are not null in the optimized tree; \"\n                    + \"after CBO \" + getQB().getParseInfo().getHints().dump());\n              } else {\n                LOG.debug(\"Propagating hints to QB: \" + oldHints);\n                getQB().getParseInfo().setHintList(oldHints);\n              }\n            }\n            Phase1Ctx ctx_1 = initPhase1Ctx();\n            if (!doPhase1(newAST, getQB(), ctx_1, null)) {\n              throw new RuntimeException(\"Couldn't do phase1 on CBO optimized query plan\");\n            }\n            // unfortunately making prunedPartitions immutable is not possible\n            // here with SemiJoins not all tables are costed in CBO, so their\n            // PartitionList is not evaluated until the run phase.\n            getMetaData(getQB());\n\n            disableJoinMerge = defaultJoinMerge;\n            sinkOp = genPlan(getQB());\n            LOG.info(\"CBO Succeeded; optimized logical plan.\");\n            this.ctx.setCboInfo(\"Plan optimized by CBO.\");\n            this.ctx.setCboSucceeded(true);\n            if (this.ctx.isExplainPlan()) {\n              ExplainConfiguration explainConfig = this.ctx.getExplainConfig();\n              if (explainConfig.isExtended() || explainConfig.isFormatted()) {\n                this.ctx.setOptimizedSql(getOptimizedSql(newPlan));\n              }\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(getOptimizedSql(newPlan));\n              LOG.trace(newAST.dump());\n            }\n          }\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n            this.ctx\n                .setCboInfo(\"Plan not optimized by CBO due to missing statistics. Please check log for more details.\");\n          } else {\n            LOG.error(\"CBO failed, skipping CBO. \", e);\n            if (e instanceof CalciteSemanticException) {\n              CalciteSemanticException calciteSemanticException = (CalciteSemanticException) e;\n              UnsupportedFeature unsupportedFeature = calciteSemanticException\n                  .getUnsupportedFeature();\n              if (unsupportedFeature != null) {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO due to missing feature [\"\n                    + unsupportedFeature + \"].\");\n              } else {\n                this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n              }\n            } else {\n              this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n            }\n          }\n          if( e instanceof CalciteSubquerySemanticException) {\n            // non-cbo path retries to execute subqueries and throws completely different exception/error\n            // to eclipse the original error message\n            // so avoid executing subqueries on non-cbo\n            throw new SemanticException(e);\n          }\n          else if( e instanceof CalciteViewSemanticException) {\n            // non-cbo path retries to execute create view and\n            // we believe it will throw the same error message\n            throw new SemanticException(e);\n          }\n          else if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n              || e instanceof CalciteSemanticException ) {\n              reAnalyzeAST = true;\n          } else if (e instanceof SemanticException && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {\n            // although, its likely to be a valid exception, we will retry\n            // with cbo off anyway.\n            // for tests we would like to avoid retrying to catch cbo failures\n              reAnalyzeAST = true;\n          } else if (e instanceof RuntimeException) {\n            throw (RuntimeException) e;\n          } else if (e instanceof SemanticException) {\n            throw e;\n          } else {\n            throw new SemanticException(e);\n          }\n        } finally {\n          runCBO = false;\n          disableJoinMerge = defaultJoinMerge;\n          disableSemJoinReordering = false;\n          if (reAnalyzeAST) {\n            init(true);\n            prunedPartitions.clear();\n            // Assumption: At this point Parse Tree gen & resolution will always\n            // be true (since we started out that way).\n            super.genResolvedParseTree(ast, new PlannerContext());\n            skipCalcitePlan = true;\n          }\n        }\n      } else {\n        this.ctx.setCboInfo(\"Plan not optimized by CBO.\");\n        skipCalcitePlan = true;\n      }\n    }\n\n    if (skipCalcitePlan) {\n      sinkOp = super.genOPTree(ast, plannerCtx);\n    }\n\n    return sinkOp;\n  }"
        ],
        [
            "CalcitePlanner::CalcitePlannerAction::apply(RelOptCluster,RelOptSchema,SchemaPlus)",
            "1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742 -\n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  ",
            "    @Override\n    public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlus rootSchema) {\n      RelNode calciteGenPlan = null;\n      RelNode calcitePreCboPlan = null;\n      RelNode calciteOptimizedPlan = null;\n      subqueryId = -1;\n\n      /*\n       * recreate cluster, so that it picks up the additional traitDef\n       */\n      RelOptPlanner planner = createPlanner(conf, corrScalarRexSQWithAgg, scalarAggNoGbyNoWin);\n      final RexBuilder rexBuilder = cluster.getRexBuilder();\n      final RelOptCluster optCluster = RelOptCluster.create(planner, rexBuilder);\n\n      this.cluster = optCluster;\n      this.relOptSchema = relOptSchema;\n\n      PerfLogger perfLogger = SessionState.getPerfLogger();\n      // 1. Gen Calcite Plan\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      try {\n        calciteGenPlan = genLogicalPlan(getQB(), true, null, null);\n        // if it is to create view, we do not use table alias\n        resultSchema = SemanticAnalyzer.convertRowSchemaToResultSetSchema(\n            relToHiveRR.get(calciteGenPlan),\n            getQB().isView() ? false : HiveConf.getBoolVar(conf,\n                HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));\n      } catch (SemanticException e) {\n        semanticException = e;\n        throw new RuntimeException(e);\n      }\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Plan generation\");\n\n      // Validate query materialization (materialized views, query results caching.\n      // This check needs to occur before constant folding, which may remove some\n      // function calls from the query plan.\n      HiveRelOpMaterializationValidator matValidator = new HiveRelOpMaterializationValidator();\n      matValidator.validateQueryMaterialization(calciteGenPlan);\n      if (!matValidator.isValidMaterialization()) {\n        String reason = matValidator.getInvalidMaterializationReason();\n        setInvalidQueryMaterializationReason(reason);\n      }\n\n      // Create executor\n      RexExecutor executorProvider = new HiveRexExecutorImpl(optCluster);\n      calciteGenPlan.getCluster().getPlanner().setExecutor(executorProvider);\n\n      // We need to get the ColumnAccessInfo and viewToTableSchema for views.\n      HiveRelFieldTrimmer fieldTrimmer = new HiveRelFieldTrimmer(null,\n          HiveRelFactories.HIVE_BUILDER.create(optCluster, null), this.columnAccessInfo,\n          this.viewProjectToTableSchema);\n\n      fieldTrimmer.trim(calciteGenPlan);\n\n      // Create and set MD provider\n      HiveDefaultRelMetadataProvider mdProvider = new HiveDefaultRelMetadataProvider(conf);\n      RelMetadataQuery.THREAD_PROVIDERS.set(\n              JaninoRelMetadataProvider.of(mdProvider.getMetadataProvider()));\n\n      //Remove subquery\n      LOG.debug(\"Plan before removing subquery:\\n\" + RelOptUtil.toString(calciteGenPlan));\n      calciteGenPlan = hepPlan(calciteGenPlan, false, mdProvider.getMetadataProvider(), null,\n              new HiveSubQueryRemoveRule(conf));\n      LOG.debug(\"Plan just after removing subquery:\\n\" + RelOptUtil.toString(calciteGenPlan));\n\n      calciteGenPlan = HiveRelDecorrelator.decorrelateQuery(calciteGenPlan);\n      LOG.debug(\"Plan after decorrelation:\\n\" + RelOptUtil.toString(calciteGenPlan));\n\n      // 2. Apply pre-join order optimizations\n      calcitePreCboPlan = applyPreJoinOrderingTransforms(calciteGenPlan,\n              mdProvider.getMetadataProvider(), executorProvider);\n\n      // 3. Materialized view based rewriting\n      // We disable it for CTAS and MV creation queries (trying to avoid any problem\n      // due to data freshness)\n      if (conf.getBoolVar(ConfVars.HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING) &&\n              !getQB().isMaterializedView() && !ctx.isLoadingMaterializedView() && !getQB().isCTAS()) {\n        calcitePreCboPlan = applyMaterializedViewRewriting(planner,\n            calcitePreCboPlan, mdProvider.getMetadataProvider(), executorProvider);\n      }\n\n      // Get rid of sq_count_check if group by key is constant\n      if (conf.getBoolVar(ConfVars.HIVE_REMOVE_SQ_COUNT_CHECK)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calcitePreCboPlan =\n            hepPlan(calcitePreCboPlan, false, mdProvider.getMetadataProvider(), null,\n                    HiveRemoveSqCountCheck.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER,\n                              \"Calcite: Removing sq_count_check UDF \");\n      }\n      //  4.1 Remove Projects between Joins so that JoinToMultiJoinRule can merge them to MultiJoin.\n      //    Don't run this rule if hive is to remove sq_count_check since that rule expects to have project b/w join.\n        calcitePreCboPlan = hepPlan(calcitePreCboPlan, true, mdProvider.getMetadataProvider(), executorProvider,\n                                    HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.LEFF_PROJECT_BTW_JOIN,\n                                    HiveJoinProjectTransposeRule.RIGHT_PROJECT_BTW_JOIN);\n\n      // 4.2 Apply join order optimizations: reordering MST algorithm\n      //    If join optimizations failed because of missing stats, we continue with\n      //    the rest of optimizations\n      if (profilesCBO.contains(ExtendedCBOProfile.JOIN_REORDERING)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        try {\n          List<RelMetadataProvider> list = Lists.newArrayList();\n          list.add(mdProvider.getMetadataProvider());\n          RelTraitSet desiredTraits = optCluster\n              .traitSetOf(HiveRelNode.CONVENTION, RelCollations.EMPTY);\n\n          HepProgramBuilder hepPgmBldr = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP);\n          hepPgmBldr.addRuleInstance(new JoinToMultiJoinRule(HiveJoin.class));\n          hepPgmBldr.addRuleInstance(new LoptOptimizeJoinRule(HiveRelFactories.HIVE_BUILDER));\n\n          HepProgram hepPgm = hepPgmBldr.build();\n          HepPlanner hepPlanner = new HepPlanner(hepPgm);\n\n          hepPlanner.registerMetadataProviders(list);\n          RelMetadataProvider chainedProvider = ChainedRelMetadataProvider.of(list);\n          optCluster.setMetadataProvider(new CachingRelMetadataProvider(chainedProvider, hepPlanner));\n\n          RelNode rootRel = calcitePreCboPlan;\n          hepPlanner.setRoot(rootRel);\n          if (!calcitePreCboPlan.getTraitSet().equals(desiredTraits)) {\n            rootRel = hepPlanner.changeTraits(calcitePreCboPlan, desiredTraits);\n          }\n          hepPlanner.setRoot(rootRel);\n\n          calciteOptimizedPlan = hepPlanner.findBestExp();\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.warn(\"Missing column stats (see previous messages), skipping join reordering in CBO\");\n            noColsMissingStats.set(0);\n            calciteOptimizedPlan = calcitePreCboPlan;\n            disableSemJoinReordering = false;\n          } else {\n            throw e;\n          }\n        }\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Join Reordering\");\n      } else {\n        calciteOptimizedPlan = calcitePreCboPlan;\n        disableSemJoinReordering = false;\n      }\n\n      // 5. Run other optimizations that do not need stats\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n          HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, HiveUnionMergeRule.INSTANCE,\n          HiveAggregateProjectMergeRule.INSTANCE, HiveProjectMergeRule.INSTANCE_NO_FORCE, HiveJoinCommuteRule.INSTANCE);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Optimizations without stats 1\");\n\n      // 6. Run aggregate-join transpose (cost based)\n      //    If it failed because of missing stats, we continue with\n      //    the rest of optimizations\n      if (conf.getBoolVar(ConfVars.AGGR_JOIN_TRANSPOSE)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        try {\n          calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                  HepMatchOrder.BOTTOM_UP, HiveAggregateJoinTransposeRule.INSTANCE);\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.warn(\"Missing column stats (see previous messages), skipping aggregate-join transpose in CBO\");\n            noColsMissingStats.set(0);\n          } else {\n            throw e;\n          }\n        }\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Aggregate join transpose\");\n      }\n\n      // 7.convert Join + GBy to semijoin\n      // run this rule at later stages, since many calcite rules cant deal with semijoin\n      if (conf.getBoolVar(ConfVars.SEMIJOIN_CONVERSION)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HiveSemiJoinRule.INSTANCE_PROJECT, HiveSemiJoinRule.INSTANCE_AGGREGATE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Semijoin conversion\");\n      }\n\n      // 8. convert SemiJoin + GBy to SemiJoin\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n            HiveRemoveGBYSemiJoinRule.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Removal of gby from semijoin\");\n\n\n      // 9. Run rule to fix windowing issue when it is done over\n      // aggregation columns (HIVE-10627)\n      if (profilesCBO.contains(ExtendedCBOProfile.WINDOWING_POSTPROCESSING)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveWindowingFixRule.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Window fixing rule\");\n      }\n\n      // 10. Apply Druid transformation rules\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n          HepMatchOrder.BOTTOM_UP,\n          HiveDruidRules.FILTER, HiveDruidRules.PROJECT_FILTER_TRANSPOSE,\n          HiveDruidRules.AGGREGATE_FILTER_TRANSPOSE,\n          HiveDruidRules.AGGREGATE_PROJECT,\n          HiveDruidRules.PROJECT,\n          HiveDruidRules.EXPAND_SINGLE_DISTINCT_AGGREGATES_DRUID_RULE,\n          HiveDruidRules.AGGREGATE,\n          HiveDruidRules.POST_AGGREGATION_PROJECT,\n          HiveDruidRules.FILTER_AGGREGATE_TRANSPOSE,\n          HiveDruidRules.FILTER_PROJECT_TRANSPOSE,\n          HiveDruidRules.HAVING_FILTER_RULE,\n          HiveDruidRules.SORT_PROJECT_TRANSPOSE,\n          HiveDruidRules.SORT,\n          HiveDruidRules.PROJECT_SORT_TRANSPOSE\n      );\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Druid transformation rules\");\n\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n              HepMatchOrder.TOP_DOWN,\n              JDBCExtractJoinFilterRule.INSTANCE,\n              JDBCAbstractSplitFilterRule.SPLIT_FILTER_ABOVE_JOIN,\n              JDBCAbstractSplitFilterRule.SPLIT_FILTER_ABOVE_CONVERTER,\n              JDBCFilterJoinRule.INSTANCE,\n              JDBCJoinPushDownRule.INSTANCE, JDBCUnionPushDownRule.INSTANCE,\n              JDBCFilterPushDownRule.INSTANCE, JDBCProjectPushDownRule.INSTANCE,\n              JDBCAggregationPushDownRule.INSTANCE, JDBCSortPushDownRule.INSTANCE\n      );\n\n      // 11. Run rules to aid in translation from Calcite tree to Hive tree\n      if (HiveConf.getBoolVar(conf, ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        // 12.1. Merge join into multijoin operators (if possible)\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER,\n                HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER,\n                HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER,\n                HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);\n        // The previous rules can pull up projections through join operators,\n        // thus we run the field trimmer again to push them back down\n        fieldTrimmer = new HiveRelFieldTrimmer(null,\n            HiveRelFactories.HIVE_BUILDER.create(optCluster, null));\n        calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE,\n                new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n                HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID,\n                HiveProjectFilterPullUpConstantsRule.INSTANCE);\n\n        // 11.2.  Introduce exchange operators below join/multijoin operators\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN,\n                HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Translation from Calcite tree to Hive tree\");\n      }\n\n      if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {\n        LOG.debug(\"CBO Planning details:\\n\");\n        LOG.debug(\"Original Plan:\\n\" + RelOptUtil.toString(calciteGenPlan));\n        LOG.debug(\"Plan After PPD, PartPruning, ColumnPruning:\\n\"\n            + RelOptUtil.toString(calcitePreCboPlan));\n        LOG.debug(\"Plan After Join Reordering:\\n\"\n            + RelOptUtil.toString(calciteOptimizedPlan, SqlExplainLevel.ALL_ATTRIBUTES));\n      }\n\n      return calciteOptimizedPlan;\n    }",
            "1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723 +\n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  ",
            "    @Override\n    public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlus rootSchema) {\n      RelNode calciteGenPlan = null;\n      RelNode calcitePreCboPlan = null;\n      RelNode calciteOptimizedPlan = null;\n      subqueryId = -1;\n\n      /*\n       * recreate cluster, so that it picks up the additional traitDef\n       */\n      RelOptPlanner planner = createPlanner(conf, corrScalarRexSQWithAgg, scalarAggNoGbyNoWin);\n      final RexBuilder rexBuilder = cluster.getRexBuilder();\n      final RelOptCluster optCluster = RelOptCluster.create(planner, rexBuilder);\n\n      this.cluster = optCluster;\n      this.relOptSchema = relOptSchema;\n\n      PerfLogger perfLogger = SessionState.getPerfLogger();\n      // 1. Gen Calcite Plan\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      try {\n        calciteGenPlan = genLogicalPlan(getQB(), true, null, null);\n        // if it is to create view, we do not use table alias\n        resultSchema = SemanticAnalyzer.convertRowSchemaToResultSetSchema(\n            relToHiveRR.get(calciteGenPlan),\n            getQB().isView() || getQB().isMaterializedView() ? false : HiveConf.getBoolVar(conf,\n                HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));\n      } catch (SemanticException e) {\n        semanticException = e;\n        throw new RuntimeException(e);\n      }\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Plan generation\");\n\n      // Validate query materialization (materialized views, query results caching.\n      // This check needs to occur before constant folding, which may remove some\n      // function calls from the query plan.\n      HiveRelOpMaterializationValidator matValidator = new HiveRelOpMaterializationValidator();\n      matValidator.validateQueryMaterialization(calciteGenPlan);\n      if (!matValidator.isValidMaterialization()) {\n        String reason = matValidator.getInvalidMaterializationReason();\n        setInvalidQueryMaterializationReason(reason);\n      }\n\n      // Create executor\n      RexExecutor executorProvider = new HiveRexExecutorImpl(optCluster);\n      calciteGenPlan.getCluster().getPlanner().setExecutor(executorProvider);\n\n      // We need to get the ColumnAccessInfo and viewToTableSchema for views.\n      HiveRelFieldTrimmer fieldTrimmer = new HiveRelFieldTrimmer(null,\n          HiveRelFactories.HIVE_BUILDER.create(optCluster, null), this.columnAccessInfo,\n          this.viewProjectToTableSchema);\n\n      fieldTrimmer.trim(calciteGenPlan);\n\n      // Create and set MD provider\n      HiveDefaultRelMetadataProvider mdProvider = new HiveDefaultRelMetadataProvider(conf);\n      RelMetadataQuery.THREAD_PROVIDERS.set(\n              JaninoRelMetadataProvider.of(mdProvider.getMetadataProvider()));\n\n      //Remove subquery\n      LOG.debug(\"Plan before removing subquery:\\n\" + RelOptUtil.toString(calciteGenPlan));\n      calciteGenPlan = hepPlan(calciteGenPlan, false, mdProvider.getMetadataProvider(), null,\n              new HiveSubQueryRemoveRule(conf));\n      LOG.debug(\"Plan just after removing subquery:\\n\" + RelOptUtil.toString(calciteGenPlan));\n\n      calciteGenPlan = HiveRelDecorrelator.decorrelateQuery(calciteGenPlan);\n      LOG.debug(\"Plan after decorrelation:\\n\" + RelOptUtil.toString(calciteGenPlan));\n\n      // 2. Apply pre-join order optimizations\n      calcitePreCboPlan = applyPreJoinOrderingTransforms(calciteGenPlan,\n              mdProvider.getMetadataProvider(), executorProvider);\n\n      // 3. Materialized view based rewriting\n      // We disable it for CTAS and MV creation queries (trying to avoid any problem\n      // due to data freshness)\n      if (conf.getBoolVar(ConfVars.HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING) &&\n              !getQB().isMaterializedView() && !ctx.isLoadingMaterializedView() && !getQB().isCTAS()) {\n        calcitePreCboPlan = applyMaterializedViewRewriting(planner,\n            calcitePreCboPlan, mdProvider.getMetadataProvider(), executorProvider);\n      }\n\n      // Get rid of sq_count_check if group by key is constant\n      if (conf.getBoolVar(ConfVars.HIVE_REMOVE_SQ_COUNT_CHECK)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calcitePreCboPlan =\n            hepPlan(calcitePreCboPlan, false, mdProvider.getMetadataProvider(), null,\n                    HiveRemoveSqCountCheck.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER,\n                              \"Calcite: Removing sq_count_check UDF \");\n      }\n      //  4.1 Remove Projects between Joins so that JoinToMultiJoinRule can merge them to MultiJoin.\n      //    Don't run this rule if hive is to remove sq_count_check since that rule expects to have project b/w join.\n        calcitePreCboPlan = hepPlan(calcitePreCboPlan, true, mdProvider.getMetadataProvider(), executorProvider,\n                                    HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.LEFF_PROJECT_BTW_JOIN,\n                                    HiveJoinProjectTransposeRule.RIGHT_PROJECT_BTW_JOIN);\n\n      // 4.2 Apply join order optimizations: reordering MST algorithm\n      //    If join optimizations failed because of missing stats, we continue with\n      //    the rest of optimizations\n      if (profilesCBO.contains(ExtendedCBOProfile.JOIN_REORDERING)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        try {\n          List<RelMetadataProvider> list = Lists.newArrayList();\n          list.add(mdProvider.getMetadataProvider());\n          RelTraitSet desiredTraits = optCluster\n              .traitSetOf(HiveRelNode.CONVENTION, RelCollations.EMPTY);\n\n          HepProgramBuilder hepPgmBldr = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP);\n          hepPgmBldr.addRuleInstance(new JoinToMultiJoinRule(HiveJoin.class));\n          hepPgmBldr.addRuleInstance(new LoptOptimizeJoinRule(HiveRelFactories.HIVE_BUILDER));\n\n          HepProgram hepPgm = hepPgmBldr.build();\n          HepPlanner hepPlanner = new HepPlanner(hepPgm);\n\n          hepPlanner.registerMetadataProviders(list);\n          RelMetadataProvider chainedProvider = ChainedRelMetadataProvider.of(list);\n          optCluster.setMetadataProvider(new CachingRelMetadataProvider(chainedProvider, hepPlanner));\n\n          RelNode rootRel = calcitePreCboPlan;\n          hepPlanner.setRoot(rootRel);\n          if (!calcitePreCboPlan.getTraitSet().equals(desiredTraits)) {\n            rootRel = hepPlanner.changeTraits(calcitePreCboPlan, desiredTraits);\n          }\n          hepPlanner.setRoot(rootRel);\n\n          calciteOptimizedPlan = hepPlanner.findBestExp();\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.warn(\"Missing column stats (see previous messages), skipping join reordering in CBO\");\n            noColsMissingStats.set(0);\n            calciteOptimizedPlan = calcitePreCboPlan;\n            disableSemJoinReordering = false;\n          } else {\n            throw e;\n          }\n        }\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Join Reordering\");\n      } else {\n        calciteOptimizedPlan = calcitePreCboPlan;\n        disableSemJoinReordering = false;\n      }\n\n      // 5. Run other optimizations that do not need stats\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n          HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE, HiveUnionMergeRule.INSTANCE,\n          HiveAggregateProjectMergeRule.INSTANCE, HiveProjectMergeRule.INSTANCE_NO_FORCE, HiveJoinCommuteRule.INSTANCE);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Optimizations without stats 1\");\n\n      // 6. Run aggregate-join transpose (cost based)\n      //    If it failed because of missing stats, we continue with\n      //    the rest of optimizations\n      if (conf.getBoolVar(ConfVars.AGGR_JOIN_TRANSPOSE)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        try {\n          calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                  HepMatchOrder.BOTTOM_UP, HiveAggregateJoinTransposeRule.INSTANCE);\n        } catch (Exception e) {\n          boolean isMissingStats = noColsMissingStats.get() > 0;\n          if (isMissingStats) {\n            LOG.warn(\"Missing column stats (see previous messages), skipping aggregate-join transpose in CBO\");\n            noColsMissingStats.set(0);\n          } else {\n            throw e;\n          }\n        }\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Aggregate join transpose\");\n      }\n\n      // 7.convert Join + GBy to semijoin\n      // run this rule at later stages, since many calcite rules cant deal with semijoin\n      if (conf.getBoolVar(ConfVars.SEMIJOIN_CONVERSION)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HiveSemiJoinRule.INSTANCE_PROJECT, HiveSemiJoinRule.INSTANCE_AGGREGATE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Semijoin conversion\");\n      }\n\n      // 8. convert SemiJoin + GBy to SemiJoin\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n            HiveRemoveGBYSemiJoinRule.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Removal of gby from semijoin\");\n\n\n      // 9. Run rule to fix windowing issue when it is done over\n      // aggregation columns (HIVE-10627)\n      if (profilesCBO.contains(ExtendedCBOProfile.WINDOWING_POSTPROCESSING)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveWindowingFixRule.INSTANCE);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Window fixing rule\");\n      }\n\n      // 10. Apply Druid transformation rules\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n          HepMatchOrder.BOTTOM_UP,\n          HiveDruidRules.FILTER, HiveDruidRules.PROJECT_FILTER_TRANSPOSE,\n          HiveDruidRules.AGGREGATE_FILTER_TRANSPOSE,\n          HiveDruidRules.AGGREGATE_PROJECT,\n          HiveDruidRules.PROJECT,\n          HiveDruidRules.EXPAND_SINGLE_DISTINCT_AGGREGATES_DRUID_RULE,\n          HiveDruidRules.AGGREGATE,\n          HiveDruidRules.POST_AGGREGATION_PROJECT,\n          HiveDruidRules.FILTER_AGGREGATE_TRANSPOSE,\n          HiveDruidRules.FILTER_PROJECT_TRANSPOSE,\n          HiveDruidRules.HAVING_FILTER_RULE,\n          HiveDruidRules.SORT_PROJECT_TRANSPOSE,\n          HiveDruidRules.SORT,\n          HiveDruidRules.PROJECT_SORT_TRANSPOSE\n      );\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Druid transformation rules\");\n\n      calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n              HepMatchOrder.TOP_DOWN,\n              JDBCExtractJoinFilterRule.INSTANCE,\n              JDBCAbstractSplitFilterRule.SPLIT_FILTER_ABOVE_JOIN,\n              JDBCAbstractSplitFilterRule.SPLIT_FILTER_ABOVE_CONVERTER,\n              JDBCFilterJoinRule.INSTANCE,\n              JDBCJoinPushDownRule.INSTANCE, JDBCUnionPushDownRule.INSTANCE,\n              JDBCFilterPushDownRule.INSTANCE, JDBCProjectPushDownRule.INSTANCE,\n              JDBCAggregationPushDownRule.INSTANCE, JDBCSortPushDownRule.INSTANCE\n      );\n\n      // 11. Run rules to aid in translation from Calcite tree to Hive tree\n      if (HiveConf.getBoolVar(conf, ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n        perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);\n        // 12.1. Merge join into multijoin operators (if possible)\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT_INCLUDE_OUTER,\n                HiveJoinProjectTransposeRule.LEFT_PROJECT_INCLUDE_OUTER,\n                HiveJoinProjectTransposeRule.RIGHT_PROJECT_INCLUDE_OUTER,\n                HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);\n        // The previous rules can pull up projections through join operators,\n        // thus we run the field trimmer again to push them back down\n        fieldTrimmer = new HiveRelFieldTrimmer(null,\n            HiveRelFactories.HIVE_BUILDER.create(optCluster, null));\n        calciteOptimizedPlan = fieldTrimmer.trim(calciteOptimizedPlan);\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, ProjectRemoveRule.INSTANCE,\n                new ProjectMergeRule(false, HiveRelFactories.HIVE_BUILDER));\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(), null,\n                HiveFilterProjectTSTransposeRule.INSTANCE, HiveFilterProjectTSTransposeRule.INSTANCE_DRUID,\n                HiveProjectFilterPullUpConstantsRule.INSTANCE);\n\n        // 11.2.  Introduce exchange operators below join/multijoin operators\n        calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, false, mdProvider.getMetadataProvider(), null,\n                HepMatchOrder.BOTTOM_UP, HiveInsertExchange4JoinRule.EXCHANGE_BELOW_JOIN,\n                HiveInsertExchange4JoinRule.EXCHANGE_BELOW_MULTIJOIN);\n        perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, \"Calcite: Translation from Calcite tree to Hive tree\");\n      }\n\n      if (LOG.isDebugEnabled() && !conf.getBoolVar(ConfVars.HIVE_IN_TEST)) {\n        LOG.debug(\"CBO Planning details:\\n\");\n        LOG.debug(\"Original Plan:\\n\" + RelOptUtil.toString(calciteGenPlan));\n        LOG.debug(\"Plan After PPD, PartPruning, ColumnPruning:\\n\"\n            + RelOptUtil.toString(calcitePreCboPlan));\n        LOG.debug(\"Plan After Join Reordering:\\n\"\n            + RelOptUtil.toString(calciteOptimizedPlan, SqlExplainLevel.ALL_ATTRIBUTES));\n      }\n\n      return calciteOptimizedPlan;\n    }"
        ]
    ],
    "8ebde04411a4ca34994019db75b57ebee9c28f71": [
        [
            "HiveProtoLoggingHook::EventLogger::writeEvent(HiveHookEventProto)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  ",
            "    private void writeEvent(HiveHookEventProto event) {\n      for (int retryCount = 0; retryCount <= MAX_RETRIES; ++retryCount) {\n        try {\n          if (writer == null || !logger.getNow().toLocalDate().equals(writerDate)) {\n            if (writer != null) {\n              // Day change over case, reset the logFileCount.\n              logFileCount = 0;\n              IOUtils.closeQuietly(writer);\n            }\n            // increment log file count, if creating a new writer.\n            writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n            writerDate = logger.getDateFromDir(writer.getPath().getParent().getName());\n          }\n          writer.writeProto(event);\n          writer.hflush();\n          return;\n        } catch (IOException e) {\n          // Something wrong with writer, lets close and reopen.\n          IOUtils.closeQuietly(writer);\n          writer = null;\n          if (retryCount < MAX_RETRIES) {\n            LOG.warn(\"Error writing proto message for query {}, eventType: {}, retryCount: {},\" +\n                \" error: {} \", event.getHiveQueryId(), event.getEventType(), retryCount,\n                e.getMessage());\n          } else {\n            LOG.error(\"Error writing proto message for query {}, eventType: {}: \",\n                event.getHiveQueryId(), event.getEventType(), e);\n          }\n          try {\n            // 0 seconds, for first retry assuming fs object was closed and open will fix it.\n            Thread.sleep(1000 * retryCount * retryCount);\n          } catch (InterruptedException e1) {\n            LOG.warn(\"Got interrupted in retry sleep.\", e1);\n          }\n        }\n      }\n    }",
            " 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 +\n 296 +\n 297 +\n 298 +\n 299 +\n 300 +\n 301 +\n 302 +\n 303 +\n 304 +\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  ",
            "    private void writeEvent(HiveHookEventProto event) {\n      for (int retryCount = 0; retryCount <= MAX_RETRIES; ++retryCount) {\n        try {\n          if (writer == null || !logger.getNow().toLocalDate().equals(writerDate)) {\n            if (writer != null) {\n              // Day change over case, reset the logFileCount.\n              logFileCount = 0;\n              IOUtils.closeQuietly(writer);\n            }\n            // increment log file count, if creating a new writer.\n            writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n            writerDate = logger.getDateFromDir(writer.getPath().getParent().getName());\n          }\n          writer.writeProto(event);\n          if (eventPerFile) {\n            if (writer != null) {\n              LOG.debug(\"Event per file enabled. Closing proto event file: {}\", writer.getPath());\n              IOUtils.closeQuietly(writer);\n            }\n            // rollover to next file\n            writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n          } else {\n            writer.hflush();\n          }\n          return;\n        } catch (IOException e) {\n          // Something wrong with writer, lets close and reopen.\n          IOUtils.closeQuietly(writer);\n          writer = null;\n          if (retryCount < MAX_RETRIES) {\n            LOG.warn(\"Error writing proto message for query {}, eventType: {}, retryCount: {},\" +\n                \" error: {} \", event.getHiveQueryId(), event.getEventType(), retryCount,\n                e.getMessage());\n          } else {\n            LOG.error(\"Error writing proto message for query {}, eventType: {}: \",\n                event.getHiveQueryId(), event.getEventType(), e);\n          }\n          try {\n            // 0 seconds, for first retry assuming fs object was closed and open will fix it.\n            Thread.sleep(1000 * retryCount * retryCount);\n          } catch (InterruptedException e1) {\n            LOG.warn(\"Got interrupted in retry sleep.\", e1);\n          }\n        }\n      }\n    }"
        ],
        [
            "HiveProtoLoggingHook::EventLogger::EventLogger(HiveConf,Clock)",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "    EventLogger(HiveConf conf, Clock clock) {\n      this.clock = clock;\n      // randomUUID is slow, since its cryptographically secure, only first query will take time.\n      this.logFileName = \"hive_\" + UUID.randomUUID().toString();\n      String baseDir = conf.getVar(ConfVars.HIVE_PROTO_EVENTS_BASE_PATH);\n      if (StringUtils.isBlank(baseDir)) {\n        baseDir = null;\n        LOG.error(ConfVars.HIVE_PROTO_EVENTS_BASE_PATH.varname + \" is not set, logging disabled.\");\n      }\n\n      DatePartitionedLogger<HiveHookEventProto> tmpLogger = null;\n      try {\n        if (baseDir != null) {\n          tmpLogger = new DatePartitionedLogger<>(HiveHookEventProto.PARSER, new Path(baseDir),\n              conf, clock);\n        }\n      } catch (IOException e) {\n        LOG.error(\"Unable to intialize logger, logging disabled.\", e);\n      }\n      this.logger = tmpLogger;\n      if (logger == null) {\n        logWriter = null;\n        return;\n      }\n\n      int queueCapacity = conf.getInt(ConfVars.HIVE_PROTO_EVENTS_QUEUE_CAPACITY.varname,\n          HIVE_HOOK_PROTO_QUEUE_CAPACITY_DEFAULT);\n\n      ThreadFactory threadFactory = new ThreadFactoryBuilder().setDaemon(true)\n          .setNameFormat(\"Hive Hook Proto Log Writer %d\").build();\n      logWriter = new ThreadPoolExecutor(1, 1, 0, TimeUnit.MILLISECONDS,\n          new LinkedBlockingQueue<Runnable>(queueCapacity), threadFactory);\n    }",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  ",
            "    EventLogger(HiveConf conf, Clock clock) {\n      this.clock = clock;\n      // randomUUID is slow, since its cryptographically secure, only first query will take time.\n      this.logFileName = \"hive_\" + UUID.randomUUID().toString();\n      String baseDir = conf.getVar(ConfVars.HIVE_PROTO_EVENTS_BASE_PATH);\n      if (StringUtils.isBlank(baseDir)) {\n        baseDir = null;\n        LOG.error(ConfVars.HIVE_PROTO_EVENTS_BASE_PATH.varname + \" is not set, logging disabled.\");\n      }\n\n      eventPerFile = conf.getBoolVar(ConfVars.HIVE_PROTO_FILE_PER_EVENT);\n      LOG.info(\"Event per file enabled: {}\", eventPerFile);\n      DatePartitionedLogger<HiveHookEventProto> tmpLogger = null;\n      try {\n        if (baseDir != null) {\n          tmpLogger = new DatePartitionedLogger<>(HiveHookEventProto.PARSER, new Path(baseDir),\n              conf, clock);\n        }\n      } catch (IOException e) {\n        LOG.error(\"Unable to intialize logger, logging disabled.\", e);\n      }\n      this.logger = tmpLogger;\n      if (logger == null) {\n        logWriter = null;\n        return;\n      }\n\n      int queueCapacity = conf.getInt(ConfVars.HIVE_PROTO_EVENTS_QUEUE_CAPACITY.varname,\n          HIVE_HOOK_PROTO_QUEUE_CAPACITY_DEFAULT);\n\n      ThreadFactory threadFactory = new ThreadFactoryBuilder().setDaemon(true)\n          .setNameFormat(\"Hive Hook Proto Log Writer %d\").build();\n      logWriter = new ThreadPoolExecutor(1, 1, 0, TimeUnit.MILLISECONDS,\n          new LinkedBlockingQueue<Runnable>(queueCapacity), threadFactory);\n    }"
        ]
    ],
    "5f039a91094237d0bdab0ce7aba36b26ff834c31": [
        [
            "VectorizationContext::getVectorExpression(ExprNodeDesc,VectorExpressionDescriptor)",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843 -\n 844 -\n 845 -\n 846 -\n 847 -\n 848 -\n 849 -\n 850 -\n 851 -\n 852 -\n 853 -\n 854 -\n 855 -\n 856 -\n 857 -\n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  ",
            "  /**\n   * Returns a vector expression for a given expression\n   * description.\n   * @param exprDesc, Expression description\n   * @param mode\n   * @return {@link VectorExpression}\n   * @throws HiveException\n   */\n  public VectorExpression getVectorExpression(ExprNodeDesc exprDesc, VectorExpressionDescriptor.Mode mode) throws HiveException {\n    VectorExpression ve = null;\n    if (exprDesc instanceof ExprNodeColumnDesc) {\n      ve = getColumnVectorExpression((ExprNodeColumnDesc) exprDesc, mode);\n    } else if (exprDesc instanceof ExprNodeGenericFuncDesc) {\n      ExprNodeGenericFuncDesc expr = (ExprNodeGenericFuncDesc) exprDesc;\n      // push not through between...\n      if (\"not\".equals(expr.getFuncText())) {\n        if (expr.getChildren() != null && expr.getChildren().size() == 1) {\n          ExprNodeDesc child = expr.getChildren().get(0);\n          if (child instanceof ExprNodeGenericFuncDesc) {\n            ExprNodeGenericFuncDesc childExpr = (ExprNodeGenericFuncDesc) child;\n            if (\"between\".equals(childExpr.getFuncText())) {\n              ExprNodeConstantDesc flag = (ExprNodeConstantDesc) childExpr.getChildren().get(0);\n              List<ExprNodeDesc> newChildren = new ArrayList<>();\n              if (Boolean.TRUE.equals(flag.getValue())) {\n                newChildren.add(new ExprNodeConstantDesc(Boolean.FALSE));\n              } else {\n                newChildren.add(new ExprNodeConstantDesc(Boolean.TRUE));\n              }\n              newChildren\n                  .addAll(childExpr.getChildren().subList(1, childExpr.getChildren().size()));\n              expr.setTypeInfo(childExpr.getTypeInfo());\n              expr.setGenericUDF(childExpr.getGenericUDF());\n              expr.setChildren(newChildren);\n            }\n          }\n        }\n      }\n      // Add cast expression if needed. Child expressions of a udf may return different data types\n      // and that would require converting their data types to evaluate the udf.\n      // For example decimal column added to an integer column would require integer column to be\n      // cast to decimal.\n      // Note: this is a no-op for custom UDFs\n      List<ExprNodeDesc> childExpressions = getChildExpressionsWithImplicitCast(expr.getGenericUDF(),\n          exprDesc.getChildren(), exprDesc.getTypeInfo());\n\n      // Are we forcing the usage of VectorUDFAdaptor for test purposes?\n      if (!testVectorAdaptorOverride) {\n        ve = getGenericUdfVectorExpression(expr.getGenericUDF(),\n            childExpressions, mode, exprDesc.getTypeInfo());\n      }\n      if (ve == null) {\n        // Ok, no vectorized class available.  No problem -- try to use the VectorUDFAdaptor\n        // when configured.\n        //\n        // NOTE: We assume if hiveVectorAdaptorUsageMode has not been set it because we are\n        // executing a test that didn't create a HiveConf, etc.  No usage of VectorUDFAdaptor in\n        // that case.\n        if (hiveVectorAdaptorUsageMode != null) {\n          switch (hiveVectorAdaptorUsageMode) {\n          case NONE:\n            // No VectorUDFAdaptor usage.\n            throw new HiveException(\n                \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                  + \" because hive.vectorized.adaptor.usage.mode=none\");\n          case CHOSEN:\n            if (isNonVectorizedPathUDF(expr, mode)) {\n              ve = getCustomUDFExpression(expr, mode);\n            } else {\n              throw new HiveException(\n                  \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                    + \" because hive.vectorized.adaptor.usage.mode=chosen\"\n                    + \" and the UDF wasn't one of the chosen ones\");\n            }\n            break;\n          case ALL:\n            // Check if this is UDF for _bucket_number\n            if (expr.getGenericUDF() instanceof GenericUDFBucketNumber) {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"UDF to handle _bucket_number : Create BucketNumExpression\");\n              }\n              int outCol = ocm.allocateOutputColumn(exprDesc.getTypeInfo());\n              ve = new BucketNumExpression(outCol);\n              ve.setInputTypeInfos(exprDesc.getTypeInfo());\n              ve.setOutputTypeInfo(exprDesc.getTypeInfo());\n            } else {\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(\"We will try to use the VectorUDFAdaptor for \" + exprDesc.toString()\n                  + \" because hive.vectorized.adaptor.usage.mode=all\");\n              }\n              ve = getCustomUDFExpression(expr, mode);\n            }\n            break;\n          default:\n            throw new RuntimeException(\"Unknown hive vector adaptor usage mode \" +\n              hiveVectorAdaptorUsageMode.name());\n          }\n          if (ve == null) {\n            throw new HiveException(\n                \"Unable vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                  + \" even for the VectorUDFAdaptor\");\n          }\n        }\n      }\n    } else if (exprDesc instanceof ExprNodeConstantDesc) {\n      ve = getConstantVectorExpression(((ExprNodeConstantDesc) exprDesc).getValue(), exprDesc.getTypeInfo(),\n        mode);\n    } else if (exprDesc instanceof ExprNodeDynamicValueDesc) {\n      ve = getDynamicValueVectorExpression((ExprNodeDynamicValueDesc) exprDesc, mode);\n    } else if (exprDesc instanceof ExprNodeFieldDesc) {\n      // Get the GenericUDFStructField to process the field of Struct type\n      ve = getGenericUDFStructField((ExprNodeFieldDesc)exprDesc,\n          mode, exprDesc.getTypeInfo());\n    }\n    if (ve == null) {\n      throw new HiveException(\n          \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Input Expression = \" + exprDesc.toString()\n          + \", Vectorized Expression = \" + ve.toString());\n    }\n\n    return ve;\n  }",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843 +\n 844 +\n 845 +\n 846  \n 847 +\n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  ",
            "  /**\n   * Returns a vector expression for a given expression\n   * description.\n   * @param exprDesc, Expression description\n   * @param mode\n   * @return {@link VectorExpression}\n   * @throws HiveException\n   */\n  public VectorExpression getVectorExpression(ExprNodeDesc exprDesc, VectorExpressionDescriptor.Mode mode) throws HiveException {\n    VectorExpression ve = null;\n    if (exprDesc instanceof ExprNodeColumnDesc) {\n      ve = getColumnVectorExpression((ExprNodeColumnDesc) exprDesc, mode);\n    } else if (exprDesc instanceof ExprNodeGenericFuncDesc) {\n      ExprNodeGenericFuncDesc expr = (ExprNodeGenericFuncDesc) exprDesc;\n      // push not through between...\n      if (\"not\".equals(expr.getFuncText())) {\n        if (expr.getChildren() != null && expr.getChildren().size() == 1) {\n          ExprNodeDesc child = expr.getChildren().get(0);\n          if (child instanceof ExprNodeGenericFuncDesc) {\n            ExprNodeGenericFuncDesc childExpr = (ExprNodeGenericFuncDesc) child;\n            if (\"between\".equals(childExpr.getFuncText())) {\n              ExprNodeConstantDesc flag = (ExprNodeConstantDesc) childExpr.getChildren().get(0);\n              List<ExprNodeDesc> newChildren = new ArrayList<>();\n              if (Boolean.TRUE.equals(flag.getValue())) {\n                newChildren.add(new ExprNodeConstantDesc(Boolean.FALSE));\n              } else {\n                newChildren.add(new ExprNodeConstantDesc(Boolean.TRUE));\n              }\n              newChildren\n                  .addAll(childExpr.getChildren().subList(1, childExpr.getChildren().size()));\n              expr.setTypeInfo(childExpr.getTypeInfo());\n              expr.setGenericUDF(childExpr.getGenericUDF());\n              expr.setChildren(newChildren);\n            }\n          }\n        }\n      }\n      // Add cast expression if needed. Child expressions of a udf may return different data types\n      // and that would require converting their data types to evaluate the udf.\n      // For example decimal column added to an integer column would require integer column to be\n      // cast to decimal.\n      // Note: this is a no-op for custom UDFs\n      List<ExprNodeDesc> childExpressions = getChildExpressionsWithImplicitCast(expr.getGenericUDF(),\n          exprDesc.getChildren(), exprDesc.getTypeInfo());\n\n      // Are we forcing the usage of VectorUDFAdaptor for test purposes?\n      if (!testVectorAdaptorOverride) {\n        ve = getGenericUdfVectorExpression(expr.getGenericUDF(),\n            childExpressions, mode, exprDesc.getTypeInfo());\n      }\n      if (ve == null) {\n        // Ok, no vectorized class available.  No problem -- try to use the VectorUDFAdaptor\n        // when configured.\n        //\n        // NOTE: We assume if hiveVectorAdaptorUsageMode has not been set it because we are\n        // executing a test that didn't create a HiveConf, etc.  No usage of VectorUDFAdaptor in\n        // that case.\n        if (hiveVectorAdaptorUsageMode != null) {\n          switch (hiveVectorAdaptorUsageMode) {\n          case NONE:\n            // No VectorUDFAdaptor usage.\n            throw new HiveException(\n                \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                  + \" because hive.vectorized.adaptor.usage.mode=none\");\n          case CHOSEN:\n            if (isNonVectorizedPathUDF(expr, mode)) {\n              ve = getCustomUDFExpression(expr, mode);\n            } else {\n              throw new HiveException(\n                  \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                    + \" because hive.vectorized.adaptor.usage.mode=chosen\"\n                    + \" and the UDF wasn't one of the chosen ones\");\n            }\n            break;\n          case ALL:\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"We will try to use the VectorUDFAdaptor for \" + exprDesc.toString()\n                + \" because hive.vectorized.adaptor.usage.mode=all\");\n            }\n            ve = getCustomUDFExpression(expr, mode);\n            break;\n          default:\n            throw new RuntimeException(\"Unknown hive vector adaptor usage mode \" +\n              hiveVectorAdaptorUsageMode.name());\n          }\n          if (ve == null) {\n            throw new HiveException(\n                \"Unable vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString()\n                  + \" even for the VectorUDFAdaptor\");\n          }\n        }\n      }\n    } else if (exprDesc instanceof ExprNodeConstantDesc) {\n      ve = getConstantVectorExpression(((ExprNodeConstantDesc) exprDesc).getValue(), exprDesc.getTypeInfo(),\n        mode);\n    } else if (exprDesc instanceof ExprNodeDynamicValueDesc) {\n      ve = getDynamicValueVectorExpression((ExprNodeDynamicValueDesc) exprDesc, mode);\n    } else if (exprDesc instanceof ExprNodeFieldDesc) {\n      // Get the GenericUDFStructField to process the field of Struct type\n      ve = getGenericUDFStructField((ExprNodeFieldDesc)exprDesc,\n          mode, exprDesc.getTypeInfo());\n    }\n    if (ve == null) {\n      throw new HiveException(\n          \"Could not vectorize expression (mode = \" + mode.name() + \"): \" + exprDesc.toString());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Input Expression = \" + exprDesc.toString()\n          + \", Vectorized Expression = \" + ve.toString());\n    }\n\n    return ve;\n  }"
        ],
        [
            "VectorReduceSinkObjectHashOperator::initializeOp(Configuration)",
            " 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207 -\n 208 -\n 209  \n 210  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    VectorExpression.doTransientInit(reduceSinkBucketExpressions);\n    VectorExpression.doTransientInit(reduceSinkPartitionExpressions);\n\n    if (!isEmptyKey) {\n\n      // For this variation, we serialize the key without caring if it single Long,\n      // single String, multi-key, etc.\n      keyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(keyOutput);\n      keyVectorSerializeRow =\n          new VectorSerializeRow<BinarySortableSerializeWrite>(\n              keyBinarySortableSerializeWrite);\n      keyVectorSerializeRow.init(reduceSinkKeyTypeInfos, reduceSinkKeyColumnMap);\n    }\n\n    // Object Hash.\n\n    if (isEmptyBuckets) {\n      numBuckets = 0;\n    } else {\n      numBuckets = conf.getNumBuckets();\n\n      bucketObjectInspectors = getObjectInspectorArray(reduceSinkBucketTypeInfos);\n      bucketVectorExtractRow = new VectorExtractRow();\n      bucketVectorExtractRow.init(reduceSinkBucketTypeInfos, reduceSinkBucketColumnMap);\n      bucketFieldValues = new Object[reduceSinkBucketTypeInfos.length];\n    }\n\n    if (isEmptyPartitions) {\n      nonPartitionRandom = new Random(12345);\n    } else {\n      partitionObjectInspectors = getObjectInspectorArray(reduceSinkPartitionTypeInfos);\n      partitionVectorExtractRow = new VectorExtractRow();\n      partitionVectorExtractRow.init(reduceSinkPartitionTypeInfos, reduceSinkPartitionColumnMap);\n      partitionFieldValues = new Object[reduceSinkPartitionTypeInfos.length];\n    }\n\n    // Set hashFunc\n    hashFunc = bucketingVersion == 2 && !vectorDesc.getIsAcidChange() ?\n      ObjectInspectorUtils::getBucketHashCode :\n      ObjectInspectorUtils::getBucketHashCodeOld;\n\n    // Set function to evaluate _bucket_number if needed.\n    try {\n      buckectEvaluatorMethod = this.getClass().getDeclaredMethod(\"evaluateBucketDummy\",\n        VectorizedRowBatch.class, int.class, int.class);\n      if (reduceSinkKeyExpressions != null) {\n        for (VectorExpression ve : reduceSinkKeyExpressions) {\n          if (ve instanceof BucketNumExpression) {\n            bucketExpr = (BucketNumExpression) ve;\n            buckectEvaluatorMethod = this.getClass().getDeclaredMethod(\"evaluateBucketExpr\",\n              VectorizedRowBatch.class, int.class, int.class);\n            break;\n          }\n        }\n      }\n    } catch (NoSuchMethodException e) {\n      throw new HiveException(\"Failed to find method to evaluate _bucket_number\");\n    }\n  }",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193  \n 194  \n 195  \n 196  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n    VectorExpression.doTransientInit(reduceSinkBucketExpressions);\n    VectorExpression.doTransientInit(reduceSinkPartitionExpressions);\n\n    if (!isEmptyKey) {\n\n      // For this variation, we serialize the key without caring if it single Long,\n      // single String, multi-key, etc.\n      keyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(keyOutput);\n      keyVectorSerializeRow =\n          new VectorSerializeRow<BinarySortableSerializeWrite>(\n              keyBinarySortableSerializeWrite);\n      keyVectorSerializeRow.init(reduceSinkKeyTypeInfos, reduceSinkKeyColumnMap);\n    }\n\n    // Object Hash.\n\n    if (isEmptyBuckets) {\n      numBuckets = 0;\n    } else {\n      numBuckets = conf.getNumBuckets();\n\n      bucketObjectInspectors = getObjectInspectorArray(reduceSinkBucketTypeInfos);\n      bucketVectorExtractRow = new VectorExtractRow();\n      bucketVectorExtractRow.init(reduceSinkBucketTypeInfos, reduceSinkBucketColumnMap);\n      bucketFieldValues = new Object[reduceSinkBucketTypeInfos.length];\n    }\n\n    if (isEmptyPartitions) {\n      nonPartitionRandom = new Random(12345);\n    } else {\n      partitionObjectInspectors = getObjectInspectorArray(reduceSinkPartitionTypeInfos);\n      partitionVectorExtractRow = new VectorExtractRow();\n      partitionVectorExtractRow.init(reduceSinkPartitionTypeInfos, reduceSinkPartitionColumnMap);\n      partitionFieldValues = new Object[reduceSinkPartitionTypeInfos.length];\n    }\n\n    // Set hashFunc\n    hashFunc = bucketingVersion == 2 && !vectorDesc.getIsAcidChange() ?\n      ObjectInspectorUtils::getBucketHashCode :\n      ObjectInspectorUtils::getBucketHashCodeOld;\n\n    // Set function to evaluate _bucket_number if needed.\n    if (reduceSinkKeyExpressions != null) {\n      for (VectorExpression ve : reduceSinkKeyExpressions) {\n        if (ve instanceof BucketNumExpression) {\n          bucketExpr = (BucketNumExpression) ve;\n          break;\n        }\n      }\n    }\n  }"
        ],
        [
            "VectorizationContext::getGenericUdfVectorExpression(GenericUDF,List,VectorExpressionDescriptor,TypeInfo)",
            "2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  ",
            "  private VectorExpression getGenericUdfVectorExpression(GenericUDF udf,\n      List<ExprNodeDesc> childExpr, VectorExpressionDescriptor.Mode mode, TypeInfo returnType)\n          throws HiveException {\n\n    List<ExprNodeDesc> castedChildren = evaluateCastOnConstants(childExpr);\n    childExpr = castedChildren;\n\n    //First handle special cases.  If one of the special case methods cannot handle it,\n    // it returns null.\n    VectorExpression ve = null;\n    if (udf instanceof GenericUDFBetween) {\n      ve = getBetweenExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFIn) {\n      ve = getInExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFIf) {\n      ve = getIfExpression((GenericUDFIf) udf, childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFWhen) {\n      ve = getWhenExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFOPPositive) {\n      ve = getIdentityExpression(childExpr);\n    } else if (udf instanceof GenericUDFCoalesce || udf instanceof GenericUDFNvl) {\n\n      // Coalesce is a special case because it can take variable number of arguments.\n      // Nvl is a specialization of the Coalesce.\n      ve = getCoalesceExpression(childExpr, returnType);\n    } else if (udf instanceof GenericUDFElt) {\n\n      // Elt is a special case because it can take variable number of arguments.\n      ve = getEltExpression(childExpr, returnType);\n    } else if (udf instanceof GenericUDFGrouping) {\n      ve = getGroupingExpression((GenericUDFGrouping) udf, childExpr, returnType);\n    } else if (udf instanceof GenericUDFBridge) {\n      ve = getGenericUDFBridgeVectorExpression((GenericUDFBridge) udf, childExpr, mode,\n          returnType);\n    } else if (udf instanceof GenericUDFToString) {\n      ve = getCastToString(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToDecimal) {\n      ve = getCastToDecimal(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToChar) {\n      ve = getCastToChar(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToVarchar) {\n      ve = getCastToVarChar(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToBinary) {\n      ve = getCastToBinary(childExpr, returnType);\n    } else if (udf instanceof GenericUDFTimestamp) {\n      ve = getCastToTimestamp((GenericUDFTimestamp)udf, childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFDate || udf instanceof GenericUDFToDate) {\n      ve = getIdentityForDateToDate(childExpr, returnType);\n    }\n    if (ve != null) {\n      return ve;\n    }\n    // Now do a general lookup\n    Class<?> udfClass = udf.getClass();\n    boolean isSubstituted = false;\n    if (udf instanceof GenericUDFBridge) {\n      udfClass = ((GenericUDFBridge) udf).getUdfClass();\n      isSubstituted = true;\n    }\n\n    ve = getVectorExpressionForUdf((!isSubstituted ? udf : null),\n        udfClass, castedChildren, mode, returnType);\n\n    return ve;\n  }",
            "2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116 +\n2117 +\n2118 +\n2119 +\n2120 +\n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  ",
            "  private VectorExpression getGenericUdfVectorExpression(GenericUDF udf,\n      List<ExprNodeDesc> childExpr, VectorExpressionDescriptor.Mode mode, TypeInfo returnType)\n          throws HiveException {\n\n    List<ExprNodeDesc> castedChildren = evaluateCastOnConstants(childExpr);\n    childExpr = castedChildren;\n\n    //First handle special cases.  If one of the special case methods cannot handle it,\n    // it returns null.\n    VectorExpression ve = null;\n    if (udf instanceof GenericUDFBetween) {\n      ve = getBetweenExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFIn) {\n      ve = getInExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFIf) {\n      ve = getIfExpression((GenericUDFIf) udf, childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFWhen) {\n      ve = getWhenExpression(childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFOPPositive) {\n      ve = getIdentityExpression(childExpr);\n    } else if (udf instanceof GenericUDFCoalesce || udf instanceof GenericUDFNvl) {\n\n      // Coalesce is a special case because it can take variable number of arguments.\n      // Nvl is a specialization of the Coalesce.\n      ve = getCoalesceExpression(childExpr, returnType);\n    } else if (udf instanceof GenericUDFElt) {\n\n      // Elt is a special case because it can take variable number of arguments.\n      ve = getEltExpression(childExpr, returnType);\n    } else if (udf instanceof GenericUDFGrouping) {\n      ve = getGroupingExpression((GenericUDFGrouping) udf, childExpr, returnType);\n    } else if (udf instanceof GenericUDFBridge) {\n      ve = getGenericUDFBridgeVectorExpression((GenericUDFBridge) udf, childExpr, mode,\n          returnType);\n    } else if (udf instanceof GenericUDFToString) {\n      ve = getCastToString(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToDecimal) {\n      ve = getCastToDecimal(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToChar) {\n      ve = getCastToChar(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToVarchar) {\n      ve = getCastToVarChar(childExpr, returnType);\n    } else if (udf instanceof GenericUDFToBinary) {\n      ve = getCastToBinary(childExpr, returnType);\n    } else if (udf instanceof GenericUDFTimestamp) {\n      ve = getCastToTimestamp((GenericUDFTimestamp)udf, childExpr, mode, returnType);\n    } else if (udf instanceof GenericUDFDate || udf instanceof GenericUDFToDate) {\n      ve = getIdentityForDateToDate(childExpr, returnType);\n    } else if (udf instanceof GenericUDFBucketNumber) {\n      int outCol = ocm.allocateOutputColumn(returnType);\n      ve = new BucketNumExpression(outCol);\n      ve.setInputTypeInfos(returnType);\n      ve.setOutputTypeInfo(returnType);\n    }\n    if (ve != null) {\n      return ve;\n    }\n    // Now do a general lookup\n    Class<?> udfClass = udf.getClass();\n    boolean isSubstituted = false;\n    if (udf instanceof GenericUDFBridge) {\n      udfClass = ((GenericUDFBridge) udf).getUdfClass();\n      isSubstituted = true;\n    }\n\n    ve = getVectorExpressionForUdf((!isSubstituted ? udf : null),\n        udfClass, castedChildren, mode, returnType);\n\n    return ve;\n  }"
        ],
        [
            "BucketNumExpression::setRowNum(int)",
            "  46 -\n  47  \n  48  ",
            "  public void setRowNum(final int rowNum) {\n    this.rowNum = rowNum;\n  }",
            "  46 +\n  47  \n  48 +\n  49 +\n  50 +\n  51 +\n  52  ",
            "  public void setRowNum(final int rowNum) throws HiveException{\n    this.rowNum = rowNum;\n    if (rowSet) {\n      throw new HiveException(\"Row number is already set\");\n    }\n    rowSet = true;\n  }"
        ],
        [
            "VectorReduceSinkObjectHashOperator::process(Object,int)",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 -\n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n\n    try {\n\n      VectorizedRowBatch batch = (VectorizedRowBatch) row;\n\n      batchCounter++;\n\n      if (batch.size == 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" empty\");\n        }\n        return;\n      }\n\n      if (!isKeyInitialized) {\n        isKeyInitialized = true;\n        if (isEmptyKey) {\n          initializeEmptyKey(tag);\n        }\n      }\n\n      // Perform any key expressions.  Results will go into scratch columns.\n      if (reduceSinkKeyExpressions != null) {\n        for (VectorExpression ve : reduceSinkKeyExpressions) {\n          // Handle _bucket_number\n          if (ve instanceof BucketNumExpression) {\n            continue; // Evaluate per row\n          }\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any value expressions.  Results will go into scratch columns.\n      if (reduceSinkValueExpressions != null) {\n        for (VectorExpression ve : reduceSinkValueExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any bucket expressions.  Results will go into scratch columns.\n      if (reduceSinkBucketExpressions != null) {\n        for (VectorExpression ve : reduceSinkBucketExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any partition expressions.  Results will go into scratch columns.\n      if (reduceSinkPartitionExpressions != null) {\n        for (VectorExpression ve : reduceSinkPartitionExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n\n      final boolean selectedInUse = batch.selectedInUse;\n      int[] selected = batch.selected;\n\n      final int size = batch.size;\n\n      if (isEmptyBuckets) { // EmptyBuckets = true\n        if (isEmptyPartitions) { // isEmptyPartition = true\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            final int hashCode = nonPartitionRandom.nextInt();\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        } else { // isEmptyPartition = false\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            partitionVectorExtractRow.extractRow(batch, batchIndex, partitionFieldValues);\n            final int hashCode = hashFunc.apply(partitionFieldValues, partitionObjectInspectors);\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        }\n      } else { // EmptyBuckets = false\n        if (isEmptyPartitions) { // isEmptyPartition = true\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            bucketVectorExtractRow.extractRow(batch, batchIndex, bucketFieldValues);\n            final int bucketNum = ObjectInspectorUtils.getBucketNumber(\n              hashFunc.apply(bucketFieldValues, bucketObjectInspectors), numBuckets);\n            final int hashCode = nonPartitionRandom.nextInt() * 31 + bucketNum;\n            buckectEvaluatorMethod.invoke(this, batch, batchIndex, bucketNum);\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        } else { // isEmptyPartition = false\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            partitionVectorExtractRow.extractRow(batch, batchIndex, partitionFieldValues);\n            bucketVectorExtractRow.extractRow(batch, batchIndex, bucketFieldValues);\n            final int bucketNum = ObjectInspectorUtils.getBucketNumber(\n              hashFunc.apply(bucketFieldValues, bucketObjectInspectors), numBuckets);\n            final int hashCode = hashFunc.apply(partitionFieldValues, partitionObjectInspectors) * 31 + bucketNum;\n            buckectEvaluatorMethod.invoke(this, batch, batchIndex, bucketNum);\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        }\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281 +\n 282 +\n 283 +\n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295 +\n 296 +\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  ",
            "  @Override\n  public void process(Object row, int tag) throws HiveException {\n\n    try {\n\n      VectorizedRowBatch batch = (VectorizedRowBatch) row;\n\n      batchCounter++;\n\n      if (batch.size == 0) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" empty\");\n        }\n        return;\n      }\n\n      if (!isKeyInitialized) {\n        isKeyInitialized = true;\n        if (isEmptyKey) {\n          initializeEmptyKey(tag);\n        }\n      }\n\n      // Perform any key expressions.  Results will go into scratch columns.\n      if (reduceSinkKeyExpressions != null) {\n        for (VectorExpression ve : reduceSinkKeyExpressions) {\n          // Handle _bucket_number\n          if (ve instanceof BucketNumExpression) {\n            continue; // Evaluate per row\n          }\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any value expressions.  Results will go into scratch columns.\n      if (reduceSinkValueExpressions != null) {\n        for (VectorExpression ve : reduceSinkValueExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any bucket expressions.  Results will go into scratch columns.\n      if (reduceSinkBucketExpressions != null) {\n        for (VectorExpression ve : reduceSinkBucketExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n  \n      // Perform any partition expressions.  Results will go into scratch columns.\n      if (reduceSinkPartitionExpressions != null) {\n        for (VectorExpression ve : reduceSinkPartitionExpressions) {\n          ve.evaluate(batch);\n        }\n      }\n\n      final boolean selectedInUse = batch.selectedInUse;\n      int[] selected = batch.selected;\n\n      final int size = batch.size;\n\n      if (isEmptyBuckets) { // EmptyBuckets = true\n        if (isEmptyPartitions) { // isEmptyPartition = true\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            final int hashCode = nonPartitionRandom.nextInt();\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        } else { // isEmptyPartition = false\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            partitionVectorExtractRow.extractRow(batch, batchIndex, partitionFieldValues);\n            final int hashCode = hashFunc.apply(partitionFieldValues, partitionObjectInspectors);\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        }\n      } else { // EmptyBuckets = false\n        if (isEmptyPartitions) { // isEmptyPartition = true\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            bucketVectorExtractRow.extractRow(batch, batchIndex, bucketFieldValues);\n            final int bucketNum = ObjectInspectorUtils.getBucketNumber(\n              hashFunc.apply(bucketFieldValues, bucketObjectInspectors), numBuckets);\n            final int hashCode = nonPartitionRandom.nextInt() * 31 + bucketNum;\n            if (bucketExpr != null) {\n              evaluateBucketExpr(batch, batchIndex, bucketNum);\n            }\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        } else { // isEmptyPartition = false\n          for (int logical = 0; logical< size; logical++) {\n            final int batchIndex = (selectedInUse ? selected[logical] : logical);\n            partitionVectorExtractRow.extractRow(batch, batchIndex, partitionFieldValues);\n            bucketVectorExtractRow.extractRow(batch, batchIndex, bucketFieldValues);\n            final int bucketNum = ObjectInspectorUtils.getBucketNumber(\n              hashFunc.apply(bucketFieldValues, bucketObjectInspectors), numBuckets);\n            final int hashCode = hashFunc.apply(partitionFieldValues, partitionObjectInspectors) * 31 + bucketNum;\n            if (bucketExpr != null) {\n              evaluateBucketExpr(batch, batchIndex, bucketNum);\n            }\n            postProcess(batch, batchIndex, tag, hashCode);\n          }\n        }\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "BucketNumExpression::setBucketNum(int)",
            "  50 -\n  51  \n  52  ",
            "  public void setBucketNum(final int bucketNum) {\n    this.bucketNum = bucketNum;\n  }",
            "  54 +\n  55  \n  56 +\n  57 +\n  58 +\n  59 +\n  60  ",
            "  public void setBucketNum(final int bucketNum) throws HiveException{\n    this.bucketNum = bucketNum;\n    if (bucketNumSet) {\n      throw new HiveException(\"Bucket number is already set\");\n    }\n    bucketNumSet = true;\n  }"
        ],
        [
            "BucketNumExpression::evaluate(VectorizedRowBatch)",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "  @Override\n  public void evaluate(VectorizedRowBatch batch) throws HiveException {\n    BytesColumnVector cv = (BytesColumnVector) batch.cols[outputColumnNum];\n    String bucketNumStr = String.valueOf(bucketNum);\n    cv.setVal(rowNum, bucketNumStr.getBytes(), 0, bucketNumStr.length());\n  }",
            "  62  \n  63  \n  64 +\n  65 +\n  66 +\n  67  \n  68  \n  69  \n  70 +\n  71 +\n  72  ",
            "  @Override\n  public void evaluate(VectorizedRowBatch batch) throws HiveException {\n    if (!rowSet || !bucketNumSet) {\n      throw new HiveException(\"row number or bucket number is not set before evaluation\");\n    }\n    BytesColumnVector cv = (BytesColumnVector) batch.cols[outputColumnNum];\n    String bucketNumStr = String.valueOf(bucketNum);\n    cv.setVal(rowNum, bucketNumStr.getBytes(), 0, bucketNumStr.length());\n    rowSet = false;\n    bucketNumSet = false;\n  }"
        ]
    ],
    "3cbc13e92b9c22fabf9eac72eaec9352eb9b43d2": [
        [
            "SyntheticJoinPredicate::JoinSynthetic::createDerivatives(List,Operator,ExprNodeDesc,ExprNodeDesc)",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 -\n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  ",
            "    private boolean createDerivatives(final List<ExprNodeDesc> resultExprs, final Operator<?> op,\n        final ExprNodeDesc currentNode, final ExprNodeDesc sourceKey) throws SemanticException {\n      // 1. Obtain join operator upstream\n      Operator<?> currentOp = op;\n      while (!(currentOp instanceof CommonJoinOperator)) {\n        if (currentOp.getParentOperators() == null || currentOp.getParentOperators().size() != 1) {\n          // Cannot backtrack\n          currentOp = null;\n          break;\n        }\n        if (!(currentOp instanceof FilterOperator) &&\n            !(currentOp instanceof SelectOperator) &&\n            !(currentOp instanceof ReduceSinkOperator) &&\n            !(currentOp instanceof GroupByOperator)) {\n          // Operator not supported\n          currentOp = null;\n          break;\n        }\n        // Move the pointer\n        currentOp = currentOp.getParentOperators().get(0);\n      }\n      if (currentOp == null) {\n        // We did not find any join, we are done\n        return true;\n      }\n      CommonJoinOperator<JoinDesc> joinOp = (CommonJoinOperator) currentOp;\n\n      // 2. Backtrack expression to join output\n      final ExprNodeDesc joinExprNode = ExprNodeDescUtils.backtrack(currentNode, op, joinOp);\n      if (joinExprNode == null || !(joinExprNode instanceof ExprNodeColumnDesc)) {\n        // TODO: We can extend to other expression types\n        // We are done\n        return true;\n      }\n      final String columnRefJoinInput = ((ExprNodeColumnDesc)joinExprNode).getColumn();\n\n      // 3. Find input position in join for expression obtained\n      String columnOutputName = null;\n      for (Map.Entry<String, ExprNodeDesc> e : joinOp.getColumnExprMap().entrySet()) {\n        if (e.getValue() == joinExprNode) {\n          columnOutputName = e.getKey();\n          break;\n        }\n      }\n      if (columnOutputName == null) {\n        // Maybe the join is pruning columns, though it should not.\n        // In any case, we are done\n        return true;\n      }\n      final int srcPos = joinOp.getConf().getReversedExprs().get(columnOutputName);\n      final int[][] targets = getTargets(joinOp);\n      final ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(srcPos);\n\n      // 4. Find expression in input RS operator.\n      final Operator<?> rsOpInput = rsOp.getParentOperators().get(0);\n      final ExprNodeDesc rsOpInputExprNode = rsOp.getColumnExprMap().get(columnRefJoinInput);\n      if (rsOpInputExprNode == null) {\n        // Unexpected, we just bail out and we do not infer additional predicates\n        return false;\n      }\n      int posInRSOpKeys = -1;\n      for (int i = 0; i < rsOp.getConf().getKeyCols().size(); i++) {\n        if (rsOpInputExprNode.isSame(rsOp.getConf().getKeyCols().get(i))) {\n          posInRSOpKeys = i;\n          break;\n        }\n      }\n\n      // 5. If it is part of the key, we can create a new semijoin.\n      // In addition, we can do the same for siblings\n      if (posInRSOpKeys >= 0) {\n        // We pass the tests, we add it to the args for the AND expression\n        addParentReduceSink(resultExprs, rsOp, posInRSOpKeys, sourceKey);\n        for (int targetPos: targets[srcPos]) {\n          if (srcPos == targetPos) {\n            continue;\n          }\n          final ReduceSinkOperator otherRsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(targetPos);\n          final Operator<?> otherRsOpInput = otherRsOp.getParentOperators().get(0);\n          // We pass the tests, we add it to the args for the AND expression\n          addParentReduceSink(resultExprs, otherRsOp, posInRSOpKeys, sourceKey);\n          // We propagate to operator below\n          boolean success = createDerivatives(\n              resultExprs, otherRsOpInput, otherRsOp.getConf().getKeyCols().get(posInRSOpKeys), sourceKey);\n          if (!success) {\n            // Something went wrong, bail out\n            return false;\n          }\n        }\n      }\n\n      // 6. Whether it was part of the key or of the value, if we reach here, we can at least\n      // continue propagating to operators below\n      boolean success = createDerivatives(\n          resultExprs, rsOpInput, rsOpInputExprNode, sourceKey);\n      if (!success) {\n        // Something went wrong, bail out\n        return false;\n      }\n\n      // 7. We are done, success\n      return true;\n    }",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 +\n 312 +\n 313 +\n 314 +\n 315 +\n 316 +\n 317 +\n 318 +\n 319 +\n 320 +\n 321 +\n 322 +\n 323 +\n 324 +\n 325 +\n 326 +\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  ",
            "    private boolean createDerivatives(final List<ExprNodeDesc> resultExprs, final Operator<?> op,\n        final ExprNodeDesc currentNode, final ExprNodeDesc sourceKey) throws SemanticException {\n      // 1. Obtain join operator upstream\n      Operator<?> currentOp = op;\n      while (!(currentOp instanceof CommonJoinOperator)) {\n        if (currentOp.getParentOperators() == null || currentOp.getParentOperators().size() != 1) {\n          // Cannot backtrack\n          currentOp = null;\n          break;\n        }\n        if (!(currentOp instanceof FilterOperator) &&\n            !(currentOp instanceof SelectOperator) &&\n            !(currentOp instanceof ReduceSinkOperator) &&\n            !(currentOp instanceof GroupByOperator)) {\n          // Operator not supported\n          currentOp = null;\n          break;\n        }\n        // Move the pointer\n        currentOp = currentOp.getParentOperators().get(0);\n      }\n      if (currentOp == null) {\n        // We did not find any join, we are done\n        return true;\n      }\n      CommonJoinOperator<JoinDesc> joinOp = (CommonJoinOperator) currentOp;\n\n      // 2. Backtrack expression to join output\n      ExprNodeDesc expr = currentNode;\n      if (currentOp != op) {\n        if (expr instanceof ExprNodeColumnDesc) {\n          // Expression refers to output of current operator, but backtrack methods works\n          // from the input columns, hence we need to make resolution for current operator\n          // here. If the operator was already the join, there is nothing to do\n          if (op.getColumnExprMap() != null) {\n            expr = op.getColumnExprMap().get(((ExprNodeColumnDesc) expr).getColumn());\n          }\n        } else {\n          // TODO: We can extend to other expression types\n          // We are done\n          return true;\n        }\n      }\n      final ExprNodeDesc joinExprNode = ExprNodeDescUtils.backtrack(expr, op, joinOp);\n      if (joinExprNode == null || !(joinExprNode instanceof ExprNodeColumnDesc)) {\n        // TODO: We can extend to other expression types\n        // We are done\n        return true;\n      }\n      final String columnRefJoinInput = ((ExprNodeColumnDesc)joinExprNode).getColumn();\n\n      // 3. Find input position in join for expression obtained\n      String columnOutputName = null;\n      for (Map.Entry<String, ExprNodeDesc> e : joinOp.getColumnExprMap().entrySet()) {\n        if (e.getValue() == joinExprNode) {\n          columnOutputName = e.getKey();\n          break;\n        }\n      }\n      if (columnOutputName == null) {\n        // Maybe the join is pruning columns, though it should not.\n        // In any case, we are done\n        return true;\n      }\n      final int srcPos = joinOp.getConf().getReversedExprs().get(columnOutputName);\n      final int[][] targets = getTargets(joinOp);\n      final ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(srcPos);\n\n      // 4. Find expression in input RS operator.\n      final Operator<?> rsOpInput = rsOp.getParentOperators().get(0);\n      final ExprNodeDesc rsOpInputExprNode = rsOp.getColumnExprMap().get(columnRefJoinInput);\n      if (rsOpInputExprNode == null) {\n        // Unexpected, we just bail out and we do not infer additional predicates\n        return false;\n      }\n      int posInRSOpKeys = -1;\n      for (int i = 0; i < rsOp.getConf().getKeyCols().size(); i++) {\n        if (rsOpInputExprNode.isSame(rsOp.getConf().getKeyCols().get(i))) {\n          posInRSOpKeys = i;\n          break;\n        }\n      }\n\n      // 5. If it is part of the key, we can create a new semijoin.\n      // In addition, we can do the same for siblings\n      if (posInRSOpKeys >= 0) {\n        // We pass the tests, we add it to the args for the AND expression\n        addParentReduceSink(resultExprs, rsOp, posInRSOpKeys, sourceKey);\n        for (int targetPos: targets[srcPos]) {\n          if (srcPos == targetPos) {\n            continue;\n          }\n          final ReduceSinkOperator otherRsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(targetPos);\n          final Operator<?> otherRsOpInput = otherRsOp.getParentOperators().get(0);\n          // We pass the tests, we add it to the args for the AND expression\n          addParentReduceSink(resultExprs, otherRsOp, posInRSOpKeys, sourceKey);\n          // We propagate to operator below\n          boolean success = createDerivatives(\n              resultExprs, otherRsOpInput, otherRsOp.getConf().getKeyCols().get(posInRSOpKeys), sourceKey);\n          if (!success) {\n            // Something went wrong, bail out\n            return false;\n          }\n        }\n      }\n\n      // 6. Whether it was part of the key or of the value, if we reach here, we can at least\n      // continue propagating to operators below\n      boolean success = createDerivatives(\n          resultExprs, rsOpInput, rsOpInputExprNode, sourceKey);\n      if (!success) {\n        // Something went wrong, bail out\n        return false;\n      }\n\n      // 7. We are done, success\n      return true;\n    }"
        ]
    ],
    "bb777512f9dd3efc0590b960778fd7ca2e3bab21": [
        [
            "JdbcRecordReader::next(LongWritable,MapWritable)",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 -\n  86 -\n  87  \n  88  ",
            "  @Override\n  public boolean next(LongWritable key, MapWritable value) throws IOException {\n    try {\n      LOGGER.trace(\"JdbcRecordReader.next called\");\n      if (dbAccessor == null) {\n        dbAccessor = DatabaseAccessorFactory.getAccessor(conf);\n        iterator = dbAccessor.getRecordIterator(conf, split.getPartitionColumn(), split.getLowerBound(), split\n                        .getUpperBound(), split.getLimit(), split.getOffset());\n      }\n\n      if (iterator.hasNext()) {\n        LOGGER.trace(\"JdbcRecordReader has more records to read.\");\n        key.set(pos);\n        pos++;\n        Map<String, Object> record = iterator.next();\n        if ((record != null) && (!record.isEmpty())) {\n          for (Entry<String, Object> entry : record.entrySet()) {\n            value.put(new Text(entry.getKey()),\n                entry.getValue() == null ? NullWritable.get() : new ObjectWritable(entry.getValue()));\n          }\n          return true;\n        }\n        else {\n          LOGGER.debug(\"JdbcRecordReader got null record.\");\n          return false;\n        }\n      }\n      else {\n        LOGGER.debug(\"JdbcRecordReader has no more records to read.\");\n        return false;\n      }\n    }\n    catch (Exception e) {\n      LOGGER.error(\"An error occurred while reading the next record from DB.\", e);\n      return false;\n    }\n  }",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  ",
            "  @Override\n  public boolean next(LongWritable key, MapWritable value) throws IOException {\n    try {\n      LOGGER.trace(\"JdbcRecordReader.next called\");\n      if (dbAccessor == null) {\n        dbAccessor = DatabaseAccessorFactory.getAccessor(conf);\n        iterator = dbAccessor.getRecordIterator(conf, split.getPartitionColumn(), split.getLowerBound(), split\n                        .getUpperBound(), split.getLimit(), split.getOffset());\n      }\n\n      if (iterator.hasNext()) {\n        LOGGER.trace(\"JdbcRecordReader has more records to read.\");\n        key.set(pos);\n        pos++;\n        Map<String, Object> record = iterator.next();\n        if ((record != null) && (!record.isEmpty())) {\n          for (Entry<String, Object> entry : record.entrySet()) {\n            value.put(new Text(entry.getKey()),\n                entry.getValue() == null ? NullWritable.get() : new ObjectWritable(entry.getValue()));\n          }\n          return true;\n        }\n        else {\n          LOGGER.debug(\"JdbcRecordReader got null record.\");\n          return false;\n        }\n      }\n      else {\n        LOGGER.debug(\"JdbcRecordReader has no more records to read.\");\n        return false;\n      }\n    }\n    catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        ],
        [
            "GenericJdbcDatabaseAccessor::getRecordIterator(Configuration,String,String,String,int,int)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178  \n 179  ",
            "  @Override\n  public JdbcRecordIterator\n    getRecordIterator(Configuration conf, String partitionColumn, String lowerBound, String upperBound, int limit, int\n          offset) throws\n          HiveJdbcDatabaseAccessException {\n\n    Connection conn = null;\n    PreparedStatement ps = null;\n    ResultSet rs = null;\n\n    try {\n      initializeDatabaseConnection(conf);\n      String tableName = conf.get(Constants.JDBC_TABLE);\n      String sql = JdbcStorageConfigManager.getQueryToExecute(conf);\n      String partitionQuery;\n      if (partitionColumn != null) {\n        partitionQuery = addBoundaryToQuery(tableName, sql, partitionColumn, lowerBound, upperBound);\n      } else {\n        partitionQuery = addLimitAndOffsetToQuery(sql, limit, offset);\n      }\n      LOGGER.info(\"Query to execute is [{}]\", partitionQuery);\n\n      conn = dbcpDataSource.getConnection();\n      ps = conn.prepareStatement(partitionQuery, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\n      ps.setFetchSize(getFetchSize(conf));\n      rs = ps.executeQuery();\n\n      return new JdbcRecordIterator(conn, ps, rs, conf);\n    }\n    catch (Exception e) {\n      LOGGER.error(\"Caught exception while trying to execute query\", e);\n      cleanupResources(conn, ps, rs);\n      throw new HiveJdbcDatabaseAccessException(\"Caught exception while trying to execute query\", e);\n    }\n  }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 +\n 178  \n 179  ",
            "  @Override\n  public JdbcRecordIterator\n    getRecordIterator(Configuration conf, String partitionColumn, String lowerBound, String upperBound, int limit, int\n          offset) throws\n          HiveJdbcDatabaseAccessException {\n\n    Connection conn = null;\n    PreparedStatement ps = null;\n    ResultSet rs = null;\n\n    try {\n      initializeDatabaseConnection(conf);\n      String tableName = conf.get(Constants.JDBC_TABLE);\n      String sql = JdbcStorageConfigManager.getQueryToExecute(conf);\n      String partitionQuery;\n      if (partitionColumn != null) {\n        partitionQuery = addBoundaryToQuery(tableName, sql, partitionColumn, lowerBound, upperBound);\n      } else {\n        partitionQuery = addLimitAndOffsetToQuery(sql, limit, offset);\n      }\n      LOGGER.info(\"Query to execute is [{}]\", partitionQuery);\n\n      conn = dbcpDataSource.getConnection();\n      ps = conn.prepareStatement(partitionQuery, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);\n      ps.setFetchSize(getFetchSize(conf));\n      rs = ps.executeQuery();\n\n      return new JdbcRecordIterator(conn, ps, rs, conf);\n    }\n    catch (Exception e) {\n      LOGGER.error(\"Caught exception while trying to execute query\", e);\n      cleanupResources(conn, ps, rs);\n      throw new HiveJdbcDatabaseAccessException(\"Caught exception while trying to execute query:\" + e.getMessage(), e);\n    }\n  }"
        ]
    ],
    "7ae4a2c81a7a3a22ab2df2c5f75638e3f5e580f0": [
        [
            "MetastoreDirectSqlUtils::setPartitionParametersWithFilter(String,boolean,PersistenceManager,String,TreeMap,String,String)",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 -\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  ",
            "  static void setPartitionParametersWithFilter(String PARTITION_PARAMS,\n      boolean convertMapNullsToEmptyStrings, PersistenceManager pm, String partIds,\n      TreeMap<Long, Partition> partitions, String includeParamKeyPattern, String excludeParamKeyPattern)\n      throws MetaException {\n    StringBuilder queryTextBuilder = new StringBuilder(\"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \")\n        .append(PARTITION_PARAMS)\n        .append(\" where \\\"PART_ID\\\" in (\")\n        .append(partIds)\n        .append(\") and \\\"PARAM_KEY\\\" is not null\");\n    List<Object> queryParams = new ArrayList<>(2);;\n    if (includeParamKeyPattern != null && !includeParamKeyPattern.isEmpty()) {\n      queryTextBuilder.append(\" and \\\"PARAM_KEY\\\" LIKE (?)\");\n      queryParams.add(includeParamKeyPattern);\n    }\n    if (excludeParamKeyPattern != null && !excludeParamKeyPattern.isEmpty()) {\n      queryTextBuilder.append(\" and \\\"PARAM_KEY\\\" NOT LIKE (?)\");\n      queryParams.add(excludeParamKeyPattern);\n    }\n\n    queryTextBuilder.append(\" order by \\\"PART_ID\\\" asc\");\n    String queryText = queryTextBuilder.toString();\n    loopJoinOrderedResult(pm, partitions, queryText, queryParams.toArray(), 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String) fields[1], (String) fields[2]);\n      }\n    });\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreServerUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n  }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  ",
            "  static void setPartitionParametersWithFilter(String PARTITION_PARAMS,\n      boolean convertMapNullsToEmptyStrings, PersistenceManager pm, String partIds,\n      TreeMap<Long, Partition> partitions, String includeParamKeyPattern, String excludeParamKeyPattern)\n      throws MetaException {\n    StringBuilder queryTextBuilder = new StringBuilder(\"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \")\n        .append(PARTITION_PARAMS)\n        .append(\" where \\\"PART_ID\\\" in (\")\n        .append(partIds)\n        .append(\") and \\\"PARAM_KEY\\\" is not null\");\n    List<Object> queryParams = new ArrayList<>(2);;\n    if (includeParamKeyPattern != null && !includeParamKeyPattern.isEmpty()) {\n      queryTextBuilder.append(\" and \\\"PARAM_KEY\\\" LIKE (?)\");\n      queryParams.add(includeParamKeyPattern);\n    }\n    if (excludeParamKeyPattern != null && !excludeParamKeyPattern.isEmpty()) {\n      queryTextBuilder.append(\" and \\\"PARAM_KEY\\\" NOT LIKE (?)\");\n      queryParams.add(excludeParamKeyPattern);\n    }\n\n    queryTextBuilder.append(\" order by \\\"PART_ID\\\" asc\");\n    String queryText = queryTextBuilder.toString();\n    loopJoinOrderedResult(pm, partitions, queryText, queryParams.toArray(), 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String) fields[1], extractSqlClob(fields[2]));\n      }\n    });\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreServerUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n  }"
        ],
        [
            "TxnDbUtil::prepDb(Configuration)",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "  public static void prepDb(Configuration conf) throws Exception {\n    // This is a bogus hack because it copies the contents of the SQL file\n    // intended for creating derby databases, and thus will inexorably get\n    // out of date with it.  I'm open to any suggestions on how to make this\n    // read the file in a build friendly way.\n\n    Connection conn = null;\n    Statement stmt = null;\n    try {\n      conn = getConnection(conf);\n      stmt = conn.createStatement();\n      stmt.execute(\"CREATE TABLE TXNS (\" +\n          \"  TXN_ID bigint PRIMARY KEY,\" +\n          \"  TXN_STATE char(1) NOT NULL,\" +\n          \"  TXN_STARTED bigint NOT NULL,\" +\n          \"  TXN_LAST_HEARTBEAT bigint NOT NULL,\" +\n          \"  TXN_USER varchar(128) NOT NULL,\" +\n          \"  TXN_HOST varchar(128) NOT NULL,\" +\n          \"  TXN_TYPE integer)\");\n\n      stmt.execute(\"CREATE TABLE TXN_COMPONENTS (\" +\n          \"  TC_TXNID bigint NOT NULL REFERENCES TXNS (TXN_ID),\" +\n          \"  TC_DATABASE varchar(128) NOT NULL,\" +\n          \"  TC_TABLE varchar(128),\" +\n          \"  TC_PARTITION varchar(767),\" +\n          \"  TC_OPERATION_TYPE char(1) NOT NULL,\" +\n          \"  TC_WRITEID bigint)\");\n      stmt.execute(\"CREATE TABLE COMPLETED_TXN_COMPONENTS (\" +\n          \"  CTC_TXNID bigint NOT NULL,\" +\n          \"  CTC_DATABASE varchar(128) NOT NULL,\" +\n          \"  CTC_TABLE varchar(128),\" +\n          \"  CTC_PARTITION varchar(767),\" +\n          \"  CTC_TIMESTAMP timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL,\" +\n          \"  CTC_WRITEID bigint,\" +\n          \"  CTC_UPDATE_DELETE char(1) NOT NULL)\");\n      stmt.execute(\"CREATE TABLE NEXT_TXN_ID (\" + \"  NTXN_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_TXN_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE TXN_TO_WRITE_ID (\" +\n          \" T2W_TXNID bigint NOT NULL,\" +\n          \" T2W_DATABASE varchar(128) NOT NULL,\" +\n          \" T2W_TABLE varchar(256) NOT NULL,\" +\n          \" T2W_WRITEID bigint NOT NULL)\");\n      stmt.execute(\"CREATE TABLE NEXT_WRITE_ID (\" +\n          \" NWI_DATABASE varchar(128) NOT NULL,\" +\n          \" NWI_TABLE varchar(256) NOT NULL,\" +\n          \" NWI_NEXT bigint NOT NULL)\");\n\n      stmt.execute(\"CREATE TABLE MIN_HISTORY_LEVEL (\" +\n          \" MHL_TXNID bigint NOT NULL,\" +\n          \" MHL_MIN_OPEN_TXNID bigint NOT NULL,\" +\n          \" PRIMARY KEY(MHL_TXNID))\");\n\n      stmt.execute(\"CREATE TABLE HIVE_LOCKS (\" +\n          \" HL_LOCK_EXT_ID bigint NOT NULL,\" +\n          \" HL_LOCK_INT_ID bigint NOT NULL,\" +\n          \" HL_TXNID bigint NOT NULL,\" +\n          \" HL_DB varchar(128) NOT NULL,\" +\n          \" HL_TABLE varchar(128),\" +\n          \" HL_PARTITION varchar(767),\" +\n          \" HL_LOCK_STATE char(1) NOT NULL,\" +\n          \" HL_LOCK_TYPE char(1) NOT NULL,\" +\n          \" HL_LAST_HEARTBEAT bigint NOT NULL,\" +\n          \" HL_ACQUIRED_AT bigint,\" +\n          \" HL_USER varchar(128) NOT NULL,\" +\n          \" HL_HOST varchar(128) NOT NULL,\" +\n          \" HL_HEARTBEAT_COUNT integer,\" +\n          \" HL_AGENT_INFO varchar(128),\" +\n          \" HL_BLOCKEDBY_EXT_ID bigint,\" +\n          \" HL_BLOCKEDBY_INT_ID bigint,\" +\n        \" PRIMARY KEY(HL_LOCK_EXT_ID, HL_LOCK_INT_ID))\");\n      stmt.execute(\"CREATE INDEX HL_TXNID_INDEX ON HIVE_LOCKS (HL_TXNID)\");\n\n      stmt.execute(\"CREATE TABLE NEXT_LOCK_ID (\" + \" NL_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_LOCK_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE COMPACTION_QUEUE (\" +\n          \" CQ_ID bigint PRIMARY KEY,\" +\n          \" CQ_DATABASE varchar(128) NOT NULL,\" +\n          \" CQ_TABLE varchar(128) NOT NULL,\" +\n          \" CQ_PARTITION varchar(767),\" +\n          \" CQ_STATE char(1) NOT NULL,\" +\n          \" CQ_TYPE char(1) NOT NULL,\" +\n          \" CQ_TBLPROPERTIES varchar(2048),\" +\n          \" CQ_WORKER_ID varchar(128),\" +\n          \" CQ_START bigint,\" +\n          \" CQ_RUN_AS varchar(128),\" +\n          \" CQ_HIGHEST_WRITE_ID bigint,\" +\n          \" CQ_META_INFO varchar(2048) for bit data,\" +\n          \" CQ_HADOOP_JOB_ID varchar(32))\");\n\n      stmt.execute(\"CREATE TABLE NEXT_COMPACTION_QUEUE_ID (NCQ_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_COMPACTION_QUEUE_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE COMPLETED_COMPACTIONS (\" +\n          \" CC_ID bigint PRIMARY KEY,\" +\n          \" CC_DATABASE varchar(128) NOT NULL,\" +\n          \" CC_TABLE varchar(128) NOT NULL,\" +\n          \" CC_PARTITION varchar(767),\" +\n          \" CC_STATE char(1) NOT NULL,\" +\n          \" CC_TYPE char(1) NOT NULL,\" +\n          \" CC_TBLPROPERTIES varchar(2048),\" +\n          \" CC_WORKER_ID varchar(128),\" +\n          \" CC_START bigint,\" +\n          \" CC_END bigint,\" +\n          \" CC_RUN_AS varchar(128),\" +\n          \" CC_HIGHEST_WRITE_ID bigint,\" +\n          \" CC_META_INFO varchar(2048) for bit data,\" +\n          \" CC_HADOOP_JOB_ID varchar(32))\");\n\n      stmt.execute(\"CREATE TABLE AUX_TABLE (\" +\n        \" MT_KEY1 varchar(128) NOT NULL,\" +\n        \" MT_KEY2 bigint NOT NULL,\" +\n        \" MT_COMMENT varchar(255),\" +\n        \" PRIMARY KEY(MT_KEY1, MT_KEY2))\");\n\n      stmt.execute(\"CREATE TABLE WRITE_SET (\" +\n        \" WS_DATABASE varchar(128) NOT NULL,\" +\n        \" WS_TABLE varchar(128) NOT NULL,\" +\n        \" WS_PARTITION varchar(767),\" +\n        \" WS_TXNID bigint NOT NULL,\" +\n        \" WS_COMMIT_ID bigint NOT NULL,\" +\n        \" WS_OPERATION_TYPE char(1) NOT NULL)\"\n      );\n\n      stmt.execute(\"CREATE TABLE REPL_TXN_MAP (\" +\n          \" RTM_REPL_POLICY varchar(256) NOT NULL, \" +\n          \" RTM_SRC_TXN_ID bigint NOT NULL, \" +\n          \" RTM_TARGET_TXN_ID bigint NOT NULL, \" +\n          \" PRIMARY KEY (RTM_REPL_POLICY, RTM_SRC_TXN_ID))\"\n      );\n\n      stmt.execute(\"CREATE TABLE MATERIALIZATION_REBUILD_LOCKS (\" +\n          \"  MRL_TXN_ID BIGINT NOT NULL, \" +\n          \"  MRL_DB_NAME VARCHAR(128) NOT NULL, \" +\n          \"  MRL_TBL_NAME VARCHAR(256) NOT NULL, \" +\n          \"  MRL_LAST_HEARTBEAT BIGINT NOT NULL, \" +\n          \"  PRIMARY KEY(MRL_TXN_ID))\"\n      );\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"TBLS\\\" (\\\"TBL_ID\\\" BIGINT NOT NULL, \" +\n            \" \\\"CREATE_TIME\\\" INTEGER NOT NULL, \\\"DB_ID\\\" BIGINT, \\\"LAST_ACCESS_TIME\\\" INTEGER NOT NULL, \" +\n            \" \\\"OWNER\\\" VARCHAR(767), \\\"OWNER_TYPE\\\" VARCHAR(10), \\\"RETENTION\\\" INTEGER NOT NULL, \" +\n            \" \\\"SD_ID\\\" BIGINT, \\\"TBL_NAME\\\" VARCHAR(256), \\\"TBL_TYPE\\\" VARCHAR(128), \" +\n            \" \\\"VIEW_EXPANDED_TEXT\\\" LONG VARCHAR, \\\"VIEW_ORIGINAL_TEXT\\\" LONG VARCHAR, \" +\n            \" \\\"IS_REWRITE_ENABLED\\\" CHAR(1) NOT NULL DEFAULT \\'N\\', \" +\n            \" \\\"WRITE_ID\\\" BIGINT DEFAULT 0, \" +\n            \" PRIMARY KEY (TBL_ID))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TBLS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"DBS\\\" (\\\"DB_ID\\\" BIGINT NOT NULL, \\\"DESC\\\" \" +\n            \"VARCHAR(4000), \\\"DB_LOCATION_URI\\\" VARCHAR(4000) NOT NULL, \\\"NAME\\\" VARCHAR(128), \" +\n            \"\\\"OWNER_NAME\\\" VARCHAR(128), \\\"OWNER_TYPE\\\" VARCHAR(10), \" +\n            \"\\\"CTLG_NAME\\\" VARCHAR(256) NOT NULL, PRIMARY KEY (DB_ID))\");\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TBLS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"PARTITIONS\\\" (\" +\n            \" \\\"PART_ID\\\" BIGINT NOT NULL, \\\"CREATE_TIME\\\" INTEGER NOT NULL, \" +\n            \" \\\"LAST_ACCESS_TIME\\\" INTEGER NOT NULL, \\\"PART_NAME\\\" VARCHAR(767), \" +\n            \" \\\"SD_ID\\\" BIGINT, \\\"TBL_ID\\\" BIGINT, \" +\n            \" \\\"WRITE_ID\\\" BIGINT DEFAULT 0, \" +\n            \" PRIMARY KEY (PART_ID))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"PARTITIONS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"TABLE_PARAMS\\\" (\" +\n            \" \\\"TBL_ID\\\" BIGINT NOT NULL, \\\"PARAM_KEY\\\" VARCHAR(256) NOT NULL, \" +\n            \" \\\"PARAM_VALUE\\\" CLOB, \" +\n            \" PRIMARY KEY (TBL_ID, PARAM_KEY))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TABLE_PARAMS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"PARTITION_PARAMS\\\" (\" +\n            \" \\\"PART_ID\\\" BIGINT NOT NULL, \\\"PARAM_KEY\\\" VARCHAR(256) NOT NULL, \" +\n            \" \\\"PARAM_VALUE\\\" VARCHAR(4000), \" +\n            \" PRIMARY KEY (PART_ID, PARAM_KEY))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"PARTITION_PARAMS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\" VARCHAR(256) NOT \" +\n\n                \"NULL, \\\"NEXT_VAL\\\" BIGINT NOT NULL)\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"SEQUENCE_TABLE table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\" (\\\"NNI_ID\\\" BIGINT NOT NULL, \" +\n\n                \"\\\"NEXT_EVENT_ID\\\" BIGINT NOT NULL)\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"NOTIFICATION_SEQUENCE table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"NOTIFICATION_LOG\\\" (\\\"NL_ID\\\" BIGINT NOT NULL, \" +\n                \"\\\"DB_NAME\\\" VARCHAR(128), \\\"EVENT_ID\\\" BIGINT NOT NULL, \\\"EVENT_TIME\\\" INTEGER NOT\" +\n\n                \" NULL, \\\"EVENT_TYPE\\\" VARCHAR(32) NOT NULL, \\\"MESSAGE\\\" CLOB, \\\"TBL_NAME\\\" \" +\n                \"VARCHAR\" +\n                \"(256), \\\"MESSAGE_FORMAT\\\" VARCHAR(16))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"NOTIFICATION_LOG table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\", \\\"NEXT_VAL\\\") \" +\n              \"SELECT * FROM (VALUES ('org.apache.hadoop.hive.metastore.model.MNotificationLog', \" +\n              \"1)) tmp_table WHERE NOT EXISTS ( SELECT \\\"NEXT_VAL\\\" FROM \\\"APP\\\"\" +\n              \".\\\"SEQUENCE_TABLE\\\" WHERE \\\"SEQUENCE_NAME\\\" = 'org.apache.hadoop.hive.metastore\" +\n              \".model.MNotificationLog')\");\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\" (\\\"NNI_ID\\\", \\\"NEXT_EVENT_ID\\\")\" +\n              \" SELECT * FROM (VALUES (1,1)) tmp_table WHERE NOT EXISTS ( SELECT \" +\n              \"\\\"NEXT_EVENT_ID\\\" FROM \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\")\");\n\n      try {\n        stmt.execute(\"CREATE TABLE TXN_WRITE_NOTIFICATION_LOG (\" +\n                \"WNL_ID bigint NOT NULL,\" +\n                \"WNL_TXNID bigint NOT NULL,\" +\n                \"WNL_WRITEID bigint NOT NULL,\" +\n                \"WNL_DATABASE varchar(128) NOT NULL,\" +\n                \"WNL_TABLE varchar(128) NOT NULL,\" +\n                \"WNL_PARTITION varchar(1024) NOT NULL,\" +\n                \"WNL_TABLE_OBJ clob NOT NULL,\" +\n                \"WNL_PARTITION_OBJ clob,\" +\n                \"WNL_FILES clob,\" +\n                \"WNL_EVENT_TIME integer NOT NULL,\" +\n                \"PRIMARY KEY (WNL_TXNID, WNL_DATABASE, WNL_TABLE, WNL_PARTITION))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TXN_WRITE_NOTIFICATION_LOG table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\", \\\"NEXT_VAL\\\") \" +\n              \"SELECT * FROM (VALUES ('org.apache.hadoop.hive.metastore.model.MTxnWriteNotificationLog', \" +\n              \"1)) tmp_table WHERE NOT EXISTS ( SELECT \\\"NEXT_VAL\\\" FROM \\\"APP\\\"\" +\n              \".\\\"SEQUENCE_TABLE\\\" WHERE \\\"SEQUENCE_NAME\\\" = 'org.apache.hadoop.hive.metastore\" +\n              \".model.MTxnWriteNotificationLog')\");\n    } catch (SQLException e) {\n      try {\n        conn.rollback();\n      } catch (SQLException re) {\n        LOG.error(\"Error rolling back: \" + re.getMessage());\n      }\n\n      // Another thread might have already created these tables.\n      if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n        LOG.info(\"Txn tables already exist, returning\");\n        return;\n      }\n\n      // This might be a deadlock, if so, let's retry\n      if (e instanceof SQLTransactionRollbackException && deadlockCnt++ < 5) {\n        LOG.warn(\"Caught deadlock, retrying db creation\");\n        prepDb(conf);\n      } else {\n        throw e;\n      }\n    } finally {\n      deadlockCnt = 0;\n      closeResources(conn, stmt, null);\n    }\n  }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "  public static void prepDb(Configuration conf) throws Exception {\n    // This is a bogus hack because it copies the contents of the SQL file\n    // intended for creating derby databases, and thus will inexorably get\n    // out of date with it.  I'm open to any suggestions on how to make this\n    // read the file in a build friendly way.\n\n    Connection conn = null;\n    Statement stmt = null;\n    try {\n      conn = getConnection(conf);\n      stmt = conn.createStatement();\n      stmt.execute(\"CREATE TABLE TXNS (\" +\n          \"  TXN_ID bigint PRIMARY KEY,\" +\n          \"  TXN_STATE char(1) NOT NULL,\" +\n          \"  TXN_STARTED bigint NOT NULL,\" +\n          \"  TXN_LAST_HEARTBEAT bigint NOT NULL,\" +\n          \"  TXN_USER varchar(128) NOT NULL,\" +\n          \"  TXN_HOST varchar(128) NOT NULL,\" +\n          \"  TXN_TYPE integer)\");\n\n      stmt.execute(\"CREATE TABLE TXN_COMPONENTS (\" +\n          \"  TC_TXNID bigint NOT NULL REFERENCES TXNS (TXN_ID),\" +\n          \"  TC_DATABASE varchar(128) NOT NULL,\" +\n          \"  TC_TABLE varchar(128),\" +\n          \"  TC_PARTITION varchar(767),\" +\n          \"  TC_OPERATION_TYPE char(1) NOT NULL,\" +\n          \"  TC_WRITEID bigint)\");\n      stmt.execute(\"CREATE TABLE COMPLETED_TXN_COMPONENTS (\" +\n          \"  CTC_TXNID bigint NOT NULL,\" +\n          \"  CTC_DATABASE varchar(128) NOT NULL,\" +\n          \"  CTC_TABLE varchar(128),\" +\n          \"  CTC_PARTITION varchar(767),\" +\n          \"  CTC_TIMESTAMP timestamp DEFAULT CURRENT_TIMESTAMP NOT NULL,\" +\n          \"  CTC_WRITEID bigint,\" +\n          \"  CTC_UPDATE_DELETE char(1) NOT NULL)\");\n      stmt.execute(\"CREATE TABLE NEXT_TXN_ID (\" + \"  NTXN_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_TXN_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE TXN_TO_WRITE_ID (\" +\n          \" T2W_TXNID bigint NOT NULL,\" +\n          \" T2W_DATABASE varchar(128) NOT NULL,\" +\n          \" T2W_TABLE varchar(256) NOT NULL,\" +\n          \" T2W_WRITEID bigint NOT NULL)\");\n      stmt.execute(\"CREATE TABLE NEXT_WRITE_ID (\" +\n          \" NWI_DATABASE varchar(128) NOT NULL,\" +\n          \" NWI_TABLE varchar(256) NOT NULL,\" +\n          \" NWI_NEXT bigint NOT NULL)\");\n\n      stmt.execute(\"CREATE TABLE MIN_HISTORY_LEVEL (\" +\n          \" MHL_TXNID bigint NOT NULL,\" +\n          \" MHL_MIN_OPEN_TXNID bigint NOT NULL,\" +\n          \" PRIMARY KEY(MHL_TXNID))\");\n\n      stmt.execute(\"CREATE TABLE HIVE_LOCKS (\" +\n          \" HL_LOCK_EXT_ID bigint NOT NULL,\" +\n          \" HL_LOCK_INT_ID bigint NOT NULL,\" +\n          \" HL_TXNID bigint NOT NULL,\" +\n          \" HL_DB varchar(128) NOT NULL,\" +\n          \" HL_TABLE varchar(128),\" +\n          \" HL_PARTITION varchar(767),\" +\n          \" HL_LOCK_STATE char(1) NOT NULL,\" +\n          \" HL_LOCK_TYPE char(1) NOT NULL,\" +\n          \" HL_LAST_HEARTBEAT bigint NOT NULL,\" +\n          \" HL_ACQUIRED_AT bigint,\" +\n          \" HL_USER varchar(128) NOT NULL,\" +\n          \" HL_HOST varchar(128) NOT NULL,\" +\n          \" HL_HEARTBEAT_COUNT integer,\" +\n          \" HL_AGENT_INFO varchar(128),\" +\n          \" HL_BLOCKEDBY_EXT_ID bigint,\" +\n          \" HL_BLOCKEDBY_INT_ID bigint,\" +\n        \" PRIMARY KEY(HL_LOCK_EXT_ID, HL_LOCK_INT_ID))\");\n      stmt.execute(\"CREATE INDEX HL_TXNID_INDEX ON HIVE_LOCKS (HL_TXNID)\");\n\n      stmt.execute(\"CREATE TABLE NEXT_LOCK_ID (\" + \" NL_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_LOCK_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE COMPACTION_QUEUE (\" +\n          \" CQ_ID bigint PRIMARY KEY,\" +\n          \" CQ_DATABASE varchar(128) NOT NULL,\" +\n          \" CQ_TABLE varchar(128) NOT NULL,\" +\n          \" CQ_PARTITION varchar(767),\" +\n          \" CQ_STATE char(1) NOT NULL,\" +\n          \" CQ_TYPE char(1) NOT NULL,\" +\n          \" CQ_TBLPROPERTIES varchar(2048),\" +\n          \" CQ_WORKER_ID varchar(128),\" +\n          \" CQ_START bigint,\" +\n          \" CQ_RUN_AS varchar(128),\" +\n          \" CQ_HIGHEST_WRITE_ID bigint,\" +\n          \" CQ_META_INFO varchar(2048) for bit data,\" +\n          \" CQ_HADOOP_JOB_ID varchar(32))\");\n\n      stmt.execute(\"CREATE TABLE NEXT_COMPACTION_QUEUE_ID (NCQ_NEXT bigint NOT NULL)\");\n      stmt.execute(\"INSERT INTO NEXT_COMPACTION_QUEUE_ID VALUES(1)\");\n\n      stmt.execute(\"CREATE TABLE COMPLETED_COMPACTIONS (\" +\n          \" CC_ID bigint PRIMARY KEY,\" +\n          \" CC_DATABASE varchar(128) NOT NULL,\" +\n          \" CC_TABLE varchar(128) NOT NULL,\" +\n          \" CC_PARTITION varchar(767),\" +\n          \" CC_STATE char(1) NOT NULL,\" +\n          \" CC_TYPE char(1) NOT NULL,\" +\n          \" CC_TBLPROPERTIES varchar(2048),\" +\n          \" CC_WORKER_ID varchar(128),\" +\n          \" CC_START bigint,\" +\n          \" CC_END bigint,\" +\n          \" CC_RUN_AS varchar(128),\" +\n          \" CC_HIGHEST_WRITE_ID bigint,\" +\n          \" CC_META_INFO varchar(2048) for bit data,\" +\n          \" CC_HADOOP_JOB_ID varchar(32))\");\n\n      stmt.execute(\"CREATE TABLE AUX_TABLE (\" +\n        \" MT_KEY1 varchar(128) NOT NULL,\" +\n        \" MT_KEY2 bigint NOT NULL,\" +\n        \" MT_COMMENT varchar(255),\" +\n        \" PRIMARY KEY(MT_KEY1, MT_KEY2))\");\n\n      stmt.execute(\"CREATE TABLE WRITE_SET (\" +\n        \" WS_DATABASE varchar(128) NOT NULL,\" +\n        \" WS_TABLE varchar(128) NOT NULL,\" +\n        \" WS_PARTITION varchar(767),\" +\n        \" WS_TXNID bigint NOT NULL,\" +\n        \" WS_COMMIT_ID bigint NOT NULL,\" +\n        \" WS_OPERATION_TYPE char(1) NOT NULL)\"\n      );\n\n      stmt.execute(\"CREATE TABLE REPL_TXN_MAP (\" +\n          \" RTM_REPL_POLICY varchar(256) NOT NULL, \" +\n          \" RTM_SRC_TXN_ID bigint NOT NULL, \" +\n          \" RTM_TARGET_TXN_ID bigint NOT NULL, \" +\n          \" PRIMARY KEY (RTM_REPL_POLICY, RTM_SRC_TXN_ID))\"\n      );\n\n      stmt.execute(\"CREATE TABLE MATERIALIZATION_REBUILD_LOCKS (\" +\n          \"  MRL_TXN_ID BIGINT NOT NULL, \" +\n          \"  MRL_DB_NAME VARCHAR(128) NOT NULL, \" +\n          \"  MRL_TBL_NAME VARCHAR(256) NOT NULL, \" +\n          \"  MRL_LAST_HEARTBEAT BIGINT NOT NULL, \" +\n          \"  PRIMARY KEY(MRL_TXN_ID))\"\n      );\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"TBLS\\\" (\\\"TBL_ID\\\" BIGINT NOT NULL, \" +\n            \" \\\"CREATE_TIME\\\" INTEGER NOT NULL, \\\"DB_ID\\\" BIGINT, \\\"LAST_ACCESS_TIME\\\" INTEGER NOT NULL, \" +\n            \" \\\"OWNER\\\" VARCHAR(767), \\\"OWNER_TYPE\\\" VARCHAR(10), \\\"RETENTION\\\" INTEGER NOT NULL, \" +\n            \" \\\"SD_ID\\\" BIGINT, \\\"TBL_NAME\\\" VARCHAR(256), \\\"TBL_TYPE\\\" VARCHAR(128), \" +\n            \" \\\"VIEW_EXPANDED_TEXT\\\" LONG VARCHAR, \\\"VIEW_ORIGINAL_TEXT\\\" LONG VARCHAR, \" +\n            \" \\\"IS_REWRITE_ENABLED\\\" CHAR(1) NOT NULL DEFAULT \\'N\\', \" +\n            \" \\\"WRITE_ID\\\" BIGINT DEFAULT 0, \" +\n            \" PRIMARY KEY (TBL_ID))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TBLS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"DBS\\\" (\\\"DB_ID\\\" BIGINT NOT NULL, \\\"DESC\\\" \" +\n            \"VARCHAR(4000), \\\"DB_LOCATION_URI\\\" VARCHAR(4000) NOT NULL, \\\"NAME\\\" VARCHAR(128), \" +\n            \"\\\"OWNER_NAME\\\" VARCHAR(128), \\\"OWNER_TYPE\\\" VARCHAR(10), \" +\n            \"\\\"CTLG_NAME\\\" VARCHAR(256) NOT NULL, PRIMARY KEY (DB_ID))\");\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TBLS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"PARTITIONS\\\" (\" +\n            \" \\\"PART_ID\\\" BIGINT NOT NULL, \\\"CREATE_TIME\\\" INTEGER NOT NULL, \" +\n            \" \\\"LAST_ACCESS_TIME\\\" INTEGER NOT NULL, \\\"PART_NAME\\\" VARCHAR(767), \" +\n            \" \\\"SD_ID\\\" BIGINT, \\\"TBL_ID\\\" BIGINT, \" +\n            \" \\\"WRITE_ID\\\" BIGINT DEFAULT 0, \" +\n            \" PRIMARY KEY (PART_ID))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"PARTITIONS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"TABLE_PARAMS\\\" (\" +\n            \" \\\"TBL_ID\\\" BIGINT NOT NULL, \\\"PARAM_KEY\\\" VARCHAR(256) NOT NULL, \" +\n            \" \\\"PARAM_VALUE\\\" CLOB, \" +\n            \" PRIMARY KEY (TBL_ID, PARAM_KEY))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TABLE_PARAMS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"PARTITION_PARAMS\\\" (\" +\n            \" \\\"PART_ID\\\" BIGINT NOT NULL, \\\"PARAM_KEY\\\" VARCHAR(256) NOT NULL, \" +\n            \" \\\"PARAM_VALUE\\\" CLOB, \" +\n            \" PRIMARY KEY (PART_ID, PARAM_KEY))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"PARTITION_PARAMS table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\" VARCHAR(256) NOT \" +\n\n                \"NULL, \\\"NEXT_VAL\\\" BIGINT NOT NULL)\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"SEQUENCE_TABLE table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\" (\\\"NNI_ID\\\" BIGINT NOT NULL, \" +\n\n                \"\\\"NEXT_EVENT_ID\\\" BIGINT NOT NULL)\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"NOTIFICATION_SEQUENCE table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      try {\n        stmt.execute(\"CREATE TABLE \\\"APP\\\".\\\"NOTIFICATION_LOG\\\" (\\\"NL_ID\\\" BIGINT NOT NULL, \" +\n                \"\\\"DB_NAME\\\" VARCHAR(128), \\\"EVENT_ID\\\" BIGINT NOT NULL, \\\"EVENT_TIME\\\" INTEGER NOT\" +\n\n                \" NULL, \\\"EVENT_TYPE\\\" VARCHAR(32) NOT NULL, \\\"MESSAGE\\\" CLOB, \\\"TBL_NAME\\\" \" +\n                \"VARCHAR\" +\n                \"(256), \\\"MESSAGE_FORMAT\\\" VARCHAR(16))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"NOTIFICATION_LOG table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\", \\\"NEXT_VAL\\\") \" +\n              \"SELECT * FROM (VALUES ('org.apache.hadoop.hive.metastore.model.MNotificationLog', \" +\n              \"1)) tmp_table WHERE NOT EXISTS ( SELECT \\\"NEXT_VAL\\\" FROM \\\"APP\\\"\" +\n              \".\\\"SEQUENCE_TABLE\\\" WHERE \\\"SEQUENCE_NAME\\\" = 'org.apache.hadoop.hive.metastore\" +\n              \".model.MNotificationLog')\");\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\" (\\\"NNI_ID\\\", \\\"NEXT_EVENT_ID\\\")\" +\n              \" SELECT * FROM (VALUES (1,1)) tmp_table WHERE NOT EXISTS ( SELECT \" +\n              \"\\\"NEXT_EVENT_ID\\\" FROM \\\"APP\\\".\\\"NOTIFICATION_SEQUENCE\\\")\");\n\n      try {\n        stmt.execute(\"CREATE TABLE TXN_WRITE_NOTIFICATION_LOG (\" +\n                \"WNL_ID bigint NOT NULL,\" +\n                \"WNL_TXNID bigint NOT NULL,\" +\n                \"WNL_WRITEID bigint NOT NULL,\" +\n                \"WNL_DATABASE varchar(128) NOT NULL,\" +\n                \"WNL_TABLE varchar(128) NOT NULL,\" +\n                \"WNL_PARTITION varchar(1024) NOT NULL,\" +\n                \"WNL_TABLE_OBJ clob NOT NULL,\" +\n                \"WNL_PARTITION_OBJ clob,\" +\n                \"WNL_FILES clob,\" +\n                \"WNL_EVENT_TIME integer NOT NULL,\" +\n                \"PRIMARY KEY (WNL_TXNID, WNL_DATABASE, WNL_TABLE, WNL_PARTITION))\"\n        );\n      } catch (SQLException e) {\n        if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n          LOG.info(\"TXN_WRITE_NOTIFICATION_LOG table already exist, ignoring\");\n        } else {\n          throw e;\n        }\n      }\n\n      stmt.execute(\"INSERT INTO \\\"APP\\\".\\\"SEQUENCE_TABLE\\\" (\\\"SEQUENCE_NAME\\\", \\\"NEXT_VAL\\\") \" +\n              \"SELECT * FROM (VALUES ('org.apache.hadoop.hive.metastore.model.MTxnWriteNotificationLog', \" +\n              \"1)) tmp_table WHERE NOT EXISTS ( SELECT \\\"NEXT_VAL\\\" FROM \\\"APP\\\"\" +\n              \".\\\"SEQUENCE_TABLE\\\" WHERE \\\"SEQUENCE_NAME\\\" = 'org.apache.hadoop.hive.metastore\" +\n              \".model.MTxnWriteNotificationLog')\");\n    } catch (SQLException e) {\n      try {\n        conn.rollback();\n      } catch (SQLException re) {\n        LOG.error(\"Error rolling back: \" + re.getMessage());\n      }\n\n      // Another thread might have already created these tables.\n      if (e.getMessage() != null && e.getMessage().contains(\"already exists\")) {\n        LOG.info(\"Txn tables already exist, returning\");\n        return;\n      }\n\n      // This might be a deadlock, if so, let's retry\n      if (e instanceof SQLTransactionRollbackException && deadlockCnt++ < 5) {\n        LOG.warn(\"Caught deadlock, retrying db creation\");\n        prepDb(conf);\n      } else {\n        throw e;\n      }\n    } finally {\n      deadlockCnt = 0;\n      closeResources(conn, stmt, null);\n    }\n  }"
        ],
        [
            "MetastoreDirectSqlUtils::setPartitionParameters(String,boolean,PersistenceManager,String,TreeMap)",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "  static void setPartitionParameters(String PARTITION_PARAMS, boolean convertMapNullsToEmptyStrings,\n      PersistenceManager pm, String partIds, TreeMap<Long, Partition> partitions)\n      throws MetaException {\n    String queryText;\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \" + PARTITION_PARAMS + \"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(pm, partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreServerUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n  }",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "  static void setPartitionParameters(String PARTITION_PARAMS, boolean convertMapNullsToEmptyStrings,\n      PersistenceManager pm, String partIds, TreeMap<Long, Partition> partitions)\n      throws MetaException {\n    String queryText;\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \" + PARTITION_PARAMS + \"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(pm, partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], extractSqlClob(fields[2]));\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreServerUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n  }"
        ]
    ],
    "69a7fc5955ceac1245233c0d297929e730750929": [
        [
            "TezCompiler::optimizeOperatorPlan(ParseContext,Set,Set)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178 -\n 179 -\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "  @Override\n  protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    // Create the context for the walker\n    OptimizeTezProcContext procCtx = new OptimizeTezProcContext(conf, pCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    runTopNKeyOptimization(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run top n key optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup dynamic partition pruning where possible\n    runDynamicPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup dynamic partition pruning\");\n\n    // need to run this; to get consistent filterop conditions(for operator tree matching)\n    if (procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup stats in the operator plan\n    runStatsAnnotation(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup stats in the operator plan\");\n\n    // Update bucketing version of ReduceSinkOp if needed\n    updateBucketingVersionForUpgrade(procCtx);\n\n    // run Sorted dynamic partition optimization\n    if(HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.DYNAMICPARTITIONING) &&\n        HiveConf.getVar(procCtx.conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE).equals(\"nonstrict\") &&\n        !HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n      new SortedDynPartitionOptimizer().transform(procCtx.parseContext);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Sorted dynamic partition optimization\");\n    }\n\n    if(HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.HIVEOPTREDUCEDEDUPLICATION)\n        || procCtx.parseContext.hasAcidWrite()) {\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n      // Dynamic sort partition adds an extra RS therefore need to de-dup\n      new ReduceSinkDeDuplication().transform(procCtx.parseContext);\n      // there is an issue with dedup logic wherein SELECT is created with wrong columns\n      // NonBlockingOpDeDupProc fixes that\n      // (kind of hackish, the issue in de-dup should be fixed but it needs more investigation)\n      new NonBlockingOpDeDupProc().transform(procCtx.parseContext);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Reduce Sink de-duplication\");\n    }\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // run the optimizations that use stats for optimization\n    runStatsDependentOptimizations(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTJOINREDUCEDEDUPLICATION)) {\n      new ReduceSinkJoinDeDuplication().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run reduce sink after join algorithm selection\");\n\n    semijoinRemovalBasedTransformations(procCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVE_SHARED_WORK_OPTIMIZATION)) {\n      new SharedWorkOptimizer().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Shared scans optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    markOperatorsWithUnstableRuntimeStats(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"markOperatorsWithUnstableRuntimeStats\");\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    // Rather than run the full constant folding just need to shortcut AND/OR expressions\n    // involving constant true/false values.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n  }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238  ",
            "  @Override\n  protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n    PerfLogger perfLogger = SessionState.getPerfLogger();\n    // Create the context for the walker\n    OptimizeTezProcContext procCtx = new OptimizeTezProcContext(conf, pCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    runTopNKeyOptimization(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run top n key optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup dynamic partition pruning where possible\n    runDynamicPartitionPruning(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup dynamic partition pruning\");\n\n    // need to run this; to get consistent filterop conditions(for operator tree matching)\n    if (procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // setup stats in the operator plan\n    runStatsAnnotation(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Setup stats in the operator plan\");\n\n    // run Sorted dynamic partition optimization\n    if(HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.DYNAMICPARTITIONING) &&\n        HiveConf.getVar(procCtx.conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE).equals(\"nonstrict\") &&\n        !HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n      new SortedDynPartitionOptimizer().transform(procCtx.parseContext);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Sorted dynamic partition optimization\");\n    }\n\n    if(HiveConf.getBoolVar(procCtx.conf, HiveConf.ConfVars.HIVEOPTREDUCEDEDUPLICATION)\n        || procCtx.parseContext.hasAcidWrite()) {\n      perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n      // Dynamic sort partition adds an extra RS therefore need to de-dup\n      new ReduceSinkDeDuplication().transform(procCtx.parseContext);\n      // there is an issue with dedup logic wherein SELECT is created with wrong columns\n      // NonBlockingOpDeDupProc fixes that\n      // (kind of hackish, the issue in de-dup should be fixed but it needs more investigation)\n      new NonBlockingOpDeDupProc().transform(procCtx.parseContext);\n      perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Reduce Sink de-duplication\");\n    }\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    // run the optimizations that use stats for optimization\n    runStatsDependentOptimizations(procCtx, inputs, outputs);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run the optimizations that use stats for optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTJOINREDUCEDEDUPLICATION)) {\n      new ReduceSinkJoinDeDuplication().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Run reduce sink after join algorithm selection\");\n\n    semijoinRemovalBasedTransformations(procCtx, inputs, outputs);\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    if(procCtx.conf.getBoolVar(ConfVars.HIVE_SHARED_WORK_OPTIMIZATION)) {\n      new SharedWorkOptimizer().transform(procCtx.parseContext);\n    }\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"Shared scans optimization\");\n\n    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.TEZ_COMPILER);\n    markOperatorsWithUnstableRuntimeStats(procCtx);\n    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, \"markOperatorsWithUnstableRuntimeStats\");\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    // Rather than run the full constant folding just need to shortcut AND/OR expressions\n    // involving constant true/false values.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n\n    // ATTENTION : DO NOT, I REPEAT, DO NOT WRITE ANYTHING AFTER updateBucketingVersionForUpgrade()\n    // ANYTHING WHICH NEEDS TO BE ADDED MUST BE ADDED ABOVE\n    // This call updates the bucketing version of final ReduceSinkOp based on\n    // the bucketing version of FileSinkOp. This operation must happen at the\n    // end to ensure there is no further rewrite of plan which may end up\n    // removing/updating the ReduceSinkOp as was the case with SortedDynPartitionOptimizer\n    // Update bucketing version of ReduceSinkOp if needed\n    updateBucketingVersionForUpgrade(procCtx);\n\n  }"
        ],
        [
            "TezCompiler::updateBucketingVersionForUpgrade(OptimizeTezProcContext)",
            "1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836 -\n1837 -\n1838 -\n1839 -\n1840 -\n1841 -\n1842 -\n1843 -\n1844 -\n1845 -\n1846 -\n1847 -\n1848  \n1849  \n1850 -\n1851  \n1852 -\n1853 -\n1854 -\n1855 -\n1856 -\n1857 -\n1858 -\n1859 -\n1860  \n1861  \n1862  ",
            "  private void updateBucketingVersionForUpgrade(OptimizeTezProcContext procCtx) {\n    // Fetch all the FileSinkOperators.\n    Set<FileSinkOperator> fsOpsAll = new HashSet<>();\n    for (TableScanOperator ts : procCtx.parseContext.getTopOps().values()) {\n      Set<FileSinkOperator> fsOps = OperatorUtils.findOperators(\n          ts, FileSinkOperator.class);\n      fsOpsAll.addAll(fsOps);\n    }\n\n\n    for (FileSinkOperator fsOp : fsOpsAll) {\n      Operator<?> parentOfFS = fsOp.getParentOperators().get(0);\n      if (parentOfFS instanceof GroupByOperator) {\n        GroupByOperator gbyOp = (GroupByOperator) parentOfFS;\n        List<String> aggs = gbyOp.getConf().getAggregatorStrings();\n        boolean compute_stats = false;\n        for (String agg : aggs) {\n          if (agg.equalsIgnoreCase(\"compute_stats\")) {\n            compute_stats = true;\n            break;\n          }\n        }\n        if (compute_stats) {\n          continue;\n        }\n      }\n\n      // Not compute_stats\n      Set<ReduceSinkOperator> rsOps = OperatorUtils.findOperatorsUpstream(parentOfFS, ReduceSinkOperator.class);\n      if (rsOps.isEmpty()) {\n        continue;\n      }\n      // Skip setting if the bucketing version is not set in FileSinkOp.\n      if (fsOp.getConf().getTableInfo().isSetBucketingVersion()) {\n        rsOps.iterator().next().setBucketingVersion(fsOp.getConf().getTableInfo().getBucketingVersion());\n      }\n    }\n  }",
            "1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842 +\n1843 +\n1844 +\n1845 +\n1846 +\n1847 +\n1848 +\n1849 +\n1850 +\n1851 +\n1852 +\n1853  \n1854  \n1855  \n1856 +\n1857 +\n1858 +\n1859  \n1860  \n1861  ",
            "  private void updateBucketingVersionForUpgrade(OptimizeTezProcContext procCtx) {\n    // Fetch all the FileSinkOperators.\n    Set<FileSinkOperator> fsOpsAll = new HashSet<>();\n    for (TableScanOperator ts : procCtx.parseContext.getTopOps().values()) {\n      Set<FileSinkOperator> fsOps = OperatorUtils.findOperators(\n          ts, FileSinkOperator.class);\n      fsOpsAll.addAll(fsOps);\n    }\n\n\n    for (FileSinkOperator fsOp : fsOpsAll) {\n      if (!fsOp.getConf().getTableInfo().isSetBucketingVersion()) {\n        continue;\n      }\n      // Look for direct parent ReduceSinkOp\n      // If there are more than 1 parent, bail out.\n      Operator<?> parent = fsOp;\n      List<Operator<?>> parentOps = parent.getParentOperators();\n      while (parentOps != null && parentOps.size() == 1) {\n        parent = parentOps.get(0);\n        if (!(parent instanceof ReduceSinkOperator)) {\n          parentOps = parent.getParentOperators();\n          continue;\n        }\n\n        // Found the target RSOp\n        parent.setBucketingVersion(fsOp.getConf().getTableInfo().getBucketingVersion());\n        break;\n      }\n    }\n  }"
        ]
    ]
}