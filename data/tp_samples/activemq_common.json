{
    "67ead201e1cb0e7dc019002d7a2e4be53184261d": [
        [
            "MessageDatabase::MessageOrderIndex::setBatch(Transaction,Long)",
            "2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895 -\n2896 -\n2897 -\n2898 -\n2899 -\n2900 -\n2901 -\n2902 -\n2903  \n2904 -\n2905  \n2906  \n2907  \n2908  \n2909  ",
            "        void setBatch(Transaction tx, Long sequence) throws IOException {\n            if (sequence != null) {\n                Long nextPosition = new Long(sequence.longValue() + 1);\n                if (defaultPriorityIndex.containsKey(tx, sequence)) {\n                    lastDefaultKey = sequence;\n                    cursor.defaultCursorPosition = nextPosition.longValue();\n                } else if (highPriorityIndex != null) {\n                    if (highPriorityIndex.containsKey(tx, sequence)) {\n                        lastHighKey = sequence;\n                        cursor.highPriorityCursorPosition = nextPosition.longValue();\n                    } else if (lowPriorityIndex.containsKey(tx, sequence)) {\n                        lastLowKey = sequence;\n                        cursor.lowPriorityCursorPosition = nextPosition.longValue();\n                    }\n                } else {\n                    LOG.warn(\"setBatch: sequence \" + sequence + \" not found in orderindex:\" + this);\n                    lastDefaultKey = sequence;\n                    cursor.defaultCursorPosition = nextPosition.longValue();\n                }\n            }\n        }",
            "2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895 +\n2896 +\n2897 +\n2898 +\n2899 +\n2900 +\n2901  \n2902  \n2903  \n2904  \n2905  \n2906  ",
            "        void setBatch(Transaction tx, Long sequence) throws IOException {\n            if (sequence != null) {\n                Long nextPosition = new Long(sequence.longValue() + 1);\n                if (defaultPriorityIndex.containsKey(tx, sequence)) {\n                    lastDefaultKey = sequence;\n                    cursor.defaultCursorPosition = nextPosition.longValue();\n                } else if (highPriorityIndex != null && highPriorityIndex.containsKey(tx, sequence)) {\n                    lastHighKey = sequence;\n                    cursor.highPriorityCursorPosition = nextPosition.longValue();\n                } else if (lowPriorityIndex.containsKey(tx, sequence)) {\n                    lastLowKey = sequence;\n                    cursor.lowPriorityCursorPosition = nextPosition.longValue();\n                } else {\n                    lastDefaultKey = sequence;\n                    cursor.defaultCursorPosition = nextPosition.longValue();\n                }\n            }\n        }"
        ],
        [
            "Queue::getMessage(String)",
            "1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203 -\n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220 -\n1221  \n1222  \n1223  ",
            "    public QueueMessageReference getMessage(String id) {\n        MessageId msgId = new MessageId(id);\n        pagedInMessagesLock.readLock().lock();\n        try {\n            QueueMessageReference ref = (QueueMessageReference)this.pagedInMessages.get(msgId);\n            if (ref != null) {\n                return ref;\n            }\n        } finally {\n            pagedInMessagesLock.readLock().unlock();\n        }\n        messagesLock.readLock().lock();\n        try{\n            try {\n                messages.reset();\n                while (messages.hasNext()) {\n                    MessageReference mr = messages.next();\n                    QueueMessageReference qmr = createMessageReference(mr.getMessage());\n                    qmr.decrementReferenceCount();\n                    messages.rollback(qmr.getMessageId());\n                    if (msgId.equals(qmr.getMessageId())) {\n                        return qmr;\n                    }\n                }\n            } finally {\n                messages.release();\n            }\n        }finally {\n            messagesLock.readLock().unlock();\n        }\n        return null;\n    }",
            "1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203 +\n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220 +\n1221  \n1222  \n1223  ",
            "    public QueueMessageReference getMessage(String id) {\n        MessageId msgId = new MessageId(id);\n        pagedInMessagesLock.readLock().lock();\n        try {\n            QueueMessageReference ref = (QueueMessageReference)this.pagedInMessages.get(msgId);\n            if (ref != null) {\n                return ref;\n            }\n        } finally {\n            pagedInMessagesLock.readLock().unlock();\n        }\n        messagesLock.writeLock().lock();\n        try{\n            try {\n                messages.reset();\n                while (messages.hasNext()) {\n                    MessageReference mr = messages.next();\n                    QueueMessageReference qmr = createMessageReference(mr.getMessage());\n                    qmr.decrementReferenceCount();\n                    messages.rollback(qmr.getMessageId());\n                    if (msgId.equals(qmr.getMessageId())) {\n                        return qmr;\n                    }\n                }\n            } finally {\n                messages.release();\n            }\n        }finally {\n            messagesLock.writeLock().unlock();\n        }\n        return null;\n    }"
        ],
        [
            "AMQ5266SingleDestTest::parameters()",
            "  92  \n  93  \n  94  \n  95 -\n  96  \n  97  ",
            "    @Parameterized.Parameters(name=\"#{0},producerThreads:{1},consumerThreads:{2},mL:{3},useCache:{4},useDefaultStore:{5},optimizedDispatch:{6}\")\n    public static Iterable<Object[]> parameters() {\n        return Arrays.asList(new Object[][]{\n                {1000,  80,  80,   1024*1024*5,  true, true, false},\n        });\n    }",
            "  89  \n  90  \n  91  \n  92 +\n  93  \n  94  ",
            "    @Parameterized.Parameters(name=\"#{0},producerThreads:{1},consumerThreads:{2},mL:{3},useCache:{4},useDefaultStore:{5},optimizedDispatch:{6}\")\n    public static Iterable<Object[]> parameters() {\n        return Arrays.asList(new Object[][]{\n                {1000,  80,  80,   1024*1024*1,  true, TestSupport.PersistenceAdapterChoice.KahaDB, false},\n        });\n    }"
        ],
        [
            "TestSupport::setPersistenceAdapter(BrokerService,PersistenceAdapterChoice)",
            " 182 -\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "    public PersistenceAdapter setPersistenceAdapter(BrokerService broker, PersistenceAdapterChoice choice) throws IOException {\n        PersistenceAdapter adapter = null;\n        switch (choice) {\n        case JDBC:\n            adapter = new JDBCPersistenceAdapter();\n            break;\n        case KahaDB:\n            adapter = new KahaDBPersistenceAdapter();\n            break;\n        case LevelDB:\n            adapter = new LevelDBPersistenceAdapter();\n            break;\n        case MEM:\n            adapter = new MemoryPersistenceAdapter();\n            break;\n        }\n        broker.setPersistenceAdapter(adapter);\n        return adapter;\n    }",
            " 182 +\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  ",
            "    public static PersistenceAdapter setPersistenceAdapter(BrokerService broker, PersistenceAdapterChoice choice) throws IOException {\n        PersistenceAdapter adapter = null;\n        switch (choice) {\n        case JDBC:\n            adapter = new JDBCPersistenceAdapter();\n            break;\n        case KahaDB:\n            adapter = new KahaDBPersistenceAdapter();\n            break;\n        case LevelDB:\n            adapter = new LevelDBPersistenceAdapter();\n            break;\n        case MEM:\n            adapter = new MemoryPersistenceAdapter();\n            break;\n        }\n        broker.setPersistenceAdapter(adapter);\n        return adapter;\n    }"
        ],
        [
            "AMQ5266SingleDestTest::startBroker()",
            " 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110 -\n 111 -\n 112 -\n 113 -\n 114 -\n 115 -\n 116 -\n 117 -\n 118 -\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  ",
            "    @Before\n    public void startBroker() throws Exception {\n        brokerService = new BrokerService();\n\n        dataSource = new EmbeddedDataSource();\n        dataSource.setDatabaseName(\"target/derbyDb\");\n        dataSource.setCreateDatabase(\"create\");\n\n        JDBCPersistenceAdapter jdbcPersistenceAdapter = new JDBCPersistenceAdapter();\n        jdbcPersistenceAdapter.setDataSource(dataSource);\n        jdbcPersistenceAdapter.setUseLock(false);\n\n        if (!useDefaultStore) {\n            brokerService.setPersistenceAdapter(jdbcPersistenceAdapter);\n        } else {\n            KahaDBPersistenceAdapter kahaDBPersistenceAdapter = (KahaDBPersistenceAdapter) brokerService.getPersistenceAdapter();\n            kahaDBPersistenceAdapter.setConcurrentStoreAndDispatchQueues(true);\n        }\n        brokerService.setDeleteAllMessagesOnStartup(true);\n        brokerService.setUseJmx(false);\n\n\n        PolicyMap policyMap = new PolicyMap();\n        PolicyEntry defaultEntry = new PolicyEntry();\n        defaultEntry.setUseConsumerPriority(false); // java.lang.IllegalArgumentException: Comparison method violates its general contract!\n        defaultEntry.setMaxProducersToAudit(publisherThreadCount);\n        defaultEntry.setEnableAudit(true);\n        defaultEntry.setUseCache(useCache);\n        defaultEntry.setMaxPageSize(1000);\n        defaultEntry.setOptimizedDispatch(optimizeDispatch);\n        defaultEntry.setMemoryLimit(destMemoryLimit);\n        defaultEntry.setExpireMessagesPeriod(0);\n        policyMap.setDefaultEntry(defaultEntry);\n        brokerService.setDestinationPolicy(policyMap);\n\n        brokerService.getSystemUsage().getMemoryUsage().setLimit(512 * 1024 * 1024);\n\n        TransportConnector transportConnector = brokerService.addConnector(\"tcp://0.0.0.0:0\");\n        brokerService.start();\n        activemqURL = transportConnector.getPublishableConnectString();\n    }",
            "  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  \n 126 +\n 127  ",
            "    @Before\n    public void startBroker() throws Exception {\n        brokerService = new BrokerService();\n\n        TestSupport.setPersistenceAdapter(brokerService, persistenceAdapterChoice);\n        brokerService.setDeleteAllMessagesOnStartup(true);\n        brokerService.setUseJmx(false);\n        brokerService.setAdvisorySupport(false);\n\n\n        PolicyMap policyMap = new PolicyMap();\n        PolicyEntry defaultEntry = new PolicyEntry();\n        defaultEntry.setUseConsumerPriority(false); // java.lang.IllegalArgumentException: Comparison method violates its general contract!\n        defaultEntry.setMaxProducersToAudit(publisherThreadCount);\n        defaultEntry.setEnableAudit(true);\n        defaultEntry.setUseCache(useCache);\n        defaultEntry.setMaxPageSize(1000);\n        defaultEntry.setOptimizedDispatch(optimizeDispatch);\n        defaultEntry.setMemoryLimit(destMemoryLimit);\n        defaultEntry.setExpireMessagesPeriod(0);\n        policyMap.setDefaultEntry(defaultEntry);\n        brokerService.setDestinationPolicy(policyMap);\n\n        brokerService.getSystemUsage().getMemoryUsage().setLimit(64 * 1024 * 1024);\n\n        TransportConnector transportConnector = brokerService.addConnector(\"tcp://0.0.0.0:0\");\n        brokerService.start();\n        activemqURL = transportConnector.getPublishableConnectString();\n        activemqURL += \"?jms.watchTopicAdvisories=false\"; // ensure all messages are queue or dlq messages\n    }"
        ],
        [
            "AMQ5266SingleDestTest::stopBroker()",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148 -\n 149 -\n 150 -\n 151 -\n 152  ",
            "    @After\n    public void stopBroker() throws Exception {\n        if (brokerService != null) {\n            brokerService.stop();\n        }\n        try {\n            dataSource.setShutdownDatabase(\"shutdown\");\n            dataSource.getConnection();\n        } catch (Exception ignored) {}\n    }",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "    @After\n    public void stopBroker() throws Exception {\n        if (brokerService != null) {\n            brokerService.stop();\n        }\n    }"
        ],
        [
            "AMQ5266SingleDestTest::test()",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 -\n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "    @Test\n    public void test() throws Exception {\n\n        String activemqQueues = \"activemq\";\n        for (int i=1;i<numDests;i++) {\n            activemqQueues +=\",activemq\"+i;\n        }\n\n        int consumerWaitForConsumption = 5 * 60 * 1000;\n\n        ExportQueuePublisher publisher = null;\n        ExportQueueConsumer consumer = null;\n\n        LOG.info(\"Publisher will publish \" + (publisherMessagesPerThread * publisherThreadCount) + \" messages to each queue specified.\");\n        LOG.info(\"\\nBuilding Publisher...\");\n\n        publisher = new ExportQueuePublisher(activemqURL, activemqQueues, publisherMessagesPerThread, publisherThreadCount);\n\n        LOG.info(\"Building Consumer...\");\n\n        consumer = new ExportQueueConsumer(activemqURL, activemqQueues, consumerThreadsPerQueue, consumerBatchSize, publisherMessagesPerThread * publisherThreadCount);\n\n        long totalStart = System.currentTimeMillis();\n\n        LOG.info(\"Starting Publisher...\");\n\n        publisher.start();\n\n        LOG.info(\"Starting Consumer...\");\n\n        consumer.start();\n\n        int distinctPublishedCount = 0;\n\n\n        LOG.info(\"Waiting For Publisher Completion...\");\n\n        publisher.waitForCompletion();\n\n        List publishedIds = publisher.getIDs();\n        distinctPublishedCount = new TreeSet(publishedIds).size();\n\n        LOG.info(\"Publisher Complete. Published: \" + publishedIds.size() + \", Distinct IDs Published: \" + distinctPublishedCount);\n        LOG.info(\"Publisher duration: {}\", TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - totalStart));\n\n\n        long endWait = System.currentTimeMillis() + consumerWaitForConsumption;\n        while (!consumer.completed() && System.currentTimeMillis() < endWait) {\n            try {\n                int secs = (int) (endWait - System.currentTimeMillis()) / 1000;\n                LOG.info(\"Waiting For Consumer Completion. Time left: \" + secs + \" secs\");\n                if (!useDefaultStore) {\n                    DefaultJDBCAdapter.dumpTables(dataSource.getConnection());\n                }\n                Thread.sleep(1000);\n            } catch (Exception e) {\n            }\n        }\n\n        LOG.info(\"\\nConsumer Complete: \" + consumer.completed() +\", Shutting Down.\");\n\n        LOG.info(\"Total duration: {}\", TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - totalStart));\n\n        consumer.shutdown();\n\n        TimeUnit.SECONDS.sleep(2);\n        LOG.info(\"DB Contents START\");\n        if (!useDefaultStore) {\n            DefaultJDBCAdapter.dumpTables(dataSource.getConnection());\n        }\n        LOG.info(\"DB Contents END\");\n\n        LOG.info(\"Consumer Stats:\");\n\n        for (Map.Entry<String, List<String>> entry : consumer.getIDs().entrySet()) {\n\n            List<String> idList = entry.getValue();\n\n            int distinctConsumed = new TreeSet<String>(idList).size();\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(\"   Queue: \" + entry.getKey() +\n                    \" -> Total Messages Consumed: \" + idList.size() +\n                    \", Distinct IDs Consumed: \" + distinctConsumed);\n\n            int diff = distinctPublishedCount - distinctConsumed;\n            sb.append(\" ( \" + (diff > 0 ? diff : \"NO\") + \" STUCK MESSAGES \" + \" ) \");\n            LOG.info(sb.toString());\n\n            assertEquals(\"expect to get all messages!\", 0, diff);\n\n        }\n    }",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221 +\n 222 +\n 223  ",
            "    @Test\n    public void test() throws Exception {\n\n        String activemqQueues = \"activemq\";\n        for (int i=1;i<numDests;i++) {\n            activemqQueues +=\",activemq\"+i;\n        }\n\n        int consumerWaitForConsumption = 5 * 60 * 1000;\n\n        ExportQueuePublisher publisher = null;\n        ExportQueueConsumer consumer = null;\n\n        LOG.info(\"Publisher will publish \" + (publisherMessagesPerThread * publisherThreadCount) + \" messages to each queue specified.\");\n        LOG.info(\"\\nBuilding Publisher...\");\n\n        publisher = new ExportQueuePublisher(activemqURL, activemqQueues, publisherMessagesPerThread, publisherThreadCount);\n\n        LOG.info(\"Building Consumer...\");\n\n        consumer = new ExportQueueConsumer(activemqURL, activemqQueues, consumerThreadsPerQueue, consumerBatchSize, publisherMessagesPerThread * publisherThreadCount);\n\n        long totalStart = System.currentTimeMillis();\n\n        LOG.info(\"Starting Publisher...\");\n\n        publisher.start();\n\n        LOG.info(\"Starting Consumer...\");\n\n        consumer.start();\n\n        int distinctPublishedCount = 0;\n\n\n        LOG.info(\"Waiting For Publisher Completion...\");\n\n        publisher.waitForCompletion();\n\n        List publishedIds = publisher.getIDs();\n        distinctPublishedCount = new TreeSet(publishedIds).size();\n\n        LOG.info(\"Publisher Complete. Published: \" + publishedIds.size() + \", Distinct IDs Published: \" + distinctPublishedCount);\n        LOG.info(\"Publisher duration: {}\", TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - totalStart));\n\n\n        long endWait = System.currentTimeMillis() + consumerWaitForConsumption;\n        while (!consumer.completed() && System.currentTimeMillis() < endWait) {\n            try {\n                int secs = (int) (endWait - System.currentTimeMillis()) / 1000;\n                LOG.info(\"Waiting For Consumer Completion. Time left: \" + secs + \" secs\");\n                Thread.sleep(1000);\n            } catch (Exception e) {\n            }\n        }\n\n        LOG.info(\"\\nConsumer Complete: \" + consumer.completed() +\", Shutting Down.\");\n\n        LOG.info(\"Total duration: {}\", TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - totalStart));\n\n        consumer.shutdown();\n\n        TimeUnit.SECONDS.sleep(2);\n\n        LOG.info(\"Consumer Stats:\");\n\n        for (Map.Entry<String, List<String>> entry : consumer.getIDs().entrySet()) {\n\n            List<String> idList = entry.getValue();\n\n            int distinctConsumed = new TreeSet<String>(idList).size();\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(\"   Queue: \" + entry.getKey() +\n                    \" -> Total Messages Consumed: \" + idList.size() +\n                    \", Distinct IDs Consumed: \" + distinctConsumed);\n\n            int diff = distinctPublishedCount - distinctConsumed;\n            sb.append(\" ( \" + (diff > 0 ? diff : \"NO\") + \" STUCK MESSAGES \" + \" ) \");\n            LOG.info(sb.toString());\n\n            assertEquals(\"expect to get all messages!\", 0, diff);\n\n        }\n\n        // verify empty dlq\n        assertEquals(\"No pending messages\", 0l, ((RegionBroker) brokerService.getRegionBroker()).getDestinationStatistics().getMessages().getCount());\n    }"
        ]
    ],
    "281d600ae2f9ba6c6bc7bee0e8025698b9a76563": [
        [
            "MessageDatabase::loadPageFile()",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "    private void loadPageFile() throws IOException {\n        this.indexLock.writeLock().lock();\n        try {\n            final PageFile pageFile = getPageFile();\n            pageFile.load();\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    if (pageFile.getPageCount() == 0) {\n                        // First time this is created.. Initialize the metadata\n                        Page<Metadata> page = tx.allocate();\n                        assert page.getPageId() == 0;\n                        page.set(metadata);\n                        metadata.page = page;\n                        metadata.state = CLOSED_STATE;\n                        metadata.destinations = new BTreeIndex<String, StoredDestination>(pageFile, tx.allocate().getPageId());\n\n                        tx.store(metadata.page, metadataMarshaller, true);\n                    } else {\n                        Page<Metadata> page = tx.load(0, metadataMarshaller);\n                        metadata = page.get();\n                        metadata.page = page;\n                    }\n                    metadata.destinations.setKeyMarshaller(StringMarshaller.INSTANCE);\n                    metadata.destinations.setValueMarshaller(new StoredDestinationMarshaller());\n                    metadata.destinations.load(tx);\n                }\n            });\n            // Load up all the destinations since we need to scan all the indexes to figure out which journal files can be deleted.\n            // Perhaps we should just keep an index of file\n            storedDestinations.clear();\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    for (Iterator<Entry<String, StoredDestination>> iterator = metadata.destinations.iterator(tx); iterator.hasNext();) {\n                        Entry<String, StoredDestination> entry = iterator.next();\n                        StoredDestination sd = loadStoredDestination(tx, entry.getKey(), entry.getValue().subscriptions!=null);\n                        storedDestinations.put(entry.getKey(), sd);\n\n                        if (checkForCorruptJournalFiles) {\n                            // sanity check the index also\n                            if (!entry.getValue().locationIndex.isEmpty(tx)) {\n                                if (entry.getValue().orderIndex.nextMessageId <= 0) {\n                                    throw new IOException(\"Detected uninitialized orderIndex nextMessageId with pending messages for \" + entry.getKey());\n                                }\n                            }\n                        }\n                    }\n                }\n            });\n            pageFile.flush();\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "    private void loadPageFile() throws IOException {\n        this.indexLock.writeLock().lock();\n        try {\n            final PageFile pageFile = getPageFile();\n            pageFile.load();\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    if (pageFile.getPageCount() == 0) {\n                        // First time this is created.. Initialize the metadata\n                        Page<Metadata> page = tx.allocate();\n                        assert page.getPageId() == 0;\n                        page.set(metadata);\n                        metadata.page = page;\n                        metadata.state = CLOSED_STATE;\n                        metadata.destinations = new BTreeIndex<>(pageFile, tx.allocate().getPageId());\n\n                        tx.store(metadata.page, metadataMarshaller, true);\n                    } else {\n                        Page<Metadata> page = tx.load(0, metadataMarshaller);\n                        metadata = page.get();\n                        metadata.page = page;\n                    }\n                    metadata.destinations.setKeyMarshaller(StringMarshaller.INSTANCE);\n                    metadata.destinations.setValueMarshaller(new StoredDestinationMarshaller());\n                    metadata.destinations.load(tx);\n                }\n            });\n            // Load up all the destinations since we need to scan all the indexes to figure out which journal files can be deleted.\n            // Perhaps we should just keep an index of file\n            storedDestinations.clear();\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    for (Iterator<Entry<String, StoredDestination>> iterator = metadata.destinations.iterator(tx); iterator.hasNext();) {\n                        Entry<String, StoredDestination> entry = iterator.next();\n                        StoredDestination sd = loadStoredDestination(tx, entry.getKey(), entry.getValue().subscriptions!=null);\n                        storedDestinations.put(entry.getKey(), sd);\n\n                        if (checkForCorruptJournalFiles) {\n                            // sanity check the index also\n                            if (!entry.getValue().locationIndex.isEmpty(tx)) {\n                                if (entry.getValue().orderIndex.nextMessageId <= 0) {\n                                    throw new IOException(\"Detected uninitialized orderIndex nextMessageId with pending messages for \" + entry.getKey());\n                                }\n                            }\n                        }\n                    }\n                }\n            });\n            pageFile.flush();\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "MessageDatabase::getTransactions()",
            " 620  \n 621  \n 622  \n 623 -\n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  ",
            "    @SuppressWarnings(\"rawtypes\")\n    public String getTransactions() {\n\n        ArrayList<TranInfo> infos = new ArrayList<TranInfo>();\n        synchronized (inflightTransactions) {\n            if (!inflightTransactions.isEmpty()) {\n                for (Entry<TransactionId, List<Operation>> entry : inflightTransactions.entrySet()) {\n                    TranInfo info = new TranInfo();\n                    info.id = entry.getKey();\n                    for (Operation operation : entry.getValue()) {\n                        info.track(operation);\n                    }\n                    infos.add(info);\n                }\n            }\n        }\n        synchronized (preparedTransactions) {\n            if (!preparedTransactions.isEmpty()) {\n                for (Entry<TransactionId, List<Operation>> entry : preparedTransactions.entrySet()) {\n                    TranInfo info = new TranInfo();\n                    info.id = entry.getKey();\n                    for (Operation operation : entry.getValue()) {\n                        info.track(operation);\n                    }\n                    infos.add(info);\n                }\n            }\n        }\n        return infos.toString();\n    }",
            " 620  \n 621  \n 622  \n 623 +\n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  ",
            "    @SuppressWarnings(\"rawtypes\")\n    public String getTransactions() {\n\n        ArrayList<TranInfo> infos = new ArrayList<>();\n        synchronized (inflightTransactions) {\n            if (!inflightTransactions.isEmpty()) {\n                for (Entry<TransactionId, List<Operation>> entry : inflightTransactions.entrySet()) {\n                    TranInfo info = new TranInfo();\n                    info.id = entry.getKey();\n                    for (Operation operation : entry.getValue()) {\n                        info.track(operation);\n                    }\n                    infos.add(info);\n                }\n            }\n        }\n        synchronized (preparedTransactions) {\n            if (!preparedTransactions.isEmpty()) {\n                for (Entry<TransactionId, List<Operation>> entry : preparedTransactions.entrySet()) {\n                    TranInfo info = new TranInfo();\n                    info.id = entry.getKey();\n                    for (Operation operation : entry.getValue()) {\n                        info.track(operation);\n                    }\n                    infos.add(info);\n                }\n            }\n        }\n        return infos.toString();\n    }"
        ],
        [
            "MessageDatabase::process(KahaRewrittenDataFileCommand,Location)",
            "1415  \n1416 -\n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  ",
            "    protected void process(KahaRewrittenDataFileCommand command, Location location)  throws IOException {\n        final TreeSet<Integer> completeFileSet = new TreeSet<Integer>(journal.getFileMap().keySet());\n\n        // Mark the current journal file as a compacted file so that gc checks can skip\n        // over logs that are smaller compaction type logs.\n        DataFile current = journal.getDataFileById(location.getDataFileId());\n        current.setTypeCode(command.getRewriteType());\n\n        if (completeFileSet.contains(command.getSourceDataFileId()) && command.getSkipIfSourceExists()) {\n            // Move offset so that next location read jumps to next file.\n            location.setOffset(journalMaxFileLength);\n        }\n    }",
            "1415  \n1416 +\n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  ",
            "    protected void process(KahaRewrittenDataFileCommand command, Location location)  throws IOException {\n        final TreeSet<Integer> completeFileSet = new TreeSet<>(journal.getFileMap().keySet());\n\n        // Mark the current journal file as a compacted file so that gc checks can skip\n        // over logs that are smaller compaction type logs.\n        DataFile current = journal.getDataFileById(location.getDataFileId());\n        current.setTypeCode(command.getRewriteType());\n\n        if (completeFileSet.contains(command.getSourceDataFileId()) && command.getSkipIfSourceExists()) {\n            // Move offset so that next location read jumps to next file.\n            location.setOffset(journalMaxFileLength);\n        }\n    }"
        ],
        [
            "MessageDatabase::forwardAllAcks(Integer,Set)",
            "2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040 -\n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 -\n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  ",
            "    private void forwardAllAcks(Integer journalToRead, Set<Integer> journalLogsReferenced) throws IllegalStateException, IOException {\n        LOG.trace(\"Attempting to move all acks in journal:{} to the front.\", journalToRead);\n\n        DataFile forwardsFile = journal.reserveDataFile();\n        forwardsFile.setTypeCode(COMPACTED_JOURNAL_FILE);\n        LOG.trace(\"Reserved now file for forwarded acks: {}\", forwardsFile);\n\n        Map<Integer, Set<Integer>> updatedAckLocations = new HashMap<Integer, Set<Integer>>();\n\n        try (TargetedDataFileAppender appender = new TargetedDataFileAppender(journal, forwardsFile);) {\n            KahaRewrittenDataFileCommand compactionMarker = new KahaRewrittenDataFileCommand();\n            compactionMarker.setSourceDataFileId(journalToRead);\n            compactionMarker.setRewriteType(forwardsFile.getTypeCode());\n\n            ByteSequence payload = toByteSequence(compactionMarker);\n            appender.storeItem(payload, Journal.USER_RECORD_TYPE, false);\n            LOG.trace(\"Marked ack rewrites file as replacing file: {}\", journalToRead);\n\n            Location nextLocation = getNextLocationForAckForward(new Location(journalToRead, 0));\n            while (nextLocation != null && nextLocation.getDataFileId() == journalToRead) {\n                JournalCommand<?> command = null;\n                try {\n                    command = load(nextLocation);\n                } catch (IOException ex) {\n                    LOG.trace(\"Error loading command during ack forward: {}\", nextLocation);\n                }\n\n                if (command != null && command instanceof KahaRemoveMessageCommand) {\n                    payload = toByteSequence(command);\n                    Location location = appender.storeItem(payload, Journal.USER_RECORD_TYPE, false);\n                    updatedAckLocations.put(location.getDataFileId(), journalLogsReferenced);\n                }\n\n                nextLocation = getNextLocationForAckForward(nextLocation);\n            }\n        }\n\n        LOG.trace(\"ACKS forwarded, updates for ack locations: {}\", updatedAckLocations);\n\n        // Lock index while we update the ackMessageFileMap.\n        indexLock.writeLock().lock();\n\n        // Update the ack map with the new locations of the acks\n        for (Entry<Integer, Set<Integer>> entry : updatedAckLocations.entrySet()) {\n            Set<Integer> referenceFileIds = metadata.ackMessageFileMap.get(entry.getKey());\n            if (referenceFileIds == null) {\n                referenceFileIds = new HashSet<Integer>();\n                referenceFileIds.addAll(entry.getValue());\n                metadata.ackMessageFileMap.put(entry.getKey(), referenceFileIds);\n            } else {\n                referenceFileIds.addAll(entry.getValue());\n            }\n        }\n\n        // remove the old location data from the ack map so that the old journal log file can\n        // be removed on next GC.\n        metadata.ackMessageFileMap.remove(journalToRead);\n\n        indexLock.writeLock().unlock();\n\n        LOG.trace(\"ACK File Map following updates: {}\", metadata.ackMessageFileMap);\n    }",
            "2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040 +\n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 +\n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  ",
            "    private void forwardAllAcks(Integer journalToRead, Set<Integer> journalLogsReferenced) throws IllegalStateException, IOException {\n        LOG.trace(\"Attempting to move all acks in journal:{} to the front.\", journalToRead);\n\n        DataFile forwardsFile = journal.reserveDataFile();\n        forwardsFile.setTypeCode(COMPACTED_JOURNAL_FILE);\n        LOG.trace(\"Reserved now file for forwarded acks: {}\", forwardsFile);\n\n        Map<Integer, Set<Integer>> updatedAckLocations = new HashMap<>();\n\n        try (TargetedDataFileAppender appender = new TargetedDataFileAppender(journal, forwardsFile);) {\n            KahaRewrittenDataFileCommand compactionMarker = new KahaRewrittenDataFileCommand();\n            compactionMarker.setSourceDataFileId(journalToRead);\n            compactionMarker.setRewriteType(forwardsFile.getTypeCode());\n\n            ByteSequence payload = toByteSequence(compactionMarker);\n            appender.storeItem(payload, Journal.USER_RECORD_TYPE, false);\n            LOG.trace(\"Marked ack rewrites file as replacing file: {}\", journalToRead);\n\n            Location nextLocation = getNextLocationForAckForward(new Location(journalToRead, 0));\n            while (nextLocation != null && nextLocation.getDataFileId() == journalToRead) {\n                JournalCommand<?> command = null;\n                try {\n                    command = load(nextLocation);\n                } catch (IOException ex) {\n                    LOG.trace(\"Error loading command during ack forward: {}\", nextLocation);\n                }\n\n                if (command != null && command instanceof KahaRemoveMessageCommand) {\n                    payload = toByteSequence(command);\n                    Location location = appender.storeItem(payload, Journal.USER_RECORD_TYPE, false);\n                    updatedAckLocations.put(location.getDataFileId(), journalLogsReferenced);\n                }\n\n                nextLocation = getNextLocationForAckForward(nextLocation);\n            }\n        }\n\n        LOG.trace(\"ACKS forwarded, updates for ack locations: {}\", updatedAckLocations);\n\n        // Lock index while we update the ackMessageFileMap.\n        indexLock.writeLock().lock();\n\n        // Update the ack map with the new locations of the acks\n        for (Entry<Integer, Set<Integer>> entry : updatedAckLocations.entrySet()) {\n            Set<Integer> referenceFileIds = metadata.ackMessageFileMap.get(entry.getKey());\n            if (referenceFileIds == null) {\n                referenceFileIds = new HashSet<>();\n                referenceFileIds.addAll(entry.getValue());\n                metadata.ackMessageFileMap.put(entry.getKey(), referenceFileIds);\n            } else {\n                referenceFileIds.addAll(entry.getValue());\n            }\n        }\n\n        // remove the old location data from the ack map so that the old journal log file can\n        // be removed on next GC.\n        metadata.ackMessageFileMap.remove(journalToRead);\n\n        indexLock.writeLock().unlock();\n\n        LOG.trace(\"ACK File Map following updates: {}\", metadata.ackMessageFileMap);\n    }"
        ],
        [
            "MessageDatabase::MessageOrderIndex::configureLast(Transaction)",
            "3523  \n3524  \n3525 -\n3526  \n3527  \n3528  \n3529  \n3530  \n3531  \n3532  \n3533  \n3534  ",
            "        void configureLast(Transaction tx) throws IOException {\n            // Figure out the next key using the last entry in the destination.\n            TreeSet<Long> orderedSet = new TreeSet<Long>();\n\n            addLast(orderedSet, highPriorityIndex, tx);\n            addLast(orderedSet, defaultPriorityIndex, tx);\n            addLast(orderedSet, lowPriorityIndex, tx);\n\n            if (!orderedSet.isEmpty()) {\n                nextMessageId = orderedSet.last() + 1;\n            }\n        }",
            "3525  \n3526  \n3527 +\n3528  \n3529  \n3530  \n3531  \n3532  \n3533  \n3534  \n3535  \n3536  ",
            "        void configureLast(Transaction tx) throws IOException {\n            // Figure out the next key using the last entry in the destination.\n            TreeSet<Long> orderedSet = new TreeSet<>();\n\n            addLast(orderedSet, highPriorityIndex, tx);\n            addLast(orderedSet, defaultPriorityIndex, tx);\n            addLast(orderedSet, lowPriorityIndex, tx);\n\n            if (!orderedSet.isEmpty()) {\n                nextMessageId = orderedSet.last() + 1;\n            }\n        }"
        ],
        [
            "MessageDatabase::removeAckLocationsForSub(KahaSubscriptionCommand,Transaction,StoredDestination,String)",
            "2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874 -\n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892 -\n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899  \n2900  \n2901  \n2902  \n2903  \n2904  ",
            "    private void removeAckLocationsForSub(KahaSubscriptionCommand command,\n            Transaction tx, StoredDestination sd, String subscriptionKey) throws IOException {\n        if (!sd.ackPositions.isEmpty(tx)) {\n            SequenceSet sequences = sd.ackPositions.remove(tx, subscriptionKey);\n            if (sequences == null || sequences.isEmpty()) {\n                return;\n            }\n\n            ArrayList<Long> unreferenced = new ArrayList<Long>();\n\n            for(Long sequenceId : sequences) {\n                Long references = sd.messageReferences.get(sequenceId);\n                if (references != null) {\n                    references = references.longValue() - 1;\n\n                    if (references.longValue() > 0) {\n                        sd.messageReferences.put(sequenceId, references);\n                    } else {\n                        sd.messageReferences.remove(sequenceId);\n                        unreferenced.add(sequenceId);\n                    }\n                }\n            }\n\n            for(Long sequenceId : unreferenced) {\n                // Find all the entries that need to get deleted.\n                ArrayList<Entry<Long, MessageKeys>> deletes = new ArrayList<Entry<Long, MessageKeys>>();\n                sd.orderIndex.getDeleteList(tx, deletes, sequenceId);\n\n                // Do the actual deletes.\n                for (Entry<Long, MessageKeys> entry : deletes) {\n                    sd.locationIndex.remove(tx, entry.getValue().location);\n                    sd.messageIdIndex.remove(tx, entry.getValue().messageId);\n                    sd.orderIndex.remove(tx, entry.getKey());\n                    decrementAndSubSizeToStoreStat(command.getDestination(), entry.getValue().location.getSize());\n                }\n            }\n        }\n    }",
            "2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874 +\n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892 +\n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899  \n2900  \n2901  \n2902  \n2903  \n2904  ",
            "    private void removeAckLocationsForSub(KahaSubscriptionCommand command,\n            Transaction tx, StoredDestination sd, String subscriptionKey) throws IOException {\n        if (!sd.ackPositions.isEmpty(tx)) {\n            SequenceSet sequences = sd.ackPositions.remove(tx, subscriptionKey);\n            if (sequences == null || sequences.isEmpty()) {\n                return;\n            }\n\n            ArrayList<Long> unreferenced = new ArrayList<>();\n\n            for(Long sequenceId : sequences) {\n                Long references = sd.messageReferences.get(sequenceId);\n                if (references != null) {\n                    references = references.longValue() - 1;\n\n                    if (references.longValue() > 0) {\n                        sd.messageReferences.put(sequenceId, references);\n                    } else {\n                        sd.messageReferences.remove(sequenceId);\n                        unreferenced.add(sequenceId);\n                    }\n                }\n            }\n\n            for(Long sequenceId : unreferenced) {\n                // Find all the entries that need to get deleted.\n                ArrayList<Entry<Long, MessageKeys>> deletes = new ArrayList<>();\n                sd.orderIndex.getDeleteList(tx, deletes, sequenceId);\n\n                // Do the actual deletes.\n                for (Entry<Long, MessageKeys> entry : deletes) {\n                    sd.locationIndex.remove(tx, entry.getValue().location);\n                    sd.messageIdIndex.remove(tx, entry.getValue().messageId);\n                    sd.orderIndex.remove(tx, entry.getKey());\n                    decrementAndSubSizeToStoreStat(command.getDestination(), entry.getValue().location.getSize());\n                }\n            }\n        }\n    }"
        ],
        [
            "MessageDatabase::Metadata::read(DataInput)",
            " 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "        public void read(DataInput is) throws IOException {\n            state = is.readInt();\n            destinations = new BTreeIndex<String, StoredDestination>(pageFile, is.readLong());\n            if (is.readBoolean()) {\n                lastUpdate = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                lastUpdate = null;\n            }\n            if (is.readBoolean()) {\n                firstInProgressTransactionLocation = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                firstInProgressTransactionLocation = null;\n            }\n            try {\n                if (is.readBoolean()) {\n                    producerSequenceIdTrackerLocation = LocationMarshaller.INSTANCE.readPayload(is);\n                } else {\n                    producerSequenceIdTrackerLocation = null;\n                }\n            } catch (EOFException expectedOnUpgrade) {\n            }\n            try {\n                version = is.readInt();\n            } catch (EOFException expectedOnUpgrade) {\n                version = 1;\n            }\n            if (version >= 5 && is.readBoolean()) {\n                ackMessageFileMapLocation = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                ackMessageFileMapLocation = null;\n            }\n            try {\n                openwireVersion = is.readInt();\n            } catch (EOFException expectedOnUpgrade) {\n                openwireVersion = OpenWireFormat.DEFAULT_LEGACY_VERSION;\n            }\n            LOG.info(\"KahaDB is version \" + version);\n        }",
            " 155  \n 156  \n 157 +\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "        public void read(DataInput is) throws IOException {\n            state = is.readInt();\n            destinations = new BTreeIndex<>(pageFile, is.readLong());\n            if (is.readBoolean()) {\n                lastUpdate = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                lastUpdate = null;\n            }\n            if (is.readBoolean()) {\n                firstInProgressTransactionLocation = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                firstInProgressTransactionLocation = null;\n            }\n            try {\n                if (is.readBoolean()) {\n                    producerSequenceIdTrackerLocation = LocationMarshaller.INSTANCE.readPayload(is);\n                } else {\n                    producerSequenceIdTrackerLocation = null;\n                }\n            } catch (EOFException expectedOnUpgrade) {\n            }\n            try {\n                version = is.readInt();\n            } catch (EOFException expectedOnUpgrade) {\n                version = 1;\n            }\n            if (version >= 5 && is.readBoolean()) {\n                ackMessageFileMapLocation = LocationMarshaller.INSTANCE.readPayload(is);\n            } else {\n                ackMessageFileMapLocation = null;\n            }\n            try {\n                openwireVersion = is.readInt();\n            } catch (EOFException expectedOnUpgrade) {\n                openwireVersion = OpenWireFormat.DEFAULT_LEGACY_VERSION;\n            }\n            LOG.info(\"KahaDB is version \" + version);\n        }"
        ],
        [
            "MessageDatabase::checkpointUpdate(Transaction,boolean)",
            "1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725 -\n1726 -\n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  ",
            "    /**\n     * @param tx\n     * @throws IOException\n     */\n    void checkpointUpdate(Transaction tx, boolean cleanup) throws IOException {\n        MDC.put(\"activemq.persistenceDir\", getDirectory().getName());\n        LOG.debug(\"Checkpoint started.\");\n\n        // reflect last update exclusive of current checkpoint\n        Location lastUpdate = metadata.lastUpdate;\n\n        metadata.state = OPEN_STATE;\n        metadata.producerSequenceIdTrackerLocation = checkpointProducerAudit();\n        metadata.ackMessageFileMapLocation = checkpointAckMessageFileMap();\n        Location[] inProgressTxRange = getInProgressTxLocationRange();\n        metadata.firstInProgressTransactionLocation = inProgressTxRange[0];\n        tx.store(metadata.page, metadataMarshaller, true);\n        pageFile.flush();\n\n        if (cleanup) {\n\n            final TreeSet<Integer> completeFileSet = new TreeSet<Integer>(journal.getFileMap().keySet());\n            final TreeSet<Integer> gcCandidateSet = new TreeSet<Integer>(completeFileSet);\n\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Last update: \" + lastUpdate + \", full gc candidates set: \" + gcCandidateSet);\n            }\n\n            if (lastUpdate != null) {\n                gcCandidateSet.remove(lastUpdate.getDataFileId());\n            }\n\n            // Don't GC files under replication\n            if( journalFilesBeingReplicated!=null ) {\n                gcCandidateSet.removeAll(journalFilesBeingReplicated);\n            }\n\n            if (metadata.producerSequenceIdTrackerLocation != null) {\n                int dataFileId = metadata.producerSequenceIdTrackerLocation.getDataFileId();\n                if (gcCandidateSet.contains(dataFileId) && gcCandidateSet.first() == dataFileId) {\n                    // rewrite so we don't prevent gc\n                    metadata.producerSequenceIdTracker.setModified(true);\n                    if (LOG.isTraceEnabled()) {\n                        LOG.trace(\"rewriting producerSequenceIdTracker:\" + metadata.producerSequenceIdTrackerLocation);\n                    }\n                }\n                gcCandidateSet.remove(dataFileId);\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after producerSequenceIdTrackerLocation:\" + dataFileId + \", \" + gcCandidateSet);\n                }\n            }\n\n            if (metadata.ackMessageFileMapLocation != null) {\n                int dataFileId = metadata.ackMessageFileMapLocation.getDataFileId();\n                gcCandidateSet.remove(dataFileId);\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after ackMessageFileMapLocation:\" + dataFileId + \", \" + gcCandidateSet);\n                }\n            }\n\n            // Don't GC files referenced by in-progress tx\n            if (inProgressTxRange[0] != null) {\n                for (int pendingTx=inProgressTxRange[0].getDataFileId(); pendingTx <= inProgressTxRange[1].getDataFileId(); pendingTx++) {\n                    gcCandidateSet.remove(pendingTx);\n                }\n            }\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"gc candidates after tx range:\" + Arrays.asList(inProgressTxRange) + \", \" + gcCandidateSet);\n            }\n\n            // Go through all the destinations to see if any of them can remove GC candidates.\n            for (Entry<String, StoredDestination> entry : storedDestinations.entrySet()) {\n                if( gcCandidateSet.isEmpty() ) {\n                    break;\n                }\n\n                // Use a visitor to cut down the number of pages that we load\n                entry.getValue().locationIndex.visit(tx, new BTreeVisitor<Location, Long>() {\n                    int last=-1;\n                    @Override\n                    public boolean isInterestedInKeysBetween(Location first, Location second) {\n                        if( first==null ) {\n                            SortedSet<Integer> subset = gcCandidateSet.headSet(second.getDataFileId()+1);\n                            if( !subset.isEmpty() && subset.last() == second.getDataFileId() ) {\n                                subset.remove(second.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        } else if( second==null ) {\n                            SortedSet<Integer> subset = gcCandidateSet.tailSet(first.getDataFileId());\n                            if( !subset.isEmpty() && subset.first() == first.getDataFileId() ) {\n                                subset.remove(first.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        } else {\n                            SortedSet<Integer> subset = gcCandidateSet.subSet(first.getDataFileId(), second.getDataFileId()+1);\n                            if( !subset.isEmpty() && subset.first() == first.getDataFileId() ) {\n                                subset.remove(first.getDataFileId());\n                            }\n                            if( !subset.isEmpty() && subset.last() == second.getDataFileId() ) {\n                                subset.remove(second.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        }\n                    }\n\n                    @Override\n                    public void visit(List<Location> keys, List<Long> values) {\n                        for (Location l : keys) {\n                            int fileId = l.getDataFileId();\n                            if( last != fileId ) {\n                                gcCandidateSet.remove(fileId);\n                                last = fileId;\n                            }\n                        }\n                    }\n                });\n\n                // Durable Subscription\n                if (entry.getValue().subLocations != null) {\n                    Iterator<Entry<String, Location>> iter = entry.getValue().subLocations.iterator(tx);\n                    while (iter.hasNext()) {\n                        Entry<String, Location> subscription = iter.next();\n                        int dataFileId = subscription.getValue().getDataFileId();\n\n                        // Move subscription along if it has no outstanding messages that need ack'd\n                        // and its in the last log file in the journal.\n                        if (!gcCandidateSet.isEmpty() && gcCandidateSet.first() == dataFileId) {\n                            final StoredDestination destination = entry.getValue();\n                            final String subscriptionKey = subscription.getKey();\n                            SequenceSet pendingAcks = destination.ackPositions.get(tx, subscriptionKey);\n\n                            // When pending is size one that is the next message Id meaning there\n                            // are no pending messages currently.\n                            if (pendingAcks == null || pendingAcks.isEmpty() ||\n                                (pendingAcks.size() == 1 && pendingAcks.getTail().range() == 1)) {\n\n                                if (LOG.isTraceEnabled()) {\n                                    LOG.trace(\"Found candidate for rewrite: {} from file {}\", entry.getKey(), dataFileId);\n                                }\n\n                                final KahaSubscriptionCommand kahaSub =\n                                    destination.subscriptions.get(tx, subscriptionKey);\n                                destination.subLocations.put(\n                                    tx, subscriptionKey, checkpointSubscriptionCommand(kahaSub));\n\n                                // Skips the remove from candidates if we rewrote the subscription\n                                // in order to prevent duplicate subscription commands on recover.\n                                // If another subscription is on the same file and isn't rewritten\n                                // than it will remove the file from the set.\n                                continue;\n                            }\n                        }\n\n                        gcCandidateSet.remove(dataFileId);\n                    }\n                }\n\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after dest:\" + entry.getKey() + \", \" + gcCandidateSet);\n                }\n            }\n\n            // check we are not deleting file with ack for in-use journal files\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"gc candidates: \" + gcCandidateSet);\n                LOG.trace(\"ackMessageFileMap: \" +  metadata.ackMessageFileMap);\n            }\n\n            boolean ackMessageFileMapMod = false;\n            Iterator<Integer> candidates = gcCandidateSet.iterator();\n            while (candidates.hasNext()) {\n                Integer candidate = candidates.next();\n                Set<Integer> referencedFileIds = metadata.ackMessageFileMap.get(candidate);\n                if (referencedFileIds != null) {\n                    for (Integer referencedFileId : referencedFileIds) {\n                        if (completeFileSet.contains(referencedFileId) && !gcCandidateSet.contains(referencedFileId)) {\n                            // active file that is not targeted for deletion is referenced so don't delete\n                            candidates.remove();\n                            break;\n                        }\n                    }\n                    if (gcCandidateSet.contains(candidate)) {\n                        ackMessageFileMapMod |= (metadata.ackMessageFileMap.remove(candidate) != null);\n                    } else {\n                        if (LOG.isTraceEnabled()) {\n                            LOG.trace(\"not removing data file: \" + candidate\n                                    + \" as contained ack(s) refer to referenced file: \" + referencedFileIds);\n                        }\n                    }\n                }\n            }\n\n            if (!gcCandidateSet.isEmpty()) {\n                LOG.debug(\"Cleanup removing the data files: {}\", gcCandidateSet);\n                journal.removeDataFiles(gcCandidateSet);\n                for (Integer candidate : gcCandidateSet) {\n                    for (Set<Integer> ackFiles : metadata.ackMessageFileMap.values()) {\n                        ackMessageFileMapMod |= ackFiles.remove(candidate);\n                    }\n                }\n                if (ackMessageFileMapMod) {\n                    checkpointUpdate(tx, false);\n                }\n            } else if (isEnableAckCompaction()) {\n                if (++checkPointCyclesWithNoGC >= getCompactAcksAfterNoGC()) {\n                    // First check length of journal to make sure it makes sense to even try.\n                    //\n                    // If there is only one journal file with Acks in it we don't need to move\n                    // it since it won't be chained to any later logs.\n                    //\n                    // If the logs haven't grown since the last time then we need to compact\n                    // otherwise there seems to still be room for growth and we don't need to incur\n                    // the overhead.  Depending on configuration this check can be avoided and\n                    // Ack compaction will run any time the store has not GC'd a journal file in\n                    // the configured amount of cycles.\n                    if (metadata.ackMessageFileMap.size() > 1 &&\n                        (journalLogOnLastCompactionCheck == journal.getCurrentDataFileId() || isCompactAcksIgnoresStoreGrowth())) {\n\n                        LOG.trace(\"No files GC'd checking if threshold to ACK compaction has been met.\");\n                        try {\n                            scheduler.execute(new AckCompactionRunner());\n                        } catch (Exception ex) {\n                            LOG.warn(\"Error on queueing the Ack Compactor\", ex);\n                        }\n                    } else {\n                        LOG.trace(\"Journal activity detected, no Ack compaction scheduled.\");\n                    }\n\n                    checkPointCyclesWithNoGC = 0;\n                } else {\n                    LOG.trace(\"Not yet time to check for compaction: {} of {} cycles\",\n                              checkPointCyclesWithNoGC, getCompactAcksAfterNoGC());\n                }\n\n                journalLogOnLastCompactionCheck = journal.getCurrentDataFileId();\n            }\n        }\n        MDC.remove(\"activemq.persistenceDir\");\n\n        LOG.debug(\"Checkpoint done.\");\n    }",
            "1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725 +\n1726 +\n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  ",
            "    /**\n     * @param tx\n     * @throws IOException\n     */\n    void checkpointUpdate(Transaction tx, boolean cleanup) throws IOException {\n        MDC.put(\"activemq.persistenceDir\", getDirectory().getName());\n        LOG.debug(\"Checkpoint started.\");\n\n        // reflect last update exclusive of current checkpoint\n        Location lastUpdate = metadata.lastUpdate;\n\n        metadata.state = OPEN_STATE;\n        metadata.producerSequenceIdTrackerLocation = checkpointProducerAudit();\n        metadata.ackMessageFileMapLocation = checkpointAckMessageFileMap();\n        Location[] inProgressTxRange = getInProgressTxLocationRange();\n        metadata.firstInProgressTransactionLocation = inProgressTxRange[0];\n        tx.store(metadata.page, metadataMarshaller, true);\n        pageFile.flush();\n\n        if (cleanup) {\n\n            final TreeSet<Integer> completeFileSet = new TreeSet<>(journal.getFileMap().keySet());\n            final TreeSet<Integer> gcCandidateSet = new TreeSet<>(completeFileSet);\n\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Last update: \" + lastUpdate + \", full gc candidates set: \" + gcCandidateSet);\n            }\n\n            if (lastUpdate != null) {\n                gcCandidateSet.remove(lastUpdate.getDataFileId());\n            }\n\n            // Don't GC files under replication\n            if( journalFilesBeingReplicated!=null ) {\n                gcCandidateSet.removeAll(journalFilesBeingReplicated);\n            }\n\n            if (metadata.producerSequenceIdTrackerLocation != null) {\n                int dataFileId = metadata.producerSequenceIdTrackerLocation.getDataFileId();\n                if (gcCandidateSet.contains(dataFileId) && gcCandidateSet.first() == dataFileId) {\n                    // rewrite so we don't prevent gc\n                    metadata.producerSequenceIdTracker.setModified(true);\n                    if (LOG.isTraceEnabled()) {\n                        LOG.trace(\"rewriting producerSequenceIdTracker:\" + metadata.producerSequenceIdTrackerLocation);\n                    }\n                }\n                gcCandidateSet.remove(dataFileId);\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after producerSequenceIdTrackerLocation:\" + dataFileId + \", \" + gcCandidateSet);\n                }\n            }\n\n            if (metadata.ackMessageFileMapLocation != null) {\n                int dataFileId = metadata.ackMessageFileMapLocation.getDataFileId();\n                gcCandidateSet.remove(dataFileId);\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after ackMessageFileMapLocation:\" + dataFileId + \", \" + gcCandidateSet);\n                }\n            }\n\n            // Don't GC files referenced by in-progress tx\n            if (inProgressTxRange[0] != null) {\n                for (int pendingTx=inProgressTxRange[0].getDataFileId(); pendingTx <= inProgressTxRange[1].getDataFileId(); pendingTx++) {\n                    gcCandidateSet.remove(pendingTx);\n                }\n            }\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"gc candidates after tx range:\" + Arrays.asList(inProgressTxRange) + \", \" + gcCandidateSet);\n            }\n\n            // Go through all the destinations to see if any of them can remove GC candidates.\n            for (Entry<String, StoredDestination> entry : storedDestinations.entrySet()) {\n                if( gcCandidateSet.isEmpty() ) {\n                    break;\n                }\n\n                // Use a visitor to cut down the number of pages that we load\n                entry.getValue().locationIndex.visit(tx, new BTreeVisitor<Location, Long>() {\n                    int last=-1;\n                    @Override\n                    public boolean isInterestedInKeysBetween(Location first, Location second) {\n                        if( first==null ) {\n                            SortedSet<Integer> subset = gcCandidateSet.headSet(second.getDataFileId()+1);\n                            if( !subset.isEmpty() && subset.last() == second.getDataFileId() ) {\n                                subset.remove(second.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        } else if( second==null ) {\n                            SortedSet<Integer> subset = gcCandidateSet.tailSet(first.getDataFileId());\n                            if( !subset.isEmpty() && subset.first() == first.getDataFileId() ) {\n                                subset.remove(first.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        } else {\n                            SortedSet<Integer> subset = gcCandidateSet.subSet(first.getDataFileId(), second.getDataFileId()+1);\n                            if( !subset.isEmpty() && subset.first() == first.getDataFileId() ) {\n                                subset.remove(first.getDataFileId());\n                            }\n                            if( !subset.isEmpty() && subset.last() == second.getDataFileId() ) {\n                                subset.remove(second.getDataFileId());\n                            }\n                            return !subset.isEmpty();\n                        }\n                    }\n\n                    @Override\n                    public void visit(List<Location> keys, List<Long> values) {\n                        for (Location l : keys) {\n                            int fileId = l.getDataFileId();\n                            if( last != fileId ) {\n                                gcCandidateSet.remove(fileId);\n                                last = fileId;\n                            }\n                        }\n                    }\n                });\n\n                // Durable Subscription\n                if (entry.getValue().subLocations != null) {\n                    Iterator<Entry<String, Location>> iter = entry.getValue().subLocations.iterator(tx);\n                    while (iter.hasNext()) {\n                        Entry<String, Location> subscription = iter.next();\n                        int dataFileId = subscription.getValue().getDataFileId();\n\n                        // Move subscription along if it has no outstanding messages that need ack'd\n                        // and its in the last log file in the journal.\n                        if (!gcCandidateSet.isEmpty() && gcCandidateSet.first() == dataFileId) {\n                            final StoredDestination destination = entry.getValue();\n                            final String subscriptionKey = subscription.getKey();\n                            SequenceSet pendingAcks = destination.ackPositions.get(tx, subscriptionKey);\n\n                            // When pending is size one that is the next message Id meaning there\n                            // are no pending messages currently.\n                            if (pendingAcks == null || pendingAcks.isEmpty() ||\n                                (pendingAcks.size() == 1 && pendingAcks.getTail().range() == 1)) {\n\n                                if (LOG.isTraceEnabled()) {\n                                    LOG.trace(\"Found candidate for rewrite: {} from file {}\", entry.getKey(), dataFileId);\n                                }\n\n                                final KahaSubscriptionCommand kahaSub =\n                                    destination.subscriptions.get(tx, subscriptionKey);\n                                destination.subLocations.put(\n                                    tx, subscriptionKey, checkpointSubscriptionCommand(kahaSub));\n\n                                // Skips the remove from candidates if we rewrote the subscription\n                                // in order to prevent duplicate subscription commands on recover.\n                                // If another subscription is on the same file and isn't rewritten\n                                // than it will remove the file from the set.\n                                continue;\n                            }\n                        }\n\n                        gcCandidateSet.remove(dataFileId);\n                    }\n                }\n\n                if (LOG.isTraceEnabled()) {\n                    LOG.trace(\"gc candidates after dest:\" + entry.getKey() + \", \" + gcCandidateSet);\n                }\n            }\n\n            // check we are not deleting file with ack for in-use journal files\n            if (LOG.isTraceEnabled()) {\n                LOG.trace(\"gc candidates: \" + gcCandidateSet);\n                LOG.trace(\"ackMessageFileMap: \" +  metadata.ackMessageFileMap);\n            }\n\n            boolean ackMessageFileMapMod = false;\n            Iterator<Integer> candidates = gcCandidateSet.iterator();\n            while (candidates.hasNext()) {\n                Integer candidate = candidates.next();\n                Set<Integer> referencedFileIds = metadata.ackMessageFileMap.get(candidate);\n                if (referencedFileIds != null) {\n                    for (Integer referencedFileId : referencedFileIds) {\n                        if (completeFileSet.contains(referencedFileId) && !gcCandidateSet.contains(referencedFileId)) {\n                            // active file that is not targeted for deletion is referenced so don't delete\n                            candidates.remove();\n                            break;\n                        }\n                    }\n                    if (gcCandidateSet.contains(candidate)) {\n                        ackMessageFileMapMod |= (metadata.ackMessageFileMap.remove(candidate) != null);\n                    } else {\n                        if (LOG.isTraceEnabled()) {\n                            LOG.trace(\"not removing data file: \" + candidate\n                                    + \" as contained ack(s) refer to referenced file: \" + referencedFileIds);\n                        }\n                    }\n                }\n            }\n\n            if (!gcCandidateSet.isEmpty()) {\n                LOG.debug(\"Cleanup removing the data files: {}\", gcCandidateSet);\n                journal.removeDataFiles(gcCandidateSet);\n                for (Integer candidate : gcCandidateSet) {\n                    for (Set<Integer> ackFiles : metadata.ackMessageFileMap.values()) {\n                        ackMessageFileMapMod |= ackFiles.remove(candidate);\n                    }\n                }\n                if (ackMessageFileMapMod) {\n                    checkpointUpdate(tx, false);\n                }\n            } else if (isEnableAckCompaction()) {\n                if (++checkPointCyclesWithNoGC >= getCompactAcksAfterNoGC()) {\n                    // First check length of journal to make sure it makes sense to even try.\n                    //\n                    // If there is only one journal file with Acks in it we don't need to move\n                    // it since it won't be chained to any later logs.\n                    //\n                    // If the logs haven't grown since the last time then we need to compact\n                    // otherwise there seems to still be room for growth and we don't need to incur\n                    // the overhead.  Depending on configuration this check can be avoided and\n                    // Ack compaction will run any time the store has not GC'd a journal file in\n                    // the configured amount of cycles.\n                    if (metadata.ackMessageFileMap.size() > 1 &&\n                        (journalLogOnLastCompactionCheck == journal.getCurrentDataFileId() || isCompactAcksIgnoresStoreGrowth())) {\n\n                        LOG.trace(\"No files GC'd checking if threshold to ACK compaction has been met.\");\n                        try {\n                            scheduler.execute(new AckCompactionRunner());\n                        } catch (Exception ex) {\n                            LOG.warn(\"Error on queueing the Ack Compactor\", ex);\n                        }\n                    } else {\n                        LOG.trace(\"Journal activity detected, no Ack compaction scheduled.\");\n                    }\n\n                    checkPointCyclesWithNoGC = 0;\n                } else {\n                    LOG.trace(\"Not yet time to check for compaction: {} of {} cycles\",\n                              checkPointCyclesWithNoGC, getCompactAcksAfterNoGC());\n                }\n\n                journalLogOnLastCompactionCheck = journal.getCurrentDataFileId();\n            }\n        }\n        MDC.remove(\"activemq.persistenceDir\");\n\n        LOG.debug(\"Checkpoint done.\");\n    }"
        ],
        [
            "MessageDatabase::StoredDestinationMarshaller::readPayload(DataInput)",
            "2304  \n2305  \n2306  \n2307 -\n2308 -\n2309 -\n2310  \n2311  \n2312 -\n2313 -\n2314  \n2315 -\n2316  \n2317  \n2318  \n2319  \n2320  \n2321 -\n2322  \n2323  \n2324  \n2325  \n2326 -\n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351 -\n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364 -\n2365  \n2366  \n2367  \n2368  \n2369  \n2370 -\n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379 -\n2380 -\n2381  \n2382  \n2383  \n2384  \n2385  \n2386 -\n2387  \n2388  \n2389  \n2390  \n2391 -\n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  ",
            "        @Override\n        public StoredDestination readPayload(final DataInput dataIn) throws IOException {\n            final StoredDestination value = new StoredDestination();\n            value.orderIndex.defaultPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, dataIn.readLong());\n            value.locationIndex = new BTreeIndex<Location, Long>(pageFile, dataIn.readLong());\n            value.messageIdIndex = new BTreeIndex<String, Long>(pageFile, dataIn.readLong());\n\n            if (dataIn.readBoolean()) {\n                value.subscriptions = new BTreeIndex<String, KahaSubscriptionCommand>(pageFile, dataIn.readLong());\n                value.subscriptionAcks = new BTreeIndex<String, LastAck>(pageFile, dataIn.readLong());\n                if (metadata.version >= 4) {\n                    value.ackPositions = new ListIndex<String, SequenceSet>(pageFile, dataIn.readLong());\n                } else {\n                    // upgrade\n                    pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                        @Override\n                        public void execute(Transaction tx) throws IOException {\n                            LinkedHashMap<String, SequenceSet> temp = new LinkedHashMap<String, SequenceSet>();\n\n                            if (metadata.version >= 3) {\n                                // migrate\n                                BTreeIndex<Long, HashSet<String>> oldAckPositions =\n                                        new BTreeIndex<Long, HashSet<String>>(pageFile, dataIn.readLong());\n                                oldAckPositions.setKeyMarshaller(LongMarshaller.INSTANCE);\n                                oldAckPositions.setValueMarshaller(HashSetStringMarshaller.INSTANCE);\n                                oldAckPositions.load(tx);\n\n\n                                // Do the initial build of the data in memory before writing into the store\n                                // based Ack Positions List to avoid a lot of disk thrashing.\n                                Iterator<Entry<Long, HashSet<String>>> iterator = oldAckPositions.iterator(tx);\n                                while (iterator.hasNext()) {\n                                    Entry<Long, HashSet<String>> entry = iterator.next();\n\n                                    for(String subKey : entry.getValue()) {\n                                        SequenceSet pendingAcks = temp.get(subKey);\n                                        if (pendingAcks == null) {\n                                            pendingAcks = new SequenceSet();\n                                            temp.put(subKey, pendingAcks);\n                                        }\n\n                                        pendingAcks.add(entry.getKey());\n                                    }\n                                }\n                            }\n                            // Now move the pending messages to ack data into the store backed\n                            // structure.\n                            value.ackPositions = new ListIndex<String, SequenceSet>(pageFile, tx.allocate());\n                            value.ackPositions.setKeyMarshaller(StringMarshaller.INSTANCE);\n                            value.ackPositions.setValueMarshaller(SequenceSet.Marshaller.INSTANCE);\n                            value.ackPositions.load(tx);\n                            for(String subscriptionKey : temp.keySet()) {\n                                value.ackPositions.put(tx, subscriptionKey, temp.get(subscriptionKey));\n                            }\n\n                        }\n                    });\n                }\n\n                if (metadata.version >= 5) {\n                    value.subLocations = new ListIndex<String, Location>(pageFile, dataIn.readLong());\n                } else {\n                    // upgrade\n                    pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                        @Override\n                        public void execute(Transaction tx) throws IOException {\n                            value.subLocations = new ListIndex<String, Location>(pageFile, tx.allocate());\n                            value.subLocations.setKeyMarshaller(StringMarshaller.INSTANCE);\n                            value.subLocations.setValueMarshaller(LocationMarshaller.INSTANCE);\n                            value.subLocations.load(tx);\n                        }\n                    });\n                }\n            }\n            if (metadata.version >= 2) {\n                value.orderIndex.lowPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, dataIn.readLong());\n                value.orderIndex.highPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, dataIn.readLong());\n            } else {\n                // upgrade\n                pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                    @Override\n                    public void execute(Transaction tx) throws IOException {\n                        value.orderIndex.lowPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, tx.allocate());\n                        value.orderIndex.lowPriorityIndex.setKeyMarshaller(LongMarshaller.INSTANCE);\n                        value.orderIndex.lowPriorityIndex.setValueMarshaller(messageKeysMarshaller);\n                        value.orderIndex.lowPriorityIndex.load(tx);\n\n                        value.orderIndex.highPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, tx.allocate());\n                        value.orderIndex.highPriorityIndex.setKeyMarshaller(LongMarshaller.INSTANCE);\n                        value.orderIndex.highPriorityIndex.setValueMarshaller(messageKeysMarshaller);\n                        value.orderIndex.highPriorityIndex.load(tx);\n                    }\n                });\n            }\n\n            return value;\n        }",
            "2304  \n2305  \n2306  \n2307 +\n2308 +\n2309 +\n2310  \n2311  \n2312 +\n2313 +\n2314  \n2315 +\n2316  \n2317  \n2318  \n2319  \n2320  \n2321 +\n2322  \n2323  \n2324  \n2325  \n2326 +\n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351 +\n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364 +\n2365  \n2366  \n2367  \n2368  \n2369  \n2370 +\n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379 +\n2380 +\n2381  \n2382  \n2383  \n2384  \n2385  \n2386 +\n2387  \n2388  \n2389  \n2390  \n2391 +\n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  ",
            "        @Override\n        public StoredDestination readPayload(final DataInput dataIn) throws IOException {\n            final StoredDestination value = new StoredDestination();\n            value.orderIndex.defaultPriorityIndex = new BTreeIndex<>(pageFile, dataIn.readLong());\n            value.locationIndex = new BTreeIndex<>(pageFile, dataIn.readLong());\n            value.messageIdIndex = new BTreeIndex<>(pageFile, dataIn.readLong());\n\n            if (dataIn.readBoolean()) {\n                value.subscriptions = new BTreeIndex<>(pageFile, dataIn.readLong());\n                value.subscriptionAcks = new BTreeIndex<>(pageFile, dataIn.readLong());\n                if (metadata.version >= 4) {\n                    value.ackPositions = new ListIndex<>(pageFile, dataIn.readLong());\n                } else {\n                    // upgrade\n                    pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                        @Override\n                        public void execute(Transaction tx) throws IOException {\n                            LinkedHashMap<String, SequenceSet> temp = new LinkedHashMap<>();\n\n                            if (metadata.version >= 3) {\n                                // migrate\n                                BTreeIndex<Long, HashSet<String>> oldAckPositions =\n                                        new BTreeIndex<>(pageFile, dataIn.readLong());\n                                oldAckPositions.setKeyMarshaller(LongMarshaller.INSTANCE);\n                                oldAckPositions.setValueMarshaller(HashSetStringMarshaller.INSTANCE);\n                                oldAckPositions.load(tx);\n\n\n                                // Do the initial build of the data in memory before writing into the store\n                                // based Ack Positions List to avoid a lot of disk thrashing.\n                                Iterator<Entry<Long, HashSet<String>>> iterator = oldAckPositions.iterator(tx);\n                                while (iterator.hasNext()) {\n                                    Entry<Long, HashSet<String>> entry = iterator.next();\n\n                                    for(String subKey : entry.getValue()) {\n                                        SequenceSet pendingAcks = temp.get(subKey);\n                                        if (pendingAcks == null) {\n                                            pendingAcks = new SequenceSet();\n                                            temp.put(subKey, pendingAcks);\n                                        }\n\n                                        pendingAcks.add(entry.getKey());\n                                    }\n                                }\n                            }\n                            // Now move the pending messages to ack data into the store backed\n                            // structure.\n                            value.ackPositions = new ListIndex<>(pageFile, tx.allocate());\n                            value.ackPositions.setKeyMarshaller(StringMarshaller.INSTANCE);\n                            value.ackPositions.setValueMarshaller(SequenceSet.Marshaller.INSTANCE);\n                            value.ackPositions.load(tx);\n                            for(String subscriptionKey : temp.keySet()) {\n                                value.ackPositions.put(tx, subscriptionKey, temp.get(subscriptionKey));\n                            }\n\n                        }\n                    });\n                }\n\n                if (metadata.version >= 5) {\n                    value.subLocations = new ListIndex<>(pageFile, dataIn.readLong());\n                } else {\n                    // upgrade\n                    pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                        @Override\n                        public void execute(Transaction tx) throws IOException {\n                            value.subLocations = new ListIndex<>(pageFile, tx.allocate());\n                            value.subLocations.setKeyMarshaller(StringMarshaller.INSTANCE);\n                            value.subLocations.setValueMarshaller(LocationMarshaller.INSTANCE);\n                            value.subLocations.load(tx);\n                        }\n                    });\n                }\n            }\n            if (metadata.version >= 2) {\n                value.orderIndex.lowPriorityIndex = new BTreeIndex<>(pageFile, dataIn.readLong());\n                value.orderIndex.highPriorityIndex = new BTreeIndex<>(pageFile, dataIn.readLong());\n            } else {\n                // upgrade\n                pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                    @Override\n                    public void execute(Transaction tx) throws IOException {\n                        value.orderIndex.lowPriorityIndex = new BTreeIndex<>(pageFile, tx.allocate());\n                        value.orderIndex.lowPriorityIndex.setKeyMarshaller(LongMarshaller.INSTANCE);\n                        value.orderIndex.lowPriorityIndex.setValueMarshaller(messageKeysMarshaller);\n                        value.orderIndex.lowPriorityIndex.load(tx);\n\n                        value.orderIndex.highPriorityIndex = new BTreeIndex<>(pageFile, tx.allocate());\n                        value.orderIndex.highPriorityIndex.setKeyMarshaller(LongMarshaller.INSTANCE);\n                        value.orderIndex.highPriorityIndex.setValueMarshaller(messageKeysMarshaller);\n                        value.orderIndex.highPriorityIndex.load(tx);\n                    }\n                });\n            }\n\n            return value;\n        }"
        ],
        [
            "MessageDatabase::MessageOrderIndex::allocate(Transaction)",
            "3515  \n3516 -\n3517  \n3518 -\n3519 -\n3520  \n3521  ",
            "        void allocate(Transaction tx) throws IOException {\n            defaultPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, tx.allocate());\n            if (metadata.version >= 2) {\n                lowPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, tx.allocate());\n                highPriorityIndex = new BTreeIndex<Long, MessageKeys>(pageFile, tx.allocate());\n            }\n        }",
            "3517  \n3518 +\n3519  \n3520 +\n3521 +\n3522  \n3523  ",
            "        void allocate(Transaction tx) throws IOException {\n            defaultPriorityIndex = new BTreeIndex<>(pageFile, tx.allocate());\n            if (metadata.version >= 2) {\n                lowPriorityIndex = new BTreeIndex<>(pageFile, tx.allocate());\n                highPriorityIndex = new BTreeIndex<>(pageFile, tx.allocate());\n            }\n        }"
        ],
        [
            "MessageDatabase::recordAckMessageReferenceLocation(Location,Location)",
            "1588  \n1589  \n1590  \n1591 -\n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  ",
            "    private void recordAckMessageReferenceLocation(Location ackLocation, Location messageLocation) {\n        Set<Integer> referenceFileIds = metadata.ackMessageFileMap.get(Integer.valueOf(ackLocation.getDataFileId()));\n        if (referenceFileIds == null) {\n            referenceFileIds = new HashSet<Integer>();\n            referenceFileIds.add(messageLocation.getDataFileId());\n            metadata.ackMessageFileMap.put(ackLocation.getDataFileId(), referenceFileIds);\n        } else {\n            Integer id = Integer.valueOf(messageLocation.getDataFileId());\n            if (!referenceFileIds.contains(id)) {\n                referenceFileIds.add(id);\n            }\n        }\n    }",
            "1588  \n1589  \n1590  \n1591 +\n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  ",
            "    private void recordAckMessageReferenceLocation(Location ackLocation, Location messageLocation) {\n        Set<Integer> referenceFileIds = metadata.ackMessageFileMap.get(Integer.valueOf(ackLocation.getDataFileId()));\n        if (referenceFileIds == null) {\n            referenceFileIds = new HashSet<>();\n            referenceFileIds.add(messageLocation.getDataFileId());\n            metadata.ackMessageFileMap.put(ackLocation.getDataFileId(), referenceFileIds);\n        } else {\n            Integer id = Integer.valueOf(messageLocation.getDataFileId());\n            if (!referenceFileIds.contains(id)) {\n                referenceFileIds.add(id);\n            }\n        }\n    }"
        ],
        [
            "MessageDatabase::load()",
            " 469  \n 470  \n 471 -\n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  ",
            "    public void load() throws IOException {\n        this.indexLock.writeLock().lock();\n        IOHelper.mkdirs(directory);\n        try {\n            if (deleteAllMessages) {\n                getJournal().start();\n                getJournal().delete();\n                getJournal().close();\n                journal = null;\n                getPageFile().delete();\n                LOG.info(\"Persistence store purged.\");\n                deleteAllMessages = false;\n            }\n\n            open();\n            store(new KahaTraceCommand().setMessage(\"LOADED \" + new Date()));\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }",
            " 469  \n 470  \n 471  \n 472 +\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  ",
            "    public void load() throws IOException {\n        this.indexLock.writeLock().lock();\n        try {\n            IOHelper.mkdirs(directory);\n            if (deleteAllMessages) {\n                getJournal().start();\n                getJournal().delete();\n                getJournal().close();\n                journal = null;\n                getPageFile().delete();\n                LOG.info(\"Persistence store purged.\");\n                deleteAllMessages = false;\n            }\n\n            open();\n            store(new KahaTraceCommand().setMessage(\"LOADED \" + new Date()));\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "MessageDatabase::AckCompactionRunner::run()",
            "1948  \n1949  \n1950  \n1951  \n1952 -\n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982 -\n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  ",
            "        @Override\n        public void run() {\n\n            int journalToAdvance = -1;\n            Set<Integer> journalLogsReferenced = new HashSet<Integer>();\n\n            //flag to know whether the ack forwarding completed without an exception\n            boolean forwarded = false;\n\n            try {\n                //acquire the checkpoint lock to prevent other threads from\n                //running a checkpoint while this is running\n                //\n                //Normally this task runs on the same executor as the checkpoint task\n                //so this ack compaction runner wouldn't run at the same time as the checkpoint task.\n                //\n                //However, there are two cases where this isn't always true.\n                //First, the checkpoint() method is public and can be called through the\n                //PersistenceAdapter interface by someone at the same time this is running.\n                //Second, a checkpoint is called during shutdown without using the executor.\n                //\n                //In the future it might be better to just remove the checkpointLock entirely\n                //and only use the executor but this would need to be examined for any unintended\n                //consequences\n                checkpointLock.readLock().lock();\n\n                try {\n\n                    // Lock index to capture the ackMessageFileMap data\n                    indexLock.writeLock().lock();\n\n                    // Map keys might not be sorted, find the earliest log file to forward acks\n                    // from and move only those, future cycles can chip away at more as needed.\n                    // We won't move files that are themselves rewritten on a previous compaction.\n                    List<Integer> journalFileIds = new ArrayList<Integer>(metadata.ackMessageFileMap.keySet());\n                    Collections.sort(journalFileIds);\n                    for (Integer journalFileId : journalFileIds) {\n                        DataFile current = journal.getDataFileById(journalFileId);\n                        if (current != null && current.getTypeCode() != COMPACTED_JOURNAL_FILE) {\n                            journalToAdvance = journalFileId;\n                            break;\n                        }\n                    }\n\n                    // Check if we found one, or if we only found the current file being written to.\n                    if (journalToAdvance == -1 || journalToAdvance == journal.getCurrentDataFileId()) {\n                        return;\n                    }\n\n                    journalLogsReferenced.addAll(metadata.ackMessageFileMap.get(journalToAdvance));\n\n                } finally {\n                    indexLock.writeLock().unlock();\n                }\n\n                try {\n                    // Background rewrite of the old acks\n                    forwardAllAcks(journalToAdvance, journalLogsReferenced);\n                    forwarded = true;\n                } catch (IOException ioe) {\n                    LOG.error(\"Forwarding of acks failed\", ioe);\n                    brokerService.handleIOException(ioe);\n                } catch (Throwable e) {\n                    LOG.error(\"Forwarding of acks failed\", e);\n                    brokerService.handleIOException(IOExceptionSupport.create(e));\n                }\n            } finally {\n                checkpointLock.readLock().unlock();\n            }\n\n            try {\n                if (forwarded) {\n                    // Checkpoint with changes from the ackMessageFileMap\n                    checkpointUpdate(false);\n                }\n            } catch (IOException ioe) {\n                LOG.error(\"Checkpoint failed\", ioe);\n                brokerService.handleIOException(ioe);\n            } catch (Throwable e) {\n                LOG.error(\"Checkpoint failed\", e);\n                brokerService.handleIOException(IOExceptionSupport.create(e));\n            }\n        }",
            "1948  \n1949  \n1950  \n1951  \n1952 +\n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982 +\n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  ",
            "        @Override\n        public void run() {\n\n            int journalToAdvance = -1;\n            Set<Integer> journalLogsReferenced = new HashSet<>();\n\n            //flag to know whether the ack forwarding completed without an exception\n            boolean forwarded = false;\n\n            try {\n                //acquire the checkpoint lock to prevent other threads from\n                //running a checkpoint while this is running\n                //\n                //Normally this task runs on the same executor as the checkpoint task\n                //so this ack compaction runner wouldn't run at the same time as the checkpoint task.\n                //\n                //However, there are two cases where this isn't always true.\n                //First, the checkpoint() method is public and can be called through the\n                //PersistenceAdapter interface by someone at the same time this is running.\n                //Second, a checkpoint is called during shutdown without using the executor.\n                //\n                //In the future it might be better to just remove the checkpointLock entirely\n                //and only use the executor but this would need to be examined for any unintended\n                //consequences\n                checkpointLock.readLock().lock();\n\n                try {\n\n                    // Lock index to capture the ackMessageFileMap data\n                    indexLock.writeLock().lock();\n\n                    // Map keys might not be sorted, find the earliest log file to forward acks\n                    // from and move only those, future cycles can chip away at more as needed.\n                    // We won't move files that are themselves rewritten on a previous compaction.\n                    List<Integer> journalFileIds = new ArrayList<>(metadata.ackMessageFileMap.keySet());\n                    Collections.sort(journalFileIds);\n                    for (Integer journalFileId : journalFileIds) {\n                        DataFile current = journal.getDataFileById(journalFileId);\n                        if (current != null && current.getTypeCode() != COMPACTED_JOURNAL_FILE) {\n                            journalToAdvance = journalFileId;\n                            break;\n                        }\n                    }\n\n                    // Check if we found one, or if we only found the current file being written to.\n                    if (journalToAdvance == -1 || journalToAdvance == journal.getCurrentDataFileId()) {\n                        return;\n                    }\n\n                    journalLogsReferenced.addAll(metadata.ackMessageFileMap.get(journalToAdvance));\n\n                } finally {\n                    indexLock.writeLock().unlock();\n                }\n\n                try {\n                    // Background rewrite of the old acks\n                    forwardAllAcks(journalToAdvance, journalLogsReferenced);\n                    forwarded = true;\n                } catch (IOException ioe) {\n                    LOG.error(\"Forwarding of acks failed\", ioe);\n                    brokerService.handleIOException(ioe);\n                } catch (Throwable e) {\n                    LOG.error(\"Forwarding of acks failed\", e);\n                    brokerService.handleIOException(IOExceptionSupport.create(e));\n                }\n            } finally {\n                checkpointLock.readLock().unlock();\n            }\n\n            try {\n                if (forwarded) {\n                    // Checkpoint with changes from the ackMessageFileMap\n                    checkpointUpdate(false);\n                }\n            } catch (IOException ioe) {\n                LOG.error(\"Checkpoint failed\", ioe);\n                brokerService.handleIOException(ioe);\n            } catch (Throwable e) {\n                LOG.error(\"Checkpoint failed\", e);\n                brokerService.handleIOException(IOExceptionSupport.create(e));\n            }\n        }"
        ],
        [
            "MessageDatabase::recover()",
            " 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719 -\n 720 -\n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  ",
            "    /**\n     * Move all the messages that were in the journal into long term storage. We\n     * just replay and do a checkpoint.\n     *\n     * @throws IOException\n     * @throws IOException\n     * @throws IllegalStateException\n     */\n    private void recover() throws IllegalStateException, IOException {\n        this.indexLock.writeLock().lock();\n        try {\n\n            long start = System.currentTimeMillis();\n            Location afterProducerAudit = recoverProducerAudit();\n            Location afterAckMessageFile = recoverAckMessageFileMap();\n            Location lastIndoubtPosition = getRecoveryPosition();\n\n            if (afterProducerAudit != null && afterProducerAudit.equals(metadata.ackMessageFileMapLocation)) {\n                // valid checkpoint, possible recover from afterAckMessageFile\n                afterProducerAudit = null;\n            }\n            Location recoveryPosition = minimum(afterProducerAudit, afterAckMessageFile);\n            recoveryPosition = minimum(recoveryPosition, lastIndoubtPosition);\n\n            if (recoveryPosition != null) {\n                int redoCounter = 0;\n                int dataFileRotationTracker = recoveryPosition.getDataFileId();\n                LOG.info(\"Recovering from the journal @\" + recoveryPosition);\n                while (recoveryPosition != null) {\n                    try {\n                        JournalCommand<?> message = load(recoveryPosition);\n                        metadata.lastUpdate = recoveryPosition;\n                        process(message, recoveryPosition, lastIndoubtPosition);\n                        redoCounter++;\n                    } catch (IOException failedRecovery) {\n                        if (isIgnoreMissingJournalfiles()) {\n                            LOG.debug(\"Failed to recover data at position:\" + recoveryPosition, failedRecovery);\n                            // track this dud location\n                            journal.corruptRecoveryLocation(recoveryPosition);\n                        } else {\n                            throw new IOException(\"Failed to recover data at position:\" + recoveryPosition, failedRecovery);\n                        }\n                    }\n                    recoveryPosition = journal.getNextLocation(recoveryPosition);\n                    // hold on to the minimum number of open files during recovery\n                    if (recoveryPosition != null && dataFileRotationTracker != recoveryPosition.getDataFileId()) {\n                        dataFileRotationTracker = recoveryPosition.getDataFileId();\n                        journal.cleanup();\n                    }\n                    if (LOG.isInfoEnabled() && redoCounter % 100000 == 0) {\n                        LOG.info(\"@\" + recoveryPosition + \", \" + redoCounter + \" entries recovered ..\");\n                    }\n                }\n                if (LOG.isInfoEnabled()) {\n                    long end = System.currentTimeMillis();\n                    LOG.info(\"Recovery replayed \" + redoCounter + \" operations from the journal in \" + ((end - start) / 1000.0f) + \" seconds.\");\n                }\n            }\n\n            // We may have to undo some index updates.\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    recoverIndex(tx);\n                }\n            });\n\n            // rollback any recovered inflight local transactions, and discard any inflight XA transactions.\n            Set<TransactionId> toRollback = new HashSet<TransactionId>();\n            Set<TransactionId> toDiscard = new HashSet<TransactionId>();\n            synchronized (inflightTransactions) {\n                for (Iterator<TransactionId> it = inflightTransactions.keySet().iterator(); it.hasNext(); ) {\n                    TransactionId id = it.next();\n                    if (id.isLocalTransaction()) {\n                        toRollback.add(id);\n                    } else {\n                        toDiscard.add(id);\n                    }\n                }\n                for (TransactionId tx: toRollback) {\n                    if (LOG.isDebugEnabled()) {\n                        LOG.debug(\"rolling back recovered indoubt local transaction \" + tx);\n                    }\n                    store(new KahaRollbackCommand().setTransactionInfo(TransactionIdConversion.convertToLocal(tx)), false, null, null);\n                }\n                for (TransactionId tx: toDiscard) {\n                    if (LOG.isDebugEnabled()) {\n                        LOG.debug(\"discarding recovered in-flight XA transaction \" + tx);\n                    }\n                    inflightTransactions.remove(tx);\n                }\n            }\n\n            synchronized (preparedTransactions) {\n                for (TransactionId txId : preparedTransactions.keySet()) {\n                    LOG.warn(\"Recovered prepared XA TX: [{}]\", txId);\n                }\n            }\n\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }",
            " 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719 +\n 720 +\n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  ",
            "    /**\n     * Move all the messages that were in the journal into long term storage. We\n     * just replay and do a checkpoint.\n     *\n     * @throws IOException\n     * @throws IOException\n     * @throws IllegalStateException\n     */\n    private void recover() throws IllegalStateException, IOException {\n        this.indexLock.writeLock().lock();\n        try {\n\n            long start = System.currentTimeMillis();\n            Location afterProducerAudit = recoverProducerAudit();\n            Location afterAckMessageFile = recoverAckMessageFileMap();\n            Location lastIndoubtPosition = getRecoveryPosition();\n\n            if (afterProducerAudit != null && afterProducerAudit.equals(metadata.ackMessageFileMapLocation)) {\n                // valid checkpoint, possible recover from afterAckMessageFile\n                afterProducerAudit = null;\n            }\n            Location recoveryPosition = minimum(afterProducerAudit, afterAckMessageFile);\n            recoveryPosition = minimum(recoveryPosition, lastIndoubtPosition);\n\n            if (recoveryPosition != null) {\n                int redoCounter = 0;\n                int dataFileRotationTracker = recoveryPosition.getDataFileId();\n                LOG.info(\"Recovering from the journal @\" + recoveryPosition);\n                while (recoveryPosition != null) {\n                    try {\n                        JournalCommand<?> message = load(recoveryPosition);\n                        metadata.lastUpdate = recoveryPosition;\n                        process(message, recoveryPosition, lastIndoubtPosition);\n                        redoCounter++;\n                    } catch (IOException failedRecovery) {\n                        if (isIgnoreMissingJournalfiles()) {\n                            LOG.debug(\"Failed to recover data at position:\" + recoveryPosition, failedRecovery);\n                            // track this dud location\n                            journal.corruptRecoveryLocation(recoveryPosition);\n                        } else {\n                            throw new IOException(\"Failed to recover data at position:\" + recoveryPosition, failedRecovery);\n                        }\n                    }\n                    recoveryPosition = journal.getNextLocation(recoveryPosition);\n                    // hold on to the minimum number of open files during recovery\n                    if (recoveryPosition != null && dataFileRotationTracker != recoveryPosition.getDataFileId()) {\n                        dataFileRotationTracker = recoveryPosition.getDataFileId();\n                        journal.cleanup();\n                    }\n                    if (LOG.isInfoEnabled() && redoCounter % 100000 == 0) {\n                        LOG.info(\"@\" + recoveryPosition + \", \" + redoCounter + \" entries recovered ..\");\n                    }\n                }\n                if (LOG.isInfoEnabled()) {\n                    long end = System.currentTimeMillis();\n                    LOG.info(\"Recovery replayed \" + redoCounter + \" operations from the journal in \" + ((end - start) / 1000.0f) + \" seconds.\");\n                }\n            }\n\n            // We may have to undo some index updates.\n            pageFile.tx().execute(new Transaction.Closure<IOException>() {\n                @Override\n                public void execute(Transaction tx) throws IOException {\n                    recoverIndex(tx);\n                }\n            });\n\n            // rollback any recovered inflight local transactions, and discard any inflight XA transactions.\n            Set<TransactionId> toRollback = new HashSet<>();\n            Set<TransactionId> toDiscard = new HashSet<>();\n            synchronized (inflightTransactions) {\n                for (Iterator<TransactionId> it = inflightTransactions.keySet().iterator(); it.hasNext(); ) {\n                    TransactionId id = it.next();\n                    if (id.isLocalTransaction()) {\n                        toRollback.add(id);\n                    } else {\n                        toDiscard.add(id);\n                    }\n                }\n                for (TransactionId tx: toRollback) {\n                    if (LOG.isDebugEnabled()) {\n                        LOG.debug(\"rolling back recovered indoubt local transaction \" + tx);\n                    }\n                    store(new KahaRollbackCommand().setTransactionInfo(TransactionIdConversion.convertToLocal(tx)), false, null, null);\n                }\n                for (TransactionId tx: toDiscard) {\n                    if (LOG.isDebugEnabled()) {\n                        LOG.debug(\"discarding recovered in-flight XA transaction \" + tx);\n                    }\n                    inflightTransactions.remove(tx);\n                }\n            }\n\n            synchronized (preparedTransactions) {\n                for (TransactionId txId : preparedTransactions.keySet()) {\n                    LOG.warn(\"Recovered prepared XA TX: [{}]\", txId);\n                }\n            }\n\n        } finally {\n            this.indexLock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "MessageDatabase::loadStoredDestination(Transaction,String,boolean)",
            "2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474 -\n2475 -\n2476  \n2477  \n2478 -\n2479 -\n2480 -\n2481 -\n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535 -\n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  ",
            "    /**\n     * @param tx\n     * @param key\n     * @param topic\n     * @return\n     * @throws IOException\n     */\n    private StoredDestination loadStoredDestination(Transaction tx, String key, boolean topic) throws IOException {\n        // Try to load the existing indexes..\n        StoredDestination rc = metadata.destinations.get(tx, key);\n        if (rc == null) {\n            // Brand new destination.. allocate indexes for it.\n            rc = new StoredDestination();\n            rc.orderIndex.allocate(tx);\n            rc.locationIndex = new BTreeIndex<Location, Long>(pageFile, tx.allocate());\n            rc.messageIdIndex = new BTreeIndex<String, Long>(pageFile, tx.allocate());\n\n            if (topic) {\n                rc.subscriptions = new BTreeIndex<String, KahaSubscriptionCommand>(pageFile, tx.allocate());\n                rc.subscriptionAcks = new BTreeIndex<String, LastAck>(pageFile, tx.allocate());\n                rc.ackPositions = new ListIndex<String, SequenceSet>(pageFile, tx.allocate());\n                rc.subLocations = new ListIndex<String, Location>(pageFile, tx.allocate());\n            }\n            metadata.destinations.put(tx, key, rc);\n        }\n\n        // Configure the marshalers and load.\n        rc.orderIndex.load(tx);\n\n        // Figure out the next key using the last entry in the destination.\n        rc.orderIndex.configureLast(tx);\n\n        rc.locationIndex.setKeyMarshaller(new LocationSizeMarshaller());\n        rc.locationIndex.setValueMarshaller(LongMarshaller.INSTANCE);\n        rc.locationIndex.load(tx);\n\n        rc.messageIdIndex.setKeyMarshaller(StringMarshaller.INSTANCE);\n        rc.messageIdIndex.setValueMarshaller(LongMarshaller.INSTANCE);\n        rc.messageIdIndex.load(tx);\n\n        //go through an upgrade old index if older than version 6\n        if (metadata.version < 6) {\n            for (Iterator<Entry<Location, Long>> iterator = rc.locationIndex.iterator(tx); iterator.hasNext(); ) {\n                Entry<Location, Long> entry = iterator.next();\n                // modify so it is upgraded\n                rc.locationIndex.put(tx, entry.getKey(), entry.getValue());\n            }\n            //upgrade the order index\n            for (Iterator<Entry<Long, MessageKeys>> iterator = rc.orderIndex.iterator(tx); iterator.hasNext(); ) {\n                Entry<Long, MessageKeys> entry = iterator.next();\n                //call get so that the last priority is updated\n                rc.orderIndex.get(tx, entry.getKey());\n                rc.orderIndex.put(tx, rc.orderIndex.lastGetPriority(), entry.getKey(), entry.getValue());\n            }\n        }\n\n        // If it was a topic...\n        if (topic) {\n\n            rc.subscriptions.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subscriptions.setValueMarshaller(KahaSubscriptionCommandMarshaller.INSTANCE);\n            rc.subscriptions.load(tx);\n\n            rc.subscriptionAcks.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subscriptionAcks.setValueMarshaller(new LastAckMarshaller());\n            rc.subscriptionAcks.load(tx);\n\n            rc.ackPositions.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.ackPositions.setValueMarshaller(SequenceSet.Marshaller.INSTANCE);\n            rc.ackPositions.load(tx);\n\n            rc.subLocations.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subLocations.setValueMarshaller(LocationMarshaller.INSTANCE);\n            rc.subLocations.load(tx);\n\n            rc.subscriptionCursors = new HashMap<String, MessageOrderCursor>();\n\n            if (metadata.version < 3) {\n\n                // on upgrade need to fill ackLocation with available messages past last ack\n                for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext(); ) {\n                    Entry<String, LastAck> entry = iterator.next();\n                    for (Iterator<Entry<Long, MessageKeys>> orderIterator =\n                            rc.orderIndex.iterator(tx, new MessageOrderCursor(entry.getValue().lastAckedSequence)); orderIterator.hasNext(); ) {\n                        Long sequence = orderIterator.next().getKey();\n                        addAckLocation(tx, rc, sequence, entry.getKey());\n                    }\n                    // modify so it is upgraded\n                    rc.subscriptionAcks.put(tx, entry.getKey(), entry.getValue());\n                }\n            }\n\n            // Configure the message references index\n            Iterator<Entry<String, SequenceSet>> subscriptions = rc.ackPositions.iterator(tx);\n            while (subscriptions.hasNext()) {\n                Entry<String, SequenceSet> subscription = subscriptions.next();\n                SequenceSet pendingAcks = subscription.getValue();\n                if (pendingAcks != null && !pendingAcks.isEmpty()) {\n                    Long lastPendingAck = pendingAcks.getTail().getLast();\n                    for (Long sequenceId : pendingAcks) {\n                        Long current = rc.messageReferences.get(sequenceId);\n                        if (current == null) {\n                            current = new Long(0);\n                        }\n\n                        // We always add a trailing empty entry for the next position to start from\n                        // so we need to ensure we don't count that as a message reference on reload.\n                        if (!sequenceId.equals(lastPendingAck)) {\n                            current = current.longValue() + 1;\n                        } else {\n                            current = Long.valueOf(0L);\n                        }\n\n                        rc.messageReferences.put(sequenceId, current);\n                    }\n                }\n            }\n\n            // Configure the subscription cache\n            for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext(); ) {\n                Entry<String, LastAck> entry = iterator.next();\n                rc.subscriptionCache.add(entry.getKey());\n            }\n\n            if (rc.orderIndex.nextMessageId == 0) {\n                // check for existing durable sub all acked out - pull next seq from acks as messages are gone\n                if (!rc.subscriptionAcks.isEmpty(tx)) {\n                    for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext();) {\n                        Entry<String, LastAck> entry = iterator.next();\n                        rc.orderIndex.nextMessageId =\n                                Math.max(rc.orderIndex.nextMessageId, entry.getValue().lastAckedSequence +1);\n                    }\n                }\n            } else {\n                // update based on ackPositions for unmatched, last entry is always the next\n                if (!rc.messageReferences.isEmpty()) {\n                    Long nextMessageId = (Long) rc.messageReferences.keySet().toArray()[rc.messageReferences.size() - 1];\n                    rc.orderIndex.nextMessageId =\n                            Math.max(rc.orderIndex.nextMessageId, nextMessageId);\n                }\n            }\n        }\n\n        if (metadata.version < VERSION) {\n            // store again after upgrade\n            metadata.destinations.put(tx, key, rc);\n        }\n        return rc;\n    }",
            "2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474 +\n2475 +\n2476  \n2477  \n2478 +\n2479 +\n2480 +\n2481 +\n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535 +\n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  ",
            "    /**\n     * @param tx\n     * @param key\n     * @param topic\n     * @return\n     * @throws IOException\n     */\n    private StoredDestination loadStoredDestination(Transaction tx, String key, boolean topic) throws IOException {\n        // Try to load the existing indexes..\n        StoredDestination rc = metadata.destinations.get(tx, key);\n        if (rc == null) {\n            // Brand new destination.. allocate indexes for it.\n            rc = new StoredDestination();\n            rc.orderIndex.allocate(tx);\n            rc.locationIndex = new BTreeIndex<>(pageFile, tx.allocate());\n            rc.messageIdIndex = new BTreeIndex<>(pageFile, tx.allocate());\n\n            if (topic) {\n                rc.subscriptions = new BTreeIndex<>(pageFile, tx.allocate());\n                rc.subscriptionAcks = new BTreeIndex<>(pageFile, tx.allocate());\n                rc.ackPositions = new ListIndex<>(pageFile, tx.allocate());\n                rc.subLocations = new ListIndex<>(pageFile, tx.allocate());\n            }\n            metadata.destinations.put(tx, key, rc);\n        }\n\n        // Configure the marshalers and load.\n        rc.orderIndex.load(tx);\n\n        // Figure out the next key using the last entry in the destination.\n        rc.orderIndex.configureLast(tx);\n\n        rc.locationIndex.setKeyMarshaller(new LocationSizeMarshaller());\n        rc.locationIndex.setValueMarshaller(LongMarshaller.INSTANCE);\n        rc.locationIndex.load(tx);\n\n        rc.messageIdIndex.setKeyMarshaller(StringMarshaller.INSTANCE);\n        rc.messageIdIndex.setValueMarshaller(LongMarshaller.INSTANCE);\n        rc.messageIdIndex.load(tx);\n\n        //go through an upgrade old index if older than version 6\n        if (metadata.version < 6) {\n            for (Iterator<Entry<Location, Long>> iterator = rc.locationIndex.iterator(tx); iterator.hasNext(); ) {\n                Entry<Location, Long> entry = iterator.next();\n                // modify so it is upgraded\n                rc.locationIndex.put(tx, entry.getKey(), entry.getValue());\n            }\n            //upgrade the order index\n            for (Iterator<Entry<Long, MessageKeys>> iterator = rc.orderIndex.iterator(tx); iterator.hasNext(); ) {\n                Entry<Long, MessageKeys> entry = iterator.next();\n                //call get so that the last priority is updated\n                rc.orderIndex.get(tx, entry.getKey());\n                rc.orderIndex.put(tx, rc.orderIndex.lastGetPriority(), entry.getKey(), entry.getValue());\n            }\n        }\n\n        // If it was a topic...\n        if (topic) {\n\n            rc.subscriptions.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subscriptions.setValueMarshaller(KahaSubscriptionCommandMarshaller.INSTANCE);\n            rc.subscriptions.load(tx);\n\n            rc.subscriptionAcks.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subscriptionAcks.setValueMarshaller(new LastAckMarshaller());\n            rc.subscriptionAcks.load(tx);\n\n            rc.ackPositions.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.ackPositions.setValueMarshaller(SequenceSet.Marshaller.INSTANCE);\n            rc.ackPositions.load(tx);\n\n            rc.subLocations.setKeyMarshaller(StringMarshaller.INSTANCE);\n            rc.subLocations.setValueMarshaller(LocationMarshaller.INSTANCE);\n            rc.subLocations.load(tx);\n\n            rc.subscriptionCursors = new HashMap<>();\n\n            if (metadata.version < 3) {\n\n                // on upgrade need to fill ackLocation with available messages past last ack\n                for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext(); ) {\n                    Entry<String, LastAck> entry = iterator.next();\n                    for (Iterator<Entry<Long, MessageKeys>> orderIterator =\n                            rc.orderIndex.iterator(tx, new MessageOrderCursor(entry.getValue().lastAckedSequence)); orderIterator.hasNext(); ) {\n                        Long sequence = orderIterator.next().getKey();\n                        addAckLocation(tx, rc, sequence, entry.getKey());\n                    }\n                    // modify so it is upgraded\n                    rc.subscriptionAcks.put(tx, entry.getKey(), entry.getValue());\n                }\n            }\n\n            // Configure the message references index\n            Iterator<Entry<String, SequenceSet>> subscriptions = rc.ackPositions.iterator(tx);\n            while (subscriptions.hasNext()) {\n                Entry<String, SequenceSet> subscription = subscriptions.next();\n                SequenceSet pendingAcks = subscription.getValue();\n                if (pendingAcks != null && !pendingAcks.isEmpty()) {\n                    Long lastPendingAck = pendingAcks.getTail().getLast();\n                    for (Long sequenceId : pendingAcks) {\n                        Long current = rc.messageReferences.get(sequenceId);\n                        if (current == null) {\n                            current = new Long(0);\n                        }\n\n                        // We always add a trailing empty entry for the next position to start from\n                        // so we need to ensure we don't count that as a message reference on reload.\n                        if (!sequenceId.equals(lastPendingAck)) {\n                            current = current.longValue() + 1;\n                        } else {\n                            current = Long.valueOf(0L);\n                        }\n\n                        rc.messageReferences.put(sequenceId, current);\n                    }\n                }\n            }\n\n            // Configure the subscription cache\n            for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext(); ) {\n                Entry<String, LastAck> entry = iterator.next();\n                rc.subscriptionCache.add(entry.getKey());\n            }\n\n            if (rc.orderIndex.nextMessageId == 0) {\n                // check for existing durable sub all acked out - pull next seq from acks as messages are gone\n                if (!rc.subscriptionAcks.isEmpty(tx)) {\n                    for (Iterator<Entry<String, LastAck>> iterator = rc.subscriptionAcks.iterator(tx); iterator.hasNext();) {\n                        Entry<String, LastAck> entry = iterator.next();\n                        rc.orderIndex.nextMessageId =\n                                Math.max(rc.orderIndex.nextMessageId, entry.getValue().lastAckedSequence +1);\n                    }\n                }\n            } else {\n                // update based on ackPositions for unmatched, last entry is always the next\n                if (!rc.messageReferences.isEmpty()) {\n                    Long nextMessageId = (Long) rc.messageReferences.keySet().toArray()[rc.messageReferences.size() - 1];\n                    rc.orderIndex.nextMessageId =\n                            Math.max(rc.orderIndex.nextMessageId, nextMessageId);\n                }\n            }\n        }\n\n        if (metadata.version < VERSION) {\n            // store again after upgrade\n            metadata.destinations.put(tx, key, rc);\n        }\n        return rc;\n    }"
        ],
        [
            "MessageDatabase::recoverIndex(Transaction)",
            " 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827 -\n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894 -\n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912 -\n 913 -\n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927 -\n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938 -\n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  ",
            "    protected void recoverIndex(Transaction tx) throws IOException {\n        long start = System.currentTimeMillis();\n        // It is possible index updates got applied before the journal updates..\n        // in that case we need to removed references to messages that are not in the journal\n        final Location lastAppendLocation = journal.getLastAppendLocation();\n        long undoCounter=0;\n\n        // Go through all the destinations to see if they have messages past the lastAppendLocation\n        for (String key : storedDestinations.keySet()) {\n            StoredDestination sd = storedDestinations.get(key);\n\n            final ArrayList<Long> matches = new ArrayList<Long>();\n            // Find all the Locations that are >= than the last Append Location.\n            sd.locationIndex.visit(tx, new BTreeVisitor.GTEVisitor<Location, Long>(lastAppendLocation) {\n                @Override\n                protected void matched(Location key, Long value) {\n                    matches.add(value);\n                }\n            });\n\n            for (Long sequenceId : matches) {\n                MessageKeys keys = sd.orderIndex.remove(tx, sequenceId);\n                if (keys != null) {\n                    sd.locationIndex.remove(tx, keys.location);\n                    sd.messageIdIndex.remove(tx, keys.messageId);\n                    metadata.producerSequenceIdTracker.rollback(keys.messageId);\n                    undoCounter++;\n                    decrementAndSubSizeToStoreStat(key, keys.location.getSize());\n                    // TODO: do we need to modify the ack positions for the pub sub case?\n                }\n            }\n        }\n\n        if (undoCounter > 0) {\n            // The rolledback operations are basically in flight journal writes.  To avoid getting\n            // these the end user should do sync writes to the journal.\n            if (LOG.isInfoEnabled()) {\n                long end = System.currentTimeMillis();\n                LOG.info(\"Rolled back \" + undoCounter + \" messages from the index in \" + ((end - start) / 1000.0f) + \" seconds.\");\n            }\n        }\n\n        undoCounter = 0;\n        start = System.currentTimeMillis();\n\n        // Lets be extra paranoid here and verify that all the datafiles being referenced\n        // by the indexes still exists.\n\n        final SequenceSet ss = new SequenceSet();\n        for (StoredDestination sd : storedDestinations.values()) {\n            // Use a visitor to cut down the number of pages that we load\n            sd.locationIndex.visit(tx, new BTreeVisitor<Location, Long>() {\n                int last=-1;\n\n                @Override\n                public boolean isInterestedInKeysBetween(Location first, Location second) {\n                    if( first==null ) {\n                        return !ss.contains(0, second.getDataFileId());\n                    } else if( second==null ) {\n                        return true;\n                    } else {\n                        return !ss.contains(first.getDataFileId(), second.getDataFileId());\n                    }\n                }\n\n                @Override\n                public void visit(List<Location> keys, List<Long> values) {\n                    for (Location l : keys) {\n                        int fileId = l.getDataFileId();\n                        if( last != fileId ) {\n                            ss.add(fileId);\n                            last = fileId;\n                        }\n                    }\n                }\n\n            });\n        }\n        HashSet<Integer> missingJournalFiles = new HashSet<Integer>();\n        while (!ss.isEmpty()) {\n            missingJournalFiles.add((int) ss.removeFirst());\n        }\n\n        for (Entry<Integer, Set<Integer>> entry : metadata.ackMessageFileMap.entrySet()) {\n            missingJournalFiles.add(entry.getKey());\n            for (Integer i : entry.getValue()) {\n                missingJournalFiles.add(i);\n            }\n        }\n\n        missingJournalFiles.removeAll(journal.getFileMap().keySet());\n\n        if (!missingJournalFiles.isEmpty()) {\n            LOG.warn(\"Some journal files are missing: \" + missingJournalFiles);\n        }\n\n        ArrayList<BTreeVisitor.Predicate<Location>> knownCorruption = new ArrayList<BTreeVisitor.Predicate<Location>>();\n        ArrayList<BTreeVisitor.Predicate<Location>> missingPredicates = new ArrayList<BTreeVisitor.Predicate<Location>>();\n        for (Integer missing : missingJournalFiles) {\n            missingPredicates.add(new BTreeVisitor.BetweenVisitor<Location, Long>(new Location(missing, 0), new Location(missing + 1, 0)));\n        }\n\n        if (checkForCorruptJournalFiles) {\n            Collection<DataFile> dataFiles = journal.getFileMap().values();\n            for (DataFile dataFile : dataFiles) {\n                int id = dataFile.getDataFileId();\n                // eof to next file id\n                missingPredicates.add(new BTreeVisitor.BetweenVisitor<Location, Long>(new Location(id, dataFile.getLength()), new Location(id + 1, 0)));\n                Sequence seq = dataFile.getCorruptedBlocks().getHead();\n                while (seq != null) {\n                    BTreeVisitor.BetweenVisitor<Location, Long> visitor =\n                        new BTreeVisitor.BetweenVisitor<Location, Long>(new Location(id, (int) seq.getFirst()), new Location(id, (int) seq.getLast() + 1));\n                    missingPredicates.add(visitor);\n                    knownCorruption.add(visitor);\n                    seq = seq.getNext();\n                }\n            }\n        }\n\n        if (!missingPredicates.isEmpty()) {\n            for (Entry<String, StoredDestination> sdEntry : storedDestinations.entrySet()) {\n                final StoredDestination sd = sdEntry.getValue();\n                final LinkedHashMap<Long, Location> matches = new LinkedHashMap<Long, Location>();\n                sd.locationIndex.visit(tx, new BTreeVisitor.OrVisitor<Location, Long>(missingPredicates) {\n                    @Override\n                    protected void matched(Location key, Long value) {\n                        matches.put(value, key);\n                    }\n                });\n\n                // If some message references are affected by the missing data files...\n                if (!matches.isEmpty()) {\n\n                    // We either 'gracefully' recover dropping the missing messages or\n                    // we error out.\n                    if( ignoreMissingJournalfiles ) {\n                        // Update the index to remove the references to the missing data\n                        for (Long sequenceId : matches.keySet()) {\n                            MessageKeys keys = sd.orderIndex.remove(tx, sequenceId);\n                            sd.locationIndex.remove(tx, keys.location);\n                            sd.messageIdIndex.remove(tx, keys.messageId);\n                            LOG.info(\"[\" + sdEntry.getKey() + \"] dropped: \" + keys.messageId + \" at corrupt location: \" + keys.location);\n                            undoCounter++;\n                            decrementAndSubSizeToStoreStat(sdEntry.getKey(), keys.location.getSize());\n                            // TODO: do we need to modify the ack positions for the pub sub case?\n                        }\n                    } else {\n                        LOG.error(\"[\" + sdEntry.getKey() + \"] references corrupt locations: \" + matches);\n                        throw new IOException(\"Detected missing/corrupt journal files referenced by:[\" + sdEntry.getKey() + \"] \" +matches.size()+\" messages affected.\");\n                    }\n                }\n            }\n        }\n\n        if (!ignoreMissingJournalfiles) {\n            if (!knownCorruption.isEmpty()) {\n                LOG.error(\"Detected corrupt journal files. \" + knownCorruption);\n                throw new IOException(\"Detected corrupt journal files. \" + knownCorruption);\n            }\n\n            if (!missingJournalFiles.isEmpty()) {\n                LOG.error(\"Detected missing journal files. \" + missingJournalFiles);\n                throw new IOException(\"Detected missing journal files. \" + missingJournalFiles);\n            }\n        }\n\n        if (undoCounter > 0) {\n            // The rolledback operations are basically in flight journal writes.  To avoid getting these the end user\n            // should do sync writes to the journal.\n            if (LOG.isInfoEnabled()) {\n                long end = System.currentTimeMillis();\n                LOG.info(\"Detected missing/corrupt journal files.  Dropped \" + undoCounter + \" messages from the index in \" + ((end - start) / 1000.0f) + \" seconds.\");\n            }\n        }\n    }",
            " 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827 +\n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894 +\n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912 +\n 913 +\n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927 +\n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938 +\n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  ",
            "    protected void recoverIndex(Transaction tx) throws IOException {\n        long start = System.currentTimeMillis();\n        // It is possible index updates got applied before the journal updates..\n        // in that case we need to removed references to messages that are not in the journal\n        final Location lastAppendLocation = journal.getLastAppendLocation();\n        long undoCounter=0;\n\n        // Go through all the destinations to see if they have messages past the lastAppendLocation\n        for (String key : storedDestinations.keySet()) {\n            StoredDestination sd = storedDestinations.get(key);\n\n            final ArrayList<Long> matches = new ArrayList<>();\n            // Find all the Locations that are >= than the last Append Location.\n            sd.locationIndex.visit(tx, new BTreeVisitor.GTEVisitor<Location, Long>(lastAppendLocation) {\n                @Override\n                protected void matched(Location key, Long value) {\n                    matches.add(value);\n                }\n            });\n\n            for (Long sequenceId : matches) {\n                MessageKeys keys = sd.orderIndex.remove(tx, sequenceId);\n                if (keys != null) {\n                    sd.locationIndex.remove(tx, keys.location);\n                    sd.messageIdIndex.remove(tx, keys.messageId);\n                    metadata.producerSequenceIdTracker.rollback(keys.messageId);\n                    undoCounter++;\n                    decrementAndSubSizeToStoreStat(key, keys.location.getSize());\n                    // TODO: do we need to modify the ack positions for the pub sub case?\n                }\n            }\n        }\n\n        if (undoCounter > 0) {\n            // The rolledback operations are basically in flight journal writes.  To avoid getting\n            // these the end user should do sync writes to the journal.\n            if (LOG.isInfoEnabled()) {\n                long end = System.currentTimeMillis();\n                LOG.info(\"Rolled back \" + undoCounter + \" messages from the index in \" + ((end - start) / 1000.0f) + \" seconds.\");\n            }\n        }\n\n        undoCounter = 0;\n        start = System.currentTimeMillis();\n\n        // Lets be extra paranoid here and verify that all the datafiles being referenced\n        // by the indexes still exists.\n\n        final SequenceSet ss = new SequenceSet();\n        for (StoredDestination sd : storedDestinations.values()) {\n            // Use a visitor to cut down the number of pages that we load\n            sd.locationIndex.visit(tx, new BTreeVisitor<Location, Long>() {\n                int last=-1;\n\n                @Override\n                public boolean isInterestedInKeysBetween(Location first, Location second) {\n                    if( first==null ) {\n                        return !ss.contains(0, second.getDataFileId());\n                    } else if( second==null ) {\n                        return true;\n                    } else {\n                        return !ss.contains(first.getDataFileId(), second.getDataFileId());\n                    }\n                }\n\n                @Override\n                public void visit(List<Location> keys, List<Long> values) {\n                    for (Location l : keys) {\n                        int fileId = l.getDataFileId();\n                        if( last != fileId ) {\n                            ss.add(fileId);\n                            last = fileId;\n                        }\n                    }\n                }\n\n            });\n        }\n        HashSet<Integer> missingJournalFiles = new HashSet<>();\n        while (!ss.isEmpty()) {\n            missingJournalFiles.add((int) ss.removeFirst());\n        }\n\n        for (Entry<Integer, Set<Integer>> entry : metadata.ackMessageFileMap.entrySet()) {\n            missingJournalFiles.add(entry.getKey());\n            for (Integer i : entry.getValue()) {\n                missingJournalFiles.add(i);\n            }\n        }\n\n        missingJournalFiles.removeAll(journal.getFileMap().keySet());\n\n        if (!missingJournalFiles.isEmpty()) {\n            LOG.warn(\"Some journal files are missing: \" + missingJournalFiles);\n        }\n\n        ArrayList<BTreeVisitor.Predicate<Location>> knownCorruption = new ArrayList<>();\n        ArrayList<BTreeVisitor.Predicate<Location>> missingPredicates = new ArrayList<>();\n        for (Integer missing : missingJournalFiles) {\n            missingPredicates.add(new BTreeVisitor.BetweenVisitor<Location, Long>(new Location(missing, 0), new Location(missing + 1, 0)));\n        }\n\n        if (checkForCorruptJournalFiles) {\n            Collection<DataFile> dataFiles = journal.getFileMap().values();\n            for (DataFile dataFile : dataFiles) {\n                int id = dataFile.getDataFileId();\n                // eof to next file id\n                missingPredicates.add(new BTreeVisitor.BetweenVisitor<Location, Long>(new Location(id, dataFile.getLength()), new Location(id + 1, 0)));\n                Sequence seq = dataFile.getCorruptedBlocks().getHead();\n                while (seq != null) {\n                    BTreeVisitor.BetweenVisitor<Location, Long> visitor =\n                        new BTreeVisitor.BetweenVisitor<>(new Location(id, (int) seq.getFirst()), new Location(id, (int) seq.getLast() + 1));\n                    missingPredicates.add(visitor);\n                    knownCorruption.add(visitor);\n                    seq = seq.getNext();\n                }\n            }\n        }\n\n        if (!missingPredicates.isEmpty()) {\n            for (Entry<String, StoredDestination> sdEntry : storedDestinations.entrySet()) {\n                final StoredDestination sd = sdEntry.getValue();\n                final LinkedHashMap<Long, Location> matches = new LinkedHashMap<>();\n                sd.locationIndex.visit(tx, new BTreeVisitor.OrVisitor<Location, Long>(missingPredicates) {\n                    @Override\n                    protected void matched(Location key, Long value) {\n                        matches.put(value, key);\n                    }\n                });\n\n                // If some message references are affected by the missing data files...\n                if (!matches.isEmpty()) {\n\n                    // We either 'gracefully' recover dropping the missing messages or\n                    // we error out.\n                    if( ignoreMissingJournalfiles ) {\n                        // Update the index to remove the references to the missing data\n                        for (Long sequenceId : matches.keySet()) {\n                            MessageKeys keys = sd.orderIndex.remove(tx, sequenceId);\n                            sd.locationIndex.remove(tx, keys.location);\n                            sd.messageIdIndex.remove(tx, keys.messageId);\n                            LOG.info(\"[\" + sdEntry.getKey() + \"] dropped: \" + keys.messageId + \" at corrupt location: \" + keys.location);\n                            undoCounter++;\n                            decrementAndSubSizeToStoreStat(sdEntry.getKey(), keys.location.getSize());\n                            // TODO: do we need to modify the ack positions for the pub sub case?\n                        }\n                    } else {\n                        LOG.error(\"[\" + sdEntry.getKey() + \"] references corrupt locations: \" + matches);\n                        throw new IOException(\"Detected missing/corrupt journal files referenced by:[\" + sdEntry.getKey() + \"] \" +matches.size()+\" messages affected.\");\n                    }\n                }\n            }\n        }\n\n        if (!ignoreMissingJournalfiles) {\n            if (!knownCorruption.isEmpty()) {\n                LOG.error(\"Detected corrupt journal files. \" + knownCorruption);\n                throw new IOException(\"Detected corrupt journal files. \" + knownCorruption);\n            }\n\n            if (!missingJournalFiles.isEmpty()) {\n                LOG.error(\"Detected missing journal files. \" + missingJournalFiles);\n                throw new IOException(\"Detected missing journal files. \" + missingJournalFiles);\n            }\n        }\n\n        if (undoCounter > 0) {\n            // The rolledback operations are basically in flight journal writes.  To avoid getting these the end user\n            // should do sync writes to the journal.\n            if (LOG.isInfoEnabled()) {\n                long end = System.currentTimeMillis();\n                LOG.info(\"Detected missing/corrupt journal files.  Dropped \" + undoCounter + \" messages from the index in \" + ((end - start) / 1000.0f) + \" seconds.\");\n            }\n        }\n    }"
        ],
        [
            "MessageDatabase::removeAckLocation(KahaRemoveMessageCommand,Transaction,StoredDestination,String,Long)",
            "2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927  \n2928  \n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944 -\n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  ",
            "    /**\n     * @param tx\n     * @param sd\n     * @param subscriptionKey\n     * @param messageSequence\n     * @throws IOException\n     */\n    private void removeAckLocation(KahaRemoveMessageCommand command,\n            Transaction tx, StoredDestination sd, String subscriptionKey,\n            Long messageSequence) throws IOException {\n        // Remove the sub from the previous location set..\n        if (messageSequence != null) {\n            SequenceSet range = sd.ackPositions.get(tx, subscriptionKey);\n            if (range != null && !range.isEmpty()) {\n                range.remove(messageSequence);\n                if (!range.isEmpty()) {\n                    sd.ackPositions.put(tx, subscriptionKey, range);\n                } else {\n                    sd.ackPositions.remove(tx, subscriptionKey);\n                }\n\n                MessageKeys key = sd.orderIndex.get(tx, messageSequence);\n                decrementAndSubSizeToStoreStat(command.getDestination(), subscriptionKey,\n                        key.location.getSize());\n\n                // Check if the message is reference by any other subscription.\n                Long count = sd.messageReferences.get(messageSequence);\n                if (count != null) {\n                    long references = count.longValue() - 1;\n                    if (references > 0) {\n                        sd.messageReferences.put(messageSequence, Long.valueOf(references));\n                        return;\n                    } else {\n                        sd.messageReferences.remove(messageSequence);\n                    }\n                }\n\n                // Find all the entries that need to get deleted.\n                ArrayList<Entry<Long, MessageKeys>> deletes = new ArrayList<Entry<Long, MessageKeys>>();\n                sd.orderIndex.getDeleteList(tx, deletes, messageSequence);\n\n                // Do the actual deletes.\n                for (Entry<Long, MessageKeys> entry : deletes) {\n                    sd.locationIndex.remove(tx, entry.getValue().location);\n                    sd.messageIdIndex.remove(tx, entry.getValue().messageId);\n                    sd.orderIndex.remove(tx, entry.getKey());\n                    decrementAndSubSizeToStoreStat(command.getDestination(), entry.getValue().location.getSize());\n                }\n            }\n        }\n    }",
            "2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927  \n2928  \n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944 +\n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  ",
            "    /**\n     * @param tx\n     * @param sd\n     * @param subscriptionKey\n     * @param messageSequence\n     * @throws IOException\n     */\n    private void removeAckLocation(KahaRemoveMessageCommand command,\n            Transaction tx, StoredDestination sd, String subscriptionKey,\n            Long messageSequence) throws IOException {\n        // Remove the sub from the previous location set..\n        if (messageSequence != null) {\n            SequenceSet range = sd.ackPositions.get(tx, subscriptionKey);\n            if (range != null && !range.isEmpty()) {\n                range.remove(messageSequence);\n                if (!range.isEmpty()) {\n                    sd.ackPositions.put(tx, subscriptionKey, range);\n                } else {\n                    sd.ackPositions.remove(tx, subscriptionKey);\n                }\n\n                MessageKeys key = sd.orderIndex.get(tx, messageSequence);\n                decrementAndSubSizeToStoreStat(command.getDestination(), subscriptionKey,\n                        key.location.getSize());\n\n                // Check if the message is reference by any other subscription.\n                Long count = sd.messageReferences.get(messageSequence);\n                if (count != null) {\n                    long references = count.longValue() - 1;\n                    if (references > 0) {\n                        sd.messageReferences.put(messageSequence, Long.valueOf(references));\n                        return;\n                    } else {\n                        sd.messageReferences.remove(messageSequence);\n                    }\n                }\n\n                // Find all the entries that need to get deleted.\n                ArrayList<Entry<Long, MessageKeys>> deletes = new ArrayList<>();\n                sd.orderIndex.getDeleteList(tx, deletes, messageSequence);\n\n                // Do the actual deletes.\n                for (Entry<Long, MessageKeys> entry : deletes) {\n                    sd.locationIndex.remove(tx, entry.getValue().location);\n                    sd.messageIdIndex.remove(tx, entry.getValue().messageId);\n                    sd.orderIndex.remove(tx, entry.getKey());\n                    decrementAndSubSizeToStoreStat(command.getDestination(), entry.getValue().location.getSize());\n                }\n            }\n        }\n    }"
        ]
    ]
}