{
    "3d3677932a4ec98c12121c879ac5e2ea71925ea5": [
        [
            "DistributedHBaseCluster::restoreRegionServers(ClusterStatus,ClusterStatus)",
            " 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450 -\n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  ",
            "  protected boolean restoreRegionServers(ClusterStatus initial, ClusterStatus current) {\n    Set<ServerName> toStart = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());\n    Set<ServerName> toKill = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());\n    toStart.addAll(initial.getServers());\n    toKill.addAll(current.getServers());\n\n    for (ServerName server : current.getServers()) {\n      toStart.remove(server);\n    }\n    for (ServerName server: initial.getServers()) {\n      toKill.remove(server);\n    }\n\n    List<IOException> deferred = new ArrayList<IOException>();\n\n    for(ServerName sn:toStart) {\n      try {\n        if (!clusterManager.isRunning(ServiceType.HBASE_REGIONSERVER,\n                sn.getHostname(),\n                sn.getPort())) {\n          LOG.info(\"Restoring cluster - starting initial region server: \" + sn.getHostAndPort());\n          startRegionServer(sn.getHostname(), sn.getPort());\n        }\n      } catch (IOException ex) {\n        deferred.add(ex);\n      }\n    }\n\n    for(ServerName sn:toKill) {\n      try {\n        if (clusterManager.isRunning(ServiceType.HBASE_REGIONSERVER,\n                sn.getHostname(),\n                sn.getPort())) {\n          LOG.info(\"Restoring cluster - stopping initial region server: \" + sn.getHostAndPort());\n          stopRegionServer(sn);\n        }\n      } catch (IOException ex) {\n        deferred.add(ex);\n      }\n    }\n    if (!deferred.isEmpty()) {\n      LOG.warn(\"Restoring cluster - restoring region servers reported \"\n              + deferred.size() + \" errors:\");\n      for (int i=0; i<deferred.size() && i < 3; i++) {\n        LOG.warn(deferred.get(i));\n      }\n    }\n\n    return deferred.isEmpty();\n  }",
            " 431  \n 432  \n 433  \n 434  \n 435  \n 436 +\n 437 +\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452 +\n 453 +\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466 +\n 467 +\n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  ",
            "  protected boolean restoreRegionServers(ClusterStatus initial, ClusterStatus current) {\n    Set<ServerName> toStart = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());\n    Set<ServerName> toKill = new TreeSet<ServerName>(new ServerNameIgnoreStartCodeComparator());\n    toStart.addAll(initial.getServers());\n    toKill.addAll(current.getServers());\n    \n    ServerName master = initial.getMaster();\n\n    for (ServerName server : current.getServers()) {\n      toStart.remove(server);\n    }\n    for (ServerName server: initial.getServers()) {\n      toKill.remove(server);\n    }\n\n    List<IOException> deferred = new ArrayList<IOException>();\n\n    for(ServerName sn:toStart) {\n      try {\n        if (!clusterManager.isRunning(ServiceType.HBASE_REGIONSERVER,\n                sn.getHostname(),\n                sn.getPort())\n                && master.getPort() != sn.getPort()) {\n          LOG.info(\"Restoring cluster - starting initial region server: \" + sn.getHostAndPort());\n          startRegionServer(sn.getHostname(), sn.getPort());\n        }\n      } catch (IOException ex) {\n        deferred.add(ex);\n      }\n    }\n\n    for(ServerName sn:toKill) {\n      try {\n        if (clusterManager.isRunning(ServiceType.HBASE_REGIONSERVER,\n                sn.getHostname(),\n                sn.getPort())\n                && master.getPort() != sn.getPort()){\n          LOG.info(\"Restoring cluster - stopping initial region server: \" + sn.getHostAndPort());\n          stopRegionServer(sn);\n        }\n      } catch (IOException ex) {\n        deferred.add(ex);\n      }\n    }\n    if (!deferred.isEmpty()) {\n      LOG.warn(\"Restoring cluster - restoring region servers reported \"\n              + deferred.size() + \" errors:\");\n      for (int i=0; i<deferred.size() && i < 3; i++) {\n        LOG.warn(deferred.get(i));\n      }\n    }\n\n    return deferred.isEmpty();\n  }"
        ]
    ],
    "8ee9158b5475b7d5bb6ba646420403820fac1d63": [
        [
            "RSRpcServices::doNonAtomicRegionMutation(Region,OperationQuota,RegionAction,CellScanner,RegionActionResult,List,long,RegionScannersCloseCallBack,RpcCallContext)",
            " 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702 -\n 703 -\n 704 -\n 705 -\n 706 -\n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  ",
            "  /**\n   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when\n   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.\n   * @param region\n   * @param actions\n   * @param cellScanner\n   * @param builder\n   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this\n   * method returns as a 'result'.\n   * @param closeCallBack the callback to be used with multigets\n   * @param context the current RpcCallContext\n   * @return Return the <code>cellScanner</code> passed\n   */\n  private List<CellScannable> doNonAtomicRegionMutation(final Region region,\n      final OperationQuota quota, final RegionAction actions, final CellScanner cellScanner,\n      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup,\n      final RegionScannersCloseCallBack closeCallBack, RpcCallContext context) {\n    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do\n    // one at a time, we instead pass them in batch.  Be aware that the corresponding\n    // ResultOrException instance that matches each Put or Delete is then added down in the\n    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched\n    List<ClientProtos.Action> mutations = null;\n    long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n    IOException sizeIOE = null;\n    Object lastBlock = null;\n    for (ClientProtos.Action action : actions.getActionList()) {\n      ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = null;\n      try {\n        Result r = null;\n\n        if (context != null\n            && context.isRetryImmediatelySupported()\n            && (context.getResponseCellSize() > maxQuotaResultSize\n              || context.getResponseBlockSize() > maxQuotaResultSize)) {\n\n          // We're storing the exception since the exception and reason string won't\n          // change after the response size limit is reached.\n          if (sizeIOE == null ) {\n            // We don't need the stack un-winding do don't throw the exception.\n            // Throwing will kill the JVM's JIT.\n            //\n            // Instead just create the exception and then store it.\n            sizeIOE = new MultiActionResultTooLarge(\"Max size exceeded\"\n                + \" CellSize: \" + context.getResponseCellSize()\n                + \" BlockSize: \" + context.getResponseBlockSize());\n\n            // Only report the exception once since there's only one request that\n            // caused the exception. Otherwise this number will dominate the exceptions count.\n            rpcServer.getMetrics().exception(sizeIOE);\n          }\n\n          // Now that there's an exception is known to be created\n          // use it for the response.\n          //\n          // This will create a copy in the builder.\n          resultOrExceptionBuilder = ResultOrException.newBuilder().\n              setException(ResponseConverter.buildException(sizeIOE));\n          resultOrExceptionBuilder.setIndex(action.getIndex());\n          builder.addResultOrException(resultOrExceptionBuilder.build());\n          continue;\n        }\n        if (action.hasGet()) {\n          Get get = ProtobufUtil.toGet(action.getGet());\n          if (context != null) {\n            r = get(get, ((HRegion) region), closeCallBack, context);\n          } else {\n            r = region.get(get);\n          }\n        } else if (action.hasServiceCall()) {\n          resultOrExceptionBuilder = ResultOrException.newBuilder();\n          try {\n            Message result = execServiceOnRegion(region, action.getServiceCall());\n            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =\n                ClientProtos.CoprocessorServiceResult.newBuilder();\n            resultOrExceptionBuilder.setServiceResult(\n                serviceResultBuilder.setValue(\n                  serviceResultBuilder.getValueBuilder()\n                    .setName(result.getClass().getName())\n                    .setValue(result.toByteString())));\n          } catch (IOException ioe) {\n            rpcServer.getMetrics().exception(ioe);\n            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));\n          }\n        } else if (action.hasMutation()) {\n          MutationType type = action.getMutation().getMutateType();\n          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&\n              !mutations.isEmpty()) {\n            // Flush out any Puts or Deletes already collected.\n            doBatchOp(builder, region, quota, mutations, cellScanner);\n            mutations.clear();\n          }\n          switch (type) {\n            case APPEND:\n              r = append(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case INCREMENT:\n              r = increment(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case PUT:\n            case DELETE:\n              // Collect the individual mutations and apply in a batch\n              if (mutations == null) {\n                mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());\n              }\n              mutations.add(action);\n              break;\n            default:\n              throw new DoNotRetryIOException(\"Unsupported mutate type: \" + type.name());\n          }\n        } else {\n          throw new HBaseIOException(\"Unexpected Action type\");\n        }\n        if (r != null) {\n          ClientProtos.Result pbResult = null;\n          if (isClientCellBlockSupport(context)) {\n            pbResult = ProtobufUtil.toResultNoData(r);\n            //  Hard to guess the size here.  Just make a rough guess.\n            if (cellsToReturn == null) {\n              cellsToReturn = new ArrayList<CellScannable>();\n            }\n            cellsToReturn.add(r);\n          } else {\n            pbResult = ProtobufUtil.toResult(r);\n          }\n          lastBlock = addSize(context, r, lastBlock);\n          resultOrExceptionBuilder =\n            ClientProtos.ResultOrException.newBuilder().setResult(pbResult);\n        }\n        // Could get to here and there was no result and no exception.  Presumes we added\n        // a Put or Delete to the collecting Mutations List for adding later.  In this\n        // case the corresponding ResultOrException instance for the Put or Delete will be added\n        // down in the doBatchOp method call rather than up here.\n      } catch (IOException ie) {\n        rpcServer.getMetrics().exception(ie);\n        resultOrExceptionBuilder = ResultOrException.newBuilder().\n          setException(ResponseConverter.buildException(ie));\n      }\n      if (resultOrExceptionBuilder != null) {\n        // Propagate index.\n        resultOrExceptionBuilder.setIndex(action.getIndex());\n        builder.addResultOrException(resultOrExceptionBuilder.build());\n      }\n    }\n    // Finish up any outstanding mutations\n    if (mutations != null && !mutations.isEmpty()) {\n      doBatchOp(builder, region, quota, mutations, cellScanner);\n    }\n    return cellsToReturn;\n  }",
            " 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702 +\n 703 +\n 704 +\n 705 +\n 706 +\n 707 +\n 708 +\n 709 +\n 710 +\n 711 +\n 712 +\n 713 +\n 714 +\n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  ",
            "  /**\n   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when\n   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.\n   * @param region\n   * @param actions\n   * @param cellScanner\n   * @param builder\n   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this\n   * method returns as a 'result'.\n   * @param closeCallBack the callback to be used with multigets\n   * @param context the current RpcCallContext\n   * @return Return the <code>cellScanner</code> passed\n   */\n  private List<CellScannable> doNonAtomicRegionMutation(final Region region,\n      final OperationQuota quota, final RegionAction actions, final CellScanner cellScanner,\n      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup,\n      final RegionScannersCloseCallBack closeCallBack, RpcCallContext context) {\n    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do\n    // one at a time, we instead pass them in batch.  Be aware that the corresponding\n    // ResultOrException instance that matches each Put or Delete is then added down in the\n    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched\n    List<ClientProtos.Action> mutations = null;\n    long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n    IOException sizeIOE = null;\n    Object lastBlock = null;\n    for (ClientProtos.Action action : actions.getActionList()) {\n      ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = null;\n      try {\n        Result r = null;\n\n        if (context != null\n            && context.isRetryImmediatelySupported()\n            && (context.getResponseCellSize() > maxQuotaResultSize\n              || context.getResponseBlockSize() > maxQuotaResultSize)) {\n\n          // We're storing the exception since the exception and reason string won't\n          // change after the response size limit is reached.\n          if (sizeIOE == null ) {\n            // We don't need the stack un-winding do don't throw the exception.\n            // Throwing will kill the JVM's JIT.\n            //\n            // Instead just create the exception and then store it.\n            sizeIOE = new MultiActionResultTooLarge(\"Max size exceeded\"\n                + \" CellSize: \" + context.getResponseCellSize()\n                + \" BlockSize: \" + context.getResponseBlockSize());\n\n            // Only report the exception once since there's only one request that\n            // caused the exception. Otherwise this number will dominate the exceptions count.\n            rpcServer.getMetrics().exception(sizeIOE);\n          }\n\n          // Now that there's an exception is known to be created\n          // use it for the response.\n          //\n          // This will create a copy in the builder.\n          resultOrExceptionBuilder = ResultOrException.newBuilder().\n              setException(ResponseConverter.buildException(sizeIOE));\n          resultOrExceptionBuilder.setIndex(action.getIndex());\n          builder.addResultOrException(resultOrExceptionBuilder.build());\n          continue;\n        }\n        if (action.hasGet()) {\n          long before = EnvironmentEdgeManager.currentTime();\n          try {\n            Get get = ProtobufUtil.toGet(action.getGet());\n            if (context != null) {\n              r = get(get, ((HRegion) region), closeCallBack, context);\n            } else {\n              r = region.get(get);\n            }\n          } finally {\n            if (regionServer.metricsRegionServer != null) {\n              regionServer.metricsRegionServer.updateGet(\n                EnvironmentEdgeManager.currentTime() - before);\n            }\n          }\n        } else if (action.hasServiceCall()) {\n          resultOrExceptionBuilder = ResultOrException.newBuilder();\n          try {\n            Message result = execServiceOnRegion(region, action.getServiceCall());\n            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =\n                ClientProtos.CoprocessorServiceResult.newBuilder();\n            resultOrExceptionBuilder.setServiceResult(\n                serviceResultBuilder.setValue(\n                  serviceResultBuilder.getValueBuilder()\n                    .setName(result.getClass().getName())\n                    .setValue(result.toByteString())));\n          } catch (IOException ioe) {\n            rpcServer.getMetrics().exception(ioe);\n            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));\n          }\n        } else if (action.hasMutation()) {\n          MutationType type = action.getMutation().getMutateType();\n          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&\n              !mutations.isEmpty()) {\n            // Flush out any Puts or Deletes already collected.\n            doBatchOp(builder, region, quota, mutations, cellScanner);\n            mutations.clear();\n          }\n          switch (type) {\n            case APPEND:\n              r = append(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case INCREMENT:\n              r = increment(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case PUT:\n            case DELETE:\n              // Collect the individual mutations and apply in a batch\n              if (mutations == null) {\n                mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());\n              }\n              mutations.add(action);\n              break;\n            default:\n              throw new DoNotRetryIOException(\"Unsupported mutate type: \" + type.name());\n          }\n        } else {\n          throw new HBaseIOException(\"Unexpected Action type\");\n        }\n        if (r != null) {\n          ClientProtos.Result pbResult = null;\n          if (isClientCellBlockSupport(context)) {\n            pbResult = ProtobufUtil.toResultNoData(r);\n            //  Hard to guess the size here.  Just make a rough guess.\n            if (cellsToReturn == null) {\n              cellsToReturn = new ArrayList<CellScannable>();\n            }\n            cellsToReturn.add(r);\n          } else {\n            pbResult = ProtobufUtil.toResult(r);\n          }\n          lastBlock = addSize(context, r, lastBlock);\n          resultOrExceptionBuilder =\n            ClientProtos.ResultOrException.newBuilder().setResult(pbResult);\n        }\n        // Could get to here and there was no result and no exception.  Presumes we added\n        // a Put or Delete to the collecting Mutations List for adding later.  In this\n        // case the corresponding ResultOrException instance for the Put or Delete will be added\n        // down in the doBatchOp method call rather than up here.\n      } catch (IOException ie) {\n        rpcServer.getMetrics().exception(ie);\n        resultOrExceptionBuilder = ResultOrException.newBuilder().\n          setException(ResponseConverter.buildException(ie));\n      }\n      if (resultOrExceptionBuilder != null) {\n        // Propagate index.\n        resultOrExceptionBuilder.setIndex(action.getIndex());\n        builder.addResultOrException(resultOrExceptionBuilder.build());\n      }\n    }\n    // Finish up any outstanding mutations\n    if (mutations != null && !mutations.isEmpty()) {\n      doBatchOp(builder, region, quota, mutations, cellScanner);\n    }\n    return cellsToReturn;\n  }"
        ]
    ],
    "b6f091e0fd02111714ae8820c8a4d3b9b4828343": [
        [
            "TableMapReduceUtil::addHBaseDependencyJars(Configuration)",
            " 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to extenral tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n    addDependencyJars(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-server\n      // pull necessary dependencies\n      org.apache.zookeeper.ZooKeeper.class,\n      io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class);\n  }",
            " 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775 +\n 776 +\n 777 +\n 778 +\n 779 +\n 780 +\n 781 +\n 782 +\n 783 +\n 784 +\n 785 +\n 786 +\n 787 +\n 788 +\n 789 +\n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797 +\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to external tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n\n    // PrefixTreeCodec is part of the hbase-prefix-tree module. If not included in MR jobs jar\n    // dependencies, MR jobs that write encoded hfiles will fail.\n    // We used reflection here so to prevent a circular module dependency.\n    // TODO - if we extract the MR into a module, make it depend on hbase-prefix-tree.\n    Class prefixTreeCodecClass = null;\n    try {\n      prefixTreeCodecClass =\n          Class.forName(\"org.apache.hadoop.hbase.code.prefixtree.PrefixTreeCodec\");\n    } catch (ClassNotFoundException e) {\n      // this will show up in unit tests but should not show in real deployments\n      LOG.warn(\"The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.\" +\n          \"  Continuing without it.\");\n    }\n\n    addDependencyJars(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-server\n      prefixTreeCodecClass, //  hbase-prefix-tree (if null will be skipped)\n      // pull necessary dependencies\n      org.apache.zookeeper.ZooKeeper.class,\n      io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class);\n  }"
        ]
    ],
    "97cce850fed130aa263d61f6a3c4f361f2629c7c": [
        [
            "SparkSQLPushDownFilter::SparkSQLPushDownFilter(DynamicLogicExpression,byte,MutableList)",
            "  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78 -\n  79  \n  80  \n  81 -\n  82 -\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93 -\n  94  \n  95 -\n  96  \n  97  ",
            "  public SparkSQLPushDownFilter(DynamicLogicExpression dynamicLogicExpression,\n                                byte[][] valueFromQueryArray,\n                                MutableList<SchemaQualifierDefinition> columnDefinitions) {\n    this.dynamicLogicExpression = dynamicLogicExpression;\n    this.valueFromQueryArray = valueFromQueryArray;\n\n    //generate family qualifier to index mapping\n    this.currentCellToColumnIndexMap =\n            new HashMap<>();\n\n    for (int i = 0; i < columnDefinitions.size(); i++) {\n      SchemaQualifierDefinition definition = columnDefinitions.get(i).get();\n\n      ByteArrayComparable familyByteComparable =\n              new ByteArrayComparable(definition.columnFamilyBytes(),\n                      0, definition.columnFamilyBytes().length);\n\n      HashMap<ByteArrayComparable, String> qualifierIndexMap =\n              currentCellToColumnIndexMap.get(familyByteComparable);\n\n      if (qualifierIndexMap == null) {\n        qualifierIndexMap = new HashMap<>();\n        currentCellToColumnIndexMap.put(familyByteComparable, qualifierIndexMap);\n      }\n      ByteArrayComparable qualifierByteComparable =\n              new ByteArrayComparable(definition.qualifierBytes(), 0,\n                      definition.qualifierBytes().length);\n\n      qualifierIndexMap.put(qualifierByteComparable, definition.columnName());\n    }\n  }",
            "  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80 +\n  81  \n  82 +\n  83  \n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95 +\n  96  \n  97 +\n  98  \n  99  ",
            "  public SparkSQLPushDownFilter(DynamicLogicExpression dynamicLogicExpression,\n                                byte[][] valueFromQueryArray,\n                                MutableList<Field> fields) {\n    this.dynamicLogicExpression = dynamicLogicExpression;\n    this.valueFromQueryArray = valueFromQueryArray;\n\n    //generate family qualifier to index mapping\n    this.currentCellToColumnIndexMap =\n            new HashMap<>();\n\n    for (int i = 0; i < fields.size(); i++) {\n      Field field = fields.apply(i);\n\n      byte[] cfBytes = field.cfBytes();\n      ByteArrayComparable familyByteComparable =\n          new ByteArrayComparable(cfBytes, 0, cfBytes.length);\n\n      HashMap<ByteArrayComparable, String> qualifierIndexMap =\n              currentCellToColumnIndexMap.get(familyByteComparable);\n\n      if (qualifierIndexMap == null) {\n        qualifierIndexMap = new HashMap<>();\n        currentCellToColumnIndexMap.put(familyByteComparable, qualifierIndexMap);\n      }\n      byte[] qBytes = field.colBytes();\n      ByteArrayComparable qualifierByteComparable =\n          new ByteArrayComparable(qBytes, 0, qBytes.length);\n\n      qualifierIndexMap.put(qualifierByteComparable, field.colName());\n    }\n  }"
        ]
    ],
    "543e7081f548e1379bbd36a301640ef20d06ccc2": [
        [
            "HBaseTestingUtility::waitUntilAllRegionsAssigned(TableName)",
            "3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166 -\n3167  ",
            "  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty\n   * info:server, up to 60 seconds. This means all regions have been deployed,\n   * master has been informed and updated hbase:meta with the regions deployed\n   * server.\n   * @param tableName the table name\n   * @throws IOException\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName) throws IOException {\n    waitUntilAllRegionsAssigned(tableName, 60000);\n  }",
            "3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167 +\n3168 +\n3169 +\n3170  ",
            "  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty\n   * info:server, up to a configuable timeout value (default is 60 seconds)\n   * This means all regions have been deployed,\n   * master has been informed and updated hbase:meta with the regions deployed\n   * server.\n   * @param tableName the table name\n   * @throws IOException\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName) throws IOException {\n    waitUntilAllRegionsAssigned(\n      tableName,\n      this.conf.getLong(\"hbase.client.sync.wait.timeout.msec\", 60000));\n  }"
        ]
    ],
    "6abe1879dd5776c887bf2ca704c12b3adc75713f": [
        [
            "FanOutOneBlockAsyncDFSOutputSaslHelper::DecryptHandler::channelRead0(ChannelHandlerContext,ByteBuf)",
            " 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882 -\n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  ",
            "    @Override\n    protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception {\n      ByteBuf inBuf;\n      boolean release = false;\n      if (msg.nioBufferCount() == 1) {\n        inBuf = msg;\n      } else {\n        inBuf = ctx.alloc().directBuffer(msg.readableBytes());\n        msg.readBytes(inBuf);\n        release = true;\n      }\n      ByteBuffer inBuffer = inBuf.nioBuffer();\n      ByteBuf outBuf = ctx.alloc().directBuffer(inBuf.readableBytes());\n      ByteBuffer outBuffer = outBuf.nioBuffer();\n      codec.decrypt(inBuffer, outBuffer);\n      outBuf.writerIndex(inBuf.readableBytes());\n      if (release) {\n        inBuf.release();\n      }\n      ctx.fireChannelRead(outBuf);\n    }",
            " 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887 +\n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  ",
            "    @Override\n    protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception {\n      ByteBuf inBuf;\n      boolean release = false;\n      if (msg.nioBufferCount() == 1) {\n        inBuf = msg;\n      } else {\n        inBuf = ctx.alloc().directBuffer(msg.readableBytes());\n        msg.readBytes(inBuf);\n        release = true;\n      }\n      ByteBuffer inBuffer = inBuf.nioBuffer();\n      ByteBuf outBuf = ctx.alloc().directBuffer(inBuf.readableBytes());\n      ByteBuffer outBuffer = outBuf.nioBuffer(0, inBuf.readableBytes());\n      codec.decrypt(inBuffer, outBuffer);\n      outBuf.writerIndex(inBuf.readableBytes());\n      if (release) {\n        inBuf.release();\n      }\n      ctx.fireChannelRead(outBuf);\n    }"
        ],
        [
            "FanOutOneBlockAsyncDFSOutputSaslHelper::EncryptHandler::encode(ChannelHandlerContext,ByteBuf,ByteBuf)",
            " 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923 -\n 924  \n 925  \n 926  \n 927  \n 928  \n 929  ",
            "    @Override\n    protected void encode(ChannelHandlerContext ctx, ByteBuf msg, ByteBuf out) throws Exception {\n      ByteBuf inBuf;\n      boolean release = false;\n      if (msg.nioBufferCount() == 1) {\n        inBuf = msg;\n      } else {\n        inBuf = ctx.alloc().directBuffer(msg.readableBytes());\n        msg.readBytes(inBuf);\n        release = true;\n      }\n      ByteBuffer inBuffer = inBuf.nioBuffer();\n      ByteBuffer outBuffer = out.nioBuffer();\n      codec.encrypt(inBuffer, outBuffer);\n      out.writerIndex(inBuf.readableBytes());\n      if (release) {\n        inBuf.release();\n      }\n    }",
            " 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928 +\n 929  \n 930  \n 931  \n 932  \n 933  \n 934  ",
            "    @Override\n    protected void encode(ChannelHandlerContext ctx, ByteBuf msg, ByteBuf out) throws Exception {\n      ByteBuf inBuf;\n      boolean release = false;\n      if (msg.nioBufferCount() == 1) {\n        inBuf = msg;\n      } else {\n        inBuf = ctx.alloc().directBuffer(msg.readableBytes());\n        msg.readBytes(inBuf);\n        release = true;\n      }\n      ByteBuffer inBuffer = inBuf.nioBuffer();\n      ByteBuffer outBuffer = out.nioBuffer(0, inBuf.readableBytes());\n      codec.encrypt(inBuffer, outBuffer);\n      out.writerIndex(inBuf.readableBytes());\n      if (release) {\n        inBuf.release();\n      }\n    }"
        ],
        [
            "TestSaslFanOutOneBlockAsyncDFSOutput::setUpBeforeClass()",
            " 130  \n 131  \n 132 -\n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    Logger.getLogger(\"org.apache.hadoop.hdfs.StateChange\").setLevel(Level.DEBUG);\n    Logger.getLogger(\"BlockStateChange\").setLevel(Level.DEBUG);\n    EVENT_LOOP_GROUP = new NioEventLoopGroup();\n    TEST_UTIL.getConfiguration().setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY, READ_TIMEOUT_MS);\n    Properties conf = MiniKdc.createConf();\n    conf.put(MiniKdc.DEBUG, true);\n    KDC = new MiniKdc(conf, new File(TEST_UTIL.getDataTestDir(\"kdc\").toUri().getPath()));\n    KDC.start();\n    USERNAME = UserGroupInformation.getLoginUser().getShortUserName();\n    PRINCIPAL = USERNAME + \"/\" + HOST;\n    HTTP_PRINCIPAL = \"HTTP/\" + HOST;\n    KDC.createPrincipal(KEYTAB_FILE, PRINCIPAL, HTTP_PRINCIPAL);\n    setHdfsSecuredConfiguration(TEST_UTIL.getConfiguration());\n    HBaseKerberosUtils.setKeytabFileForTesting(KEYTAB_FILE.getAbsolutePath());\n    HBaseKerberosUtils.setPrincipalForTesting(PRINCIPAL + \"@\" + KDC.getRealm());\n    HBaseKerberosUtils.setSecuredConfiguration(TEST_UTIL.getConfiguration());\n    UserGroupInformation.setConfiguration(TEST_UTIL.getConfiguration());\n  }",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    EVENT_LOOP_GROUP = new NioEventLoopGroup();\n    TEST_UTIL.getConfiguration().setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY, READ_TIMEOUT_MS);\n    Properties conf = MiniKdc.createConf();\n    conf.put(MiniKdc.DEBUG, true);\n    KDC = new MiniKdc(conf, new File(TEST_UTIL.getDataTestDir(\"kdc\").toUri().getPath()));\n    KDC.start();\n    USERNAME = UserGroupInformation.getLoginUser().getShortUserName();\n    PRINCIPAL = USERNAME + \"/\" + HOST;\n    HTTP_PRINCIPAL = \"HTTP/\" + HOST;\n    KDC.createPrincipal(KEYTAB_FILE, PRINCIPAL, HTTP_PRINCIPAL);\n    setHdfsSecuredConfiguration(TEST_UTIL.getConfiguration());\n    HBaseKerberosUtils.setKeytabFileForTesting(KEYTAB_FILE.getAbsolutePath());\n    HBaseKerberosUtils.setPrincipalForTesting(PRINCIPAL + \"@\" + KDC.getRealm());\n    HBaseKerberosUtils.setSecuredConfiguration(TEST_UTIL.getConfiguration());\n    UserGroupInformation.setConfiguration(TEST_UTIL.getConfiguration());\n  }"
        ],
        [
            "TestSaslFanOutOneBlockAsyncDFSOutput::data()",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  ",
            "  @Parameters(name = \"{index}: protection={0}, encryption={1}\")\n  public static Iterable<Object[]> data() {\n    List<Object[]> params = new ArrayList<>();\n    for (String protection : Arrays.asList(\"authentication\", \"integrity\", \"privacy\")) {\n      for (String encryptionAlgorithm : Arrays.asList(\"\", \"3des\", \"rc4\")) {\n        params.add(new Object[] { protection, encryptionAlgorithm });\n      }\n    }\n    return params;\n  }",
            " 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107 +\n 108 +\n 109  \n 110  \n 111  \n 112  ",
            "  @Parameters(name = \"{index}: protection={0}, encryption={1}, cipherSuite={2}\")\n  public static Iterable<Object[]> data() {\n    List<Object[]> params = new ArrayList<>();\n    for (String protection : Arrays.asList(\"authentication\", \"integrity\", \"privacy\")) {\n      for (String encryptionAlgorithm : Arrays.asList(\"\", \"3des\", \"rc4\")) {\n        for (String cipherSuite : Arrays.asList(\"\", AES_CTR_NOPADDING)) {\n          params.add(new Object[] { protection, encryptionAlgorithm, cipherSuite });\n        }\n      }\n    }\n    return params;\n  }"
        ],
        [
            "TestSaslFanOutOneBlockAsyncDFSOutput::setUp()",
            " 161  \n 162  \n 163  \n 164 -\n 165  \n 166 -\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "  @Before\n  public void setUp() throws Exception {\n    TEST_UTIL.getConfiguration().set(\"dfs.data.transfer.protection\", protection);\n    if (StringUtils.isBlank(encryptionAlgorithm)) {\n      TEST_UTIL.getConfiguration().setBoolean(DFS_ENCRYPT_DATA_TRANSFER_KEY, false);\n      TEST_UTIL.getConfiguration().unset(DFS_DATA_ENCRYPTION_ALGORITHM_KEY);\n    } else {\n      TEST_UTIL.getConfiguration().setBoolean(DFS_ENCRYPT_DATA_TRANSFER_KEY, true);\n      TEST_UTIL.getConfiguration().set(DFS_DATA_ENCRYPTION_ALGORITHM_KEY, encryptionAlgorithm);\n    }\n    TEST_UTIL.startMiniDFSCluster(3);\n    FS = TEST_UTIL.getDFSCluster().getFileSystem();\n  }",
            " 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174 +\n 175  \n 176  \n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183  \n 184  \n 185  ",
            "  @Before\n  public void setUp() throws Exception {\n    TEST_UTIL.getConfiguration().set(\"dfs.data.transfer.protection\", protection);\n    if (StringUtils.isBlank(encryptionAlgorithm) && StringUtils.isBlank(cipherSuite)) {\n      TEST_UTIL.getConfiguration().setBoolean(DFS_ENCRYPT_DATA_TRANSFER_KEY, false);\n    } else {\n      TEST_UTIL.getConfiguration().setBoolean(DFS_ENCRYPT_DATA_TRANSFER_KEY, true);\n    }\n    if (StringUtils.isBlank(encryptionAlgorithm)) {\n      TEST_UTIL.getConfiguration().unset(DFS_DATA_ENCRYPTION_ALGORITHM_KEY);\n    } else {\n      TEST_UTIL.getConfiguration().set(DFS_DATA_ENCRYPTION_ALGORITHM_KEY, encryptionAlgorithm);\n    }\n    if (StringUtils.isBlank(cipherSuite)) {\n      TEST_UTIL.getConfiguration().unset(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);\n    } else {\n      TEST_UTIL.getConfiguration().set(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY, cipherSuite);\n    }\n\n    TEST_UTIL.startMiniDFSCluster(3);\n    FS = TEST_UTIL.getDFSCluster().getFileSystem();\n  }"
        ]
    ],
    "15c03fd1c97c271aca6dc30feab35ec0c9f8edbe": [
        [
            "ReplicationPeerConfig::toString()",
            "  96  \n  97  \n  98  \n  99 -\n 100 -\n 101  \n 102  ",
            "  @Override\n  public String toString() {\n    StringBuilder builder = new StringBuilder(\"clusterKey=\").append(clusterKey).append(\",\");\n    builder.append(\"replicationEndpointImpl=\").append(replicationEndpointImpl).append(\",\")\n        .append(\"tableCFs=\").append(tableCFsMap.toString());\n    return builder.toString();\n  }",
            "  96  \n  97  \n  98  \n  99 +\n 100 +\n 101 +\n 102 +\n 103  \n 104  ",
            "  @Override\n  public String toString() {\n    StringBuilder builder = new StringBuilder(\"clusterKey=\").append(clusterKey).append(\",\");\n    builder.append(\"replicationEndpointImpl=\").append(replicationEndpointImpl).append(\",\");\n    if (tableCFsMap != null) {\n      builder.append(\"tableCFs=\").append(tableCFsMap.toString());\n    }\n    return builder.toString();\n  }"
        ]
    ],
    "60b79e2daa41927f5a2e00b78cca855f05048f0a": [
        [
            "MetaTableAccessor::getClosestRegionInfo(Connection,TableName,byte)",
            " 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 -\n 801 -\n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  ",
            "  /**\n   * @return Get closest metatable region row to passed <code>row</code>\n   * @throws java.io.IOException\n   */\n  @Nonnull\n  public static HRegionInfo getClosestRegionInfo(Connection connection,\n      @Nonnull final TableName tableName,\n      @Nonnull final byte[] row)\n      throws IOException {\n    byte[] searchRow = HRegionInfo.createRegionName(tableName, row, HConstants.NINES, false);\n    Scan scan = getMetaScan(connection, 1);\n    scan.setReversed(true);\n    scan.setStartRow(searchRow);\n    try (ResultScanner resultScanner = getMetaHTable(connection).getScanner(scan)) {\n      Result result = resultScanner.next();\n      if (result == null) {\n        throw new TableNotFoundException(\"Cannot find row in META \" +\n            \" for table: \" + tableName + \", row=\" + Bytes.toStringBinary(row));\n      }\n      HRegionInfo regionInfo = getHRegionInfo(result);\n      if (regionInfo == null) {\n        throw new IOException(\"HRegionInfo was null or empty in Meta for \" +\n            tableName + \", row=\" + Bytes.toStringBinary(row));\n      }\n      return regionInfo;\n    }\n  }",
            " 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800 +\n 801 +\n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  ",
            "  /**\n   * @return Get closest metatable region row to passed <code>row</code>\n   * @throws java.io.IOException\n   */\n  @NonNull\n  public static HRegionInfo getClosestRegionInfo(Connection connection,\n      @NonNull final TableName tableName,\n      @NonNull final byte[] row)\n      throws IOException {\n    byte[] searchRow = HRegionInfo.createRegionName(tableName, row, HConstants.NINES, false);\n    Scan scan = getMetaScan(connection, 1);\n    scan.setReversed(true);\n    scan.setStartRow(searchRow);\n    try (ResultScanner resultScanner = getMetaHTable(connection).getScanner(scan)) {\n      Result result = resultScanner.next();\n      if (result == null) {\n        throw new TableNotFoundException(\"Cannot find row in META \" +\n            \" for table: \" + tableName + \", row=\" + Bytes.toStringBinary(row));\n      }\n      HRegionInfo regionInfo = getHRegionInfo(result);\n      if (regionInfo == null) {\n        throw new IOException(\"HRegionInfo was null or empty in Meta for \" +\n            tableName + \", row=\" + Bytes.toStringBinary(row));\n      }\n      return regionInfo;\n    }\n  }"
        ]
    ],
    "0b6eccf4c3273dbe4355e179e94814dc6131d87b": [
        [
            "AccessController::preListSnapshot(ObserverContext,SnapshotDescription)",
            "1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327 -\n1328  \n1329 -\n1330  \n1331  ",
            "  @Override\n  public void preListSnapshot(ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot) throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      // list it, if user is the owner of snapshot\n      // TODO: We are not logging this for audit\n    } else {\n      requirePermission(user, \"listSnapshot\", Action.ADMIN);\n    }\n  }",
            "1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327 +\n1328 +\n1329 +\n1330  \n1331 +\n1332  \n1333  ",
            "  @Override\n  public void preListSnapshot(ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot) throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      // list it, if user is the owner of snapshot\n      AuthResult result = AuthResult.allow(\"listSnapshot \" + snapshot.getName(),\n          \"Snapshot owner check allowed\", user, null, null, null);\n      logResult(result);\n    } else {\n      requirePermission(user, \"listSnapshot \" + snapshot.getName(), Action.ADMIN);\n    }\n  }"
        ],
        [
            "TestAccessController::testSnapshot()",
            "2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056 -\n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  ",
            "  @Test (timeout=180000)\n  public void testSnapshot() throws Exception {\n    Admin admin = TEST_UTIL.getHBaseAdmin();\n    final HTableDescriptor htd = admin.getTableDescriptor(TEST_TABLE);\n    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();\n    builder.setName(TEST_TABLE.getNameAsString() + \"-snapshot\");\n    builder.setTable(TEST_TABLE.getNameAsString());\n    final SnapshotDescription snapshot = builder.build();\n    AccessTestAction snapshotAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n\n    AccessTestAction deleteAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preDeleteSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot);\n        return null;\n      }\n    };\n\n    AccessTestAction restoreAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preRestoreSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n\n    AccessTestAction cloneAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preCloneSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          null, null);\n        return null;\n      }\n    };\n\n    verifyAllowed(snapshotAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(snapshotAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(cloneAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(deleteAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(restoreAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(restoreAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(deleteAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(cloneAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n  }",
            "2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056 +\n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  ",
            "  @Test (timeout=180000)\n  public void testSnapshot() throws Exception {\n    Admin admin = TEST_UTIL.getHBaseAdmin();\n    final HTableDescriptor htd = admin.getTableDescriptor(TEST_TABLE);\n    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();\n    builder.setName(TEST_TABLE.getNameAsString() + \"-snapshot\");\n    builder.setTable(TEST_TABLE.getNameAsString());\n    final SnapshotDescription snapshot = builder.build();\n    AccessTestAction snapshotAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n\n    AccessTestAction deleteAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preDeleteSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot);\n        return null;\n      }\n    };\n\n    AccessTestAction restoreAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preRestoreSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n\n    AccessTestAction cloneAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preCloneSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, null);\n        return null;\n      }\n    };\n\n    verifyAllowed(snapshotAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(snapshotAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(cloneAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(deleteAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(restoreAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(restoreAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    verifyAllowed(deleteAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(cloneAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n  }"
        ],
        [
            "AccessController::preSnapshot(ObserverContext,SnapshotDescription,HTableDescriptor)",
            "1313  \n1314  \n1315  \n1316  \n1317 -\n1318  \n1319  ",
            "  @Override\n  public void preSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    requirePermission(getActiveUser(ctx), \"snapshot\", hTableDescriptor.getTableName(), null, null,\n      Permission.Action.ADMIN);\n  }",
            "1313  \n1314  \n1315  \n1316  \n1317 +\n1318  \n1319  ",
            "  @Override\n  public void preSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    requirePermission(getActiveUser(ctx), \"snapshot \" + snapshot.getName(), hTableDescriptor.getTableName(), null, null,\n      Permission.Action.ADMIN);\n  }"
        ],
        [
            "AccessController::preCloneSnapshot(ObserverContext,SnapshotDescription,HTableDescriptor)",
            "1333  \n1334  \n1335  \n1336  \n1337 -\n1338  ",
            "  @Override\n  public void preCloneSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    requirePermission(getActiveUser(ctx), \"clone\", Action.ADMIN);\n  }",
            "1335  \n1336  \n1337  \n1338  \n1339 +\n1340  ",
            "  @Override\n  public void preCloneSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    requirePermission(getActiveUser(ctx), \"cloneSnapshot \" + snapshot.getName(), Action.ADMIN);\n  }"
        ],
        [
            "AccessController::preDeleteSnapshot(ObserverContext,SnapshotDescription)",
            "1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359 -\n1360  \n1361 -\n1362  \n1363  ",
            "  @Override\n  public void preDeleteSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot) throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      // Snapshot owner is allowed to delete the snapshot\n      // TODO: We are not logging this for audit\n    } else {\n      requirePermission(user, \"deleteSnapshot\", Action.ADMIN);\n    }\n  }",
            "1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361 +\n1362 +\n1363 +\n1364  \n1365 +\n1366  \n1367  ",
            "  @Override\n  public void preDeleteSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot) throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      // Snapshot owner is allowed to delete the snapshot\n      AuthResult result = AuthResult.allow(\"deleteSnapshot \" + snapshot.getName(),\n          \"Snapshot owner check allowed\", user, null, null, null);\n      logResult(result);\n    } else {\n      requirePermission(user, \"deleteSnapshot \" + snapshot.getName(), Action.ADMIN);\n    }\n  }"
        ],
        [
            "AccessController::preRestoreSnapshot(ObserverContext,SnapshotDescription,HTableDescriptor)",
            "1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346 -\n1347  \n1348  \n1349 -\n1350  \n1351  ",
            "  @Override\n  public void preRestoreSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      requirePermission(user, \"restoreSnapshot\", hTableDescriptor.getTableName(), null, null,\n        Permission.Action.ADMIN);\n    } else {\n      requirePermission(user, \"restoreSnapshot\", Action.ADMIN);\n    }\n  }",
            "1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348 +\n1349  \n1350  \n1351 +\n1352  \n1353  ",
            "  @Override\n  public void preRestoreSnapshot(final ObserverContext<MasterCoprocessorEnvironment> ctx,\n      final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)\n      throws IOException {\n    User user = getActiveUser(ctx);\n    if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {\n      requirePermission(user, \"restoreSnapshot \" + snapshot.getName(), hTableDescriptor.getTableName(), null, null,\n        Permission.Action.ADMIN);\n    } else {\n      requirePermission(user, \"restoreSnapshot \" + snapshot.getName(), Action.ADMIN);\n    }\n  }"
        ],
        [
            "TestAccessController::testSnapshotWithOwner()",
            "2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127 -\n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  ",
            "  @Test (timeout=180000)\n  public void testSnapshotWithOwner() throws Exception {\n    Admin admin = TEST_UTIL.getHBaseAdmin();\n    final HTableDescriptor htd = admin.getTableDescriptor(TEST_TABLE);\n    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();\n    builder.setName(TEST_TABLE.getNameAsString() + \"-snapshot\");\n    builder.setTable(TEST_TABLE.getNameAsString());\n    builder.setOwner(USER_OWNER.getName());\n    final SnapshotDescription snapshot = builder.build();\n    AccessTestAction snapshotAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n            snapshot, htd);\n        return null;\n      }\n    };\n    verifyAllowed(snapshotAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(snapshotAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction deleteAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preDeleteSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot);\n        return null;\n      }\n    };\n    verifyAllowed(deleteAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(deleteAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction restoreAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preRestoreSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n    verifyAllowed(restoreAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(restoreAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction cloneAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preCloneSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          null, null);\n        return null;\n      }\n    };\n    // Clone by snapshot owner is not allowed , because clone operation creates a new table,\n    // which needs global admin permission.\n    verifyAllowed(cloneAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(cloneAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n  }",
            "2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127 +\n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  ",
            "  @Test (timeout=180000)\n  public void testSnapshotWithOwner() throws Exception {\n    Admin admin = TEST_UTIL.getHBaseAdmin();\n    final HTableDescriptor htd = admin.getTableDescriptor(TEST_TABLE);\n    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();\n    builder.setName(TEST_TABLE.getNameAsString() + \"-snapshot\");\n    builder.setTable(TEST_TABLE.getNameAsString());\n    builder.setOwner(USER_OWNER.getName());\n    final SnapshotDescription snapshot = builder.build();\n    AccessTestAction snapshotAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n            snapshot, htd);\n        return null;\n      }\n    };\n    verifyAllowed(snapshotAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(snapshotAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction deleteAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preDeleteSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot);\n        return null;\n      }\n    };\n    verifyAllowed(deleteAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(deleteAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction restoreAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preRestoreSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, htd);\n        return null;\n      }\n    };\n    verifyAllowed(restoreAction, SUPERUSER, USER_ADMIN, USER_OWNER, USER_GROUP_ADMIN);\n    verifyDenied(restoreAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_GROUP_READ,\n      USER_GROUP_WRITE, USER_GROUP_CREATE);\n\n    AccessTestAction cloneAction = new AccessTestAction() {\n      @Override\n      public Object run() throws Exception {\n        ACCESS_CONTROLLER.preCloneSnapshot(ObserverContext.createAndPrepare(CP_ENV, null),\n          snapshot, null);\n        return null;\n      }\n    };\n    // Clone by snapshot owner is not allowed , because clone operation creates a new table,\n    // which needs global admin permission.\n    verifyAllowed(cloneAction, SUPERUSER, USER_ADMIN, USER_GROUP_ADMIN);\n    verifyDenied(cloneAction, USER_CREATE, USER_RW, USER_RO, USER_NONE, USER_OWNER,\n      USER_GROUP_READ, USER_GROUP_WRITE, USER_GROUP_CREATE);\n  }"
        ]
    ],
    "c4be87d0508cda019c15f929396d053887bf0e5e": [
        [
            "ExportSnapshot::run(String)",
            " 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980 -\n 981  \n 982  \n 983 -\n 984  \n 985 -\n 986 -\n 987 -\n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  ",
            "  /**\n   * Execute the export snapshot by copying the snapshot metadata, hfiles and wals.\n   * @return 0 on success, and != 0 upon failure.\n   */\n  @Override\n  public int run(String[] args) throws IOException {\n    boolean verifyTarget = true;\n    boolean verifyChecksum = true;\n    String snapshotName = null;\n    String targetName = null;\n    boolean overwrite = false;\n    String filesGroup = null;\n    String filesUser = null;\n    Path outputRoot = null;\n    int bandwidthMB = Integer.MAX_VALUE;\n    int filesMode = 0;\n    int mappers = 0;\n\n    Configuration conf = getConf();\n    Path inputRoot = FSUtils.getRootDir(conf);\n\n    // Process command line args\n    for (int i = 0; i < args.length; i++) {\n      String cmd = args[i];\n      if (cmd.equals(\"-snapshot\")) {\n        snapshotName = args[++i];\n      } else if (cmd.equals(\"-target\")) {\n        targetName = args[++i];\n      } else if (cmd.equals(\"-copy-to\")) {\n        outputRoot = new Path(args[++i]);\n      } else if (cmd.equals(\"-copy-from\")) {\n        inputRoot = new Path(args[++i]);\n        FSUtils.setRootDir(conf, inputRoot);\n      } else if (cmd.equals(\"-no-checksum-verify\")) {\n        verifyChecksum = false;\n      } else if (cmd.equals(\"-no-target-verify\")) {\n        verifyTarget = false;\n      } else if (cmd.equals(\"-mappers\")) {\n        mappers = Integer.parseInt(args[++i]);\n      } else if (cmd.equals(\"-chuser\")) {\n        filesUser = args[++i];\n      } else if (cmd.equals(\"-chgroup\")) {\n        filesGroup = args[++i];\n      } else if (cmd.equals(\"-bandwidth\")) {\n        bandwidthMB = Integer.parseInt(args[++i]);\n      } else if (cmd.equals(\"-chmod\")) {\n        filesMode = Integer.parseInt(args[++i], 8);\n      } else if (cmd.equals(\"-overwrite\")) {\n        overwrite = true;\n      } else if (cmd.equals(\"-h\") || cmd.equals(\"--help\")) {\n        printUsageAndExit();\n      } else {\n        System.err.println(\"UNEXPECTED: \" + cmd);\n        printUsageAndExit();\n      }\n    }\n\n    // Check user options\n    if (snapshotName == null) {\n      System.err.println(\"Snapshot name not provided.\");\n      printUsageAndExit();\n    }\n\n    if (outputRoot == null) {\n      System.err.println(\"Destination file-system not provided.\");\n      printUsageAndExit();\n    }\n\n    if (targetName == null) {\n      targetName = snapshotName;\n    }\n\n    Configuration srcConf = HBaseConfiguration.createClusterConf(conf, null, CONF_SOURCE_PREFIX);\n    srcConf.setBoolean(\"fs.\" + inputRoot.toUri().getScheme() + \".impl.disable.cache\", true);\n    FileSystem inputFs = FileSystem.get(inputRoot.toUri(), srcConf);\n    LOG.debug(\"inputFs=\" + inputFs.getUri().toString() + \" inputRoot=\" + inputRoot);\n    Configuration destConf = HBaseConfiguration.createClusterConf(conf, null, CONF_DEST_PREFIX);\n    destConf.setBoolean(\"fs.\" + outputRoot.toUri().getScheme() + \".impl.disable.cache\", true);\n    FileSystem outputFs = FileSystem.get(outputRoot.toUri(), destConf);\n    LOG.debug(\"outputFs=\" + outputFs.getUri().toString() + \" outputRoot=\" + outputRoot.toString());\n\n    boolean skipTmp = conf.getBoolean(CONF_SKIP_TMP, false);\n\n    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, inputRoot);\n    Path snapshotTmpDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(targetName, outputRoot);\n    Path outputSnapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(targetName, outputRoot);\n    Path initialOutputSnapshotDir = skipTmp ? outputSnapshotDir : snapshotTmpDir;\n\n    // Check if the snapshot already exists\n    if (outputFs.exists(outputSnapshotDir)) {\n      if (overwrite) {\n        if (!outputFs.delete(outputSnapshotDir, true)) {\n          System.err.println(\"Unable to remove existing snapshot directory: \" + outputSnapshotDir);\n          return 1;\n        }\n      } else {\n        System.err.println(\"The snapshot '\" + targetName +\n          \"' already exists in the destination: \" + outputSnapshotDir);\n        return 1;\n      }\n    }\n\n    if (!skipTmp) {\n      // Check if the snapshot already in-progress\n      if (outputFs.exists(snapshotTmpDir)) {\n        if (overwrite) {\n          if (!outputFs.delete(snapshotTmpDir, true)) {\n            System.err.println(\"Unable to remove existing snapshot tmp directory: \"+snapshotTmpDir);\n            return 1;\n          }\n        } else {\n          System.err.println(\"A snapshot with the same name '\"+ targetName +\"' may be in-progress\");\n          System.err.println(\"Please check \"+snapshotTmpDir+\". If the snapshot has completed, \");\n          System.err.println(\"consider removing \"+snapshotTmpDir+\" by using the -overwrite option\");\n          return 1;\n        }\n      }\n    }\n\n    // Step 1 - Copy fs1:/.snapshot/<snapshot> to  fs2:/.snapshot/.tmp/<snapshot>\n    // The snapshot references must be copied before the hfiles otherwise the cleaner\n    // will remove them because they are unreferenced.\n    try {\n      LOG.info(\"Copy Snapshot Manifest\");\n      FileUtil.copy(inputFs, snapshotDir, outputFs, initialOutputSnapshotDir, false, false, conf);\n      if (filesUser != null || filesGroup != null) {\n        setOwner(outputFs, snapshotTmpDir, filesUser, filesGroup, true);\n      }\n      if (filesMode > 0) {\n        setPermission(outputFs, snapshotTmpDir, (short)filesMode, true);\n      }\n    } catch (IOException e) {\n      throw new ExportSnapshotException(\"Failed to copy the snapshot directory: from=\" +\n        snapshotDir + \" to=\" + initialOutputSnapshotDir, e);\n    }\n\n    // Write a new .snapshotinfo if the target name is different from the source name\n    if (!targetName.equals(snapshotName)) {\n      SnapshotDescription snapshotDesc =\n        SnapshotDescriptionUtils.readSnapshotInfo(inputFs, snapshotDir)\n          .toBuilder()\n          .setName(targetName)\n          .build();\n      SnapshotDescriptionUtils.writeSnapshotInfo(snapshotDesc, snapshotTmpDir, outputFs);\n    }\n\n    // Step 2 - Start MR Job to copy files\n    // The snapshot references must be copied before the files otherwise the files gets removed\n    // by the HFileArchiver, since they have no references.\n    try {\n      runCopyJob(inputRoot, outputRoot, snapshotName, snapshotDir, verifyChecksum,\n                 filesUser, filesGroup, filesMode, mappers, bandwidthMB);\n\n      LOG.info(\"Finalize the Snapshot Export\");\n      if (!skipTmp) {\n        // Step 3 - Rename fs2:/.snapshot/.tmp/<snapshot> fs2:/.snapshot/<snapshot>\n        if (!outputFs.rename(snapshotTmpDir, outputSnapshotDir)) {\n          throw new ExportSnapshotException(\"Unable to rename snapshot directory from=\" +\n            snapshotTmpDir + \" to=\" + outputSnapshotDir);\n        }\n      }\n\n      // Step 4 - Verify snapshot integrity\n      if (verifyTarget) {\n        LOG.info(\"Verify snapshot integrity\");\n        verifySnapshot(destConf, outputFs, outputRoot, outputSnapshotDir);\n      }\n\n      LOG.info(\"Export Completed: \" + targetName);\n      return 0;\n    } catch (Exception e) {\n      LOG.error(\"Snapshot export failed\", e);\n      if (!skipTmp) {\n        outputFs.delete(snapshotTmpDir, true);\n      }\n      outputFs.delete(outputSnapshotDir, true);\n      return 1;\n    } finally {\n      IOUtils.closeStream(inputFs);\n      IOUtils.closeStream(outputFs);\n    }\n  }",
            " 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942 +\n 943 +\n 944 +\n 945 +\n 946 +\n 947 +\n 948 +\n 949 +\n 950 +\n 951 +\n 952 +\n 953 +\n 954 +\n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992 +\n 993 +\n 994 +\n 995 +\n 996  \n 997 +\n 998 +\n 999 +\n1000 +\n1001 +\n1002  \n1003  \n1004 +\n1005 +\n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  ",
            "  /**\n   * Execute the export snapshot by copying the snapshot metadata, hfiles and wals.\n   * @return 0 on success, and != 0 upon failure.\n   */\n  @Override\n  public int run(String[] args) throws IOException {\n    boolean verifyTarget = true;\n    boolean verifyChecksum = true;\n    String snapshotName = null;\n    String targetName = null;\n    boolean overwrite = false;\n    String filesGroup = null;\n    String filesUser = null;\n    Path outputRoot = null;\n    int bandwidthMB = Integer.MAX_VALUE;\n    int filesMode = 0;\n    int mappers = 0;\n\n    Configuration conf = getConf();\n    Path inputRoot = FSUtils.getRootDir(conf);\n\n    // Process command line args\n    for (int i = 0; i < args.length; i++) {\n      String cmd = args[i];\n      if (cmd.equals(\"-snapshot\")) {\n        snapshotName = args[++i];\n      } else if (cmd.equals(\"-target\")) {\n        targetName = args[++i];\n      } else if (cmd.equals(\"-copy-to\")) {\n        outputRoot = new Path(args[++i]);\n      } else if (cmd.equals(\"-copy-from\")) {\n        inputRoot = new Path(args[++i]);\n        FSUtils.setRootDir(conf, inputRoot);\n      } else if (cmd.equals(\"-no-checksum-verify\")) {\n        verifyChecksum = false;\n      } else if (cmd.equals(\"-no-target-verify\")) {\n        verifyTarget = false;\n      } else if (cmd.equals(\"-mappers\")) {\n        mappers = Integer.parseInt(args[++i]);\n      } else if (cmd.equals(\"-chuser\")) {\n        filesUser = args[++i];\n      } else if (cmd.equals(\"-chgroup\")) {\n        filesGroup = args[++i];\n      } else if (cmd.equals(\"-bandwidth\")) {\n        bandwidthMB = Integer.parseInt(args[++i]);\n      } else if (cmd.equals(\"-chmod\")) {\n        filesMode = Integer.parseInt(args[++i], 8);\n      } else if (cmd.equals(\"-overwrite\")) {\n        overwrite = true;\n      } else if (cmd.equals(\"-h\") || cmd.equals(\"--help\")) {\n        printUsageAndExit();\n      } else {\n        System.err.println(\"UNEXPECTED: \" + cmd);\n        printUsageAndExit();\n      }\n    }\n\n    // Check user options\n    if (snapshotName == null) {\n      System.err.println(\"Snapshot name not provided.\");\n      printUsageAndExit();\n    }\n\n    if (outputRoot == null) {\n      System.err.println(\"Destination file-system not provided.\");\n      printUsageAndExit();\n    }\n\n    if (targetName == null) {\n      targetName = snapshotName;\n    }\n\n    Configuration srcConf = HBaseConfiguration.createClusterConf(conf, null, CONF_SOURCE_PREFIX);\n    srcConf.setBoolean(\"fs.\" + inputRoot.toUri().getScheme() + \".impl.disable.cache\", true);\n    FileSystem inputFs = FileSystem.get(inputRoot.toUri(), srcConf);\n    LOG.debug(\"inputFs=\" + inputFs.getUri().toString() + \" inputRoot=\" + inputRoot);\n    Configuration destConf = HBaseConfiguration.createClusterConf(conf, null, CONF_DEST_PREFIX);\n    destConf.setBoolean(\"fs.\" + outputRoot.toUri().getScheme() + \".impl.disable.cache\", true);\n    FileSystem outputFs = FileSystem.get(outputRoot.toUri(), destConf);\n    LOG.debug(\"outputFs=\" + outputFs.getUri().toString() + \" outputRoot=\" + outputRoot.toString());\n\n    boolean skipTmp = conf.getBoolean(CONF_SKIP_TMP, false);\n\n    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, inputRoot);\n    Path snapshotTmpDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(targetName, outputRoot);\n    Path outputSnapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(targetName, outputRoot);\n    Path initialOutputSnapshotDir = skipTmp ? outputSnapshotDir : snapshotTmpDir;\n\n    // Find the necessary directory which need to change owner and group\n    Path needSetOwnerDir = SnapshotDescriptionUtils.getSnapshotRootDir(outputRoot);\n    if (outputFs.exists(needSetOwnerDir)) {\n      if (skipTmp) {\n        needSetOwnerDir = outputSnapshotDir;\n      } else {\n        needSetOwnerDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(outputRoot);\n        if (outputFs.exists(needSetOwnerDir)) {\n          needSetOwnerDir = snapshotTmpDir;\n        }\n      }\n    }\n\n    // Check if the snapshot already exists\n    if (outputFs.exists(outputSnapshotDir)) {\n      if (overwrite) {\n        if (!outputFs.delete(outputSnapshotDir, true)) {\n          System.err.println(\"Unable to remove existing snapshot directory: \" + outputSnapshotDir);\n          return 1;\n        }\n      } else {\n        System.err.println(\"The snapshot '\" + targetName +\n          \"' already exists in the destination: \" + outputSnapshotDir);\n        return 1;\n      }\n    }\n\n    if (!skipTmp) {\n      // Check if the snapshot already in-progress\n      if (outputFs.exists(snapshotTmpDir)) {\n        if (overwrite) {\n          if (!outputFs.delete(snapshotTmpDir, true)) {\n            System.err.println(\"Unable to remove existing snapshot tmp directory: \"+snapshotTmpDir);\n            return 1;\n          }\n        } else {\n          System.err.println(\"A snapshot with the same name '\"+ targetName +\"' may be in-progress\");\n          System.err.println(\"Please check \"+snapshotTmpDir+\". If the snapshot has completed, \");\n          System.err.println(\"consider removing \"+snapshotTmpDir+\" by using the -overwrite option\");\n          return 1;\n        }\n      }\n    }\n\n    // Step 1 - Copy fs1:/.snapshot/<snapshot> to  fs2:/.snapshot/.tmp/<snapshot>\n    // The snapshot references must be copied before the hfiles otherwise the cleaner\n    // will remove them because they are unreferenced.\n    try {\n      LOG.info(\"Copy Snapshot Manifest\");\n      FileUtil.copy(inputFs, snapshotDir, outputFs, initialOutputSnapshotDir, false, false, conf);\n    } catch (IOException e) {\n      throw new ExportSnapshotException(\"Failed to copy the snapshot directory: from=\" +\n        snapshotDir + \" to=\" + initialOutputSnapshotDir, e);\n    } finally {\n      if (filesUser != null || filesGroup != null) {\n        LOG.warn((filesUser == null ? \"\" : \"Change the owner of \" + needSetOwnerDir + \" to \"\n            + filesUser)\n            + (filesGroup == null ? \"\" : \", Change the group of \" + needSetOwnerDir + \" to \"\n            + filesGroup));\n        setOwner(outputFs, needSetOwnerDir, filesUser, filesGroup, true);\n      }\n      if (filesMode > 0) {\n        LOG.warn(\"Change the permission of \" + needSetOwnerDir + \" to \" + filesMode);\n        setPermission(outputFs, needSetOwnerDir, (short)filesMode, true);\n      }\n    }\n\n    // Write a new .snapshotinfo if the target name is different from the source name\n    if (!targetName.equals(snapshotName)) {\n      SnapshotDescription snapshotDesc =\n        SnapshotDescriptionUtils.readSnapshotInfo(inputFs, snapshotDir)\n          .toBuilder()\n          .setName(targetName)\n          .build();\n      SnapshotDescriptionUtils.writeSnapshotInfo(snapshotDesc, snapshotTmpDir, outputFs);\n    }\n\n    // Step 2 - Start MR Job to copy files\n    // The snapshot references must be copied before the files otherwise the files gets removed\n    // by the HFileArchiver, since they have no references.\n    try {\n      runCopyJob(inputRoot, outputRoot, snapshotName, snapshotDir, verifyChecksum,\n                 filesUser, filesGroup, filesMode, mappers, bandwidthMB);\n\n      LOG.info(\"Finalize the Snapshot Export\");\n      if (!skipTmp) {\n        // Step 3 - Rename fs2:/.snapshot/.tmp/<snapshot> fs2:/.snapshot/<snapshot>\n        if (!outputFs.rename(snapshotTmpDir, outputSnapshotDir)) {\n          throw new ExportSnapshotException(\"Unable to rename snapshot directory from=\" +\n            snapshotTmpDir + \" to=\" + outputSnapshotDir);\n        }\n      }\n\n      // Step 4 - Verify snapshot integrity\n      if (verifyTarget) {\n        LOG.info(\"Verify snapshot integrity\");\n        verifySnapshot(destConf, outputFs, outputRoot, outputSnapshotDir);\n      }\n\n      LOG.info(\"Export Completed: \" + targetName);\n      return 0;\n    } catch (Exception e) {\n      LOG.error(\"Snapshot export failed\", e);\n      if (!skipTmp) {\n        outputFs.delete(snapshotTmpDir, true);\n      }\n      outputFs.delete(outputSnapshotDir, true);\n      return 1;\n    } finally {\n      IOUtils.closeStream(inputFs);\n      IOUtils.closeStream(outputFs);\n    }\n  }"
        ]
    ],
    "140ce1453ec24b5776c26c23fef866406b6344fe": [
        [
            "AbstractTestShell::setUpBeforeClass()",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  \n  65  \n  66  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setBoolean(\"hbase.quota.enabled\", true);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n\n    // Below settings are necessary for task monitor test.\n    TEST_UTIL.getConfiguration().setInt(HConstants.MASTER_INFO_PORT, 0);\n    TEST_UTIL.getConfiguration().setInt(HConstants.REGIONSERVER_INFO_PORT, 0);\n    TEST_UTIL.getConfiguration().setBoolean(HConstants.REGIONSERVER_INFO_PORT_AUTO, true);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    TEST_UTIL.startMiniCluster();\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.getProvider().setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  \n  65  \n  66  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setBoolean(\"hbase.quota.enabled\", true);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n\n    // Below settings are necessary for task monitor test.\n    TEST_UTIL.getConfiguration().setInt(HConstants.MASTER_INFO_PORT, 0);\n    TEST_UTIL.getConfiguration().setInt(HConstants.REGIONSERVER_INFO_PORT, 0);\n    TEST_UTIL.getConfiguration().setBoolean(HConstants.REGIONSERVER_INFO_PORT_AUTO, true);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    TEST_UTIL.startMiniCluster();\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }"
        ],
        [
            "TestShellNoCluster::setUpBeforeClass()",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42 -\n  43  \n  44  \n  45  \n  46  \n  47  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // no cluster\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.getProvider().setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42 +\n  43  \n  44  \n  45  \n  46  \n  47  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    // no cluster\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    jruby.setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }"
        ],
        [
            "TestShellRSGroups::setUpBeforeClass()",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  \n  83  \n  84  \n  85  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    basePath = System.getProperty(\"basedir\");\n\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    //Setup RegionServer Groups\n    TEST_UTIL.getConfiguration().set(\n        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,\n        RSGroupBasedLoadBalancer.class.getName());\n    TEST_UTIL.getConfiguration().set(\n        CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,\n        RSGroupAdminEndpoint.class.getName());\n\n    TEST_UTIL.startMiniCluster(1,4);\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(basePath+\"/src/main/ruby\");\n    loadPaths.add(basePath+\"/src/test/ruby\");\n    jruby.getProvider().setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81  \n  82  \n  83  \n  84  \n  85  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    basePath = System.getProperty(\"basedir\");\n\n    // Start mini cluster\n    TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    TEST_UTIL.getConfiguration().setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 6);\n    TEST_UTIL.getConfiguration().setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n    TEST_UTIL.getConfiguration().setInt(\"hfile.format.version\", 3);\n    // Security setup configuration\n    SecureTestUtil.enableSecurity(TEST_UTIL.getConfiguration());\n    VisibilityTestUtil.enableVisiblityLabels(TEST_UTIL.getConfiguration());\n\n    //Setup RegionServer Groups\n    TEST_UTIL.getConfiguration().set(\n        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,\n        RSGroupBasedLoadBalancer.class.getName());\n    TEST_UTIL.getConfiguration().set(\n        CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,\n        RSGroupAdminEndpoint.class.getName());\n\n    TEST_UTIL.startMiniCluster(1,4);\n\n    // Configure jruby runtime\n    List<String> loadPaths = new ArrayList<>(2);\n    loadPaths.add(basePath+\"/src/main/ruby\");\n    loadPaths.add(basePath+\"/src/test/ruby\");\n    jruby.setLoadPaths(loadPaths);\n    jruby.put(\"$TEST_CLUSTER\", TEST_UTIL);\n    System.setProperty(\"jruby.jit.logging.verbose\", \"true\");\n    System.setProperty(\"jruby.jit.logging\", \"true\");\n    System.setProperty(\"jruby.native.verbose\", \"true\");\n  }"
        ]
    ],
    "af466bf722450ce6f61ee2bc1e26ab202dea7fb5": [
        [
            "FSDataInputStreamWrapper::unbuffer()",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 -\n 278 -\n 279 -\n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293 -\n 294 -\n 295  \n 296  \n 297 -\n 298 -\n 299 -\n 300 -\n 301  \n 302  \n 303  ",
            "  /**\n   * This will free sockets and file descriptors held by the stream only when the stream implements\n   * org.apache.hadoop.fs.CanUnbuffer. NOT THREAD SAFE. Must be called only when all the clients\n   * using this stream to read the blocks have finished reading. If by chance the stream is\n   * unbuffered and there are clients still holding this stream for read then on next client read\n   * request a new socket will be opened by Datanode without client knowing about it and will serve\n   * its read request. Note: If this socket is idle for some time then the DataNode will close the\n   * socket and the socket will move into CLOSE_WAIT state and on the next client request on this\n   * stream, the current socket will be closed and a new socket will be opened to serve the\n   * requests.\n   */\n  @SuppressWarnings({ \"rawtypes\" })\n  public void unbuffer() {\n    FSDataInputStream stream = this.getStream(this.shouldUseHBaseChecksum());\n    if (stream != null) {\n      InputStream wrappedStream = stream.getWrappedStream();\n      // CanUnbuffer interface was added as part of HDFS-7694 and the fix is available in Hadoop\n      // 2.6.4+ and 2.7.1+ versions only so check whether the stream object implements the\n      // CanUnbuffer interface or not and based on that call the unbuffer api.\n      final Class<? extends InputStream> streamClass = wrappedStream.getClass();\n      if (this.instanceOfCanUnbuffer == null) {\n        // To ensure we compute whether the stream is instance of CanUnbuffer only once.\n        this.instanceOfCanUnbuffer = false;\n        Class<?>[] streamInterfaces = streamClass.getInterfaces();\n        for (Class c : streamInterfaces) {\n          if (c.getCanonicalName().toString().equals(\"org.apache.hadoop.fs.CanUnbuffer\")) {\n            try {\n              this.unbuffer = streamClass.getDeclaredMethod(\"unbuffer\");\n            } catch (NoSuchMethodException | SecurityException e) {\n              LOG.warn(\"Failed to find 'unbuffer' method in class \" + streamClass\n                  + \" . So there may be a TCP socket connection \"\n                  + \"left open in CLOSE_WAIT state.\",\n                e);\n              return;\n            }\n            this.instanceOfCanUnbuffer = true;\n            break;\n          }\n        }\n      }\n      if (this.instanceOfCanUnbuffer) {\n        try {\n          this.unbuffer.invoke(wrappedStream);\n        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {\n          LOG.warn(\"Failed to invoke 'unbuffer' method in class \" + streamClass\n              + \" . So there may be a TCP socket connection left open in CLOSE_WAIT state.\",\n            e);\n        }\n      } else {\n        LOG.warn(\"Failed to find 'unbuffer' method in class \" + streamClass\n            + \" . So there may be a TCP socket connection \"\n            + \"left open in CLOSE_WAIT state. For more details check \"\n            + \"https://issues.apache.org/jira/browse/HBASE-9393\");\n      }\n    }\n  }",
            " 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 +\n 279 +\n 280 +\n 281 +\n 282 +\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295 +\n 296 +\n 297 +\n 298  \n 299  \n 300 +\n 301 +\n 302 +\n 303 +\n 304 +\n 305 +\n 306  \n 307  \n 308  ",
            "  /**\n   * This will free sockets and file descriptors held by the stream only when the stream implements\n   * org.apache.hadoop.fs.CanUnbuffer. NOT THREAD SAFE. Must be called only when all the clients\n   * using this stream to read the blocks have finished reading. If by chance the stream is\n   * unbuffered and there are clients still holding this stream for read then on next client read\n   * request a new socket will be opened by Datanode without client knowing about it and will serve\n   * its read request. Note: If this socket is idle for some time then the DataNode will close the\n   * socket and the socket will move into CLOSE_WAIT state and on the next client request on this\n   * stream, the current socket will be closed and a new socket will be opened to serve the\n   * requests.\n   */\n  @SuppressWarnings({ \"rawtypes\" })\n  public void unbuffer() {\n    FSDataInputStream stream = this.getStream(this.shouldUseHBaseChecksum());\n    if (stream != null) {\n      InputStream wrappedStream = stream.getWrappedStream();\n      // CanUnbuffer interface was added as part of HDFS-7694 and the fix is available in Hadoop\n      // 2.6.4+ and 2.7.1+ versions only so check whether the stream object implements the\n      // CanUnbuffer interface or not and based on that call the unbuffer api.\n      final Class<? extends InputStream> streamClass = wrappedStream.getClass();\n      if (this.instanceOfCanUnbuffer == null) {\n        // To ensure we compute whether the stream is instance of CanUnbuffer only once.\n        this.instanceOfCanUnbuffer = false;\n        Class<?>[] streamInterfaces = streamClass.getInterfaces();\n        for (Class c : streamInterfaces) {\n          if (c.getCanonicalName().toString().equals(\"org.apache.hadoop.fs.CanUnbuffer\")) {\n            try {\n              this.unbuffer = streamClass.getDeclaredMethod(\"unbuffer\");\n            } catch (NoSuchMethodException | SecurityException e) {\n              if (isLogTraceEnabled) {\n                LOG.trace(\"Failed to find 'unbuffer' method in class \" + streamClass\n                    + \" . So there may be a TCP socket connection \"\n                    + \"left open in CLOSE_WAIT state.\", e);\n              }\n              return;\n            }\n            this.instanceOfCanUnbuffer = true;\n            break;\n          }\n        }\n      }\n      if (this.instanceOfCanUnbuffer) {\n        try {\n          this.unbuffer.invoke(wrappedStream);\n        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {\n          if (isLogTraceEnabled) {\n            LOG.trace(\"Failed to invoke 'unbuffer' method in class \" + streamClass\n                + \" . So there may be a TCP socket connection left open in CLOSE_WAIT state.\", e);\n          }\n        }\n      } else {\n        if (isLogTraceEnabled) {\n          LOG.trace(\"Failed to find 'unbuffer' method in class \" + streamClass\n              + \" . So there may be a TCP socket connection \"\n              + \"left open in CLOSE_WAIT state. For more details check \"\n              + \"https://issues.apache.org/jira/browse/HBASE-9393\");\n        }\n      }\n    }\n  }"
        ]
    ],
    "0acfba0e35933d1008c9ade721055dafc137b50f": [
        [
            "HStore::HStore(HRegion,ColumnFamilyDescriptor,Configuration)",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267 -\n 268 -\n 269 -\n 270 -\n 271 -\n 272  \n 273  \n 274  \n 275 -\n 276 -\n 277 -\n 278 -\n 279 -\n 280 -\n 281 -\n 282 -\n 283 -\n 284 -\n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  ",
            "  /**\n   * Constructor\n   * @param region\n   * @param family HColumnDescriptor for this column\n   * @param confParam configuration object\n   * failed.  Can be null.\n   * @throws IOException\n   */\n  protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n      final Configuration confParam) throws IOException {\n\n    this.fs = region.getRegionFileSystem();\n\n    // Assemble the store's home directory and Ensure it exists.\n    fs.createStoreDir(family.getNameAsString());\n    this.region = region;\n    this.family = family;\n    // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n    // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n    // add global config first, then table and cf overrides, then cf metadata.\n    this.conf = new CompoundConfiguration()\n      .add(confParam)\n      .addBytesMap(region.getTableDescriptor().getValues())\n      .addStringMap(family.getConfiguration())\n      .addBytesMap(family.getValues());\n    this.blocksize = family.getBlocksize();\n\n    // set block storage policy for store directory\n    String policyName = family.getStoragePolicy();\n    if (null == policyName) {\n      policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n    }\n    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n\n    this.dataBlockEncoder =\n        new HFileDataBlockEncoderImpl(family.getDataBlockEncoding());\n\n    this.comparator = region.getCellComparator();\n    // used by ScanQueryMatcher\n    long timeToPurgeDeletes =\n        Math.max(conf.getLong(\"hbase.hstore.time.to.purge.deletes\", 0), 0);\n    LOG.trace(\"Time to purge deletes set to \" + timeToPurgeDeletes +\n        \"ms in store \" + this);\n    // Get TTL\n    long ttl = determineTTLFromFamily(family);\n    // Why not just pass a HColumnDescriptor in here altogether?  Even if have\n    // to clone it?\n    scanInfo = new ScanInfo(conf, family, ttl, timeToPurgeDeletes, this.comparator);\n    MemoryCompactionPolicy inMemoryCompaction = family.getInMemoryCompaction();\n    if(inMemoryCompaction == null) {\n      inMemoryCompaction = MemoryCompactionPolicy.valueOf(\n          conf.get(CompactingMemStore.COMPACTING_MEMSTORE_TYPE_KEY,\n              CompactingMemStore.COMPACTING_MEMSTORE_TYPE_DEFAULT));\n    }\n    String className;\n    switch (inMemoryCompaction) {\n      case BASIC :\n      case EAGER :\n        Class<? extends CompactingMemStore> clz = conf.getClass(MEMSTORE_CLASS_NAME,\n          CompactingMemStore.class, CompactingMemStore.class);\n        className = clz.getName();\n        this.memstore = ReflectionUtils.newInstance(clz, new Object[] { conf, this.comparator, this,\n            this.getHRegion().getRegionServicesForStores(), inMemoryCompaction});\n        break;\n      case NONE :\n      default:\n      className = DefaultMemStore.class.getName();\n      this.memstore = ReflectionUtils.newInstance(DefaultMemStore.class,\n        new Object[] { conf, this.comparator });\n    }\n    LOG.info(\"Memstore class name is \" + className);\n    this.offPeakHours = OffPeakHours.getInstance(conf);\n\n    // Setting up cache configuration for this family\n    createCacheConf(family);\n\n    this.verifyBulkLoads = conf.getBoolean(\"hbase.hstore.bulkload.verify\", false);\n\n    this.blockingFileCount =\n        conf.getInt(BLOCKING_STOREFILES_KEY, DEFAULT_BLOCKING_STOREFILE_COUNT);\n    this.compactionCheckMultiplier = conf.getInt(\n        COMPACTCHECKER_INTERVAL_MULTIPLIER_KEY, DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n    if (this.compactionCheckMultiplier <= 0) {\n      LOG.error(\"Compaction check period multiplier must be positive, setting default: \"\n          + DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n      this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n    }\n\n    if (HStore.closeCheckInterval == 0) {\n      HStore.closeCheckInterval = conf.getInt(\n          \"hbase.hstore.close.check.interval\", 10*1000*1000 /* 10 MB */);\n    }\n\n    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n    this.storeEngine.getStoreFileManager().loadFiles(loadStoreFiles());\n\n    // Initialize checksum type from name. The names are CRC32, CRC32C, etc.\n    this.checksumType = getChecksumType(conf);\n    // initilize bytes per checksum\n    this.bytesPerChecksum = getBytesPerChecksum(conf);\n    flushRetriesNumber = conf.getInt(\n        \"hbase.hstore.flush.retries.number\", DEFAULT_FLUSH_RETRIES_NUMBER);\n    pauseTime = conf.getInt(HConstants.HBASE_SERVER_PAUSE, HConstants.DEFAULT_HBASE_SERVER_PAUSE);\n    if (flushRetriesNumber <= 0) {\n      throw new IllegalArgumentException(\n          \"hbase.hstore.flush.retries.number must be > 0, not \"\n              + flushRetriesNumber);\n    }\n    cryptoContext = EncryptionUtil.createEncryptionContext(conf, family);\n  }",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273 +\n 274 +\n 275 +\n 276 +\n 277 +\n 278  \n 279  \n 280  \n 281 +\n 282 +\n 283 +\n 284 +\n 285 +\n 286 +\n 287 +\n 288 +\n 289 +\n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "  /**\n   * Constructor\n   * @param region\n   * @param family HColumnDescriptor for this column\n   * @param confParam configuration object\n   * failed.  Can be null.\n   * @throws IOException\n   */\n  protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n      final Configuration confParam) throws IOException {\n\n    this.fs = region.getRegionFileSystem();\n\n    // Assemble the store's home directory and Ensure it exists.\n    fs.createStoreDir(family.getNameAsString());\n    this.region = region;\n    this.family = family;\n    // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n    // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n    // add global config first, then table and cf overrides, then cf metadata.\n    this.conf = new CompoundConfiguration()\n      .add(confParam)\n      .addBytesMap(region.getTableDescriptor().getValues())\n      .addStringMap(family.getConfiguration())\n      .addBytesMap(family.getValues());\n    this.blocksize = family.getBlocksize();\n\n    // set block storage policy for store directory\n    String policyName = family.getStoragePolicy();\n    if (null == policyName) {\n      policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n    }\n    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n\n    this.dataBlockEncoder =\n        new HFileDataBlockEncoderImpl(family.getDataBlockEncoding());\n\n    this.comparator = region.getCellComparator();\n    // used by ScanQueryMatcher\n    long timeToPurgeDeletes =\n        Math.max(conf.getLong(\"hbase.hstore.time.to.purge.deletes\", 0), 0);\n    LOG.trace(\"Time to purge deletes set to \" + timeToPurgeDeletes +\n        \"ms in store \" + this);\n    // Get TTL\n    long ttl = determineTTLFromFamily(family);\n    // Why not just pass a HColumnDescriptor in here altogether?  Even if have\n    // to clone it?\n    scanInfo = new ScanInfo(conf, family, ttl, timeToPurgeDeletes, this.comparator);\n    MemoryCompactionPolicy inMemoryCompaction = null;\n    if (this.getTableName().isSystemTable()) {\n      inMemoryCompaction = MemoryCompactionPolicy\n          .valueOf(conf.get(\"hbase.systemtables.compacting.memstore.type\", \"NONE\"));\n    } else {\n      inMemoryCompaction = family.getInMemoryCompaction();\n    }\n    if (inMemoryCompaction == null) {\n      inMemoryCompaction =\n          MemoryCompactionPolicy.valueOf(conf.get(CompactingMemStore.COMPACTING_MEMSTORE_TYPE_KEY,\n            CompactingMemStore.COMPACTING_MEMSTORE_TYPE_DEFAULT));\n    }\n    String className;\n    switch (inMemoryCompaction) {\n    case BASIC:\n    case EAGER:\n      Class<? extends CompactingMemStore> clz =\n          conf.getClass(MEMSTORE_CLASS_NAME, CompactingMemStore.class, CompactingMemStore.class);\n      className = clz.getName();\n      this.memstore = ReflectionUtils.newInstance(clz, new Object[] { conf, this.comparator, this,\n          this.getHRegion().getRegionServicesForStores(), inMemoryCompaction });\n      break;\n    case NONE:\n    default:\n      className = DefaultMemStore.class.getName();\n      this.memstore = ReflectionUtils.newInstance(DefaultMemStore.class,\n        new Object[] { conf, this.comparator });\n    }\n    LOG.info(\"Memstore class name is \" + className);\n    this.offPeakHours = OffPeakHours.getInstance(conf);\n\n    // Setting up cache configuration for this family\n    createCacheConf(family);\n\n    this.verifyBulkLoads = conf.getBoolean(\"hbase.hstore.bulkload.verify\", false);\n\n    this.blockingFileCount =\n        conf.getInt(BLOCKING_STOREFILES_KEY, DEFAULT_BLOCKING_STOREFILE_COUNT);\n    this.compactionCheckMultiplier = conf.getInt(\n        COMPACTCHECKER_INTERVAL_MULTIPLIER_KEY, DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n    if (this.compactionCheckMultiplier <= 0) {\n      LOG.error(\"Compaction check period multiplier must be positive, setting default: \"\n          + DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n      this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n    }\n\n    if (HStore.closeCheckInterval == 0) {\n      HStore.closeCheckInterval = conf.getInt(\n          \"hbase.hstore.close.check.interval\", 10*1000*1000 /* 10 MB */);\n    }\n\n    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n    this.storeEngine.getStoreFileManager().loadFiles(loadStoreFiles());\n\n    // Initialize checksum type from name. The names are CRC32, CRC32C, etc.\n    this.checksumType = getChecksumType(conf);\n    // initilize bytes per checksum\n    this.bytesPerChecksum = getBytesPerChecksum(conf);\n    flushRetriesNumber = conf.getInt(\n        \"hbase.hstore.flush.retries.number\", DEFAULT_FLUSH_RETRIES_NUMBER);\n    pauseTime = conf.getInt(HConstants.HBASE_SERVER_PAUSE, HConstants.DEFAULT_HBASE_SERVER_PAUSE);\n    if (flushRetriesNumber <= 0) {\n      throw new IllegalArgumentException(\n          \"hbase.hstore.flush.retries.number must be > 0, not \"\n              + flushRetriesNumber);\n    }\n    cryptoContext = EncryptionUtil.createEncryptionContext(conf, family);\n  }"
        ]
    ],
    "75f512bd717a14e0c7b7bbe5594de9270759706e": [
        [
            "JSONMetricUtil::getCommmand()",
            " 182  \n 183 -\n 184  \n 185  \n 186  ",
            "  public static String getCommmand() throws MalformedObjectNameException,\n  IOException, JSONException {\n    RuntimeMXBean runtimeBean = ManagementFactory.getRuntimeMXBean();\n    return runtimeBean.getSystemProperties().get(\"sun.java.command\");\n  }",
            " 181  \n 182 +\n 183  \n 184  \n 185  ",
            "  public static String getCommmand() throws MalformedObjectNameException,\n  IOException {\n    RuntimeMXBean runtimeBean = ManagementFactory.getRuntimeMXBean();\n    return runtimeBean.getSystemProperties().get(\"sun.java.command\");\n  }"
        ]
    ],
    "7cab24729d4585689af745df179d6ae92b2a6248": [
        [
            "PerformanceEvaluation::printUsage(String,String)",
            "1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830 -\n1831  \n1832  \n1833 -\n1834 -\n1835 -\n1836  \n1837  \n1838  \n1839  \n1840  \n1841 -\n1842 -\n1843  \n1844  \n1845  \n1846  \n1847  \n1848 -\n1849  \n1850  \n1851 -\n1852 -\n1853 -\n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861 -\n1862 -\n1863 -\n1864 -\n1865 -\n1866 -\n1867 -\n1868 -\n1869 -\n1870 -\n1871 -\n1872  \n1873  \n1874  \n1875  \n1876 -\n1877 -\n1878  \n1879 -\n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  ",
            "  protected static void printUsage(final String className, final String message) {\n    if (message != null && message.length() > 0) {\n      System.err.println(message);\n    }\n    System.err.println(\"Usage: java \" + className + \" \\\\\");\n    System.err.println(\"  <OPTIONS> [-D<property=value>]* <command> <nclients>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" nomapred        Run multiple clients using threads \" +\n      \"(rather than use mapreduce)\");\n    System.err.println(\" rows            Rows each client runs. Default: One million\");\n    System.err.println(\" size            Total size in GiB. Mutually exclusive with --rows. \" +\n      \"Default: 1.0.\");\n    System.err.println(\" sampleRate      Execute test on a sample of total \" +\n      \"rows. Only supported by randomRead. Default: 1.0\");\n    System.err.println(\" traceRate       Enable HTrace spans. Initiate tracing every N rows. \" +\n      \"Default: 0\");\n    System.err.println(\" table           Alternate table name. Default: 'TestTable'\");\n    System.err.println(\" multiGet        If >0, when doing RandomRead, perform multiple gets \" +\n      \"instead of single gets. Default: 0\");\n    System.err.println(\" compress        Compression type to use (GZ, LZO, ...). Default: 'NONE'\");\n    System.err.println(\" flushCommits    Used to determine if the test should flush the table. \" +\n      \"Default: false\");\n    System.err.println(\" writeToWAL      Set writeToWAL on puts. Default: True\");\n    System.err.println(\" autoFlush       Set autoFlush on htable. Default: False\");\n    System.err.println(\" oneCon          all the threads share the same connection. Default: False\");\n    System.err.println(\" presplit        Create presplit table. Recommended for accurate perf \" +\n      \"analysis (see guide).  Default: disabled\");\n    System.err.println(\" inmemory        Tries to keep the HFiles of the CF \" +\n      \"inmemory as far as possible. Not guaranteed that reads are always served \" +\n      \"from memory.  Default: false\");\n    System.err.println(\" usetags         Writes tags along with KVs. Use with HFile V3. \" +\n      \"Default: false\");\n    System.err.println(\" numoftags       Specify the no of tags that would be needed. \" +\n       \"This works only if usetags is true.\");\n    System.err.println(\" filterAll       Helps to filter out all the rows on the server side\"\n        + \" there by not returning any thing back to the client.  Helps to check the server side\"\n        + \" performance.  Uses FilterAllFilter internally. \");\n    System.err.println(\" latency         Set to report operation latencies. Default: False\");\n    System.err.println(\" measureAfter    Start to measure the latency once 'measureAfter'\" +\n        \" rows have been treated. Default: 0\");\n    System.err.println(\" bloomFilter      Bloom filter type, one of \" + Arrays.toString(BloomType.values()));\n    System.err.println(\" valueSize       Pass value size to use: Default: 1024\");\n    System.err.println(\" valueRandom     Set if we should vary value size between 0 and \" +\n        \"'valueSize'; set on read for stats on size: Default: Not set.\");\n    System.err.println(\" valueZipf       Set if we should vary value size between 0 and \" +\n        \"'valueSize' in zipf form: Default: Not set.\");\n    System.err.println(\" period          Report every 'period' rows: \" +\n      \"Default: opts.perClientRunRows / 10\");\n    System.err.println(\" multiGet        Batch gets together into groups of N. Only supported \" +\n      \"by randomRead. Default: disabled\");\n    System.err.println(\" addColumns      Adds columns to scans/gets explicitly. Default: true\");\n    System.err.println(\" replicas        Enable region replica testing. Defaults: 1.\");\n    System.err.println(\" cycles          How many times to cycle the test. Defaults: 1.\");\n    System.err.println(\" splitPolicy     Specify a custom RegionSplitPolicy for the table.\");\n    System.err.println(\" randomSleep     Do a random sleep before each get between 0 and entered value. Defaults: 0\");\n    System.err.println(\" columns         Columns to write per row. Default: 1\");\n    System.err.println(\" caching         Scan caching to use. Default: 30\");\n    System.err.println();\n    System.err.println(\" Note: -D properties will be applied to the conf used. \");\n    System.err.println(\"  For example: \");\n    System.err.println(\"   -Dmapreduce.output.fileoutputformat.compress=true\");\n    System.err.println(\"   -Dmapreduce.task.timeout=60000\");\n    System.err.println();\n    System.err.println(\"Command:\");\n    for (CmdDescriptor command : COMMANDS.values()) {\n      System.err.println(String.format(\" %-15s %s\", command.getName(), command.getDescription()));\n    }\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" nclients        Integer. Required. Total number of \" +\n      \"clients (and HRegionServers)\");\n    System.err.println(\"                 running: 1 <= value <= 500\");\n    System.err.println(\"Examples:\");\n    System.err.println(\" To run a single client doing the default 1M sequentialWrites:\");\n    System.err.println(\" $ bin/hbase \" + className + \" sequentialWrite 1\");\n    System.err.println(\" To run 10 clients doing increments over ten rows:\");\n    System.err.println(\" $ bin/hbase \" + className + \" --rows=10 --nomapred increment 10\");\n  }",
            "1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830 +\n1831  \n1832  \n1833 +\n1834  \n1835  \n1836 +\n1837 +\n1838 +\n1839  \n1840  \n1841 +\n1842 +\n1843 +\n1844 +\n1845 +\n1846 +\n1847 +\n1848 +\n1849  \n1850 +\n1851 +\n1852 +\n1853  \n1854  \n1855  \n1856 +\n1857 +\n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866 +\n1867 +\n1868 +\n1869 +\n1870  \n1871  \n1872  \n1873  \n1874  \n1875 +\n1876 +\n1877 +\n1878 +\n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  ",
            "  protected static void printUsage(final String className, final String message) {\n    if (message != null && message.length() > 0) {\n      System.err.println(message);\n    }\n    System.err.println(\"Usage: java \" + className + \" \\\\\");\n    System.err.println(\"  <OPTIONS> [-D<property=value>]* <command> <nclients>\");\n    System.err.println();\n    System.err.println(\"General Options:\");\n    System.err.println(\" nomapred        Run multiple clients using threads \" +\n      \"(rather than use mapreduce)\");\n    System.err.println(\" oneCon          all the threads share the same connection. Default: False\");\n    System.err.println(\" sampleRate      Execute test on a sample of total \" +\n      \"rows. Only supported by randomRead. Default: 1.0\");\n    System.err.println(\" period          Report every 'period' rows: \" +\n      \"Default: opts.perClientRunRows / 10\");\n    System.err.println(\" cycles          How many times to cycle the test. Defaults: 1.\");\n    System.err.println(\" traceRate       Enable HTrace spans. Initiate tracing every N rows. \" +\n      \"Default: 0\");\n    System.err.println(\" latency         Set to report operation latencies. Default: False\");\n    System.err.println(\" measureAfter    Start to measure the latency once 'measureAfter'\" +\n        \" rows have been treated. Default: 0\");\n    System.err.println(\" valueSize       Pass value size to use: Default: 1024\");\n    System.err.println(\" valueRandom     Set if we should vary value size between 0 and \" +\n        \"'valueSize'; set on read for stats on size: Default: Not set.\");\n    System.err.println();\n    System.err.println(\"Table Creation / Write Tests:\");\n    System.err.println(\" table           Alternate table name. Default: 'TestTable'\");\n    System.err.println(\" rows            Rows each client runs. Default: One million\");\n    System.err.println(\" size            Total size in GiB. Mutually exclusive with --rows. \" +\n      \"Default: 1.0.\");\n    System.err.println(\" compress        Compression type to use (GZ, LZO, ...). Default: 'NONE'\");\n    System.err.println(\" flushCommits    Used to determine if the test should flush the table. \" +\n      \"Default: false\");\n    System.err.println(\" valueZipf       Set if we should vary value size between 0 and \" +\n        \"'valueSize' in zipf form: Default: Not set.\");\n    System.err.println(\" writeToWAL      Set writeToWAL on puts. Default: True\");\n    System.err.println(\" autoFlush       Set autoFlush on htable. Default: False\");\n    System.err.println(\" presplit        Create presplit table. Recommended for accurate perf \" +\n      \"analysis (see guide).  Default: disabled\");\n    System.err.println(\" usetags         Writes tags along with KVs. Use with HFile V3. \" +\n      \"Default: false\");\n    System.err.println(\" numoftags       Specify the no of tags that would be needed. \" +\n       \"This works only if usetags is true.\");\n    System.err.println(\" splitPolicy     Specify a custom RegionSplitPolicy for the table.\");\n    System.err.println(\" columns         Columns to write per row. Default: 1\");\n    System.err.println();\n    System.err.println(\"Read Tests:\");\n    System.err.println(\" filterAll       Helps to filter out all the rows on the server side\"\n        + \" there by not returning any thing back to the client.  Helps to check the server side\"\n        + \" performance.  Uses FilterAllFilter internally. \");\n    System.err.println(\" multiGet        Batch gets together into groups of N. Only supported \" +\n      \"by randomRead. Default: disabled\");\n    System.err.println(\" inmemory        Tries to keep the HFiles of the CF \" +\n      \"inmemory as far as possible. Not guaranteed that reads are always served \" +\n      \"from memory.  Default: false\");\n    System.err.println(\" bloomFilter      Bloom filter type, one of \" + Arrays.toString(BloomType.values()));\n    System.err.println(\" addColumns      Adds columns to scans/gets explicitly. Default: true\");\n    System.err.println(\" replicas        Enable region replica testing. Defaults: 1.\");\n    System.err.println(\" randomSleep     Do a random sleep before each get between 0 and entered value. Defaults: 0\");\n    System.err.println(\" caching         Scan caching to use. Default: 30\");\n    System.err.println();\n    System.err.println(\" Note: -D properties will be applied to the conf used. \");\n    System.err.println(\"  For example: \");\n    System.err.println(\"   -Dmapreduce.output.fileoutputformat.compress=true\");\n    System.err.println(\"   -Dmapreduce.task.timeout=60000\");\n    System.err.println();\n    System.err.println(\"Command:\");\n    for (CmdDescriptor command : COMMANDS.values()) {\n      System.err.println(String.format(\" %-15s %s\", command.getName(), command.getDescription()));\n    }\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" nclients        Integer. Required. Total number of \" +\n      \"clients (and HRegionServers)\");\n    System.err.println(\"                 running: 1 <= value <= 500\");\n    System.err.println(\"Examples:\");\n    System.err.println(\" To run a single client doing the default 1M sequentialWrites:\");\n    System.err.println(\" $ bin/hbase \" + className + \" sequentialWrite 1\");\n    System.err.println(\" To run 10 clients doing increments over ten rows:\");\n    System.err.println(\" $ bin/hbase \" + className + \" --rows=10 --nomapred increment 10\");\n  }"
        ],
        [
            "PerformanceEvaluation::printUsage(String)",
            "1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369 -\n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380 -\n1381 -\n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  ",
            "  protected void printUsage(final String message) {\n    if (message != null && message.length() > 0) {\n      System.err.println(message);\n    }\n    System.err.println(\"Usage: java \" + this.getClass().getName() + \" \\\\\");\n    System.err.println(\"  [--nomapred] [--rows=ROWS] [--table=NAME] \\\\\");\n    System.err.println(\"  [--compress=TYPE] [--blockEncoding=TYPE] \" +\n      \"[-D<property=value>]* <command> <nclients>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" nomapred        Run multiple clients using threads \" +\n      \"(rather than use mapreduce)\");\n    System.err.println(\" rows            Rows each client runs. Default: One million\");\n    System.err.println(\" table           Alternate table name. Default: 'TestTable'\");\n    System.err.println(\" compress        Compression type to use (GZ, LZO, ...). Default: 'NONE'\");\n    System.err.println(\" flushCommits    Used to determine if the test should flush the table. \" +\n      \"Default: false\");\n    System.err.println(\" writeToWAL      Set writeToWAL on puts. Default: True\");\n    System.err.println(\" presplit        Create presplit table. Recommended for accurate perf \" +\n      \"analysis (see guide).  Default: disabled\");\n    System.err.println(\" inmemory        Tries to keep the HFiles of the CF inmemory as far as \" +\n      \"possible.  Not guaranteed that reads are always served from inmemory.  Default: false\");\n    System.err.println(\" usetags         Writes tags along with KVs.  Use with HFile V3. \" +\n      \"Default : false\");\n    System.err.println(\" numoftags        Specify the no of tags that would be needed. \" +\n      \"This works only if usetags is true.\");\n    System.err.println();\n    System.err.println(\" Note: -D properties will be applied to the conf used. \");\n    System.err.println(\"  For example: \");\n    System.err.println(\"   -Dmapreduce.output.fileoutputformat.compress=true\");\n    System.err.println(\"   -Dmapreduce.task.timeout=60000\");\n    System.err.println();\n    System.err.println(\"Command:\");\n    for (CmdDescriptor command : commands.values()) {\n      System.err.println(String.format(\" %-15s %s\", command.getName(), command.getDescription()));\n    }\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" nclients      Integer. Required. Total number of \" +\n      \"clients (and HRegionServers)\");\n    System.err.println(\"               running: 1 <= value <= 500\");\n    System.err.println(\"Examples:\");\n    System.err.println(\" To run a single evaluation client:\");\n    System.err.println(\" $ bin/hbase \" + this.getClass().getName()\n        + \" sequentialWrite 1\");\n  }",
            "1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369 +\n1370  \n1371  \n1372  \n1373 +\n1374 +\n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387 +\n1388 +\n1389 +\n1390 +\n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  ",
            "  protected void printUsage(final String message) {\n    if (message != null && message.length() > 0) {\n      System.err.println(message);\n    }\n    System.err.println(\"Usage: java \" + this.getClass().getName() + \" \\\\\");\n    System.err.println(\"  [--nomapred] [--rows=ROWS] [--table=NAME] \\\\\");\n    System.err.println(\"  [--compress=TYPE] [--blockEncoding=TYPE] \" +\n      \"[-D<property=value>]* <command> <nclients>\");\n    System.err.println();\n    System.err.println(\"General Options:\");\n    System.err.println(\" nomapred        Run multiple clients using threads \" +\n      \"(rather than use mapreduce)\");\n    System.err.println(\" rows            Rows each client runs. Default: One million\");\n    System.err.println();\n    System.err.println(\"Table Creation / Write Tests:\");\n    System.err.println(\" table           Alternate table name. Default: 'TestTable'\");\n    System.err.println(\" compress        Compression type to use (GZ, LZO, ...). Default: 'NONE'\");\n    System.err.println(\" flushCommits    Used to determine if the test should flush the table. \" +\n      \"Default: false\");\n    System.err.println(\" writeToWAL      Set writeToWAL on puts. Default: True\");\n    System.err.println(\" presplit        Create presplit table. Recommended for accurate perf \" +\n      \"analysis (see guide).  Default: disabled\");\n    System.err.println(\" usetags         Writes tags along with KVs.  Use with HFile V3. \" +\n      \"Default : false\");\n    System.err.println(\" numoftags        Specify the no of tags that would be needed. \" +\n      \"This works only if usetags is true.\");\n    System.err.println();\n    System.err.println(\"Read Tests:\");\n    System.err.println(\" inmemory        Tries to keep the HFiles of the CF inmemory as far as \" +\n      \"possible.  Not guaranteed that reads are always served from inmemory.  Default: false\");\n    System.err.println();\n    System.err.println(\" Note: -D properties will be applied to the conf used. \");\n    System.err.println(\"  For example: \");\n    System.err.println(\"   -Dmapreduce.output.fileoutputformat.compress=true\");\n    System.err.println(\"   -Dmapreduce.task.timeout=60000\");\n    System.err.println();\n    System.err.println(\"Command:\");\n    for (CmdDescriptor command : commands.values()) {\n      System.err.println(String.format(\" %-15s %s\", command.getName(), command.getDescription()));\n    }\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" nclients      Integer. Required. Total number of \" +\n      \"clients (and HRegionServers)\");\n    System.err.println(\"               running: 1 <= value <= 500\");\n    System.err.println(\"Examples:\");\n    System.err.println(\" To run a single evaluation client:\");\n    System.err.println(\" $ bin/hbase \" + this.getClass().getName()\n        + \" sequentialWrite 1\");\n  }"
        ]
    ],
    "5e395c42941251e1557f5e304eff4f858f320b50": [
        [
            "TestRegionMover::testExclude()",
            " 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190 -\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  ",
            "  /**\n   * To test that we successfully exclude a server from the unloading process We test for the number\n   * of regions on Excluded server and also test that regions are unloaded successfully\n   * @throws Exception\n   */\n  @Test\n  public void testExclude() throws Exception {\n    MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n    FileWriter fos = new FileWriter(\"/tmp/exclude_file\");\n    HRegionServer excludeServer = cluster.getRegionServer(1);\n    String excludeHostname = excludeServer.getServerName().getHostname();\n    int excludeServerPort = excludeServer.getServerName().getPort();\n    int regionsExcludeServer = excludeServer.getNumberOfOnlineRegions();\n    String excludeServerName = excludeHostname + \":\" + Integer.toString(excludeServerPort);\n    fos.write(excludeServerName);\n    fos.close();\n    HRegionServer regionServer = cluster.getRegionServer(0);\n    String rsName = regionServer.getServerName().getHostname();\n    int port = regionServer.getServerName().getPort();\n    String rs = rsName + \":\" + Integer.toString(port);\n    RegionMoverBuilder rmBuilder =\n        new RegionMoverBuilder(rs).ack(true).excludeFile(\"/tmp/exclude_file\");\n    RegionMover rm = rmBuilder.build();\n    rm.setConf(TEST_UTIL.getConfiguration());\n    rm.unload();\n    LOG.info(\"Unloading \" + rs);\n    assertEquals(0, regionServer.getNumberOfOnlineRegions());\n    assertEquals(regionsExcludeServer, cluster.getRegionServer(1).getNumberOfOnlineRegions());\n    LOG.info(\"Before:\" + regionsExcludeServer + \" After:\"\n        + cluster.getRegionServer(1).getNumberOfOnlineRegions());\n  }",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179 +\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  /**\n   * To test that we successfully exclude a server from the unloading process We test for the number\n   * of regions on Excluded server and also test that regions are unloaded successfully\n   * @throws Exception\n   */\n  @Test\n  public void testExclude() throws Exception {\n    MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n    File excludeFile = new File(TEST_UTIL.getDataTestDir().toUri().getPath(), \"exclude_file\");\n    FileWriter fos = new FileWriter(excludeFile);\n    HRegionServer excludeServer = cluster.getRegionServer(1);\n    String excludeHostname = excludeServer.getServerName().getHostname();\n    int excludeServerPort = excludeServer.getServerName().getPort();\n    int regionsExcludeServer = excludeServer.getNumberOfOnlineRegions();\n    String excludeServerName = excludeHostname + \":\" + Integer.toString(excludeServerPort);\n    fos.write(excludeServerName);\n    fos.close();\n    HRegionServer regionServer = cluster.getRegionServer(0);\n    String rsName = regionServer.getServerName().getHostname();\n    int port = regionServer.getServerName().getPort();\n    String rs = rsName + \":\" + Integer.toString(port);\n    RegionMoverBuilder rmBuilder =\n        new RegionMoverBuilder(rs).ack(true).excludeFile(excludeFile.getCanonicalPath());\n    RegionMover rm = rmBuilder.build();\n    rm.setConf(TEST_UTIL.getConfiguration());\n    rm.unload();\n    LOG.info(\"Unloading \" + rs);\n    assertEquals(0, regionServer.getNumberOfOnlineRegions());\n    assertEquals(regionsExcludeServer, cluster.getRegionServer(1).getNumberOfOnlineRegions());\n    LOG.info(\"Before:\" + regionsExcludeServer + \" After:\"\n        + cluster.getRegionServer(1).getNumberOfOnlineRegions());\n  }"
        ]
    ],
    "60e19f60a9767db6b145cf9884d2295412baffe0": [
        [
            "RSRpcServices::scan(RpcController,ScanRequest)",
            "2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545 -\n2546  \n2547 -\n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803  \n2804  \n2805  \n2806  \n2807  \n2808  \n2809  \n2810  \n2811  \n2812  \n2813  \n2814  \n2815  \n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  \n2829  \n2830  \n2831  \n2832  \n2833  \n2834  \n2835  \n2836  \n2837  \n2838  \n2839  \n2840  \n2841  \n2842  \n2843  \n2844  \n2845  \n2846  \n2847  \n2848  \n2849  \n2850  \n2851  \n2852  \n2853  \n2854  \n2855  \n2856  \n2857  \n2858  \n2859  \n2860  \n2861  \n2862  \n2863  \n2864  \n2865  \n2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  ",
            "  /**\n   * Scan data in a table.\n   *\n   * @param controller the RPC controller\n   * @param request the scan request\n   * @throws ServiceException\n   */\n  @Override\n  public ScanResponse scan(final RpcController controller, final ScanRequest request)\n  throws ServiceException {\n    OperationQuota quota = null;\n    Leases.Lease lease = null;\n    String scannerName = null;\n    try {\n      if (!request.hasScannerId() && !request.hasScan()) {\n        throw new DoNotRetryIOException(\n          \"Missing required input: scannerId or scan\");\n      }\n      long scannerId = -1;\n      if (request.hasScannerId()) {\n        scannerId = request.getScannerId();\n        scannerName = String.valueOf(scannerId);\n      }\n      try {\n        checkOpen();\n      } catch (IOException e) {\n        // If checkOpen failed, server not running or filesystem gone,\n        // cancel this lease; filesystem is gone or we're closing or something.\n        if (scannerName != null) {\n          LOG.debug(\"Server shutting down and client tried to access missing scanner \"\n            + scannerName);\n          if (regionServer.leases != null) {\n            try {\n              regionServer.leases.cancelLease(scannerName);\n            } catch (LeaseException le) {\n              // No problem, ignore\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Un-able to cancel lease of scanner. It could already be closed.\");\n              }\n             }\n          }\n        }\n        throw e;\n      }\n      requestCount.increment();\n      rpcScanRequestCount.increment();\n\n      int ttl = 0;\n      Region region = null;\n      RegionScanner scanner = null;\n      RegionScannerHolder rsh = null;\n      boolean moreResults = true;\n      boolean closeScanner = false;\n      boolean isSmallScan = false;\n      ScanResponse.Builder builder = ScanResponse.newBuilder();\n      if (request.hasCloseScanner()) {\n        closeScanner = request.getCloseScanner();\n      }\n      int rows = closeScanner ? 0 : 1;\n      if (request.hasNumberOfRows()) {\n        rows = request.getNumberOfRows();\n      }\n      if (request.hasScannerId()) {\n        rsh = scanners.get(scannerName);\n        if (rsh == null) {\n          LOG.info(\"Client tried to access missing scanner \" + scannerName);\n          throw new UnknownScannerException(\n            \"Name: \" + scannerName + \", already closed?\");\n        }\n        scanner = rsh.s;\n        HRegionInfo hri = scanner.getRegionInfo();\n        region = regionServer.getRegion(hri.getRegionName());\n        if (region != rsh.r) { // Yes, should be the same instance\n          throw new NotServingRegionException(\"Region was re-opened after the scanner\"\n            + scannerName + \" was created: \" + hri.getRegionNameAsString());\n        }\n      } else {\n        region = getRegion(request.getRegion());\n        ClientProtos.Scan protoScan = request.getScan();\n        boolean isLoadingCfsOnDemandSet = protoScan.hasLoadColumnFamiliesOnDemand();\n        Scan scan = ProtobufUtil.toScan(protoScan);\n        // if the request doesn't set this, get the default region setting.\n        if (!isLoadingCfsOnDemandSet) {\n          scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());\n        }\n\n        isSmallScan = scan.isSmall();\n        if (!scan.hasFamilies()) {\n          // Adding all families to scanner\n          for (byte[] family: region.getTableDesc().getFamiliesKeys()) {\n            scan.addFamily(family);\n          }\n        }\n\n        if (region.getCoprocessorHost() != null) {\n          scanner = region.getCoprocessorHost().preScannerOpen(scan);\n        }\n        if (scanner == null) {\n          scanner = region.getScanner(scan);\n        }\n        if (region.getCoprocessorHost() != null) {\n          scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);\n        }\n        scannerId = this.scannerIdGen.incrementAndGet();\n        scannerName = String.valueOf(scannerId);\n        rsh = addScanner(scannerName, scanner, region);\n        ttl = this.scannerLeaseTimeoutPeriod;\n      }\n      assert scanner != null;\n      if (request.hasRenew() && request.getRenew()) {\n        rsh = scanners.get(scannerName);\n        lease = regionServer.leases.removeLease(scannerName);\n        if (lease != null && rsh != null) {\n          regionServer.leases.addLease(lease);\n          // Increment the nextCallSeq value which is the next expected from client.\n          rsh.incNextCallSeq();\n        }\n        return builder.build();\n      }\n      RpcCallContext context = RpcServer.getCurrentCall();\n      Object lastBlock = null;\n\n      quota = getQuotaManager().checkQuota(region, OperationQuota.OperationType.SCAN);\n      long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n\n      if (rows > 0) {\n        // if nextCallSeq does not match throw Exception straight away. This needs to be\n        // performed even before checking of Lease.\n        // See HBASE-5974\n        if (request.hasNextCallSeq()) {\n          if (rsh != null) {\n            if (request.getNextCallSeq() != rsh.getNextCallSeq()) {\n              throw new OutOfOrderScannerNextException(\n                \"Expected nextCallSeq: \" + rsh.getNextCallSeq()\n                + \" But the nextCallSeq got from client: \" + request.getNextCallSeq() +\n                \"; request=\" + TextFormat.shortDebugString(request));\n            }\n            // Increment the nextCallSeq value which is the next expected from client.\n            rsh.incNextCallSeq();\n          }\n        }\n        try {\n          // Remove lease while its being processed in server; protects against case\n          // where processing of request takes > lease expiration time.\n          lease = regionServer.leases.removeLease(scannerName);\n          List<Result> results = new ArrayList<Result>();\n\n          boolean done = false;\n          // Call coprocessor. Get region info from scanner.\n          if (region != null && region.getCoprocessorHost() != null) {\n            Boolean bypass = region.getCoprocessorHost().preScannerNext(\n              scanner, results, rows);\n            if (!results.isEmpty()) {\n              for (Result r : results) {\n                lastBlock = addSize(context, r, lastBlock);\n              }\n            }\n            if (bypass != null && bypass.booleanValue()) {\n              done = true;\n            }\n          }\n\n          if (!done) {\n            long maxResultSize = Math.min(scanner.getMaxResultSize(), maxQuotaResultSize);\n            if (maxResultSize <= 0) {\n              maxResultSize = maxQuotaResultSize;\n            }\n            // This is cells inside a row. Default size is 10 so if many versions or many cfs,\n            // then we'll resize. Resizings show in profiler. Set it higher than 10. For now\n            // arbitrary 32. TODO: keep record of general size of results being returned.\n            List<Cell> values = new ArrayList<Cell>(32);\n            region.startRegionOperation(Operation.SCAN);\n            try {\n              int i = 0;\n              long before = EnvironmentEdgeManager.currentTime();\n              synchronized(scanner) {\n                boolean stale = (region.getRegionInfo().getReplicaId() != 0);\n                boolean clientHandlesPartials =\n                    request.hasClientHandlesPartials() && request.getClientHandlesPartials();\n                boolean clientHandlesHeartbeats =\n                    request.hasClientHandlesHeartbeats() && request.getClientHandlesHeartbeats();\n\n                // On the server side we must ensure that the correct ordering of partial results is\n                // returned to the client to allow them to properly reconstruct the partial results.\n                // If the coprocessor host is adding to the result list, we cannot guarantee the\n                // correct ordering of partial results and so we prevent partial results from being\n                // formed.\n                boolean serverGuaranteesOrderOfPartials = results.isEmpty();\n                boolean allowPartialResults =\n                    clientHandlesPartials && serverGuaranteesOrderOfPartials && !isSmallScan;\n                boolean moreRows = false;\n\n                // Heartbeat messages occur when the processing of the ScanRequest is exceeds a\n                // certain time threshold on the server. When the time threshold is exceeded, the\n                // server stops the scan and sends back whatever Results it has accumulated within\n                // that time period (may be empty). Since heartbeat messages have the potential to\n                // create partial Results (in the event that the timeout occurs in the middle of a\n                // row), we must only generate heartbeat messages when the client can handle both\n                // heartbeats AND partials\n                boolean allowHeartbeatMessages = clientHandlesHeartbeats && allowPartialResults;\n\n                // Default value of timeLimit is negative to indicate no timeLimit should be\n                // enforced.\n                long timeLimit = -1;\n\n                // Set the time limit to be half of the more restrictive timeout value (one of the\n                // timeout values must be positive). In the event that both values are positive, the\n                // more restrictive of the two is used to calculate the limit.\n                if (allowHeartbeatMessages && (scannerLeaseTimeoutPeriod > 0 || rpcTimeout > 0)) {\n                  long timeLimitDelta;\n                  if (scannerLeaseTimeoutPeriod > 0 && rpcTimeout > 0) {\n                    timeLimitDelta = Math.min(scannerLeaseTimeoutPeriod, rpcTimeout);\n                  } else {\n                    timeLimitDelta =\n                        scannerLeaseTimeoutPeriod > 0 ? scannerLeaseTimeoutPeriod : rpcTimeout;\n                  }\n                  // Use half of whichever timeout value was more restrictive... But don't allow\n                  // the time limit to be less than the allowable minimum (could cause an\n                  // immediatate timeout before scanning any data).\n                  timeLimitDelta = Math.max(timeLimitDelta / 2, minimumScanTimeLimitDelta);\n                  timeLimit = System.currentTimeMillis() + timeLimitDelta;\n                }\n\n                final LimitScope sizeScope =\n                    allowPartialResults ? LimitScope.BETWEEN_CELLS : LimitScope.BETWEEN_ROWS;\n                final LimitScope timeScope =\n                    allowHeartbeatMessages ? LimitScope.BETWEEN_CELLS : LimitScope.BETWEEN_ROWS;\n\n                boolean trackMetrics =\n                    request.hasTrackScanMetrics() && request.getTrackScanMetrics();\n\n                // Configure with limits for this RPC. Set keep progress true since size progress\n                // towards size limit should be kept between calls to nextRaw\n                ScannerContext.Builder contextBuilder = ScannerContext.newBuilder(true);\n                contextBuilder.setSizeLimit(sizeScope, maxResultSize);\n                contextBuilder.setBatchLimit(scanner.getBatch());\n                contextBuilder.setTimeLimit(timeScope, timeLimit);\n                contextBuilder.setTrackMetrics(trackMetrics);\n                ScannerContext scannerContext = contextBuilder.build();\n                boolean limitReached = false;\n                while (i < rows) {\n                  // Reset the batch progress to 0 before every call to RegionScanner#nextRaw. The\n                  // batch limit is a limit on the number of cells per Result. Thus, if progress is\n                  // being tracked (i.e. scannerContext.keepProgress() is true) then we need to\n                  // reset the batch progress between nextRaw invocations since we don't want the\n                  // batch progress from previous calls to affect future calls\n                  scannerContext.setBatchProgress(0);\n\n                  // Collect values to be returned here\n                  moreRows = scanner.nextRaw(values, scannerContext);\n\n                  if (!values.isEmpty()) {\n                    final boolean partial = scannerContext.partialResultFormed();\n                    Result r = Result.create(values, null, stale, partial);\n                    lastBlock = addSize(context, r, lastBlock);\n                    results.add(r);\n                    i++;\n                  }\n\n                  boolean sizeLimitReached = scannerContext.checkSizeLimit(LimitScope.BETWEEN_ROWS);\n                  boolean timeLimitReached = scannerContext.checkTimeLimit(LimitScope.BETWEEN_ROWS);\n                  boolean rowLimitReached = i >= rows;\n                  limitReached = sizeLimitReached || timeLimitReached || rowLimitReached;\n\n                  if (limitReached || !moreRows) {\n                    if (LOG.isTraceEnabled()) {\n                      LOG.trace(\"Done scanning. limitReached: \" + limitReached + \" moreRows: \"\n                          + moreRows + \" scannerContext: \" + scannerContext);\n                    }\n                    // We only want to mark a ScanResponse as a heartbeat message in the event that\n                    // there are more values to be read server side. If there aren't more values,\n                    // marking it as a heartbeat is wasteful because the client will need to issue\n                    // another ScanRequest only to realize that they already have all the values\n                    if (moreRows) {\n                      // Heartbeat messages occur when the time limit has been reached.\n                      builder.setHeartbeatMessage(timeLimitReached);\n                    }\n                    break;\n                  }\n                  values.clear();\n                }\n\n                if (limitReached || moreRows) {\n                  // We stopped prematurely\n                  builder.setMoreResultsInRegion(true);\n                } else {\n                  // We didn't get a single batch\n                  builder.setMoreResultsInRegion(false);\n                }\n\n                // Check to see if the client requested that we track metrics server side. If the\n                // client requested metrics, retrieve the metrics from the scanner context.\n                if (trackMetrics) {\n                  Map<String, Long> metrics = scannerContext.getMetrics().getMetricsMap();\n                  ScanMetrics.Builder metricBuilder = ScanMetrics.newBuilder();\n                  NameInt64Pair.Builder pairBuilder = NameInt64Pair.newBuilder();\n\n                  for (Entry<String, Long> entry : metrics.entrySet()) {\n                    pairBuilder.setName(entry.getKey());\n                    pairBuilder.setValue(entry.getValue());\n                    metricBuilder.addMetrics(pairBuilder.build());\n                  }\n\n                  builder.setScanMetrics(metricBuilder.build());\n                }\n              }\n              region.updateReadRequestsCount(i);\n              long end = EnvironmentEdgeManager.currentTime();\n              long responseCellSize = context != null ? context.getResponseCellSize() : 0;\n              region.getMetrics().updateScanSize(responseCellSize);\n              region.getMetrics().updateScanTime(end - before);\n              if (regionServer.metricsRegionServer != null) {\n                regionServer.metricsRegionServer.updateScanSize(responseCellSize);\n                regionServer.metricsRegionServer.updateScanTime(end - before);\n              }\n            } finally {\n              region.closeRegionOperation();\n            }\n            // coprocessor postNext hook\n            if (region != null && region.getCoprocessorHost() != null) {\n              region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);\n            }\n          }\n\n          quota.addScanResult(results);\n\n          // If the scanner's filter - if any - is done with the scan\n          // and wants to tell the client to stop the scan. This is done by passing\n          // a null result, and setting moreResults to false.\n          if (scanner.isFilterDone() && results.isEmpty()) {\n            moreResults = false;\n            results = null;\n          } else {\n            addResults(builder, results, controller,\n                RegionReplicaUtil.isDefaultReplica(region.getRegionInfo()),\n                isClientCellBlockSupport(context));\n          }\n        } catch (IOException e) {\n          // if we have an exception on scanner next and we are using the callSeq\n          // we should rollback because the client will retry with the same callSeq\n          // and get an OutOfOrderScannerNextException if we don't do so.\n          if (rsh != null && request.hasNextCallSeq()) {\n            rsh.rollbackNextCallSeq();\n          }\n          throw e;\n        } finally {\n          if (context != null) {\n            context.setCallBack(rsh.shippedCallback);\n          }\n          // Adding resets expiration time on lease.\n          if (scanners.containsKey(scannerName)) {\n            ttl = this.scannerLeaseTimeoutPeriod;\n            // When context != null, adding back the lease will be done in callback set above.\n            if (context == null) {\n              if (lease != null) regionServer.leases.addLease(lease);\n            }\n          }\n        }\n      }\n\n      if (!moreResults || closeScanner) {\n        ttl = 0;\n        moreResults = false;\n        if (region != null && region.getCoprocessorHost() != null) {\n          if (region.getCoprocessorHost().preScannerClose(scanner)) {\n            return builder.build(); // bypass\n          }\n        }\n        rsh = scanners.remove(scannerName);\n        if (rsh != null) {\n          if (context != null) {\n            context.setCallBack(rsh.closeCallBack);\n          } else {\n            rsh.s.close();\n          }\n          try {\n            regionServer.leases.cancelLease(scannerName);\n          } catch (LeaseException le) {\n            // No problem, ignore\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Un-able to cancel lease of scanner. It could already be closed.\");\n            }\n          }\n          if (region != null && region.getCoprocessorHost() != null) {\n            region.getCoprocessorHost().postScannerClose(scanner);\n          }\n        }\n      }\n\n      if (ttl > 0) {\n        builder.setTtl(ttl);\n      }\n      builder.setScannerId(scannerId);\n      builder.setMoreResults(moreResults);\n      return builder.build();\n    } catch (IOException ie) {\n      if (scannerName != null && ie instanceof NotServingRegionException) {\n        RegionScannerHolder rsh = scanners.remove(scannerName);\n        if (rsh != null) {\n          try {\n            RegionScanner scanner = rsh.s;\n            LOG.warn(scannerName + \" encountered \" + ie.getMessage() + \", closing ...\");\n            scanner.close();\n            regionServer.leases.cancelLease(scannerName);\n          } catch (IOException e) {\n           LOG.warn(\"Getting exception closing \" + scannerName, e);\n          }\n        }\n      }\n      throw new ServiceException(ie);\n    } finally {\n      if (quota != null) {\n        quota.close();\n      }\n    }\n  }",
            "2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545 +\n2546  \n2547 +\n2548 +\n2549 +\n2550 +\n2551 +\n2552 +\n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  \n2790  \n2791  \n2792  \n2793  \n2794  \n2795  \n2796  \n2797  \n2798  \n2799  \n2800  \n2801  \n2802  \n2803  \n2804  \n2805  \n2806  \n2807  \n2808  \n2809  \n2810  \n2811  \n2812  \n2813  \n2814  \n2815  \n2816  \n2817  \n2818  \n2819  \n2820  \n2821  \n2822  \n2823  \n2824  \n2825  \n2826  \n2827  \n2828  \n2829  \n2830  \n2831  \n2832  \n2833  \n2834  \n2835  \n2836  \n2837  \n2838  \n2839  \n2840  \n2841  \n2842  \n2843  \n2844  \n2845  \n2846  \n2847  \n2848  \n2849  \n2850  \n2851  \n2852  \n2853  \n2854  \n2855  \n2856  \n2857  \n2858  \n2859  \n2860  \n2861  \n2862  \n2863  \n2864  \n2865  \n2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899  \n2900  ",
            "  /**\n   * Scan data in a table.\n   *\n   * @param controller the RPC controller\n   * @param request the scan request\n   * @throws ServiceException\n   */\n  @Override\n  public ScanResponse scan(final RpcController controller, final ScanRequest request)\n  throws ServiceException {\n    OperationQuota quota = null;\n    Leases.Lease lease = null;\n    String scannerName = null;\n    try {\n      if (!request.hasScannerId() && !request.hasScan()) {\n        throw new DoNotRetryIOException(\n          \"Missing required input: scannerId or scan\");\n      }\n      long scannerId = -1;\n      if (request.hasScannerId()) {\n        scannerId = request.getScannerId();\n        scannerName = String.valueOf(scannerId);\n      }\n      try {\n        checkOpen();\n      } catch (IOException e) {\n        // If checkOpen failed, server not running or filesystem gone,\n        // cancel this lease; filesystem is gone or we're closing or something.\n        if (scannerName != null) {\n          LOG.debug(\"Server shutting down and client tried to access missing scanner \"\n            + scannerName);\n          if (regionServer.leases != null) {\n            try {\n              regionServer.leases.cancelLease(scannerName);\n            } catch (LeaseException le) {\n              // No problem, ignore\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"Un-able to cancel lease of scanner. It could already be closed.\");\n              }\n             }\n          }\n        }\n        throw e;\n      }\n      requestCount.increment();\n      rpcScanRequestCount.increment();\n\n      int ttl = 0;\n      Region region = null;\n      RegionScanner scanner = null;\n      RegionScannerHolder rsh = null;\n      boolean moreResults = true;\n      boolean closeScanner = false;\n      boolean isSmallScan = false;\n      ScanResponse.Builder builder = ScanResponse.newBuilder();\n      if (request.hasCloseScanner()) {\n        closeScanner = request.getCloseScanner();\n      }\n      int rows = closeScanner ? 0 : 1;\n      if (request.hasNumberOfRows()) {\n        rows = request.getNumberOfRows();\n      }\n      if (request.hasScannerId()) {\n        rsh = scanners.get(scannerName);\n        if (rsh == null) {\n          LOG.warn(\"Client tried to access missing scanner \" + scannerName);\n          throw new UnknownScannerException(\n            \"Unknown scanner '\" + scannerName + \"'. This can happen due to any of the following \"\n                + \"reasons: a) Scanner id given is wrong, b) Scanner lease expired because of \"\n                + \"long wait between consecutive client checkins, c) Server may be closing down, \"\n                + \"d) RegionServer restart during upgrade.\\nIf the issue is due to reason (b), a \"\n                + \"possible fix would be increasing the value of\"\n                + \"'hbase.client.scanner.timeout.period' configuration.\");\n        }\n        scanner = rsh.s;\n        HRegionInfo hri = scanner.getRegionInfo();\n        region = regionServer.getRegion(hri.getRegionName());\n        if (region != rsh.r) { // Yes, should be the same instance\n          throw new NotServingRegionException(\"Region was re-opened after the scanner\"\n            + scannerName + \" was created: \" + hri.getRegionNameAsString());\n        }\n      } else {\n        region = getRegion(request.getRegion());\n        ClientProtos.Scan protoScan = request.getScan();\n        boolean isLoadingCfsOnDemandSet = protoScan.hasLoadColumnFamiliesOnDemand();\n        Scan scan = ProtobufUtil.toScan(protoScan);\n        // if the request doesn't set this, get the default region setting.\n        if (!isLoadingCfsOnDemandSet) {\n          scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());\n        }\n\n        isSmallScan = scan.isSmall();\n        if (!scan.hasFamilies()) {\n          // Adding all families to scanner\n          for (byte[] family: region.getTableDesc().getFamiliesKeys()) {\n            scan.addFamily(family);\n          }\n        }\n\n        if (region.getCoprocessorHost() != null) {\n          scanner = region.getCoprocessorHost().preScannerOpen(scan);\n        }\n        if (scanner == null) {\n          scanner = region.getScanner(scan);\n        }\n        if (region.getCoprocessorHost() != null) {\n          scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);\n        }\n        scannerId = this.scannerIdGen.incrementAndGet();\n        scannerName = String.valueOf(scannerId);\n        rsh = addScanner(scannerName, scanner, region);\n        ttl = this.scannerLeaseTimeoutPeriod;\n      }\n      assert scanner != null;\n      if (request.hasRenew() && request.getRenew()) {\n        rsh = scanners.get(scannerName);\n        lease = regionServer.leases.removeLease(scannerName);\n        if (lease != null && rsh != null) {\n          regionServer.leases.addLease(lease);\n          // Increment the nextCallSeq value which is the next expected from client.\n          rsh.incNextCallSeq();\n        }\n        return builder.build();\n      }\n      RpcCallContext context = RpcServer.getCurrentCall();\n      Object lastBlock = null;\n\n      quota = getQuotaManager().checkQuota(region, OperationQuota.OperationType.SCAN);\n      long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n\n      if (rows > 0) {\n        // if nextCallSeq does not match throw Exception straight away. This needs to be\n        // performed even before checking of Lease.\n        // See HBASE-5974\n        if (request.hasNextCallSeq()) {\n          if (rsh != null) {\n            if (request.getNextCallSeq() != rsh.getNextCallSeq()) {\n              throw new OutOfOrderScannerNextException(\n                \"Expected nextCallSeq: \" + rsh.getNextCallSeq()\n                + \" But the nextCallSeq got from client: \" + request.getNextCallSeq() +\n                \"; request=\" + TextFormat.shortDebugString(request));\n            }\n            // Increment the nextCallSeq value which is the next expected from client.\n            rsh.incNextCallSeq();\n          }\n        }\n        try {\n          // Remove lease while its being processed in server; protects against case\n          // where processing of request takes > lease expiration time.\n          lease = regionServer.leases.removeLease(scannerName);\n          List<Result> results = new ArrayList<Result>();\n\n          boolean done = false;\n          // Call coprocessor. Get region info from scanner.\n          if (region != null && region.getCoprocessorHost() != null) {\n            Boolean bypass = region.getCoprocessorHost().preScannerNext(\n              scanner, results, rows);\n            if (!results.isEmpty()) {\n              for (Result r : results) {\n                lastBlock = addSize(context, r, lastBlock);\n              }\n            }\n            if (bypass != null && bypass.booleanValue()) {\n              done = true;\n            }\n          }\n\n          if (!done) {\n            long maxResultSize = Math.min(scanner.getMaxResultSize(), maxQuotaResultSize);\n            if (maxResultSize <= 0) {\n              maxResultSize = maxQuotaResultSize;\n            }\n            // This is cells inside a row. Default size is 10 so if many versions or many cfs,\n            // then we'll resize. Resizings show in profiler. Set it higher than 10. For now\n            // arbitrary 32. TODO: keep record of general size of results being returned.\n            List<Cell> values = new ArrayList<Cell>(32);\n            region.startRegionOperation(Operation.SCAN);\n            try {\n              int i = 0;\n              long before = EnvironmentEdgeManager.currentTime();\n              synchronized(scanner) {\n                boolean stale = (region.getRegionInfo().getReplicaId() != 0);\n                boolean clientHandlesPartials =\n                    request.hasClientHandlesPartials() && request.getClientHandlesPartials();\n                boolean clientHandlesHeartbeats =\n                    request.hasClientHandlesHeartbeats() && request.getClientHandlesHeartbeats();\n\n                // On the server side we must ensure that the correct ordering of partial results is\n                // returned to the client to allow them to properly reconstruct the partial results.\n                // If the coprocessor host is adding to the result list, we cannot guarantee the\n                // correct ordering of partial results and so we prevent partial results from being\n                // formed.\n                boolean serverGuaranteesOrderOfPartials = results.isEmpty();\n                boolean allowPartialResults =\n                    clientHandlesPartials && serverGuaranteesOrderOfPartials && !isSmallScan;\n                boolean moreRows = false;\n\n                // Heartbeat messages occur when the processing of the ScanRequest is exceeds a\n                // certain time threshold on the server. When the time threshold is exceeded, the\n                // server stops the scan and sends back whatever Results it has accumulated within\n                // that time period (may be empty). Since heartbeat messages have the potential to\n                // create partial Results (in the event that the timeout occurs in the middle of a\n                // row), we must only generate heartbeat messages when the client can handle both\n                // heartbeats AND partials\n                boolean allowHeartbeatMessages = clientHandlesHeartbeats && allowPartialResults;\n\n                // Default value of timeLimit is negative to indicate no timeLimit should be\n                // enforced.\n                long timeLimit = -1;\n\n                // Set the time limit to be half of the more restrictive timeout value (one of the\n                // timeout values must be positive). In the event that both values are positive, the\n                // more restrictive of the two is used to calculate the limit.\n                if (allowHeartbeatMessages && (scannerLeaseTimeoutPeriod > 0 || rpcTimeout > 0)) {\n                  long timeLimitDelta;\n                  if (scannerLeaseTimeoutPeriod > 0 && rpcTimeout > 0) {\n                    timeLimitDelta = Math.min(scannerLeaseTimeoutPeriod, rpcTimeout);\n                  } else {\n                    timeLimitDelta =\n                        scannerLeaseTimeoutPeriod > 0 ? scannerLeaseTimeoutPeriod : rpcTimeout;\n                  }\n                  // Use half of whichever timeout value was more restrictive... But don't allow\n                  // the time limit to be less than the allowable minimum (could cause an\n                  // immediatate timeout before scanning any data).\n                  timeLimitDelta = Math.max(timeLimitDelta / 2, minimumScanTimeLimitDelta);\n                  timeLimit = System.currentTimeMillis() + timeLimitDelta;\n                }\n\n                final LimitScope sizeScope =\n                    allowPartialResults ? LimitScope.BETWEEN_CELLS : LimitScope.BETWEEN_ROWS;\n                final LimitScope timeScope =\n                    allowHeartbeatMessages ? LimitScope.BETWEEN_CELLS : LimitScope.BETWEEN_ROWS;\n\n                boolean trackMetrics =\n                    request.hasTrackScanMetrics() && request.getTrackScanMetrics();\n\n                // Configure with limits for this RPC. Set keep progress true since size progress\n                // towards size limit should be kept between calls to nextRaw\n                ScannerContext.Builder contextBuilder = ScannerContext.newBuilder(true);\n                contextBuilder.setSizeLimit(sizeScope, maxResultSize);\n                contextBuilder.setBatchLimit(scanner.getBatch());\n                contextBuilder.setTimeLimit(timeScope, timeLimit);\n                contextBuilder.setTrackMetrics(trackMetrics);\n                ScannerContext scannerContext = contextBuilder.build();\n                boolean limitReached = false;\n                while (i < rows) {\n                  // Reset the batch progress to 0 before every call to RegionScanner#nextRaw. The\n                  // batch limit is a limit on the number of cells per Result. Thus, if progress is\n                  // being tracked (i.e. scannerContext.keepProgress() is true) then we need to\n                  // reset the batch progress between nextRaw invocations since we don't want the\n                  // batch progress from previous calls to affect future calls\n                  scannerContext.setBatchProgress(0);\n\n                  // Collect values to be returned here\n                  moreRows = scanner.nextRaw(values, scannerContext);\n\n                  if (!values.isEmpty()) {\n                    final boolean partial = scannerContext.partialResultFormed();\n                    Result r = Result.create(values, null, stale, partial);\n                    lastBlock = addSize(context, r, lastBlock);\n                    results.add(r);\n                    i++;\n                  }\n\n                  boolean sizeLimitReached = scannerContext.checkSizeLimit(LimitScope.BETWEEN_ROWS);\n                  boolean timeLimitReached = scannerContext.checkTimeLimit(LimitScope.BETWEEN_ROWS);\n                  boolean rowLimitReached = i >= rows;\n                  limitReached = sizeLimitReached || timeLimitReached || rowLimitReached;\n\n                  if (limitReached || !moreRows) {\n                    if (LOG.isTraceEnabled()) {\n                      LOG.trace(\"Done scanning. limitReached: \" + limitReached + \" moreRows: \"\n                          + moreRows + \" scannerContext: \" + scannerContext);\n                    }\n                    // We only want to mark a ScanResponse as a heartbeat message in the event that\n                    // there are more values to be read server side. If there aren't more values,\n                    // marking it as a heartbeat is wasteful because the client will need to issue\n                    // another ScanRequest only to realize that they already have all the values\n                    if (moreRows) {\n                      // Heartbeat messages occur when the time limit has been reached.\n                      builder.setHeartbeatMessage(timeLimitReached);\n                    }\n                    break;\n                  }\n                  values.clear();\n                }\n\n                if (limitReached || moreRows) {\n                  // We stopped prematurely\n                  builder.setMoreResultsInRegion(true);\n                } else {\n                  // We didn't get a single batch\n                  builder.setMoreResultsInRegion(false);\n                }\n\n                // Check to see if the client requested that we track metrics server side. If the\n                // client requested metrics, retrieve the metrics from the scanner context.\n                if (trackMetrics) {\n                  Map<String, Long> metrics = scannerContext.getMetrics().getMetricsMap();\n                  ScanMetrics.Builder metricBuilder = ScanMetrics.newBuilder();\n                  NameInt64Pair.Builder pairBuilder = NameInt64Pair.newBuilder();\n\n                  for (Entry<String, Long> entry : metrics.entrySet()) {\n                    pairBuilder.setName(entry.getKey());\n                    pairBuilder.setValue(entry.getValue());\n                    metricBuilder.addMetrics(pairBuilder.build());\n                  }\n\n                  builder.setScanMetrics(metricBuilder.build());\n                }\n              }\n              region.updateReadRequestsCount(i);\n              long end = EnvironmentEdgeManager.currentTime();\n              long responseCellSize = context != null ? context.getResponseCellSize() : 0;\n              region.getMetrics().updateScanSize(responseCellSize);\n              region.getMetrics().updateScanTime(end - before);\n              if (regionServer.metricsRegionServer != null) {\n                regionServer.metricsRegionServer.updateScanSize(responseCellSize);\n                regionServer.metricsRegionServer.updateScanTime(end - before);\n              }\n            } finally {\n              region.closeRegionOperation();\n            }\n            // coprocessor postNext hook\n            if (region != null && region.getCoprocessorHost() != null) {\n              region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);\n            }\n          }\n\n          quota.addScanResult(results);\n\n          // If the scanner's filter - if any - is done with the scan\n          // and wants to tell the client to stop the scan. This is done by passing\n          // a null result, and setting moreResults to false.\n          if (scanner.isFilterDone() && results.isEmpty()) {\n            moreResults = false;\n            results = null;\n          } else {\n            addResults(builder, results, controller,\n                RegionReplicaUtil.isDefaultReplica(region.getRegionInfo()),\n                isClientCellBlockSupport(context));\n          }\n        } catch (IOException e) {\n          // if we have an exception on scanner next and we are using the callSeq\n          // we should rollback because the client will retry with the same callSeq\n          // and get an OutOfOrderScannerNextException if we don't do so.\n          if (rsh != null && request.hasNextCallSeq()) {\n            rsh.rollbackNextCallSeq();\n          }\n          throw e;\n        } finally {\n          if (context != null) {\n            context.setCallBack(rsh.shippedCallback);\n          }\n          // Adding resets expiration time on lease.\n          if (scanners.containsKey(scannerName)) {\n            ttl = this.scannerLeaseTimeoutPeriod;\n            // When context != null, adding back the lease will be done in callback set above.\n            if (context == null) {\n              if (lease != null) regionServer.leases.addLease(lease);\n            }\n          }\n        }\n      }\n\n      if (!moreResults || closeScanner) {\n        ttl = 0;\n        moreResults = false;\n        if (region != null && region.getCoprocessorHost() != null) {\n          if (region.getCoprocessorHost().preScannerClose(scanner)) {\n            return builder.build(); // bypass\n          }\n        }\n        rsh = scanners.remove(scannerName);\n        if (rsh != null) {\n          if (context != null) {\n            context.setCallBack(rsh.closeCallBack);\n          } else {\n            rsh.s.close();\n          }\n          try {\n            regionServer.leases.cancelLease(scannerName);\n          } catch (LeaseException le) {\n            // No problem, ignore\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Un-able to cancel lease of scanner. It could already be closed.\");\n            }\n          }\n          if (region != null && region.getCoprocessorHost() != null) {\n            region.getCoprocessorHost().postScannerClose(scanner);\n          }\n        }\n      }\n\n      if (ttl > 0) {\n        builder.setTtl(ttl);\n      }\n      builder.setScannerId(scannerId);\n      builder.setMoreResults(moreResults);\n      return builder.build();\n    } catch (IOException ie) {\n      if (scannerName != null && ie instanceof NotServingRegionException) {\n        RegionScannerHolder rsh = scanners.remove(scannerName);\n        if (rsh != null) {\n          try {\n            RegionScanner scanner = rsh.s;\n            LOG.warn(scannerName + \" encountered \" + ie.getMessage() + \", closing ...\");\n            scanner.close();\n            regionServer.leases.cancelLease(scannerName);\n          } catch (IOException e) {\n           LOG.warn(\"Getting exception closing \" + scannerName, e);\n          }\n        }\n      }\n      throw new ServiceException(ie);\n    } finally {\n      if (quota != null) {\n        quota.close();\n      }\n    }\n  }"
        ]
    ],
    "4b541d63804e752f536560f7d96e222c0ffd877c": [
        [
            "TestRSGroupsBase::getTableServerRegionMap()",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "  public Map<TableName, Map<ServerName, List<String>>> getTableServerRegionMap()\n      throws IOException {\n    Map<TableName, Map<ServerName, List<String>>> map = Maps.newTreeMap();\n    ClusterStatus status = TEST_UTIL.getHBaseClusterInterface().getClusterStatus();\n    for(ServerName serverName : status.getServers()) {\n      for(RegionLoad rl : status.getLoad(serverName).getRegionsLoad().values()) {\n        TableName tableName = HRegionInfo.getTable(rl.getName());\n        if(!map.containsKey(tableName)) {\n          map.put(tableName, new TreeMap<>());\n        }\n        if(!map.get(tableName).containsKey(serverName)) {\n          map.get(tableName).put(serverName, new LinkedList<>());\n        }\n        map.get(tableName).get(serverName).add(rl.getNameAsString());\n      }\n    }\n    return map;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "  public Map<TableName, Map<ServerName, List<String>>> getTableServerRegionMap()\n      throws IOException {\n    Map<TableName, Map<ServerName, List<String>>> map = Maps.newTreeMap();\n    ClusterStatus status = TEST_UTIL.getHBaseClusterInterface().getClusterStatus();\n    for(ServerName serverName : status.getServers()) {\n      for(RegionLoad rl : status.getLoad(serverName).getRegionsLoad().values()) {\n        TableName tableName = null;\n        try {\n          tableName = HRegionInfo.getTable(rl.getName());\n        } catch (IllegalArgumentException e) {\n          LOG.warn(\"Failed parse a table name from regionname=\" +\n              Bytes.toStringBinary(rl.getName()));\n          continue;\n        }\n        if(!map.containsKey(tableName)) {\n          map.put(tableName, new TreeMap<>());\n        }\n        if(!map.get(tableName).containsKey(serverName)) {\n          map.get(tableName).put(serverName, new LinkedList<>());\n        }\n        map.get(tableName).get(serverName).add(rl.getNameAsString());\n      }\n    }\n    return map;\n  }"
        ]
    ],
    "c463e9c8403645597141b18cb9d502623fa7f104": [
        [
            "TableMapReduceUtil::addHBaseDependencyJars(Configuration)",
            " 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823 -\n 824  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to external tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n    addDependencyJarsForClasses(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.ipc.RpcServer.class,                   // hbase-server\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.JobUtil.class,               // hbase-hadoop2-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-mapreduce\n      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class,  // hbase-metrics\n      org.apache.hadoop.hbase.metrics.Snapshot.class,                // hbase-metrics-api\n      org.apache.zookeeper.ZooKeeper.class,\n      org.apache.hadoop.hbase.shaded.io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations.class,\n      org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class,\n      org.apache.commons.lang3.ArrayUtils.class);\n  }",
            " 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823 +\n 824 +\n 825 +\n 826 +\n 827  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to external tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n    addDependencyJarsForClasses(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.ipc.RpcServer.class,                   // hbase-server\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.JobUtil.class,               // hbase-hadoop2-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-mapreduce\n      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class,  // hbase-metrics\n      org.apache.hadoop.hbase.metrics.Snapshot.class,                // hbase-metrics-api\n      org.apache.zookeeper.ZooKeeper.class,\n      org.apache.hadoop.hbase.shaded.io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations.class,\n      org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class,\n      org.apache.commons.lang3.ArrayUtils.class,\n      com.fasterxml.jackson.databind.ObjectMapper.class,\n      com.fasterxml.jackson.core.Versioned.class,\n      com.fasterxml.jackson.annotation.JsonView.class);\n  }"
        ]
    ],
    "777b653b45e54c89dd69e86eff2b261054465623": [
        [
            "ConnectionImplementation::locateRegionInMeta(TableName,byte,boolean,boolean,int)",
            " 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791 -\n 792  \n 793  \n 794  \n 795 -\n 796 -\n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845 -\n 846 -\n 847 -\n 848 -\n 849  \n 850  \n 851 -\n 852 -\n 853 -\n 854  \n 855  \n 856  \n 857  \n 858 -\n 859 -\n 860 -\n 861 -\n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888 -\n 889 -\n 890 -\n 891 -\n 892 -\n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  ",
            "  private RegionLocations locateRegionInMeta(TableName tableName, byte[] row,\n                 boolean useCache, boolean retry, int replicaId) throws IOException {\n\n    // If we are supposed to be using the cache, look in the cache to see if\n    // we already have the region.\n    if (useCache) {\n      RegionLocations locations = getCachedLocation(tableName, row);\n      if (locations != null && locations.getRegionLocation(replicaId) != null) {\n        return locations;\n      }\n    }\n\n    // build the key of the meta region we should be looking for.\n    // the extra 9's on the end are necessary to allow \"exact\" matches\n    // without knowing the precise region names.\n    byte[] metaKey = RegionInfo.createRegionName(tableName, row, HConstants.NINES, false);\n\n    Scan s = new Scan();\n    s.setReversed(true);\n    s.withStartRow(metaKey);\n    s.addFamily(HConstants.CATALOG_FAMILY);\n\n    if (this.useMetaReplicas) {\n      s.setConsistency(Consistency.TIMELINE);\n    }\n\n    int maxAttempts = (retry ? numTries : 1);\n\n    for (int tries = 0; true; tries++) {\n      if (tries >= maxAttempts) {\n        throw new NoServerForRegionException(\"Unable to find region for \"\n            + Bytes.toStringBinary(row) + \" in \" + tableName +\n            \" after \" + tries + \" tries.\");\n      }\n      if (useCache) {\n        RegionLocations locations = getCachedLocation(tableName, row);\n        if (locations != null && locations.getRegionLocation(replicaId) != null) {\n          return locations;\n        }\n      } else {\n        // If we are not supposed to be using the cache, delete any existing cached location\n        // so it won't interfere.\n        // We are only supposed to clean the cache for the specific replicaId\n        metaCache.clearCache(tableName, row, replicaId);\n      }\n\n      // Query the meta region\n      long pauseBase = this.pause;\n      try {\n        Result regionInfoRow = null;\n        s.resetMvccReadPoint();\n        s.setOneRowLimit();\n        try (ReversedClientScanner rcs =\n            new ReversedClientScanner(conf, s, TableName.META_TABLE_NAME, this, rpcCallerFactory,\n                rpcControllerFactory, getMetaLookupPool(), metaReplicaCallTimeoutScanInMicroSecond)) {\n          regionInfoRow = rcs.next();\n        }\n\n        if (regionInfoRow == null) {\n          throw new TableNotFoundException(tableName);\n        }\n        // convert the row result into the HRegionLocation we need!\n        RegionLocations locations = MetaTableAccessor.getRegionLocations(regionInfoRow);\n        if (locations == null || locations.getRegionLocation(replicaId) == null) {\n          throw new IOException(\"HRegionInfo was null in \" +\n            tableName + \", row=\" + regionInfoRow);\n        }\n        RegionInfo regionInfo = locations.getRegionLocation(replicaId).getRegion();\n        if (regionInfo == null) {\n          throw new IOException(\"HRegionInfo was null or empty in \" +\n            TableName.META_TABLE_NAME + \", row=\" + regionInfoRow);\n        }\n\n        // possible we got a region of a different table...\n        if (!regionInfo.getTable().equals(tableName)) {\n          throw new TableNotFoundException(\n            \"Region of '\" + regionInfo.getRegionNameAsString() + \"' is expected in the table of '\" + tableName + \"', \" +\n            \"but hbase:meta says it is in the table of '\" + regionInfo.getTable() + \"'. \" +\n            \"hbase:meta might be damaged.\");\n        }\n        if (regionInfo.isSplit()) {\n          throw new RegionOfflineException(\"the only available region for\" +\n            \" the required row is a split parent,\" +\n            \" the daughters should be online soon: \" +\n            regionInfo.getRegionNameAsString());\n        }\n        if (regionInfo.isOffline()) {\n          throw new RegionOfflineException(\"the region is offline, could\" +\n            \" be caused by a disable table call: \" +\n            regionInfo.getRegionNameAsString());\n        }\n\n        ServerName serverName = locations.getRegionLocation(replicaId).getServerName();\n        if (serverName == null) {\n          throw new NoServerForRegionException(\"No server address listed \" +\n            \"in \" + TableName.META_TABLE_NAME + \" for region \" +\n            regionInfo.getRegionNameAsString() + \" containing row \" +\n            Bytes.toStringBinary(row));\n        }\n\n        if (isDeadServer(serverName)){\n          throw new RegionServerStoppedException(\"hbase:meta says the region \"+\n              regionInfo.getRegionNameAsString()+\" is managed by the server \" + serverName +\n              \", but it is dead.\");\n        }\n        // Instantiate the location\n        cacheLocation(tableName, locations);\n        return locations;\n      } catch (TableNotFoundException e) {\n        // if we got this error, probably means the table just plain doesn't\n        // exist. rethrow the error immediately. this should always be coming\n        // from the HTable constructor.\n        throw e;\n      } catch (IOException e) {\n        ExceptionUtil.rethrowIfInterrupt(e);\n        if (e instanceof RemoteException) {\n          e = ((RemoteException)e).unwrapRemoteException();\n        }\n        if (e instanceof CallQueueTooBigException) {\n          // Give a special check on CallQueueTooBigException, see #HBASE-17114\n          pauseBase = this.pauseForCQTBE;\n        }\n        if (tries < maxAttempts - 1) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"locateRegionInMeta parentTable=\" +\n                TableName.META_TABLE_NAME + \", metaLocation=\" +\n              \", attempt=\" + tries + \" of \" +\n              maxAttempts + \" failed; retrying after sleep of \" +\n              ConnectionUtils.getPauseTime(pauseBase, tries) + \" because: \" + e.getMessage());\n          }\n        } else {\n          throw e;\n        }\n        // Only relocate the parent region if necessary\n        if(!(e instanceof RegionOfflineException ||\n            e instanceof NoServerForRegionException)) {\n          relocateRegion(TableName.META_TABLE_NAME, metaKey, replicaId);\n        }\n      }\n      try{\n        Thread.sleep(ConnectionUtils.getPauseTime(pauseBase, tries));\n      } catch (InterruptedException e) {\n        throw new InterruptedIOException(\"Giving up trying to location region in \" +\n          \"meta: thread is interrupted.\");\n      }\n    }\n  }",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798 +\n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814 +\n 815  \n 816 +\n 817 +\n 818 +\n 819 +\n 820 +\n 821 +\n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 +\n 855 +\n 856 +\n 857  \n 858  \n 859 +\n 860 +\n 861  \n 862  \n 863  \n 864  \n 865 +\n 866 +\n 867 +\n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894 +\n 895 +\n 896 +\n 897 +\n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907 +\n 908 +\n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  ",
            "  private RegionLocations locateRegionInMeta(TableName tableName, byte[] row,\n                 boolean useCache, boolean retry, int replicaId) throws IOException {\n\n    // If we are supposed to be using the cache, look in the cache to see if\n    // we already have the region.\n    if (useCache) {\n      RegionLocations locations = getCachedLocation(tableName, row);\n      if (locations != null && locations.getRegionLocation(replicaId) != null) {\n        return locations;\n      }\n    }\n\n    // build the key of the meta region we should be looking for.\n    // the extra 9's on the end are necessary to allow \"exact\" matches\n    // without knowing the precise region names.\n    byte[] metaKey = RegionInfo.createRegionName(tableName, row, HConstants.NINES, false);\n\n    Scan s = new Scan();\n    s.setReversed(true);\n    s.withStartRow(metaKey);\n    s.addFamily(HConstants.CATALOG_FAMILY);\n\n    if (this.useMetaReplicas) {\n      s.setConsistency(Consistency.TIMELINE);\n    }\n\n    int maxAttempts = (retry ? numTries : 1);\n    for (int tries = 0; true; tries++) {\n      if (tries >= maxAttempts) {\n        throw new NoServerForRegionException(\"Unable to find region for \"\n            + Bytes.toStringBinary(row) + \" in \" + tableName + \" after \" + tries + \" tries.\");\n      }\n      if (useCache) {\n        RegionLocations locations = getCachedLocation(tableName, row);\n        if (locations != null && locations.getRegionLocation(replicaId) != null) {\n          return locations;\n        }\n      } else {\n        // If we are not supposed to be using the cache, delete any existing cached location\n        // so it won't interfere.\n        // We are only supposed to clean the cache for the specific replicaId\n        metaCache.clearCache(tableName, row, replicaId);\n      }\n\n      // Query the meta region\n      long pauseBase = this.pause;\n      userRegionLock.lock();\n      try {\n        if (useCache) {// re-check cache after get lock\n          RegionLocations locations = getCachedLocation(tableName, row);\n          if (locations != null && locations.getRegionLocation(replicaId) != null) {\n            return locations;\n          }\n        }\n        Result regionInfoRow = null;\n        s.resetMvccReadPoint();\n        s.setOneRowLimit();\n        try (ReversedClientScanner rcs =\n            new ReversedClientScanner(conf, s, TableName.META_TABLE_NAME, this, rpcCallerFactory,\n                rpcControllerFactory, getMetaLookupPool(), metaReplicaCallTimeoutScanInMicroSecond)) {\n          regionInfoRow = rcs.next();\n        }\n\n        if (regionInfoRow == null) {\n          throw new TableNotFoundException(tableName);\n        }\n        // convert the row result into the HRegionLocation we need!\n        RegionLocations locations = MetaTableAccessor.getRegionLocations(regionInfoRow);\n        if (locations == null || locations.getRegionLocation(replicaId) == null) {\n          throw new IOException(\"HRegionInfo was null in \" +\n            tableName + \", row=\" + regionInfoRow);\n        }\n        RegionInfo regionInfo = locations.getRegionLocation(replicaId).getRegion();\n        if (regionInfo == null) {\n          throw new IOException(\"HRegionInfo was null or empty in \" +\n            TableName.META_TABLE_NAME + \", row=\" + regionInfoRow);\n        }\n\n        // possible we got a region of a different table...\n        if (!regionInfo.getTable().equals(tableName)) {\n          throw new TableNotFoundException(\n            \"Region of '\" + regionInfo.getRegionNameAsString() + \"' is expected in the table of '\" + tableName + \"', \" +\n            \"but hbase:meta says it is in the table of '\" + regionInfo.getTable() + \"'. \" +\n            \"hbase:meta might be damaged.\");\n        }\n        if (regionInfo.isSplit()) {\n          throw new RegionOfflineException(\n              \"the only available region for the required row is a split parent,\"\n                  + \" the daughters should be online soon: \" + regionInfo.getRegionNameAsString());\n        }\n        if (regionInfo.isOffline()) {\n          throw new RegionOfflineException(\"the region is offline, could\"\n              + \" be caused by a disable table call: \" + regionInfo.getRegionNameAsString());\n        }\n\n        ServerName serverName = locations.getRegionLocation(replicaId).getServerName();\n        if (serverName == null) {\n          throw new NoServerForRegionException(\"No server address listed in \"\n              + TableName.META_TABLE_NAME + \" for region \" + regionInfo.getRegionNameAsString()\n              + \" containing row \" + Bytes.toStringBinary(row));\n        }\n\n        if (isDeadServer(serverName)){\n          throw new RegionServerStoppedException(\"hbase:meta says the region \"+\n              regionInfo.getRegionNameAsString()+\" is managed by the server \" + serverName +\n              \", but it is dead.\");\n        }\n        // Instantiate the location\n        cacheLocation(tableName, locations);\n        return locations;\n      } catch (TableNotFoundException e) {\n        // if we got this error, probably means the table just plain doesn't\n        // exist. rethrow the error immediately. this should always be coming\n        // from the HTable constructor.\n        throw e;\n      } catch (IOException e) {\n        ExceptionUtil.rethrowIfInterrupt(e);\n        if (e instanceof RemoteException) {\n          e = ((RemoteException)e).unwrapRemoteException();\n        }\n        if (e instanceof CallQueueTooBigException) {\n          // Give a special check on CallQueueTooBigException, see #HBASE-17114\n          pauseBase = this.pauseForCQTBE;\n        }\n        if (tries < maxAttempts - 1) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"locateRegionInMeta parentTable=\" + TableName.META_TABLE_NAME\n                + \", metaLocation=\" + \", attempt=\" + tries + \" of \" + maxAttempts\n                + \" failed; retrying after sleep of \"\n                + ConnectionUtils.getPauseTime(pauseBase, tries) + \" because: \" + e.getMessage());\n          }\n        } else {\n          throw e;\n        }\n        // Only relocate the parent region if necessary\n        if(!(e instanceof RegionOfflineException ||\n            e instanceof NoServerForRegionException)) {\n          relocateRegion(TableName.META_TABLE_NAME, metaKey, replicaId);\n        }\n      } finally {\n        userRegionLock.unlock();\n      }\n      try{\n        Thread.sleep(ConnectionUtils.getPauseTime(pauseBase, tries));\n      } catch (InterruptedException e) {\n        throw new InterruptedIOException(\"Giving up trying to location region in \" +\n          \"meta: thread is interrupted.\");\n      }\n    }\n  }"
        ]
    ],
    "9a809077602f2e339bf484e357ac71bf672bf080": [
        [
            "LruBlockCache::cacheBlock(BlockCacheKey,Cacheable,boolean)",
            " 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 -\n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  ",
            "  /**\n   * Cache the block with the specified name and buffer.\n   * <p>\n   * It is assumed this will NOT be called on an already cached block. In rare cases (HBASE-8547)\n   * this can happen, for which we compare the buffer contents.\n   *\n   * @param cacheKey block's cache key\n   * @param buf      block buffer\n   * @param inMemory if block is in-memory\n   */\n  @Override\n  public void cacheBlock(BlockCacheKey cacheKey, Cacheable buf, boolean inMemory) {\n    if (buf.heapSize() > maxBlockSize) {\n      // If there are a lot of blocks that are too\n      // big this can make the logs way too noisy.\n      // So we log 2%\n      if (stats.failInsert() % 50 == 0) {\n        LOG.warn(\"Trying to cache too large a block \"\n            + cacheKey.getHfileName() + \" @ \"\n            + cacheKey.getOffset()\n            + \" is \" + buf.heapSize()\n            + \" which is larger than \" + maxBlockSize);\n      }\n      return;\n    }\n\n    LruCachedBlock cb = map.get(cacheKey);\n    if (cb != null) {\n      int comparison = BlockCacheUtil.validateBlockAddition(cb.getBuffer(), buf, cacheKey);\n      if (comparison != 0) {\n        if (comparison < 0) {\n          LOG.warn(\"Cached block contents differ by nextBlockOnDiskSize. Keeping cached block.\");\n          return;\n        } else {\n          LOG.warn(\"Cached block contents differ by nextBlockOnDiskSize. Caching new block.\");\n        }\n      } else {\n        String msg = \"Cached an already cached block: \" + cacheKey + \" cb:\" + cb.getCacheKey();\n        msg += \". This is harmless and can happen in rare cases (see HBASE-8547)\";\n        LOG.warn(msg);\n        return;\n      }\n    }\n    long currentSize = size.get();\n    long currentAcceptableSize = acceptableSize();\n    long hardLimitSize = (long) (hardCapacityLimitFactor * currentAcceptableSize);\n    if (currentSize >= hardLimitSize) {\n      stats.failInsert();\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"LruBlockCache current size \" + StringUtils.byteDesc(currentSize)\n          + \" has exceeded acceptable size \" + StringUtils.byteDesc(currentAcceptableSize) + \".\"\n          + \" The hard limit size is \" + StringUtils.byteDesc(hardLimitSize)\n          + \", failed to put cacheKey:\" + cacheKey + \" into LruBlockCache.\");\n      }\n      if (!evictionInProgress) {\n        runEviction();\n      }\n      return;\n    }\n    cb = new LruCachedBlock(cacheKey, buf, count.incrementAndGet(), inMemory);\n    long newSize = updateSizeMetrics(cb, false);\n    map.put(cacheKey, cb);\n    long val = elements.incrementAndGet();\n    if (buf.getBlockType().isData()) {\n       dataBlockElements.increment();\n    }\n    if (LOG.isTraceEnabled()) {\n      long size = map.size();\n      assertCounterSanity(size, val);\n    }\n    if (newSize > currentAcceptableSize && !evictionInProgress) {\n      runEviction();\n    }\n  }",
            " 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 +\n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  ",
            "  /**\n   * Cache the block with the specified name and buffer.\n   * <p>\n   * It is assumed this will NOT be called on an already cached block. In rare cases (HBASE-8547)\n   * this can happen, for which we compare the buffer contents.\n   *\n   * @param cacheKey block's cache key\n   * @param buf      block buffer\n   * @param inMemory if block is in-memory\n   */\n  @Override\n  public void cacheBlock(BlockCacheKey cacheKey, Cacheable buf, boolean inMemory) {\n    if (buf.heapSize() > maxBlockSize) {\n      // If there are a lot of blocks that are too\n      // big this can make the logs way too noisy.\n      // So we log 2%\n      if (stats.failInsert() % 50 == 0) {\n        LOG.warn(\"Trying to cache too large a block \"\n            + cacheKey.getHfileName() + \" @ \"\n            + cacheKey.getOffset()\n            + \" is \" + buf.heapSize()\n            + \" which is larger than \" + maxBlockSize);\n      }\n      return;\n    }\n\n    LruCachedBlock cb = map.get(cacheKey);\n    if (cb != null) {\n      int comparison = BlockCacheUtil.validateBlockAddition(cb.getBuffer(), buf, cacheKey);\n      if (comparison != 0) {\n        if (comparison < 0) {\n          LOG.warn(\"Cached block contents differ by nextBlockOnDiskSize. Keeping cached block.\");\n          return;\n        } else {\n          LOG.warn(\"Cached block contents differ by nextBlockOnDiskSize. Caching new block.\");\n        }\n      } else {\n        String msg = \"Cached an already cached block: \" + cacheKey + \" cb:\" + cb.getCacheKey();\n        msg += \". This is harmless and can happen in rare cases (see HBASE-8547)\";\n        LOG.debug(msg);\n        return;\n      }\n    }\n    long currentSize = size.get();\n    long currentAcceptableSize = acceptableSize();\n    long hardLimitSize = (long) (hardCapacityLimitFactor * currentAcceptableSize);\n    if (currentSize >= hardLimitSize) {\n      stats.failInsert();\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"LruBlockCache current size \" + StringUtils.byteDesc(currentSize)\n          + \" has exceeded acceptable size \" + StringUtils.byteDesc(currentAcceptableSize) + \".\"\n          + \" The hard limit size is \" + StringUtils.byteDesc(hardLimitSize)\n          + \", failed to put cacheKey:\" + cacheKey + \" into LruBlockCache.\");\n      }\n      if (!evictionInProgress) {\n        runEviction();\n      }\n      return;\n    }\n    cb = new LruCachedBlock(cacheKey, buf, count.incrementAndGet(), inMemory);\n    long newSize = updateSizeMetrics(cb, false);\n    map.put(cacheKey, cb);\n    long val = elements.incrementAndGet();\n    if (buf.getBlockType().isData()) {\n       dataBlockElements.increment();\n    }\n    if (LOG.isTraceEnabled()) {\n      long size = map.size();\n      assertCounterSanity(size, val);\n    }\n    if (newSize > currentAcceptableSize && !evictionInProgress) {\n      runEviction();\n    }\n  }"
        ]
    ],
    "a8ad61ec88f2d147e557f26543157db54dd7fcef": [
        [
            "ProtobufLogReader::readNext(Entry)",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 -\n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412 -\n 413  \n 414  \n 415  \n 416 -\n 417 -\n 418 -\n 419 -\n 420  \n 421 -\n 422  \n 423  \n 424  \n 425  \n 426  ",
            "  @Override\n  protected boolean readNext(Entry entry) throws IOException {\n    while (true) {\n      // OriginalPosition might be < 0 on local fs; if so, it is useless to us.\n      long originalPosition = this.inputStream.getPos();\n      if (trailerPresent && originalPosition > 0 && originalPosition == this.walEditsStopOffset) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Reached end of expected edits area at offset \" + originalPosition);\n        }\n        return false;\n      }\n      WALKey.Builder builder = WALKey.newBuilder();\n      long size = 0;\n      try {\n        long available = -1;\n        try {\n          int firstByte = this.inputStream.read();\n          if (firstByte == -1) {\n            throw new EOFException(\"First byte is negative at offset \" + originalPosition);\n          }\n          size = CodedInputStream.readRawVarint32(firstByte, this.inputStream);\n          // available may be < 0 on local fs for instance.  If so, can't depend on it.\n          available = this.inputStream.available();\n          if (available > 0 && available < size) {\n            throw new EOFException(\"Available stream not enough for edit, \" +\n                \"inputStream.available()= \" + this.inputStream.available() + \", \" +\n                \"entry size= \" + size + \" at offset = \" + this.inputStream.getPos());\n          }\n          ProtobufUtil.mergeFrom(builder, ByteStreams.limit(this.inputStream, size),\n            (int)size);\n        } catch (InvalidProtocolBufferException ipbe) {\n          throw (EOFException) new EOFException(\"Invalid PB, EOF? Ignoring; originalPosition=\" +\n            originalPosition + \", currentPosition=\" + this.inputStream.getPos() +\n            \", messageSize=\" + size + \", currentAvailable=\" + available).initCause(ipbe);\n        }\n        if (!builder.isInitialized()) {\n          // TODO: not clear if we should try to recover from corrupt PB that looks semi-legit.\n          //       If we can get the KV count, we could, theoretically, try to get next record.\n          throw new EOFException(\"Partial PB while reading WAL, \" +\n              \"probably an unexpected EOF, ignoring. current offset=\" + this.inputStream.getPos());\n        }\n        WALKey walKey = builder.build();\n        entry.getKey().readFieldsFromPb(walKey, this.byteStringUncompressor);\n        if (!walKey.hasFollowingKvCount() || 0 == walKey.getFollowingKvCount()) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"WALKey has no KVs that follow it; trying the next one. current offset=\" +\n                this.inputStream.getPos());\n          }\n          continue;\n        }\n        int expectedCells = walKey.getFollowingKvCount();\n        long posBefore = this.inputStream.getPos();\n        try {\n          int actualCells = entry.getEdit().readFromCells(cellDecoder, expectedCells);\n          if (expectedCells != actualCells) {\n            throw new EOFException(\"Only read \" + actualCells); // other info added in catch\n          }\n        } catch (Exception ex) {\n          String posAfterStr = \"<unknown>\";\n          try {\n            posAfterStr = this.inputStream.getPos() + \"\";\n          } catch (Throwable t) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Error getting pos for error message - ignoring\", t);\n            }\n          }\n          String message = \" while reading \" + expectedCells + \" WAL KVs; started reading at \"\n              + posBefore + \" and read up to \" + posAfterStr;\n          IOException realEofEx = extractHiddenEof(ex);\n          throw (EOFException) new EOFException(\"EOF \" + message).\n              initCause(realEofEx != null ? realEofEx : ex);\n        }\n        if (trailerPresent && this.inputStream.getPos() > this.walEditsStopOffset) {\n          LOG.error(\"Read WALTrailer while reading WALEdits. wal: \" + this.path\n              + \", inputStream.getPos(): \" + this.inputStream.getPos() + \", walEditsStopOffset: \"\n              + this.walEditsStopOffset);\n          throw new EOFException(\"Read WALTrailer while reading WALEdits\");\n        }\n      } catch (EOFException eof) {\n        // If originalPosition is < 0, it is rubbish and we cannot use it (probably local fs)\n        if (originalPosition < 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Encountered a malformed edit, but can't seek back to last good position because originalPosition is negative. last offset=\" + this.inputStream.getPos(), eof);\n          }\n          throw eof;\n        }\n        // Else restore our position to original location in hope that next time through we will\n        // read successfully.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Encountered a malformed edit, seeking back to last good position in file, from \"+ inputStream.getPos()+\" to \" + originalPosition, eof);\n        }\n        seekOnFs(originalPosition);\n        return false;\n      }\n      return true;\n    }\n  }",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343 +\n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380 +\n 381 +\n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 +\n 417 +\n 418 +\n 419  \n 420  \n 421  \n 422 +\n 423 +\n 424 +\n 425 +\n 426 +\n 427 +\n 428 +\n 429 +\n 430 +\n 431 +\n 432 +\n 433 +\n 434 +\n 435 +\n 436 +\n 437  \n 438  \n 439  \n 440  \n 441  \n 442  ",
            "  @Override\n  protected boolean readNext(Entry entry) throws IOException {\n    while (true) {\n      // OriginalPosition might be < 0 on local fs; if so, it is useless to us.\n      long originalPosition = this.inputStream.getPos();\n      if (trailerPresent && originalPosition > 0 && originalPosition == this.walEditsStopOffset) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Reached end of expected edits area at offset \" + originalPosition);\n        }\n        return false;\n      }\n      WALKey.Builder builder = WALKey.newBuilder();\n      long size = 0;\n      boolean resetPosition = false;\n      try {\n        long available = -1;\n        try {\n          int firstByte = this.inputStream.read();\n          if (firstByte == -1) {\n            throw new EOFException(\"First byte is negative at offset \" + originalPosition);\n          }\n          size = CodedInputStream.readRawVarint32(firstByte, this.inputStream);\n          // available may be < 0 on local fs for instance.  If so, can't depend on it.\n          available = this.inputStream.available();\n          if (available > 0 && available < size) {\n            throw new EOFException(\"Available stream not enough for edit, \" +\n                \"inputStream.available()= \" + this.inputStream.available() + \", \" +\n                \"entry size= \" + size + \" at offset = \" + this.inputStream.getPos());\n          }\n          ProtobufUtil.mergeFrom(builder, ByteStreams.limit(this.inputStream, size),\n            (int)size);\n        } catch (InvalidProtocolBufferException ipbe) {\n          resetPosition = true;\n          throw (EOFException) new EOFException(\"Invalid PB, EOF? Ignoring; originalPosition=\" +\n            originalPosition + \", currentPosition=\" + this.inputStream.getPos() +\n            \", messageSize=\" + size + \", currentAvailable=\" + available).initCause(ipbe);\n        }\n        if (!builder.isInitialized()) {\n          // TODO: not clear if we should try to recover from corrupt PB that looks semi-legit.\n          //       If we can get the KV count, we could, theoretically, try to get next record.\n          throw new EOFException(\"Partial PB while reading WAL, \" +\n              \"probably an unexpected EOF, ignoring. current offset=\" + this.inputStream.getPos());\n        }\n        WALKey walKey = builder.build();\n        entry.getKey().readFieldsFromPb(walKey, this.byteStringUncompressor);\n        if (!walKey.hasFollowingKvCount() || 0 == walKey.getFollowingKvCount()) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"WALKey has no KVs that follow it; trying the next one. current offset=\" +\n                this.inputStream.getPos());\n          }\n          seekOnFs(originalPosition);\n          return false;\n        }\n        int expectedCells = walKey.getFollowingKvCount();\n        long posBefore = this.inputStream.getPos();\n        try {\n          int actualCells = entry.getEdit().readFromCells(cellDecoder, expectedCells);\n          if (expectedCells != actualCells) {\n            resetPosition = true;\n            throw new EOFException(\"Only read \" + actualCells); // other info added in catch\n          }\n        } catch (Exception ex) {\n          String posAfterStr = \"<unknown>\";\n          try {\n            posAfterStr = this.inputStream.getPos() + \"\";\n          } catch (Throwable t) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Error getting pos for error message - ignoring\", t);\n            }\n          }\n          String message = \" while reading \" + expectedCells + \" WAL KVs; started reading at \"\n              + posBefore + \" and read up to \" + posAfterStr;\n          IOException realEofEx = extractHiddenEof(ex);\n          throw (EOFException) new EOFException(\"EOF \" + message).\n              initCause(realEofEx != null ? realEofEx : ex);\n        }\n        if (trailerPresent && this.inputStream.getPos() > this.walEditsStopOffset) {\n          LOG.error(\"Read WALTrailer while reading WALEdits. wal: \" + this.path\n              + \", inputStream.getPos(): \" + this.inputStream.getPos() + \", walEditsStopOffset: \"\n              + this.walEditsStopOffset);\n          throw new EOFException(\"Read WALTrailer while reading WALEdits\");\n        }\n      } catch (EOFException eof) {\n        // If originalPosition is < 0, it is rubbish and we cannot use it (probably local fs)\n        if (originalPosition < 0) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Encountered a malformed edit, but can't seek back to last good position \"\n                + \"because originalPosition is negative. last offset=\"\n                + this.inputStream.getPos(), eof);\n          }\n          throw eof;\n        }\n        // If stuck at the same place and we got and exception, lets go back at the beginning.\n        if (inputStream.getPos() == originalPosition && resetPosition) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Encountered a malformed edit, seeking to the beginning of the WAL since \"\n                + \"current position and original position match at \" + originalPosition);\n          }\n          seekOnFs(0);\n        } else {\n          // Else restore our position to original location in hope that next time through we will\n          // read successfully.\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Encountered a malformed edit, seeking back to last good position in file, \"\n                + \"from \" + inputStream.getPos()+\" to \" + originalPosition, eof);\n          }\n          seekOnFs(originalPosition);\n        }\n        return false;\n      }\n      return true;\n    }\n  }"
        ]
    ],
    "960a5fdc2a34d2aa96dda00f37665224c2d8767c": [
        [
            "isOverlap(RegionInfo)",
            " 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802 -\n 803  \n 804  \n 805  \n 806 -\n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  ",
            "  /**\n   * @return True if an overlap in region range.\n   * @see #isDegenerate()\n   */\n  default boolean isOverlap(RegionInfo other) {\n    if (!getTable().equals(other.getTable())) {\n      return false;\n    }\n    int startKeyCompare = Bytes.compareTo(getStartKey(), other.getStartKey());\n    if (startKeyCompare == 0) {\n      return true;\n    }\n    if (startKeyCompare < 0) {\n      if (isLast()) {\n        return true;\n      }\n      return Bytes.compareTo(getEndKey(), other.getStartKey()) > 0;\n    }\n    if (other.isLast()) {\n      return true;\n    }\n    return Bytes.compareTo(getStartKey(), other.getEndKey()) < 0;\n  }",
            " 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802 +\n 803  \n 804  \n 805  \n 806 +\n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  ",
            "  /**\n   * @return True if an overlap in region range.\n   * @see #isDegenerate()\n   */\n  default boolean isOverlap(RegionInfo other) {\n    if (!getTable().equals(other.getTable())) {\n      return false;\n    }\n    int startKeyCompare = Bytes.compareTo(getStartKey(), other.getStartKey());\n    if (startKeyCompare == 0) {\n      return !this.isSplitParent();\n    }\n    if (startKeyCompare < 0) {\n      if (isLast()) {\n        return !this.isSplitParent();\n      }\n      return Bytes.compareTo(getEndKey(), other.getStartKey()) > 0;\n    }\n    if (other.isLast()) {\n      return true;\n    }\n    return Bytes.compareTo(getStartKey(), other.getEndKey()) < 0;\n  }"
        ],
        [
            "TestHRegionInfo::testIsOverlap()",
            "  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "  @Test\n  public void testIsOverlap() {\n    byte [] a = Bytes.toBytes(\"a\");\n    byte [] b = Bytes.toBytes(\"b\");\n    byte [] c = Bytes.toBytes(\"c\");\n    byte [] d = Bytes.toBytes(\"d\");\n    org.apache.hadoop.hbase.client.RegionInfo all =\n        RegionInfoBuilder.FIRST_META_REGIONINFO;\n    org.apache.hadoop.hbase.client.RegionInfo ari =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setEndKey(a).build();\n    org.apache.hadoop.hbase.client.RegionInfo abri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(a).setEndKey(b).build();\n    org.apache.hadoop.hbase.client.RegionInfo adri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(a).setEndKey(d).build();\n    org.apache.hadoop.hbase.client.RegionInfo cdri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(c).setEndKey(d).build();\n    org.apache.hadoop.hbase.client.RegionInfo dri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(d).build();\n    assertTrue(all.isOverlap(all));\n    assertTrue(all.isOverlap(abri));\n    assertFalse(abri.isOverlap(cdri));\n    assertTrue(all.isOverlap(ari));\n    assertFalse(ari.isOverlap(abri));\n    assertFalse(ari.isOverlap(abri));\n    assertTrue(ari.isOverlap(all));\n    assertTrue(dri.isOverlap(all));\n    assertTrue(abri.isOverlap(adri));\n    assertFalse(dri.isOverlap(ari));\n    assertTrue(abri.isOverlap(adri));\n  }",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136 +\n 137  ",
            "  @Test\n  public void testIsOverlap() {\n    byte [] a = Bytes.toBytes(\"a\");\n    byte [] b = Bytes.toBytes(\"b\");\n    byte [] c = Bytes.toBytes(\"c\");\n    byte [] d = Bytes.toBytes(\"d\");\n    org.apache.hadoop.hbase.client.RegionInfo all =\n        RegionInfoBuilder.FIRST_META_REGIONINFO;\n    org.apache.hadoop.hbase.client.RegionInfo ari =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setEndKey(a).build();\n    org.apache.hadoop.hbase.client.RegionInfo abri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(a).setEndKey(b).build();\n    org.apache.hadoop.hbase.client.RegionInfo adri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(a).setEndKey(d).build();\n    org.apache.hadoop.hbase.client.RegionInfo cdri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(c).setEndKey(d).build();\n    org.apache.hadoop.hbase.client.RegionInfo dri =\n        org.apache.hadoop.hbase.client.RegionInfoBuilder.newBuilder(TableName.META_TABLE_NAME).\n            setStartKey(d).build();\n    assertTrue(all.isOverlap(all));\n    assertTrue(all.isOverlap(abri));\n    assertFalse(abri.isOverlap(cdri));\n    assertTrue(all.isOverlap(ari));\n    assertFalse(ari.isOverlap(abri));\n    assertFalse(ari.isOverlap(abri));\n    assertTrue(ari.isOverlap(all));\n    assertTrue(dri.isOverlap(all));\n    assertTrue(abri.isOverlap(adri));\n    assertFalse(dri.isOverlap(ari));\n    assertTrue(abri.isOverlap(adri));\n    assertTrue(adri.isOverlap(abri));\n    // Check that splitParent is not reported as an overlap.\n    RegionInfo splitParent = RegionInfoBuilder.newBuilder(adri.getTable()).\n        setStartKey(adri.getStartKey()).setEndKey(adri.getEndKey()).setOffline(true).\n        setSplit(true).build();\n    assertFalse(splitParent.isOverlap(abri));\n  }"
        ]
    ]
}