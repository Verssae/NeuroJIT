{
    "e2a4f47ed8c95c7045f79bf9fe59cab39518710b": [
        [
            "TaskManagerStartupTest::testStartupWhenNetworkStackFailsToInitialize()",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  ",
            "\t/**\n\t * Tests that the task manager start-up fails if the network stack cannot be initialized.\n\t * @throws Exception\n\t */\n\t@Test(expected = IOException.class)\n\tpublic void testStartupWhenNetworkStackFailsToInitialize() throws Exception {\n\n\t\tServerSocket blocker = null;\n\n\t\ttry {\n\t\t\tblocker = new ServerSocket(0, 50, InetAddress.getByName(\"localhost\"));\n\n\t\t\tfinal Configuration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_DATA_PORT_KEY, blocker.getLocalPort());\n\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 1);\n\n\t\t\tTaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\tcfg,\n\t\t\t\tResourceID.generate(),\n\t\t\t\tnull,\n\t\t\t\t\"localhost\",\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tOption.<LeaderRetrievalService>empty(),\n\t\t\t\tfalse,\n\t\t\t\tTaskManager.class);\n\t\t}\n\t\tfinally {\n\t\t\tif (blocker != null) {\n\t\t\t\ttry {\n\t\t\t\t\tblocker.close();\n\t\t\t\t}",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\t/**\n\t * Tests that the task manager start-up fails if the network stack cannot be initialized.\n\t * @throws Exception\n\t */\n\t@Test(expected = IOException.class)\n\tpublic void testStartupWhenNetworkStackFailsToInitialize() throws Exception {\n\n\t\tServerSocket blocker = null;\n\n\t\ttry {\n\t\t\tblocker = new ServerSocket(0, 50, InetAddress.getByName(\"localhost\"));\n\n\t\t\tfinal Configuration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_DATA_PORT_KEY, blocker.getLocalPort());\n\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 1L);\n\n\t\t\tTaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\tcfg,\n\t\t\t\tResourceID.generate(),\n\t\t\t\tnull,\n\t\t\t\t\"localhost\",\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tOption.<LeaderRetrievalService>empty(),\n\t\t\t\tfalse,\n\t\t\t\tTaskManager.class);\n\t\t}\n\t\tfinally {\n\t\t\tif (blocker != null) {\n\t\t\t\ttry {\n\t\t\t\t\tblocker.close();\n\t\t\t\t}"
        ],
        [
            "AbstractEventTimeWindowCheckpointingITCase::startTestCluster()",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 48);\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 48L);\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}"
        ],
        [
            "WebFrontendITCase::initialize()",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74 -\n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "\t@BeforeClass\n\tpublic static void initialize() throws Exception {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 12);\n\t\tconfig.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, true);\n\n\t\tFile logDir = File.createTempFile(\"TestBaseUtils-logdir\", null);\n\t\tassertTrue(\"Unable to delete temp file\", logDir.delete());\n\t\tassertTrue(\"Unable to create temp directory\", logDir.mkdir());\n\t\tFile logFile = new File(logDir, \"jobmanager.log\");\n\t\tFile outFile = new File(logDir, \"jobmanager.out\");\n\t\t\n\t\tFiles.createFile(logFile.toPath());\n\t\tFiles.createFile(outFile.toPath());\n\t\t\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_WEB_LOG_PATH_KEY, logFile.getAbsolutePath());\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_LOG_PATH_KEY, logFile.getAbsolutePath());\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t\t\n\t\tport = cluster.webMonitor().get().getServerPort();\n\t}",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "\t@BeforeClass\n\tpublic static void initialize() throws Exception {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS);\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 12L);\n\t\tconfig.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, true);\n\n\t\tFile logDir = File.createTempFile(\"TestBaseUtils-logdir\", null);\n\t\tassertTrue(\"Unable to delete temp file\", logDir.delete());\n\t\tassertTrue(\"Unable to create temp directory\", logDir.mkdir());\n\t\tFile logFile = new File(logDir, \"jobmanager.log\");\n\t\tFile outFile = new File(logDir, \"jobmanager.out\");\n\t\t\n\t\tFiles.createFile(logFile.toPath());\n\t\tFiles.createFile(outFile.toPath());\n\t\t\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_WEB_LOG_PATH_KEY, logFile.getAbsolutePath());\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_LOG_PATH_KEY, logFile.getAbsolutePath());\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t\t\n\t\tport = cluster.webMonitor().get().getServerPort();\n\t}"
        ],
        [
            "AccumulatorErrorITCase::startCluster()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 3);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 12);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 3);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 12L);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "LocalStreamEnvironment::execute(String)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "\t/**\n\t * Executes the JobGraph of the on a mini cluster of CLusterUtil with a user\n\t * specified name.\n\t * \n\t * @param jobName\n\t *            name of the job\n\t * @return The result of the job execution, containing elapsed time and accumulators.\n\t */\n\t@Override\n\tpublic JobExecutionResult execute(String jobName) throws Exception {\n\t\t// transform the streaming program into a JobGraph\n\t\tStreamGraph streamGraph = getStreamGraph();\n\t\tstreamGraph.setJobName(jobName);\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tConfiguration configuration = new Configuration();\n\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\n\t\tconfiguration.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\t\tconfiguration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());\n\t\t\n\t\t// add (and override) the settings with what the user defined\n\t\tconfiguration.addAll(this.conf);\n\t\t\n\t\tif (LOG.isInfoEnabled()) {\n\t\t\tLOG.info(\"Running job on local embedded Flink mini cluster\");\n\t\t}\n\n\t\tLocalFlinkMiniCluster exec = new LocalFlinkMiniCluster(configuration, true);\n\t\ttry {\n\t\t\texec.start();\n\t\t\treturn exec.submitJobAndWait(jobGraph, getConfig().isSysoutLoggingEnabled());\n\t\t}\n\t\tfinally {\n\t\t\ttransformations.clear();\n\t\t\texec.stop();\n\t\t}\n\t}",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "\t/**\n\t * Executes the JobGraph of the on a mini cluster of CLusterUtil with a user\n\t * specified name.\n\t * \n\t * @param jobName\n\t *            name of the job\n\t * @return The result of the job execution, containing elapsed time and accumulators.\n\t */\n\t@Override\n\tpublic JobExecutionResult execute(String jobName) throws Exception {\n\t\t// transform the streaming program into a JobGraph\n\t\tStreamGraph streamGraph = getStreamGraph();\n\t\tstreamGraph.setJobName(jobName);\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tConfiguration configuration = new Configuration();\n\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\n\t\tconfiguration.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, -1L);\n\t\tconfiguration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());\n\t\t\n\t\t// add (and override) the settings with what the user defined\n\t\tconfiguration.addAll(this.conf);\n\t\t\n\t\tif (LOG.isInfoEnabled()) {\n\t\t\tLOG.info(\"Running job on local embedded Flink mini cluster\");\n\t\t}\n\n\t\tLocalFlinkMiniCluster exec = new LocalFlinkMiniCluster(configuration, true);\n\t\ttry {\n\t\t\texec.start();\n\t\t\treturn exec.submitJobAndWait(jobGraph, getConfig().isSysoutLoggingEnabled());\n\t\t}\n\t\tfinally {\n\t\t\ttransformations.clear();\n\t\t\texec.stop();\n\t\t}\n\t}"
        ],
        [
            "MiniClusterConfiguration::generateConfiguration()",
            " 164  \n 165  \n 166  \n 167  \n 168 -\n 169  \n 170  \n 171  ",
            "\tpublic Configuration generateConfiguration() {\n\t\tConfiguration newConfiguration = new Configuration(config);\n\t\t// set the memory\n\t\tlong memory = getOrCalculateManagedMemoryPerTaskManager();\n\t\tnewConfiguration.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, memory);\n\n\t\treturn newConfiguration;\n\t}",
            " 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  ",
            "\tpublic Configuration generateConfiguration() {\n\t\tConfiguration newConfiguration = new Configuration(config);\n\t\t// set the memory\n\t\tlong memory = getOrCalculateManagedMemoryPerTaskManager();\n\t\tnewConfiguration.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, memory);\n\n\t\treturn newConfiguration;\n\t}"
        ],
        [
            "TimestampITCase::startCluster()",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_TASK_SLOTS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 12);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_TASK_SLOTS);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 12L);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "SuccessAfterNetworkBuffersFailureITCase::testSuccessfulProgramAfterFailure()",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51 -\n  52  \n  53 -\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "\t@Test\n\tpublic void testSuccessfulProgramAfterFailure() {\n\t\tLocalFlinkMiniCluster cluster = null;\n\t\t\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 80);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 840);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t\t\n\t\t\ttry {\n\t\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program Execution should have succeeded.\");\n\t\t\t}\n\t\n\t\t\ttry {\n\t\t\t\trunKMeans(cluster.getLeaderRPCPort());\n\t\t\t\tfail(\"This program execution should have failed.\");\n\t\t\t}\n\t\t\tcatch (ProgramInvocationException e) {\n\t\t\t\tassertTrue(e.getCause().getCause().getMessage().contains(\"Insufficient number of network buffers\"));\n\t\t\t}\n\t\n\t\t\ttry {\n\t\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program Execution should have succeeded.\");\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54 +\n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "\t@Test\n\tpublic void testSuccessfulProgramAfterFailure() {\n\t\tLocalFlinkMiniCluster cluster = null;\n\t\t\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 80L);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 840);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t\t\n\t\t\ttry {\n\t\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program Execution should have succeeded.\");\n\t\t\t}\n\t\n\t\t\ttry {\n\t\t\t\trunKMeans(cluster.getLeaderRPCPort());\n\t\t\t\tfail(\"This program execution should have failed.\");\n\t\t\t}\n\t\t\tcatch (ProgramInvocationException e) {\n\t\t\t\tassertTrue(e.getCause().getCause().getMessage().contains(\"Insufficient number of network buffers\"));\n\t\t\t}\n\t\n\t\t\ttry {\n\t\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program Execution should have succeeded.\");\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "AbstractQueryableStateITCase::setup()",
            " 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TMS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS_PER_TM);\n\t\t\tconfig.setInteger(QueryableStateOptions.CLIENT_NETWORK_THREADS, 1);\n\t\t\tconfig.setBoolean(QueryableStateOptions.SERVER_ENABLE, true);\n\t\t\tconfig.setInteger(QueryableStateOptions.SERVER_NETWORK_THREADS, 1);\n\n\t\t\tcluster = new TestingCluster(config, false);\n\t\t\tcluster.start(true);\n\n\t\t\tTEST_ACTOR_SYSTEM = AkkaUtils.createDefaultActorSystem();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}",
            " 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TMS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS_PER_TM);\n\t\t\tconfig.setInteger(QueryableStateOptions.CLIENT_NETWORK_THREADS, 1);\n\t\t\tconfig.setBoolean(QueryableStateOptions.SERVER_ENABLE, true);\n\t\t\tconfig.setInteger(QueryableStateOptions.SERVER_NETWORK_THREADS, 1);\n\n\t\t\tcluster = new TestingCluster(config, false);\n\t\t\tcluster.start(true);\n\n\t\t\tTEST_ACTOR_SYSTEM = AkkaUtils.createDefaultActorSystem();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "TaskManagerTest::testFailingScheduleOrUpdateConsumersMessage()",
            "1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501 -\n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  ",
            "\t/**\n\t * Test that a failing schedule or update consumers call leads to the failing of the respective\n\t * task.\n\t *\n\t * IMPORTANT: We have to make sure that the invokable's cancel method is called, because only\n\t * then the future is completed. We do this by not eagerly deploy consumer tasks and requiring\n\t * the invokable to fill one memory segment. The completed memory segment will trigger the\n\t * scheduling of the downstream operator since it is in pipeline mode. After we've filled the\n\t * memory segment, we'll block the invokable and wait for the task failure due to the failed\n\t * schedule or update consumers call.\n\t */\n\t@Test(timeout = 10000L)\n\tpublic void testFailingScheduleOrUpdateConsumersMessage() throws Exception {\n\t\tnew JavaTestKit(system) {{\n\t\t\tfinal Configuration configuration = new Configuration();\n\n\t\t\t// set the memory segment to the smallest size possible, because we have to fill one\n\t\t\t// memory buffer to trigger the schedule or update consumers message to the downstream\n\t\t\t// operators\n\t\t\tconfiguration.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY, 4096);\n\n\t\t\tfinal JobID jid = new JobID();\n\t\t\tfinal JobVertexID vid = new JobVertexID();\n\t\t\tfinal ExecutionAttemptID eid = new ExecutionAttemptID();\n\t\t\tfinal SerializedValue<ExecutionConfig> executionConfig = new SerializedValue<>(new ExecutionConfig());\n\n\t\t\tfinal ResultPartitionDeploymentDescriptor resultPartitionDeploymentDescriptor = new ResultPartitionDeploymentDescriptor(\n\t\t\t\tnew IntermediateDataSetID(),\n\t\t\t\tnew IntermediateResultPartitionID(),\n\t\t\t\tResultPartitionType.PIPELINED,\n\t\t\t\t1,\n\t\t\t\t1,\n\t\t\t\ttrue);\n\n\t\t\tfinal TaskDeploymentDescriptor tdd = createTaskDeploymentDescriptor(jid, \"TestJob\", vid, eid, executionConfig,\n\t\t\t\t\"TestTask\", 1, 0, 1, 0, new Configuration(), new Configuration(),\n\t\t\t\tTestInvokableRecordCancel.class.getName(),\n\t\t\t\tCollections.singletonList(resultPartitionDeploymentDescriptor),\n\t\t\t\tCollections.<InputGateDeploymentDescriptor>emptyList(),\n\t\t\t\tnew ArrayList<BlobKey>(), Collections.<URL>emptyList(), 0);\n\n\n\t\t\tActorRef jmActorRef = system.actorOf(Props.create(FailingScheduleOrUpdateConsumersJobManager.class, leaderSessionID), \"jobmanager\");\n\t\t\tActorGateway jobManager = new AkkaActorGateway(jmActorRef, leaderSessionID);\n\n\t\t\tfinal ActorGateway taskManager = TestingUtils.createTaskManager(\n\t\t\t\tsystem,\n\t\t\t\tjobManager,\n\t\t\t\tconfiguration,\n\t\t\t\ttrue,\n\t\t\t\ttrue);\n\n\t\t\ttry {\n\t\t\t\tTestInvokableRecordCancel.resetGotCanceledFuture();\n\n\t\t\t\tFuture<Object> result = taskManager.ask(new SubmitTask(tdd), timeout);\n\n\t\t\t\tAwait.result(result, timeout);\n\n\t\t\t\torg.apache.flink.runtime.concurrent.Future<Boolean> cancelFuture = TestInvokableRecordCancel.gotCanceled();\n\n\t\t\t\tassertEquals(true, cancelFuture.get());\n\t\t\t} finally {\n\t\t\t\tTestingUtils.stopActor(taskManager);\n\t\t\t\tTestingUtils.stopActor(jobManager);\n\t\t\t}\n\t\t}};\n\t}",
            "1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501 +\n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  ",
            "\t/**\n\t * Test that a failing schedule or update consumers call leads to the failing of the respective\n\t * task.\n\t *\n\t * IMPORTANT: We have to make sure that the invokable's cancel method is called, because only\n\t * then the future is completed. We do this by not eagerly deploy consumer tasks and requiring\n\t * the invokable to fill one memory segment. The completed memory segment will trigger the\n\t * scheduling of the downstream operator since it is in pipeline mode. After we've filled the\n\t * memory segment, we'll block the invokable and wait for the task failure due to the failed\n\t * schedule or update consumers call.\n\t */\n\t@Test(timeout = 10000L)\n\tpublic void testFailingScheduleOrUpdateConsumersMessage() throws Exception {\n\t\tnew JavaTestKit(system) {{\n\t\t\tfinal Configuration configuration = new Configuration();\n\n\t\t\t// set the memory segment to the smallest size possible, because we have to fill one\n\t\t\t// memory buffer to trigger the schedule or update consumers message to the downstream\n\t\t\t// operators\n\t\t\tconfiguration.setInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE, 4096);\n\n\t\t\tfinal JobID jid = new JobID();\n\t\t\tfinal JobVertexID vid = new JobVertexID();\n\t\t\tfinal ExecutionAttemptID eid = new ExecutionAttemptID();\n\t\t\tfinal SerializedValue<ExecutionConfig> executionConfig = new SerializedValue<>(new ExecutionConfig());\n\n\t\t\tfinal ResultPartitionDeploymentDescriptor resultPartitionDeploymentDescriptor = new ResultPartitionDeploymentDescriptor(\n\t\t\t\tnew IntermediateDataSetID(),\n\t\t\t\tnew IntermediateResultPartitionID(),\n\t\t\t\tResultPartitionType.PIPELINED,\n\t\t\t\t1,\n\t\t\t\t1,\n\t\t\t\ttrue);\n\n\t\t\tfinal TaskDeploymentDescriptor tdd = createTaskDeploymentDescriptor(jid, \"TestJob\", vid, eid, executionConfig,\n\t\t\t\t\"TestTask\", 1, 0, 1, 0, new Configuration(), new Configuration(),\n\t\t\t\tTestInvokableRecordCancel.class.getName(),\n\t\t\t\tCollections.singletonList(resultPartitionDeploymentDescriptor),\n\t\t\t\tCollections.<InputGateDeploymentDescriptor>emptyList(),\n\t\t\t\tnew ArrayList<BlobKey>(), Collections.<URL>emptyList(), 0);\n\n\n\t\t\tActorRef jmActorRef = system.actorOf(Props.create(FailingScheduleOrUpdateConsumersJobManager.class, leaderSessionID), \"jobmanager\");\n\t\t\tActorGateway jobManager = new AkkaActorGateway(jmActorRef, leaderSessionID);\n\n\t\t\tfinal ActorGateway taskManager = TestingUtils.createTaskManager(\n\t\t\t\tsystem,\n\t\t\t\tjobManager,\n\t\t\t\tconfiguration,\n\t\t\t\ttrue,\n\t\t\t\ttrue);\n\n\t\t\ttry {\n\t\t\t\tTestInvokableRecordCancel.resetGotCanceledFuture();\n\n\t\t\t\tFuture<Object> result = taskManager.ask(new SubmitTask(tdd), timeout);\n\n\t\t\t\tAwait.result(result, timeout);\n\n\t\t\t\torg.apache.flink.runtime.concurrent.Future<Boolean> cancelFuture = TestInvokableRecordCancel.gotCanceled();\n\n\t\t\t\tassertEquals(true, cancelFuture.get());\n\t\t\t} finally {\n\t\t\t\tTestingUtils.stopActor(taskManager);\n\t\t\t\tTestingUtils.stopActor(jobManager);\n\t\t\t}\n\t\t}};\n\t}"
        ],
        [
            "TaskManagerStartupTest::testIODirectoryNotWritable()",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "\t/**\n\t * Tests that the TaskManager startup fails synchronously when the I/O\n\t * directories are not writable.\n\t */\n\t@Test\n\tpublic void testIODirectoryNotWritable() {\n\t\tFile tempDir = new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH);\n\t\tFile nonWritable = new File(tempDir, UUID.randomUUID().toString());\n\n\t\tif (!nonWritable.mkdirs() || !nonWritable.setWritable(false, false)) {\n\t\t\tSystem.err.println(\"Cannot create non-writable temporary file directory. Skipping test.\");\n\t\t\treturn;\n\t\t}\n\n\t\ttry {\n\t\t\tConfiguration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, nonWritable.getAbsolutePath());\n\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 21656);\n\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (IOException e) {\n\t\t\t\t// splendid!\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\t// noinspection ResultOfMethodCallIgnored\n\t\t\tnonWritable.setWritable(true, false);\n\t\t\ttry {\n\t\t\t\tFileUtils.deleteDirectory(nonWritable);\n\t\t\t}",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "\t/**\n\t * Tests that the TaskManager startup fails synchronously when the I/O\n\t * directories are not writable.\n\t */\n\t@Test\n\tpublic void testIODirectoryNotWritable() {\n\t\tFile tempDir = new File(ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH);\n\t\tFile nonWritable = new File(tempDir, UUID.randomUUID().toString());\n\n\t\tif (!nonWritable.mkdirs() || !nonWritable.setWritable(false, false)) {\n\t\t\tSystem.err.println(\"Cannot create non-writable temporary file directory. Skipping test.\");\n\t\t\treturn;\n\t\t}\n\n\t\ttry {\n\t\t\tConfiguration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, nonWritable.getAbsolutePath());\n\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 21656);\n\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (IOException e) {\n\t\t\t\t// splendid!\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\t// noinspection ResultOfMethodCallIgnored\n\t\t\tnonWritable.setWritable(true, false);\n\t\t\ttry {\n\t\t\t\tFileUtils.deleteDirectory(nonWritable);\n\t\t\t}"
        ],
        [
            "TaskManagerProcess::TaskManagerProcessEntryPoint::main(String)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107  \n 108  \n 109 -\n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t\t/**\n\t\t * All arguments are parsed to a {@link Configuration} and passed to the Taskmanager,\n\t\t * for instance: <code>--high-availability ZOOKEEPER --high-availability.zookeeper.quorum \"xyz:123:456\"</code>.\n\t\t */\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\ttry {\n\t\t\t\tConfiguration config = ParameterTool.fromArgs(args).getConfiguration();\n\n\t\t\t\tif (!config.containsKey(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY)) {\n\t\t\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\t\t}\n\n\t\t\t\tif (!config.containsKey(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY)) {\n\t\t\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 100);\n\t\t\t\t}\n\n\n\t\t\t\tLOG.info(\"Configuration: {}.\", config);\n\n\t\t\t\t// Run the TaskManager\n\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(\n\t\t\t\t\tconfig,\n\t\t\t\t\tResourceID.generate(),\n\t\t\t\t\tTaskManager.class);\n\n\t\t\t\t// Run forever\n\t\t\t\tnew CountDownLatch(1).await();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to start TaskManager process\", t);\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106 +\n 107  \n 108  \n 109 +\n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t\t/**\n\t\t * All arguments are parsed to a {@link Configuration} and passed to the Taskmanager,\n\t\t * for instance: <code>--high-availability ZOOKEEPER --high-availability.zookeeper.quorum \"xyz:123:456\"</code>.\n\t\t */\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\ttry {\n\t\t\t\tConfiguration config = ParameterTool.fromArgs(args).getConfiguration();\n\n\t\t\t\tif (!config.contains(TaskManagerOptions.MANAGED_MEMORY_SIZE)) {\n\t\t\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\t\t}\n\n\t\t\t\tif (!config.contains(TaskManagerOptions.NETWORK_NUM_BUFFERS)) {\n\t\t\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 100);\n\t\t\t\t}\n\n\n\t\t\t\tLOG.info(\"Configuration: {}.\", config);\n\n\t\t\t\t// Run the TaskManager\n\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(\n\t\t\t\t\tconfig,\n\t\t\t\t\tResourceID.generate(),\n\t\t\t\t\tTaskManager.class);\n\n\t\t\t\t// Run forever\n\t\t\t\tnew CountDownLatch(1).await();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to start TaskManager process\", t);\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}"
        ],
        [
            "TaskManagerFailureRecoveryITCase::testRestartWithFailingTaskManager()",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  ",
            "\t@Test\n\tpublic void testRestartWithFailingTaskManager() {\n\n\t\tfinal int PARALLELISM = 4;\n\n\t\tLocalFlinkMiniCluster cluster = null;\n\t\tActorSystem additionalSystem = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 16);\n\t\t\t\n\t\t\tconfig.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_INTERVAL, \"500 ms\");\n\t\t\tconfig.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_PAUSE, \"20 s\");\n\t\t\tconfig.setInteger(ConfigConstants.AKKA_WATCH_THRESHOLD, 20);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\n\t\t\t// for the result\n\t\t\tList<Long> resultCollection = new ArrayList<Long>();\n\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(\n\t\t\t\t\t\"localhost\", cluster.getLeaderRPCPort());\n\n\t\t\tenv.setParallelism(PARALLELISM);\n\t\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 1000));\n\t\t\tenv.getConfig().disableSysoutLogging();\n\n\t\t\tenv.generateSequence(1, 10)\n\t\t\t\t\t.map(new FailingMapper<Long>())\n\t\t\t\t\t.reduce(new ReduceFunction<Long>() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic Long reduce(Long value1, Long value2) {\n\t\t\t\t\t\t\treturn value1 + value2;\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.output(new LocalCollectionOutputFormat<Long>(resultCollection));\n\n\n\t\t\t// simple reference (atomic does not matter) to pass back an exception from the trigger thread\n\t\t\tfinal AtomicReference<Throwable> ref = new AtomicReference<Throwable>();\n\n\t\t\t// trigger the execution from a separate thread, so we are available to temper with the\n\t\t\t// cluster during the execution\n\t\t\tThread trigger = new Thread(\"program trigger\") {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tenv.execute();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tref.set(t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\ttrigger.setDaemon(true);\n\t\t\ttrigger.start();\n\n\t\t\t// block until all the mappers are actually deployed\n\t\t\t// the mappers in turn are waiting\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.TASK_TO_COORD_QUEUE.take();\n\t\t\t}\n\n\t\t\t// bring up one more task manager and wait for it to appear\n\t\t\t{\n\t\t\t\tadditionalSystem = cluster.startTaskManagerActorSystem(2);\n\t\t\t\tActorRef additionalTaskManager = cluster.startTaskManager(2, additionalSystem);\n\t\t\t\tObject message = TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage();\n\t\t\t\tFuture<Object> future = Patterns.ask(additionalTaskManager, message, 30000);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.result(future, new FiniteDuration(30000, TimeUnit.MILLISECONDS));\n\t\t\t\t}\n\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\tfail (\"The additional TaskManager did not come up within 30 seconds\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// kill the two other TaskManagers\n\t\t\tfor (ActorRef tm : cluster.getTaskManagersAsJava()) {\n\t\t\t\ttm.tell(PoisonPill.getInstance(), null);\n\t\t\t}\n\n\t\t\t// wait for the next set of mappers (the recovery ones) to come online\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.TASK_TO_COORD_QUEUE.take();\n\t\t\t}\n\n\t\t\t// tell the mappers that they may continue this time\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.COORD_TO_TASK_QUEUE.add(new Object());\n\t\t\t}\n\n\t\t\t// wait for the program to finish\n\t\t\ttrigger.join();\n\t\t\tif (ref.get() != null) {\n\t\t\t\tThrowable t = ref.get();\n\t\t\t\tt.printStackTrace();\n\t\t\t\tfail(\"Program execution caused an exception: \" + t.getMessage());\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (additionalSystem != null) {\n\t\t\t\tadditionalSystem.shutdown();\n\t\t\t}\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.stop();\n\t\t\t}\n\t\t}\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 +\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "\t@Test\n\tpublic void testRestartWithFailingTaskManager() {\n\n\t\tfinal int PARALLELISM = 4;\n\n\t\tLocalFlinkMiniCluster cluster = null;\n\t\tActorSystem additionalSystem = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n\t\t\t\n\t\t\tconfig.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_INTERVAL, \"500 ms\");\n\t\t\tconfig.setString(ConfigConstants.AKKA_WATCH_HEARTBEAT_PAUSE, \"20 s\");\n\t\t\tconfig.setInteger(ConfigConstants.AKKA_WATCH_THRESHOLD, 20);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\n\t\t\t// for the result\n\t\t\tList<Long> resultCollection = new ArrayList<Long>();\n\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(\n\t\t\t\t\t\"localhost\", cluster.getLeaderRPCPort());\n\n\t\t\tenv.setParallelism(PARALLELISM);\n\t\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 1000));\n\t\t\tenv.getConfig().disableSysoutLogging();\n\n\t\t\tenv.generateSequence(1, 10)\n\t\t\t\t\t.map(new FailingMapper<Long>())\n\t\t\t\t\t.reduce(new ReduceFunction<Long>() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic Long reduce(Long value1, Long value2) {\n\t\t\t\t\t\t\treturn value1 + value2;\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.output(new LocalCollectionOutputFormat<Long>(resultCollection));\n\n\n\t\t\t// simple reference (atomic does not matter) to pass back an exception from the trigger thread\n\t\t\tfinal AtomicReference<Throwable> ref = new AtomicReference<Throwable>();\n\n\t\t\t// trigger the execution from a separate thread, so we are available to temper with the\n\t\t\t// cluster during the execution\n\t\t\tThread trigger = new Thread(\"program trigger\") {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tenv.execute();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tref.set(t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\ttrigger.setDaemon(true);\n\t\t\ttrigger.start();\n\n\t\t\t// block until all the mappers are actually deployed\n\t\t\t// the mappers in turn are waiting\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.TASK_TO_COORD_QUEUE.take();\n\t\t\t}\n\n\t\t\t// bring up one more task manager and wait for it to appear\n\t\t\t{\n\t\t\t\tadditionalSystem = cluster.startTaskManagerActorSystem(2);\n\t\t\t\tActorRef additionalTaskManager = cluster.startTaskManager(2, additionalSystem);\n\t\t\t\tObject message = TaskManagerMessages.getNotifyWhenRegisteredAtJobManagerMessage();\n\t\t\t\tFuture<Object> future = Patterns.ask(additionalTaskManager, message, 30000);\n\n\t\t\t\ttry {\n\t\t\t\t\tAwait.result(future, new FiniteDuration(30000, TimeUnit.MILLISECONDS));\n\t\t\t\t}\n\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\tfail (\"The additional TaskManager did not come up within 30 seconds\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// kill the two other TaskManagers\n\t\t\tfor (ActorRef tm : cluster.getTaskManagersAsJava()) {\n\t\t\t\ttm.tell(PoisonPill.getInstance(), null);\n\t\t\t}\n\n\t\t\t// wait for the next set of mappers (the recovery ones) to come online\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.TASK_TO_COORD_QUEUE.take();\n\t\t\t}\n\n\t\t\t// tell the mappers that they may continue this time\n\t\t\tfor (int i = 0; i < PARALLELISM; i++) {\n\t\t\t\tFailingMapper.COORD_TO_TASK_QUEUE.add(new Object());\n\t\t\t}\n\n\t\t\t// wait for the program to finish\n\t\t\ttrigger.join();\n\t\t\tif (ref.get() != null) {\n\t\t\t\tThrowable t = ref.get();\n\t\t\t\tt.printStackTrace();\n\t\t\t\tfail(\"Program execution caused an exception: \" + t.getMessage());\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (additionalSystem != null) {\n\t\t\t\tadditionalSystem.shutdown();\n\t\t\t}\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.stop();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "ContaineredTaskManagerParameters::create(Configuration,long,int)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146 -\n 147  \n 148  \n 149  \n 150 -\n 151 -\n 152  \n 153  \n 154 -\n 155 -\n 156 -\n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "\t/**\n\t * Computes the parameters to be used to start a TaskManager Java process.\n\t *\n\t * @param config The Flink configuration.\n\t * @param containerMemoryMB The size of the complete container, in megabytes.\n\t * @return The parameters to start the TaskManager processes with.\n\t */\n\tpublic static ContaineredTaskManagerParameters create(\n\t\tConfiguration config, long containerMemoryMB, int numSlots)\n\t{\n\t\t// (1) compute how much memory we subtract from the total memory, to get the Java memory\n\n\t\tfinal float memoryCutoffRatio = config.getFloat(\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF_RATIO);\n\n\t\tfinal int minCutoff = config.getInteger(\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF);\n\n\t\tif (memoryCutoffRatio >= 1 || memoryCutoffRatio <= 0) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO + \"' must be between 0 and 1. Value given=\"\n\t\t\t\t+ memoryCutoffRatio);\n\t\t}\n\n\t\tif (minCutoff >= containerMemoryMB) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN + \"'='\" + minCutoff\n\t\t\t\t+ \"' is larger than the total container memory \" + containerMemoryMB);\n\t\t}\n\n\t\tlong cutoff = (long) (containerMemoryMB * memoryCutoffRatio);\n\t\tif (cutoff < minCutoff) {\n\t\t\tcutoff = minCutoff;\n\t\t}\n\n\t\tfinal long javaMemorySizeMB = containerMemoryMB - cutoff;\n\n\t\t// (2) split the Java memory between heap and off-heap\n\n\t\tfinal boolean useOffHeap = config.getBoolean(\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_OFF_HEAP_KEY, false);\n\n\t\tfinal long heapSizeMB;\n\t\tif (useOffHeap) {\n\t\t\tlong offHeapSize = config.getLong(\n\t\t\t\tConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\n\t\t\tif (offHeapSize <= 0) {\n\t\t\t\tdouble fraction = config.getFloat(\n\t\t\t\t\tConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,\n\t\t\t\t\tConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION);\n\n\n\t\t\t\toffHeapSize = (long) (fraction * javaMemorySizeMB);\n\t\t\t}\n\n\t\t\theapSizeMB = javaMemorySizeMB - offHeapSize;\n\t\t} else {\n\t\t\theapSizeMB = javaMemorySizeMB;\n\t\t}\n\t\t\n\t\t// (3) obtain the additional environment variables from the configuration\n\t\tfinal HashMap<String, String> envVars = new HashMap<>();\n\t\tfinal String prefix = ConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX;\n\t\t\n\t\tfor (String key : config.keySet()) {\n\t\t\tif (key.startsWith(prefix) && key.length() > prefix.length()) {\n\t\t\t\t// remove prefix\n\t\t\t\tString envVarKey = key.substring(prefix.length());\n\t\t\t\tenvVars.put(envVarKey, config.getString(key, null));\n\t\t\t}\n\t\t}\n\t\t\n\t\t// done\n\t\treturn new ContaineredTaskManagerParameters(\n\t\t\tcontainerMemoryMB, heapSizeMB, javaMemorySizeMB, numSlots, envVars);\n\t}",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  \n 148  \n 149  \n 150 +\n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  ",
            "\t/**\n\t * Computes the parameters to be used to start a TaskManager Java process.\n\t *\n\t * @param config The Flink configuration.\n\t * @param containerMemoryMB The size of the complete container, in megabytes.\n\t * @return The parameters to start the TaskManager processes with.\n\t */\n\tpublic static ContaineredTaskManagerParameters create(\n\t\tConfiguration config, long containerMemoryMB, int numSlots)\n\t{\n\t\t// (1) compute how much memory we subtract from the total memory, to get the Java memory\n\n\t\tfinal float memoryCutoffRatio = config.getFloat(\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF_RATIO);\n\n\t\tfinal int minCutoff = config.getInteger(\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF);\n\n\t\tif (memoryCutoffRatio >= 1 || memoryCutoffRatio <= 0) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO + \"' must be between 0 and 1. Value given=\"\n\t\t\t\t+ memoryCutoffRatio);\n\t\t}\n\n\t\tif (minCutoff >= containerMemoryMB) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN + \"'='\" + minCutoff\n\t\t\t\t+ \"' is larger than the total container memory \" + containerMemoryMB);\n\t\t}\n\n\t\tlong cutoff = (long) (containerMemoryMB * memoryCutoffRatio);\n\t\tif (cutoff < minCutoff) {\n\t\t\tcutoff = minCutoff;\n\t\t}\n\n\t\tfinal long javaMemorySizeMB = containerMemoryMB - cutoff;\n\n\t\t// (2) split the Java memory between heap and off-heap\n\n\t\tfinal boolean useOffHeap = config.getBoolean(TaskManagerOptions.MEMORY_OFF_HEAP);\n\n\t\tfinal long heapSizeMB;\n\t\tif (useOffHeap) {\n\t\t\tlong offHeapSize = config.getLong(TaskManagerOptions.MANAGED_MEMORY_SIZE);\n\n\t\t\tif (offHeapSize <= 0) {\n\t\t\t\tdouble fraction = config.getFloat(TaskManagerOptions.MANAGED_MEMORY_FRACTION);\n\n\t\t\t\toffHeapSize = (long) (fraction * javaMemorySizeMB);\n\t\t\t}\n\n\t\t\theapSizeMB = javaMemorySizeMB - offHeapSize;\n\t\t} else {\n\t\t\theapSizeMB = javaMemorySizeMB;\n\t\t}\n\t\t\n\t\t// (3) obtain the additional environment variables from the configuration\n\t\tfinal HashMap<String, String> envVars = new HashMap<>();\n\t\tfinal String prefix = ConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX;\n\t\t\n\t\tfor (String key : config.keySet()) {\n\t\t\tif (key.startsWith(prefix) && key.length() > prefix.length()) {\n\t\t\t\t// remove prefix\n\t\t\t\tString envVarKey = key.substring(prefix.length());\n\t\t\t\tenvVars.put(envVarKey, config.getString(key, null));\n\t\t\t}\n\t\t}\n\t\t\n\t\t// done\n\t\treturn new ContaineredTaskManagerParameters(\n\t\t\tcontainerMemoryMB, heapSizeMB, javaMemorySizeMB, numSlots, envVars);\n\t}"
        ],
        [
            "TaskManagerStartupTest::testMemoryConfigWrong()",
            " 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168 -\n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\t/**\n\t * Tests that the TaskManager startup fails synchronously when the I/O directories are\n\t * not writable.\n\t */\n\t@Test\n\tpublic void testMemoryConfigWrong() {\n\t\ttry {\n\t\t\tConfiguration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 21656);\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY, \"true\");\n\n\t\t\t// something invalid\n\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -42);\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// splendid!\n\t\t\t}\n\n\t\t\t// something ridiculously high\n\t\t\tfinal long memSize = (((long) Integer.MAX_VALUE - 1) *\n\t\t\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_MEMORY_SEGMENT_SIZE) >> 20;\n\t\t\tcfg.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, memSize);\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\t// splendid!\n\t\t\t\tassertTrue(e.getCause() instanceof OutOfMemoryError);\n\t\t\t}\n\t\t} \n\t\tcatch(Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 +\n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "\t/**\n\t * Tests that the TaskManager startup fails synchronously when the I/O directories are\n\t * not writable.\n\t */\n\t@Test\n\tpublic void testMemoryConfigWrong() {\n\t\ttry {\n\t\t\tConfiguration cfg = new Configuration();\n\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 21656);\n\t\t\tcfg.setString(ConfigConstants.TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY, \"true\");\n\n\t\t\t// something invalid\n\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, -42L);\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// splendid!\n\t\t\t}\n\n\t\t\t// something ridiculously high\n\t\t\tfinal long memSize = (((long) Integer.MAX_VALUE - 1) *\n\t\t\t\t\tTaskManagerOptions.MEMORY_SEGMENT_SIZE.defaultValue()) >> 20;\n\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, memSize);\n\t\t\ttry {\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), 0, cfg);\n\t\t\t\tfail(\"Should fail synchronously with an exception\");\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\t// splendid!\n\t\t\t\tassertTrue(e.getCause() instanceof OutOfMemoryError);\n\t\t\t}\n\t\t} \n\t\tcatch(Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "IPv6HostnamesITCase::testClusterWithIPv6host()",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  ",
            "\t@Test\n\tpublic void testClusterWithIPv6host() {\n\n\t\tfinal Inet6Address ipv6address = getLocalIPv6Address();\n\t\tif (ipv6address == null) {\n\t\t\tSystem.err.println(\"--- Cannot find a non-loopback local IPv6 address that Akka/Netty can bind to; skipping IPv6HostnamesITCase\");\n\t\t\treturn;\n\t\t}\n\n\t\t\n\t\t\n\t\tLocalFlinkMiniCluster flink = null;\n\t\ttry {\n\t\t\tfinal String addressString = ipv6address.getHostAddress();\n\t\t\tlog.info(\"Test will use IPv6 address \" + addressString + \" for connection tests\");\n\t\t\t\n\t\t\tConfiguration conf = new Configuration();\n\t\t\tconf.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, addressString);\n\t\t\tconf.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, addressString);\n\t\t\tconf.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconf.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\t\t\tconf.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 16);\n\t\t\t\n\t\t\tflink = new LocalFlinkMiniCluster(conf, false);\n\t\t\tflink.start();\n\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(addressString, flink.getLeaderRPCPort());\n\t\t\tenv.setParallelism(4);\n\t\t\tenv.getConfig().disableSysoutLogging();\n\t\t\t\n\t\t\t// get input data\n\t\t\tDataSet<String> text = env.fromElements(WordCountData.TEXT.split(\"\\n\"));\n\n\t\t\tDataSet<Tuple2<String, Integer>> counts =text\n\t\t\t\t\t.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {\n\t\t\t\t\t\t\tfor (String token : value.toLowerCase().split(\"\\\\W+\")) {\n\t\t\t\t\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.groupBy(0).sum(1);\n\n\t\t\tList<Tuple2<String, Integer>> result = counts.collect();\n\n\t\t\tTestBaseUtils.compareResultAsText(result, WordCountData.COUNTS_AS_TUPLES);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (flink != null) {\n\t\t\t\tflink.shutdown();\n\t\t\t}\n\t\t}\n\t}",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  ",
            "\t@Test\n\tpublic void testClusterWithIPv6host() {\n\n\t\tfinal Inet6Address ipv6address = getLocalIPv6Address();\n\t\tif (ipv6address == null) {\n\t\t\tSystem.err.println(\"--- Cannot find a non-loopback local IPv6 address that Akka/Netty can bind to; skipping IPv6HostnamesITCase\");\n\t\t\treturn;\n\t\t}\n\n\t\t\n\t\t\n\t\tLocalFlinkMiniCluster flink = null;\n\t\ttry {\n\t\t\tfinal String addressString = ipv6address.getHostAddress();\n\t\t\tlog.info(\"Test will use IPv6 address \" + addressString + \" for connection tests\");\n\t\t\t\n\t\t\tConfiguration conf = new Configuration();\n\t\t\tconf.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, addressString);\n\t\t\tconf.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, addressString);\n\t\t\tconf.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconf.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\t\t\tconf.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n\t\t\t\n\t\t\tflink = new LocalFlinkMiniCluster(conf, false);\n\t\t\tflink.start();\n\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(addressString, flink.getLeaderRPCPort());\n\t\t\tenv.setParallelism(4);\n\t\t\tenv.getConfig().disableSysoutLogging();\n\t\t\t\n\t\t\t// get input data\n\t\t\tDataSet<String> text = env.fromElements(WordCountData.TEXT.split(\"\\n\"));\n\n\t\t\tDataSet<Tuple2<String, Integer>> counts =text\n\t\t\t\t\t.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {\n\t\t\t\t\t\t\tfor (String token : value.toLowerCase().split(\"\\\\W+\")) {\n\t\t\t\t\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.groupBy(0).sum(1);\n\n\t\t\tList<Tuple2<String, Integer>> result = counts.collect();\n\n\t\t\tTestBaseUtils.compareResultAsText(result, WordCountData.COUNTS_AS_TUPLES);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (flink != null) {\n\t\t\t\tflink.shutdown();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "TaskCancelAsyncProducerConsumerITCase::testCancelAsyncProducerAndConsumer()",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 -\n  82 -\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "\t/**\n\t * Tests that a task waiting on an async producer/consumer that is stuck\n\t * in a blocking buffer request can be properly cancelled.\n\t *\n\t * <p>This is currently required for the Flink Kafka sources, which spawn\n\t * a separate Thread consuming from Kafka and producing the intermediate\n\t * streams in the spawned Thread instead of the main task Thread.\n\t */\n\t@Test\n\tpublic void testCancelAsyncProducerAndConsumer() throws Exception {\n\t\tDeadline deadline = new FiniteDuration(2, TimeUnit.MINUTES).fromNow();\n\t\tTestingCluster flink = null;\n\n\t\ttry {\n\t\t\t// Cluster\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY, 4096);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 8);\n\n\t\t\tflink = new TestingCluster(config, true);\n\t\t\tflink.start();\n\n\t\t\t// Job with async producer and consumer\n\t\t\tJobVertex producer = new JobVertex(\"AsyncProducer\");\n\t\t\tproducer.setParallelism(1);\n\t\t\tproducer.setInvokableClass(AsyncProducer.class);\n\n\t\t\tJobVertex consumer = new JobVertex(\"AsyncConsumer\");\n\t\t\tconsumer.setParallelism(1);\n\t\t\tconsumer.setInvokableClass(AsyncConsumer.class);\n\t\t\tconsumer.connectNewDataSetAsInput(producer, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);\n\n\t\t\tSlotSharingGroup slot = new SlotSharingGroup(producer.getID(), consumer.getID());\n\t\t\tproducer.setSlotSharingGroup(slot);\n\t\t\tconsumer.setSlotSharingGroup(slot);\n\n\t\t\tJobGraph jobGraph = new JobGraph(producer, consumer);\n\n\t\t\t// Submit job and wait until running\n\t\t\tActorGateway jobManager = flink.getLeaderGateway(deadline.timeLeft());\n\t\t\tflink.submitJobDetached(jobGraph);\n\n\t\t\tObject msg = new WaitForAllVerticesToBeRunning(jobGraph.getJobID());\n\t\t\tFuture<?> runningFuture = jobManager.ask(msg, deadline.timeLeft());\n\t\t\tAwait.ready(runningFuture, deadline.timeLeft());\n\n\t\t\t// Wait for blocking requests, cancel and wait for cancellation\n\t\t\tmsg = new NotifyWhenJobStatus(jobGraph.getJobID(), JobStatus.CANCELED);\n\t\t\tFuture<?> cancelledFuture = jobManager.ask(msg, deadline.timeLeft());\n\n\t\t\tboolean producerBlocked = false;\n\t\t\tfor (int i = 0; i < 50; i++) {\n\t\t\t\tThread thread = ASYNC_PRODUCER_THREAD;\n\n\t\t\t\tif (thread != null && thread.isAlive()) {\n\t\t\t\t\tStackTraceElement[] stackTrace = thread.getStackTrace();\n\t\t\t\t\tproducerBlocked = isInBlockingBufferRequest(stackTrace);\n\t\t\t\t}\n\n\t\t\t\tif (producerBlocked) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// Retry\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Verify that async producer is in blocking request\n\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n\n\t\t\tboolean consumerWaiting = false;\n\t\t\tfor (int i = 0; i < 50; i++) {\n\t\t\t\tThread thread = ASYNC_CONSUMER_THREAD;\n\n\t\t\t\tif (thread != null && thread.isAlive()) {\n\t\t\t\t\tconsumerWaiting = thread.getState() == Thread.State.WAITING;\n\t\t\t\t}\n\n\t\t\t\tif (consumerWaiting) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// Retry\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Verify that async consumer is in blocking request\n\t\t\tassertTrue(\"Consumer thread is not blocked.\", consumerWaiting);\n\n\t\t\tmsg = new CancelJob(jobGraph.getJobID());\n\t\t\tFuture<?> cancelFuture = jobManager.ask(msg, deadline.timeLeft());\n\t\t\tAwait.ready(cancelFuture, deadline.timeLeft());\n\n\t\t\tAwait.ready(cancelledFuture, deadline.timeLeft());\n\n\t\t\t// Verify the expected Exceptions\n\t\t\tassertNotNull(ASYNC_PRODUCER_EXCEPTION);\n\t\t\tassertEquals(IllegalStateException.class, ASYNC_PRODUCER_EXCEPTION.getClass());\n\n\t\t\tassertNotNull(ASYNC_CONSUMER_EXCEPTION);\n\t\t\tassertEquals(IllegalStateException.class, ASYNC_CONSUMER_EXCEPTION.getClass());\n\t\t} finally {\n\t\t\tif (flink != null) {\n\t\t\t\tflink.shutdown();\n\t\t\t}\n\t\t}\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t/**\n\t * Tests that a task waiting on an async producer/consumer that is stuck\n\t * in a blocking buffer request can be properly cancelled.\n\t *\n\t * <p>This is currently required for the Flink Kafka sources, which spawn\n\t * a separate Thread consuming from Kafka and producing the intermediate\n\t * streams in the spawned Thread instead of the main task Thread.\n\t */\n\t@Test\n\tpublic void testCancelAsyncProducerAndConsumer() throws Exception {\n\t\tDeadline deadline = new FiniteDuration(2, TimeUnit.MINUTES).fromNow();\n\t\tTestingCluster flink = null;\n\n\t\ttry {\n\t\t\t// Cluster\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\t\tconfig.setInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE, 4096);\n\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 8);\n\n\t\t\tflink = new TestingCluster(config, true);\n\t\t\tflink.start();\n\n\t\t\t// Job with async producer and consumer\n\t\t\tJobVertex producer = new JobVertex(\"AsyncProducer\");\n\t\t\tproducer.setParallelism(1);\n\t\t\tproducer.setInvokableClass(AsyncProducer.class);\n\n\t\t\tJobVertex consumer = new JobVertex(\"AsyncConsumer\");\n\t\t\tconsumer.setParallelism(1);\n\t\t\tconsumer.setInvokableClass(AsyncConsumer.class);\n\t\t\tconsumer.connectNewDataSetAsInput(producer, DistributionPattern.POINTWISE, ResultPartitionType.PIPELINED);\n\n\t\t\tSlotSharingGroup slot = new SlotSharingGroup(producer.getID(), consumer.getID());\n\t\t\tproducer.setSlotSharingGroup(slot);\n\t\t\tconsumer.setSlotSharingGroup(slot);\n\n\t\t\tJobGraph jobGraph = new JobGraph(producer, consumer);\n\n\t\t\t// Submit job and wait until running\n\t\t\tActorGateway jobManager = flink.getLeaderGateway(deadline.timeLeft());\n\t\t\tflink.submitJobDetached(jobGraph);\n\n\t\t\tObject msg = new WaitForAllVerticesToBeRunning(jobGraph.getJobID());\n\t\t\tFuture<?> runningFuture = jobManager.ask(msg, deadline.timeLeft());\n\t\t\tAwait.ready(runningFuture, deadline.timeLeft());\n\n\t\t\t// Wait for blocking requests, cancel and wait for cancellation\n\t\t\tmsg = new NotifyWhenJobStatus(jobGraph.getJobID(), JobStatus.CANCELED);\n\t\t\tFuture<?> cancelledFuture = jobManager.ask(msg, deadline.timeLeft());\n\n\t\t\tboolean producerBlocked = false;\n\t\t\tfor (int i = 0; i < 50; i++) {\n\t\t\t\tThread thread = ASYNC_PRODUCER_THREAD;\n\n\t\t\t\tif (thread != null && thread.isAlive()) {\n\t\t\t\t\tStackTraceElement[] stackTrace = thread.getStackTrace();\n\t\t\t\t\tproducerBlocked = isInBlockingBufferRequest(stackTrace);\n\t\t\t\t}\n\n\t\t\t\tif (producerBlocked) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// Retry\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Verify that async producer is in blocking request\n\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n\n\t\t\tboolean consumerWaiting = false;\n\t\t\tfor (int i = 0; i < 50; i++) {\n\t\t\t\tThread thread = ASYNC_CONSUMER_THREAD;\n\n\t\t\t\tif (thread != null && thread.isAlive()) {\n\t\t\t\t\tconsumerWaiting = thread.getState() == Thread.State.WAITING;\n\t\t\t\t}\n\n\t\t\t\tif (consumerWaiting) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// Retry\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Verify that async consumer is in blocking request\n\t\t\tassertTrue(\"Consumer thread is not blocked.\", consumerWaiting);\n\n\t\t\tmsg = new CancelJob(jobGraph.getJobID());\n\t\t\tFuture<?> cancelFuture = jobManager.ask(msg, deadline.timeLeft());\n\t\t\tAwait.ready(cancelFuture, deadline.timeLeft());\n\n\t\t\tAwait.ready(cancelledFuture, deadline.timeLeft());\n\n\t\t\t// Verify the expected Exceptions\n\t\t\tassertNotNull(ASYNC_PRODUCER_EXCEPTION);\n\t\t\tassertEquals(IllegalStateException.class, ASYNC_PRODUCER_EXCEPTION.getClass());\n\n\t\t\tassertNotNull(ASYNC_CONSUMER_EXCEPTION);\n\t\t\tassertEquals(IllegalStateException.class, ASYNC_CONSUMER_EXCEPTION.getClass());\n\t\t} finally {\n\t\t\tif (flink != null) {\n\t\t\t\tflink.shutdown();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "JobSubmissionFailsITCase::setup()",
            "  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS / 2);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config);\n\n\t\t\tcluster.start();\n\t\t\t\n\t\t\tfinal JobVertex jobVertex = new JobVertex(\"Working job vertex.\");\n\t\t\tjobVertex.setInvokableClass(NoOpInvokable.class);\n\t\t\tworkingJobGraph = new JobGraph(\"Working testing job\", jobVertex);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}",
            "  54  \n  55  \n  56  \n  57  \n  58 +\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_SLOTS / 2);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config);\n\n\t\t\tcluster.start();\n\t\t\t\n\t\t\tfinal JobVertex jobVertex = new JobVertex(\"Working job vertex.\");\n\t\t\tjobVertex.setInvokableClass(NoOpInvokable.class);\n\t\t\tworkingJobGraph = new JobGraph(\"Working testing job\", jobVertex);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "JobManagerTest::testKvStateMessages()",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591 -\n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  ",
            "\t/**\n\t * Tests that the JobManager handles {@link org.apache.flink.runtime.query.KvStateMessage}\n\t * instances as expected.\n\t */\n\t@Test\n\tpublic void testKvStateMessages() throws Exception {\n\t\tDeadline deadline = new FiniteDuration(100, TimeUnit.SECONDS).fromNow();\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, \"100ms\");\n\n\t\tActorGateway jobManager = new AkkaActorGateway(\n\t\t\t\tJobManager.startJobManagerActors(\n\t\t\t\t\tconfig,\n\t\t\t\t\tsystem,\n\t\t\t\t\tTestingUtils.defaultExecutor(),\n\t\t\t\t\tTestingUtils.defaultExecutor(),\n\t\t\t\t\tTestingJobManager.class,\n\t\t\t\t\tMemoryArchivist.class)._1(),\n\t\t\t\tHighAvailabilityServices.DEFAULT_LEADER_ID);\n\n\t\tLeaderRetrievalService leaderRetrievalService = new StandaloneLeaderRetrievalService(\n\t\t\t\tAkkaUtils.getAkkaURL(system, jobManager.actor()));\n\n\t\tConfiguration tmConfig = new Configuration();\n\t\ttmConfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\ttmConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\n\t\tActorRef taskManager = TaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\ttmConfig,\n\t\t\t\tResourceID.generate(),\n\t\t\t\tsystem,\n\t\t\t\t\"localhost\",\n\t\t\t\tscala.Option.<String>empty(),\n\t\t\t\tscala.Option.apply(leaderRetrievalService),\n\t\t\t\ttrue,\n\t\t\t\tTestingTaskManager.class);\n\n\t\tFuture<Object> registrationFuture = jobManager\n\t\t\t\t.ask(new NotifyWhenAtLeastNumTaskManagerAreRegistered(1), deadline.timeLeft());\n\n\t\tAwait.ready(registrationFuture, deadline.timeLeft());\n\n\t\t//\n\t\t// Location lookup\n\t\t//\n\t\tLookupKvStateLocation lookupNonExistingJob = new LookupKvStateLocation(\n\t\t\t\tnew JobID(),\n\t\t\t\t\"any-name\");\n\n\t\tFuture<KvStateLocation> lookupFuture = jobManager\n\t\t\t\t.ask(lookupNonExistingJob, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (IllegalStateException ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\tJobGraph jobGraph = new JobGraph(\"croissant\");\n\t\tJobVertex jobVertex1 = new JobVertex(\"cappuccino\");\n\t\tjobVertex1.setParallelism(4);\n\t\tjobVertex1.setMaxParallelism(16);\n\t\tjobVertex1.setInvokableClass(BlockingNoOpInvokable.class);\n\n\t\tJobVertex jobVertex2 = new JobVertex(\"americano\");\n\t\tjobVertex2.setParallelism(4);\n\t\tjobVertex2.setMaxParallelism(16);\n\t\tjobVertex2.setInvokableClass(BlockingNoOpInvokable.class);\n\n\t\tjobGraph.addVertex(jobVertex1);\n\t\tjobGraph.addVertex(jobVertex2);\n\n\t\tFuture<JobSubmitSuccess> submitFuture = jobManager\n\t\t\t\t.ask(new SubmitJob(jobGraph, ListeningBehaviour.DETACHED), deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<JobSubmitSuccess>apply(JobSubmitSuccess.class));\n\n\t\tAwait.result(submitFuture, deadline.timeLeft());\n\n\t\tObject lookupUnknownRegistrationName = new LookupKvStateLocation(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\t\"unknown\");\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupUnknownRegistrationName, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (UnknownKvStateLocation ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\t//\n\t\t// Registration\n\t\t//\n\t\tNotifyKvStateRegistered registerNonExistingJob = new NotifyKvStateRegistered(\n\t\t\t\tnew JobID(),\n\t\t\t\tnew JobVertexID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"any-name\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1233));\n\n\t\tjobManager.tell(registerNonExistingJob);\n\n\t\tLookupKvStateLocation lookupAfterRegistration = new LookupKvStateLocation(\n\t\t\t\tregisterNonExistingJob.getJobId(),\n\t\t\t\tregisterNonExistingJob.getRegistrationName());\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (IllegalStateException ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\tNotifyKvStateRegistered registerForExistingJob = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex1.getID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"register-me\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tjobManager.tell(registerForExistingJob);\n\n\t\tlookupAfterRegistration = new LookupKvStateLocation(\n\t\t\t\tregisterForExistingJob.getJobId(),\n\t\t\t\tregisterForExistingJob.getRegistrationName());\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\tKvStateLocation location = Await.result(lookupFuture, deadline.timeLeft());\n\t\tassertNotNull(location);\n\n\t\tassertEquals(jobGraph.getJobID(), location.getJobId());\n\t\tassertEquals(jobVertex1.getID(), location.getJobVertexId());\n\t\tassertEquals(jobVertex1.getMaxParallelism(), location.getNumKeyGroups());\n\t\tassertEquals(1, location.getNumRegisteredKeyGroups());\n\t\tKeyGroupRange keyGroupRange = registerForExistingJob.getKeyGroupRange();\n\t\tassertEquals(1, keyGroupRange.getNumberOfKeyGroups());\n\t\tassertEquals(registerForExistingJob.getKvStateId(), location.getKvStateID(keyGroupRange.getStartKeyGroup()));\n\t\tassertEquals(registerForExistingJob.getKvStateServerAddress(), location.getKvStateServerAddress(keyGroupRange.getStartKeyGroup()));\n\n\t\t//\n\t\t// Unregistration\n\t\t//\n\t\tNotifyKvStateUnregistered unregister = new NotifyKvStateUnregistered(\n\t\t\t\tregisterForExistingJob.getJobId(),\n\t\t\t\tregisterForExistingJob.getJobVertexId(),\n\t\t\t\tregisterForExistingJob.getKeyGroupRange(),\n\t\t\t\tregisterForExistingJob.getRegistrationName());\n\n\t\tjobManager.tell(unregister);\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (UnknownKvStateLocation ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\t//\n\t\t// Duplicate registration fails task\n\t\t//\n\t\tNotifyKvStateRegistered register = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex1.getID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"duplicate-me\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tNotifyKvStateRegistered duplicate = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex2.getID(), // <--- different operator, but...\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"duplicate-me\", // ...same name\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tFuture<TestingJobManagerMessages.JobStatusIs> failedFuture = jobManager\n\t\t\t\t.ask(new NotifyWhenJobStatus(jobGraph.getJobID(), JobStatus.FAILED), deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<JobStatusIs>apply(JobStatusIs.class));\n\n\t\tjobManager.tell(register);\n\t\tjobManager.tell(duplicate);\n\n\t\t// Wait for failure\n\t\tJobStatusIs jobStatus = Await.result(failedFuture, deadline.timeLeft());\n\t\tassertEquals(JobStatus.FAILED, jobStatus.state());\n\t}",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591 +\n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  ",
            "\t/**\n\t * Tests that the JobManager handles {@link org.apache.flink.runtime.query.KvStateMessage}\n\t * instances as expected.\n\t */\n\t@Test\n\tpublic void testKvStateMessages() throws Exception {\n\t\tDeadline deadline = new FiniteDuration(100, TimeUnit.SECONDS).fromNow();\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, \"100ms\");\n\n\t\tActorGateway jobManager = new AkkaActorGateway(\n\t\t\t\tJobManager.startJobManagerActors(\n\t\t\t\t\tconfig,\n\t\t\t\t\tsystem,\n\t\t\t\t\tTestingUtils.defaultExecutor(),\n\t\t\t\t\tTestingUtils.defaultExecutor(),\n\t\t\t\t\tTestingJobManager.class,\n\t\t\t\t\tMemoryArchivist.class)._1(),\n\t\t\t\tHighAvailabilityServices.DEFAULT_LEADER_ID);\n\n\t\tLeaderRetrievalService leaderRetrievalService = new StandaloneLeaderRetrievalService(\n\t\t\t\tAkkaUtils.getAkkaURL(system, jobManager.actor()));\n\n\t\tConfiguration tmConfig = new Configuration();\n\t\ttmConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\ttmConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\n\t\tActorRef taskManager = TaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\ttmConfig,\n\t\t\t\tResourceID.generate(),\n\t\t\t\tsystem,\n\t\t\t\t\"localhost\",\n\t\t\t\tscala.Option.<String>empty(),\n\t\t\t\tscala.Option.apply(leaderRetrievalService),\n\t\t\t\ttrue,\n\t\t\t\tTestingTaskManager.class);\n\n\t\tFuture<Object> registrationFuture = jobManager\n\t\t\t\t.ask(new NotifyWhenAtLeastNumTaskManagerAreRegistered(1), deadline.timeLeft());\n\n\t\tAwait.ready(registrationFuture, deadline.timeLeft());\n\n\t\t//\n\t\t// Location lookup\n\t\t//\n\t\tLookupKvStateLocation lookupNonExistingJob = new LookupKvStateLocation(\n\t\t\t\tnew JobID(),\n\t\t\t\t\"any-name\");\n\n\t\tFuture<KvStateLocation> lookupFuture = jobManager\n\t\t\t\t.ask(lookupNonExistingJob, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (IllegalStateException ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\tJobGraph jobGraph = new JobGraph(\"croissant\");\n\t\tJobVertex jobVertex1 = new JobVertex(\"cappuccino\");\n\t\tjobVertex1.setParallelism(4);\n\t\tjobVertex1.setMaxParallelism(16);\n\t\tjobVertex1.setInvokableClass(BlockingNoOpInvokable.class);\n\n\t\tJobVertex jobVertex2 = new JobVertex(\"americano\");\n\t\tjobVertex2.setParallelism(4);\n\t\tjobVertex2.setMaxParallelism(16);\n\t\tjobVertex2.setInvokableClass(BlockingNoOpInvokable.class);\n\n\t\tjobGraph.addVertex(jobVertex1);\n\t\tjobGraph.addVertex(jobVertex2);\n\n\t\tFuture<JobSubmitSuccess> submitFuture = jobManager\n\t\t\t\t.ask(new SubmitJob(jobGraph, ListeningBehaviour.DETACHED), deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<JobSubmitSuccess>apply(JobSubmitSuccess.class));\n\n\t\tAwait.result(submitFuture, deadline.timeLeft());\n\n\t\tObject lookupUnknownRegistrationName = new LookupKvStateLocation(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\t\"unknown\");\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupUnknownRegistrationName, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (UnknownKvStateLocation ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\t//\n\t\t// Registration\n\t\t//\n\t\tNotifyKvStateRegistered registerNonExistingJob = new NotifyKvStateRegistered(\n\t\t\t\tnew JobID(),\n\t\t\t\tnew JobVertexID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"any-name\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1233));\n\n\t\tjobManager.tell(registerNonExistingJob);\n\n\t\tLookupKvStateLocation lookupAfterRegistration = new LookupKvStateLocation(\n\t\t\t\tregisterNonExistingJob.getJobId(),\n\t\t\t\tregisterNonExistingJob.getRegistrationName());\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (IllegalStateException ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\tNotifyKvStateRegistered registerForExistingJob = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex1.getID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"register-me\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tjobManager.tell(registerForExistingJob);\n\n\t\tlookupAfterRegistration = new LookupKvStateLocation(\n\t\t\t\tregisterForExistingJob.getJobId(),\n\t\t\t\tregisterForExistingJob.getRegistrationName());\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\tKvStateLocation location = Await.result(lookupFuture, deadline.timeLeft());\n\t\tassertNotNull(location);\n\n\t\tassertEquals(jobGraph.getJobID(), location.getJobId());\n\t\tassertEquals(jobVertex1.getID(), location.getJobVertexId());\n\t\tassertEquals(jobVertex1.getMaxParallelism(), location.getNumKeyGroups());\n\t\tassertEquals(1, location.getNumRegisteredKeyGroups());\n\t\tKeyGroupRange keyGroupRange = registerForExistingJob.getKeyGroupRange();\n\t\tassertEquals(1, keyGroupRange.getNumberOfKeyGroups());\n\t\tassertEquals(registerForExistingJob.getKvStateId(), location.getKvStateID(keyGroupRange.getStartKeyGroup()));\n\t\tassertEquals(registerForExistingJob.getKvStateServerAddress(), location.getKvStateServerAddress(keyGroupRange.getStartKeyGroup()));\n\n\t\t//\n\t\t// Unregistration\n\t\t//\n\t\tNotifyKvStateUnregistered unregister = new NotifyKvStateUnregistered(\n\t\t\t\tregisterForExistingJob.getJobId(),\n\t\t\t\tregisterForExistingJob.getJobVertexId(),\n\t\t\t\tregisterForExistingJob.getKeyGroupRange(),\n\t\t\t\tregisterForExistingJob.getRegistrationName());\n\n\t\tjobManager.tell(unregister);\n\n\t\tlookupFuture = jobManager\n\t\t\t\t.ask(lookupAfterRegistration, deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<KvStateLocation>apply(KvStateLocation.class));\n\n\t\ttry {\n\t\t\tAwait.result(lookupFuture, deadline.timeLeft());\n\t\t\tfail(\"Did not throw expected Exception\");\n\t\t} catch (UnknownKvStateLocation ignored) {\n\t\t\t// Expected\n\t\t}\n\n\t\t//\n\t\t// Duplicate registration fails task\n\t\t//\n\t\tNotifyKvStateRegistered register = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex1.getID(),\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"duplicate-me\",\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tNotifyKvStateRegistered duplicate = new NotifyKvStateRegistered(\n\t\t\t\tjobGraph.getJobID(),\n\t\t\t\tjobVertex2.getID(), // <--- different operator, but...\n\t\t\t\tnew KeyGroupRange(0, 0),\n\t\t\t\t\"duplicate-me\", // ...same name\n\t\t\t\tnew KvStateID(),\n\t\t\t\tnew KvStateServerAddress(InetAddress.getLocalHost(), 1293));\n\n\t\tFuture<TestingJobManagerMessages.JobStatusIs> failedFuture = jobManager\n\t\t\t\t.ask(new NotifyWhenJobStatus(jobGraph.getJobID(), JobStatus.FAILED), deadline.timeLeft())\n\t\t\t\t.mapTo(ClassTag$.MODULE$.<JobStatusIs>apply(JobStatusIs.class));\n\n\t\tjobManager.tell(register);\n\t\tjobManager.tell(duplicate);\n\n\t\t// Wait for failure\n\t\tJobStatusIs jobStatus = Await.result(failedFuture, deadline.timeLeft());\n\t\tassertEquals(JobStatus.FAILED, jobStatus.state());\n\t}"
        ],
        [
            "PartialConsumePipelinedResultTest::setUp()",
            "  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54 -\n  55  \n  56  \n  57  \n  58  \n  59  ",
            "\t@BeforeClass\n\tpublic static void setUp() throws Exception {\n\t\tfinal Configuration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUMBER_OF_TMS);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUMBER_OF_SLOTS_PER_TM);\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT());\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, NUMBER_OF_NETWORK_BUFFERS);\n\n\t\tflink = new TestingCluster(config, true);\n\n\t\tflink.start();\n\t}",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55 +\n  56  \n  57  \n  58  \n  59  \n  60  ",
            "\t@BeforeClass\n\tpublic static void setUp() throws Exception {\n\t\tfinal Configuration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUMBER_OF_TMS);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUMBER_OF_SLOTS_PER_TM);\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT());\n\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, NUMBER_OF_NETWORK_BUFFERS);\n\n\t\tflink = new TestingCluster(config, true);\n\n\t\tflink.start();\n\t}"
        ],
        [
            "AbstractTaskManagerProcessFailureRecoveryTest::TaskManagerProcessEntryPoint::main(String)",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392 -\n 393 -\n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  ",
            "\t\tpublic static void main(String[] args) {\n\t\t\ttry {\n\t\t\t\tint jobManagerPort = Integer.parseInt(args[0]);\n\n\t\t\t\tConfiguration cfg = new Configuration();\n\t\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, jobManagerPort);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 100);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\t\t\t\tcfg.setString(ConfigConstants.AKKA_ASK_TIMEOUT, \"100 s\");\n\n\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(cfg,\n\t\t\t\t\tResourceID.generate(), TaskManager.class);\n\n\t\t\t\t// wait forever\n\t\t\t\tObject lock = new Object();\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tlock.wait();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to start TaskManager process\", t);\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393 +\n 394 +\n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  ",
            "\t\tpublic static void main(String[] args) {\n\t\t\ttry {\n\t\t\t\tint jobManagerPort = Integer.parseInt(args[0]);\n\n\t\t\t\tConfiguration cfg = new Configuration();\n\t\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, jobManagerPort);\n\t\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\t\tcfg.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 100);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\t\t\t\tcfg.setString(ConfigConstants.AKKA_ASK_TIMEOUT, \"100 s\");\n\n\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(cfg,\n\t\t\t\t\tResourceID.generate(), TaskManager.class);\n\n\t\t\t\t// wait forever\n\t\t\t\tObject lock = new Object();\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tlock.wait();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to start TaskManager process\", t);\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}"
        ],
        [
            "StreamFaultToleranceTestBase::startCluster()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_TASK_SLOTS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 12);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TASK_MANAGERS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, NUM_TASK_SLOTS);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 12L);\n\t\t\t\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "Flip6LocalStreamEnvironment::execute(String)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "\t/**\n\t * Executes the JobGraph of the on a mini cluster of CLusterUtil with a user\n\t * specified name.\n\t * \n\t * @param jobName\n\t *            name of the job\n\t * @return The result of the job execution, containing elapsed time and accumulators.\n\t */\n\t@Override\n\tpublic JobExecutionResult execute(String jobName) throws Exception {\n\t\t// transform the streaming program into a JobGraph\n\t\tStreamGraph streamGraph = getStreamGraph();\n\t\tstreamGraph.setJobName(jobName);\n\n\t\t// TODO - temp fix to enforce restarts due to a bug in the allocation protocol\n\t\tstreamGraph.getExecutionConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 5));\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\t\tjobGraph.setAllowQueuedScheduling(true);\n\n\t\tConfiguration configuration = new Configuration();\n\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\t\tconfiguration.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\n\t\t// add (and override) the settings with what the user defined\n\t\tconfiguration.addAll(this.conf);\n\n\t\tMiniClusterConfiguration cfg = new MiniClusterConfiguration(configuration);\n\n\t\t// Currently we do not reuse slot anymore,\n\t\t// so we need to sum up the parallelism of all vertices\n\t\tint slotsCount = 0;\n\t\tfor (JobVertex jobVertex : jobGraph.getVertices()) {\n\t\t\tslotsCount += jobVertex.getParallelism();\n\t\t}\n\t\tcfg.setNumTaskManagerSlots(slotsCount);\n\n\t\tif (LOG.isInfoEnabled()) {\n\t\t\tLOG.info(\"Running job on local embedded Flink mini cluster\");\n\t\t}\n\n\t\tMiniCluster miniCluster = new MiniCluster(cfg);\n\t\ttry {\n\t\t\tminiCluster.start();\n\t\t\tminiCluster.waitUntilTaskManagerRegistrationsComplete();\n\t\t\treturn miniCluster.runJobBlocking(jobGraph);\n\t\t}\n\t\tfinally {\n\t\t\ttransformations.clear();\n\t\t\tminiCluster.shutdown();\n\t\t}\n\t}",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "\t/**\n\t * Executes the JobGraph of the on a mini cluster of CLusterUtil with a user\n\t * specified name.\n\t * \n\t * @param jobName\n\t *            name of the job\n\t * @return The result of the job execution, containing elapsed time and accumulators.\n\t */\n\t@Override\n\tpublic JobExecutionResult execute(String jobName) throws Exception {\n\t\t// transform the streaming program into a JobGraph\n\t\tStreamGraph streamGraph = getStreamGraph();\n\t\tstreamGraph.setJobName(jobName);\n\n\t\t// TODO - temp fix to enforce restarts due to a bug in the allocation protocol\n\t\tstreamGraph.getExecutionConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 5));\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\t\tjobGraph.setAllowQueuedScheduling(true);\n\n\t\tConfiguration configuration = new Configuration();\n\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\t\tconfiguration.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, -1L);\n\n\t\t// add (and override) the settings with what the user defined\n\t\tconfiguration.addAll(this.conf);\n\n\t\tMiniClusterConfiguration cfg = new MiniClusterConfiguration(configuration);\n\n\t\t// Currently we do not reuse slot anymore,\n\t\t// so we need to sum up the parallelism of all vertices\n\t\tint slotsCount = 0;\n\t\tfor (JobVertex jobVertex : jobGraph.getVertices()) {\n\t\t\tslotsCount += jobVertex.getParallelism();\n\t\t}\n\t\tcfg.setNumTaskManagerSlots(slotsCount);\n\n\t\tif (LOG.isInfoEnabled()) {\n\t\t\tLOG.info(\"Running job on local embedded Flink mini cluster\");\n\t\t}\n\n\t\tMiniCluster miniCluster = new MiniCluster(cfg);\n\t\ttry {\n\t\t\tminiCluster.start();\n\t\t\tminiCluster.waitUntilTaskManagerRegistrationsComplete();\n\t\t\treturn miniCluster.runJobBlocking(jobGraph);\n\t\t}\n\t\tfinally {\n\t\t\ttransformations.clear();\n\t\t\tminiCluster.shutdown();\n\t\t}\n\t}"
        ],
        [
            "FlinkLocalCluster::submitTopologyWithOpts(String,Map,FlinkTopology,SubmitOptions)",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "\t@SuppressWarnings(\"rawtypes\")\n\tpublic void submitTopologyWithOpts(final String topologyName, final Map conf, final FlinkTopology topology, final SubmitOptions submitOpts) throws Exception {\n\t\tLOG.info(\"Running Storm topology on FlinkLocalCluster\");\n\n\t\tboolean submitBlocking = false;\n\t\tif (conf != null) {\n\t\t\tObject blockingFlag = conf.get(SUBMIT_BLOCKING);\n\t\t\tif(blockingFlag != null && blockingFlag instanceof Boolean) {\n\t\t\t\tsubmitBlocking = ((Boolean)blockingFlag).booleanValue();\n\t\t\t}\n\t\t}\n\n\t\tFlinkClient.addStormConfigToTopology(topology, conf);\n\n\t\tStreamGraph streamGraph = topology.getExecutionEnvironment().getStreamGraph();\n\t\tstreamGraph.setJobName(topologyName);\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tif (this.flink == null) {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\n\t\t\tconfiguration.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\t\t\tconfiguration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());\n\n\t\t\tthis.flink = new LocalFlinkMiniCluster(configuration, true);\n\t\t\tthis.flink.start();\n\t\t}\n\n\t\tif (submitBlocking) {\n\t\t\tthis.flink.submitJobAndWait(jobGraph, false);\n\t\t} else {\n\t\t\tthis.flink.submitJobDetached(jobGraph);\n\t\t}\n\t}",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\t@SuppressWarnings(\"rawtypes\")\n\tpublic void submitTopologyWithOpts(final String topologyName, final Map conf, final FlinkTopology topology, final SubmitOptions submitOpts) throws Exception {\n\t\tLOG.info(\"Running Storm topology on FlinkLocalCluster\");\n\n\t\tboolean submitBlocking = false;\n\t\tif (conf != null) {\n\t\t\tObject blockingFlag = conf.get(SUBMIT_BLOCKING);\n\t\t\tif(blockingFlag != null && blockingFlag instanceof Boolean) {\n\t\t\t\tsubmitBlocking = ((Boolean)blockingFlag).booleanValue();\n\t\t\t}\n\t\t}\n\n\t\tFlinkClient.addStormConfigToTopology(topology, conf);\n\n\t\tStreamGraph streamGraph = topology.getExecutionEnvironment().getStreamGraph();\n\t\tstreamGraph.setJobName(topologyName);\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tif (this.flink == null) {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tconfiguration.addAll(jobGraph.getJobConfiguration());\n\n\t\t\tconfiguration.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, -1L);\n\t\t\tconfiguration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());\n\n\t\t\tthis.flink = new LocalFlinkMiniCluster(configuration, true);\n\t\t\tthis.flink.start();\n\t\t}\n\n\t\tif (submitBlocking) {\n\t\t\tthis.flink.submitJobAndWait(jobGraph, false);\n\t\t} else {\n\t\t\tthis.flink.submitJobDetached(jobGraph);\n\t\t}\n\t}"
        ],
        [
            "StreamingScalabilityAndLatency::main(String)",
            "  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tif ((Runtime.getRuntime().maxMemory() >>> 20) < 5000) {\n\t\t\tthrow new RuntimeException(\"This test program needs to run with at least 5GB of heap space.\");\n\t\t}\n\t\t\n\t\tfinal int TASK_MANAGERS = 1;\n\t\tfinal int SLOTS_PER_TASK_MANAGER = 80;\n\t\tfinal int PARALLELISM = TASK_MANAGERS * SLOTS_PER_TASK_MANAGER;\n\n\t\tLocalFlinkMiniCluster cluster = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, TASK_MANAGERS);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 80);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, SLOTS_PER_TASK_MANAGER);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 20000);\n\n\t\t\tconfig.setInteger(\"taskmanager.net.server.numThreads\", 1);\n\t\t\tconfig.setInteger(\"taskmanager.net.client.numThreads\", 1);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\t\t\t\n\t\t\trunPartitioningProgram(cluster.getLeaderRPCPort(), PARALLELISM);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51  \n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tif ((Runtime.getRuntime().maxMemory() >>> 20) < 5000) {\n\t\t\tthrow new RuntimeException(\"This test program needs to run with at least 5GB of heap space.\");\n\t\t}\n\t\t\n\t\tfinal int TASK_MANAGERS = 1;\n\t\tfinal int SLOTS_PER_TASK_MANAGER = 80;\n\t\tfinal int PARALLELISM = TASK_MANAGERS * SLOTS_PER_TASK_MANAGER;\n\n\t\tLocalFlinkMiniCluster cluster = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, TASK_MANAGERS);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 80L);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, SLOTS_PER_TASK_MANAGER);\n\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 20000);\n\n\t\t\tconfig.setInteger(\"taskmanager.net.server.numThreads\", 1);\n\t\t\tconfig.setInteger(\"taskmanager.net.client.numThreads\", 1);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\t\t\t\n\t\t\trunPartitioningProgram(cluster.getLeaderRPCPort(), PARALLELISM);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "TaskManagerProcessReapingTestBase::TaskManagerTestEntryPoint::main(String)",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231 -\n 232 -\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  ",
            "\t\tpublic static void main(String[] args) {\n\t\t\ttry {\n\t\t\t\tint jobManagerPort = Integer.parseInt(args[0]);\n\t\t\t\tint taskManagerPort = Integer.parseInt(args[1]);\n\n\t\t\t\tConfiguration cfg = new Configuration();\n\t\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, jobManagerPort);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\t\tcfg.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 256);\n\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), taskManagerPort, cfg);\n\n\t\t\t\t// wait forever\n\t\t\t\tObject lock = new Object();\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tlock.wait();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}",
            " 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232 +\n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t\tpublic static void main(String[] args) {\n\t\t\ttry {\n\t\t\t\tint jobManagerPort = Integer.parseInt(args[0]);\n\t\t\t\tint taskManagerPort = Integer.parseInt(args[1]);\n\n\t\t\t\tConfiguration cfg = new Configuration();\n\t\t\t\tcfg.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\t\t\tcfg.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, jobManagerPort);\n\t\t\t\tcfg.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\t\tcfg.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 256);\n\n\t\t\t\tTaskManager.runTaskManager(\"localhost\", ResourceID.generate(), taskManagerPort, cfg);\n\n\t\t\t\t// wait forever\n\t\t\t\tObject lock = new Object();\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tlock.wait();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tSystem.exit(1);\n\t\t\t}\n\t\t}"
        ],
        [
            "KafkaShortRetentionTestBase::prepare()",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  ",
            "\t@BeforeClass\n\tpublic static void prepare() throws IOException, ClassNotFoundException {\n\t\tLOG.info(\"-------------------------------------------------------------------------\");\n\t\tLOG.info(\"    Starting KafkaShortRetentionTestBase \");\n\t\tLOG.info(\"-------------------------------------------------------------------------\");\n\n\t\tConfiguration flinkConfig = new Configuration();\n\n\t\t// dynamically load the implementation for the test\n\t\tClass<?> clazz = Class.forName(\"org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl\");\n\t\tkafkaServer = (KafkaTestEnvironment) InstantiationUtil.instantiate(clazz);\n\n\t\tLOG.info(\"Starting KafkaTestBase.prepare() for Kafka \" + kafkaServer.getVersion());\n\n\t\tif(kafkaServer.isSecureRunSupported()) {\n\t\t\tsecureProps = kafkaServer.getSecureProperties();\n\t\t}\n\n\t\tProperties specificProperties = new Properties();\n\t\tspecificProperties.setProperty(\"log.retention.hours\", \"0\");\n\t\tspecificProperties.setProperty(\"log.retention.minutes\", \"0\");\n\t\tspecificProperties.setProperty(\"log.retention.ms\", \"250\");\n\t\tspecificProperties.setProperty(\"log.retention.check.interval.ms\", \"100\");\n\t\tkafkaServer.prepare(1, specificProperties, false);\n\n\t\tstandardProps = kafkaServer.getStandardProperties();\n\n\t\t// start also a re-usable Flink mini cluster\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tflink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\t}",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  ",
            "\t@BeforeClass\n\tpublic static void prepare() throws IOException, ClassNotFoundException {\n\t\tLOG.info(\"-------------------------------------------------------------------------\");\n\t\tLOG.info(\"    Starting KafkaShortRetentionTestBase \");\n\t\tLOG.info(\"-------------------------------------------------------------------------\");\n\n\t\tConfiguration flinkConfig = new Configuration();\n\n\t\t// dynamically load the implementation for the test\n\t\tClass<?> clazz = Class.forName(\"org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl\");\n\t\tkafkaServer = (KafkaTestEnvironment) InstantiationUtil.instantiate(clazz);\n\n\t\tLOG.info(\"Starting KafkaTestBase.prepare() for Kafka \" + kafkaServer.getVersion());\n\n\t\tif(kafkaServer.isSecureRunSupported()) {\n\t\t\tsecureProps = kafkaServer.getSecureProperties();\n\t\t}\n\n\t\tProperties specificProperties = new Properties();\n\t\tspecificProperties.setProperty(\"log.retention.hours\", \"0\");\n\t\tspecificProperties.setProperty(\"log.retention.minutes\", \"0\");\n\t\tspecificProperties.setProperty(\"log.retention.ms\", \"250\");\n\t\tspecificProperties.setProperty(\"log.retention.check.interval.ms\", \"100\");\n\t\tkafkaServer.prepare(1, specificProperties, false);\n\n\t\tstandardProps = kafkaServer.getStandardProperties();\n\n\t\t// start also a re-usable Flink mini cluster\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tflink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\t}"
        ],
        [
            "TaskManagerServicesConfiguration::fromConfiguration(Configuration,InetAddress,boolean)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186 -\n 187 -\n 188  \n 189  \n 190  \n 191  \n 192 -\n 193 -\n 194 -\n 195  \n 196 -\n 197 -\n 198 -\n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t/**\n\t * Utility method to extract TaskManager config parameters from the configuration and to\n\t * sanity check them.\n\t *\n\t * @param configuration The configuration.\n\t * @param remoteAddress identifying the IP address under which the TaskManager will be accessible\n\t * @param localCommunication True, to skip initializing the network stack.\n\t *                                      Use only in cases where only one task manager runs.\n\t * @return TaskExecutorConfiguration that wrappers InstanceConnectionInfo, NetworkEnvironmentConfiguration, etc.\n\t */\n\tpublic static TaskManagerServicesConfiguration fromConfiguration(\n\t\t\tConfiguration configuration,\n\t\t\tInetAddress remoteAddress,\n\t\t\tboolean localCommunication) throws Exception {\n\n\t\t// we need this because many configs have been written with a \"-1\" entry\n\t\tint slots = configuration.getInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\tif (slots == -1) {\n\t\t\tslots = 1;\n\t\t}\n\n\t\tfinal String[] tmpDirs = configuration.getString(\n\t\t\tConfigConstants.TASK_MANAGER_TMP_DIR_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH).split(\",|\" + File.pathSeparator);\n\n\t\tfinal NetworkEnvironmentConfiguration networkConfig = parseNetworkEnvironmentConfiguration(\n\t\t\tconfiguration,\n\t\t\tlocalCommunication,\n\t\t\tremoteAddress,\n\t\t\tslots);\n\n\t\tfinal QueryableStateConfiguration queryableStateConfig = localCommunication ?\n\t\t\t\tQueryableStateConfiguration.disabled() :\n\t\t\t\tparseQueryableStateConfiguration(configuration);\n\n\t\t// extract memory settings\n\t\tlong configuredMemory = configuration.getLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\t\tcheckConfigParameter(configuredMemory == -1 || configuredMemory > 0, configuredMemory,\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY,\n\t\t\t\"MemoryManager needs at least one MB of memory. \" +\n\t\t\t\t\"If you leave this config parameter empty, the system automatically \" +\n\t\t\t\t\"pick a fraction of the available memory.\");\n\n\t\tboolean preAllocateMemory = configuration.getBoolean(\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_PRE_ALLOCATE_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_MEMORY_PRE_ALLOCATE);\n\n\t\tfloat memoryFraction = configuration.getFloat(\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,\n\t\t\tConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION);\n\t\tcheckConfigParameter(memoryFraction > 0.0f && memoryFraction < 1.0f, memoryFraction,\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY,\n\t\t\t\"MemoryManager fraction of the free memory must be between 0.0 and 1.0\");\n\n\t\tfinal MetricRegistryConfiguration metricRegistryConfiguration = MetricRegistryConfiguration.fromConfiguration(configuration);\n\n\t\tlong timerServiceShutdownTimeout = AkkaUtils.getTimeout(configuration).toMillis();\n\n\t\treturn new TaskManagerServicesConfiguration(\n\t\t\tremoteAddress,\n\t\t\ttmpDirs,\n\t\t\tnetworkConfig,\n\t\t\tqueryableStateConfig,\n\t\t\tslots,\n\t\t\tconfiguredMemory,\n\t\t\tpreAllocateMemory,\n\t\t\tmemoryFraction,\n\t\t\tmetricRegistryConfiguration,\n\t\t\ttimerServiceShutdownTimeout);\n\t}",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  \n 196 +\n 197  \n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  ",
            "\t/**\n\t * Utility method to extract TaskManager config parameters from the configuration and to\n\t * sanity check them.\n\t *\n\t * @param configuration The configuration.\n\t * @param remoteAddress identifying the IP address under which the TaskManager will be accessible\n\t * @param localCommunication True, to skip initializing the network stack.\n\t *                                      Use only in cases where only one task manager runs.\n\t * @return TaskExecutorConfiguration that wrappers InstanceConnectionInfo, NetworkEnvironmentConfiguration, etc.\n\t */\n\tpublic static TaskManagerServicesConfiguration fromConfiguration(\n\t\t\tConfiguration configuration,\n\t\t\tInetAddress remoteAddress,\n\t\t\tboolean localCommunication) throws Exception {\n\n\t\t// we need this because many configs have been written with a \"-1\" entry\n\t\tint slots = configuration.getInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\tif (slots == -1) {\n\t\t\tslots = 1;\n\t\t}\n\n\t\tfinal String[] tmpDirs = configuration.getString(\n\t\t\tConfigConstants.TASK_MANAGER_TMP_DIR_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH).split(\",|\" + File.pathSeparator);\n\n\t\tfinal NetworkEnvironmentConfiguration networkConfig = parseNetworkEnvironmentConfiguration(\n\t\t\tconfiguration,\n\t\t\tlocalCommunication,\n\t\t\tremoteAddress,\n\t\t\tslots);\n\n\t\tfinal QueryableStateConfiguration queryableStateConfig = localCommunication ?\n\t\t\t\tQueryableStateConfiguration.disabled() :\n\t\t\t\tparseQueryableStateConfiguration(configuration);\n\n\t\t// extract memory settings\n\t\tlong configuredMemory = configuration.getLong(TaskManagerOptions.MANAGED_MEMORY_SIZE);\n\t\tcheckConfigParameter(\n\t\t\tconfiguredMemory == TaskManagerOptions.MANAGED_MEMORY_SIZE.defaultValue() ||\n\t\t\t\tconfiguredMemory > 0, configuredMemory,\n\t\t\tTaskManagerOptions.MANAGED_MEMORY_SIZE.key(),\n\t\t\t\"MemoryManager needs at least one MB of memory. \" +\n\t\t\t\t\"If you leave this config parameter empty, the system automatically \" +\n\t\t\t\t\"pick a fraction of the available memory.\");\n\n\t\tboolean preAllocateMemory = configuration.getBoolean(TaskManagerOptions.MANAGED_MEMORY_PRE_ALLOCATE);\n\n\t\tfloat memoryFraction = configuration.getFloat(TaskManagerOptions.MANAGED_MEMORY_FRACTION);\n\t\tcheckConfigParameter(memoryFraction > 0.0f && memoryFraction < 1.0f, memoryFraction,\n\t\t\tTaskManagerOptions.MANAGED_MEMORY_FRACTION.key(),\n\t\t\t\"MemoryManager fraction of the free memory must be between 0.0 and 1.0\");\n\n\t\tfinal MetricRegistryConfiguration metricRegistryConfiguration = MetricRegistryConfiguration.fromConfiguration(configuration);\n\n\t\tlong timerServiceShutdownTimeout = AkkaUtils.getTimeout(configuration).toMillis();\n\n\t\treturn new TaskManagerServicesConfiguration(\n\t\t\tremoteAddress,\n\t\t\ttmpDirs,\n\t\t\tnetworkConfig,\n\t\t\tqueryableStateConfig,\n\t\t\tslots,\n\t\t\tconfiguredMemory,\n\t\t\tpreAllocateMemory,\n\t\t\tmemoryFraction,\n\t\t\tmetricRegistryConfiguration,\n\t\t\ttimerServiceShutdownTimeout);\n\t}"
        ],
        [
            "JobManagerHAProcessFailureBatchRecoveryITCase::testJobManagerProcessFailure()",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262 -\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  ",
            "\t@Test\n\tpublic void testJobManagerProcessFailure() throws Exception {\n\t\t// Config\n\t\tfinal int numberOfJobManagers = 2;\n\t\tfinal int numberOfTaskManagers = 2;\n\t\tfinal int numberOfSlotsPerTaskManager = 2;\n\n\t\tassertEquals(PARALLELISM, numberOfTaskManagers * numberOfSlotsPerTaskManager);\n\n\t\t// Setup\n\t\t// Test actor system\n\t\tActorSystem testActorSystem;\n\n\t\t// Job managers\n\t\tfinal JobManagerProcess[] jmProcess = new JobManagerProcess[numberOfJobManagers];\n\n\t\t// Task managers\n\t\tfinal ActorSystem[] tmActorSystem = new ActorSystem[numberOfTaskManagers];\n\n\t\t// Leader election service\n\t\tLeaderRetrievalService leaderRetrievalService = null;\n\n\t\t// Coordination between the processes goes through a directory\n\t\tFile coordinateTempDir = null;\n\n\t\ttry {\n\t\t\tfinal Deadline deadline = TestTimeOut.fromNow();\n\n\t\t\t// Coordination directory\n\t\t\tcoordinateTempDir = createTempDirectory();\n\n\t\t\t// Job Managers\n\t\t\tConfiguration config = ZooKeeperTestUtils.createZooKeeperHAConfig(\n\t\t\t\t\tZooKeeper.getConnectString(), FileStateBackendBasePath.getPath());\n\n\t\t\t// Start first process\n\t\t\tjmProcess[0] = new JobManagerProcess(0, config);\n\t\t\tjmProcess[0].startProcess();\n\n\t\t\t// Task manager configuration\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 4);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 100);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\n\t\t\t// Start the task manager process\n\t\t\tfor (int i = 0; i < numberOfTaskManagers; i++) {\n\t\t\t\ttmActorSystem[i] = AkkaUtils.createActorSystem(AkkaUtils.getDefaultAkkaConfig());\n\t\t\t\tTaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\t\t\tconfig, ResourceID.generate(), tmActorSystem[i], \"localhost\",\n\t\t\t\t\t\tOption.<String>empty(), Option.<LeaderRetrievalService>empty(),\n\t\t\t\t\t\tfalse, TaskManager.class);\n\t\t\t}\n\n\t\t\t// Test actor system\n\t\t\ttestActorSystem = AkkaUtils.createActorSystem(AkkaUtils.getDefaultAkkaConfig());\n\n\t\t\tjmProcess[0].getActorRef(testActorSystem, deadline.timeLeft());\n\n\t\t\t// Leader listener\n\t\t\tTestingListener leaderListener = new TestingListener();\n\t\t\tleaderRetrievalService = ZooKeeperUtils.createLeaderRetrievalService(config);\n\t\t\tleaderRetrievalService.start(leaderListener);\n\n\t\t\t// Initial submission\n\t\t\tleaderListener.waitForNewLeader(deadline.timeLeft().toMillis());\n\n\t\t\tString leaderAddress = leaderListener.getAddress();\n\t\t\tUUID leaderId = leaderListener.getLeaderSessionID();\n\n\t\t\t// Get the leader ref\n\t\t\tActorRef leaderRef = AkkaUtils.getActorRef(leaderAddress, testActorSystem, deadline.timeLeft());\n\t\t\tActorGateway leaderGateway = new AkkaActorGateway(leaderRef, leaderId);\n\n\t\t\t// Wait for all task managers to connect to the leading job manager\n\t\t\tJobManagerActorTestUtils.waitForTaskManagers(numberOfTaskManagers, leaderGateway,\n\t\t\t\t\tdeadline.timeLeft());\n\n\t\t\tfinal File coordinateDirClosure = coordinateTempDir;\n\t\t\tfinal Throwable[] errorRef = new Throwable[1];\n\n\t\t\t// we trigger program execution in a separate thread\n\t\t\tThread programTrigger = new Thread(\"Program Trigger\") {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\ttestJobManagerFailure(ZooKeeper.getConnectString(), coordinateDirClosure);\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tt.printStackTrace();\n\t\t\t\t\t\terrorRef[0] = t;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\t//start the test program\n\t\t\tprogramTrigger.start();\n\n\t\t\t// wait until all marker files are in place, indicating that all tasks have started\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.waitForMarkerFiles(coordinateTempDir,\n\t\t\t\t\tREADY_MARKER_FILE_PREFIX, PARALLELISM, deadline.timeLeft().toMillis());\n\n\t\t\t// Kill one of the job managers and trigger recovery\n\t\t\tjmProcess[0].destroy();\n\n\t\t\tjmProcess[1] = new JobManagerProcess(1, config);\n\t\t\tjmProcess[1].startProcess();\n\n\t\t\tjmProcess[1].getActorRef(testActorSystem, deadline.timeLeft());\n\n\t\t\t// we create the marker file which signals the program functions tasks that they can complete\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.touchFile(new File(coordinateTempDir, PROCEED_MARKER_FILE));\n\n\t\t\tprogramTrigger.join(deadline.timeLeft().toMillis());\n\n\t\t\t// We wait for the finish marker file. We don't wait for the program trigger, because\n\t\t\t// we submit in detached mode.\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.waitForMarkerFiles(coordinateTempDir,\n\t\t\t\t\tFINISH_MARKER_FILE_PREFIX, 1, deadline.timeLeft().toMillis());\n\n\t\t\t// check that the program really finished\n\t\t\tassertFalse(\"The program did not finish in time\", programTrigger.isAlive());\n\n\t\t\t// check whether the program encountered an error\n\t\t\tif (errorRef[0] != null) {\n\t\t\t\tThrowable error = errorRef[0];\n\t\t\t\terror.printStackTrace();\n\t\t\t\tfail(\"The program encountered a \" + error.getClass().getSimpleName() + \" : \" + error.getMessage());\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// Print early (in some situations the process logs get too big\n\t\t\t// for Travis and the root problem is not shown)\n\t\t\tt.printStackTrace();\n\n\t\t\tfor (JobManagerProcess p : jmProcess) {\n\t\t\t\tif (p != null) {\n\t\t\t\t\tp.printProcessLog();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tthrow t;\n\t\t}\n\t\tfinally {\n\t\t\tfor (int i = 0; i < numberOfTaskManagers; i++) {\n\t\t\t\tif (tmActorSystem[i] != null) {\n\t\t\t\t\ttmActorSystem[i].shutdown();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (leaderRetrievalService != null) {\n\t\t\t\tleaderRetrievalService.stop();\n\t\t\t}\n\n\t\t\tfor (JobManagerProcess jmProces : jmProcess) {\n\t\t\t\tif (jmProces != null) {\n\t\t\t\t\tjmProces.destroy();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Delete coordination directory\n\t\t\tif (coordinateTempDir != null) {\n\t\t\t\ttry {\n\t\t\t\t\tFileUtils.deleteDirectory(coordinateTempDir);\n\t\t\t\t}",
            " 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 +\n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  ",
            "\t@Test\n\tpublic void testJobManagerProcessFailure() throws Exception {\n\t\t// Config\n\t\tfinal int numberOfJobManagers = 2;\n\t\tfinal int numberOfTaskManagers = 2;\n\t\tfinal int numberOfSlotsPerTaskManager = 2;\n\n\t\tassertEquals(PARALLELISM, numberOfTaskManagers * numberOfSlotsPerTaskManager);\n\n\t\t// Setup\n\t\t// Test actor system\n\t\tActorSystem testActorSystem;\n\n\t\t// Job managers\n\t\tfinal JobManagerProcess[] jmProcess = new JobManagerProcess[numberOfJobManagers];\n\n\t\t// Task managers\n\t\tfinal ActorSystem[] tmActorSystem = new ActorSystem[numberOfTaskManagers];\n\n\t\t// Leader election service\n\t\tLeaderRetrievalService leaderRetrievalService = null;\n\n\t\t// Coordination between the processes goes through a directory\n\t\tFile coordinateTempDir = null;\n\n\t\ttry {\n\t\t\tfinal Deadline deadline = TestTimeOut.fromNow();\n\n\t\t\t// Coordination directory\n\t\t\tcoordinateTempDir = createTempDirectory();\n\n\t\t\t// Job Managers\n\t\t\tConfiguration config = ZooKeeperTestUtils.createZooKeeperHAConfig(\n\t\t\t\t\tZooKeeper.getConnectString(), FileStateBackendBasePath.getPath());\n\n\t\t\t// Start first process\n\t\t\tjmProcess[0] = new JobManagerProcess(0, config);\n\t\t\tjmProcess[0].startProcess();\n\n\t\t\t// Task manager configuration\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 4L);\n\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 100);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);\n\n\t\t\t// Start the task manager process\n\t\t\tfor (int i = 0; i < numberOfTaskManagers; i++) {\n\t\t\t\ttmActorSystem[i] = AkkaUtils.createActorSystem(AkkaUtils.getDefaultAkkaConfig());\n\t\t\t\tTaskManager.startTaskManagerComponentsAndActor(\n\t\t\t\t\t\tconfig, ResourceID.generate(), tmActorSystem[i], \"localhost\",\n\t\t\t\t\t\tOption.<String>empty(), Option.<LeaderRetrievalService>empty(),\n\t\t\t\t\t\tfalse, TaskManager.class);\n\t\t\t}\n\n\t\t\t// Test actor system\n\t\t\ttestActorSystem = AkkaUtils.createActorSystem(AkkaUtils.getDefaultAkkaConfig());\n\n\t\t\tjmProcess[0].getActorRef(testActorSystem, deadline.timeLeft());\n\n\t\t\t// Leader listener\n\t\t\tTestingListener leaderListener = new TestingListener();\n\t\t\tleaderRetrievalService = ZooKeeperUtils.createLeaderRetrievalService(config);\n\t\t\tleaderRetrievalService.start(leaderListener);\n\n\t\t\t// Initial submission\n\t\t\tleaderListener.waitForNewLeader(deadline.timeLeft().toMillis());\n\n\t\t\tString leaderAddress = leaderListener.getAddress();\n\t\t\tUUID leaderId = leaderListener.getLeaderSessionID();\n\n\t\t\t// Get the leader ref\n\t\t\tActorRef leaderRef = AkkaUtils.getActorRef(leaderAddress, testActorSystem, deadline.timeLeft());\n\t\t\tActorGateway leaderGateway = new AkkaActorGateway(leaderRef, leaderId);\n\n\t\t\t// Wait for all task managers to connect to the leading job manager\n\t\t\tJobManagerActorTestUtils.waitForTaskManagers(numberOfTaskManagers, leaderGateway,\n\t\t\t\t\tdeadline.timeLeft());\n\n\t\t\tfinal File coordinateDirClosure = coordinateTempDir;\n\t\t\tfinal Throwable[] errorRef = new Throwable[1];\n\n\t\t\t// we trigger program execution in a separate thread\n\t\t\tThread programTrigger = new Thread(\"Program Trigger\") {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\ttestJobManagerFailure(ZooKeeper.getConnectString(), coordinateDirClosure);\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tt.printStackTrace();\n\t\t\t\t\t\terrorRef[0] = t;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\t//start the test program\n\t\t\tprogramTrigger.start();\n\n\t\t\t// wait until all marker files are in place, indicating that all tasks have started\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.waitForMarkerFiles(coordinateTempDir,\n\t\t\t\t\tREADY_MARKER_FILE_PREFIX, PARALLELISM, deadline.timeLeft().toMillis());\n\n\t\t\t// Kill one of the job managers and trigger recovery\n\t\t\tjmProcess[0].destroy();\n\n\t\t\tjmProcess[1] = new JobManagerProcess(1, config);\n\t\t\tjmProcess[1].startProcess();\n\n\t\t\tjmProcess[1].getActorRef(testActorSystem, deadline.timeLeft());\n\n\t\t\t// we create the marker file which signals the program functions tasks that they can complete\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.touchFile(new File(coordinateTempDir, PROCEED_MARKER_FILE));\n\n\t\t\tprogramTrigger.join(deadline.timeLeft().toMillis());\n\n\t\t\t// We wait for the finish marker file. We don't wait for the program trigger, because\n\t\t\t// we submit in detached mode.\n\t\t\tAbstractTaskManagerProcessFailureRecoveryTest.waitForMarkerFiles(coordinateTempDir,\n\t\t\t\t\tFINISH_MARKER_FILE_PREFIX, 1, deadline.timeLeft().toMillis());\n\n\t\t\t// check that the program really finished\n\t\t\tassertFalse(\"The program did not finish in time\", programTrigger.isAlive());\n\n\t\t\t// check whether the program encountered an error\n\t\t\tif (errorRef[0] != null) {\n\t\t\t\tThrowable error = errorRef[0];\n\t\t\t\terror.printStackTrace();\n\t\t\t\tfail(\"The program encountered a \" + error.getClass().getSimpleName() + \" : \" + error.getMessage());\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// Print early (in some situations the process logs get too big\n\t\t\t// for Travis and the root problem is not shown)\n\t\t\tt.printStackTrace();\n\n\t\t\tfor (JobManagerProcess p : jmProcess) {\n\t\t\t\tif (p != null) {\n\t\t\t\t\tp.printProcessLog();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tthrow t;\n\t\t}\n\t\tfinally {\n\t\t\tfor (int i = 0; i < numberOfTaskManagers; i++) {\n\t\t\t\tif (tmActorSystem[i] != null) {\n\t\t\t\t\ttmActorSystem[i].shutdown();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (leaderRetrievalService != null) {\n\t\t\t\tleaderRetrievalService.stop();\n\t\t\t}\n\n\t\t\tfor (JobManagerProcess jmProces : jmProcess) {\n\t\t\t\tif (jmProces != null) {\n\t\t\t\t\tjmProces.destroy();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Delete coordination directory\n\t\t\tif (coordinateTempDir != null) {\n\t\t\t\ttry {\n\t\t\t\t\tFileUtils.deleteDirectory(coordinateTempDir);\n\t\t\t\t}"
        ],
        [
            "WindowCheckpointingITCase::startTestCluster()",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 48);\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 48L);\n\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}"
        ],
        [
            "NetworkBufferPool::createBufferPool(int,int)",
            " 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t@Override\n\tpublic BufferPool createBufferPool(int numRequiredBuffers, int maxUsedBuffers) throws IOException {\n\t\t// It is necessary to use a separate lock from the one used for buffer\n\t\t// requests to ensure deadlock freedom for failure cases.\n\t\tsynchronized (factoryLock) {\n\t\t\tif (isDestroyed) {\n\t\t\t\tthrow new IllegalStateException(\"Network buffer pool has already been destroyed.\");\n\t\t\t}\n\n\t\t\t// Ensure that the number of required buffers can be satisfied.\n\t\t\t// With dynamic memory management this should become obsolete.\n\t\t\tif (numTotalRequiredBuffers + numRequiredBuffers > totalNumberOfMemorySegments) {\n\t\t\t\tthrow new IOException(String.format(\"Insufficient number of network buffers: \" +\n\t\t\t\t\t\t\t\t\"required %d, but only %d available. The total number of network \" +\n\t\t\t\t\t\t\t\t\"buffers is currently set to %d. You can increase this \" +\n\t\t\t\t\t\t\t\t\"number by setting the configuration key '%s'.\",\n\t\t\t\t\t\tnumRequiredBuffers,\n\t\t\t\t\t\ttotalNumberOfMemorySegments - numTotalRequiredBuffers,\n\t\t\t\t\t\ttotalNumberOfMemorySegments,\n\t\t\t\t\t\tConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY));\n\t\t\t}\n\n\t\t\tthis.numTotalRequiredBuffers += numRequiredBuffers;\n\n\t\t\t// We are good to go, create a new buffer pool and redistribute\n\t\t\t// non-fixed size buffers.\n\t\t\tLocalBufferPool localBufferPool =\n\t\t\t\tnew LocalBufferPool(this, numRequiredBuffers, maxUsedBuffers);\n\n\t\t\tallBufferPools.add(localBufferPool);\n\n\t\t\tredistributeBuffers();\n\n\t\t\treturn localBufferPool;\n\t\t}\n\t}",
            " 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t@Override\n\tpublic BufferPool createBufferPool(int numRequiredBuffers, int maxUsedBuffers) throws IOException {\n\t\t// It is necessary to use a separate lock from the one used for buffer\n\t\t// requests to ensure deadlock freedom for failure cases.\n\t\tsynchronized (factoryLock) {\n\t\t\tif (isDestroyed) {\n\t\t\t\tthrow new IllegalStateException(\"Network buffer pool has already been destroyed.\");\n\t\t\t}\n\n\t\t\t// Ensure that the number of required buffers can be satisfied.\n\t\t\t// With dynamic memory management this should become obsolete.\n\t\t\tif (numTotalRequiredBuffers + numRequiredBuffers > totalNumberOfMemorySegments) {\n\t\t\t\tthrow new IOException(String.format(\"Insufficient number of network buffers: \" +\n\t\t\t\t\t\t\t\t\"required %d, but only %d available. The total number of network \" +\n\t\t\t\t\t\t\t\t\"buffers is currently set to %d. You can increase this \" +\n\t\t\t\t\t\t\t\t\"number by setting the configuration key '%s'.\",\n\t\t\t\t\t\tnumRequiredBuffers,\n\t\t\t\t\t\ttotalNumberOfMemorySegments - numTotalRequiredBuffers,\n\t\t\t\t\t\ttotalNumberOfMemorySegments,\n\t\t\t\t\t\tTaskManagerOptions.NETWORK_NUM_BUFFERS.key()));\n\t\t\t}\n\n\t\t\tthis.numTotalRequiredBuffers += numRequiredBuffers;\n\n\t\t\t// We are good to go, create a new buffer pool and redistribute\n\t\t\t// non-fixed size buffers.\n\t\t\tLocalBufferPool localBufferPool =\n\t\t\t\tnew LocalBufferPool(this, numRequiredBuffers, maxUsedBuffers);\n\n\t\t\tallBufferPools.add(localBufferPool);\n\n\t\t\tredistributeBuffers();\n\n\t\t\treturn localBufferPool;\n\t\t}\n\t}"
        ],
        [
            "NotSoMiniClusterIterations::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53 -\n  54  \n  55 -\n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "\tpublic static void main(String[] args) {\n\t\tif ((Runtime.getRuntime().maxMemory() >>> 20) < 5000) {\n\t\t\tthrow new RuntimeException(\"This test program needs to run with at least 5GB of heap space.\");\n\t\t}\n\t\t\n\t\tLocalFlinkMiniCluster cluster = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, PARALLELISM);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 8);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 1000);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY, 8 * 1024);\n\t\t\t\n\t\t\tconfig.setInteger(\"taskmanager.net.server.numThreads\", 1);\n\t\t\tconfig.setInteger(\"taskmanager.net.client.numThreads\", 1);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\n\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54 +\n  55  \n  56 +\n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  ",
            "\tpublic static void main(String[] args) {\n\t\tif ((Runtime.getRuntime().maxMemory() >>> 20) < 5000) {\n\t\t\tthrow new RuntimeException(\"This test program needs to run with at least 5GB of heap space.\");\n\t\t}\n\t\t\n\t\tLocalFlinkMiniCluster cluster = null;\n\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, PARALLELISM);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 8L);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\t\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 1000);\n\t\t\tconfig.setInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE, 8 * 1024);\n\t\t\t\n\t\t\tconfig.setInteger(\"taskmanager.net.server.numThreads\", 1);\n\t\t\tconfig.setInteger(\"taskmanager.net.client.numThreads\", 1);\n\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\n\t\t\trunConnectedComponents(cluster.getLeaderRPCPort());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\tif (cluster != null) {\n\t\t\t\tcluster.shutdown();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "KafkaTestBase::getFlinkConfiguration()",
            " 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\tprotected static Configuration getFlinkConfiguration() {\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\t\tflinkConfig.setString(ConfigConstants.METRICS_REPORTERS_LIST, \"my_reporter\");\n\t\tflinkConfig.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"my_reporter.\" + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, JMXReporter.class.getName());\n\t\treturn flinkConfig;\n\t}",
            " 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  ",
            "\tprotected static Configuration getFlinkConfiguration() {\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\t\tflinkConfig.setString(ConfigConstants.METRICS_REPORTERS_LIST, \"my_reporter\");\n\t\tflinkConfig.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"my_reporter.\" + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, JMXReporter.class.getName());\n\t\treturn flinkConfig;\n\t}"
        ],
        [
            "EventTimeAllWindowCheckpointingITCase::startTestCluster()",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 48);\n\t\tconfig.setString(ConfigConstants.DEFAULT_AKKA_LOOKUP_TIMEOUT, \"60 s\");\n\t\tconfig.setString(ConfigConstants.DEFAULT_AKKA_ASK_TIMEOUT, \"60 s\");\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@BeforeClass\n\tpublic static void startTestCluster() {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM / 2);\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 48L);\n\t\tconfig.setString(ConfigConstants.DEFAULT_AKKA_LOOKUP_TIMEOUT, \"60 s\");\n\t\tconfig.setString(ConfigConstants.DEFAULT_AKKA_ASK_TIMEOUT, \"60 s\");\n\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\tcluster.start();\n\t}"
        ],
        [
            "CancelingTestBase::startCluster()",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92 -\n  93  \n  94  \n  95  \n  96  ",
            "\t@Before\n\tpublic void startCluster() throws Exception {\n\t\tverifyJvmOptions();\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(ConfigConstants.FILESYSTEM_DEFAULT_OVERWRITE_KEY, true);\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 4);\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT());\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY, 4096);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, 2048);\n\n\t\tthis.executor = new LocalFlinkMiniCluster(config, false);\n\t\tthis.executor.start();\n\t}",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93 +\n  94  \n  95  \n  96  \n  97  ",
            "\t@Before\n\tpublic void startCluster() throws Exception {\n\t\tverifyJvmOptions();\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(ConfigConstants.FILESYSTEM_DEFAULT_OVERWRITE_KEY, true);\n\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 4);\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT());\n\t\tconfig.setInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE, 4096);\n\t\tconfig.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS, 2048);\n\n\t\tthis.executor = new LocalFlinkMiniCluster(config, false);\n\t\tthis.executor.start();\n\t}"
        ],
        [
            "SavepointITCase::testSavepointForJobWithIteration()",
            " 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786 -\n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  ",
            "\t@Test\n\tpublic void testSavepointForJobWithIteration() throws Exception {\n\n\t\tfor (int i = 0; i < ITER_TEST_PARALLELISM; ++i) {\n\t\t\tITER_TEST_SNAPSHOT_WAIT[i] = new OneShotLatch();\n\t\t\tITER_TEST_RESTORE_WAIT[i] = new OneShotLatch();\n\t\t\tITER_TEST_CHECKPOINT_VERIFY[i] = 0;\n\t\t}\n\n\t\tTemporaryFolder folder = new TemporaryFolder();\n\t\tfolder.create();\n\t\t// Temporary directory for file state backend\n\t\tfinal File tmpDir = folder.newFolder();\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tfinal IntegerStreamSource source = new IntegerStreamSource();\n\t\tIterativeStream<Integer> iteration = env.addSource(source)\n\t\t\t\t.flatMap(new RichFlatMapFunction<Integer, Integer>() {\n\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(Integer in, Collector<Integer> clctr) throws Exception {\n\t\t\t\t\t\tclctr.collect(in);\n\t\t\t\t\t}\n\t\t\t\t}).setParallelism(ITER_TEST_PARALLELISM)\n\t\t\t\t.keyBy(new KeySelector<Integer, Object>() {\n\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Object getKey(Integer value) throws Exception {\n\t\t\t\t\t\treturn value;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.flatMap(new DuplicateFilter())\n\t\t\t\t.setParallelism(ITER_TEST_PARALLELISM)\n\t\t\t\t.iterate();\n\n\t\tDataStream<Integer> iterationBody = iteration\n\t\t\t\t.map(new MapFunction<Integer, Integer>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Integer map(Integer value) throws Exception {\n\t\t\t\t\t\treturn value;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.setParallelism(ITER_TEST_PARALLELISM);\n\n\t\titeration.closeWith(iterationBody);\n\n\t\tStreamGraph streamGraph = env.getStreamGraph();\n\t\tstreamGraph.setJobName(\"Test\");\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.addAll(jobGraph.getJobConfiguration());\n\t\tconfig.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, -1L);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2 * jobGraph.getMaximumParallelism());\n\t\tfinal File checkpointDir = new File(tmpDir, \"checkpoints\");\n\t\tfinal File savepointDir = new File(tmpDir, \"savepoints\");\n\n\t\tif (!checkpointDir.mkdir() || !savepointDir.mkdirs()) {\n\t\t\tfail(\"Test setup failed: failed to create temporary directories.\");\n\t\t}\n\n\t\tconfig.setString(CoreOptions.STATE_BACKEND, \"filesystem\");\n\t\tconfig.setString(FsStateBackendFactory.CHECKPOINT_DIRECTORY_URI_CONF_KEY,\n\t\t\t\tcheckpointDir.toURI().toString());\n\t\tconfig.setString(FsStateBackendFactory.MEMORY_THRESHOLD_CONF_KEY, \"0\");\n\t\tconfig.setString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY,\n\t\t\t\tsavepointDir.toURI().toString());\n\n\t\tTestingCluster cluster = new TestingCluster(config, false);\n\t\tString savepointPath = null;\n\t\ttry {\n\t\t\tcluster.start();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\t\t\tfor (OneShotLatch latch : ITER_TEST_SNAPSHOT_WAIT) {\n\t\t\t\tlatch.await();\n\t\t\t}\n\t\t\tsavepointPath = cluster.triggerSavepoint(jobGraph.getJobID());\n\t\t\tsource.cancel();\n\n\t\t\tjobGraph = streamGraph.getJobGraph();\n\t\t\tjobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\t\t\tfor (OneShotLatch latch : ITER_TEST_RESTORE_WAIT) {\n\t\t\t\tlatch.await();\n\t\t\t}\n\t\t\tsource.cancel();\n\t\t} finally {\n\t\t\tif (null != savepointPath) {\n\t\t\t\tcluster.disposeSavepoint(savepointPath);\n\t\t\t}\n\t\t\tcluster.stop();\n\t\t\tcluster.awaitTermination();\n\t\t}\n\t}",
            " 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787 +\n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  ",
            "\t@Test\n\tpublic void testSavepointForJobWithIteration() throws Exception {\n\n\t\tfor (int i = 0; i < ITER_TEST_PARALLELISM; ++i) {\n\t\t\tITER_TEST_SNAPSHOT_WAIT[i] = new OneShotLatch();\n\t\t\tITER_TEST_RESTORE_WAIT[i] = new OneShotLatch();\n\t\t\tITER_TEST_CHECKPOINT_VERIFY[i] = 0;\n\t\t}\n\n\t\tTemporaryFolder folder = new TemporaryFolder();\n\t\tfolder.create();\n\t\t// Temporary directory for file state backend\n\t\tfinal File tmpDir = folder.newFolder();\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tfinal IntegerStreamSource source = new IntegerStreamSource();\n\t\tIterativeStream<Integer> iteration = env.addSource(source)\n\t\t\t\t.flatMap(new RichFlatMapFunction<Integer, Integer>() {\n\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(Integer in, Collector<Integer> clctr) throws Exception {\n\t\t\t\t\t\tclctr.collect(in);\n\t\t\t\t\t}\n\t\t\t\t}).setParallelism(ITER_TEST_PARALLELISM)\n\t\t\t\t.keyBy(new KeySelector<Integer, Object>() {\n\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Object getKey(Integer value) throws Exception {\n\t\t\t\t\t\treturn value;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.flatMap(new DuplicateFilter())\n\t\t\t\t.setParallelism(ITER_TEST_PARALLELISM)\n\t\t\t\t.iterate();\n\n\t\tDataStream<Integer> iterationBody = iteration\n\t\t\t\t.map(new MapFunction<Integer, Integer>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Integer map(Integer value) throws Exception {\n\t\t\t\t\t\treturn value;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.setParallelism(ITER_TEST_PARALLELISM);\n\n\t\titeration.closeWith(iterationBody);\n\n\t\tStreamGraph streamGraph = env.getStreamGraph();\n\t\tstreamGraph.setJobName(\"Test\");\n\n\t\tJobGraph jobGraph = streamGraph.getJobGraph();\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.addAll(jobGraph.getJobConfiguration());\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, -1L);\n\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2 * jobGraph.getMaximumParallelism());\n\t\tfinal File checkpointDir = new File(tmpDir, \"checkpoints\");\n\t\tfinal File savepointDir = new File(tmpDir, \"savepoints\");\n\n\t\tif (!checkpointDir.mkdir() || !savepointDir.mkdirs()) {\n\t\t\tfail(\"Test setup failed: failed to create temporary directories.\");\n\t\t}\n\n\t\tconfig.setString(CoreOptions.STATE_BACKEND, \"filesystem\");\n\t\tconfig.setString(FsStateBackendFactory.CHECKPOINT_DIRECTORY_URI_CONF_KEY,\n\t\t\t\tcheckpointDir.toURI().toString());\n\t\tconfig.setString(FsStateBackendFactory.MEMORY_THRESHOLD_CONF_KEY, \"0\");\n\t\tconfig.setString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY,\n\t\t\t\tsavepointDir.toURI().toString());\n\n\t\tTestingCluster cluster = new TestingCluster(config, false);\n\t\tString savepointPath = null;\n\t\ttry {\n\t\t\tcluster.start();\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\t\t\tfor (OneShotLatch latch : ITER_TEST_SNAPSHOT_WAIT) {\n\t\t\t\tlatch.await();\n\t\t\t}\n\t\t\tsavepointPath = cluster.triggerSavepoint(jobGraph.getJobID());\n\t\t\tsource.cancel();\n\n\t\t\tjobGraph = streamGraph.getJobGraph();\n\t\t\tjobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));\n\n\t\t\tcluster.submitJobDetached(jobGraph);\n\t\t\tfor (OneShotLatch latch : ITER_TEST_RESTORE_WAIT) {\n\t\t\t\tlatch.await();\n\t\t\t}\n\t\t\tsource.cancel();\n\t\t} finally {\n\t\t\tif (null != savepointPath) {\n\t\t\t\tcluster.disposeSavepoint(savepointPath);\n\t\t\t}\n\t\t\tcluster.stop();\n\t\t\tcluster.awaitTermination();\n\t\t}\n\t}"
        ],
        [
            "TaskManagerServicesConfiguration::parseNetworkEnvironmentConfiguration(Configuration,boolean,InetAddress,int)",
            " 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251 -\n 252 -\n 253  \n 254  \n 255 -\n 256  \n 257 -\n 258 -\n 259 -\n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268 -\n 269  \n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  ",
            "\t/**\n\t * Creates the {@link NetworkEnvironmentConfiguration} from the given {@link Configuration}.\n\t *\n\t * @param configuration to create the network environment configuration from\n\t * @param localTaskManagerCommunication true if task manager communication is local\n\t * @param taskManagerAddress address of the task manager\n\t * @param slots to start the task manager with\n\t * @return Network environment configuration\n\t */\n\tprivate static NetworkEnvironmentConfiguration parseNetworkEnvironmentConfiguration(\n\t\tConfiguration configuration,\n\t\tboolean localTaskManagerCommunication,\n\t\tInetAddress taskManagerAddress,\n\t\tint slots) throws Exception {\n\n\t\t// ----> hosts / ports for communication and data exchange\n\n\t\tint dataport = configuration.getInteger(ConfigConstants.TASK_MANAGER_DATA_PORT_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_DATA_PORT);\n\n\t\tcheckConfigParameter(dataport >= 0, dataport, ConfigConstants.TASK_MANAGER_DATA_PORT_KEY,\n\t\t\t\"Leave config parameter empty or use 0 to let the system choose a port automatically.\");\n\n\t\tcheckConfigParameter(slots >= 1, slots, ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS,\n\t\t\t\"Number of task slots must be at least one.\");\n\n\t\tfinal int numNetworkBuffers = configuration.getInteger(\n\t\t\tConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_NUM_BUFFERS);\n\n\t\tcheckConfigParameter(numNetworkBuffers > 0, numNetworkBuffers,\n\t\t\tConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY, \"\");\n\n\t\tfinal int pageSize = configuration.getInteger(\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_MEMORY_SEGMENT_SIZE);\n\n\t\t// check page size of for minimum size\n\t\tcheckConfigParameter(pageSize >= MemoryManager.MIN_PAGE_SIZE, pageSize,\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY,\n\t\t\t\"Minimum memory segment size is \" + MemoryManager.MIN_PAGE_SIZE);\n\n\t\t// check page size for power of two\n\t\tcheckConfigParameter(MathUtils.isPowerOf2(pageSize), pageSize,\n\t\t\tConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY,\n\t\t\t\"Memory segment size must be a power of 2.\");\n\n\t\t// check whether we use heap or off-heap memory\n\t\tfinal MemoryType memType;\n\t\tif (configuration.getBoolean(ConfigConstants.TASK_MANAGER_MEMORY_OFF_HEAP_KEY, false)) {\n\t\t\tmemType = MemoryType.OFF_HEAP;\n\t\t} else {\n\t\t\tmemType = MemoryType.HEAP;\n\t\t}\n\n\t\t// initialize the memory segment factory accordingly\n\t\t// TODO - this should be in the TaskManager, not the configuration\n\t\tif (memType == MemoryType.HEAP) {\n\t\t\tif (!MemorySegmentFactory.initializeIfNotInitialized(HeapMemorySegment.FACTORY)) {\n\t\t\t\tthrow new Exception(\"Memory type is set to heap memory, but memory segment \" +\n\t\t\t\t\t\"factory has been initialized for off-heap memory segments\");\n\t\t\t}\n\t\t} else {\n\t\t\tif (!MemorySegmentFactory.initializeIfNotInitialized(HybridMemorySegment.FACTORY)) {\n\t\t\t\tthrow new Exception(\"Memory type is set to off-heap memory, but memory segment \" +\n\t\t\t\t\t\"factory has been initialized for heap memory segments\");\n\t\t\t}\n\t\t}\n\n\t\tfinal NettyConfig nettyConfig;\n\t\tif (!localTaskManagerCommunication) {\n\t\t\tfinal InetSocketAddress taskManagerInetSocketAddress = new InetSocketAddress(taskManagerAddress, dataport);\n\n\t\t\tnettyConfig = new NettyConfig(taskManagerInetSocketAddress.getAddress(),\n\t\t\t\ttaskManagerInetSocketAddress.getPort(), pageSize, slots, configuration);\n\t\t} else {\n\t\t\tnettyConfig = null;\n\t\t}\n\n\t\t// Default spill I/O mode for intermediate results\n\t\tfinal String syncOrAsync = configuration.getString(\n\t\t\tConfigConstants.TASK_MANAGER_NETWORK_DEFAULT_IO_MODE,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_DEFAULT_IO_MODE);\n\n\t\tfinal IOManager.IOMode ioMode;\n\t\tif (syncOrAsync.equals(\"async\")) {\n\t\t\tioMode = IOManager.IOMode.ASYNC;\n\t\t} else {\n\t\t\tioMode = IOManager.IOMode.SYNC;\n\t\t}\n\n\t\tint initialRequestBackoff = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_REQUEST_BACKOFF_INITIAL);\n\t\tint maxRequestBackoff = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_REQUEST_BACKOFF_MAX);\n\n\t\tint buffersPerChannel = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_BUFFERS_PER_CHANNEL);\n\t\tint extraBuffersPerGate = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_EXTRA_BUFFERS_PER_GATE);\n\n\t\treturn new NetworkEnvironmentConfiguration(\n\t\t\tnumNetworkBuffers,\n\t\t\tpageSize,\n\t\t\tmemType,\n\t\t\tioMode,\n\t\t\tinitialRequestBackoff,\n\t\t\tmaxRequestBackoff,\n\t\t\tbuffersPerChannel,\n\t\t\textraBuffersPerGate,\n\t\t\tnettyConfig);\n\t}",
            " 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248 +\n 249  \n 250  \n 251 +\n 252  \n 253 +\n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262 +\n 263  \n 264  \n 265  \n 266  \n 267 +\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "\t/**\n\t * Creates the {@link NetworkEnvironmentConfiguration} from the given {@link Configuration}.\n\t *\n\t * @param configuration to create the network environment configuration from\n\t * @param localTaskManagerCommunication true if task manager communication is local\n\t * @param taskManagerAddress address of the task manager\n\t * @param slots to start the task manager with\n\t * @return Network environment configuration\n\t */\n\tprivate static NetworkEnvironmentConfiguration parseNetworkEnvironmentConfiguration(\n\t\tConfiguration configuration,\n\t\tboolean localTaskManagerCommunication,\n\t\tInetAddress taskManagerAddress,\n\t\tint slots) throws Exception {\n\n\t\t// ----> hosts / ports for communication and data exchange\n\n\t\tint dataport = configuration.getInteger(ConfigConstants.TASK_MANAGER_DATA_PORT_KEY,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_DATA_PORT);\n\n\t\tcheckConfigParameter(dataport >= 0, dataport, ConfigConstants.TASK_MANAGER_DATA_PORT_KEY,\n\t\t\t\"Leave config parameter empty or use 0 to let the system choose a port automatically.\");\n\n\t\tcheckConfigParameter(slots >= 1, slots, ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS,\n\t\t\t\"Number of task slots must be at least one.\");\n\n\t\tfinal int numNetworkBuffers = configuration.getInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS);\n\n\t\tcheckConfigParameter(numNetworkBuffers > 0, numNetworkBuffers,\n\t\t\tTaskManagerOptions.NETWORK_NUM_BUFFERS.key(), \"\");\n\n\t\tfinal int pageSize = configuration.getInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE);\n\n\t\t// check page size of for minimum size\n\t\tcheckConfigParameter(pageSize >= MemoryManager.MIN_PAGE_SIZE, pageSize,\n\t\t\tTaskManagerOptions.MEMORY_SEGMENT_SIZE.key(),\n\t\t\t\"Minimum memory segment size is \" + MemoryManager.MIN_PAGE_SIZE);\n\n\t\t// check page size for power of two\n\t\tcheckConfigParameter(MathUtils.isPowerOf2(pageSize), pageSize,\n\t\t\tTaskManagerOptions.MEMORY_SEGMENT_SIZE.key(),\n\t\t\t\"Memory segment size must be a power of 2.\");\n\n\t\t// check whether we use heap or off-heap memory\n\t\tfinal MemoryType memType;\n\t\tif (configuration.getBoolean(TaskManagerOptions.MEMORY_OFF_HEAP)) {\n\t\t\tmemType = MemoryType.OFF_HEAP;\n\t\t} else {\n\t\t\tmemType = MemoryType.HEAP;\n\t\t}\n\n\t\t// initialize the memory segment factory accordingly\n\t\t// TODO - this should be in the TaskManager, not the configuration\n\t\tif (memType == MemoryType.HEAP) {\n\t\t\tif (!MemorySegmentFactory.initializeIfNotInitialized(HeapMemorySegment.FACTORY)) {\n\t\t\t\tthrow new Exception(\"Memory type is set to heap memory, but memory segment \" +\n\t\t\t\t\t\"factory has been initialized for off-heap memory segments\");\n\t\t\t}\n\t\t} else {\n\t\t\tif (!MemorySegmentFactory.initializeIfNotInitialized(HybridMemorySegment.FACTORY)) {\n\t\t\t\tthrow new Exception(\"Memory type is set to off-heap memory, but memory segment \" +\n\t\t\t\t\t\"factory has been initialized for heap memory segments\");\n\t\t\t}\n\t\t}\n\n\t\tfinal NettyConfig nettyConfig;\n\t\tif (!localTaskManagerCommunication) {\n\t\t\tfinal InetSocketAddress taskManagerInetSocketAddress = new InetSocketAddress(taskManagerAddress, dataport);\n\n\t\t\tnettyConfig = new NettyConfig(taskManagerInetSocketAddress.getAddress(),\n\t\t\t\ttaskManagerInetSocketAddress.getPort(), pageSize, slots, configuration);\n\t\t} else {\n\t\t\tnettyConfig = null;\n\t\t}\n\n\t\t// Default spill I/O mode for intermediate results\n\t\tfinal String syncOrAsync = configuration.getString(\n\t\t\tConfigConstants.TASK_MANAGER_NETWORK_DEFAULT_IO_MODE,\n\t\t\tConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_DEFAULT_IO_MODE);\n\n\t\tfinal IOManager.IOMode ioMode;\n\t\tif (syncOrAsync.equals(\"async\")) {\n\t\t\tioMode = IOManager.IOMode.ASYNC;\n\t\t} else {\n\t\t\tioMode = IOManager.IOMode.SYNC;\n\t\t}\n\n\t\tint initialRequestBackoff = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_REQUEST_BACKOFF_INITIAL);\n\t\tint maxRequestBackoff = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_REQUEST_BACKOFF_MAX);\n\n\t\tint buffersPerChannel = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_BUFFERS_PER_CHANNEL);\n\t\tint extraBuffersPerGate = configuration.getInteger(\n\t\t\tTaskManagerOptions.NETWORK_EXTRA_BUFFERS_PER_GATE);\n\n\t\treturn new NetworkEnvironmentConfiguration(\n\t\t\tnumNetworkBuffers,\n\t\t\tpageSize,\n\t\t\tmemType,\n\t\t\tioMode,\n\t\t\tinitialRequestBackoff,\n\t\t\tmaxRequestBackoff,\n\t\t\tbuffersPerChannel,\n\t\t\textraBuffersPerGate,\n\t\t\tnettyConfig);\n\t}"
        ],
        [
            "MiniClusterConfiguration::getOrCalculateManagedMemoryPerTaskManager()",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 -\n 200 -\n 201 -\n 202  \n 203 -\n 204  \n 205 -\n 206  \n 207  \n 208 -\n 209 -\n 210 -\n 211  \n 212 -\n 213 -\n 214 -\n 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "\t/**\n\t * Get or calculate the managed memory per task manager. The memory is calculated in the\n\t * following order:\n\t *\n\t * 1. Return {@link #managedMemoryPerTaskManager} if set\n\t * 2. Return config.getInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY) if set\n\t * 3. Distribute the available free memory equally among all components (JMs, RMs and TMs) and\n\t * calculate the managed memory from the share of memory for a single task manager.\n\t *\n\t * @return\n\t */\n\tprivate long getOrCalculateManagedMemoryPerTaskManager() {\n\t\tif (managedMemoryPerTaskManager == -1) {\n\t\t\t// no memory set in the mini cluster configuration\n\t\t\tfinal ConfigOption<Integer> memorySizeOption = ConfigOptions\n\t\t\t\t.key(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY)\n\t\t\t\t.defaultValue(-1);\n\n\t\t\tint memorySize = config.getInteger(memorySizeOption);\n\n\t\t\tif (memorySize == -1) {\n\t\t\t\t// no memory set in the flink configuration\n\t\t\t\t// share the available memory among all running components\n\t\t\t\tfinal ConfigOption<Integer> bufferSizeOption = ConfigOptions\n\t\t\t\t\t.key(ConfigConstants.TASK_MANAGER_MEMORY_SEGMENT_SIZE_KEY)\n\t\t\t\t\t.defaultValue(ConfigConstants.DEFAULT_TASK_MANAGER_MEMORY_SEGMENT_SIZE);\n\n\t\t\t\tfinal ConfigOption<Long> bufferMemoryOption = ConfigOptions\n\t\t\t\t\t.key(ConfigConstants.TASK_MANAGER_NETWORK_NUM_BUFFERS_KEY)\n\t\t\t\t\t.defaultValue((long) ConfigConstants.DEFAULT_TASK_MANAGER_NETWORK_NUM_BUFFERS);\n\n\t\t\t\tfinal ConfigOption<Float> memoryFractionOption = ConfigOptions\n\t\t\t\t\t.key(ConfigConstants.TASK_MANAGER_MEMORY_FRACTION_KEY)\n\t\t\t\t\t.defaultValue(ConfigConstants.DEFAULT_MEMORY_MANAGER_MEMORY_FRACTION);\n\n\t\t\t\tfloat memoryFraction = config.getFloat(memoryFractionOption);\n\t\t\t\tlong networkBuffersMemory = config.getLong(bufferMemoryOption) * config.getInteger(bufferSizeOption);\n\n\t\t\t\tlong freeMemory = EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag();\n\n\t\t\t\t// we assign each component the same amount of free memory\n\t\t\t\t// (might be a bit of overkill for the JMs and RMs)\n\t\t\t\tlong memoryPerComponent = freeMemory / (numTaskManagers + numResourceManagers + numJobManagers);\n\n\t\t\t\t// subtract the network buffer memory\n\t\t\t\tlong memoryMinusNetworkBuffers = memoryPerComponent - networkBuffersMemory;\n\n\t\t\t\t// calculate the managed memory size\n\t\t\t\tlong managedMemoryBytes = (long) (memoryMinusNetworkBuffers * memoryFraction);\n\n\t\t\t\treturn managedMemoryBytes >>> 20;\n\t\t\t} else {\n\t\t\t\treturn memorySize;\n\t\t\t}\n\t\t} else {\n\t\t\treturn managedMemoryPerTaskManager;\n\t\t}\n\t}",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 +\n 200  \n 201 +\n 202 +\n 203 +\n 204 +\n 205  \n 206  \n 207  \n 208 +\n 209 +\n 210 +\n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "\t/**\n\t * Get or calculate the managed memory per task manager. The memory is calculated in the\n\t * following order:\n\t *\n\t * 1. Return {@link #managedMemoryPerTaskManager} if set\n\t * 2. Return config.getInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY) if set\n\t * 3. Distribute the available free memory equally among all components (JMs, RMs and TMs) and\n\t * calculate the managed memory from the share of memory for a single task manager.\n\t *\n\t * @return\n\t */\n\tprivate long getOrCalculateManagedMemoryPerTaskManager() {\n\t\tif (managedMemoryPerTaskManager == -1) {\n\t\t\t// no memory set in the mini cluster configuration\n\n\t\t\tlong memorySize = config.getLong(TaskManagerOptions.MANAGED_MEMORY_SIZE);\n\n\t\t\t// we could probably use config.contains() but the previous implementation compared to\n\t\t\t// the default (-1) thus allowing the user to explicitly specify this as well\n\t\t\t// -> don't change this behaviour now\n\t\t\tif (memorySize == TaskManagerOptions.MANAGED_MEMORY_SIZE.defaultValue()) {\n\t\t\t\t// no memory set in the flink configuration\n\t\t\t\t// share the available memory among all running components\n\n\t\t\t\tfloat memoryFraction = config.getFloat(TaskManagerOptions.MANAGED_MEMORY_FRACTION);\n\t\t\t\tlong networkBuffersMemory =\n\t\t\t\t\t(long) config.getInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS) *\n\t\t\t\t\t\t(long) config.getInteger(TaskManagerOptions.MEMORY_SEGMENT_SIZE);\n\n\t\t\t\tlong freeMemory = EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag();\n\n\t\t\t\t// we assign each component the same amount of free memory\n\t\t\t\t// (might be a bit of overkill for the JMs and RMs)\n\t\t\t\tlong memoryPerComponent = freeMemory / (numTaskManagers + numResourceManagers + numJobManagers);\n\n\t\t\t\t// subtract the network buffer memory\n\t\t\t\tlong memoryMinusNetworkBuffers = memoryPerComponent - networkBuffersMemory;\n\n\t\t\t\t// calculate the managed memory size\n\t\t\t\tlong managedMemoryBytes = (long) (memoryMinusNetworkBuffers * memoryFraction);\n\n\t\t\t\treturn managedMemoryBytes >>> 20;\n\t\t\t} else {\n\t\t\t\treturn memorySize;\n\t\t\t}\n\t\t} else {\n\t\t\treturn managedMemoryPerTaskManager;\n\t\t}\n\t}"
        ],
        [
            "MiscellaneousIssuesITCase::startCluster()",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 3);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 12);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 2);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 3);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 12L);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "TestBaseUtils::startCluster(Configuration,boolean)",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 -\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\tpublic static LocalFlinkMiniCluster startCluster(\n\t\tConfiguration config,\n\t\tboolean singleActorSystem) throws Exception {\n\n\t\tlogDir = File.createTempFile(\"TestBaseUtils-logdir\", null);\n\t\tAssert.assertTrue(\"Unable to delete temp file\", logDir.delete());\n\t\tAssert.assertTrue(\"Unable to create temp directory\", logDir.mkdir());\n\t\tPath logFile = Files.createFile(new File(logDir, \"jobmanager.log\").toPath());\n\t\tFiles.createFile(new File(logDir, \"jobmanager.out\").toPath());\n\n\t\tconfig.setLong(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, TASK_MANAGER_MEMORY_SIZE);\n\t\tconfig.setBoolean(ConfigConstants.FILESYSTEM_DEFAULT_OVERWRITE_KEY, true);\n\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, DEFAULT_AKKA_ASK_TIMEOUT + \"s\");\n\t\tconfig.setString(ConfigConstants.AKKA_STARTUP_TIMEOUT, DEFAULT_AKKA_STARTUP_TIMEOUT);\n\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 8081);\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_WEB_LOG_PATH_KEY, logFile.toString());\n\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_LOG_PATH_KEY, logFile.toString());\n\n\t\tLocalFlinkMiniCluster cluster =  new LocalFlinkMiniCluster(config, singleActorSystem);\n\n\t\tcluster.start();\n\n\t\treturn cluster;\n\t}",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "\tpublic static LocalFlinkMiniCluster startCluster(\n\t\tConfiguration config,\n\t\tboolean singleActorSystem) throws Exception {\n\n\t\tlogDir = File.createTempFile(\"TestBaseUtils-logdir\", null);\n\t\tAssert.assertTrue(\"Unable to delete temp file\", logDir.delete());\n\t\tAssert.assertTrue(\"Unable to create temp directory\", logDir.mkdir());\n\t\tPath logFile = Files.createFile(new File(logDir, \"jobmanager.log\").toPath());\n\t\tFiles.createFile(new File(logDir, \"jobmanager.out\").toPath());\n\n\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, TASK_MANAGER_MEMORY_SIZE);\n\t\tconfig.setBoolean(ConfigConstants.FILESYSTEM_DEFAULT_OVERWRITE_KEY, true);\n\n\t\tconfig.setString(ConfigConstants.AKKA_ASK_TIMEOUT, DEFAULT_AKKA_ASK_TIMEOUT + \"s\");\n\t\tconfig.setString(ConfigConstants.AKKA_STARTUP_TIMEOUT, DEFAULT_AKKA_STARTUP_TIMEOUT);\n\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 8081);\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_WEB_LOG_PATH_KEY, logFile.toString());\n\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_LOG_PATH_KEY, logFile.toString());\n\n\t\tLocalFlinkMiniCluster cluster =  new LocalFlinkMiniCluster(config, singleActorSystem);\n\n\t\tcluster.start();\n\n\t\treturn cluster;\n\t}"
        ],
        [
            "CustomSerializationITCase::startCluster()",
            "  48  \n  49  \n  50  \n  51  \n  52  \n  53 -\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARLLELISM);\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 30);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54 +\n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\t@BeforeClass\n\tpublic static void startCluster() {\n\t\ttry {\n\t\t\tConfiguration config = new Configuration();\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARLLELISM);\n\t\t\tconfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 30L);\n\t\t\tcluster = new LocalFlinkMiniCluster(config, false);\n\t\t\tcluster.start();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Failed to start test cluster: \" + e.getMessage());\n\t\t}\n\t}"
        ]
    ],
    "b1f3408f4cc5cca4536fe85300efcd5267eba73a": [
        [
            "HadoopModule::install(SecurityUtils)",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "\t@Override\n\tpublic void install(SecurityUtils.SecurityConfiguration securityConfig) throws SecurityInstallException {\n\n\t\tUserGroupInformation.setConfiguration(securityConfig.getHadoopConfiguration());\n\n\t\ttry {\n\t\t\tif (UserGroupInformation.isSecurityEnabled() &&\n\t\t\t\t!StringUtils.isBlank(securityConfig.getKeytab()) && !StringUtils.isBlank(securityConfig.getPrincipal())) {\n\t\t\t\tString keytabPath = (new File(securityConfig.getKeytab())).getAbsolutePath();\n\n\t\t\t\tUserGroupInformation.loginUserFromKeytab(securityConfig.getPrincipal(), keytabPath);\n\n\t\t\t\tloginUser = UserGroupInformation.getLoginUser();\n\n\t\t\t\t// supplement with any available tokens\n\t\t\t\tString fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n\t\t\t\tif (fileLocation != null) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Use reflection API since the API semantics are not available in Hadoop1 profile. Below APIs are\n\t\t\t\t\t * used in the context of reading the stored tokens from UGI.\n\t\t\t\t\t * Credentials cred = Credentials.readTokenStorageFile(new File(fileLocation), config.hadoopConf);\n\t\t\t\t\t * loginUser.addCredentials(cred);\n\t\t\t\t\t*/\n\t\t\t\t\ttry {\n\t\t\t\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\"readTokenStorageFile\",\n\t\t\t\t\t\t\tFile.class, org.apache.hadoop.conf.Configuration.class);\n\t\t\t\t\t\tCredentials cred = (Credentials) readTokenStorageFileMethod.invoke(null, new File(fileLocation),\n\t\t\t\t\t\t\tsecurityConfig.getHadoopConfiguration());\n\t\t\t\t\t\tMethod addCredentialsMethod = UserGroupInformation.class.getMethod(\"addCredentials\",\n\t\t\t\t\t\t\tCredentials.class);\n\t\t\t\t\t\taddCredentialsMethod.invoke(loginUser, cred);\n\t\t\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t\t\tLOG.warn(\"Could not find method implementations in the shaded jar. Exception: {}\", e);\n\t\t\t\t\t} catch (InvocationTargetException e) {\n\t\t\t\t\t\tthrow e.getTargetException();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// login with current user credentials (e.g. ticket cache, OS login)\n\t\t\t\t// note that the stored tokens are read automatically\n\t\t\t\ttry {\n\t\t\t\t\t//Use reflection API to get the login user object\n\t\t\t\t\t//UserGroupInformation.loginUserFromSubject(null);\n\t\t\t\t\tMethod loginUserFromSubjectMethod = UserGroupInformation.class.getMethod(\"loginUserFromSubject\", Subject.class);\n\t\t\t\t\tloginUserFromSubjectMethod.invoke(null, (Subject) null);\n\t\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t\tLOG.warn(\"Could not find method implementations in the shaded jar. Exception: {}\", e);\n\t\t\t\t} catch (InvocationTargetException e) {\n\t\t\t\t\tthrow e.getTargetException();\n\t\t\t\t}\n\n\t\t\t\tloginUser = UserGroupInformation.getLoginUser();\n\t\t\t}\n\n\t\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t\t// so we check only in ticket cache scenario.\n\t\t\t\tif (securityConfig.useTicketCache() && !loginUser.hasKerberosCredentials()) {\n\t\t\t\t\t// a delegation token is an adequate substitute in most cases\n\t\t\t\t\tif (!HadoopUtils.hasHDFSDelegationToken()) {\n\t\t\t\t\t\tLOG.warn(\"Hadoop security is enabled but current login user does not have Kerberos credentials\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tLOG.info(\"Hadoop user set to {}\", loginUser);\n\n\t\t} catch (Throwable ex) {\n\t\t\tthrow new SecurityInstallException(\"Unable to set the Hadoop login user\", ex);\n\t\t}\n\t}",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85 +\n  86  \n  87  \n  88 +\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Override\n\tpublic void install(SecurityUtils.SecurityConfiguration securityConfig) throws SecurityInstallException {\n\n\t\tUserGroupInformation.setConfiguration(securityConfig.getHadoopConfiguration());\n\n\t\ttry {\n\t\t\tif (UserGroupInformation.isSecurityEnabled() &&\n\t\t\t\t!StringUtils.isBlank(securityConfig.getKeytab()) && !StringUtils.isBlank(securityConfig.getPrincipal())) {\n\t\t\t\tString keytabPath = (new File(securityConfig.getKeytab())).getAbsolutePath();\n\n\t\t\t\tUserGroupInformation.loginUserFromKeytab(securityConfig.getPrincipal(), keytabPath);\n\n\t\t\t\tloginUser = UserGroupInformation.getLoginUser();\n\n\t\t\t\t// supplement with any available tokens\n\t\t\t\tString fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n\t\t\t\tif (fileLocation != null) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Use reflection API since the API semantics are not available in Hadoop1 profile. Below APIs are\n\t\t\t\t\t * used in the context of reading the stored tokens from UGI.\n\t\t\t\t\t * Credentials cred = Credentials.readTokenStorageFile(new File(fileLocation), config.hadoopConf);\n\t\t\t\t\t * loginUser.addCredentials(cred);\n\t\t\t\t\t * Notify:If UGI use the keytab for login, do not load HDFS delegation token.\n\t\t\t\t\t*/\n\t\t\t\t\ttry {\n\t\t\t\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\"readTokenStorageFile\",\n\t\t\t\t\t\t\tFile.class, org.apache.hadoop.conf.Configuration.class);\n\t\t\t\t\t\tCredentials cred = (Credentials) readTokenStorageFileMethod.invoke(null, new File(fileLocation),\n\t\t\t\t\t\t\tsecurityConfig.getHadoopConfiguration());\n\t\t\t\t\t\tMethod getAllTokensMethod = Credentials.class.getMethod(\"getAllTokens\");\n\t\t\t\t\t\tCredentials credentials = new Credentials();\n\t\t\t\t\t\tfinal Text HDFS_DELEGATION_TOKEN_KIND = new Text(\"HDFS_DELEGATION_TOKEN\");\n\t\t\t\t\t\tCollection<Token<? extends TokenIdentifier>> usrTok = (Collection<Token<? extends TokenIdentifier>>) getAllTokensMethod.invoke(cred);\n\t\t\t\t\t\t//If UGI use keytab for login, do not load HDFS delegation token.\n\t\t\t\t\t\tfor (Token<? extends TokenIdentifier> token : usrTok) {\n\t\t\t\t\t\t\tif (!token.getKind().equals(HDFS_DELEGATION_TOKEN_KIND)) {\n\t\t\t\t\t\t\t\tfinal Text id = new Text(token.getIdentifier());\n\t\t\t\t\t\t\t\tcredentials.addToken(id, token);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tMethod addCredentialsMethod = UserGroupInformation.class.getMethod(\"addCredentials\",\n\t\t\t\t\t\t\tCredentials.class);\n\t\t\t\t\t\taddCredentialsMethod.invoke(loginUser, credentials);\n\t\t\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t\t\tLOG.warn(\"Could not find method implementations in the shaded jar. Exception: {}\", e);\n\t\t\t\t\t} catch (InvocationTargetException e) {\n\t\t\t\t\t\tthrow e.getTargetException();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// login with current user credentials (e.g. ticket cache, OS login)\n\t\t\t\t// note that the stored tokens are read automatically\n\t\t\t\ttry {\n\t\t\t\t\t//Use reflection API to get the login user object\n\t\t\t\t\t//UserGroupInformation.loginUserFromSubject(null);\n\t\t\t\t\tMethod loginUserFromSubjectMethod = UserGroupInformation.class.getMethod(\"loginUserFromSubject\", Subject.class);\n\t\t\t\t\tloginUserFromSubjectMethod.invoke(null, (Subject) null);\n\t\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t\tLOG.warn(\"Could not find method implementations in the shaded jar. Exception: {}\", e);\n\t\t\t\t} catch (InvocationTargetException e) {\n\t\t\t\t\tthrow e.getTargetException();\n\t\t\t\t}\n\n\t\t\t\tloginUser = UserGroupInformation.getLoginUser();\n\t\t\t}\n\n\t\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t\t// so we check only in ticket cache scenario.\n\t\t\t\tif (securityConfig.useTicketCache() && !loginUser.hasKerberosCredentials()) {\n\t\t\t\t\t// a delegation token is an adequate substitute in most cases\n\t\t\t\t\tif (!HadoopUtils.hasHDFSDelegationToken()) {\n\t\t\t\t\t\tLOG.warn(\"Hadoop security is enabled but current login user does not have Kerberos credentials\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tLOG.info(\"Hadoop user set to {}\", loginUser);\n\n\t\t} catch (Throwable ex) {\n\t\t\tthrow new SecurityInstallException(\"Unable to set the Hadoop login user\", ex);\n\t\t}\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::startAppMaster(JobGraph,YarnClient,YarnClientApplication)",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 -\n 797 -\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif (krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"), Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif (keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif (UserGroupInformation.isSecurityEnabled() && keytab == null) {\n\t\t\t//set tokens only when keytab is not provided\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString());\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString());\n\t\t}\n\n\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif (customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif (detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif (yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while (true) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif (System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 +\n 797 +\n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif (krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"), Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif (keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t//set tokens when security is enable\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString());\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString());\n\t\t}\n\n\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif (customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif (detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif (yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while (true) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif (System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}"
        ],
        [
            "Utils::createTaskExecutorContext(org,YarnConfiguration,Map,ContaineredTaskManagerParameters,org,String,Class,Logger)",
            " 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 -\n 459 -\n 460 -\n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "\t/**\n\t * Creates the launch context, which describes how to bring up a TaskExecutor / TaskManager process in\n\t * an allocated YARN container.\n\t *\n\t * <p>This code is extremely YARN specific and registers all the resources that the TaskExecutor\n\t * needs (such as JAR file, config file, ...) and all environment variables in a YARN\n\t * container launch context. The launch context then ensures that those resources will be\n\t * copied into the containers transient working directory.\n\t *\n\t * @param flinkConfig\n\t *\t\t The Flink configuration object.\n\t * @param yarnConfig\n\t *\t\t The YARN configuration object.\n\t * @param env\n\t *\t\t The environment variables.\n\t * @param tmParams\n\t *\t\t The TaskExecutor container memory parameters.\n\t * @param taskManagerConfig\n\t *\t\t The configuration for the TaskExecutors.\n\t * @param workingDirectory\n\t *\t\t The current application master container's working directory.\n\t * @param taskManagerMainClass\n\t *\t\t The class with the main method.\n\t * @param log\n\t *\t\t The logger.\n\t *\n\t * @return The launch context for the TaskManager processes.\n\t *\n\t * @throws Exception Thrown if teh launch context could not be created, for example if\n\t *\t\t\t\t   the resources could not be copied.\n\t */\n\tstatic ContainerLaunchContext createTaskExecutorContext(\n\t\torg.apache.flink.configuration.Configuration flinkConfig,\n\t\tYarnConfiguration yarnConfig,\n\t\tMap<String, String> env,\n\t\tContaineredTaskManagerParameters tmParams,\n\t\torg.apache.flink.configuration.Configuration taskManagerConfig,\n\t\tString workingDirectory,\n\t\tClass<?> taskManagerMainClass,\n\t\tLogger log) throws Exception {\n\n\t\t// get and validate all relevant variables\n\n\t\tString remoteFlinkJarPath = env.get(YarnConfigKeys.FLINK_JAR_PATH);\n\t\trequire(remoteFlinkJarPath != null, \"Environment variable %s not set\", YarnConfigKeys.FLINK_JAR_PATH);\n\n\t\tString appId = env.get(YarnConfigKeys.ENV_APP_ID);\n\t\trequire(appId != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_APP_ID);\n\n\t\tString clientHomeDir = env.get(YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\t\trequire(clientHomeDir != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\n\t\tString shipListString = env.get(YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\t\trequire(shipListString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\n\t\tString yarnClientUsername = env.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\trequire(yarnClientUsername != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\tfinal String remoteKeytabPath = env.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tlog.info(\"TM:remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = env.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tlog.info(\"TM:remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\tfinal String remoteYarnConfPath = env.get(YarnConfigKeys.ENV_YARN_SITE_XML_PATH);\n\t\tlog.info(\"TM:remote yarn conf path obtained {}\", remoteYarnConfPath);\n\n\t\tfinal String remoteKrb5Path = env.get(YarnConfigKeys.ENV_KRB5_PATH);\n\t\tlog.info(\"TM:remote krb5 path obtained {}\", remoteKrb5Path);\n\n\t\tString classPathString = env.get(ENV_FLINK_CLASSPATH);\n\t\trequire(classPathString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_FLINK_CLASSPATH);\n\n\t\t//register keytab\n\t\tLocalResource keytabResource = null;\n\t\tif (remoteKeytabPath != null) {\n\t\t\tlog.info(\"Adding keytab {} to the AM container local resource bucket\", remoteKeytabPath);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(remoteKeytabPath);\n\t\t\tFileSystem fs = keytabPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, keytabPath, keytabResource);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tLocalResource yarnConfResource = null;\n\t\tLocalResource krb5ConfResource = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (remoteYarnConfPath != null && remoteKrb5Path != null) {\n\t\t\tlog.info(\"TM:Adding remoteYarnConfPath {} to the container local resource bucket\", remoteYarnConfPath);\n\t\t\tyarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath yarnConfPath = new Path(remoteYarnConfPath);\n\t\t\tFileSystem fs = yarnConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, yarnConfPath, yarnConfResource);\n\n\t\t\tlog.info(\"TM:Adding remoteKrb5Path {} to the container local resource bucket\", remoteKrb5Path);\n\t\t\tkrb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath krb5ConfPath = new Path(remoteKrb5Path);\n\t\t\tfs = krb5ConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, krb5ConfPath, krb5ConfResource);\n\n\t\t\thasKrb5 = true;\n\t\t}\n\n\t\t// register Flink Jar with remote HDFS\n\t\tLocalResource flinkJar = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\tPath remoteJarPath = new Path(remoteFlinkJarPath);\n\t\t\tFileSystem fs = remoteJarPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, remoteJarPath, flinkJar);\n\t\t}\n\n\t\t// register conf with local fs\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\t// write the TaskManager configuration to a local file\n\t\t\tfinal File taskManagerConfigFile =\n\t\t\t\t\tnew File(workingDirectory, UUID.randomUUID() + \"-taskmanager-conf.yaml\");\n\t\t\tlog.debug(\"Writing TaskManager configuration to {}\", taskManagerConfigFile.getAbsolutePath());\n\t\t\tBootstrapTools.writeConfiguration(taskManagerConfig, taskManagerConfigFile);\n\n\t\t\tPath homeDirPath = new Path(clientHomeDir);\n\t\t\tFileSystem fs = homeDirPath.getFileSystem(yarnConfig);\n\t\t\tsetupLocalResource(fs, appId,\n\t\t\t\t\tnew Path(taskManagerConfigFile.toURI()), flinkConf, new Path(clientHomeDir));\n\n\t\t\tlog.info(\"Prepared local resource for modified yaml: {}\", flinkConf);\n\t\t}\n\n\t\tMap<String, LocalResource> taskManagerLocalResources = new HashMap<>();\n\t\ttaskManagerLocalResources.put(\"flink.jar\", flinkJar);\n\t\ttaskManagerLocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (yarnConfResource != null && krb5ConfResource != null) {\n\t\t\ttaskManagerLocalResources.put(YARN_SITE_FILE_NAME, yarnConfResource);\n\t\t\ttaskManagerLocalResources.put(KRB5_FILE_NAME, krb5ConfResource);\n\t\t}\n\n\t\tif (keytabResource != null) {\n\t\t\ttaskManagerLocalResources.put(KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\t// prepare additional files to be shipped\n\t\tfor (String pathStr : shipListString.split(\",\")) {\n\t\t\tif (!pathStr.isEmpty()) {\n\t\t\t\tLocalResource resource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath path = new Path(pathStr);\n\t\t\t\tregisterLocalResource(path.getFileSystem(yarnConfig), path, resource);\n\t\t\t\ttaskManagerLocalResources.put(path.getName(), resource);\n\t\t\t}\n\t\t}\n\n\t\t// now that all resources are prepared, we can create the launch context\n\n\t\tlog.info(\"Creating container launch context for TaskManagers\");\n\n\t\tboolean hasLogback = new File(workingDirectory, \"logback.xml\").exists();\n\t\tboolean hasLog4j = new File(workingDirectory, \"log4j.properties\").exists();\n\n\t\tString launchCommand = BootstrapTools.getTaskManagerShellCommand(\n\t\t\t\tflinkConfig, tmParams, \".\", ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n\t\t\t\thasLogback, hasLog4j, hasKrb5, taskManagerMainClass);\n\n\t\tlog.info(\"Starting TaskManagers with command: \" + launchCommand);\n\n\t\tContainerLaunchContext ctx = Records.newRecord(ContainerLaunchContext.class);\n\t\tctx.setCommands(Collections.singletonList(launchCommand));\n\t\tctx.setLocalResources(taskManagerLocalResources);\n\n\t\tMap<String, String> containerEnv = new HashMap<>();\n\t\tcontainerEnv.putAll(tmParams.taskManagerEnv());\n\n\t\t// add YARN classpath, etc to the container environment\n\t\tcontainerEnv.put(ENV_FLINK_CLASSPATH, classPathString);\n\t\tsetupYarnClassPath(yarnConfig, containerEnv);\n\n\t\tcontainerEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (remoteKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PATH, remoteKeytabPath);\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, remoteKeytabPrincipal);\n\t\t}\n\n\t\tctx.setEnvironment(containerEnv);\n\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tlog.debug(\"Adding security tokens to Task Executor Container launch Context....\");\n\t\t\tUserGroupInformation user = UserGroupInformation.getCurrentUser();\n\t\t\tCredentials credentials = user.getCredentials();\n\t\t\tcredentials.writeTokenStorageToStream(dob);\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tctx.setTokens(securityTokens);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);\n\t\t}\n\n\t\treturn ctx;\n\t}",
            " 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 +\n 461 +\n 462 +\n 463 +\n 464 +\n 465 +\n 466 +\n 467 +\n 468 +\n 469 +\n 470 +\n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  ",
            "\t/**\n\t * Creates the launch context, which describes how to bring up a TaskExecutor / TaskManager process in\n\t * an allocated YARN container.\n\t *\n\t * <p>This code is extremely YARN specific and registers all the resources that the TaskExecutor\n\t * needs (such as JAR file, config file, ...) and all environment variables in a YARN\n\t * container launch context. The launch context then ensures that those resources will be\n\t * copied into the containers transient working directory.\n\t *\n\t * @param flinkConfig\n\t *\t\t The Flink configuration object.\n\t * @param yarnConfig\n\t *\t\t The YARN configuration object.\n\t * @param env\n\t *\t\t The environment variables.\n\t * @param tmParams\n\t *\t\t The TaskExecutor container memory parameters.\n\t * @param taskManagerConfig\n\t *\t\t The configuration for the TaskExecutors.\n\t * @param workingDirectory\n\t *\t\t The current application master container's working directory.\n\t * @param taskManagerMainClass\n\t *\t\t The class with the main method.\n\t * @param log\n\t *\t\t The logger.\n\t *\n\t * @return The launch context for the TaskManager processes.\n\t *\n\t * @throws Exception Thrown if teh launch context could not be created, for example if\n\t *\t\t\t\t   the resources could not be copied.\n\t */\n\tstatic ContainerLaunchContext createTaskExecutorContext(\n\t\torg.apache.flink.configuration.Configuration flinkConfig,\n\t\tYarnConfiguration yarnConfig,\n\t\tMap<String, String> env,\n\t\tContaineredTaskManagerParameters tmParams,\n\t\torg.apache.flink.configuration.Configuration taskManagerConfig,\n\t\tString workingDirectory,\n\t\tClass<?> taskManagerMainClass,\n\t\tLogger log) throws Exception {\n\n\t\t// get and validate all relevant variables\n\n\t\tString remoteFlinkJarPath = env.get(YarnConfigKeys.FLINK_JAR_PATH);\n\t\trequire(remoteFlinkJarPath != null, \"Environment variable %s not set\", YarnConfigKeys.FLINK_JAR_PATH);\n\n\t\tString appId = env.get(YarnConfigKeys.ENV_APP_ID);\n\t\trequire(appId != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_APP_ID);\n\n\t\tString clientHomeDir = env.get(YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\t\trequire(clientHomeDir != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\n\t\tString shipListString = env.get(YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\t\trequire(shipListString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\n\t\tString yarnClientUsername = env.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\trequire(yarnClientUsername != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\tfinal String remoteKeytabPath = env.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tlog.info(\"TM:remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = env.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tlog.info(\"TM:remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\tfinal String remoteYarnConfPath = env.get(YarnConfigKeys.ENV_YARN_SITE_XML_PATH);\n\t\tlog.info(\"TM:remote yarn conf path obtained {}\", remoteYarnConfPath);\n\n\t\tfinal String remoteKrb5Path = env.get(YarnConfigKeys.ENV_KRB5_PATH);\n\t\tlog.info(\"TM:remote krb5 path obtained {}\", remoteKrb5Path);\n\n\t\tString classPathString = env.get(ENV_FLINK_CLASSPATH);\n\t\trequire(classPathString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_FLINK_CLASSPATH);\n\n\t\t//register keytab\n\t\tLocalResource keytabResource = null;\n\t\tif (remoteKeytabPath != null) {\n\t\t\tlog.info(\"Adding keytab {} to the AM container local resource bucket\", remoteKeytabPath);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(remoteKeytabPath);\n\t\t\tFileSystem fs = keytabPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, keytabPath, keytabResource);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tLocalResource yarnConfResource = null;\n\t\tLocalResource krb5ConfResource = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (remoteYarnConfPath != null && remoteKrb5Path != null) {\n\t\t\tlog.info(\"TM:Adding remoteYarnConfPath {} to the container local resource bucket\", remoteYarnConfPath);\n\t\t\tyarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath yarnConfPath = new Path(remoteYarnConfPath);\n\t\t\tFileSystem fs = yarnConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, yarnConfPath, yarnConfResource);\n\n\t\t\tlog.info(\"TM:Adding remoteKrb5Path {} to the container local resource bucket\", remoteKrb5Path);\n\t\t\tkrb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath krb5ConfPath = new Path(remoteKrb5Path);\n\t\t\tfs = krb5ConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, krb5ConfPath, krb5ConfResource);\n\n\t\t\thasKrb5 = true;\n\t\t}\n\n\t\t// register Flink Jar with remote HDFS\n\t\tLocalResource flinkJar = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\tPath remoteJarPath = new Path(remoteFlinkJarPath);\n\t\t\tFileSystem fs = remoteJarPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, remoteJarPath, flinkJar);\n\t\t}\n\n\t\t// register conf with local fs\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\t// write the TaskManager configuration to a local file\n\t\t\tfinal File taskManagerConfigFile =\n\t\t\t\t\tnew File(workingDirectory, UUID.randomUUID() + \"-taskmanager-conf.yaml\");\n\t\t\tlog.debug(\"Writing TaskManager configuration to {}\", taskManagerConfigFile.getAbsolutePath());\n\t\t\tBootstrapTools.writeConfiguration(taskManagerConfig, taskManagerConfigFile);\n\n\t\t\tPath homeDirPath = new Path(clientHomeDir);\n\t\t\tFileSystem fs = homeDirPath.getFileSystem(yarnConfig);\n\t\t\tsetupLocalResource(fs, appId,\n\t\t\t\t\tnew Path(taskManagerConfigFile.toURI()), flinkConf, new Path(clientHomeDir));\n\n\t\t\tlog.info(\"Prepared local resource for modified yaml: {}\", flinkConf);\n\t\t}\n\n\t\tMap<String, LocalResource> taskManagerLocalResources = new HashMap<>();\n\t\ttaskManagerLocalResources.put(\"flink.jar\", flinkJar);\n\t\ttaskManagerLocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (yarnConfResource != null && krb5ConfResource != null) {\n\t\t\ttaskManagerLocalResources.put(YARN_SITE_FILE_NAME, yarnConfResource);\n\t\t\ttaskManagerLocalResources.put(KRB5_FILE_NAME, krb5ConfResource);\n\t\t}\n\n\t\tif (keytabResource != null) {\n\t\t\ttaskManagerLocalResources.put(KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\t// prepare additional files to be shipped\n\t\tfor (String pathStr : shipListString.split(\",\")) {\n\t\t\tif (!pathStr.isEmpty()) {\n\t\t\t\tLocalResource resource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath path = new Path(pathStr);\n\t\t\t\tregisterLocalResource(path.getFileSystem(yarnConfig), path, resource);\n\t\t\t\ttaskManagerLocalResources.put(path.getName(), resource);\n\t\t\t}\n\t\t}\n\n\t\t// now that all resources are prepared, we can create the launch context\n\n\t\tlog.info(\"Creating container launch context for TaskManagers\");\n\n\t\tboolean hasLogback = new File(workingDirectory, \"logback.xml\").exists();\n\t\tboolean hasLog4j = new File(workingDirectory, \"log4j.properties\").exists();\n\n\t\tString launchCommand = BootstrapTools.getTaskManagerShellCommand(\n\t\t\t\tflinkConfig, tmParams, \".\", ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n\t\t\t\thasLogback, hasLog4j, hasKrb5, taskManagerMainClass);\n\n\t\tlog.info(\"Starting TaskManagers with command: \" + launchCommand);\n\n\t\tContainerLaunchContext ctx = Records.newRecord(ContainerLaunchContext.class);\n\t\tctx.setCommands(Collections.singletonList(launchCommand));\n\t\tctx.setLocalResources(taskManagerLocalResources);\n\n\t\tMap<String, String> containerEnv = new HashMap<>();\n\t\tcontainerEnv.putAll(tmParams.taskManagerEnv());\n\n\t\t// add YARN classpath, etc to the container environment\n\t\tcontainerEnv.put(ENV_FLINK_CLASSPATH, classPathString);\n\t\tsetupYarnClassPath(yarnConfig, containerEnv);\n\n\t\tcontainerEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (remoteKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PATH, remoteKeytabPath);\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, remoteKeytabPrincipal);\n\t\t}\n\n\t\tctx.setEnvironment(containerEnv);\n\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tlog.debug(\"Adding security tokens to Task Executor Container launch Context....\");\n\t\t\t/*\n\t\t\t * For taskmanager yarn container context, read the tokens from the jobmanager yarn container local flie.\n\t\t\t * Notify: must read the tokens from the local file, but not from UGI context.Because if UGI is login\n\t\t\t * from Keytab, there is no HDFS degegation token in UGI context.\n\t\t\t */\n\t\t\tString fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\"readTokenStorageFile\",\n\t\t\t\tFile.class, org.apache.hadoop.conf.Configuration.class);\n\t\t\tCredentials cred = (Credentials) readTokenStorageFileMethod.invoke(null, new File(fileLocation),\n\t\t\t\tnew SecurityUtils.SecurityConfiguration(flinkConfig).getHadoopConfiguration());\n\t\t\tcred.writeTokenStorageToStream(dob);\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tctx.setTokens(securityTokens);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);\n\t\t}\n\n\t\treturn ctx;\n\t}"
        ]
    ],
    "67bf467a10f68a26ad573d5426c307f0402423bd": [
        [
            "SSLUtils::getSSLEnabled(Configuration)",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59  ",
            "\t/**\n\t * Retrieves the global ssl flag from configuration\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return true if global ssl flag is set\n\t */\n\tpublic static boolean getSSLEnabled(Configuration sslConfig) {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\n\t\treturn sslConfig.getBoolean( ConfigConstants.SECURITY_SSL_ENABLED,\n\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_ENABLED);\n\t}",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  ",
            "\t/**\n\t * Retrieves the global ssl flag from configuration\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return true if global ssl flag is set\n\t */\n\tpublic static boolean getSSLEnabled(Configuration sslConfig) {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\n\t\treturn sslConfig.getBoolean(SecurityOptions.SSL_ENABLED);\n\t}"
        ],
        [
            "SSLUtils::setSSLVerifyHostname(Configuration,SSLParameters)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118 -\n 119  \n 120  \n 121  \n 122  ",
            "\t/**\n\t * Sets SSL options to verify peer's hostname in the certificate\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @param sslParams\n\t *        The SSL parameters that need to be updated\n\t */\n\tpublic static void setSSLVerifyHostname(Configuration sslConfig, SSLParameters sslParams) {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tPreconditions.checkNotNull(sslParams);\n\n\t\tboolean verifyHostname = sslConfig.getBoolean(ConfigConstants.SECURITY_SSL_VERIFY_HOSTNAME,\n\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_VERIFY_HOSTNAME);\n\t\tif (verifyHostname) {\n\t\t\tsslParams.setEndpointIdentificationAlgorithm(\"HTTPS\");\n\t\t}\n\t}",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111  \n 112  ",
            "\t/**\n\t * Sets SSL options to verify peer's hostname in the certificate\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @param sslParams\n\t *        The SSL parameters that need to be updated\n\t */\n\tpublic static void setSSLVerifyHostname(Configuration sslConfig, SSLParameters sslParams) {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tPreconditions.checkNotNull(sslParams);\n\n\t\tboolean verifyHostname = sslConfig.getBoolean(SecurityOptions.SSL_VERIFY_HOSTNAME);\n\t\tif (verifyHostname) {\n\t\t\tsslParams.setEndpointIdentificationAlgorithm(\"HTTPS\");\n\t\t}\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLServerContextMisconfiguration()",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120 -\n 121 -\n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "\t/**\n\t * Tests if SSL Server Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextMisconfiguration() {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"badpassword\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"badpassword\");\n\n\t\ttry {\n\t\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\t\tAssert.fail(\"SSL server context created even with bad SSL configuration \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120 +\n 121 +\n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "\t/**\n\t * Tests if SSL Server Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextMisconfiguration() {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"badpassword\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"badpassword\");\n\n\t\ttry {\n\t\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\t\tAssert.fail(\"SSL server context created even with bad SSL configuration \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLClientContextMisconfiguration()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72 -\n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t/**\n\t * Tests if SSL Client Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContextMisconfiguration() {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD, \"badpassword\");\n\n\t\ttry {\n\t\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\t\tAssert.fail(\"SSL client context created even with bad SSL configuration \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72 +\n  73 +\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t/**\n\t * Tests if SSL Client Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContextMisconfiguration() {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD, \"badpassword\");\n\n\t\ttry {\n\t\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\t\tAssert.fail(\"SSL client context created even with bad SSL configuration \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLServerContextWithSSLDisabled()",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107  \n 108  \n 109  \n 110  ",
            "\t/**\n\t * Tests if SSL Server Context is not created if SSL is disabled\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextWithSSLDisabled() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, false);\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tAssert.assertNull(serverContext);\n\t}",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  ",
            "\t/**\n\t * Tests if SSL Server Context is not created if SSL is disabled\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextWithSSLDisabled() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, false);\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tAssert.assertNull(serverContext);\n\t}"
        ],
        [
            "NettyClientServerSslTest::testSslHandshakeError()",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "\t/**\n\t * Verify SSL handshake error when untrusted server certificate is used\n\t *\n\t */\n\t@Test\n\tpublic void testSslHandshakeError() throws Exception {\n\t\tNettyProtocol protocol = new NettyProtocol() {\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getServerChannelHandlers() {\n\t\t\t\treturn new ChannelHandler[0];\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getClientChannelHandlers() { return new ChannelHandler[0]; }\n\t\t};\n\n\t\tConfiguration config = createSslConfig();\n\n\t\t// Use a server certificate which is not present in the truststore\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/untrusted.keystore\");\n\n\t\tNettyConfig nettyConfig = new NettyConfig(\n\t\t\tInetAddress.getLoopbackAddress(),\n\t\t\tNetUtils.getAvailablePort(),\n\t\t\tNettyTestUtil.DEFAULT_SEGMENT_SIZE,\n\t\t\t1,\n\t\t\tconfig);\n\n\t\tNettyTestUtil.NettyServerAndClient serverAndClient = NettyTestUtil.initServerAndClient(protocol, nettyConfig);\n\n\t\tChannel ch = NettyTestUtil.connect(serverAndClient);\n\t\tch.pipeline().addLast(new StringDecoder()).addLast(new StringEncoder());\n\n\t\t// Attempting to write data over ssl should fail\n\t\tassertFalse(ch.writeAndFlush(\"test\").await().isSuccess());\n\n\t\tNettyTestUtil.shutdown(serverAndClient);\n\t}",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "\t/**\n\t * Verify SSL handshake error when untrusted server certificate is used\n\t *\n\t */\n\t@Test\n\tpublic void testSslHandshakeError() throws Exception {\n\t\tNettyProtocol protocol = new NettyProtocol() {\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getServerChannelHandlers() {\n\t\t\t\treturn new ChannelHandler[0];\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getClientChannelHandlers() { return new ChannelHandler[0]; }\n\t\t};\n\n\t\tConfiguration config = createSslConfig();\n\n\t\t// Use a server certificate which is not present in the truststore\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/untrusted.keystore\");\n\n\t\tNettyConfig nettyConfig = new NettyConfig(\n\t\t\tInetAddress.getLoopbackAddress(),\n\t\t\tNetUtils.getAvailablePort(),\n\t\t\tNettyTestUtil.DEFAULT_SEGMENT_SIZE,\n\t\t\t1,\n\t\t\tconfig);\n\n\t\tNettyTestUtil.NettyServerAndClient serverAndClient = NettyTestUtil.initServerAndClient(protocol, nettyConfig);\n\n\t\tChannel ch = NettyTestUtil.connect(serverAndClient);\n\t\tch.pipeline().addLast(new StringDecoder()).addLast(new StringEncoder());\n\n\t\t// Attempting to write data over ssl should fail\n\t\tassertFalse(ch.writeAndFlush(\"test\").await().isSuccess());\n\n\t\tNettyTestUtil.shutdown(serverAndClient);\n\t}"
        ],
        [
            "BlobClientSslTest::startNonSSLServer()",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91 -\n  92 -\n  93 -\n  94  \n  95  \n  96  \n  97 -\n  98  \n  99 -\n 100 -\n 101  ",
            "\t/**\n\t * Starts the SSL disabled BLOB server.\n\t */\n\t@BeforeClass\n\tpublic static void startNonSSLServer() throws IOException {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tconfig.setBoolean(BlobServerOptions.SSL_ENABLED, false);\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tBLOB_SERVER = new BlobServer(config, new VoidBlobStore());\n\n\t\tclientConfig = new Configuration();\n\t\tclientConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tclientConfig.setBoolean(BlobServerOptions.SSL_ENABLED, false);\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t}",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89  \n  90 +\n  91 +\n  92 +\n  93  \n  94  \n  95  \n  96 +\n  97  \n  98 +\n  99 +\n 100  ",
            "\t/**\n\t * Starts the SSL disabled BLOB server.\n\t */\n\t@BeforeClass\n\tpublic static void startNonSSLServer() throws IOException {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tconfig.setBoolean(BlobServerOptions.SSL_ENABLED, false);\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tconfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tBLOB_SERVER = new BlobServer(config, new VoidBlobStore());\n\n\t\tclientConfig = new Configuration();\n\t\tclientConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tclientConfig.setBoolean(BlobServerOptions.SSL_ENABLED, false);\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLClientContextWithSSLDisabled()",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 -\n  59  \n  60  \n  61  \n  62  ",
            "\t/**\n\t * Tests if SSL Client Context is not created if SSL is not configured\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContextWithSSLDisabled() throws Exception {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, false);\n\n\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\tAssert.assertNull(clientContext);\n\t}",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 +\n  59  \n  60  \n  61  \n  62  ",
            "\t/**\n\t * Tests if SSL Client Context is not created if SSL is not configured\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContextWithSSLDisabled() throws Exception {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(SecurityOptions.SSL_ENABLED, false);\n\n\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\tAssert.assertNull(clientContext);\n\t}"
        ],
        [
            "SSLUtilsTest::testSetSSLVersionAndCipherSuitesForSSLEngine()",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 -\n 202 -\n 203 -\n 204 -\n 205 -\n 206 -\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "\t/**\n\t * Tests if SSLUtils set the right ssl version and cipher suites for SSLEngine\n\t */\n\t@Test\n\tpublic void testSetSSLVersionAndCipherSuitesForSSLEngine() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_PROTOCOL, \"TLSv1\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_ALGORITHMS, \"TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tSSLEngine engine = serverContext.createSSLEngine();\n\n\t\tString[] protocols = engine.getEnabledProtocols();\n\t\tString[] algorithms = engine.getEnabledCipherSuites();\n\n\t\tAssert.assertNotEquals(1, protocols.length);\n\t\tAssert.assertNotEquals(2, algorithms.length);\n\n\t\tSSLUtils.setSSLVerAndCipherSuites(engine, serverConfig);\n\t\tprotocols = engine.getEnabledProtocols();\n\t\talgorithms = engine.getEnabledCipherSuites();\n\n\t\tAssert.assertEquals(1, protocols.length);\n\t\tAssert.assertEquals(\"TLSv1\", protocols[0]);\n\t\tAssert.assertEquals(2, algorithms.length);\n\t\tAssert.assertTrue(algorithms[0].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA\") || algorithms[0].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\tAssert.assertTrue(algorithms[1].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA\") || algorithms[1].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\"));\n\t}",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "\t/**\n\t * Tests if SSLUtils set the right ssl version and cipher suites for SSLEngine\n\t */\n\t@Test\n\tpublic void testSetSSLVersionAndCipherSuitesForSSLEngine() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_PROTOCOL, \"TLSv1\");\n\t\tserverConfig.setString(SecurityOptions.SSL_ALGORITHMS, \"TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tSSLEngine engine = serverContext.createSSLEngine();\n\n\t\tString[] protocols = engine.getEnabledProtocols();\n\t\tString[] algorithms = engine.getEnabledCipherSuites();\n\n\t\tAssert.assertNotEquals(1, protocols.length);\n\t\tAssert.assertNotEquals(2, algorithms.length);\n\n\t\tSSLUtils.setSSLVerAndCipherSuites(engine, serverConfig);\n\t\tprotocols = engine.getEnabledProtocols();\n\t\talgorithms = engine.getEnabledCipherSuites();\n\n\t\tAssert.assertEquals(1, protocols.length);\n\t\tAssert.assertEquals(\"TLSv1\", protocols[0]);\n\t\tAssert.assertEquals(2, algorithms.length);\n\t\tAssert.assertTrue(algorithms[0].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA\") || algorithms[0].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\tAssert.assertTrue(algorithms[1].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA\") || algorithms[1].equals(\"TLS_DHE_RSA_WITH_AES_128_CBC_SHA256\"));\n\t}"
        ],
        [
            "BlobClientSslTest::startSSLServer()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71 -\n  72 -\n  73 -\n  74  \n  75  \n  76 -\n  77  \n  78 -\n  79 -\n  80 -\n  81  ",
            "\t/**\n\t * Starts the SSL enabled BLOB server.\n\t */\n\t@BeforeClass\n\tpublic static void startSSLServer() throws IOException {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tBLOB_SSL_SERVER = new BlobServer(config, new VoidBlobStore());\n\n\n\t\tsslClientConfig = new Configuration();\n\t\tsslClientConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tsslClientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tsslClientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 +\n  71 +\n  72 +\n  73 +\n  74  \n  75  \n  76  \n  77 +\n  78 +\n  79 +\n  80  ",
            "\t/**\n\t * Starts the SSL enabled BLOB server.\n\t */\n\t@BeforeClass\n\tpublic static void startSSLServer() throws IOException {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tconfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tBLOB_SSL_SERVER = new BlobServer(config, new VoidBlobStore());\n\n\t\tsslClientConfig = new Configuration();\n\t\tsslClientConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tsslClientConfig.setString(SecurityOptions.SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tsslClientConfig.setString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLClientContext()",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 -\n  44 -\n  45 -\n  46  \n  47  \n  48  \n  49  ",
            "\t/**\n\t * Tests if SSL Client Context is created given a valid SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContext() throws Exception {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD, \"password\");\n\n\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\tAssert.assertNotNull(clientContext);\n\t}",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 +\n  44 +\n  45 +\n  46  \n  47  \n  48  \n  49  ",
            "\t/**\n\t * Tests if SSL Client Context is created given a valid SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLClientContext() throws Exception {\n\n\t\tConfiguration clientConfig = new Configuration();\n\t\tclientConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tclientConfig.setString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD, \"password\");\n\n\t\tSSLContext clientContext = SSLUtils.createSSLClientContext(clientConfig);\n\t\tAssert.assertNotNull(clientContext);\n\t}"
        ],
        [
            "SSLStoreOverlay::Builder::fromEnvironment(Configuration)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "\t\t/**\n\t\t * Configures the overlay using the current environment (and global configuration).\n\t\t *\n\t\t * The following Flink configuration settings are used to source the keystore and truststore:\n\t\t *  - security.ssl.keystore\n\t\t *  - security.ssl.truststore\n\t\t */\n\t\tpublic Builder fromEnvironment(Configuration globalConfiguration)  {\n\n\t\t\tString keystore = globalConfiguration.getString(ConfigConstants.SECURITY_SSL_KEYSTORE, null);\n\t\t\tif(keystore != null) {\n\t\t\t\tkeystorePath = new File(keystore);\n\t\t\t\tif(!keystorePath.exists()) {\n\t\t\t\t\tthrow new IllegalStateException(\"Invalid configuration for \" + ConfigConstants.SECURITY_SSL_KEYSTORE);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tString truststore = globalConfiguration.getString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, null);\n\t\t\tif(truststore != null) {\n\t\t\t\ttruststorePath = new File(truststore);\n\t\t\t\tif(!truststorePath.exists()) {\n\t\t\t\t\tthrow new IllegalStateException(\"Invalid configuration for \" + ConfigConstants.SECURITY_SSL_TRUSTSTORE);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn this;\n\t\t}",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  ",
            "\t\t/**\n\t\t * Configures the overlay using the current environment (and global configuration).\n\t\t *\n\t\t * The following Flink configuration settings are used to source the keystore and truststore:\n\t\t *  - security.ssl.keystore\n\t\t *  - security.ssl.truststore\n\t\t */\n\t\tpublic Builder fromEnvironment(Configuration globalConfiguration)  {\n\n\t\t\tString keystore = globalConfiguration.getString(SecurityOptions.SSL_KEYSTORE);\n\t\t\tif(keystore != null) {\n\t\t\t\tkeystorePath = new File(keystore);\n\t\t\t\tif(!keystorePath.exists()) {\n\t\t\t\t\tthrow new IllegalStateException(\"Invalid configuration for \" + SecurityOptions.SSL_KEYSTORE.key());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tString truststore = globalConfiguration.getString(SecurityOptions.SSL_TRUSTSTORE);\n\t\t\tif(truststore != null) {\n\t\t\t\ttruststorePath = new File(truststore);\n\t\t\t\tif(!truststorePath.exists()) {\n\t\t\t\t\tthrow new IllegalStateException(\"Invalid configuration for \" + SecurityOptions.SSL_TRUSTSTORE.key());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn this;\n\t\t}"
        ],
        [
            "SSLUtils::createSSLServerContext(Configuration)",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 -\n 197 -\n 198 -\n 199  \n 200 -\n 201 -\n 202 -\n 203  \n 204 -\n 205 -\n 206 -\n 207  \n 208 -\n 209 -\n 210 -\n 211  \n 212 -\n 213 -\n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  ",
            "\t/**\n\t * Creates the SSL Context for the server if SSL is configured\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return The SSLContext object which can be used by the ssl transport server\n\t * \t       Returns null if SSL is disabled\n\t * @throws Exception\n\t *         Thrown if there is any misconfiguration\n\t */\n\tpublic static SSLContext createSSLServerContext(Configuration sslConfig) throws Exception {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tSSLContext serverSSLContext = null;\n\n\t\tif (getSSLEnabled(sslConfig)) {\n\t\t\tLOG.debug(\"Creating server SSL context from configuration\");\n\n\t\t\tString keystoreFilePath = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_KEYSTORE,\n\t\t\t\tnull);\n\n\t\t\tString keystorePassword = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD,\n\t\t\t\tnull);\n\n\t\t\tString certPassword = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_KEY_PASSWORD,\n\t\t\t\tnull);\n\n\t\t\tString sslProtocolVersion = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_PROTOCOL,\n\t\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_PROTOCOL);\n\n\t\t\tPreconditions.checkNotNull(keystoreFilePath, ConfigConstants.SECURITY_SSL_KEYSTORE + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(keystorePassword, ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(certPassword, ConfigConstants.SECURITY_SSL_KEY_PASSWORD + \" was not configured.\");\n\n\t\t\tKeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType());\n\t\t\tFileInputStream keyStoreFile = null;\n\t\t\ttry {\n\t\t\t\tkeyStoreFile = new FileInputStream(new File(keystoreFilePath));\n\t\t\t\tks.load(keyStoreFile, keystorePassword.toCharArray());\n\t\t\t} finally {\n\t\t\t\tif (keyStoreFile != null) {\n\t\t\t\t\tkeyStoreFile.close();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Set up key manager factory to use the server key store\n\t\t\tKeyManagerFactory kmf = KeyManagerFactory.getInstance(\n\t\t\t\t\tKeyManagerFactory.getDefaultAlgorithm());\n\t\t\tkmf.init(ks, certPassword.toCharArray());\n\n\t\t\t// Initialize the SSLContext\n\t\t\tserverSSLContext = SSLContext.getInstance(sslProtocolVersion);\n\t\t\tserverSSLContext.init(kmf.getKeyManagers(), null, null);\n\t\t}\n\n\t\treturn serverSSLContext;\n\t}",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182 +\n 183  \n 184 +\n 185  \n 186 +\n 187  \n 188 +\n 189 +\n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  ",
            "\t/**\n\t * Creates the SSL Context for the server if SSL is configured\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return The SSLContext object which can be used by the ssl transport server\n\t * \t       Returns null if SSL is disabled\n\t * @throws Exception\n\t *         Thrown if there is any misconfiguration\n\t */\n\tpublic static SSLContext createSSLServerContext(Configuration sslConfig) throws Exception {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tSSLContext serverSSLContext = null;\n\n\t\tif (getSSLEnabled(sslConfig)) {\n\t\t\tLOG.debug(\"Creating server SSL context from configuration\");\n\n\t\t\tString keystoreFilePath = sslConfig.getString(SecurityOptions.SSL_KEYSTORE);\n\n\t\t\tString keystorePassword = sslConfig.getString(SecurityOptions.SSL_KEYSTORE_PASSWORD);\n\n\t\t\tString certPassword = sslConfig.getString(SecurityOptions.SSL_KEY_PASSWORD);\n\n\t\t\tString sslProtocolVersion = sslConfig.getString(SecurityOptions.SSL_PROTOCOL);\n\n\t\t\tPreconditions.checkNotNull(keystoreFilePath, SecurityOptions.SSL_KEYSTORE.key() + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(keystorePassword, SecurityOptions.SSL_KEYSTORE_PASSWORD.key() + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(certPassword, SecurityOptions.SSL_KEY_PASSWORD.key() + \" was not configured.\");\n\n\t\t\tKeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType());\n\t\t\tFileInputStream keyStoreFile = null;\n\t\t\ttry {\n\t\t\t\tkeyStoreFile = new FileInputStream(new File(keystoreFilePath));\n\t\t\t\tks.load(keyStoreFile, keystorePassword.toCharArray());\n\t\t\t} finally {\n\t\t\t\tif (keyStoreFile != null) {\n\t\t\t\t\tkeyStoreFile.close();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Set up key manager factory to use the server key store\n\t\t\tKeyManagerFactory kmf = KeyManagerFactory.getInstance(\n\t\t\t\t\tKeyManagerFactory.getDefaultAlgorithm());\n\t\t\tkmf.init(ks, certPassword.toCharArray());\n\n\t\t\t// Initialize the SSLContext\n\t\t\tserverSSLContext = SSLContext.getInstance(sslProtocolVersion);\n\t\t\tserverSSLContext.init(kmf.getKeyManagers(), null, null);\n\t\t}\n\n\t\treturn serverSSLContext;\n\t}"
        ],
        [
            "SSLStoreOverlayTest::testBuilderFromEnvironment()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72 -\n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\t@Test\n\tpublic void testBuilderFromEnvironment() throws Exception {\n\n\t\tfinal Configuration conf = new Configuration();\n\t\tFile keystore = tempFolder.newFile();\n\t\tFile truststore = tempFolder.newFile();\n\n\t\tconf.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, keystore.getAbsolutePath());\n\t\tconf.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, truststore.getAbsolutePath());\n\n\t\tSSLStoreOverlay.Builder builder = SSLStoreOverlay.newBuilder().fromEnvironment(conf);\n\t\tassertEquals(builder.keystorePath, keystore);\n\t\tassertEquals(builder.truststorePath, truststore);\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\t@Test\n\tpublic void testBuilderFromEnvironment() throws Exception {\n\n\t\tfinal Configuration conf = new Configuration();\n\t\tFile keystore = tempFolder.newFile();\n\t\tFile truststore = tempFolder.newFile();\n\n\t\tconf.setString(SecurityOptions.SSL_KEYSTORE, keystore.getAbsolutePath());\n\t\tconf.setString(SecurityOptions.SSL_TRUSTSTORE, truststore.getAbsolutePath());\n\n\t\tSSLStoreOverlay.Builder builder = SSLStoreOverlay.newBuilder().fromEnvironment(conf);\n\t\tassertEquals(builder.keystorePath, keystore);\n\t\tassertEquals(builder.truststorePath, truststore);\n\t}"
        ],
        [
            "SSLUtils::setSSLVerAndCipherSuites(ServerSocket,Configuration)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71 -\n  72 -\n  73  \n  74 -\n  75 -\n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  ",
            "\t/**\n\t * Sets SSl version and cipher suites for SSLServerSocket\n\t * @param socket\n\t *        Socket to be handled\n\t * @param config\n\t *        The application configuration\n\t */\n\tpublic static void setSSLVerAndCipherSuites(ServerSocket socket, Configuration config) {\n\t\tif (socket instanceof SSLServerSocket) {\n\t\t\tfinal String[] protocols = config.getString(\n\t\t\t\t\tConfigConstants.SECURITY_SSL_PROTOCOL,\n\t\t\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_PROTOCOL).split(\",\");\n\n\t\t\tfinal String[] cipherSuites = config.getString(\n\t\t\t\t\tConfigConstants.SECURITY_SSL_ALGORITHMS,\n\t\t\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_ALGORITHMS).split(\",\");\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Configuring TLS version and cipher suites on SSL socket {} / {}\",\n\t\t\t\t\t\tArrays.toString(protocols), Arrays.toString(cipherSuites));\n\t\t\t}\n\n\t\t\t((SSLServerSocket) socket).setEnabledProtocols(protocols);\n\t\t\t((SSLServerSocket) socket).setEnabledCipherSuites(cipherSuites);\n\t\t}\n\t}",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t/**\n\t * Sets SSl version and cipher suites for SSLServerSocket\n\t * @param socket\n\t *        Socket to be handled\n\t * @param config\n\t *        The application configuration\n\t */\n\tpublic static void setSSLVerAndCipherSuites(ServerSocket socket, Configuration config) {\n\t\tif (socket instanceof SSLServerSocket) {\n\t\t\tfinal String[] protocols = config.getString(SecurityOptions.SSL_PROTOCOL).split(\",\");\n\n\t\t\tfinal String[] cipherSuites = config.getString(SecurityOptions.SSL_ALGORITHMS).split(\",\");\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Configuring TLS version and cipher suites on SSL socket {} / {}\",\n\t\t\t\t\t\tArrays.toString(protocols), Arrays.toString(cipherSuites));\n\t\t\t}\n\n\t\t\t((SSLServerSocket) socket).setEnabledProtocols(protocols);\n\t\t\t((SSLServerSocket) socket).setEnabledCipherSuites(cipherSuites);\n\t\t}\n\t}"
        ],
        [
            "SSLStoreOverlayTest::testConfigure()",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51  \n  52 -\n  53  \n  54  ",
            "\t@Test\n\tpublic void testConfigure() throws Exception {\n\n\t\tFile keystore = tempFolder.newFile();\n\t\tFile truststore = tempFolder.newFile();\n\t\tSSLStoreOverlay overlay = new SSLStoreOverlay(keystore, truststore);\n\n\t\tContainerSpecification spec = new ContainerSpecification();\n\t\toverlay.configure(spec);\n\n\t\tassertEquals(TARGET_KEYSTORE_PATH.getPath(), spec.getDynamicConfiguration().getString(ConfigConstants.SECURITY_SSL_KEYSTORE, null));\n\t\tcheckArtifact(spec, TARGET_KEYSTORE_PATH);\n\n\t\tassertEquals(TARGET_TRUSTSTORE_PATH.getPath(), spec.getDynamicConfiguration().getString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, null));\n\t\tcheckArtifact(spec, TARGET_TRUSTSTORE_PATH);\n\t}",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 +\n  50  \n  51  \n  52 +\n  53  \n  54  ",
            "\t@Test\n\tpublic void testConfigure() throws Exception {\n\n\t\tFile keystore = tempFolder.newFile();\n\t\tFile truststore = tempFolder.newFile();\n\t\tSSLStoreOverlay overlay = new SSLStoreOverlay(keystore, truststore);\n\n\t\tContainerSpecification spec = new ContainerSpecification();\n\t\toverlay.configure(spec);\n\n\t\tassertEquals(TARGET_KEYSTORE_PATH.getPath(), spec.getDynamicConfiguration().getString(SecurityOptions.SSL_KEYSTORE));\n\t\tcheckArtifact(spec, TARGET_KEYSTORE_PATH);\n\n\t\tassertEquals(TARGET_TRUSTSTORE_PATH.getPath(), spec.getDynamicConfiguration().getString(SecurityOptions.SSL_TRUSTSTORE));\n\t\tcheckArtifact(spec, TARGET_TRUSTSTORE_PATH);\n\t}"
        ],
        [
            "SSLUtils::setSSLVerAndCipherSuites(SSLEngine,Configuration)",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97 -\n  98 -\n  99 -\n 100 -\n 101 -\n 102  ",
            "\t/**\n\t * Sets SSL version and cipher suites for SSLEngine\n\t * @param engine\n\t *        SSLEngine to be handled\n\t * @param config\n\t *        The application configuration\n\t */\n\tpublic static void setSSLVerAndCipherSuites(SSLEngine engine, Configuration config) {\n\t\tengine.setEnabledProtocols(config.getString(\n\t\t\tConfigConstants.SECURITY_SSL_PROTOCOL,\n\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_PROTOCOL).split(\",\"));\n\t\tengine.setEnabledCipherSuites(config.getString(\n\t\t\tConfigConstants.SECURITY_SSL_ALGORITHMS,\n\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_ALGORITHMS).split(\",\"));\n\t}",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92 +\n  93  ",
            "\t/**\n\t * Sets SSL version and cipher suites for SSLEngine\n\t * @param engine\n\t *        SSLEngine to be handled\n\t * @param config\n\t *        The application configuration\n\t */\n\tpublic static void setSSLVerAndCipherSuites(SSLEngine engine, Configuration config) {\n\t\tengine.setEnabledProtocols(config.getString(SecurityOptions.SSL_PROTOCOL).split(\",\"));\n\t\tengine.setEnabledCipherSuites(config.getString(SecurityOptions.SSL_ALGORITHMS).split(\",\"));\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLServerContext()",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93 -\n  94  \n  95  \n  96  \n  97  ",
            "\t/**\n\t * Tests if SSL Server Context is created given a valid SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContext() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tAssert.assertNotNull(serverContext);\n\t}",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92 +\n  93 +\n  94  \n  95  \n  96  \n  97  ",
            "\t/**\n\t * Tests if SSL Server Context is created given a valid SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContext() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tAssert.assertNotNull(serverContext);\n\t}"
        ],
        [
            "SSLUtils::createSSLClientContext(Configuration)",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143 -\n 144 -\n 145 -\n 146 -\n 147 -\n 148 -\n 149 -\n 150 -\n 151  \n 152 -\n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  ",
            "\t/**\n\t * Creates the SSL Context for the client if SSL is configured\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return The SSLContext object which can be used by the ssl transport client\n\t * \t       Returns null if SSL is disabled\n\t * @throws Exception\n\t *         Thrown if there is any misconfiguration\n\t */\n\tpublic static SSLContext createSSLClientContext(Configuration sslConfig) throws Exception {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tSSLContext clientSSLContext = null;\n\n\t\tif (getSSLEnabled(sslConfig)) {\n\t\t\tLOG.debug(\"Creating client SSL context from configuration\");\n\n\t\t\tString trustStoreFilePath = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_TRUSTSTORE,\n\t\t\t\tnull);\n\t\t\tString trustStorePassword = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD,\n\t\t\t\tnull);\n\t\t\tString sslProtocolVersion = sslConfig.getString(\n\t\t\t\tConfigConstants.SECURITY_SSL_PROTOCOL,\n\t\t\t\tConfigConstants.DEFAULT_SECURITY_SSL_PROTOCOL);\n\n\t\t\tPreconditions.checkNotNull(trustStoreFilePath, ConfigConstants.SECURITY_SSL_TRUSTSTORE + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(trustStorePassword, ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD + \" was not configured.\");\n\n\t\t\tKeyStore trustStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\n\t\t\tFileInputStream trustStoreFile = null;\n\t\t\ttry {\n\t\t\t\ttrustStoreFile = new FileInputStream(new File(trustStoreFilePath));\n\t\t\t\ttrustStore.load(trustStoreFile, trustStorePassword.toCharArray());\n\t\t\t} finally {\n\t\t\t\tif (trustStoreFile != null) {\n\t\t\t\t\ttrustStoreFile.close();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tTrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(\n\t\t\t\tTrustManagerFactory.getDefaultAlgorithm());\n\t\t\ttrustManagerFactory.init(trustStore);\n\n\t\t\tclientSSLContext = SSLContext.getInstance(sslProtocolVersion);\n\t\t\tclientSSLContext.init(null, trustManagerFactory.getTrustManagers(), null);\n\t\t}\n\n\t\treturn clientSSLContext;\n\t}",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133 +\n 134 +\n 135  \n 136 +\n 137 +\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\t/**\n\t * Creates the SSL Context for the client if SSL is configured\n\t *\n\t * @param sslConfig\n\t *        The application configuration\n\t * @return The SSLContext object which can be used by the ssl transport client\n\t * \t       Returns null if SSL is disabled\n\t * @throws Exception\n\t *         Thrown if there is any misconfiguration\n\t */\n\tpublic static SSLContext createSSLClientContext(Configuration sslConfig) throws Exception {\n\n\t\tPreconditions.checkNotNull(sslConfig);\n\t\tSSLContext clientSSLContext = null;\n\n\t\tif (getSSLEnabled(sslConfig)) {\n\t\t\tLOG.debug(\"Creating client SSL context from configuration\");\n\n\t\t\tString trustStoreFilePath = sslConfig.getString(SecurityOptions.SSL_TRUSTSTORE);\n\t\t\tString trustStorePassword = sslConfig.getString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD);\n\t\t\tString sslProtocolVersion = sslConfig.getString(SecurityOptions.SSL_PROTOCOL);\n\n\t\t\tPreconditions.checkNotNull(trustStoreFilePath, SecurityOptions.SSL_TRUSTSTORE.key() + \" was not configured.\");\n\t\t\tPreconditions.checkNotNull(trustStorePassword, SecurityOptions.SSL_TRUSTSTORE_PASSWORD.key() + \" was not configured.\");\n\n\t\t\tKeyStore trustStore = KeyStore.getInstance(KeyStore.getDefaultType());\n\n\t\t\tFileInputStream trustStoreFile = null;\n\t\t\ttry {\n\t\t\t\ttrustStoreFile = new FileInputStream(new File(trustStoreFilePath));\n\t\t\t\ttrustStore.load(trustStoreFile, trustStorePassword.toCharArray());\n\t\t\t} finally {\n\t\t\t\tif (trustStoreFile != null) {\n\t\t\t\t\ttrustStoreFile.close();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tTrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(\n\t\t\t\tTrustManagerFactory.getDefaultAlgorithm());\n\t\t\ttrustManagerFactory.init(trustStore);\n\n\t\t\tclientSSLContext = SSLContext.getInstance(sslProtocolVersion);\n\t\t\tclientSSLContext.init(null, trustManagerFactory.getTrustManagers(), null);\n\t\t}\n\n\t\treturn clientSSLContext;\n\t}"
        ],
        [
            "SSLUtilsTest::testCreateSSLServerContextWithMultiProtocols()",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140 -\n 141 -\n 142 -\n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\t/**\n\t * Tests if SSL Server Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextWithMultiProtocols() {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_PROTOCOL, \"TLSv1,TLSv1.2\");\n\n\t\ttry {\n\t\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\t\tAssert.fail(\"SSL server context created even with multiple protocols set \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140 +\n 141 +\n 142 +\n 143 +\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\t/**\n\t * Tests if SSL Server Context creation fails with bad SSL configuration\n\t */\n\t@Test\n\tpublic void testCreateSSLServerContextWithMultiProtocols() {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_PROTOCOL, \"TLSv1,TLSv1.2\");\n\n\t\ttry {\n\t\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\t\tAssert.fail(\"SSL server context created even with multiple protocols set \");\n\t\t} catch (Exception e) {\n\t\t\t// Exception here is valid\n\t\t}\n\t}"
        ],
        [
            "SSLStoreOverlay::configure(ContainerSpecification)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 -\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  ",
            "\t@Override\n\tpublic void configure(ContainerSpecification container) throws IOException {\n\t\tif(keystore != null) {\n\t\t\tcontainer.getArtifacts().add(ContainerSpecification.Artifact.newBuilder()\n\t\t\t\t.setSource(keystore)\n\t\t\t\t.setDest(TARGET_KEYSTORE_PATH)\n\t\t\t\t.setCachable(false)\n\t\t\t\t.build());\n\t\t\tcontainer.getDynamicConfiguration().setString(ConfigConstants.SECURITY_SSL_KEYSTORE, TARGET_KEYSTORE_PATH.getPath());\n\t\t}\n\t\tif(truststore != null) {\n\t\t\tcontainer.getArtifacts().add(ContainerSpecification.Artifact.newBuilder()\n\t\t\t\t.setSource(truststore)\n\t\t\t\t.setDest(TARGET_TRUSTSTORE_PATH)\n\t\t\t\t.setCachable(false)\n\t\t\t\t.build());\n\t\t\tcontainer.getDynamicConfiguration().setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, TARGET_TRUSTSTORE_PATH.getPath());\n\t\t}\n\t}",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  ",
            "\t@Override\n\tpublic void configure(ContainerSpecification container) throws IOException {\n\t\tif(keystore != null) {\n\t\t\tcontainer.getArtifacts().add(ContainerSpecification.Artifact.newBuilder()\n\t\t\t\t.setSource(keystore)\n\t\t\t\t.setDest(TARGET_KEYSTORE_PATH)\n\t\t\t\t.setCachable(false)\n\t\t\t\t.build());\n\t\t\tcontainer.getDynamicConfiguration().setString(SecurityOptions.SSL_KEYSTORE, TARGET_KEYSTORE_PATH.getPath());\n\t\t}\n\t\tif(truststore != null) {\n\t\t\tcontainer.getArtifacts().add(ContainerSpecification.Artifact.newBuilder()\n\t\t\t\t.setSource(truststore)\n\t\t\t\t.setDest(TARGET_TRUSTSTORE_PATH)\n\t\t\t\t.setCachable(false)\n\t\t\t\t.build());\n\t\t\tcontainer.getDynamicConfiguration().setString(SecurityOptions.SSL_TRUSTSTORE, TARGET_TRUSTSTORE_PATH.getPath());\n\t\t}\n\t}"
        ],
        [
            "NettyClientServerSslTest::testInvalidSslConfiguration()",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\t/**\n\t * Verify failure on invalid ssl configuration\n\t *\n\t */\n\t@Test\n\tpublic void testInvalidSslConfiguration() throws Exception {\n\t\tNettyProtocol protocol = new NettyProtocol() {\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getServerChannelHandlers() {\n\t\t\t\treturn new ChannelHandler[0];\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getClientChannelHandlers() { return new ChannelHandler[0]; }\n\t\t};\n\n\t\tConfiguration config = createSslConfig();\n\t\t// Modify the keystore password to an incorrect one\n\t\tconfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"invalidpassword\");\n\n\t\tNettyConfig nettyConfig = new NettyConfig(\n\t\t\tInetAddress.getLoopbackAddress(),\n\t\t\tNetUtils.getAvailablePort(),\n\t\t\tNettyTestUtil.DEFAULT_SEGMENT_SIZE,\n\t\t\t1,\n\t\t\tconfig);\n\n\t\tNettyTestUtil.NettyServerAndClient serverAndClient = null;\n\t\ttry {\n\t\t\tserverAndClient = NettyTestUtil.initServerAndClient(protocol, nettyConfig);\n\t\t\tAssert.fail(\"Created server and client from invalid configuration\");\n\t\t} catch (Exception e) {\n\t\t\t// Exception should be thrown as expected\n\t\t}\n\n\t\tNettyTestUtil.shutdown(serverAndClient);\n\t}",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\t/**\n\t * Verify failure on invalid ssl configuration\n\t *\n\t */\n\t@Test\n\tpublic void testInvalidSslConfiguration() throws Exception {\n\t\tNettyProtocol protocol = new NettyProtocol() {\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getServerChannelHandlers() {\n\t\t\t\treturn new ChannelHandler[0];\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic ChannelHandler[] getClientChannelHandlers() { return new ChannelHandler[0]; }\n\t\t};\n\n\t\tConfiguration config = createSslConfig();\n\t\t// Modify the keystore password to an incorrect one\n\t\tconfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"invalidpassword\");\n\n\t\tNettyConfig nettyConfig = new NettyConfig(\n\t\t\tInetAddress.getLoopbackAddress(),\n\t\t\tNetUtils.getAvailablePort(),\n\t\t\tNettyTestUtil.DEFAULT_SEGMENT_SIZE,\n\t\t\t1,\n\t\t\tconfig);\n\n\t\tNettyTestUtil.NettyServerAndClient serverAndClient = null;\n\t\ttry {\n\t\t\tserverAndClient = NettyTestUtil.initServerAndClient(protocol, nettyConfig);\n\t\t\tAssert.fail(\"Created server and client from invalid configuration\");\n\t\t} catch (Exception e) {\n\t\t\t// Exception should be thrown as expected\n\t\t}\n\n\t\tNettyTestUtil.shutdown(serverAndClient);\n\t}"
        ],
        [
            "SSLUtilsTest::testSetSSLVersionAndCipherSuitesForSSLServerSocket()",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161 -\n 162 -\n 163 -\n 164 -\n 165 -\n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "\t/**\n\t * Tests if SSLUtils set the right ssl version and cipher suites for SSLServerSocket\n\t */\n\t@Test\n\tpublic void testSetSSLVersionAndCipherSuitesForSSLServerSocket() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_PROTOCOL, \"TLSv1.1\");\n\t\tserverConfig.setString(ConfigConstants.SECURITY_SSL_ALGORITHMS, \"TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tServerSocket socket = null;\n\t\ttry {\n\t\t\tsocket = serverContext.getServerSocketFactory().createServerSocket(0);\n\n\t\t\tString[] protocols = ((SSLServerSocket) socket).getEnabledProtocols();\n\t\t\tString[] algorithms = ((SSLServerSocket) socket).getEnabledCipherSuites();\n\n\t\t\tAssert.assertNotEquals(1, protocols.length);\n\t\t\tAssert.assertNotEquals(2, algorithms.length);\n\n\t\t\tSSLUtils.setSSLVerAndCipherSuites(socket, serverConfig);\n\t\t\tprotocols = ((SSLServerSocket) socket).getEnabledProtocols();\n\t\t\talgorithms = ((SSLServerSocket) socket).getEnabledCipherSuites();\n\n\t\t\tAssert.assertEquals(1, protocols.length);\n\t\t\tAssert.assertEquals(\"TLSv1.1\", protocols[0]);\n\t\t\tAssert.assertEquals(2, algorithms.length);\n\t\t\tAssert.assertTrue(algorithms[0].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA\") || algorithms[0].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\t\tAssert.assertTrue(algorithms[1].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA\") || algorithms[1].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\t} finally {\n\t\t\tif (socket != null) {\n\t\t\t\tsocket.close();\n\t\t\t}\n\t\t}\n\t}",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "\t/**\n\t * Tests if SSLUtils set the right ssl version and cipher suites for SSLServerSocket\n\t */\n\t@Test\n\tpublic void testSetSSLVersionAndCipherSuitesForSSLServerSocket() throws Exception {\n\n\t\tConfiguration serverConfig = new Configuration();\n\t\tserverConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tserverConfig.setString(SecurityOptions.SSL_PROTOCOL, \"TLSv1.1\");\n\t\tserverConfig.setString(SecurityOptions.SSL_ALGORITHMS, \"TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256\");\n\n\t\tSSLContext serverContext = SSLUtils.createSSLServerContext(serverConfig);\n\t\tServerSocket socket = null;\n\t\ttry {\n\t\t\tsocket = serverContext.getServerSocketFactory().createServerSocket(0);\n\n\t\t\tString[] protocols = ((SSLServerSocket) socket).getEnabledProtocols();\n\t\t\tString[] algorithms = ((SSLServerSocket) socket).getEnabledCipherSuites();\n\n\t\t\tAssert.assertNotEquals(1, protocols.length);\n\t\t\tAssert.assertNotEquals(2, algorithms.length);\n\n\t\t\tSSLUtils.setSSLVerAndCipherSuites(socket, serverConfig);\n\t\t\tprotocols = ((SSLServerSocket) socket).getEnabledProtocols();\n\t\t\talgorithms = ((SSLServerSocket) socket).getEnabledCipherSuites();\n\n\t\t\tAssert.assertEquals(1, protocols.length);\n\t\t\tAssert.assertEquals(\"TLSv1.1\", protocols[0]);\n\t\t\tAssert.assertEquals(2, algorithms.length);\n\t\t\tAssert.assertTrue(algorithms[0].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA\") || algorithms[0].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\t\tAssert.assertTrue(algorithms[1].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA\") || algorithms[1].equals(\"TLS_RSA_WITH_AES_128_CBC_SHA256\"));\n\t\t} finally {\n\t\t\tif (socket != null) {\n\t\t\t\tsocket.close();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "NettyClientServerSslTest::createSslConfig()",
            " 149  \n 150  \n 151  \n 152 -\n 153 -\n 154 -\n 155 -\n 156 -\n 157 -\n 158  \n 159  ",
            "\tprivate Configuration createSslConfig() throws Exception {\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setBoolean(ConfigConstants.SECURITY_SSL_ENABLED, true);\n\t\tflinkConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tflinkConfig.setString(ConfigConstants.SECURITY_SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tflinkConfig.setString(ConfigConstants.SECURITY_SSL_KEY_PASSWORD, \"password\");\n\t\tflinkConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tflinkConfig.setString(ConfigConstants.SECURITY_SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t\treturn flinkConfig;\n\t}",
            " 149  \n 150  \n 151  \n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158  \n 159  ",
            "\tprivate Configuration createSslConfig() throws Exception {\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setBoolean(SecurityOptions.SSL_ENABLED, true);\n\t\tflinkConfig.setString(SecurityOptions.SSL_KEYSTORE, \"src/test/resources/local127.keystore\");\n\t\tflinkConfig.setString(SecurityOptions.SSL_KEYSTORE_PASSWORD, \"password\");\n\t\tflinkConfig.setString(SecurityOptions.SSL_KEY_PASSWORD, \"password\");\n\t\tflinkConfig.setString(SecurityOptions.SSL_TRUSTSTORE, \"src/test/resources/local127.truststore\");\n\t\tflinkConfig.setString(SecurityOptions.SSL_TRUSTSTORE_PASSWORD, \"password\");\n\t\treturn flinkConfig;\n\t}"
        ]
    ],
    "3f0ac26e9f502f9e032af0375d52c5e4af2126f3": [
        [
            "TaskManagerConfigurationTest::testActorSystemPortConfig()",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98 -\n  99 -\n 100  \n 101  \n 102  \n 103 -\n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114 -\n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "\t@Test\n\tpublic void testActorSystemPortConfig() throws Exception {\n\t\t// config with pre-configured hostname to speed up tests (no interface selection)\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, \"localhost\");\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 7891);\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\t\t\t// auto port\n\t\t\tassertEquals(0, TaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices)._2());\n\n\t\t\t// pre-defined port\n\t\t\tfinal int testPort = 22551;\n\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, testPort);\n\t\t\tassertEquals(testPort, TaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices)._2());\n\n\t\t\t// invalid port\n\t\t\ttry {\n\t\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, -1);\n\t\t\t\tTaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices);\n\t\t\t\tfail(\"should fail with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// bam!\n\t\t\t}\n\n\t\t\t// invalid port\n\t\t\ttry {\n\t\t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_IPC_PORT_KEY, 100000);\n\t\t\t\tTaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices);\n\t\t\t\tfail(\"should fail with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// bam!\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t} finally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\t\t}\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96 +\n  97 +\n  98  \n  99  \n 100  \n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 +\n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "\t@Test\n\tpublic void testActorSystemPortConfig() throws Exception {\n\t\t// config with pre-configured hostname to speed up tests (no interface selection)\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, \"localhost\");\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 7891);\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\t\t\t// auto port\n\t\t\tIterator<Integer> portsIter = TaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices)._2();\n\t\t\tassertTrue(portsIter.hasNext());\n\t\t\tassertEquals(0, (int) portsIter.next());\n\n\t\t\t// pre-defined port\n\t\t\tfinal int testPort = 22551;\n\t\t\tconfig.setString(TaskManagerOptions.RPC_PORT, String.valueOf(testPort));\n\n\t\t\tportsIter = TaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices)._2();\n\t\t\tassertTrue(portsIter.hasNext());\n\t\t\tassertEquals(testPort, (int) portsIter.next());\n\n\t\t\t// port range\n\t\t\tconfig.setString(TaskManagerOptions.RPC_PORT, \"8000-8001\");\n\t\t\tportsIter = TaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices)._2();\n\t\t\tassertTrue(portsIter.hasNext());\n\t\t\tassertEquals(8000, (int) portsIter.next());\n\t\t\tassertEquals(8001, (int) portsIter.next());\n\n\t\t\t// invalid port\n\t\t\ttry {\n\t\t\t\tconfig.setString(TaskManagerOptions.RPC_PORT, \"-1\");\n\t\t\t\tTaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices);\n\t\t\t\tfail(\"should fail with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// bam!\n\t\t\t}\n\n\t\t\t// invalid port\n\t\t\ttry {\n\t\t\t\tconfig.setString(TaskManagerOptions.RPC_PORT, \"100000\");\n\t\t\t\tTaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices);\n\t\t\t\tfail(\"should fail with an exception\");\n\t\t\t}\n\t\t\tcatch (IllegalConfigurationException e) {\n\t\t\t\t// bam!\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t} finally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\t\t}\n\t}"
        ],
        [
            "TaskManagerConfigurationTest::testUsePreconfiguredNetworkInterface()",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 -\n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\t@Test\n\tpublic void testUsePreconfiguredNetworkInterface() throws Exception {\n\t\tfinal String TEST_HOST_NAME = \"testhostname\";\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, TEST_HOST_NAME);\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 7891);\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\n\n\t\t\tTuple2<String, Object> address = TaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices);\n\n\t\t\t// validate the configured test host name\n\t\t\tassertEquals(TEST_HOST_NAME, address._1());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t} finally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\t\t}\n\t}",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Test\n\tpublic void testUsePreconfiguredNetworkInterface() throws Exception {\n\t\tfinal String TEST_HOST_NAME = \"testhostname\";\n\n\t\tConfiguration config = new Configuration();\n\t\tconfig.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, TEST_HOST_NAME);\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 7891);\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\n\t\t\tTuple2<String, Iterator<Integer>> address = TaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices);\n\n\t\t\t// validate the configured test host name\n\t\t\tassertEquals(TEST_HOST_NAME, address._1());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t} finally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\t\t}\n\t}"
        ],
        [
            "TaskManagerConfigurationTest::testNetworkInterfaceSelection()",
            " 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 -\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  ",
            "\t@Test\n\tpublic void testNetworkInterfaceSelection() throws Exception {\n\t\tServerSocket server;\n\t\tString hostname = \"localhost\";\n\n\t\ttry {\n\t\t\tInetAddress localhostAddress = InetAddress.getByName(hostname);\n\t\t\tserver = new ServerSocket(0, 50, localhostAddress);\n\t\t} catch (IOException e) {\n\t\t\t// may happen in certain test setups, skip test.\n\t\t\tSystem.err.println(\"Skipping 'testNetworkInterfaceSelection' test.\");\n\t\t\treturn;\n\t\t}\n\n\t\t// open a server port to allow the system to connect\n\t\tConfiguration config = new Configuration();\n\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, hostname);\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, server.getLocalPort());\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\t\t\tassertNotNull(TaskManager.selectNetworkInterfaceAndPort(config, highAvailabilityServices)._1());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\n\t\t\ttry {\n\t\t\t\tserver.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// ignore shutdown errors\n\t\t\t}\n\t\t}\n\t}",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "\t@Test\n\tpublic void testNetworkInterfaceSelection() throws Exception {\n\t\tServerSocket server;\n\t\tString hostname = \"localhost\";\n\n\t\ttry {\n\t\t\tInetAddress localhostAddress = InetAddress.getByName(hostname);\n\t\t\tserver = new ServerSocket(0, 50, localhostAddress);\n\t\t} catch (IOException e) {\n\t\t\t// may happen in certain test setups, skip test.\n\t\t\tSystem.err.println(\"Skipping 'testNetworkInterfaceSelection' test.\");\n\t\t\treturn;\n\t\t}\n\n\t\t// open a server port to allow the system to connect\n\t\tConfiguration config = new Configuration();\n\n\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, hostname);\n\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, server.getLocalPort());\n\n\t\tHighAvailabilityServices highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\tconfig,\n\t\t\tExecutors.directExecutor(),\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\ttry {\n\t\t\tassertNotNull(TaskManager.selectNetworkInterfaceAndPortRange(config, highAvailabilityServices)._1());\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t\tfinally {\n\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n\n\t\t\ttry {\n\t\t\t\tserver.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// ignore shutdown errors\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "NetUtils::getPortRangeFromString(String)",
            " 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  ",
            "\t/**\n\t * Returns an iterator over available ports defined by the range definition.\n\t *\n\t * @param rangeDefinition String describing a single port, a range of ports or multiple ranges.\n\t * @return Set of ports from the range definition\n\t * @throws NumberFormatException If an invalid string is passed.\n\t */\n\tpublic static Iterator<Integer> getPortRangeFromString(String rangeDefinition) throws NumberFormatException {\n\t\tfinal String[] ranges = rangeDefinition.trim().split(\",\");\n\t\t\n\t\tUnionIterator<Integer> iterators = new UnionIterator<>();\n\t\t\n\t\tfor (String rawRange: ranges) {\n\t\t\tIterator<Integer> rangeIterator;\n\t\t\tString range = rawRange.trim();\n\t\t\tint dashIdx = range.indexOf('-');\n\t\t\tif (dashIdx == -1) {\n\t\t\t\t// only one port in range:\n\t\t\t\trangeIterator = Collections.singleton(Integer.valueOf(range)).iterator();\n\t\t\t} else {\n\t\t\t\t// evaluate range\n\t\t\t\tfinal int start = Integer.valueOf(range.substring(0, dashIdx));\n\t\t\t\tfinal int end = Integer.valueOf(range.substring(dashIdx+1, range.length()));\n\t\t\t\trangeIterator = new Iterator<Integer>() {\n\t\t\t\t\tint i = start;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic boolean hasNext() {\n\t\t\t\t\t\treturn i <= end;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Integer next() {\n\t\t\t\t\t\treturn i++;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void remove() {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\"Remove not supported\");\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t\titerators.add(rangeIterator);\n\t\t}\n\t\t\n\t\treturn iterators;\n\t}",
            " 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 +\n 323 +\n 324 +\n 325 +\n 326 +\n 327  \n 328  \n 329  \n 330  \n 331 +\n 332 +\n 333 +\n 334 +\n 335  \n 336 +\n 337 +\n 338 +\n 339 +\n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "\t/**\n\t * Returns an iterator over available ports defined by the range definition.\n\t *\n\t * @param rangeDefinition String describing a single port, a range of ports or multiple ranges.\n\t * @return Set of ports from the range definition\n\t * @throws NumberFormatException If an invalid string is passed.\n\t */\n\tpublic static Iterator<Integer> getPortRangeFromString(String rangeDefinition) throws NumberFormatException {\n\t\tfinal String[] ranges = rangeDefinition.trim().split(\",\");\n\t\t\n\t\tUnionIterator<Integer> iterators = new UnionIterator<>();\n\t\t\n\t\tfor (String rawRange: ranges) {\n\t\t\tIterator<Integer> rangeIterator;\n\t\t\tString range = rawRange.trim();\n\t\t\tint dashIdx = range.indexOf('-');\n\t\t\tif (dashIdx == -1) {\n\t\t\t\t// only one port in range:\n\t\t\t\tfinal int port = Integer.valueOf(range);\n\t\t\t\tif (port < 0 || port > 65535) {\n\t\t\t\t\tthrow new IllegalConfigurationException(\"Invalid port configuration. Port must be between 0\" +\n\t\t\t\t\t\t\"and 65535, but was \" + port + \".\");\n\t\t\t\t}\n\t\t\t\trangeIterator = Collections.singleton(Integer.valueOf(range)).iterator();\n\t\t\t} else {\n\t\t\t\t// evaluate range\n\t\t\t\tfinal int start = Integer.valueOf(range.substring(0, dashIdx));\n\t\t\t\tif (start < 0 || start > 65535) {\n\t\t\t\t\tthrow new IllegalConfigurationException(\"Invalid port configuration. Port must be between 0\" +\n\t\t\t\t\t\t\"and 65535, but was \" + start + \".\");\n\t\t\t\t}\n\t\t\t\tfinal int end = Integer.valueOf(range.substring(dashIdx+1, range.length()));\n\t\t\t\tif (end < 0 || end > 65535) {\n\t\t\t\t\tthrow new IllegalConfigurationException(\"Invalid port configuration. Port must be between 0\" +\n\t\t\t\t\t\t\"and 65535, but was \" + end + \".\");\n\t\t\t\t}\n\t\t\t\trangeIterator = new Iterator<Integer>() {\n\t\t\t\t\tint i = start;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic boolean hasNext() {\n\t\t\t\t\t\treturn i <= end;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Integer next() {\n\t\t\t\t\t\treturn i++;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void remove() {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\"Remove not supported\");\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t\titerators.add(rangeIterator);\n\t\t}\n\t\t\n\t\treturn iterators;\n\t}"
        ]
    ],
    "59eab45458b3b1637ccbc5dafd326cc84ffb9655": [
        [
            "ManualConsumerProducerTest::main(String)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 -\n  60 -\n  61 -\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(4);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new ProduceIntoKinesis.EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew KinesisSerializationSchema<String>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic ByteBuffer serialize(String element) {\n\t\t\t\t\t\treturn ByteBuffer.wrap(element.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t\t}\n\n\t\t\t\t\t// every 10th element goes into a different stream\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String getTargetStream(String element) {\n\t\t\t\t\t\tif (element.split(\"-\")[0].endsWith(\"0\")) {\n\t\t\t\t\t\t\treturn \"flink-test-2\";\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null; // send to default stream\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tkinesisProducerConfig\n\t\t);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"test-flink\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\t\tkinesis.setCustomPartitioner(new KinesisPartitioner<String>() {\n\t\t\t@Override\n\t\t\tpublic String getPartitionId(String element) {\n\t\t\t\tint l = element.length();\n\t\t\t\treturn element.substring(l - 1, l);\n\t\t\t}\n\t\t});\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\t// consuming topology\n\t\tProperties consumerProps = new Properties();\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tDataStream<String> consuming = see.addSource(new FlinkKinesisConsumer<>(\"test-flink\", new SimpleStringSchema(), consumerProps));\n\t\t// validate consumed records for correctness\n\t\tconsuming.flatMap(new FlatMapFunction<String, String>() {\n\t\t\t@Override\n\t\t\tpublic void flatMap(String value, Collector<String> out) throws Exception {\n\t\t\t\tString[] parts = value.split(\"-\");\n\t\t\t\ttry {\n\t\t\t\t\tlong l = Long.parseLong(parts[0]);\n\t\t\t\t\tif (l < 0) {\n\t\t\t\t\t\tthrow new RuntimeException(\"Negative\");\n\t\t\t\t\t}\n\t\t\t\t} catch (NumberFormatException nfe) {\n\t\t\t\t\tthrow new RuntimeException(\"First part of '\" + value + \"' is not a valid numeric type\");\n\t\t\t\t}\n\t\t\t\tif (parts[1].length() != 12) {\n\t\t\t\t\tthrow new RuntimeException(\"Second part of '\" + value + \"' doesn't have 12 characters\");\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t\tconsuming.print();\n\n\t\tsee.execute();\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(4);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new ProduceIntoKinesis.EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew KinesisSerializationSchema<String>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic ByteBuffer serialize(String element) {\n\t\t\t\t\t\treturn ByteBuffer.wrap(element.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t\t}\n\n\t\t\t\t\t// every 10th element goes into a different stream\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String getTargetStream(String element) {\n\t\t\t\t\t\tif (element.split(\"-\")[0].endsWith(\"0\")) {\n\t\t\t\t\t\t\treturn \"flink-test-2\";\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null; // send to default stream\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tkinesisProducerConfig\n\t\t);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"test-flink\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\t\tkinesis.setCustomPartitioner(new KinesisPartitioner<String>() {\n\t\t\t@Override\n\t\t\tpublic String getPartitionId(String element) {\n\t\t\t\tint l = element.length();\n\t\t\t\treturn element.substring(l - 1, l);\n\t\t\t}\n\t\t});\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\t// consuming topology\n\t\tProperties consumerProps = new Properties();\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\t\tconsumerProps.setProperty(ConsumerConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tDataStream<String> consuming = see.addSource(new FlinkKinesisConsumer<>(\"test-flink\", new SimpleStringSchema(), consumerProps));\n\t\t// validate consumed records for correctness\n\t\tconsuming.flatMap(new FlatMapFunction<String, String>() {\n\t\t\t@Override\n\t\t\tpublic void flatMap(String value, Collector<String> out) throws Exception {\n\t\t\t\tString[] parts = value.split(\"-\");\n\t\t\t\ttry {\n\t\t\t\t\tlong l = Long.parseLong(parts[0]);\n\t\t\t\t\tif (l < 0) {\n\t\t\t\t\t\tthrow new RuntimeException(\"Negative\");\n\t\t\t\t\t}\n\t\t\t\t} catch (NumberFormatException nfe) {\n\t\t\t\t\tthrow new RuntimeException(\"First part of '\" + value + \"' is not a valid numeric type\");\n\t\t\t\t}\n\t\t\t\tif (parts[1].length() != 12) {\n\t\t\t\t\tthrow new RuntimeException(\"Second part of '\" + value + \"' doesn't have 12 characters\");\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t\tconsuming.print();\n\n\t\tsee.execute();\n\t}"
        ],
        [
            "ProduceIntoKinesis::main(String)",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46 -\n  47 -\n  48 -\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(1);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew SimpleStringSchema(), kinesisProducerConfig);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"flink-test\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\tsee.execute();\n\t}",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48 +\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(1);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew SimpleStringSchema(), kinesisProducerConfig);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"flink-test\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\tsee.execute();\n\t}"
        ],
        [
            "ManualProducerTest::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 -\n  57 -\n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(4);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new ProduceIntoKinesis.EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(ProducerConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew KinesisSerializationSchema<String>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic ByteBuffer serialize(String element) {\n\t\t\t\t\t\treturn ByteBuffer.wrap(element.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t\t}\n\n\t\t\t\t\t// every 10th element goes into a different stream\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String getTargetStream(String element) {\n\t\t\t\t\t\tif (element.split(\"-\")[0].endsWith(\"0\")) {\n\t\t\t\t\t\t\treturn \"flink-test-2\";\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null; // send to default stream\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tkinesisProducerConfig\n\t\t);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"test-flink\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\t\tkinesis.setCustomPartitioner(new KinesisPartitioner<String>() {\n\t\t\t@Override\n\t\t\tpublic String getPartitionId(String element) {\n\t\t\t\tint l = element.length();\n\t\t\t\treturn element.substring(l - 1, l);\n\t\t\t}\n\t\t});\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\tsee.execute();\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57 +\n  58 +\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool pt = ParameterTool.fromArgs(args);\n\n\t\tStreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsee.setParallelism(4);\n\n\t\tDataStream<String> simpleStringStream = see.addSource(new ProduceIntoKinesis.EventsGenerator());\n\n\t\tProperties kinesisProducerConfig = new Properties();\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_REGION, pt.getRequired(\"region\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, pt.getRequired(\"accessKey\"));\n\t\tkinesisProducerConfig.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, pt.getRequired(\"secretKey\"));\n\n\t\tFlinkKinesisProducer<String> kinesis = new FlinkKinesisProducer<>(\n\t\t\t\tnew KinesisSerializationSchema<String>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic ByteBuffer serialize(String element) {\n\t\t\t\t\t\treturn ByteBuffer.wrap(element.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t\t}\n\n\t\t\t\t\t// every 10th element goes into a different stream\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String getTargetStream(String element) {\n\t\t\t\t\t\tif (element.split(\"-\")[0].endsWith(\"0\")) {\n\t\t\t\t\t\t\treturn \"flink-test-2\";\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn null; // send to default stream\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tkinesisProducerConfig\n\t\t);\n\n\t\tkinesis.setFailOnError(true);\n\t\tkinesis.setDefaultStream(\"test-flink\");\n\t\tkinesis.setDefaultPartition(\"0\");\n\t\tkinesis.setCustomPartitioner(new KinesisPartitioner<String>() {\n\t\t\t@Override\n\t\t\tpublic String getPartitionId(String element) {\n\t\t\t\tint l = element.length();\n\t\t\t\treturn element.substring(l - 1, l);\n\t\t\t}\n\t\t});\n\t\tsimpleStringStream.addSink(kinesis);\n\n\t\tsee.execute();\n\t}"
        ],
        [
            "KinesisConfigUtilTest::testUnparsableLongForProducerConfiguration()",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  ",
            "\t@Test\n\tpublic void testUnparsableLongForProducerConfiguration() {\n\t\texception.expect(IllegalArgumentException.class);\n\t\texception.expectMessage(\"Error trying to set field RateLimit with the value 'unparsableLong'\");\n\n\t\tProperties testConfig = new Properties();\n\t\ttestConfig.setProperty(ProducerConfigConstants.AWS_REGION, \"us-east-1\");\n\t\ttestConfig.setProperty(\"RateLimit\", \"unparsableLong\");\n\n\t\tKinesisConfigUtil.validateProducerConfiguration(testConfig);\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51  \n  52  \n  53  \n  54  ",
            "\t@Test\n\tpublic void testUnparsableLongForProducerConfiguration() {\n\t\texception.expect(IllegalArgumentException.class);\n\t\texception.expectMessage(\"Error trying to set field RateLimit with the value 'unparsableLong'\");\n\n\t\tProperties testConfig = new Properties();\n\t\ttestConfig.setProperty(AWSConfigConstants.AWS_REGION, \"us-east-1\");\n\t\ttestConfig.setProperty(\"RateLimit\", \"unparsableLong\");\n\n\t\tKinesisConfigUtil.validateProducerConfiguration(testConfig);\n\t}"
        ],
        [
            "KinesisConfigUtilTest::testReplaceDeprecatedKeys()",
            "  55  \n  56  \n  57  \n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  ",
            "\t@Test\n\tpublic void testReplaceDeprecatedKeys() {\n\t\tProperties testConfig = new Properties();\n\t\ttestConfig.setProperty(ProducerConfigConstants.AWS_REGION, \"us-east-1\");\n\t\ttestConfig.setProperty(ProducerConfigConstants.AGGREGATION_MAX_COUNT, \"1\");\n\t\ttestConfig.setProperty(ProducerConfigConstants.COLLECTION_MAX_COUNT, \"2\");\n\t\tProperties replacedConfig = KinesisConfigUtil.replaceDeprecatedProducerKeys(testConfig);\n\n\t\tassertEquals(\"1\", replacedConfig.getProperty(KinesisConfigUtil.AGGREGATION_MAX_COUNT));\n\t\tassertEquals(\"2\", replacedConfig.getProperty(KinesisConfigUtil.COLLECTION_MAX_COUNT));\n\t}",
            "  56  \n  57  \n  58  \n  59 +\n  60 +\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\t@Test\n\tpublic void testReplaceDeprecatedKeys() {\n\t\tProperties testConfig = new Properties();\n\t\ttestConfig.setProperty(AWSConfigConstants.AWS_REGION, \"us-east-1\");\n\t\t// these deprecated keys should be replaced\n\t\ttestConfig.setProperty(ProducerConfigConstants.AGGREGATION_MAX_COUNT, \"1\");\n\t\ttestConfig.setProperty(ProducerConfigConstants.COLLECTION_MAX_COUNT, \"2\");\n\t\tProperties replacedConfig = KinesisConfigUtil.replaceDeprecatedProducerKeys(testConfig);\n\n\t\tassertEquals(\"1\", replacedConfig.getProperty(KinesisConfigUtil.AGGREGATION_MAX_COUNT));\n\t\tassertEquals(\"2\", replacedConfig.getProperty(KinesisConfigUtil.COLLECTION_MAX_COUNT));\n\t}"
        ]
    ],
    "2cb37cb937c6f225ad7afe829a28a6eda043ffc1": [
        [
            "HBaseConnectorITCase::testTableSourceProjection()",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "\t@Test\n\tpublic void testTableSourceProjection() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sql(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  h.family1.col1, \" +\n\t\t\t\t\"  h.family3.col1, \" +\n\t\t\t\t\"  h.family3.col2, \" +\n\t\t\t\t\"  h.family3.col3 \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"10,1.01,false,Welt-1\\n\" +\n\t\t\t\"20,2.02,true,Welt-2\\n\" +\n\t\t\t\"30,3.03,false,Welt-3\\n\" +\n\t\t\t\"40,4.04,true,Welt-4\\n\" +\n\t\t\t\"50,5.05,false,Welt-5\\n\" +\n\t\t\t\"60,6.06,true,Welt-6\\n\" +\n\t\t\t\"70,7.07,false,Welt-7\\n\" +\n\t\t\t\"80,8.08,true,Welt-8\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 +\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "\t@Test\n\tpublic void testTableSourceProjection() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sqlQuery(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  h.family1.col1, \" +\n\t\t\t\t\"  h.family3.col1, \" +\n\t\t\t\t\"  h.family3.col2, \" +\n\t\t\t\t\"  h.family3.col3 \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"10,1.01,false,Welt-1\\n\" +\n\t\t\t\"20,2.02,true,Welt-2\\n\" +\n\t\t\t\"30,3.03,false,Welt-3\\n\" +\n\t\t\t\"40,4.04,true,Welt-4\\n\" +\n\t\t\t\"50,5.05,false,Welt-5\\n\" +\n\t\t\t\"60,6.06,true,Welt-6\\n\" +\n\t\t\t\"70,7.07,false,Welt-7\\n\" +\n\t\t\t\"80,8.08,true,Welt-8\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testJoin()",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\t@Test\n\tpublic void testJoin() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds1 = CollectionDataSets.getSmall3TupleDataSet(env);\n\t\tDataSet<Tuple5<Integer, Long, Integer, String, Long>> ds2 = CollectionDataSets.get5TupleDataSet(env);\n\n\t\ttableEnv.registerDataSet(\"t1\", ds1, \"a, b, c\");\n\t\ttableEnv.registerDataSet(\"t2\", ds2, \"d, e, f, g, h\");\n\n\t\tString sqlQuery = \"SELECT c, g FROM t1, t2 WHERE b = e\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"Hi,Hallo\\n\" + \"Hello,Hallo Welt\\n\" + \"Hello world,Hallo Welt\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\t@Test\n\tpublic void testJoin() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds1 = CollectionDataSets.getSmall3TupleDataSet(env);\n\t\tDataSet<Tuple5<Integer, Long, Integer, String, Long>> ds2 = CollectionDataSets.get5TupleDataSet(env);\n\n\t\ttableEnv.registerDataSet(\"t1\", ds1, \"a, b, c\");\n\t\ttableEnv.registerDataSet(\"t2\", ds2, \"d, e, f, g, h\");\n\n\t\tString sqlQuery = \"SELECT c, g FROM t1, t2 WHERE b = e\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"Hi,Hallo\\n\" + \"Hello,Hallo Welt\\n\" + \"Hello world,Hallo Welt\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "HBaseConnectorITCase::testTableSourceFieldOrder()",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 -\n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "\t@Test\n\tpublic void testTableSourceFieldOrder() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\t// shuffle order of column registration\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sql(\n\t\t\t\"SELECT * FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"Hello-1,100,1.01,false,Welt-1,10\\n\" +\n\t\t\t\"Hello-2,200,2.02,true,Welt-2,20\\n\" +\n\t\t\t\"Hello-3,300,3.03,false,Welt-3,30\\n\" +\n\t\t\t\"null,400,4.04,true,Welt-4,40\\n\" +\n\t\t\t\"Hello-5,500,5.05,false,Welt-5,50\\n\" +\n\t\t\t\"Hello-6,600,6.06,true,Welt-6,60\\n\" +\n\t\t\t\"Hello-7,700,7.07,false,Welt-7,70\\n\" +\n\t\t\t\"null,800,8.08,true,Welt-8,80\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239 +\n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "\t@Test\n\tpublic void testTableSourceFieldOrder() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\t// shuffle order of column registration\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sqlQuery(\n\t\t\t\"SELECT * FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"Hello-1,100,1.01,false,Welt-1,10\\n\" +\n\t\t\t\"Hello-2,200,2.02,true,Welt-2,20\\n\" +\n\t\t\t\"Hello-3,300,3.03,false,Welt-3,30\\n\" +\n\t\t\t\"null,400,4.04,true,Welt-4,40\\n\" +\n\t\t\t\"Hello-5,500,5.05,false,Welt-5,50\\n\" +\n\t\t\t\"Hello-6,600,6.06,true,Welt-6,60\\n\" +\n\t\t\t\"Hello-7,700,7.07,false,Welt-7,70\\n\" +\n\t\t\t\"null,800,8.08,true,Welt-8,80\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}"
        ],
        [
            "HBaseConnectorITCase::testTableSourceFullScan()",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "\t@Test\n\tpublic void testTableSourceFullScan() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sql(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  h.family1.col1, \" +\n\t\t\t\t\"  h.family2.col1, \" +\n\t\t\t\t\"  h.family2.col2, \" +\n\t\t\t\t\"  h.family3.col1, \" +\n\t\t\t\t\"  h.family3.col2, \" +\n\t\t\t\t\"  h.family3.col3 \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"10,Hello-1,100,1.01,false,Welt-1\\n\" +\n\t\t\t\"20,Hello-2,200,2.02,true,Welt-2\\n\" +\n\t\t\t\"30,Hello-3,300,3.03,false,Welt-3\\n\" +\n\t\t\t\"40,null,400,4.04,true,Welt-4\\n\" +\n\t\t\t\"50,Hello-5,500,5.05,false,Welt-5\\n\" +\n\t\t\t\"60,Hello-6,600,6.06,true,Welt-6\\n\" +\n\t\t\t\"70,Hello-7,700,7.07,false,Welt-7\\n\" +\n\t\t\t\"80,null,800,8.08,true,Welt-8\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "\t@Test\n\tpublic void testTableSourceFullScan() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY1, F1COL1, Integer.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, String.class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, Long.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL1, Double.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL2, Boolean.class);\n\t\thbaseTable.addColumn(FAMILY3, F3COL3, String.class);\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\n\t\tTable result = tableEnv.sqlQuery(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  h.family1.col1, \" +\n\t\t\t\t\"  h.family2.col1, \" +\n\t\t\t\t\"  h.family2.col2, \" +\n\t\t\t\t\"  h.family3.col1, \" +\n\t\t\t\t\"  h.family3.col2, \" +\n\t\t\t\t\"  h.family3.col3 \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"10,Hello-1,100,1.01,false,Welt-1\\n\" +\n\t\t\t\"20,Hello-2,200,2.02,true,Welt-2\\n\" +\n\t\t\t\"30,Hello-3,300,3.03,false,Welt-3\\n\" +\n\t\t\t\"40,null,400,4.04,true,Welt-4\\n\" +\n\t\t\t\"50,Hello-5,500,5.05,false,Welt-5\\n\" +\n\t\t\t\"60,Hello-6,600,6.06,true,Welt-6\\n\" +\n\t\t\t\"70,Hello-7,700,7.07,false,Welt-7\\n\" +\n\t\t\t\"80,null,800,8.08,true,Welt-8\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}"
        ],
        [
            "GroupingSetsITCase::checkSql(String,String)",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190  \n 191  \n 192  \n 193  ",
            "\t/**\n\t * Execute SQL query and check results.\n\t *\n\t * @param query    SQL query.\n\t * @param expected Expected result.\n\t */\n\tprivate void checkSql(String query, String expected) throws Exception {\n\t\tTable resultTable = tableEnv.sql(query);\n\t\tDataSet<Row> resultDataSet = tableEnv.toDataSet(resultTable, Row.class);\n\t\tList<Row> results = resultDataSet.collect();\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  ",
            "\t/**\n\t * Execute SQL query and check results.\n\t *\n\t * @param query    SQL query.\n\t * @param expected Expected result.\n\t */\n\tprivate void checkSql(String query, String expected) throws Exception {\n\t\tTable resultTable = tableEnv.sqlQuery(query);\n\t\tDataSet<Row> resultDataSet = tableEnv.toDataSet(resultTable, Row.class);\n\t\tList<Row> results = resultDataSet.collect();\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaTableSourceITCase::testBatchTableSourceSQL()",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\t@Test\n\tpublic void testBatchTableSourceSQL() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\t\tBatchTableSource csvTable = CommonTestData.getCsvTableSource();\n\n\t\ttableEnv.registerTableSource(\"persons\", csvTable);\n\n\t\tTable result = tableEnv\n\t\t\t.sql(\"SELECT `last`, FLOOR(id), score * 2 FROM persons WHERE score < 20\");\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected = \"Smith,1,24.6\\n\" +\n\t\t\t\"Miller,3,15.78\\n\" +\n\t\t\t\"Smith,4,0.24\\n\" +\n\t\t\t\"Miller,6,13.56\\n\" +\n\t\t\t\"Williams,8,4.68\\n\";\n\n\t\tcompareResultAsText(results, expected);\n\t}",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\t@Test\n\tpublic void testBatchTableSourceSQL() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\t\tBatchTableSource csvTable = CommonTestData.getCsvTableSource();\n\n\t\ttableEnv.registerTableSource(\"persons\", csvTable);\n\n\t\tTable result = tableEnv\n\t\t\t.sqlQuery(\"SELECT `last`, FLOOR(id), score * 2 FROM persons WHERE score < 20\");\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected = \"Smith,1,24.6\\n\" +\n\t\t\t\"Miller,3,15.78\\n\" +\n\t\t\t\"Smith,4,0.24\\n\" +\n\t\t\t\"Miller,6,13.56\\n\" +\n\t\t\t\"Williams,8,4.68\\n\";\n\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testAggregation()",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  ",
            "\t@Test\n\tpublic void testAggregation() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\ttableEnv.registerDataSet(\"AggTable\", ds, \"x, y, z\");\n\n\t\tString sqlQuery = \"SELECT sum(x), min(x), max(x), count(y), avg(x) FROM AggTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"231,1,21,21,11\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  ",
            "\t@Test\n\tpublic void testAggregation() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\ttableEnv.registerDataSet(\"AggTable\", ds, \"x, y, z\");\n\n\t\tString sqlQuery = \"SELECT sum(x), min(x), max(x), count(y), avg(x) FROM AggTable\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"231,1,21,21,11\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testMap()",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 -\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\t@Test\n\tpublic void testMap() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tList<Tuple2<Integer, Map<String, String>>> rows = new ArrayList<>();\n\t\trows.add(new Tuple2<>(1, Collections.singletonMap(\"foo\", \"bar\")));\n\t\trows.add(new Tuple2<>(2, Collections.singletonMap(\"foo\", \"spam\")));\n\n\t\tTypeInformation<Tuple2<Integer, Map<String, String>>> ty = new TupleTypeInfo<>(\n\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\tnew MapTypeInfo<>(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO));\n\n\t\tDataSet<Tuple2<Integer, Map<String, String>>> ds1 = env.fromCollection(rows, ty);\n\t\ttableEnv.registerDataSet(\"t1\", ds1, \"a, b\");\n\n\t\tString sqlQuery = \"SELECT b['foo'] FROM t1\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"bar\\n\" + \"spam\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\t@Test\n\tpublic void testMap() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tList<Tuple2<Integer, Map<String, String>>> rows = new ArrayList<>();\n\t\trows.add(new Tuple2<>(1, Collections.singletonMap(\"foo\", \"bar\")));\n\t\trows.add(new Tuple2<>(2, Collections.singletonMap(\"foo\", \"spam\")));\n\n\t\tTypeInformation<Tuple2<Integer, Map<String, String>>> ty = new TupleTypeInfo<>(\n\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\tnew MapTypeInfo<>(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO));\n\n\t\tDataSet<Tuple2<Integer, Map<String, String>>> ds1 = env.fromCollection(rows, ty);\n\t\ttableEnv.registerDataSet(\"t1\", ds1, \"a, b\");\n\n\t\tString sqlQuery = \"SELECT b['foo'] FROM t1\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"bar\\n\" + \"spam\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testSelectFromTable()",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "\t@Test\n\tpublic void testSelectFromTable() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataSet(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T\", in);\n\n\t\tString sqlQuery = \"SELECT a, c FROM T\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"1,Hi\\n\" + \"2,Hello\\n\" + \"3,Hello world\\n\" +\n\t\t\t\"4,Hello world, how are you?\\n\" + \"5,I am fine.\\n\" + \"6,Luke Skywalker\\n\" +\n\t\t\t\"7,Comment#1\\n\" + \"8,Comment#2\\n\" + \"9,Comment#3\\n\" + \"10,Comment#4\\n\" +\n\t\t\t\"11,Comment#5\\n\" + \"12,Comment#6\\n\" + \"13,Comment#7\\n\" +\n\t\t\t\"14,Comment#8\\n\" + \"15,Comment#9\\n\" + \"16,Comment#10\\n\" +\n\t\t\t\"17,Comment#11\\n\" + \"18,Comment#12\\n\" + \"19,Comment#13\\n\" +\n\t\t\t\"20,Comment#14\\n\" + \"21,Comment#15\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "\t@Test\n\tpublic void testSelectFromTable() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataSet(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T\", in);\n\n\t\tString sqlQuery = \"SELECT a, c FROM T\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"1,Hi\\n\" + \"2,Hello\\n\" + \"3,Hello world\\n\" +\n\t\t\t\"4,Hello world, how are you?\\n\" + \"5,I am fine.\\n\" + \"6,Luke Skywalker\\n\" +\n\t\t\t\"7,Comment#1\\n\" + \"8,Comment#2\\n\" + \"9,Comment#3\\n\" + \"10,Comment#4\\n\" +\n\t\t\t\"11,Comment#5\\n\" + \"12,Comment#6\\n\" + \"13,Comment#7\\n\" +\n\t\t\t\"14,Comment#8\\n\" + \"15,Comment#9\\n\" + \"16,Comment#10\\n\" +\n\t\t\t\"17,Comment#11\\n\" + \"18,Comment#12\\n\" + \"19,Comment#13\\n\" +\n\t\t\t\"20,Comment#14\\n\" + \"21,Comment#15\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testRowRegisterRowWithNames()",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString[] names = {\"a\", \"b\", \"c\"};\n\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString[] names = {\"a\", \"b\", \"c\"};\n\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testUnion()",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = JavaStreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = JavaStreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = JavaStreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = JavaStreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testValues()",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64 -\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  ",
            "\t@Test\n\tpublic void testValues() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tString sqlQuery = \"VALUES (1, 'Test', TRUE, DATE '1944-02-24', 12.4444444444444445),\" +\n\t\t\t\"(2, 'Hello', TRUE, DATE '1944-02-24', 12.666666665),\" +\n\t\t\t\"(3, 'World', FALSE, DATE '1944-12-24', 12.54444445)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"3,World,false,1944-12-24,12.5444444500000000\\n\" +\n\t\t\t\"2,Hello,true,1944-02-24,12.6666666650000000\\n\" +\n\t\t\t// Calcite converts to decimals and strings with equal length\n\t\t\t\"1,Test ,true,1944-02-24,12.4444444444444445\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  ",
            "\t@Test\n\tpublic void testValues() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tString sqlQuery = \"VALUES (1, 'Test', TRUE, DATE '1944-02-24', 12.4444444444444445),\" +\n\t\t\t\"(2, 'Hello', TRUE, DATE '1944-02-24', 12.666666665),\" +\n\t\t\t\"(3, 'World', FALSE, DATE '1944-12-24', 12.54444445)\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"3,World,false,1944-12-24,12.5444444500000000\\n\" +\n\t\t\t\"2,Hello,true,1944-02-24,12.6666666650000000\\n\" +\n\t\t\t// Calcite converts to decimals and strings with equal length\n\t\t\t\"1,Test ,true,1944-02-24,12.4444444444444445\\n\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testSelect()",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = JavaStreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = JavaStreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "JavaSqlITCase::testFilterFromDataSet()",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  ",
            "\t@Test\n\tpublic void testFilterFromDataSet() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\ttableEnv.registerDataSet(\"DataSetTable\", ds, \"x, y, z\");\n\n\t\tString sqlQuery = \"SELECT x FROM DataSetTable WHERE z LIKE '%Hello%'\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"2\\n\" + \"3\\n\" + \"4\";\n\t\tcompareResultAsText(results, expected);\n\t}",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  ",
            "\t@Test\n\tpublic void testFilterFromDataSet() throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\ttableEnv.registerDataSet(\"DataSetTable\", ds, \"x, y, z\");\n\n\t\tString sqlQuery = \"SELECT x FROM DataSetTable WHERE z LIKE '%Hello%'\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\t\tString expected = \"2\\n\" + \"3\\n\" + \"4\";\n\t\tcompareResultAsText(results, expected);\n\t}"
        ],
        [
            "GroupingSetsITCase::compareSql(String,String)",
            " 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212 -\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "\tprivate void compareSql(String query1, String query2) throws Exception {\n\n\t\t// Function to map row to string\n\t\tMapFunction<Row, String> mapFunction = new MapFunction<Row, String>() {\n\n\t\t\t@Override\n\t\t\tpublic String map(Row value) throws Exception {\n\t\t\t\treturn value == null ? \"null\" : value.toString();\n\t\t\t}\n\t\t};\n\n\t\t// Execute first query and store results\n\t\tTable resultTable1 = tableEnv.sql(query1);\n\t\tDataSet<Row> resultDataSet1 = tableEnv.toDataSet(resultTable1, Row.class);\n\t\tList<String> results1 = resultDataSet1.map(mapFunction).collect();\n\n\t\t// Execute second query and store results\n\t\tTable resultTable2 = tableEnv.sql(query2);\n\t\tDataSet<Row> resultDataSet2 = tableEnv.toDataSet(resultTable2, Row.class);\n\t\tList<String> results2 = resultDataSet2.map(mapFunction).collect();\n\n\t\t// Compare results\n\t\tTestBaseUtils.compareResultCollections(results1, results2, new Comparator<String>() {\n\n\t\t\t@Override\n\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\treturn o2 == null ? o1 == null ? 0 : 1 : o1.compareTo(o2);\n\t\t\t}\n\t\t});\n\t}",
            " 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208  \n 209  \n 210  \n 211  \n 212 +\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "\tprivate void compareSql(String query1, String query2) throws Exception {\n\n\t\t// Function to map row to string\n\t\tMapFunction<Row, String> mapFunction = new MapFunction<Row, String>() {\n\n\t\t\t@Override\n\t\t\tpublic String map(Row value) throws Exception {\n\t\t\t\treturn value == null ? \"null\" : value.toString();\n\t\t\t}\n\t\t};\n\n\t\t// Execute first query and store results\n\t\tTable resultTable1 = tableEnv.sqlQuery(query1);\n\t\tDataSet<Row> resultDataSet1 = tableEnv.toDataSet(resultTable1, Row.class);\n\t\tList<String> results1 = resultDataSet1.map(mapFunction).collect();\n\n\t\t// Execute second query and store results\n\t\tTable resultTable2 = tableEnv.sqlQuery(query2);\n\t\tDataSet<Row> resultDataSet2 = tableEnv.toDataSet(resultTable2, Row.class);\n\t\tList<String> results2 = resultDataSet2.map(mapFunction).collect();\n\n\t\t// Compare results\n\t\tTestBaseUtils.compareResultCollections(results1, results2, new Comparator<String>() {\n\n\t\t\t@Override\n\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\treturn o2 == null ? o1 == null ? 0 : 1 : o1.compareTo(o2);\n\t\t\t}\n\t\t});\n\t}"
        ],
        [
            "WordCountSQL::main(String)",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env);\n\n\t\tDataSet<WC> input = env.fromElements(\n\t\t\tnew WC(\"Hello\", 1),\n\t\t\tnew WC(\"Ciao\", 1),\n\t\t\tnew WC(\"Hello\", 1));\n\n\t\t// register the DataSet as table \"WordCount\"\n\t\ttEnv.registerDataSet(\"WordCount\", input, \"word, frequency\");\n\n\t\t// run a SQL query on the Table and retrieve the result as a new Table\n\t\tTable table = tEnv.sql(\n\t\t\t\"SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word\");\n\n\t\tDataSet<WC> result = tEnv.toDataSet(table, WC.class);\n\n\t\tresult.print();\n\t}",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env);\n\n\t\tDataSet<WC> input = env.fromElements(\n\t\t\tnew WC(\"Hello\", 1),\n\t\t\tnew WC(\"Ciao\", 1),\n\t\t\tnew WC(\"Hello\", 1));\n\n\t\t// register the DataSet as table \"WordCount\"\n\t\ttEnv.registerDataSet(\"WordCount\", input, \"word, frequency\");\n\n\t\t// run a SQL query on the Table and retrieve the result as a new Table\n\t\tTable table = tEnv.sqlQuery(\n\t\t\t\"SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word\");\n\n\t\tDataSet<WC> result = tEnv.toDataSet(table, WC.class);\n\n\t\tresult.print();\n\t}"
        ],
        [
            "JavaSqlITCase::testFilter()",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = JavaStreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = JavaStreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sqlQuery(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink<Row>());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "HBaseConnectorITCase::testTableSourceReadAsByteArray()",
            " 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "\t@Test\n\tpublic void testTableSourceReadAsByteArray() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\t// fetch row2 from the table till the end\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, byte[].class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, byte[].class);\n\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\t\ttableEnv.registerFunction(\"toUTF8\", new ToUTF8());\n\t\ttableEnv.registerFunction(\"toLong\", new ToLong());\n\n\t\tTable result = tableEnv.sql(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  toUTF8(h.family2.col1), \" +\n\t\t\t\t\"  toLong(h.family2.col2) \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"Hello-1,100\\n\" +\n\t\t\t\"Hello-2,200\\n\" +\n\t\t\t\"Hello-3,300\\n\" +\n\t\t\t\"null,400\\n\" +\n\t\t\t\"Hello-5,500\\n\" +\n\t\t\t\"Hello-6,600\\n\" +\n\t\t\t\"Hello-7,700\\n\" +\n\t\t\t\"null,800\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}",
            " 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 +\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "\t@Test\n\tpublic void testTableSourceReadAsByteArray() throws Exception {\n\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, new TableConfig());\n\t\t// fetch row2 from the table till the end\n\t\tHBaseTableSource hbaseTable = new HBaseTableSource(getConf(), TEST_TABLE);\n\t\thbaseTable.addColumn(FAMILY2, F2COL1, byte[].class);\n\t\thbaseTable.addColumn(FAMILY2, F2COL2, byte[].class);\n\n\t\ttableEnv.registerTableSource(\"hTable\", hbaseTable);\n\t\ttableEnv.registerFunction(\"toUTF8\", new ToUTF8());\n\t\ttableEnv.registerFunction(\"toLong\", new ToLong());\n\n\t\tTable result = tableEnv.sqlQuery(\n\t\t\t\"SELECT \" +\n\t\t\t\t\"  toUTF8(h.family2.col1), \" +\n\t\t\t\t\"  toLong(h.family2.col2) \" +\n\t\t\t\t\"FROM hTable AS h\"\n\t\t);\n\t\tDataSet<Row> resultSet = tableEnv.toDataSet(result, Row.class);\n\t\tList<Row> results = resultSet.collect();\n\n\t\tString expected =\n\t\t\t\"Hello-1,100\\n\" +\n\t\t\t\"Hello-2,200\\n\" +\n\t\t\t\"Hello-3,300\\n\" +\n\t\t\t\"null,400\\n\" +\n\t\t\t\"Hello-5,500\\n\" +\n\t\t\t\"Hello-6,600\\n\" +\n\t\t\t\"Hello-7,700\\n\" +\n\t\t\t\"null,800\\n\";\n\n\t\tTestBaseUtils.compareResultAsText(results, expected);\n\t}"
        ]
    ],
    "78df079bec8e63361f246fc698e0787ba232f083": [
        [
            "SingleInputGate::getNextBufferOrEvent()",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495 -\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510 -\n 511  \n 512  \n 513  \n 514  \n 515  \n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  ",
            "\t@Override\n\tpublic BufferOrEvent getNextBufferOrEvent() throws IOException, InterruptedException {\n\t\tif (hasReceivedAllEndOfPartitionEvents) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (isReleased) {\n\t\t\tthrow new IllegalStateException(\"Released\");\n\t\t}\n\n\t\trequestPartitions();\n\n\t\tInputChannel currentChannel;\n\t\tboolean moreAvailable;\n\n\t\tsynchronized (inputChannelsWithData) {\n\t\t\twhile (inputChannelsWithData.size() == 0) {\n\t\t\t\tif (isReleased) {\n\t\t\t\t\tthrow new IllegalStateException(\"Released\");\n\t\t\t\t}\n\n\t\t\t\tinputChannelsWithData.wait();\n\t\t\t}\n\n\t\t\tcurrentChannel = inputChannelsWithData.remove();\n\t\t\tmoreAvailable = inputChannelsWithData.size() > 0;\n\t\t}\n\n\t\tfinal BufferAndAvailability result = currentChannel.getNextBuffer();\n\n\t\t// Sanity check that notifications only happen when data is available\n\t\tif (result == null) {\n\t\t\tthrow new IllegalStateException(\"Bug in input gate/channel logic: input gate got \" +\n\t\t\t\t\t\"notified by channel about available data, but none was available.\");\n\t\t}\n\n\t\t// this channel was now removed from the non-empty channels queue\n\t\t// we re-add it in case it has more data, because in that case no \"non-empty\" notification\n\t\t// will come for that channel\n\t\tif (result.moreAvailable()) {\n\t\t\tqueueChannel(currentChannel);\n\t\t}\n\n\t\tfinal Buffer buffer = result.buffer();\n\t\tif (buffer.isBuffer()) {\n\t\t\treturn new BufferOrEvent(buffer, currentChannel.getChannelIndex(), moreAvailable);\n\t\t}\n\t\telse {\n\t\t\tfinal AbstractEvent event = EventSerializer.fromBuffer(buffer, getClass().getClassLoader());\n\n\t\t\tif (event.getClass() == EndOfPartitionEvent.class) {\n\t\t\t\tchannelsWithEndOfPartitionEvents.set(currentChannel.getChannelIndex());\n\n\t\t\t\tif (channelsWithEndOfPartitionEvents.cardinality() == numberOfInputChannels) {\n\t\t\t\t\thasReceivedAllEndOfPartitionEvents = true;\n\t\t\t\t}\n\n\t\t\t\tcurrentChannel.notifySubpartitionConsumed();\n\n\t\t\t\tcurrentChannel.releaseAllResources();\n\t\t\t}\n\n\t\t\treturn new BufferOrEvent(event, currentChannel.getChannelIndex(), moreAvailable);\n\t\t}\n\t}",
            " 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512 +\n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  ",
            "\t@Override\n\tpublic BufferOrEvent getNextBufferOrEvent() throws IOException, InterruptedException {\n\t\tif (hasReceivedAllEndOfPartitionEvents) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (isReleased) {\n\t\t\tthrow new IllegalStateException(\"Released\");\n\t\t}\n\n\t\trequestPartitions();\n\n\t\tInputChannel currentChannel;\n\t\tboolean moreAvailable;\n\t\tsynchronized (inputChannelsWithData) {\n\t\t\twhile (inputChannelsWithData.size() == 0) {\n\t\t\t\tif (isReleased) {\n\t\t\t\t\tthrow new IllegalStateException(\"Released\");\n\t\t\t\t}\n\n\t\t\t\tinputChannelsWithData.wait();\n\t\t\t}\n\n\t\t\tcurrentChannel = inputChannelsWithData.remove();\n\t\t\tenqueuedInputChannelsWithData.clear(currentChannel.getChannelIndex());\n\t\t\tmoreAvailable = inputChannelsWithData.size() > 0;\n\t\t}\n\n\t\tfinal BufferAndAvailability result = currentChannel.getNextBuffer();\n\t\t// Sanity check that notifications only happen when data is available\n\t\tif (result == null) {\n\t\t\tthrow new IllegalStateException(\"Bug in input gate/channel logic: input gate got \" +\n\t\t\t\t\t\"notified by channel about available data, but none was available.\");\n\t\t}\n\t\t// this channel was now removed from the non-empty channels queue\n\t\t// we re-add it in case it has more data, because in that case no \"non-empty\" notification\n\t\t// will come for that channel\n\t\tif (result.moreAvailable()) {\n\t\t\tqueueChannel(currentChannel);\n\t\t}\n\n\t\tfinal Buffer buffer = result.buffer();\n\t\tif (buffer.isBuffer()) {\n\t\t\treturn new BufferOrEvent(buffer, currentChannel.getChannelIndex(), moreAvailable);\n\t\t}\n\t\telse {\n\t\t\tfinal AbstractEvent event = EventSerializer.fromBuffer(buffer, getClass().getClassLoader());\n\n\t\t\tif (event.getClass() == EndOfPartitionEvent.class) {\n\t\t\t\tchannelsWithEndOfPartitionEvents.set(currentChannel.getChannelIndex());\n\n\t\t\t\tif (channelsWithEndOfPartitionEvents.cardinality() == numberOfInputChannels) {\n\t\t\t\t\thasReceivedAllEndOfPartitionEvents = true;\n\t\t\t\t}\n\n\t\t\t\tcurrentChannel.notifySubpartitionConsumed();\n\n\t\t\t\tcurrentChannel.releaseAllResources();\n\t\t\t}\n\n\t\t\treturn new BufferOrEvent(event, currentChannel.getChannelIndex(), moreAvailable);\n\t\t}\n\t}"
        ],
        [
            "StreamTestSingleInputGate::setupInputChannels()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 -\n 115  \n 116  \n 117 -\n 118  \n 119 -\n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  ",
            "\t@SuppressWarnings(\"unchecked\")\n\tprivate void setupInputChannels() throws IOException, InterruptedException {\n\n\t\tfor (int i = 0; i < numInputChannels; i++) {\n\t\t\tfinal int channelIndex = i;\n\t\t\tfinal RecordSerializer<SerializationDelegate<Object>> recordSerializer = new SpanningRecordSerializer<SerializationDelegate<Object>>();\n\t\t\tfinal SerializationDelegate<Object> delegate = (SerializationDelegate<Object>) (SerializationDelegate<?>)\n\t\t\t\tnew SerializationDelegate<StreamElement>(new StreamElementSerializer<T>(serializer));\n\n\t\t\tinputQueues[channelIndex] = new ConcurrentLinkedQueue<InputValue<Object>>();\n\t\t\tinputChannels[channelIndex] = new TestInputChannel(inputGate, i);\n\n\t\t\tfinal Answer<BufferAndAvailability> answer = new Answer<BufferAndAvailability>() {\n\t\t\t\t@Override\n\t\t\t\tpublic BufferAndAvailability answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\t\tInputValue<Object> input = inputQueues[channelIndex].poll();\n\t\t\t\t\tif (input != null && input.isStreamEnd()) {\n\t\t\t\t\t\twhen(inputChannels[channelIndex].getInputChannel().isReleased()).thenReturn(\n\t\t\t\t\t\t\ttrue);\n\t\t\t\t\t\treturn new BufferAndAvailability(EventSerializer.toBuffer(EndOfPartitionEvent.INSTANCE), false, 0);\n\t\t\t\t\t} else if (input != null && input.isStreamRecord()) {\n\t\t\t\t\t\tObject inputElement = input.getStreamRecord();\n\n\t\t\t\t\t\tBufferBuilder bufferBuilder = createBufferBuilder(bufferSize);\n\t\t\t\t\t\trecordSerializer.setNextBufferBuilder(bufferBuilder);\n\t\t\t\t\t\tdelegate.setInstance(inputElement);\n\t\t\t\t\t\trecordSerializer.addRecord(delegate);\n\t\t\t\t\t\tbufferBuilder.finish();\n\n\t\t\t\t\t\t// Call getCurrentBuffer to ensure size is set\n\t\t\t\t\t\treturn new BufferAndAvailability(buildSingleBuffer(bufferBuilder), false, 0);\n\t\t\t\t\t} else if (input != null && input.isEvent()) {\n\t\t\t\t\t\tAbstractEvent event = input.getEvent();\n\t\t\t\t\t\treturn new BufferAndAvailability(EventSerializer.toBuffer(event), false, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tsynchronized (inputQueues[channelIndex]) {\n\t\t\t\t\t\t\tinputQueues[channelIndex].wait();\n\t\t\t\t\t\t\treturn answer(invocationOnMock);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\twhen(inputChannels[channelIndex].getInputChannel().getNextBuffer()).thenAnswer(answer);\n\n\t\t\tinputGate.setInputChannel(new IntermediateResultPartitionID(),\n\t\t\t\tinputChannels[channelIndex].getInputChannel());\n\t\t}\n\t}",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  \n 123 +\n 124  \n 125 +\n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "\t@SuppressWarnings(\"unchecked\")\n\tprivate void setupInputChannels() throws IOException, InterruptedException {\n\n\t\tfor (int i = 0; i < numInputChannels; i++) {\n\t\t\tfinal int channelIndex = i;\n\t\t\tfinal RecordSerializer<SerializationDelegate<Object>> recordSerializer = new SpanningRecordSerializer<SerializationDelegate<Object>>();\n\t\t\tfinal SerializationDelegate<Object> delegate = (SerializationDelegate<Object>) (SerializationDelegate<?>)\n\t\t\t\tnew SerializationDelegate<StreamElement>(new StreamElementSerializer<T>(serializer));\n\n\t\t\tinputQueues[channelIndex] = new ConcurrentLinkedQueue<InputValue<Object>>();\n\t\t\tinputChannels[channelIndex] = new TestInputChannel(inputGate, i);\n\n\t\t\tfinal Answer<BufferAndAvailability> answer = new Answer<BufferAndAvailability>() {\n\t\t\t\t@Override\n\t\t\t\tpublic BufferAndAvailability answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\t\tConcurrentLinkedQueue<InputValue<Object>> inputQueue = inputQueues[channelIndex];\n\t\t\t\t\tInputValue<Object> input;\n\t\t\t\t\tboolean moreAvailable;\n\t\t\t\t\tsynchronized (inputQueue) {\n\t\t\t\t\t\tinput = inputQueue.poll();\n\t\t\t\t\t\tmoreAvailable = !inputQueue.isEmpty();\n\t\t\t\t\t}\n\t\t\t\t\tif (input != null && input.isStreamEnd()) {\n\t\t\t\t\t\twhen(inputChannels[channelIndex].getInputChannel().isReleased()).thenReturn(\n\t\t\t\t\t\t\ttrue);\n\t\t\t\t\t\treturn new BufferAndAvailability(EventSerializer.toBuffer(EndOfPartitionEvent.INSTANCE), moreAvailable, 0);\n\t\t\t\t\t} else if (input != null && input.isStreamRecord()) {\n\t\t\t\t\t\tObject inputElement = input.getStreamRecord();\n\n\t\t\t\t\t\tBufferBuilder bufferBuilder = createBufferBuilder(bufferSize);\n\t\t\t\t\t\trecordSerializer.setNextBufferBuilder(bufferBuilder);\n\t\t\t\t\t\tdelegate.setInstance(inputElement);\n\t\t\t\t\t\trecordSerializer.addRecord(delegate);\n\t\t\t\t\t\tbufferBuilder.finish();\n\n\t\t\t\t\t\t// Call getCurrentBuffer to ensure size is set\n\t\t\t\t\t\treturn new BufferAndAvailability(buildSingleBuffer(bufferBuilder), moreAvailable, 0);\n\t\t\t\t\t} else if (input != null && input.isEvent()) {\n\t\t\t\t\t\tAbstractEvent event = input.getEvent();\n\t\t\t\t\t\treturn new BufferAndAvailability(EventSerializer.toBuffer(event), moreAvailable, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tsynchronized (inputQueue) {\n\t\t\t\t\t\t\tinputQueue.wait();\n\t\t\t\t\t\t\treturn answer(invocationOnMock);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\twhen(inputChannels[channelIndex].getInputChannel().getNextBuffer()).thenAnswer(answer);\n\n\t\t\tinputGate.setInputChannel(new IntermediateResultPartitionID(),\n\t\t\t\tinputChannels[channelIndex].getInputChannel());\n\t\t}\n\t}"
        ],
        [
            "SingleInputGate::queueChannel(InputChannel)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  ",
            "\tprivate void queueChannel(InputChannel channel) {\n\t\tint availableChannels;\n\n\t\tsynchronized (inputChannelsWithData) {\n\t\t\tavailableChannels = inputChannelsWithData.size();\n\n\t\t\tinputChannelsWithData.add(channel);\n\n\t\t\tif (availableChannels == 0) {\n\t\t\t\tinputChannelsWithData.notifyAll();\n\t\t\t}\n\t\t}\n\n\t\tif (availableChannels == 0) {\n\t\t\tInputGateListener listener = inputGateListener;\n\t\t\tif (listener != null) {\n\t\t\t\tlistener.notifyInputGateNonEmpty(this);\n\t\t\t}\n\t\t}\n\t}",
            " 586  \n 587  \n 588  \n 589  \n 590 +\n 591 +\n 592 +\n 593  \n 594  \n 595  \n 596 +\n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  ",
            "\tprivate void queueChannel(InputChannel channel) {\n\t\tint availableChannels;\n\n\t\tsynchronized (inputChannelsWithData) {\n\t\t\tif (enqueuedInputChannelsWithData.get(channel.getChannelIndex())) {\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tavailableChannels = inputChannelsWithData.size();\n\n\t\t\tinputChannelsWithData.add(channel);\n\t\t\tenqueuedInputChannelsWithData.set(channel.getChannelIndex());\n\n\t\t\tif (availableChannels == 0) {\n\t\t\t\tinputChannelsWithData.notifyAll();\n\t\t\t}\n\t\t}\n\n\t\tif (availableChannels == 0) {\n\t\t\tInputGateListener listener = inputGateListener;\n\t\t\tif (listener != null) {\n\t\t\t\tlistener.notifyInputGateNonEmpty(this);\n\t\t\t}\n\t\t}\n\t}"
        ]
    ],
    "6fcc1e9a868ff3ce2bd0376b6b3493a08c7b604c": [
        [
            "LocalExecutorITCase::testGetSessionProperties()",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "\t@Test\n\tpublic void testGetSessionProperties() throws Exception {\n\t\tfinal Executor executor = createDefaultExecutor(clusterClient);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\tsession.setSessionProperty(\"execution.result-mode\", \"changelog\");\n\n\t\texecutor.getSessionProperties(session);\n\n\t\t// modify defaults\n\t\tsession.setSessionProperty(\"execution.type\", \"streaming\");\n\t\tsession.setSessionProperty(\"execution.result-mode\", \"table\");\n\n\t\tfinal Map<String, String> actualProperties = executor.getSessionProperties(session);\n\n\t\tfinal Map<String, String> expectedProperties = new HashMap<>();\n\t\texpectedProperties.put(\"execution.type\", \"streaming\");\n\t\texpectedProperties.put(\"execution.time-characteristic\", \"event-time\");\n\t\texpectedProperties.put(\"execution.periodic-watermarks-interval\", \"99\");\n\t\texpectedProperties.put(\"execution.parallelism\", \"1\");\n\t\texpectedProperties.put(\"execution.max-parallelism\", \"16\");\n\t\texpectedProperties.put(\"execution.max-idle-state-retention\", \"0\");\n\t\texpectedProperties.put(\"execution.min-idle-state-retention\", \"0\");\n\t\texpectedProperties.put(\"execution.result-mode\", \"table\");\n\t\texpectedProperties.put(\"deployment.response-timeout\", \"5000\");\n\n\t\tassertEquals(expectedProperties, actualProperties);\n\t}",
            " 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  ",
            "\t@Test\n\tpublic void testGetSessionProperties() throws Exception {\n\t\tfinal Executor executor = createDefaultExecutor(clusterClient);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\tsession.setSessionProperty(\"execution.result-mode\", \"changelog\");\n\n\t\texecutor.getSessionProperties(session);\n\n\t\t// modify defaults\n\t\tsession.setSessionProperty(\"execution.result-mode\", \"table\");\n\n\t\tfinal Map<String, String> actualProperties = executor.getSessionProperties(session);\n\n\t\tfinal Map<String, String> expectedProperties = new HashMap<>();\n\t\texpectedProperties.put(\"execution.type\", \"batch\");\n\t\texpectedProperties.put(\"execution.time-characteristic\", \"event-time\");\n\t\texpectedProperties.put(\"execution.periodic-watermarks-interval\", \"99\");\n\t\texpectedProperties.put(\"execution.parallelism\", \"1\");\n\t\texpectedProperties.put(\"execution.max-parallelism\", \"16\");\n\t\texpectedProperties.put(\"execution.max-idle-state-retention\", \"0\");\n\t\texpectedProperties.put(\"execution.min-idle-state-retention\", \"0\");\n\t\texpectedProperties.put(\"execution.result-mode\", \"table\");\n\t\texpectedProperties.put(\"deployment.response-timeout\", \"5000\");\n\n\t\tassertEquals(expectedProperties, actualProperties);\n\t}"
        ],
        [
            "TestTableSinkFactory::requiredContext()",
            "  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tfinal Map<String, String> context = new HashMap<>();\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE);\n\t\treturn context;\n\t}",
            "  56  \n  57  \n  58  \n  59 +\n  60  \n  61  \n  62  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tfinal Map<String, String> context = new HashMap<>();\n\t\tcontext.put(UPDATE_MODE(), UPDATE_MODE_VALUE_APPEND());\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE);\n\t\treturn context;\n\t}"
        ],
        [
            "LocalExecutorITCase::testStreamQueryExecutionTable()",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionTable() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_3\", \"table\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job and retrieval\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(\n\t\t\t\tsession,\n\t\t\t\t\"SELECT scalarUDF(IntegerField1), StringField1 FROM TableNumber1\");\n\n\t\t\tassertTrue(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults = retrieveTableResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"47,Hello World\");\n\t\t\texpectedResults.add(\"27,Hello World\");\n\t\t\texpectedResults.add(\"37,Hello World\");\n\t\t\texpectedResults.add(\"37,Hello World\");\n\t\t\texpectedResults.add(\"47,Hello World\");\n\t\t\texpectedResults.add(\"57,Hello World!!!!\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionTable() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_3\", \"table\");\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"update-mode: append\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job and retrieval\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(\n\t\t\t\tsession,\n\t\t\t\t\"SELECT scalarUDF(IntegerField1), StringField1 FROM TableNumber1\");\n\n\t\t\tassertTrue(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults = retrieveTableResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"47,Hello World\");\n\t\t\texpectedResults.add(\"27,Hello World\");\n\t\t\texpectedResults.add(\"37,Hello World\");\n\t\t\texpectedResults.add(\"37,Hello World\");\n\t\t\texpectedResults.add(\"47,Hello World\");\n\t\t\texpectedResults.add(\"57,Hello World!!!!\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}"
        ],
        [
            "LocalExecutorITCase::createDefaultExecutor(ClusterClient)",
            " 335  \n 336  \n 337 -\n 338  \n 339  \n 340  \n 341  ",
            "\tprivate <T> LocalExecutor createDefaultExecutor(ClusterClient<T> clusterClient) throws Exception {\n\t\treturn new LocalExecutor(\n\t\t\tEnvironmentFileUtil.parseModified(DEFAULTS_ENVIRONMENT_FILE, Collections.singletonMap(\"$VAR_2\", \"batch\")),\n\t\t\tCollections.emptyList(),\n\t\t\tclusterClient.getFlinkConfiguration(),\n\t\t\tnew DummyCustomCommandLine<T>(clusterClient));\n\t}",
            " 338  \n 339 +\n 340 +\n 341 +\n 342  \n 343 +\n 344  \n 345  \n 346  \n 347  ",
            "\tprivate <T> LocalExecutor createDefaultExecutor(ClusterClient<T> clusterClient) throws Exception {\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_2\", \"batch\");\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"\");\n\t\treturn new LocalExecutor(\n\t\t\tEnvironmentFileUtil.parseModified(DEFAULTS_ENVIRONMENT_FILE, replaceVars),\n\t\t\tCollections.emptyList(),\n\t\t\tclusterClient.getFlinkConfiguration(),\n\t\t\tnew DummyCustomCommandLine<T>(clusterClient));\n\t}"
        ],
        [
            "KafkaJsonTableSourceFactoryTestBase::testTableSource(FormatDescriptor)",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144 -\n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonRowSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_DEC)\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.withRowtimeAttribute(\"event-time\", new ExistingField(\"time\"), new AscendingTimestamps())\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(format)\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_DEC) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP).rowtime(\n\t\t\t\t\t\t\tnew Rowtime().timestampsFromField(\"time\").watermarksPeriodicAscending())\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime());\n\n\t\tDescriptorProperties properties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(properties);\n\t\tfinal TableSource<?> factorySource =\n\t\t\t\tTableFactoryService.find(StreamTableSourceFactory.class, testDesc)\n\t\t\t\t\t\t.createStreamTableSource(properties.asMap());\n\n\t\tassertEquals(builderSource, factorySource);\n\t}",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152 +\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonRowSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_DEC)\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.withRowtimeAttribute(\"event-time\", new ExistingField(\"time\"), new AscendingTimestamps())\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableDescriptor testDesc = new TestTableDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.withFormat(format)\n\t\t\t.withSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_DEC) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP).rowtime(\n\t\t\t\t\t\t\tnew Rowtime().timestampsFromField(\"time\").watermarksPeriodicAscending())\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime())\n\t\t\t.inAppendMode();\n\n\t\tDescriptorProperties properties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(properties);\n\t\tfinal TableSource<?> factorySource =\n\t\t\t\tTableFactoryService.find(StreamTableSourceFactory.class, testDesc)\n\t\t\t\t\t\t.createStreamTableSource(properties.asMap());\n\n\t\tassertEquals(builderSource, factorySource);\n\t}"
        ],
        [
            "LocalExecutorITCase::testStreamQueryExecutionSink()",
            " 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionSink() throws Exception {\n\t\tfinal String csvOutputPath = new File(tempFolder.newFolder().getAbsolutePath(), \"test-out.csv\").toURI().toString();\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_4\", csvOutputPath);\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job\n\t\t\tfinal ProgramTargetDescriptor targetDescriptor = executor.executeUpdate(\n\t\t\t\tsession,\n\t\t\t\t\"INSERT INTO TableSourceSink SELECT IntegerField1 = 42, StringField1 FROM TableNumber1\");\n\n\t\t\t// wait for job completion and verify result\n\t\t\tboolean isRunning = true;\n\t\t\twhile (isRunning) {\n\t\t\t\tThread.sleep(50); // slow the processing down\n\t\t\t\tfinal JobStatus jobStatus = clusterClient.getJobStatus(JobID.fromHexString(targetDescriptor.getJobId())).get();\n\t\t\t\tswitch (jobStatus) {\n\t\t\t\t\tcase CREATED:\n\t\t\t\t\tcase RUNNING:\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tcase FINISHED:\n\t\t\t\t\t\tisRunning = false;\n\t\t\t\t\t\tverifySinkResult(csvOutputPath);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tfail(\"Unexpected job status.\");\n\t\t\t\t}\n\t\t\t}\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}",
            " 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 +\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionSink() throws Exception {\n\t\tfinal String csvOutputPath = new File(tempFolder.newFolder().getAbsolutePath(), \"test-out.csv\").toURI().toString();\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_4\", csvOutputPath);\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"update-mode: append\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job\n\t\t\tfinal ProgramTargetDescriptor targetDescriptor = executor.executeUpdate(\n\t\t\t\tsession,\n\t\t\t\t\"INSERT INTO TableSourceSink SELECT IntegerField1 = 42, StringField1 FROM TableNumber1\");\n\n\t\t\t// wait for job completion and verify result\n\t\t\tboolean isRunning = true;\n\t\t\twhile (isRunning) {\n\t\t\t\tThread.sleep(50); // slow the processing down\n\t\t\t\tfinal JobStatus jobStatus = clusterClient.getJobStatus(JobID.fromHexString(targetDescriptor.getJobId())).get();\n\t\t\t\tswitch (jobStatus) {\n\t\t\t\t\tcase CREATED:\n\t\t\t\t\tcase RUNNING:\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tcase FINISHED:\n\t\t\t\t\t\tisRunning = false;\n\t\t\t\t\t\tverifySinkResult(csvOutputPath);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tfail(\"Unexpected job status.\");\n\t\t\t\t}\n\t\t\t}\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}"
        ],
        [
            "KafkaTableSourceFactoryTestBase::testTableSource()",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(new TestTableFormat())\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime());\n\t\tfinal DescriptorProperties descriptorProperties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(descriptorProperties);\n\t\tfinal Map<String, String> propertiesMap = descriptorProperties.asMap();\n\n\t\tfinal TableSource<?> actualSource = TableFactoryService.find(StreamTableSourceFactory.class, testDesc)\n\t\t\t.createStreamTableSource(propertiesMap);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableDescriptor testDesc = new TestTableDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.withFormat(new TestTableFormat())\n\t\t\t.withSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime())\n\t\t\t.inAppendMode();\n\t\tfinal DescriptorProperties descriptorProperties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(descriptorProperties);\n\t\tfinal Map<String, String> propertiesMap = descriptorProperties.asMap();\n\n\t\tfinal TableSource<?> actualSource = TableFactoryService.find(StreamTableSourceFactory.class, testDesc)\n\t\t\t.createStreamTableSource(propertiesMap);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}"
        ],
        [
            "TestTableSourceFactory::requiredContext()",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tfinal Map<String, String> context = new HashMap<>();\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE);\n\t\treturn context;\n\t}",
            "  59  \n  60  \n  61  \n  62 +\n  63  \n  64  \n  65  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tfinal Map<String, String> context = new HashMap<>();\n\t\tcontext.put(UPDATE_MODE(), UPDATE_MODE_VALUE_APPEND());\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE);\n\t\treturn context;\n\t}"
        ],
        [
            "KafkaTableSourceFactory::requiredContext()",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tMap<String, String> context = new HashMap<>();\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE_KAFKA); // kafka\n\t\tcontext.put(CONNECTOR_VERSION(), kafkaVersion()); // version\n\t\tcontext.put(CONNECTOR_PROPERTY_VERSION(), \"1\"); // backwards compatibility\n\t\treturn context;\n\t}",
            "  78  \n  79  \n  80  \n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  ",
            "\t@Override\n\tpublic Map<String, String> requiredContext() {\n\t\tMap<String, String> context = new HashMap<>();\n\t\tcontext.put(UPDATE_MODE(), UPDATE_MODE_VALUE_APPEND()); // append mode\n\t\tcontext.put(CONNECTOR_TYPE(), CONNECTOR_TYPE_VALUE_KAFKA); // kafka\n\t\tcontext.put(CONNECTOR_VERSION(), kafkaVersion()); // version\n\t\tcontext.put(CONNECTOR_PROPERTY_VERSION(), \"1\"); // backwards compatibility\n\t\treturn context;\n\t}"
        ],
        [
            "LocalExecutorITCase::testStreamQueryExecutionChangelog()",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionChangelog() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_3\", \"changelog\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job and retrieval\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(\n\t\t\t\tsession,\n\t\t\t\t\"SELECT scalarUDF(IntegerField1), StringField1 FROM TableNumber1\");\n\n\t\t\tassertFalse(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults =\n\t\t\t\t\tretrieveChangelogResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"(true,47,Hello World)\");\n\t\t\texpectedResults.add(\"(true,27,Hello World)\");\n\t\t\texpectedResults.add(\"(true,37,Hello World)\");\n\t\t\texpectedResults.add(\"(true,37,Hello World)\");\n\t\t\texpectedResults.add(\"(true,47,Hello World)\");\n\t\t\texpectedResults.add(\"(true,57,Hello World!!!!)\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}",
            " 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testStreamQueryExecutionChangelog() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_3\", \"changelog\");\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"update-mode: append\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\t// start job and retrieval\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(\n\t\t\t\tsession,\n\t\t\t\t\"SELECT scalarUDF(IntegerField1), StringField1 FROM TableNumber1\");\n\n\t\t\tassertFalse(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults =\n\t\t\t\t\tretrieveChangelogResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"(true,47,Hello World)\");\n\t\t\texpectedResults.add(\"(true,27,Hello World)\");\n\t\t\texpectedResults.add(\"(true,37,Hello World)\");\n\t\t\texpectedResults.add(\"(true,37,Hello World)\");\n\t\t\texpectedResults.add(\"(true,47,Hello World)\");\n\t\t\texpectedResults.add(\"(true,57,Hello World!!!!)\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}"
        ],
        [
            "ExecutionContextTest::createExecutionContext()",
            " 114  \n 115  \n 116  \n 117 -\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "\tprivate <T> ExecutionContext<T> createExecutionContext() throws Exception {\n\t\tfinal Environment env = EnvironmentFileUtil.parseModified(\n\t\t\tDEFAULTS_ENVIRONMENT_FILE,\n\t\t\tCollections.singletonMap(\"$VAR_2\", \"streaming\"));\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\treturn new ExecutionContext<>(\n\t\t\tenv,\n\t\t\tsession,\n\t\t\tCollections.emptyList(),\n\t\t\tflinkConfig,\n\t\t\tnew Options(),\n\t\t\tCollections.singletonList(new DefaultCLI(flinkConfig)));\n\t}",
            " 115  \n 116 +\n 117 +\n 118 +\n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  ",
            "\tprivate <T> ExecutionContext<T> createExecutionContext() throws Exception {\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_2\", \"streaming\");\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"update-mode: append\");\n\t\tfinal Environment env = EnvironmentFileUtil.parseModified(\n\t\t\tDEFAULTS_ENVIRONMENT_FILE,\n\t\t\treplaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\treturn new ExecutionContext<>(\n\t\t\tenv,\n\t\t\tsession,\n\t\t\tCollections.emptyList(),\n\t\t\tflinkConfig,\n\t\t\tnew Options(),\n\t\t\tCollections.singletonList(new DefaultCLI(flinkConfig)));\n\t}"
        ],
        [
            "EnvironmentTest::testMerging()",
            "  41  \n  42  \n  43 -\n  44  \n  45  \n  46 -\n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  ",
            "\t@Test\n\tpublic void testMerging() throws Exception {\n\t\tfinal Environment env1 = EnvironmentFileUtil.parseUnmodified(DEFAULTS_ENVIRONMENT_FILE);\n\t\tfinal Environment env2 = EnvironmentFileUtil.parseModified(\n\t\t\tFACTORY_ENVIRONMENT_FILE,\n\t\t\tCollections.singletonMap(\"TableNumber1\", \"NewTable\"));\n\n\t\tfinal Environment merged = Environment.merge(env1, env2);\n\n\t\tfinal Set<String> tables = new HashSet<>();\n\t\ttables.add(\"TableNumber1\");\n\t\ttables.add(\"TableNumber2\");\n\t\ttables.add(\"NewTable\");\n\t\ttables.add(\"TableSourceSink\");\n\n\t\tassertEquals(tables, merged.getTables().keySet());\n\t\tassertTrue(merged.getExecution().isStreamingExecution());\n\t\tassertEquals(16, merged.getExecution().getMaxParallelism());\n\t}",
            "  42  \n  43  \n  44 +\n  45 +\n  46 +\n  47 +\n  48 +\n  49 +\n  50 +\n  51 +\n  52  \n  53  \n  54 +\n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\t@Test\n\tpublic void testMerging() throws Exception {\n\t\tfinal Map<String, String> replaceVars1 = new HashMap<>();\n\t\treplaceVars1.put(\"$VAR_UPDATE_MODE\", \"update-mode: append\");\n\t\tfinal Environment env1 = EnvironmentFileUtil.parseModified(\n\t\t\tDEFAULTS_ENVIRONMENT_FILE,\n\t\t\treplaceVars1);\n\n\t\tfinal Map<String, String> replaceVars2 = new HashMap<>(replaceVars1);\n\t\treplaceVars2.put(\"TableNumber1\", \"NewTable\");\n\t\tfinal Environment env2 = EnvironmentFileUtil.parseModified(\n\t\t\tFACTORY_ENVIRONMENT_FILE,\n\t\t\treplaceVars2);\n\n\t\tfinal Environment merged = Environment.merge(env1, env2);\n\n\t\tfinal Set<String> tables = new HashSet<>();\n\t\ttables.add(\"TableNumber1\");\n\t\ttables.add(\"TableNumber2\");\n\t\ttables.add(\"NewTable\");\n\t\ttables.add(\"TableSourceSink\");\n\n\t\tassertEquals(tables, merged.getTables().keySet());\n\t\tassertTrue(merged.getExecution().isStreamingExecution());\n\t\tassertEquals(16, merged.getExecution().getMaxParallelism());\n\t}"
        ],
        [
            "LocalExecutorITCase::testBatchQueryExecution()",
            " 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testBatchQueryExecution() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"batch\");\n\t\treplaceVars.put(\"$VAR_3\", \"table\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(session, \"SELECT IntegerField1 FROM TableNumber1\");\n\n\t\t\tassertTrue(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults = retrieveTableResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"42\");\n\t\t\texpectedResults.add(\"22\");\n\t\t\texpectedResults.add(\"32\");\n\t\t\texpectedResults.add(\"32\");\n\t\t\texpectedResults.add(\"42\");\n\t\t\texpectedResults.add(\"52\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}",
            " 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  ",
            "\t@Test(timeout = 30_000L)\n\tpublic void testBatchQueryExecution() throws Exception {\n\t\tfinal URL url = getClass().getClassLoader().getResource(\"test-data.csv\");\n\t\tObjects.requireNonNull(url);\n\t\tfinal Map<String, String> replaceVars = new HashMap<>();\n\t\treplaceVars.put(\"$VAR_0\", url.getPath());\n\t\treplaceVars.put(\"$VAR_1\", \"/\");\n\t\treplaceVars.put(\"$VAR_2\", \"batch\");\n\t\treplaceVars.put(\"$VAR_3\", \"table\");\n\t\treplaceVars.put(\"$VAR_UPDATE_MODE\", \"\");\n\n\t\tfinal Executor executor = createModifiedExecutor(clusterClient, replaceVars);\n\t\tfinal SessionContext session = new SessionContext(\"test-session\", new Environment());\n\n\t\ttry {\n\t\t\tfinal ResultDescriptor desc = executor.executeQuery(session, \"SELECT IntegerField1 FROM TableNumber1\");\n\n\t\t\tassertTrue(desc.isMaterialized());\n\n\t\t\tfinal List<String> actualResults = retrieveTableResult(executor, session, desc.getResultId());\n\n\t\t\tfinal List<String> expectedResults = new ArrayList<>();\n\t\t\texpectedResults.add(\"42\");\n\t\t\texpectedResults.add(\"22\");\n\t\t\texpectedResults.add(\"32\");\n\t\t\texpectedResults.add(\"32\");\n\t\t\texpectedResults.add(\"42\");\n\t\t\texpectedResults.add(\"52\");\n\n\t\t\tTestBaseUtils.compareResultCollections(expectedResults, actualResults, Comparator.naturalOrder());\n\t\t} finally {\n\t\t\texecutor.stop(session);\n\t\t}\n\t}"
        ]
    ]
}